{"index":"<h1>Overview </h1>  <p>The Pig Documentation provides the information you need to get started using Pig. If you haven't already, download Pig now: .</p> <p>Begin with the <a href=\"start\">Getting Started</a> guide which shows you how to set up Pig and how to form simple Pig Latin statements. When you are ready to start writing your own scripts, review the <a href=\"basic\">Pig Latin Basics</a> manual to become familiar with the Pig Latin operators and the supported data types.</p> <p>Functions can be a part of almost every operator in Pig. The <a href=\"func\">Built In Functions</a> guide describes Pig's built in functions. The <a href=\"udf\">User Defined Functions</a> manual shows you how to how to write your own functions and how to access/contribute functions using the Piggy Bank repository.</p> <p>The mechanisms featured in the <a href=\"cont\">Control Structures</a> guide give you greater control over how your Pig scripts are structured and executed. The <a href=\"cont\">Performance and Efficiency</a> guide provides valuable examples and suggestions for optimizing your code.</p> <p>Use Pig's Administration features <a href=\"admin\">Administration</a> which provides properties that could be set to be used by all your users.</p> <p>Finally, use Pig's <a href=\"cmds\">Shell and Utility Commands</a> to run your programs and Pig's expanded <a href=\"test\">Testing and Diagnostics</a> tools to examine and/or debug your programs.</p> <p>If you have more questions, you can ask on the <a href=\"http://hadoop.apache.org/pig/mailing_lists.html\">Pig Mailing Lists</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/</a>\n  </p>\n</div>\n","admin":"<h1>Administration</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#Output+location+strict+check\">Output location strict check</a> </li> <li> <a href=\"#Disabling+Pig+commands+and+operators\">Disabling Pig commands and operators</a> <ul class=\"minitoc\"> <li> <a href=\"#Blacklisting\">Blacklisting</a> </li> <li> <a href=\"#Whitelisting\">Whitelisting</a> </li> <li> <a href=\"#Note\">Note</a> </li> </ul> </li> </ul> </div> </div>    <h2 id=\"Output+location+strict+check\">Output location strict check</h2> <div class=\"section\"> <p>Pig scripts could contain multiple STORE statements. There are cases when one would like to avoid writing to the same output location. Pig provides admins/script writers with a property to check if multiple STORE statements make an attempt to write to the same output directory. And fail fast letting the user know of the same.</p> <p>Specifically this makes sense for file-based output locations (HDFS, Local FS, S3..) to avoid Pig script from failing when multiple MR jobs write to the same location. </p> <p>To enforce strict checking of output location, set <strong>pig.location.check.strict=true</strong>. See also <a href=\"start#properties\">Pig Properties</a> on how to set this property.</p> </div>   <h2 id=\"Disabling+Pig+commands+and+operators\">Disabling Pig commands and operators</h2> <div class=\"section\"> <p>This is an admin feature providing ability to blacklist or/and whitelist certain commands and operations. Pig exposes a few of these that could be not very safe in a multitenant environment. For example, \"sh\" invokes shell commands, \"set\" allows users to change non-final configs. While these are tremendously useful in general, having an ability to disable would make Pig a safer platform. The goal is to allow administrators to be able to have more control over user scripts. Default behaviour would still be the same - no filters applied on commands and operators.</p> <p>There are two properties you can use to control what users are able to do</p> <ul> <li>pig.blacklist</li> <li>pig.whitelist</li> </ul>  <h3 id=\"Blacklisting\">Blacklisting</h3> <p>Set \"pig.blacklist\" to a comma-delimited set of operators and commands. For eg, <strong>pig.blacklist=rm,kill,cross</strong> would disable users from executing any of \"rm\", \"kill\" commands and \"cross\" operator.</p>  <h3 id=\"Whitelisting\">Whitelisting</h3> <p>This is an even safer approach to disallowing functionality in Pig. Using this you will be able to disable all commands and operators that are not a part of the whitelist. For eg, <strong>pig.whitelist=load,filter,store</strong> will disallow every command and operator other than \"load\", \"filter\" and \"store\".</p>  <h3 id=\"Note\">Note</h3> <p>There should not be any conflicts between blacklist and whitelist. Make sure to have them entirely distinct or Pig will complain.</p> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/admin.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/admin.html</a>\n  </p>\n</div>\n","cmds":"<h1>Shell and Utility Commands</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#shell-cmds\">Shell Commands</a> <ul class=\"minitoc\"> <li> <a href=\"#fs\">fs</a> </li> <li> <a href=\"#sh\">sh</a> </li> </ul> </li> <li> <a href=\"#utillity-cmds\">Utility Commands</a> <ul class=\"minitoc\"> <li> <a href=\"#clear\">clear</a> </li> <li> <a href=\"#exec\">exec</a> </li> <li> <a href=\"#help\">help</a> </li> <li> <a href=\"#history\">history</a> </li> <li> <a href=\"#kill\">kill</a> </li> <li> <a href=\"#quit\">quit</a> </li> <li> <a href=\"#run\">run</a> </li> <li> <a href=\"#set\">set</a> </li> </ul> </li> </ul> </div> </div>    <h2 id=\"shell-cmds\">Shell Commands</h2> <div class=\"section\">  <h3 id=\"fs\">fs</h3> <p>Invokes any FsShell command from within a Pig script or the Grunt shell.</p>  <h4 id=\"Syntax\">Syntax </h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>fs subcommand subcommand_parameters </p> </td> </tr> </table>  <h4 id=\"Terms\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>subcommand</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The FsShell command.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>subcommand_parameters</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The FsShell command parameters.</p> </td> </tr> </table>  <h4 id=\"Usage\">Usage</h4> <p>Use the fs command to invoke any FsShell command from within a Pig script or Grunt shell. The fs command greatly extends the set of supported file system commands and the capabilities supported for existing commands such as ls that will now support globing. For a complete list of FsShell commands, see <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html\">File System Shell Guide</a> </p>  <h4 id=\"Examples\">Examples</h4> <p>In these examples a directory is created, a file is copied, a file is listed.</p> <pre class=\"code\">\nfs -mkdir /tmp\nfs -copyFromLocal file-x file-y\nfs -ls file-y\n</pre>  <h3 id=\"sh\">sh</h3> <p>Invokes any sh shell command from within a Pig script or the Grunt shell.</p>  <h4 id=\"Syntax-N1008E\">Syntax </h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>sh subcommand subcommand_parameters </p> </td> </tr> </table>  <h4 id=\"Terms-N100A2\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>subcommand</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The sh shell command.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>subcommand_parameters</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The sh shell command parameters.</p> </td> </tr> </table>  <h4 id=\"Usage-N100D2\">Usage</h4> <p>Use the sh command to invoke any sh shell command from within a Pig script or Grunt shell.</p> <p> Note that only real programs can be run form the sh command. Commands such as cd are not programs but part of the shell environment and as such cannot be executed unless the user invokes the shell explicitly, like \"bash cd\". </p>  <h4 id=\"Example\">Example</h4> <p>In this example the ls command is invoked.</p> <pre class=\"code\">\ngrunt&gt; sh ls \nbigdata.conf \nnightly.conf \n..... \ngrunt&gt; \n</pre> </div>   <h2 id=\"utillity-cmds\">Utility Commands</h2> <div class=\"section\">  <h3 id=\"clear\">clear</h3> <p>Clear the screen of Pig grunt shell and position the cursor at top of the screen.</p>  <h4 id=\"Syntax-N10102\">Syntax </h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>clear</p> </td> </tr> </table>  <h4 id=\"Terms-N10117\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>key</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Description.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>none</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>no parameters</p> </td> </tr> </table>  <h4 id=\"Example-N10146\">Example</h4> <p>In this example the clear command clean up Pig grunt shell.</p> <pre class=\"code\">\ngrunt&gt; clear</pre>  <h3 id=\"exec\">exec</h3> <p>Run a Pig script.</p>  <h4 id=\"Syntax-N10160\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>exec [–param param_name = param_value] [–param_file file_name] [script] </p> </td> </tr> </table>  <h4 id=\"Terms-N10174\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–param param_name = param_value</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>See <a href=\"cont#Parameter-Sub\">Parameter Substitution</a>.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–param_file file_name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>See <a href=\"cont#Parameter-Sub\">Parameter Substitution</a>. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>script</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a Pig script.</p> </td> </tr> </table>  <h4 id=\"Usage-N101BE\">Usage</h4> <p>Use the exec command to run a Pig script with no interaction between the script and the Grunt shell (batch mode). Aliases defined in the script are not available to the shell; however, the files produced as the output of the script and stored on the system are visible after the script is run. Aliases defined via the shell are not available to the script. </p> <p>With the exec command, store statements will not trigger execution; rather, the entire script is parsed before execution starts. Unlike the run command, exec does not change the command history or remembers the handles used inside the script. Exec without any parameters can be used in scripts to force execution up to the point in the script where the exec occurs. </p>  <p id=\"exec-debug\">For comparison, see the <a href=\"#run\">run</a> command. Both the exec and run commands are useful for debugging because you can modify a Pig script in an editor and then rerun the script in the Grunt shell without leaving the shell. Also, both commands promote Pig script modularity as they allow you to reuse existing components.</p>  <h4 id=\"Examples-N101D3\">Examples</h4> <p>In this example the script is displayed and run.</p> <pre class=\"code\">\ngrunt&gt; cat myscript.pig\na = LOAD 'student' AS (name, age, gpa);\nb = LIMIT a 3;\nDUMP b;\n\ngrunt&gt; exec myscript.pig\n(alice,20,2.47)\n(luke,18,4.00)\n(holly,24,3.27)\n</pre> <p>In this example parameter substitution is used with the exec command.</p> <pre class=\"code\">\ngrunt&gt; cat myscript.pig\na = LOAD 'student' AS (name, age, gpa);\nb = ORDER a BY name;\n\nSTORE b into '$out';\n\ngrunt&gt; exec –param out=myoutput myscript.pig\n</pre> <p>In this example multiple parameters are specified.</p> <pre class=\"code\">\ngrunt&gt; exec –param p1=myparam1 –param p2=myparam2 myscript.pig\n</pre>  <h3 id=\"help\">help</h3> <p>Prints a list of Pig commands or properties.</p>  <h4 id=\"Syntax-N101FB\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-help [properties] </p> </td> </tr> </table>  <h4 id=\"Terms-N1020F\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>properties</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>List Pig properties.</p> </td> </tr> </table>  <h4 id=\"Usage-N1022B\">Usage</h4> <p>The help command prints a list of Pig commands or properties.</p>  <h4 id=\"Example-N10234\">Example</h4> <p>Use \"-help\" to get a list of commands.</p> <pre class=\"code\">\n$ pig -help\n\nApache Pig version 0.8.0-dev (r987348)\ncompiled Aug 19 2010, 16:38:44\n\nUSAGE: Pig [options] [-] : Run interactively in grunt shell.\n       Pig [options] -e[xecute] cmd [cmd ...] : Run cmd(s).\n       Pig [options] [-f[ile]] file : Run cmds found in file.\n  options include:\n    -4, -log4jconf - Log4j configuration file, overrides log conf\n    -b, -brief - Brief logging (no timestamps)\n    -c, -check - Syntax check\n<em>etc …</em>\n</pre> <p>Use \"-help properties\" to get a list of properties.</p> <pre class=\"code\">\n$ pig -help properties\n\nThe following properties are supported:\n    Logging:\n        verbose=true|false; default is false. This property is the same as -v switch\n        brief=true|false; default is false. This property is the same as -b switch\n        debug=OFF|ERROR|WARN|INFO|DEBUG; default is INFO. This property is the same as -d switch\n        aggregate.warning=true|false; default is true. If true, prints count of warnings\n            of each type rather than logging each warning.\n<em>etc …</em>\n</pre>  <h3 id=\"history\">history</h3> <p>Display the list of statements used so far.</p>  <h4 id=\"Syntax-N10259\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>history [-n]</p> </td> </tr> </table>  <h4 id=\"Terms-N1026E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>key</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Description.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Omit line numbers in the list.</p> </td> </tr> </table>  <h4 id=\"Usage-N1029E\">Usage</h4> <p>The history command shows the statements used so far.</p>  <h4 id=\"Example-N102A7\">Example</h4> <p>In this example the history command shows all the statements with line numbers and without them.</p> <pre class=\"code\">\ngrunt&gt; a = LOAD 'student' AS (name, age, gpa);\ngrunt&gt; b = order a by name;\ngrunt&gt; history\n1 a = LOAD 'student' AS (name, age, gpa);\n2 b = order a by name;\n\ngrunt&gt; c = order a by name;\ngrunt&gt; history -n\na = LOAD 'student' AS (name, age, gpa);\nb = order a by name;\nc = order a by name;</pre>  <h3 id=\"kill\">kill</h3> <p>Kills a job.</p>  <h4 id=\"Syntax-N102C1\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>kill jobid</p> </td> </tr> </table>  <h4 id=\"Terms-N102D5\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>jobid</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The job id.</p> </td> </tr> </table>  <h4 id=\"Usage-N102F1\">Usage</h4> <p>Use the kill command to kill a Pig job based on the job id.</p> <p>The kill command will attempt to kill any MapReduce jobs associated with the Pig job. Under certain conditions, however, this may fail; for example, when a Pig job is killed and does not have a chance to call its shutdown procedures.</p>  <h4 id=\"Example-N102FE\">Example</h4> <p>In this example the job with id job_0001 is killed.</p> <pre class=\"code\">\ngrunt&gt; kill job_0001\n</pre>  <h3 id=\"quit\">quit</h3> <p>Quits from the Pig grunt shell.</p>  <h4 id=\"Syntax-N10317\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>exit</p> </td> </tr> </table>  <h4 id=\"Terms-N1032B\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>none</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>no parameters</p> </td> </tr> </table>  <h4 id=\"Usage-N10347\">Usage</h4> <p>The quit command enables you to quit or exit the Pig grunt shell.</p>  <h4 id=\"Example-N10350\">Example</h4> <p>In this example the quit command exits the Pig grunt shall.</p> <pre class=\"code\">\ngrunt&gt; quit\n</pre>  <h3 id=\"run\">run</h3> <p>Run a Pig script.</p>  <h4 id=\"Syntax-N1036A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>run [–param param_name = param_value] [–param_file file_name] script </p> </td> </tr> </table>  <h4 id=\"Terms-N1037E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–param param_name = param_value</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>See <a href=\"cont#Parameter-Sub\">Parameter Substitution</a>.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–param_file file_name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>See <a href=\"cont#Parameter-Sub\">Parameter Substitution</a>. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>script</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a Pig script.</p> </td> </tr> </table>  <h4 id=\"Usage-N103C8\">Usage</h4> <p>Use the run command to run a Pig script that can interact with the Grunt shell (interactive mode). The script has access to aliases defined externally via the Grunt shell. The Grunt shell has access to aliases defined within the script. All commands from the script are visible in the command history. </p> <p>With the run command, every store triggers execution. The statements from the script are put into the command history and all the aliases defined in the script can be referenced in subsequent statements after the run command has completed. Issuing a run command on the grunt command line has basically the same effect as typing the statements manually. </p> <p>For comparison, see the <a href=\"#exec\">exec</a> command. Both the run and exec commands are useful for debugging because you can modify a Pig script in an editor and then rerun the script in the Grunt shell without leaving the shell. Also, both commands promote Pig script modularity as they allow you to reuse existing components.</p>  <h4 id=\"Example-N103DC\">Example</h4> <p>In this example the script interacts with the results of commands issued via the Grunt shell.</p> <pre class=\"code\">\ngrunt&gt; cat myscript.pig\nb = ORDER a BY name;\nc = LIMIT b 10;\n\ngrunt&gt; a = LOAD 'student' AS (name, age, gpa);\n\ngrunt&gt; run myscript.pig\n\ngrunt&gt; d = LIMIT c 3;\n\ngrunt&gt; DUMP d;\n(alice,20,2.47)\n(alice,27,1.95)\n(alice,36,2.27)\n</pre> <p>In this example parameter substitution is used with the run command.</p> <pre class=\"code\">\ngrunt&gt; a = LOAD 'student' AS (name, age, gpa);\n\ngrunt&gt; cat myscript.pig\nb = ORDER a BY name;\nSTORE b into '$out';\n\ngrunt&gt; run –param out=myoutput myscript.pig\n</pre>  <h3 id=\"set\">set</h3> <p>Shows/Assigns values to keys used in Pig.</p>  <h4 id=\"Syntax-N103FC\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>set [key 'value']</p> </td> </tr> </table>  <h4 id=\"Terms-N10410\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>key</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Key (see table). Case sensitive.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>value</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Value for key (see table). Case sensitive.</p> </td> </tr> </table>  <h4 id=\"Usage-N10440\">Usage</h4> <p>Use the set command to assign values to keys, as shown in the table. All keys and their corresponding values (for Pig and Hadoop) are case sensitive. If set command is used without key/value pair argument, Pig prints all the configurations and system properties.</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Key </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Value </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Description </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>default_parallel</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>a whole number </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Sets the number of reducers for all MapReduce jobs generated by Pig (see <a href=\"perf#parallel\">Use the Parallel Features</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>debug </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>on/off </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Turns debug-level logging on or off. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>job.name </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Single-quoted string that contains the job name.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Sets user-specified name for the job </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>job.priority </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Acceptable values (case insensitive): very_low, low, normal, high, very_high </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Sets the priority of a Pig job.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>stream.skippath</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>String that contains the path.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>For streaming, sets the path from which not to ship data (see <a href=\"basic#define-udfs\">DEFINE (UDFs, streaming)</a> and <a href=\"basic#autoship\"> About Auto-Ship</a>).</p> </td> </tr> </table>  <p> All Pig and Hadoop properties can be set, either in the Pig script or via the Grunt command line. </p>  <h4 id=\"Examples-N10500\">Examples</h4> <p>In this example key value pairs are set at the command line.</p> <pre class=\"code\">\ngrunt&gt; SET debug 'on'\ngrunt&gt; SET job.name 'my job'\ngrunt&gt; SET default_parallel 100\n</pre> <p>In this example default_parallel is set in the Pig script; all MapReduce jobs that get launched will use 20 reducers.</p> <pre class=\"code\">\nSET default_parallel 20;\nA = LOAD 'myfile.txt' USING PigStorage() AS (t, u, v);\nB = GROUP A BY t;\nC = FOREACH B GENERATE group, COUNT(A.t) as mycount;\nD = ORDER C BY mycount;\nSTORE D INTO 'mysortedcount' USING PigStorage();\n</pre> <p>In this example multiple key value pairs are set in the Pig script. These key value pairs are put in job-conf by Pig (making the pairs available to Pig and Hadoop). This is a script-wide setting; if a key value is defined multiple times in the script the last value will take effect and will be set for all jobs generated by the script. </p> <pre class=\"code\">\n...\nSET mapred.map.tasks.speculative.execution false; \nSET pig.logfile mylogfile.log; \nSET my.arbitrary.key my.arbitary.value; \n...\n</pre> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/cmds.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/cmds.html</a>\n  </p>\n</div>\n","test":"<h1>Testing and Diagnostics</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#diagnostic-ops\">Diagnostic Operators</a> <ul class=\"minitoc\"> <li> <a href=\"#describe\">DESCRIBE</a> </li> <li> <a href=\"#dump\">DUMP</a> </li> <li> <a href=\"#explain\">EXPLAIN</a> </li> <li> <a href=\"#illustrate\">ILLUSTRATE</a> </li> </ul> </li> <li> <a href=\"#mapreduce-job-ids\">Pig Scripts and MapReduce Job IDs (MapReduce mode only)</a> </li> <li> <a href=\"#pig-statistics\">Pig Statistics</a> <ul class=\"minitoc\"> <li> <a href=\"#Java+API\">Java API</a> </li> <li> <a href=\"#Job+XML\">Job XML</a> </li> <li> <a href=\"#hadoop-job-history-loader\">Hadoop Job History Loader</a> </li> <li> <a href=\"#Examples\">Examples</a> </li> </ul> </li> <li> <a href=\"#ppnl\">Pig Progress Notification Listener</a> </li> <li> <a href=\"#pigunit\">PigUnit</a> <ul class=\"minitoc\"> <li> <a href=\"#Build+PigUnit\">Build PigUnit</a> </li> <li> <a href=\"#Run+PigUnit\">Run PigUnit</a> </li> <li> <a href=\"#PigUnit+Example\">PigUnit Example</a> </li> <li> <a href=\"#Troubleshooting+Tips\">Troubleshooting Tips</a> </li> <li> <a href=\"#Future+Enhancements\">Future Enhancements</a> </li> </ul> </li> </ul> </div> </div>    <h2 id=\"diagnostic-ops\">Diagnostic Operators</h2> <div class=\"section\">  <h3 id=\"describe\">DESCRIBE</h3> <p>Returns the schema of a relation.</p>  <h4 id=\"Syntax\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DESCRIBE alias; </p> </td> </tr> </table>  <h4 id=\"Terms\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> </table>  <h4 id=\"Usage\">Usage</h4> <p>Use the DESCRIBE operator to view the schema of a relation. You can view outer relations as well as relations defined in a nested FOREACH statement.</p>  <h4 id=\"Example\">Example</h4> <p>In this example a schema is specified using the AS clause. If all data conforms to the schema, Pig will use the assigned types.</p> <pre class=\"code\">\nA = LOAD 'student' AS (name:chararray, age:int, gpa:float);\n\nB = FILTER A BY name matches 'J.+';\n\nC = GROUP B BY name;\n\nD = FOREACH C GENERATE COUNT(B.age);\n\nDESCRIBE A;\nA: {name: chararray,age: int,gpa: float}\n\nDESCRIBE B;\nB: {name: chararray,age: int,gpa: float}\n\nDESCRIBE C;\nC: {group: chararray,B: {(name: chararray,age: int,gpa: float)}}\n\nDESCRIBE D;\nD: {long}\n</pre> <p>In this example no schema is specified. All fields default to type bytearray or long (see Data Types).</p> <pre class=\"code\">\na = LOAD 'student';\n\nb = FILTER a BY $0 matches 'J.+';\n\nc = GROUP b BY $0;\n\nd = FOREACH c GENERATE COUNT(b.$1);\n\nDESCRIBE a;\nSchema for a unknown.\n\nDESCRIBE b;\n2008-12-05 01:17:15,316 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly cast to chararray under LORegexp Operator\nSchema for b unknown.\n\nDESCRIBE c;\n2008-12-05 01:17:23,343 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly caste to chararray under LORegexp Operator\nc: {group: bytearray,b: {null}}\n\nDESCRIBE d;\n2008-12-05 03:04:30,076 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly caste to chararray under LORegexp Operator\nd: {long}\n</pre> <p>This example shows how to view the schema of a nested relation using the :: operator.</p> <pre class=\"code\">\nA = LOAD 'studentab10k' AS (name, age, gpa); \nB = GROUP A BY name; \nC = FOREACH B { \n     D = DISTINCT A.age; \n     GENERATE COUNT(D), group;} \n\nDESCRIBE C::D; \nD: {age: bytearray} \n</pre>  <h3 id=\"dump\">DUMP</h3> <p>Dumps or displays results to screen.</p>  <h4 id=\"Syntax-N10084\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DUMP alias; </p> </td> </tr> </table>  <h4 id=\"Terms-N10098\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> </table>  <h4 id=\"Usage-N100B4\">Usage</h4> <p>Use the DUMP operator to run (execute) Pig Latin statements and display the results to your screen. DUMP is meant for interactive mode; statements are executed immediately and the results are not saved (persisted). You can use DUMP as a debugging device to make sure that the results you are expecting are actually generated. </p> <p> Note that production scripts SHOULD NOT use DUMP as it will disable multi-query optimizations and is likely to slow down execution (see <a href=\"perf#store-dump\">Store vs. Dump</a>). </p>  <h4 id=\"Example-N100C5\">Example</h4> <p>In this example a dump is performed after each statement.</p> <pre class=\"code\">\nA = LOAD 'student' AS (name:chararray, age:int, gpa:float);\n\nDUMP A;\n(John,18,4.0F)\n(Mary,19,3.7F)\n(Bill,20,3.9F)\n(Joe,22,3.8F)\n(Jill,20,4.0F)\n\nB = FILTER A BY name matches 'J.+';\n\nDUMP B;\n(John,18,4.0F)\n(Joe,22,3.8F)\n(Jill,20,4.0F)\n</pre>  <h3 id=\"explain\">EXPLAIN</h3> <p>Displays execution plans.</p>  <h4 id=\"Syntax-N100DE\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>EXPLAIN [–script pigscript] [–out path] [–brief] [–dot] [-xml] [–param param_name = param_value] [–param_file file_name] alias; </p> </td> </tr> </table>  <h4 id=\"Terms-N100F2\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–script</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to specify a Pig script.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–out</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to specify the output path (directory).</p> <p>Will generate a logical_plan[.txt|.dot], physical_plan[.text|.dot], exec_plan[.text|.dot] file in the specified path.</p> <p>Default (no path specified): Stdout </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–brief</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Does not expand nested plans (presenting a smaller graph for overview). </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–dot, -xml</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Text mode (default): multiple output (split) will be broken out in sections. </p> <p>Dot mode: outputs a format that can be passed to the dot utility for graphical display – will generate a directed-acyclic-graph (DAG) of the plans in any supported format (.gif, .jpg ...).</p> <p>Xml mode: outputs a xml which represent the plan (only logical plan is shown currently). </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–param param_name = param_value</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>See <a href=\"cont#Parameter-Sub\">Parameter Substitution</a>.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>–param_file file_name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>See <a href=\"cont#Parameter-Sub\">Parameter Substitution</a>. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> </table>  <h4 id=\"execution-plans\">Usage</h4> <p>Use the EXPLAIN operator to review the logical, physical, and map reduce execution plans that are used to compute the specified relationship. </p> <p>If no script is given:</p> <ul>  <li id=\"logical-plan\"> <p>The logical plan shows a pipeline of operators to be executed to build the relation. Type checking and backend-independent optimizations (such as applying filters early on) also apply.</p> </li>  <li id=\"physical-plan\"> <p>The physical plan shows how the logical operators are translated to backend-specific physical operators. Some backend optimizations also apply.</p> </li>  <li id=\"mapreduce-plan\"> <p>The mapreduce plan shows how the physical operators are grouped into map reduce jobs.</p> </li> </ul>  <p>If a script without an alias is specified, it will output the entire execution graph (logical, physical, or map reduce). </p> <p>If a script with a alias is specified, it will output the plan for the given alias. </p>  <h4 id=\"Example-N101C1\">Example</h4> <p>In this example the EXPLAIN operator produces all three plans. (Note that only a portion of the output is shown in this example.)</p> <pre class=\"code\">\nA = LOAD 'student' AS (name:chararray, age:int, gpa:float);\n\nB = GROUP A BY name;\n\nC = FOREACH B GENERATE COUNT(A.age);\n\nEXPLAIN C;\n-----------------------------------------------\nLogical Plan:\n-----------------------------------------------\nStore xxx-Fri Dec 05 19:42:29 UTC 2008-23 Schema: {long} Type: Unknown\n|\n|---ForEach xxx-Fri Dec 05 19:42:29 UTC 2008-15 Schema: {long} Type: bag\n <em>etc ... </em> \n\n-----------------------------------------------\nPhysical Plan:\n-----------------------------------------------\nStore(fakefile:org.apache.pig.builtin.PigStorage) - xxx-Fri Dec 05 19:42:29 UTC 2008-40\n|\n|---New For Each(false)[bag] - xxx-Fri Dec 05 19:42:29 UTC 2008-39\n    |   |\n    |   POUserFunc(org.apache.pig.builtin.COUNT)[long] - xxx-Fri Dec 05 \n <em>etc ... </em> \n\n--------------------------------------------------\n| Map Reduce Plan                               \n-------------------------------------------------\nMapReduce node xxx-Fri Dec 05 19:42:29 UTC 2008-41\nMap Plan\nLocal Rearrange[tuple]{chararray}(false) - xxx-Fri Dec 05 19:42:29 UTC 2008-34\n|   |\n|   Project[chararray][0] - xxx-Fri Dec 05 19:42:29 UTC 2008-35\n <em>etc ... </em> \n\nIf you are running in Tez mode, Map Reduce Plan will be replaced with Tez Plan:\n\n#--------------------------------------------------\n# There are 1 DAGs in the session\n#--------------------------------------------------\n#--------------------------------------------------\n# TEZ DAG plan: PigLatin:185.pig-0_scope-0\n#--------------------------------------------------\nTez vertex scope-21\t-&gt;\tTez vertex scope-22,\nTez vertex scope-22\n\nTez vertex scope-21\n# Plan on vertex\nB: Local Rearrange[tuple]{chararray}(false) - scope-35\t-&gt;\t scope-22\n <em>etc ... </em> \n\n</pre>  <h3 id=\"illustrate\">ILLUSTRATE</h3> <p>Displays a step-by-step execution of a sequence of statements.</p>  <h4 id=\"Syntax-N101E6\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ILLUSTRATE {alias | -script scriptfile}; </p> </td> </tr> </table>  <h4 id=\"Terms-N101FA\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-script scriptfile</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The script keyword followed by the name of a Pig script (for example, myscript.pig). </p> <p>The script file should not contain an ILLUSTRATE statement.</p> </td> </tr> </table>  <h4 id=\"Usage-N1022C\">Usage</h4> <p>Use the ILLUSTRATE operator to review how data is transformed through a sequence of Pig Latin statements. ILLUSTRATE allows you to test your programs on small datasets and get faster turnaround times. </p>  <p id=\"example-generator\">ILLUSTRATE is based on an example generator (see <a href=\"http://research.yahoo.com/files/paper_5.pdf\">Generating Example Data for Dataflow Programs</a>). The algorithm works by retrieving a small sample of the input data and then propagating this data through the pipeline. However, some operators, such as JOIN and FILTER, can eliminate tuples from the data - and this could result in no data following through the pipeline. To address this issue, the algorithm will automatically generate example data, in near real-time. Thus, you might see data propagating through the pipeline that was not found in the original input data, but this data changes nothing and ensures that you will be able to examine the semantics of your Pig Latin statements.</p> <p>As shown in the examples below, you can use ILLUSTRATE to review a relation or an entire Pig script.</p>  <h4 id=\"Example+-+Relation\">Example - Relation</h4> <p>This example demonstrates how to use ILLUSTRATE with a relation. Note that the LOAD statement must include a schema (the AS clause).</p> <pre class=\"code\">\ngrunt&gt; visits = LOAD 'visits.txt' AS (user:chararray, url:chararray, timestamp:chararray);\ngrunt&gt; DUMP visits;\n\n(Amy,yahoo.com,19990421)\n(Fred,harvard.edu,19991104)\n(Amy,cnn.com,20070218)\n(Frank,nba.com,20070305)\n(Fred,berkeley.edu,20071204)\n(Fred,stanford.edu,20071206)\n\ngrunt&gt; recent_visits = FILTER visits BY timestamp &gt;= '20071201';\ngrunt&gt; user_visits = GROUP recent_visits BY user;\ngrunt&gt; num_user_visits = FOREACH user_visits GENERATE group, COUNT(recent_visits);\ngrunt&gt; DUMP num_user_visits;\n\n(Fred,2)\n\ngrunt&gt; ILLUSTRATE num_user_visits;\n------------------------------------------------------------------------\n| visits     | user: chararray | url: chararray | timestamp: chararray |\n------------------------------------------------------------------------\n|            | Fred            | berkeley.edu   | 20071204             |\n|            | Fred            | stanford.edu   | 20071206             |\n|            | Frank           | nba.com        | 20070305             |\n------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n| recent_visits     | user: chararray | url: chararray | timestamp: chararray |\n-------------------------------------------------------------------------------\n|                   | Fred            | berkeley.edu   | 20071204             |\n|                   | Fred            | stanford.edu   | 20071206             |\n-------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------------------------------\n| user_visits     | group: chararray | recent_visits: bag({user: chararray,url: chararray,timestamp: chararray}) |\n------------------------------------------------------------------------------------------------------------------\n|                 | Fred             | {(Fred, berkeley.edu, 20071204), (Fred, stanford.edu, 20071206)}          |\n------------------------------------------------------------------------------------------------------------------\n--------------------------------------------------\n| num_user_visits     | group: chararray | long  |\n--------------------------------------------------\n|                     | Fred             | 2     |\n--------------------------------------------------\n</pre>  <h4 id=\"Example+-+Script\">Example - Script</h4> <p>This example demonstrates how to use ILLUSTRATE with a Pig script. Note that the script itself should not contain an ILLUSTRATE statement.</p> <pre class=\"code\">\ngrunt&gt; cat visits.txt\nAmy     yahoo.com       19990421\nFred    harvard.edu     19991104\nAmy     cnn.com 20070218\nFrank   nba.com 20070305\nFred    berkeley.edu    20071204\nFred    stanford.edu    20071206\n\ngrunt&gt; cat visits.pig\nvisits = LOAD 'visits.txt' AS (user, url, timestamp);\nrecent_visits = FILTER visits BY timestamp &gt;= '20071201';\nhistorical_visits = FILTER visits BY timestamp &lt;= '20000101';\nDUMP recent_visits;\nDUMP historical_visits;\nSTORE recent_visits INTO 'recent';\nSTORE historical_visits INTO 'historical';\n\ngrunt&gt; exec visits.pig\n\n(Fred,berkeley.edu,20071204)\n(Fred,stanford.edu,20071206)\n\n(Amy,yahoo.com,19990421)\n(Fred,harvard.edu,19991104)\n\n\ngrunt&gt; illustrate -script visits.pig\n\n------------------------------------------------------------------------\n| visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n------------------------------------------------------------------------\n|            | Amy             | yahoo.com      | 19990421             |\n|            | Fred            | stanford.edu   | 20071206             |\n------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n| recent_visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n-------------------------------------------------------------------------------\n|                   | Fred            | stanford.edu   | 20071206             |\n-------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------\n| Store : recent_visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n---------------------------------------------------------------------------------------\n|                           | Fred            | stanford.edu   | 20071206             |\n---------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------\n| historical_visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n-----------------------------------------------------------------------------------\n|                       | Amy             | yahoo.com      | 19990421             |\n-----------------------------------------------------------------------------------\n-------------------------------------------------------------------------------------------\n| Store : historical_visits     | user: bytearray | url: bytearray | timestamp: bytearray |\n-------------------------------------------------------------------------------------------\n|                               | Amy             | yahoo.com      | 19990421             |\n-------------------------------------------------------------------------------------------\n</pre> </div>    <h2 id=\"mapreduce-job-ids\">Pig Scripts and MapReduce Job IDs (MapReduce mode only)</h2> <div class=\"section\"> <p>Complex Pig scripts often generate many MapReduce jobs. To help you debug a script, Pig prints a summary of the execution that shows which relations (aliases) are mapped to each MapReduce job. </p> <pre class=\"code\">\nJobId Maps Reduces MaxMapTime MinMapTIme AvgMapTime MaxReduceTime \n    MinReduceTime AvgReduceTime Alias Feature Outputs\njob_201004271216_12712 1 1 3 3 3 12 12 12 B,C GROUP_BY,COMBINER\njob_201004271216_12713 1 1 3 3 3 12 12 12 D SAMPLER\njob_201004271216_12714 1 1 3 3 3 12 12 12 D ORDER_BY,COMBINER \n    hdfs://mymachine.com:9020/tmp/temp743703298/tmp-2019944040,\n</pre> </div>    <h2 id=\"pig-statistics\">Pig Statistics</h2> <div class=\"section\"> <p>Pig Statistics is a framework for collecting and storing script-level statistics for Pig Latin. Characteristics of Pig Latin scripts and the resulting MapReduce jobs are collected while the script is executed. These statistics are then available for Pig users and tools using Pig (such as Oozie) to retrieve after the job is done.</p> <p>The new Pig statistics and the existing Hadoop statistics can also be accessed via the Hadoop job history file (and job xml file). Piggybank has a HadoopJobHistoryLoader which acts as an example of using Pig itself to query these statistics (the loader can be used as a reference implementation but is NOT supported for production use).</p>  <h3 id=\"Java+API\">Java API</h3> <p>Several new public classes make it easier for external tools such as Oozie to integrate with Pig statistics. </p> <p>The Pig statistics are available here: <a href=\"http://pig.apache.org/docs/r0.16.0/api/\">http://pig.apache.org/docs/r0.16.0/api/</a> </p>  <p id=\"stats-classes\">The stats classes are in the package: org.apache.pig.tools.pigstats</p> <ul> <li>PigStats</li> <li>SimplePigStats</li> <li>EmbeddedPigStats</li> <li>JobStats</li> <li>TezPigScriptStats</li> <li>TezDAGStats</li> <li>TezVertexStats</li> <li>OutputStats</li> <li>InputStats</li> </ul>  <p>The PigRunner class mimics the behavior of the Main class but gives users a statistics object back. Optionally, you can call the API with an implementation of progress listener which will be invoked by Pig runtime during the execution. </p> <pre class=\"code\">\npackage org.apache.pig;\n\npublic abstract class PigRunner {\n    public static PigStats run(String[] args, PigProgressNotificationListener listener)\n}\n\npublic interface PigProgressNotificationListener extends java.util.EventListener {\n    // just before the launch of MR jobs for the script\n    public void LaunchStartedNotification(int numJobsToLaunch);\n    // number of jobs submitted in a batch\n    public void jobsSubmittedNotification(int numJobsSubmitted);\n    // a job is started\n    public void jobStartedNotification(String assignedJobId);\n    // a job is completed successfully\n    public void jobFinishedNotification(JobStats jobStats);\n    // a job is failed\n    public void jobFailedNotification(JobStats jobStats);\n    // a user output is completed successfully\n    public void outputCompletedNotification(OutputStats outputStats);\n    // updates the progress as percentage\n    public void progressUpdatedNotification(int progress);\n    // the script execution is done\n    public void launchCompletedNotification(int numJobsSucceeded);\n}\n</pre> <p>Depends on the type of the pig script, PigRunner.run() returns a particular subclass of PigStats: SimplePigStats(MapReduce/local mode), TezPigScriptStats(Tez/Tez local mode) or EmbeddedPigStats(embedded script). SimplePigStats contains a map of JobStats which capture the stats for each MapReduce job of the Pig script. TezPigScriptStats contains a map of TezDAGStats which capture the stats for each Tez DAG of the Pig script, and TezDAGStats contains a map of TezVertexStats which capture the stats for each vertex within the Tez DAG. Depending on the execution type, EmbeddedPigStats contains a map of SimplePigStats or TezPigScriptStats, which captures the Pig job launched in the embeded script. </p> <p>If one is running Pig in Tez mode (or both Tez/MapReduce mode), should pass PigTezProgressNotificationListener which extends PigProgressNotificationListener to PigRunner.run() to make sure to get notification in both Tez mode or MapReduce mode. </p>  <h3 id=\"Job+XML\">Job XML</h3> <p>The following entries are included in job conf: </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Pig Statistic</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Description</strong> </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-script-id\">pig.script.id</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The UUID for the script. All jobs spawned by the script have the same script ID.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-script\">pig.script</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The base64 encoded script text.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-command-line\">pig.command.line</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The command line used to invoke the script.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-hadoop-version\">pig.hadoop.version</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The Hadoop version installed.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-version\">pig.version</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The Pig version used.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-input-dirs\">pig.input.dirs</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A comma-separated list of input directories for the job.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-map-output-dirs\">pig.map.output.dirs</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A comma-separated list of output directories in the map phase of the job.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-reduce-output-dirs\">pig.reduce.output.dirs</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A comma-separated list of output directories in the reduce phase of the job.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-parent-jobid\">pig.parent.jobid</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A comma-separated list of parent job ids.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-script-features\">pig.script.features</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A list of Pig features used in the script.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-job-feature\">pig.job.feature</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A list of Pig features used in the job.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pig-alias\">pig.alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The alias associated with the job.</p> </td> </tr> </table>  <h3 id=\"hadoop-job-history-loader\">Hadoop Job History Loader</h3> <p>The HadoopJobHistoryLoader in Piggybank loads Hadoop job history files and job xml files from file system. For each MapReduce job, the loader produces a tuple with schema (j:map[], m:map[], r:map[]). The first map in the schema contains job-related entries. Here are some of important key names in the map: </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PIG_SCRIPT_ID</p> <p>CLUSTER </p> <p>QUEUE_NAME</p> <p>JOBID</p> <p>JOBNAME</p> <p>STATUS</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>USER </p> <p>HADOOP_VERSION </p> <p>PIG_VERSION</p> <p>PIG_JOB_FEATURE</p> <p>PIG_JOB_ALIAS </p> <p>PIG_JOB_PARENTS</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>SUBMIT_TIME</p> <p>LAUNCH_TIME</p> <p>FINISH_TIME</p> <p>TOTAL_MAPS</p> <p>TOTAL_REDUCES</p> </td> </tr> </table>  <p>Examples that use the loader to query Pig statistics are shown below.</p>  <h3 id=\"Examples\">Examples</h3> <p>Find scripts that generate more then three MapReduce jobs:</p> <pre class=\"code\">\na = load '/mapred/history/done' using HadoopJobHistoryLoader() as (j:map[], m:map[], r:map[]);\nb = group a by (j#'PIG_SCRIPT_ID', j#'USER', j#'JOBNAME');\nc = foreach b generate group.$1, group.$2, COUNT(a);\nd = filter c by $2 &gt; 3;\ndump d;\n</pre> <p>Find the running time of each script (in seconds): </p> <pre class=\"code\">\na = load '/mapred/history/done' using HadoopJobHistoryLoader() as (j:map[], m:map[], r:map[]);\nb = foreach a generate j#'PIG_SCRIPT_ID' as id, j#'USER' as user, j#'JOBNAME' as script_name, \n         (Long) j#'SUBMIT_TIME' as start, (Long) j#'FINISH_TIME' as end;\nc = group b by (id, user, script_name)\nd = foreach c generate group.user, group.script_name, (MAX(b.end) - MIN(b.start)/1000;\ndump d;\n</pre> <p>Find the number of scripts run by user and queue on a cluster: </p> <pre class=\"code\">\na = load '/mapred/history/done' using HadoopJobHistoryLoader() as (j:map[], m:map[], r:map[]);\nb = foreach a generate j#'PIG_SCRIPT_ID' as id, j#'USER' as user, j#'QUEUE_NAME' as queue;\nc = group b by (id, user, queue) parallel 10;\nd = foreach c generate group.user, group.queue, COUNT(b);\ndump d;\n</pre> <p>Find scripts that have failed jobs: </p> <pre class=\"code\">\na = load '/mapred/history/done' using HadoopJobHistoryLoader() as (j:map[], m:map[], r:map[]);\nb = foreach a generate (Chararray) j#'STATUS' as status, j#'PIG_SCRIPT_ID' as id, j#'USER' as user, j#'JOBNAME' as script_name, j#'JOBID' as job;\nc = filter b by status != 'SUCCESS';\ndump c;\n</pre> <p>Find scripts that use only the default parallelism: </p> <pre class=\"code\">\na = load '/mapred/history/done' using HadoopJobHistoryLoader() as (j:map[], m:map[], r:map[]);\nb = foreach a generate j#'PIG_SCRIPT_ID' as id, j#'USER' as user, j#'JOBNAME' as script_name, (Long) r#'NUMBER_REDUCES' as reduces;\nc = group b by (id, user, script_name) parallel 10;\nd = foreach c generate group.user, group.script_name, MAX(b.reduces) as max_reduces;\ne = filter d by max_reduces == 1;\ndump e;\n</pre> </div>    <h2 id=\"ppnl\">Pig Progress Notification Listener</h2> <div class=\"section\"> <p> Pig provides the ability to register a listener to receive event notifications during the execution of a script. Events include MapReduce plan creation, script launch, script progress, script completion, job submit, job start, job completion and job failure. </p> <p>To register a listener, set the pig.notification.listener parameter to the fully qualified class name of an implementation of <a href=\"http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/tools/pigstats/PigProgressNotificationListener.java\">org.apache.pig.tools.pigstats.PigProgressNotificationListener</a>. The class must exist on the classpath of the process submitting the Pig job. If the pig.notification.listener.arg parameter is set, the value will be passed to a constructor of the implementing class that takes a single String. </p> </div>    <h2 id=\"pigunit\">PigUnit</h2> <div class=\"section\"> <p>PigUnit is a simple xUnit framework that enables you to easily test your Pig scripts. With PigUnit you can perform unit testing, regression testing, and rapid prototyping. No cluster set up is required if you run Pig in local mode. </p>  <h3 id=\"Build+PigUnit\">Build PigUnit</h3> <p>To compile PigUnit run the command shown below from the Pig trunk. The compile will create the pigunit.jar file.</p> <pre class=\"code\">\n$pig_trunk ant pigunit-jar   \n</pre>  <h3 id=\"Run+PigUnit\">Run PigUnit</h3> <p>You can run PigUnit using Pig's local mode or mapreduce mode.</p>  <h4 id=\"Local+Mode\">Local Mode</h4> <p> PigUnit runs in Pig's local mode by default. Local mode is fast and enables you to use your local file system as the HDFS cluster. Local mode does not require a real cluster but a new local one is created each time. </p>  <h4 id=\"Other+Modes\">Other Modes</h4> <p>PigUnit also runs in Pig's mapreduce/tez/tez_local mode. Mapreduce/Tez mode requires you to use a Hadoop cluster and HDFS installation. It is enabled when the Java system property pigunit.exectype is set to specific values (mr/tez/tez_local): e.g. -Dpigunit.exectype=mr or System.getProperties().setProperty(\"pigunit.exectype\", \"mr\"), which means PigUnit will run in mr mode. The cluster you select to run mr/tez test must be specified in the CLASSPATH (similar to the HADOOP_CONF_DIR variable). </p>  <h3 id=\"PigUnit+Example\">PigUnit Example</h3> <p> Many PigUnit examples are available in the <a href=\"http://svn.apache.org/viewvc/pig/trunk/test/org/apache/pig/test/pigunit/TestPigTest.java\">PigUnit tests</a>. </p> <p>The example included here computes the top N of the most common queries. The Pig script, top_queries.pig, is similar to the <a href=\"start#pig-script-1\">Query Phrase Popularity</a> in the Pig tutorial. It expects an input a file of queries and a parameter n (n is 2 in our case in order to do a top 2). </p> <p>Setting up a test for this script is easy because the argument and the input data are specified by two text arrays. It is the same for the expected output of the script that will be compared to the actual result of the execution of the Pig script. </p>  <h4 id=\"Java+Test\">Java Test</h4> <pre class=\"code\">\n  @Test\n  public void testTop2Queries() {\n    String[] args = {\n        \"n=2\",\n        };\n \n    PigTest test = new PigTest(\"top_queries.pig\", args);\n \n    String[] input = {\n        \"yahoo\",\n        \"yahoo\",\n        \"yahoo\",\n        \"twitter\",\n        \"facebook\",\n        \"facebook\",\n        \"linkedin\",\n    };\n \n    String[] output = {\n        \"(yahoo,3)\",\n        \"(facebook,2)\",\n    };\n \n    test.assertOutput(\"data\", input, \"queries_limit\", output);\n  }\n</pre>  <h4 id=\"top_queries.pig\">top_queries.pig</h4> <pre class=\"code\">\ndata =\n    LOAD 'input'\n    AS (query:CHARARRAY);\n     \nqueries_group =\n    GROUP data\n    BY query; \n    \nqueries_count = \n    FOREACH queries_group \n    GENERATE \n        group AS query, \n        COUNT(data) AS total;\n        \nqueries_ordered =\n    ORDER queries_count\n    BY total DESC, query;\n            \nqueries_limit =\n    LIMIT queries_ordered $n;\n\nSTORE queries_limit INTO 'output';\n</pre>  <h4 id=\"Run\">Run</h4> <p>The test can be executed by JUnit (or any other Java testing framework). It requires: </p> <ol> <li>pig.jar</li> <li>pigunit.jar</li> </ol> <p>The test takes about 25s to run and should pass. In case of error (for example change the parameter n to n=3), the diff of output is displayed: </p> <pre class=\"code\">\njunit.framework.ComparisonFailure: null expected:&lt;...ahoo,3)\n(facebook,2)[]&gt; but was:&lt;...ahoo,3)\n(facebook,2)[\n(linkedin,1)]&gt;\n        at junit.framework.Assert.assertEquals(Assert.java:81)\n        at junit.framework.Assert.assertEquals(Assert.java:87)\n        at org.apache.pig.pigunit.PigTest.assertEquals(PigTest.java:272)\n</pre>  <h4 id=\"Mocking\">Mocking</h4> <p>Sometimes you need to mock out the data in specific aliases. Using PigTest's mocking you can override an alias everywhere it is assigned. If you do not know the schema (or want to keep your test dynamic) you can use PigTest.getAliasToSchemaMap() to determine the schema. If you chose to go this route, you should cache the map for the specific script to ensure efficient execution. </p> <pre class=\"code\">\n  @Test\n  public void testTop2Queries() {\n    String[] args = {\n        \"n=2\",\n        };\n \n    PigTest test = new PigTest(\"top_queries.pig\", args);\n \n    String[] mockData = {\n        \"yahoo\",\n        \"yahoo\",\n        \"yahoo\",\n        \"twitter\",\n        \"facebook\",\n        \"facebook\",\n        \"linkedin\",\n    };\n    \n    //You should cache the map if you can\n    String schema = test.getAliasToSchemaMap().get(\"data\");\n    test.mockAlias(\"data\", mockData, schema);\n \n    String[] output = {\n        \"(yahoo,3)\",\n        \"(facebook,2)\",\n    };\n \n    test.assertOutputAnyOrder(\"queries_limit\", output);\n  }\n</pre>  <h3 id=\"Troubleshooting+Tips\">Troubleshooting Tips</h3> <p>Common problems you may encounter are discussed below.</p>  <h4 id=\"Classpath+in+Mapreduce+Mode\">Classpath in Mapreduce Mode</h4> <p>When using PigUnit in mapreduce mode, be sure to include the $HADOOP_CONF_DIR of the cluster in your CLASSPATH.</p> <p> MiniCluster generates one in build/classes. </p> <pre class=\"code\">\norg.apache.pig.backend.executionengine.ExecException: \nERROR 4010: Cannot find hadoop configurations in classpath \n(neither hadoop-site.xml nor core-site.xml was found in the classpath).\nIf you plan to use local mode, please put -x local option in command line\n</pre>  <h4 id=\"UDF+jars+Not+Found\">UDF jars Not Found</h4> <p>This error means that you are missing some jars in your test environment.</p> <pre class=\"code\">\nWARN util.JarManager: Couldn't find the jar for \norg.apache.pig.piggybank.evaluation.string.LOWER, skip it\n</pre>  <h4 id=\"Storing+Data\">Storing Data</h4> <p>Pig currently drops all STORE and DUMP commands. You can tell PigUnit to keep the commands and execute the script:</p> <pre class=\"code\">\ntest = new PigTest(PIG_SCRIPT, args);   \ntest.unoverride(\"STORE\");\ntest.runScript();\n</pre>  <h4 id=\"Cache+Archive\">Cache Archive</h4> <p>For cache archive to work, your test environment needs to have the cache archive options specified by Java properties or in an additional XML configuration in its CLASSPATH.</p> <p>If you use a local cluster, you need to set the required environment variables before starting it:</p> <pre class=\"code\">export LD_LIBRARY_PATH=/home/path/to/lib</pre>  <h3 id=\"Future+Enhancements\">Future Enhancements</h3> <p>Improvements and other components based on PigUnit that could be built later.</p> <p>For example, we could build a PigTestCase and PigTestSuite on top of PigTest to:</p> <ol> <li>Add the notion of workspaces for each test.</li> <li>Remove the boiler plate code appearing when there is more than one test methods.</li> <li>Add a standalone utility that reads test configurations and generates a test report. </li> </ol> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/test.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/test.html</a>\n  </p>\n</div>\n","start":"<h1>Getting Started</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#Pig+Setup\">Pig Setup</a> <ul class=\"minitoc\"> <li> <a href=\"#req\">Requirements</a> </li> <li> <a href=\"#download\">Download Pig</a> </li> <li> <a href=\"#build\">Build Pig</a> </li> </ul> </li> <li> <a href=\"#run\">Running Pig </a> <ul class=\"minitoc\"> <li> <a href=\"#execution-modes\">Execution Modes</a> </li> <li> <a href=\"#interactive-mode\">Interactive Mode</a> </li> <li> <a href=\"#batch-mode\">Batch Mode</a> </li> </ul> </li> <li> <a href=\"#kerberos\">Running jobs on a Kerberos secured cluster</a> <ul class=\"minitoc\"> <li> <a href=\"#kerberos-short\">Short lived jobs</a> </li> <li> <a href=\"#kerberos-long\">Long lived jobs</a> </li> </ul> </li> <li> <a href=\"#pl-statements\">Pig Latin Statements</a> <ul class=\"minitoc\"> <li> <a href=\"#data-load\">Loading Data</a> </li> <li> <a href=\"#data-work-with\">Working with Data</a> </li> <li> <a href=\"#data-store\">Storing Intermediate Results</a> </li> <li> <a href=\"#data-results\">Storing Final Results</a> </li> <li> <a href=\"#debug\">Debugging Pig Latin</a> </li> </ul> </li> <li> <a href=\"#properties\">Pig Properties</a> </li> <li> <a href=\"#tutorial\">Pig Tutorial </a> <ul class=\"minitoc\"> <li> <a href=\"#Running+the+Pig+Scripts+in+Local+Mode\"> Running the Pig Scripts in Local Mode</a> </li> <li> <a href=\"#Running+the+Pig+Scripts+in+Mapreduce+Mode+or+Tez+Mode\"> Running the Pig Scripts in Mapreduce Mode or Tez Mode</a> </li> <li> <a href=\"#pig-tutorial-files\"> Pig Tutorial Files</a> </li> <li> <a href=\"#pig-script-1\"> Pig Script 1: Query Phrase Popularity</a> </li> <li> <a href=\"#pig-script-2\">Pig Script 2: Temporal Query Phrase Popularity</a> </li> </ul> </li> </ul> </div> </div>    <h2 id=\"Pig+Setup\">Pig Setup</h2> <div class=\"section\">  <h3 id=\"req\">Requirements</h3> <p> <strong>Mandatory</strong> </p> <p>Unix and Windows users need the following:</p> <ul> <li> <strong>Hadoop 0.23.X, 1.X or 2.X</strong> - <a href=\"http://hadoop.apache.org/common/releases.html\">http://hadoop.apache.org/common/releases.html</a> (You can run Pig with different versions of Hadoop by setting HADOOP_HOME to point to the directory where you have installed Hadoop. If you do not set HADOOP_HOME, by default Pig will run with the embedded version, currently Hadoop 1.0.4.)</li> <li> <strong>Java 1.7</strong> - <a href=\"http://java.sun.com/javase/downloads/index.jsp\">http://java.sun.com/javase/downloads/index.jsp</a> (set JAVA_HOME to the root of your Java installation)</li> </ul>  <p> <strong>Optional</strong> </p> <ul> <li> <strong>Python 2.7</strong> - <a href=\"http://jython.org/downloads.html\">https://www.python.org</a> (when using Streaming Python UDFs) </li> <li> <strong>Ant 1.8</strong> - <a href=\"http://ant.apache.org/\">http://ant.apache.org/</a> (for builds) </li> </ul>  <h3 id=\"download\">Download Pig</h3> <p>To get a Pig distribution, do the following:</p> <ol> <li>Download a recent stable release from one of the Apache Download Mirrors (see <a href=\"http://hadoop.apache.org/pig/releases.html\"> Pig Releases</a>).</li> <li>Unpack the downloaded Pig distribution, and then note the following: <ul> <li>The Pig script file, pig, is located in the bin directory (/pig-n.n.n/bin/pig). The Pig environment variables are described in the Pig script file.</li> <li>The Pig properties file, pig.properties, is located in the conf directory (/pig-n.n.n/conf/pig.properties). You can specify an alternate location using the PIG_CONF_DIR environment variable.</li> </ul> </li> <li>Add /pig-n.n.n/bin to your path. Use export (bash,sh,ksh) or setenv (tcsh,csh). For example: <br> <span class=\"codefrag\">$ export PATH=/&lt;my-path-to-pig&gt;/pig-n.n.n/bin:$PATH</span> </li> <li> Test the Pig installation with this simple command: <span class=\"codefrag\">$ pig -help</span> </li> </ol>  <h3 id=\"build\">Build Pig</h3> <p>To build pig, do the following:</p> <ol> <li> Check out the Pig code from SVN: <span class=\"codefrag\">svn co http://svn.apache.org/repos/asf/pig/trunk</span> </li> <li> Build the code from the top directory: <span class=\"codefrag\">ant</span> <br> If the build is successful, you should see the pig.jar file created in that directory. </li> <li> Validate the pig.jar by running a unit test: <span class=\"codefrag\">ant test</span> </li> <li> If you are using Hadoop 0.23.X or 2.X, please add -Dhadoopversion=23 in your ant command line in the previous steps</li> </ol> </div>    <h2 id=\"run\">Running Pig </h2> <div class=\"section\"> <p>You can run Pig (execute Pig Latin statements and Pig commands) using various modes.</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"></td> <td colspan=\"1\" rowspan=\"1\"><strong>Local Mode</strong></td> <td colspan=\"1\" rowspan=\"1\"><strong>Tez Local Mode</strong></td> <td colspan=\"1\" rowspan=\"1\"><strong>Mapreduce Mode</strong></td> <td colspan=\"1\" rowspan=\"1\"><strong>Tez Mode</strong></td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"><strong>Interactive Mode </strong></td> <td colspan=\"1\" rowspan=\"1\">yes</td> <td colspan=\"1\" rowspan=\"1\">experimental</td> <td colspan=\"1\" rowspan=\"1\">yes</td> <td colspan=\"1\" rowspan=\"1\">yes</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">\n<strong>Batch Mode</strong> </td> <td colspan=\"1\" rowspan=\"1\">yes</td> <td colspan=\"1\" rowspan=\"1\">experimental</td> <td colspan=\"1\" rowspan=\"1\">yes</td> <td colspan=\"1\" rowspan=\"1\">yes</td> </tr> </table>  <h3 id=\"execution-modes\">Execution Modes</h3> <p>Pig has two execution modes or exectypes: </p> <ul> <li> <strong>Local Mode</strong> - To run Pig in local mode, you need access to a single machine; all files are installed and run using your local host and file system. Specify local mode using the -x flag (pig -x local). </li> <li> <strong>Tez Local Mode</strong> - To run Pig in tez local mode. It is similar to local mode, except internally Pig will invoke tez runtime engine. Specify Tez local mode using the -x flag (pig -x tez_local). <p> <strong>Note:</strong> Tez local mode is experimental. There are some queries which just error out on bigger data in local mode.</p> </li> <li> <strong>Mapreduce Mode</strong> - To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation. Mapreduce mode is the default mode; you can, <em>but don't need to</em>, specify it using the -x flag (pig OR pig -x mapreduce). </li> <li> <strong>Tez Mode</strong> - To run Pig in Tez mode, you need access to a Hadoop cluster and HDFS installation. Specify Tez mode using the -x flag (-x tez). </li> </ul>  <p>You can run Pig in either mode using the \"pig\" command (the bin/pig Perl script) or the \"java\" command (java -cp pig.jar ...). </p>  <h4 id=\"Examples\">Examples</h4> <p>This example shows how to run Pig in local and mapreduce mode using the pig command.</p> <pre class=\"code\">\n/* local mode */\n$ pig -x local ...\n \n/* Tez local mode */\n$ pig -x tez_local ...\n \n/* mapreduce mode */\n$ pig ...\nor\n$ pig -x mapreduce ...\n\n/* Tez mode */\n$ pig -x tez ...\n</pre>  <h3 id=\"interactive-mode\">Interactive Mode</h3> <p>You can run Pig in interactive mode using the Grunt shell. Invoke the Grunt shell using the \"pig\" command (as shown below) and then enter your Pig Latin statements and Pig commands interactively at the command line. </p>  <h4 id=\"Example\">Example</h4> <p>These Pig Latin statements extract all user IDs from the /etc/passwd file. First, copy the /etc/passwd file to your local working directory. Next, invoke the Grunt shell by typing the \"pig\" command (in local or hadoop mode). Then, enter the Pig Latin statements interactively at the grunt prompt (be sure to include the semicolon after each statement). The DUMP operator will display the results to your terminal screen.</p> <pre class=\"code\">\ngrunt&gt; A = load 'passwd' using PigStorage(':'); \ngrunt&gt; B = foreach A generate $0 as id; \ngrunt&gt; dump B; \n</pre> <p> <strong>Local Mode</strong> </p> <pre class=\"code\">\n$ pig -x local\n... - Connecting to ...\ngrunt&gt; \n</pre> <p> <strong>Tez Local Mode</strong> </p> <pre class=\"code\">\n$ pig -x tez_local\n... - Connecting to ...\ngrunt&gt; \n</pre> <p> <strong>Mapreduce Mode</strong> </p> <pre class=\"code\">\n$ pig -x mapreduce\n... - Connecting to ...\ngrunt&gt; \n\nor\n\n$ pig \n... - Connecting to ...\ngrunt&gt; \n</pre> <p> <strong>Tez Mode</strong> </p> <pre class=\"code\">\n$ pig -x tez\n... - Connecting to ...\ngrunt&gt; \n</pre>  <h3 id=\"batch-mode\">Batch Mode</h3> <p>You can run Pig in batch mode using <a href=\"#pig-scripts\">Pig scripts</a> and the \"pig\" command (in local or hadoop mode).</p>  <h4 id=\"Example-N101A5\">Example</h4> <p>The Pig Latin statements in the Pig script (id.pig) extract all user IDs from the /etc/passwd file. First, copy the /etc/passwd file to your local working directory. Next, run the Pig script from the command line (using local or mapreduce mode). The STORE operator will write the results to a file (id.out).</p> <pre class=\"code\">\n/* id.pig */\n\nA = load 'passwd' using PigStorage(':');  -- load the passwd file \nB = foreach A generate $0 as id;  -- extract the user IDs \nstore B into 'id.out';  -- write the results to a file name id.out\n</pre> <p> <strong>Local Mode</strong> </p> <pre class=\"code\">\n$ pig -x local id.pig\n</pre> <p> <strong>Tez Local Mode</strong> </p> <pre class=\"code\">\n$ pig -x tez_local id.pig\n</pre> <p> <strong>Mapreduce Mode</strong> </p> <pre class=\"code\">\n$ pig id.pig\nor\n$ pig -x mapreduce id.pig\n</pre> <p> <strong>Tez Mode</strong> </p> <pre class=\"code\">\n$ pig -x tez id.pig\n</pre>  <h4 id=\"pig-scripts\">Pig Scripts</h4> <p>Use Pig scripts to place Pig Latin statements and Pig commands in a single file. While not required, it is good practice to identify the file using the *.pig extension.</p> <p>You can run Pig scripts from the command line and from the Grunt shell (see the <a href=\"cmds#run\">run</a> and <a href=\"cmds#exec\">exec</a> commands). </p> <p>Pig scripts allow you to pass values to parameters using <a href=\"cont#Parameter-Sub\">parameter substitution</a>. </p>  <p id=\"comments\"> <strong>Comments in Scripts</strong> </p> <p>You can include comments in Pig scripts:</p> <ul> <li> <p>For multi-line comments use /* …. */</p> </li> <li> <p>For single-line comments use --</p> </li> </ul> <pre class=\"code\">\n/* myscript.pig\nMy script is simple.\nIt includes three Pig Latin statements.\n*/\n\nA = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); -- loading data\nB = FOREACH A GENERATE name;  -- transforming data\nDUMP B;  -- retrieving results\n</pre>  <p id=\"dfs\"> <strong>Scripts and Distributed File Systems</strong> </p> <p>Pig supports running scripts (and Jar files) that are stored in HDFS, Amazon S3, and other distributed file systems. The script's full location URI is required (see <a href=\"basic#register\">REGISTER</a> for information about Jar files). For example, to run a Pig script on HDFS, do the following:</p> <pre class=\"code\">\n$ pig hdfs://nn.mydomain.com:9020/myscripts/script.pig\n</pre> </div>   <h2 id=\"kerberos\">Running jobs on a Kerberos secured cluster</h2> <div class=\"section\"> <p>Kerberos is a authentication system that uses tickets with a limited validity time.<br> As a consequence running a pig script on a kerberos secured hadoop cluster limits the running time to at most the remaining validity time of these kerberos tickets. When doing really complex analytics this may become a problem as the job may need to run for a longer time than these ticket times allow.</p>  <h3 id=\"kerberos-short\">Short lived jobs</h3> <p>When running short jobs all you need to do is ensure that the user has been logged in into Kerberos via the normal kinit method.<br> The Hadoop job will automatically pickup these credentials and the job will run fine.</p>  <h3 id=\"kerberos-long\">Long lived jobs</h3> <p>A kerberos keytab file is essentially a Kerberos specific form of the password of a user. <br> It is possible to enable a Hadoop job to request new tickets when they expire by creating a keytab file and make it part of the job that is running in the cluster. This will extend the maximum job duration beyond the maximum renew time of the kerberos tickets.</p> <p>Usage:</p> <ol> <li>Create a keytab file for the required principal.<br> Using the ktutil tool you can create a keytab using roughly these commands:<br> <pre class=\"code\">addent -password -p niels@EXAMPLE.NL -k 1 -e rc4-hmac\naddent -password -p niels@EXAMPLE.NL -k 1 -e aes256-cts\nwkt niels.keytab</pre> </li> <li>Set the following properties (either via the .pigrc file or on the command line via -P file) <ul> <li> <em>java.security.krb5.conf</em> <br> The path to the local krb5.conf file.<br> Usually this is \"/etc/krb5.conf\"</li> <li> <em>hadoop.security.krb5.principal</em> <br> The pricipal you want to login with.<br> Usually this would look like this \"niels@EXAMPLE.NL\"</li> <li> <em>hadoop.security.krb5.keytab</em> <br> The path to the local keytab file that must be used to authenticate with.<br> Usually this would look like this \"/home/niels/.krb/niels.keytab\"</li> </ul> </li> </ol> <p> <strong>NOTE:</strong>All paths in these variables are local to the client system starting the actual pig script. This can be run without any special access to the cluster nodes.</p> <p>Overall you would create a file that looks like this (assume we call it niels.kerberos.properties):</p> <pre class=\"code\">java.security.krb5.conf=/etc/krb5.conf\nhadoop.security.krb5.principal=niels@EXAMPLE.NL\nhadoop.security.krb5.keytab=/home/niels/.krb/niels.keytab</pre> <p>and start your script like this:</p> <pre class=\"code\">pig -P niels.kerberos.properties script.pig</pre> </div>    <h2 id=\"pl-statements\">Pig Latin Statements</h2> <div class=\"section\"> <p>Pig Latin statements are the basic constructs you use to process data using Pig. A Pig Latin statement is an operator that takes a <a href=\"basic#relations\">relation</a> as input and produces another relation as output. (This definition applies to all Pig Latin operators except LOAD and STORE which read data from and write data to the file system.) Pig Latin statements may include <a href=\"basic#expressions\">expressions</a> and <a href=\"basic#schemas\">schemas</a>. Pig Latin statements can span multiple lines and must end with a semi-colon ( ; ). By default, Pig Latin statements are processed using <a href=\"perf#multi-query-execution\">multi-query execution</a>. </p> <p>Pig Latin statements are generally organized as follows:</p> <ul> <li> <p>A LOAD statement to read data from the file system. </p> </li> <li> <p>A series of \"transformation\" statements to process the data. </p> </li> <li> <p>A DUMP statement to view results or a STORE statement to save the results.</p> </li> </ul>  <p>Note that a DUMP or STORE statement is required to generate output.</p> <ul> <li> <p>In this example Pig will validate, but not execute, the LOAD and FOREACH statements.</p> <pre class=\"code\">\nA = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);\nB = FOREACH A GENERATE name;\n</pre> </li> <li> <p>In this example, Pig will validate and then execute the LOAD, FOREACH, and DUMP statements.</p> <pre class=\"code\">\nA = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);\nB = FOREACH A GENERATE name;\nDUMP B;\n(John)\n(Mary)\n(Bill)\n(Joe)\n</pre> </li> </ul>  <h3 id=\"data-load\">Loading Data</h3> <p>Use the <a href=\"basic#load\">LOAD</a> operator and the <a href=\"udf#load-store-functions\">load/store functions</a> to read data into Pig (PigStorage is the default load function).</p>  <h3 id=\"data-work-with\">Working with Data</h3> <p>Pig allows you to transform data in many ways. As a starting point, become familiar with these operators:</p> <ul> <li> <p>Use the <a href=\"basic#filter\">FILTER</a> operator to work with tuples or rows of data. Use the <a href=\"basic#foreach\">FOREACH</a> operator to work with columns of data.</p> </li> <li> <p>Use the <a href=\"basic#group\">GROUP</a> operator to group data in a single relation. Use the <a href=\"basic#cogroup\">COGROUP</a>, <a href=\"basic#join-inner\">inner JOIN</a>, and <a href=\"basic#join-outer\">outer JOIN</a> operators to group or join data in two or more relations.</p> </li> <li> <p>Use the <a href=\"basic#union\">UNION</a> operator to merge the contents of two or more relations. Use the <a href=\"basic#split\">SPLIT</a> operator to partition the contents of a relation into multiple relations.</p> </li> </ul>  <h3 id=\"data-store\">Storing Intermediate Results</h3> <p>Pig stores the intermediate data generated between MapReduce jobs in a temporary location on HDFS. This location must already exist on HDFS prior to use. This location can be configured using the pig.temp.dir property. The property's default value is \"/tmp\" which is the same as the hardcoded location in Pig 0.7.0 and earlier versions. </p>  <h3 id=\"data-results\">Storing Final Results</h3> <p>Use the <a href=\"basic#store\">STORE</a> operator and the <a href=\"udf#load-store-functions\">load/store functions</a> to write results to the file system (PigStorage is the default store function). </p> <p> <strong>Note:</strong> During the testing/debugging phase of your implementation, you can use DUMP to display results to your terminal screen. However, in a production environment you always want to use the STORE operator to save your results (see <a href=\"perf#store-dump\">Store vs. Dump</a>).</p>  <h3 id=\"debug\">Debugging Pig Latin</h3> <p>Pig Latin provides operators that can help you debug your Pig Latin statements:</p> <ul> <li> <p>Use the <a href=\"test#dump\">DUMP</a> operator to display results to your terminal screen. </p> </li> <li> <p>Use the <a href=\"test#describe\">DESCRIBE</a> operator to review the schema of a relation.</p> </li> <li> <p>Use the <a href=\"test#explain\">EXPLAIN</a> operator to view the logical, physical, or map reduce execution plans to compute a relation.</p> </li> <li> <p>Use the <a href=\"test#illustrate\">ILLUSTRATE</a> operator to view the step-by-step execution of a series of statements.</p> </li> </ul> <p> <strong>Shortcuts for Debugging Operators</strong> </p> <p>Pig provides shortcuts for the frequently used debugging operators (DUMP, DESCRIBE, EXPLAIN, ILLUSTRATE). These shortcuts can be used in Grunt shell or within pig scripts. Following are the shortcuts supported by pig</p> <ul> <li> <p> \\d alias - shourtcut for <a href=\"test#dump\">DUMP</a> operator. If alias is ignored last defined alias will be used.</p> </li> <li> <p> \\de alias - shourtcut for <a href=\"test#describe\">DESCRIBE</a> operator. If alias is ignored last defined alias will be used.</p> </li> <li> <p> \\e alias - shourtcut for <a href=\"test#explain\">EXPLAIN</a> operator. If alias is ignored last defined alias will be used.</p> </li> <li> <p> \\i alias - shourtcut for <a href=\"test#illustrate\">ILLUSTRATE</a> operator. If alias is ignored last defined alias will be used.</p> </li> <li> <p> \\q - To quit grunt shell </p> </li> </ul> </div>    <h2 id=\"properties\">Pig Properties</h2> <div class=\"section\"> <p>Pig supports a number of Java properties that you can use to customize Pig behavior. You can retrieve a list of the properties using the <a href=\"cmds#help\">help properties</a> command. All of these properties are optional; none are required. </p>   <p id=\"pig-properties\">To specify Pig properties use one of these mechanisms:</p> <ul> <li>The pig.properties file (add the directory that contains the pig.properties file to the classpath)</li> <li>The -D and a Pig property in PIG_OPTS environment variable (export PIG_OPTS=-Dpig.tmpfilecompression=true)</li> <li>The -P command line option and a properties file (pig -P mypig.properties)</li> <li>The <a href=\"cmds#set\">set</a> command (set pig.exec.nocombiner true)</li> </ul> <p> <strong>Note:</strong> The properties file uses standard Java property file format.</p> <p>The following precedence order is supported: pig.properties &lt; -D Pig property &lt; -P properties file &lt; set command. This means that if the same property is provided using the –D command line option as well as the –P command line option (properties file), the value of the property in the properties file will take precedence.</p>  <p id=\"hadoop-properties\">To specify Hadoop properties you can use the same mechanisms:</p> <ul> <li>Hadoop configuration files (include pig-cluster-hadoop-site.xml)</li> <li>The -D and a Hadoop property in PIG_OPTS environment variable (export PIG_OPTS=–Dmapreduce.task.profile=true) </li> <li>The -P command line option and a property file (pig -P property_file)</li> <li>The <a href=\"cmds#set\">set</a> command (set mapred.map.tasks.speculative.execution false)</li> </ul>  <p>The same precedence holds: Hadoop configuration files &lt; -D Hadoop property &lt; -P properties_file &lt; set command.</p> <p>Hadoop properties are not interpreted by Pig but are passed directly to Hadoop. Any Hadoop property can be passed this way. </p> <p>All properties that Pig collects, including Hadoop properties, are available to any UDF via the UDFContext object. To get access to the properties, you can call the getJobConf method.</p> </div>    <h2 id=\"tutorial\">Pig Tutorial </h2> <div class=\"section\"> <p>The Pig tutorial shows you how to run Pig scripts using Pig's local mode, mapreduce mode and Tez mode (see <a href=\"#execution-modes\">Execution Modes</a>).</p> <p>To get started, do the following preliminary tasks:</p> <ol> <li>Make sure the JAVA_HOME environment variable is set the root of your Java installation.</li> <li>Make sure your PATH includes bin/pig (this enables you to run the tutorials using the \"pig\" command). <pre class=\"code\">\n$ export PATH=/&lt;my-path-to-pig&gt;/pig-0.16.0/bin:$PATH \n</pre> </li> <li>Set the PIG_HOME environment variable: <pre class=\"code\">\n$ export PIG_HOME=/&lt;my-path-to-pig&gt;/pig-0.16.0 \n</pre> </li> <li>Create the pigtutorial.tar.gz file: <ul> <li>Move to the Pig tutorial directory (.../pig-0.16.0/tutorial).</li> <li>Run the \"ant\" command from the tutorial directory. This will create the pigtutorial.tar.gz file. </li> </ul> </li> <li>Copy the pigtutorial.tar.gz file from the Pig tutorial directory to your local directory. </li> <li>Unzip the pigtutorial.tar.gz file. <pre class=\"code\">\n$ tar -xzf pigtutorial.tar.gz\n</pre> </li> <li>A new directory named pigtmp is created. This directory contains the <a href=\"#pig-tutorial-files\">Pig Tutorial Files</a>. These files work with Hadoop 0.20.2 and include everything you need to run <a href=\"#pig-script-1\">Pig Script 1</a> and <a href=\"#pig-script-2\">Pig Script 2</a>.</li> </ol>  <h3 id=\"Running+the+Pig+Scripts+in+Local+Mode\"> Running the Pig Scripts in Local Mode</h3> <p>To run the Pig scripts in local mode, do the following: </p> <ol> <li>Move to the pigtmp directory.</li> <li>Execute the following command (using either script1-local.pig or script2-local.pig). <pre class=\"code\">\n$ pig -x local script1-local.pig\n</pre> Or if you are using Tez local mode: <pre class=\"code\">\n$ pig -x tez_local script1-local.pig\n</pre> </li> <li>Review the result files, located in the script1-local-results.txt directory. <p>The output may contain a few Hadoop warnings which can be ignored:</p> <pre class=\"code\">\n2010-04-08 12:55:33,642 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics \n- Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n</pre> </li> </ol>  <h3 id=\"Running+the+Pig+Scripts+in+Mapreduce+Mode+or+Tez+Mode\"> Running the Pig Scripts in Mapreduce Mode or Tez Mode</h3> <p>To run the Pig scripts in mapreduce mode, do the following: </p> <ol> <li>Move to the pigtmp directory.</li> <li>Copy the excite.log.bz2 file from the pigtmp directory to the HDFS directory. <pre class=\"code\">\n$ hadoop fs –copyFromLocal excite.log.bz2 .\n</pre> </li> <li>Set the PIG_CLASSPATH environment variable to the location of the cluster configuration directory (the directory that contains the core-site.xml, hdfs-site.xml and mapred-site.xml files): <pre class=\"code\">\nexport PIG_CLASSPATH=/mycluster/conf\n</pre> <p>If you are using Tez, you will also need to put Tez configuration directory (the directory that contains the tez-site.xml):</p> <pre class=\"code\">\nexport PIG_CLASSPATH=/mycluster/conf:/tez/conf\n</pre> <p> <strong>Note:</strong> The PIG_CLASSPATH can also be used to add any other 3rd party dependencies or resource files a pig script may require. If there is also a need to make the added entries take the highest precedence in the Pig JVM's classpath order, one may also set the env-var PIG_USER_CLASSPATH_FIRST to any value, such as 'true' (and unset the env-var to disable).</p> </li> <li>Set the HADOOP_CONF_DIR environment variable to the location of the cluster configuration directory: <pre class=\"code\">\nexport HADOOP_CONF_DIR=/mycluster/conf\n</pre> </li> <li>Execute the following command (using either script1-hadoop.pig or script2-hadoop.pig): <pre class=\"code\">\n$ pig script1-hadoop.pig\n</pre> Or if you are using Tez: <pre class=\"code\">\n$ pig -x tez script1-hadoop.pig\n</pre> </li> <li>Review the result files, located in the script1-hadoop-results or script2-hadoop-results HDFS directory: <pre class=\"code\">\n$ hadoop fs -ls script1-hadoop-results\n$ hadoop fs -cat 'script1-hadoop-results/*' | less\n</pre> </li> </ol>  <h3 id=\"pig-tutorial-files\"> Pig Tutorial Files</h3> <p>The contents of the Pig tutorial file (pigtutorial.tar.gz) are described here. </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>File</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Description</strong> </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> pig.jar </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Pig JAR file </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> tutorial.jar </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> User defined functions (UDFs) and Java classes </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> script1-local.pig </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Pig Script 1, Query Phrase Popularity (local mode) </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> script1-hadoop.pig </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Pig Script 1, Query Phrase Popularity (mapreduce mode) </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> script2-local.pig </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Pig Script 2, Temporal Query Phrase Popularity (local mode)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> script2-hadoop.pig </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Pig Script 2, Temporal Query Phrase Popularity (mapreduce mode) </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> excite-small.log </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Log file, Excite search engine (local mode) </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> excite.log.bz2 </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Log file, Excite search engine (mapreduce) </p> </td> </tr> </table> <p>The user defined functions (UDFs) are described here. </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>UDF</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Description</strong> </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> ExtractHour </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Extracts the hour from the record.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> NGramGenerator </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Composes n-grams from the set of words. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> NonURLDetector </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Removes the record if the query field is empty or a URL. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> ScoreGenerator </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Calculates a \"popularity\" score for the n-gram.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> ToLower </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Changes the query field to lowercase. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> TutorialUtil </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Divides the query string into a set of words.</p> </td> </tr> </table>  <h3 id=\"pig-script-1\"> Pig Script 1: Query Phrase Popularity</h3> <p>The Query Phrase Popularity script (script1-local.pig or script1-hadoop.pig) processes a search query log file from the Excite search engine and finds search phrases that occur with particular high frequency during certain times of the day. </p> <p>The script is shown here: </p> <ul> <li> <p> Register the tutorial JAR file so that the included UDFs can be called in the script. </p> </li> </ul> <pre class=\"code\">\nREGISTER ./tutorial.jar; \n</pre> <ul> <li> <p> Use the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields <strong>user</strong>, <strong>time</strong>, and <strong>query</strong>. </p> </li> </ul> <pre class=\"code\">\nraw = LOAD 'excite.log' USING PigStorage('\\t') AS (user, time, query);\n</pre> <ul> <li> <p> Call the NonURLDetector UDF to remove records if the query field is empty or a URL. </p> </li> </ul> <pre class=\"code\">\nclean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);\n</pre> <ul> <li> <p> Call the ToLower UDF to change the query field to lowercase. </p> </li> </ul> <pre class=\"code\">\nclean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;\n</pre> <ul> <li> <p> Because the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour (HH) from the time field. </p> </li> </ul> <pre class=\"code\">\nhoured = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;\n</pre> <ul> <li> <p> Call the NGramGenerator UDF to compose the n-grams of the query. </p> </li> </ul> <pre class=\"code\">\nngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;\n</pre> <ul> <li> <p> Use the DISTINCT operator to get the unique n-grams for all records. </p> </li> </ul> <pre class=\"code\">\nngramed2 = DISTINCT ngramed1;\n</pre> <ul> <li> <p> Use the GROUP operator to group records by n-gram and hour. </p> </li> </ul> <pre class=\"code\">\nhour_frequency1 = GROUP ngramed2 BY (ngram, hour);\n</pre> <ul> <li> <p> Use the COUNT function to get the count (occurrences) of each n-gram. </p> </li> </ul> <pre class=\"code\">\nhour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;\n</pre> <ul> <li> <p> Use the GROUP operator to group records by n-gram only. Each group now corresponds to a distinct n-gram and has the count for each hour. </p> </li> </ul> <pre class=\"code\">\nuniq_frequency1 = GROUP hour_frequency2 BY group::ngram;\n</pre> <ul> <li> <p> For each group, identify the hour in which this n-gram is used with a particularly high frequency. Call the ScoreGenerator UDF to calculate a \"popularity\" score for the n-gram. </p> </li> </ul> <pre class=\"code\">\nuniq_frequency2 = FOREACH uniq_frequency1 GENERATE flatten($0), flatten(org.apache.pig.tutorial.ScoreGenerator($1));\n</pre> <ul> <li> <p> Use the FOREACH-GENERATE operator to assign names to the fields. </p> </li> </ul> <pre class=\"code\">\nuniq_frequency3 = FOREACH uniq_frequency2 GENERATE $1 as hour, $0 as ngram, $2 as score, $3 as count, $4 as mean;\n</pre> <ul> <li> <p> Use the FILTER operator to remove all records with a score less than or equal to 2.0. </p> </li> </ul> <pre class=\"code\">\nfiltered_uniq_frequency = FILTER uniq_frequency3 BY score &gt; 2.0;\n</pre> <ul> <li> <p> Use the ORDER operator to sort the remaining records by hour and score. </p> </li> </ul> <pre class=\"code\">\nordered_uniq_frequency = ORDER filtered_uniq_frequency BY hour, score;\n</pre> <ul> <li> <p> Use the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: <strong>hour</strong>, <strong>ngram</strong>, <strong>score</strong>, <strong>count</strong>, <strong>mean</strong>. </p> </li> </ul> <pre class=\"code\">\nSTORE ordered_uniq_frequency INTO '/tmp/tutorial-results' USING PigStorage(); \n</pre>  <h3 id=\"pig-script-2\">Pig Script 2: Temporal Query Phrase Popularity</h3> <p>The Temporal Query Phrase Popularity script (script2-local.pig or script2-hadoop.pig) processes a search query log file from the Excite search engine and compares the occurrence of frequency of search phrases across two time periods separated by twelve hours. </p> <p>The script is shown here: </p> <ul> <li> <p> Register the tutorial JAR file so that the user defined functions (UDFs) can be called in the script. </p> </li> </ul> <pre class=\"code\">\nREGISTER ./tutorial.jar;\n</pre> <ul> <li> <p> Use the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields <strong>user</strong>, <strong>time</strong>, and <strong>query</strong>. </p> </li> </ul> <pre class=\"code\">\nraw = LOAD 'excite.log' USING PigStorage('\\t') AS (user, time, query);\n</pre> <ul> <li> <p> Call the NonURLDetector UDF to remove records if the query field is empty or a URL. </p> </li> </ul> <pre class=\"code\">\nclean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);\n</pre> <ul> <li> <p> Call the ToLower UDF to change the query field to lowercase. </p> </li> </ul> <pre class=\"code\">\nclean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;\n</pre> <ul> <li> <p> Because the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour from the time field. </p> </li> </ul> <pre class=\"code\">\nhoured = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;\n</pre> <ul> <li> <p> Call the NGramGenerator UDF to compose the n-grams of the query. </p> </li> </ul> <pre class=\"code\">\nngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;\n</pre> <ul> <li> <p> Use the DISTINCT operator to get the unique n-grams for all records. </p> </li> </ul> <pre class=\"code\">\nngramed2 = DISTINCT ngramed1;\n</pre> <ul> <li> <p> Use the GROUP operator to group the records by n-gram and hour. </p> </li> </ul> <pre class=\"code\">\nhour_frequency1 = GROUP ngramed2 BY (ngram, hour);\n</pre> <ul> <li> <p> Use the COUNT function to get the count (occurrences) of each n-gram. </p> </li> </ul> <pre class=\"code\">\nhour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;\n</pre> <ul> <li> <p> Use the FOREACH-GENERATE operator to assign names to the fields. </p> </li> </ul> <pre class=\"code\">\nhour_frequency3 = FOREACH hour_frequency2 GENERATE $0 as ngram, $1 as hour, $2 as count;\n</pre> <ul> <li> <p> Use the FILTERoperator to get the n-grams for hour ‘00’ </p> </li> </ul> <pre class=\"code\">\nhour00 = FILTER hour_frequency2 BY hour eq '00';\n</pre> <ul> <li> <p> Uses the FILTER operators to get the n-grams for hour ‘12’ </p> </li> </ul> <pre class=\"code\">\nhour12 = FILTER hour_frequency3 BY hour eq '12';\n</pre> <ul> <li> <p> Use the JOIN operator to get the n-grams that appear in both hours. </p> </li> </ul> <pre class=\"code\">\nsame = JOIN hour00 BY $0, hour12 BY $0;\n</pre> <ul> <li> <p> Use the FOREACH-GENERATE operator to record their frequency. </p> </li> </ul> <pre class=\"code\">\nsame1 = FOREACH same GENERATE hour_frequency2::hour00::group::ngram as ngram, $2 as count00, $5 as count12;\n</pre> <ul> <li> <p> Use the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: <strong>ngram</strong>, <strong>count00</strong>, <strong>count12</strong>. </p> </li> </ul> <pre class=\"code\">\nSTORE same1 INTO '/tmp/tutorial-join-results' USING PigStorage();\n</pre> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/start.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/start.html</a>\n  </p>\n</div>\n","cont":"<h1>Control Structures</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#embed-python\">Embedded Pig - Python, JavaScript and Groovy</a> <ul class=\"minitoc\"> <li> <a href=\"#invocation-basics\">Invocation Basics</a> </li> <li> <a href=\"#invocation-details\">Invocation Details</a> </li> <li> <a href=\"#pigrunner-api\">PigRunner API</a> </li> <li> <a href=\"#Usage+Examples\">Usage Examples</a> </li> <li> <a href=\"#java-objects\">Java Objects</a> </li> </ul> </li> <li> <a href=\"#embed-java\">Embedded Pig - Java </a> <ul class=\"minitoc\"> <li> <a href=\"#pigserver\">PigServer Interface</a> </li> <li> <a href=\"#Usage+Examples-N1020F\">Usage Examples</a> </li> </ul> </li> <li> <a href=\"#macros\">Pig Macros</a> <ul class=\"minitoc\"> <li> <a href=\"#define-macros\">DEFINE (macros)</a> </li> <li> <a href=\"#import-macros\">IMPORT (macros)</a> </li> </ul> </li> <li> <a href=\"#Parameter-Sub\">Parameter Substitution</a> <ul class=\"minitoc\"> <li> <a href=\"#Description\">Description</a> </li> <li> <a href=\"#Usage-N1058F\">Usage</a> </li> <li> <a href=\"#Examples-N1060C\">Examples</a> </li> </ul> </li> </ul> </div> </div>   <h2 id=\"embed-python\">Embedded Pig - Python, JavaScript and Groovy</h2> <div class=\"section\"> <p>To enable control flow, you can embed Pig Latin statements and Pig commands in the Python, JavaScript and Groovy scripting languages using a JDBC-like compile, bind, run model. For Python, make sure the Jython jar is included in your class path. For JavaScript, make sure the Rhino jar is included in your classpath. For Groovy, make sure the groovy-all jar is included in your classpath.</p> <p>Note that host languages and the languages of UDFs (included as part of the embedded Pig) are completely orthogonal. For example, a Pig Latin statement that registers a Python UDF may be embedded in Python, JavaScript, or Java. The exception to this rule is \"combined\" scripts – here the languages must match (see the <a href=\"udf#jython-advanced\">Advanced Topics for Python</a>, <a href=\"udf#js-advanced\">Advanced Topics for JavaScript</a> and <a href=\"udf#groovy-advanced\">Advanced Topics for Groovy</a>). </p>  <h3 id=\"invocation-basics\">Invocation Basics</h3> <p>Embedded Pig is supported in batch mode only, not interactive mode. You can request that embedded Pig be used by adding the <span class=\"codefrag\">--embedded</span> option to the Pig command line. If this option is passed as an argument, that argument will refer to the language Pig is embedded in, either Python, JavaScript or Groovy. If no argument is specified, it is taken to refer to the reference implementation for Python.</p> <p> <strong>Python</strong> </p> <pre class=\"code\">\n $ pig myembedded.py\n </pre>  <p>Pig will look for the <span class=\"codefrag\">#!/usr/bin/python</span> line in the script.</p> <pre class=\"code\">\n#!/usr/bin/python \n\n# explicitly import Pig class \nfrom org.apache.pig.scripting import Pig \n\n# COMPILE: compile method returns a Pig object that represents the pipeline\nP = Pig.compile(\"a = load '$in'; store a into '$out';\")\n\ninput = 'original'\noutput = 'output'\n\n# BIND and RUN \nresult = P.bind({'in':input, 'out':output}).runSingle()\n\nif result.isSuccessful() :\n    print 'Pig job succeeded'\nelse :\n    raise 'Pig job failed'\n </pre> <p> <strong>JavaScript</strong> </p> <pre class=\"code\">\n$ pig myembedded.js\n</pre>  <p>Pig will look for the *.js extension in the script.</p> <pre class=\"code\">\nimportPackage(Packages.org.apache.pig.scripting.js) \n\nPig = org.apache.pig.scripting.js.JSPig\n\nfunction main() {\n    input = \"original\"\n    output = \"output\"\n\n    P = Pig.compile(\"A = load '$in'; store A into '$out';\") \n\n    result = P.bind({'in':input, 'out':output}).runSingle() \n\n    if (result.isSuccessful()) {\n        print(\"Pig job succeeded\")\n    } else {\n        print(\"Pig job failed\")\n    }   \n}\n</pre> <p> <strong>Groovy</strong> </p> <pre class=\"code\">\n$ pig myembedded.groovy\n</pre>  <p>Pig will look for the *.groovy extension in the script.</p> <pre class=\"code\">\nimport org.apache.pig.scripting.Pig;\n\npublic static void main(String[] args) {\n  String input = \"original\"\n  String output = \"output\"\n\n  Pig P = Pig.compile(\"A = load '\\$in'; store A into '\\$out';\")  \n\n  result = P.bind(['in':input, 'out':output]).runSingle()  \n\n  if (result.isSuccessful()) {\n    print(\"Pig job succeeded\")\n  } else {\n    print(\"Pig job failed\")\n  }  \n}\n</pre> <p> <strong>Invocation Process</strong> </p> <p>You invoke Pig in the host scripting language through an embedded <a href=\"#pig-object\">Pig object</a>. </p> <p> <strong>Compile:</strong> Compile is a static function on the Pig class and in its simplest form takes a fragment of Pig Latin that defines the pipeline as its input:</p> <pre class=\"code\">\n# COMPILE: complie method returns a Pig object that represents the pipeline\nP = Pig.compile(\"\"\"A = load '$in'; store A into '$out';\"\"\")\n</pre> <p>Compile returns an instance of Pig object. This object can have certain values undefined. For example, you may want to define a pipeline without yet specifying the location of the input to the pipeline. The parameter will be indicated by a dollar sign followed by a sequence of alpha-numeric or underscore characters. Values for these parameters must be provided later at the time bind() is called on the Pig object. To call run() on a Pig object without all parameters being bound is an error. </p>  <p> <strong>Bind:</strong> Resolve the parameters during the bind call.</p> <pre class=\"code\">\ninput = \"original”\noutput = \"output”\n\n# BIND: bind method binds the variables with the parameters in the pipeline and returns a BoundScript object\nQ = P.bind({'in':input, 'out':output}) \n</pre> <p>Please note that all parameters must be resolved during bind. Having unbound parameters while running your script is an error. Also note that even if your script is fully defined during compile, bind without parameters still must be called.</p> <p> <strong>Run:</strong> Bind call returns an instance of <a href=\"#BoundScript-Object\">BoundScript object</a> that can be used to execute the pipeline. The simplest way to execute the pipeline is to call runSingle function. (However, as mentioned later, this works only if a single set of variables is bound to the parameters. Otherwise, if multiple set of variables are bound, an exception will be thrown if runSingle is called.)</p> <pre class=\"code\">\n\nresult = Q.runSingle()\n</pre> <p>The function returns a <a href=\"#PigStats-Object\">PigStats object</a> that tells you whether the run succeeded or failed. In case of success, additional run statistics are provided.</p> <p> <strong>Embedded Python Example</strong> </p> <p>A complete embedded example is shown below.</p> <pre class=\"code\">\n#!/usr/bin/python\n\n# explicitly import Pig class\nfrom org.apache.pig.scripting import Pig\n\n# COMPILE: compile method returns a Pig object that represents the pipeline\nP = Pig.compile(\"\"\"A = load '$in'; store A into '$out';\"\"\")\n\ninput = \"original”\noutput = \"output”\n\n# BIND: bind method binds the variables with the parameters in the pipeline and returns a BoundScript object\nQ = P.bind({'in':input, 'out':output}) \n\n# In this case, only one set of variables is bound to the pipeline, runSingle method returns a PigStats object. \n# If multiple sets of variables are bound to the pipeline, run method instead must be called and it returns \n# a list of PigStats objects.\nresult = Q.runSingle()\n\n# check the result\nif result.isSuccessful():\n    print \"Pig job succeeded\"\nelse:\n    raise \"Pig job failed\"    \n\n\nOR, SIMPLY DO THIS:\n\n\n#!/usr/bin/python\n\n# explicitly import Pig class\nfrom org.apache.pig.scripting import Pig\n\nin = \"original”\nout = \"output”\n\n# implicitly bind the parameters to the local variables \nresult= Pig.compile(\"\"\"A = load '$in'; store A into '$out';\"\"\").bind().runSingle() \n\nif result.isSuccessful():\n    print \"Pig job succeeded\"\nelse:\n    raise \"Pig job failed\"\n</pre>  <h3 id=\"invocation-details\">Invocation Details</h3> <p>All three APIs (compile, bind, run) discussed in the previous section have several versions depending on what you are trying to do.</p>  <h4 id=\"Compile\">Compile</h4> <p>In its basic form, compile just takes a Pig Latin fragment that defines the pipeline as described in the previous section. Additionally, the pipeline can be given a name. This name is only used when the embedded script is invoked via the PigRunner Java API (as discussed later in this document).</p> <pre class=\"code\">\n\n P = Pig.compile(\"P1\", \"\"\"A = load '$in'; store A into '$out';\"\"\")\n</pre> <p>In addition to providing Pig script via a string, you can store it in a file and pass the file to the compile call:</p> <pre class=\"code\">\n\nP = Pig.compileFromFile(\"myscript.pig\")\n</pre> <p>You can also name a pipeline stored in the script:</p> <pre class=\"code\">\n\nP = Pig.compileFromFile(\"P2\", \"myscript.pig\")\n</pre>  <h4 id=\"Bind\">Bind</h4> <p>In its simplest form, bind takes no parameters. In this case, an implicit bind is performed; Pig internally constructs a map of parameters from the local variables specified by the user in the script.</p> <pre class=\"code\">\nQ = P.bind() \n</pre> <p>Finally, you might want to run the same pipeline in parallel with a different set of parameters, for instance for multiple dates. In this case, bind function, needs to be passed a list of maps with each element of the list containing parameters for a single invocation. In the example below, the pipeline is run for the US, the UK, and Brazil.</p> <pre class=\"code\">\nP = Pig.compile(\"\"\"A = load '$in';\n                   B = filter A by user is not null;\n                   ...\n                   store Z into '$out';\n                \"\"\")\n\nQ = P.bind([{'in':'us_raw','out':'us_processed'},\n        {'in':'uk_raw','out':'uk_processed'},\n        {'in':'brazil_raw','out':'brazil_processed'}])\n\nresults = Q.run() # it blocks until all pipelines are completed\n\nfor i in [0, 1, 2]:\n    result = results[i]\n    ... # check result for each pipeline\n\n</pre>  <h4 id=\"Run\">Run</h4> <p>We have already seen that the simplest way to run a script is to call runSingle without any parameters. Additionally, a Java Properties object or a file containing a list of properties can be passed to this call. The properties are passed to Pig and a treated as any other properties passed from command line.</p> <pre class=\"code\">\n# In a jython script \n\nfrom java.util import Properties\n... ...\n\nprops = Properties()\nprops.put(key1, val1)  \nprops.put(key2, val2) \n... ... \n\nPig.compile(...).bind(...).runSingle(props)\n</pre> <p>A more general version of run allows to run one or more pipelines concurrently. In this case, a list of PigStats results is returned – one for each pipeline run. The example in the previous section shows how to make use of this call.</p> <p>As the case with runSingle, a set of Java Properties or a property file can be passed to the call.</p>  <h4 id=\"Passing+Parameters+to+a+Script\">Passing Parameters to a Script</h4> <p>Inside your script, you can define parameters and then pass parameters from command line to your script. There are two ways to pass parameters to your script:</p>  <h5 id=\"1.+-param\">1. -param</h5> <p> Similar to regular Pig parameter substitution, you can define parameters using -param/–param_file on Pig's command line. This variable will be treated as one of the binding variables when binding the Pig Latin script. For example, you can invoke the below Python script using: pig –param loadfile=student.txt script.py. </p> <pre class=\"code\">\n#!/usr/bin/python\nfrom org.apache.pig.scripting import Pig\n\nP = Pig.compile(\"\"\"A = load '$loadfile' as (name, age, gpa);\nstore A into 'output';\"\"\")\n\nQ = P.bind()\n\nresult = Q.runSingle()\n</pre>  <h5 id=\"2.+Command+line+arguments\">2. Command line arguments</h5> <p> Currently this feature is only available in Python and Groovy. You can pass command line arguments (the arguments after the script file name) to Python. These will become sys.argv in Python and will be passed as main's arguments in Groovy. For example: pig script.py student.txt. The corresponding script is: </p> <pre class=\"code\">\n#!/usr/bin/python\nimport sys\nfrom org.apache.pig.scripting import Pig\n\nP = Pig.compile(\"A = load '\" + sys.argv[1] + \"' as (name, age, gpa);\" +\n\"store A into 'output';\");\n\nQ = P.bind()\n\nresult = Q.runSingle()\n</pre> <p>and in Groovy, pig script.groovy student.txt:</p> <pre class=\"code\">\nimport org.apache.pig.scripting.Pig;\n\npublic static void main(String[] args) {\n\n  P = Pig.compile(\"A = load '\" + args[1] + \"' as (name, age, gpa);\" +\n                      \"store A into 'output';\");\n\n  Q = P.bind()\n\n  result = Q.runSingle()\n}\n</pre>  <h3 id=\"pigrunner-api\">PigRunner API</h3> <p>Starting with Pig 0.8, some applications such as Oozie workflow invoke Pig using the PigRunner Java class rather than through the command line. For these applications, the PigRunner interface has been expanded to accommodate embedded Pig. PigRunner accepts Python and JavaScript scripts as input. These scripts can potentially contain multiple Pig pipelines; therefore, we need a way to return results for all of them.</p> <p>To do this and to preserve backward compatibility PigStats and related objects were expanded as shown below:</p> <ul>  <li id=\"PigStats\">PigStats is now an abstract class. (PigStats as it was before has become SimplePigStats.)</li>  <li id=\"SimplePigStats\">SimplePigStats is a new class that extends PigStats. SimplePigStats.getAllStats() will return null. </li>  <li id=\"EmbeddedPigStats\">EmbeddedPigStats is a new class that extends PigStats. EmbeddedPigStats will return null for methods not listed in the proposal below. </li>  <li id=\"isembedded\">isEmbedded() is a new abstract method that accommodates embedded Pig.</li>  <li id=\"stats-messages\">getAllStats() and List&lt; &gt; getAllErrorMessages() methods were added to the PigStats class. The map returned from getAllStats is keyed on the name of the pipeline provided in the compile call. If the name was not compiled an internally generated id would be used. </li>  <li id=\"PigProgressNotificationListener2\">The PigProgressNotificationListener interface was modified to add script id to all its methods. </li> </ul>  <p>For more details, see <a href=\"#java-objects\">Java Objects</a>.</p>  <h3 id=\"Usage+Examples\">Usage Examples</h3>  <h4 id=\"pig-files\">Passing a Pig Script </h4> <p>This example shows you how to pass an entire Pig script to the compile call.</p> <pre class=\"code\">\n#!/usr/bin/python\n\nfrom org.apache.pig.scripting import Pig\n\nP = Pig.compileFromFile(\"\"\"myscript.pig\"\"\")\n\ninput = \"original\"\noutput = \"output\"\n\nresult = p.bind({'in':input, 'out':output}).runSingle()\nif result.isSuccessful():\n    print \"Pig job succeeded\"\nelse:\n    raise \"Pig job failed\" \n</pre>  <h4 id=\"convergence\">Convergence</h4> <p>There is a class of problems that involve iterating over a data pipeline an indeterminate number of times until a certain value is reached. Examples arise in machine learning, graph traversal, and a host of numerical analysis problems which involve finding interpolations, extrapolations or regressions. The Python example below shows one way to achieve convergence using Pig scripts.</p> <pre class=\"code\">\n#!/usr/bin/python\n\n# explicitly import Pig class\nfrom org.apache.pig.scripting import Pig\n\nP = Pig.compile(\"\"\"A = load '$input' as (user, age, gpa);\n                   B = group A all;\n                   C = foreach B generate AVG(A.gpa);\n                   store C into '$output';\n                \"\"\")\n# initial output\ninput = \"studenttab5\"\noutput = \"output-5\"\nfinal = \"final-output\"\n\nfor i in range(1, 4):\n    Q = P.bind({'input':input, 'output':output}) # attaches $input, $output in Pig Latin to input, output Python variable\n    results = Q.runSingle()\n\n    if results.isSuccessful() == \"FAILED\":\n        raise \"Pig job failed\"\n    iter = results.result(\"C\").iterator()\n    if iter.hasNext():\n        tuple = iter.next()\n        value = tuple.get(0)\n        if float(str(value)) &lt; 3:\n            print \"value: \" + str(value)\n            input = \"studenttab\" + str(i+5)\n            output = \"output-\" + str(i+5)\n            print \"output: \" + output\n        else:\n           Pig.fs(\"mv \" + output + \" \" + final)\n           break\n</pre>  <h4 id=\"automated-pig-latin\">Automated Pig Latin Generation</h4> <p>A number of user frameworks do automated generation of Pig Latin.</p>  <h5 id=\"Conditional+Compilation\">Conditional Compilation</h5> <p>A sub-use case of automated generation is conditional code generation. Different processing might be required based on whether this is weekday or a weekend.</p> <pre class=\"code\">\nstr = \"A = load 'input';\" \nif today.isWeekday():\n    str = str + \"B = filter A by weekday_filter(*);\" \nelse:\n    str = str + \"B = filter A by weekend_filter(*);\" \nstr = str + \"C = group B by user;\" \nresults = Pig.compile(str).bind().runSingle()\n</pre>  <h5 id=\"Parallel+Execution\">Parallel Execution</h5> <p>Another sub-use case of automated generation is parallel execution of identical pipelines. You may have a single pipeline that you would like to run multiple data sets through in parallel. In the example below, the pipeline is run for the US, the UK, and Brazil.</p> <pre class=\"code\">\n\nP = Pig.compile(\"\"\"A = load '$in';\n                   B = filter A by user is not null;\n                   ...\n                   store Z into '$out';\n                \"\"\")\n\nQ = P.bind([{'in':'us_raw','out':'us_processed'},\n        {'in':'uk_raw','out':'uk_processed'},\n        {'in':'brazil_raw','out':'brazil_processed'}])\n\nresults = Q.run() # it blocks until all pipelines are completed\n\nfor i in [0, 1, 2]:\n    result = results[i]\n    ... # check result for each pipeline\n</pre>  <h3 id=\"java-objects\">Java Objects</h3>  <h4 id=\"pig-object\">Pig Object</h4> <pre class=\"code\">\npublic class Pig {    \n    /**\n     * Run a filesystem command.  Any output from this command is written to\n     * stdout or stderr as appropriate.\n     * @param cmd Filesystem command to run along with its arguments as one\n     * string.\n     * @throws IOException\n     */\n    public static void fs(String cmd) throws IOException {...}\n    \n    /**\n     * Register a jar for use in Pig.  Once this is done this jar will be\n     * registered for ALL SUBSEQUENT Pig pipelines in this script.  \n     * If you wish to register it for only a single Pig pipeline, use \n     * register within that definition.\n     * @param jarfile Path of jar to include.\n     * @throws IOException if the indicated jarfile cannot be found.\n     */\n    public static void registerJar(String jarfile) throws IOException {...}\n    \n    /**\n     * Register script UDFs for use in Pig. Once this is done all UDFs\n     * defined in the file will be available for ALL SUBSEQUENT \n     * Pig pipelines in this script. If you wish to register UDFS for \n     * only a single Pig pipeline, use register within that definition.\n     * @param udffile Path of the script UDF file\n     * @param namespace namespace of the UDFs\n     * @throws IOException\n     */\n    public static void registerUDF(String udffile, String namespace) throws IOException {...}\n    \n    /**\n     * Define an alias for a UDF or a streaming command.  This definition\n     * will then be present for ALL SUBSEQUENT Pig pipelines defined in this \n     * script.  If you wish to define it for only a single Pig pipeline, use\n     * define within that definition.\n     * @param alias name of the defined alias\n     * @param definition string this alias is defined as\n     */\n    public static void define(String alias, String definition) throws IOException {...}\n\n    /**\n     * Set a variable for use in Pig Latin.  This set\n     * will then be present for ALL SUBSEQUENT Pig pipelines defined in this \n     * script.  If you wish to set it for only a single Pig pipeline, use\n     * set within that definition.\n     * @param var variable to set\n     * @param value to set it to\n     */\n    public static void set(String var, String value) throws IOException {...}\n            \n    /**\n     * Define a Pig pipeline.  \n     * @param pl Pig Latin definition of the pipeline.\n     * @return Pig object representing this pipeline.\n     * @throws IOException if the Pig Latin does not compile.\n     */\n    public static Pig compile(String pl) throws IOException {...}\n\n    /**\n     * Define a named portion of a Pig pipeline.  This allows it\n     * to be imported into another pipeline.\n     * @param name Name that will be used to define this pipeline.\n     * The namespace is global.\n     * @param pl Pig Latin definition of the pipeline.\n     * @return Pig object representing this pipeline.\n     * @throws IOException if the Pig Latin does not compile.\n     */\n    public static Pig compile(String name, String pl) throws IOException {...}\n\n    /**\n     * Define a Pig pipeline based on Pig Latin in a separate file.\n     * @param filename File to read Pig Latin from.  This must be a purely \n     * Pig Latin file.  It cannot contain host language constructs in it.\n     * @return Pig object representing this pipeline.\n     * @throws IOException if the Pig Latin does not compile or the file\n     * cannot be found.\n     */\n    public static Pig compileFromFile(String filename) throws IOException {...}\n\n    /**\n     * Define a named Pig pipeline based on Pig Latin in a separate file.\n     * This allows it to be imported into another pipeline.\n     * @param name Name that will be used to define this pipeline.\n     * The namespace is global.\n     * @param filename File to read Pig Latin from.  This must be a purely \n     * Pig Latin file.  It cannot contain host language constructs in it.\n     * @return Pig object representing this pipeline.\n     * @throws IOException if the Pig Latin does not compile or the file\n     * cannot be found.\n     */\n    public static Pig compileFromFile(String name, String filename) throws IOException {...}\n    \n    /**\n     * Bind this to a set of variables. Values must be provided\n     * for all Pig Latin parameters.\n     * @param vars map of variables to bind.  Keys should be parameters defined \n     * in the Pig Latin.  Values should be strings that provide values for those\n     * parameters.  They can be either constants or variables from the host\n     * language.  Host language variables must contain strings.\n     * @return a {@link BoundScript} object \n     * @throws IOException if there is not a key for each\n     * Pig Latin parameter or if they contain unsupported types.\n     */\n    public BoundScript bind(Map&lt;String, String&gt; vars) throws IOException {...}\n        \n    /**\n     * Bind this to multiple sets of variables.  This will \n     * cause the Pig Latin script to be executed in parallel over these sets of \n     * variables.\n     * @param vars list of maps of variables to bind.  Keys should be parameters defined \n     * in the Pig Latin.  Values should be strings that provide values for those\n     * variables.  They can be either constants or variables from the host\n     * language.  Host language variables must be strings.\n     * @return a {@link BoundScript} object \n     * @throws IOException  if there is not a key for each\n     * Pig Latin parameter or if they contain unsupported types.\n     */\n    public BoundScript bind(List&lt;Map&lt;String, String&gt;&gt; vars) throws IOException {...}\n\n    /**\n     * Bind a Pig object to variables in the host language (optional\n     * operation).  This does an implicit mapping of variables in the host\n     * language to parameters in Pig Latin.  For example, if the user\n     * provides a Pig Latin statement\n     * p = Pig.compile(\"A = load '$input';\");\n     * and then calls this function it will look for a variable called\n     * input in the host language.  Scoping rules of the host\n     * language will be followed in selecting which variable to bind.  The \n     * variable bound must contain a string value.  This method is optional\n     * because not all host languages may support searching for in scope\n     * variables.\n     * @throws IOException if host language variables are not found to resolve all\n     * Pig Latin parameters or if they contain unsupported types.\n     */\n    public BoundScript bind() throws IOException {...}\n\n}\n</pre>  <h4 id=\"BoundScript-Object\">BoundScript Object</h4> <pre class=\"code\">\npublic class BoundScript {\n    \n    /**\n     * Run a pipeline on Hadoop.  \n     * If there are no stores in this pipeline then nothing will be run. \n     * @return {@link PigStats}, null if there is no bound query to run.\n     * @throws IOException\n     */\n    public PigStats runSingle() throws IOException {...}\n     \n    /**\n     * Run a pipeline on Hadoop.  \n     * If there are no stores in this pipeline then nothing will be run.  \n     * @param prop Map of properties that Pig should set when running the script.\n     * This is intended for use with scripting languages that do not support\n     * the Properties object.\n     * @return {@link PigStats}, null if there is no bound query to run.\n     * @throws IOException\n     */\n    public PigStats runSingle(Properties prop) throws IOException {...}\n    \n    /**\n     * Run a pipeline on Hadoop.  \n     * If there are no stores in this pipeline then nothing will be run.  \n     * @param propfile File with properties that Pig should set when running the script.\n     * @return {@link PigStats}, null if there is no bound query to run.\n     * @throws IOException\n     */\n    public PigStats runSingle(String propfile) throws IOException {...}\n\n    /**\n     * Run multiple instances of bound pipeline on Hadoop in parallel.  \n     * If there are no stores in this pipeline then nothing will be run.  \n     * Bind is called first with the list of maps of variables to bind. \n     * @return a list of {@link PigStats}, one for each map of variables passed\n     * to bind.\n     * @throws IOException\n     */    \n    public List&lt;PigStats&gt; run() throws IOException {...}\n    \n    /**\n     * Run multiple instances of bound pipeline on Hadoop in parallel.\n     * @param prop Map of properties that Pig should set when running the script.\n     * This is intended for use with scripting languages that do not support\n     * the Properties object.\n     * @return a list of {@link PigStats}, one for each map of variables passed\n     * to bind.\n     * @throws IOException\n     */\n    public List&lt;PigStats&gt;  run(Properties prop) throws IOException {...}\n    \n    /**\n     * Run multiple instances of bound pipeline on Hadoop in parallel.\n     * @param propfile File with properties that Pig should set when running the script.\n     * @return a list of PigResults, one for each map of variables passed\n     * to bind.\n     * @throws IOException\n     */\n    public List&lt;PigStats&gt;  run(String propfile) throws IOException {...}\n\n    /**\n     * Run illustrate for this pipeline.  Results will be printed to stdout.  \n     * @throws IOException if illustrate fails.\n     */\n    public void illustrate() throws IOException {...}\n\n    /**\n     * Explain this pipeline.  Results will be printed to stdout.\n     * @throws IOException if explain fails.\n     */\n    public void explain() throws IOException {...}\n\n    /**\n     * Describe the schema of an alias in this pipeline.\n     * Results will be printed to stdout.\n     * @param alias to be described\n     * @throws IOException if describe fails.\n     */\n    public void describe(String alias) throws IOException {...}\n\n}\n</pre>  <h4 id=\"PigStats-Object\">PigStats Object</h4> <pre class=\"code\">\npublic abstract class PigStats {\n    public abstract boolean isEmbedded();\n    \n    /**\n     * An embedded script contains one or more pipelines. \n     * For a named pipeline in the script, the key in the returning map is the name of the pipeline. \n     * Otherwise, the key in the returning map is the script id of the pipeline.\n     */\n    public abstract Map&lt;String, List&lt;PigStats&gt;&gt; getAllStats();\n    \n    public abstract List&lt;String&gt; getAllErrorMessages();      \n}\n</pre>  <h4 id=\"PigProgressNotificationListener\">PigProgressNotificationListener Object</h4> <pre class=\"code\">\npublic interface PigProgressNotificationListener extends java.util.EventListener {\n\n    /** \n     * Invoked just before launching MR jobs spawned by the script.\n     * @param scriptId id of the script\n     * @param numJobsToLaunch the total number of MR jobs spawned by the script\n     */\n    public void launchStartedNotification(String scriptId, int numJobsToLaunch);\n    \n    /**\n     * Invoked just before submitting a batch of MR jobs.\n     * @param scriptId id of the script\n     * @param numJobsSubmitted the number of MR jobs in the batch\n     */\n    public void jobsSubmittedNotification(String scriptId, int numJobsSubmitted);\n    \n    /**\n     * Invoked after a MR job is started.\n     * @param scriptId id of the script \n     * @param assignedJobId the MR job id\n     */\n    public void jobStartedNotification(String scriptId, String assignedJobId);\n    \n    /**\n     * Invoked just after a MR job is completed successfully. \n     * @param scriptId id of the script \n     * @param jobStats the {@link JobStats} object associated with the MR job\n     */\n    public void jobFinishedNotification(String scriptId, JobStats jobStats);\n    \n    /**\n     * Invoked when a MR job fails.\n     * @param scriptId id of the script \n     * @param jobStats the {@link JobStats} object associated with the MR job\n     */\n    public void jobFailedNotification(String scriptId, JobStats jobStats);\n    \n    /**\n     * Invoked just after an output is successfully written.\n     * @param scriptId id of the script\n     * @param outputStats the {@link OutputStats} object associated with the output\n     */\n    public void outputCompletedNotification(String scriptId, OutputStats outputStats);\n    \n    /**\n     * Invoked to update the execution progress. \n     * @param scriptId id of the script\n     * @param progress the percentage of the execution progress\n     */\n    public void progressUpdatedNotification(String scriptId, int progress);\n    \n    /**\n     * Invoked just after all MR jobs spawned by the script are completed.\n     * @param scriptId id of the script\n     * @param numJobsSucceeded the total number of MR jobs succeeded\n     */\n    public void launchCompletedNotification(String scriptId, int numJobsSucceeded);\n}\n</pre> </div>   <h2 id=\"embed-java\">Embedded Pig - Java </h2> <div class=\"section\"> <p>To enable control flow, you can embed Pig Latin statements and Pig commands in the Java programming language. </p> <p>Note that host languages and the languages of UDFs (included as part of the embedded Pig) are completely orthogonal. For example, a Pig Latin statement that registers a Java UDF may be embedded in Python, JavaScript, Groovy, or Java. The exception to this rule is \"combined\" scripts – here the languages must match (see the <a href=\"udf#jython-advanced\">Advanced Topics for Python</a>, <a href=\"udf#js-advanced\">Advanced Topics for JavaScript</a> and <a href=\"udf#groovy-advanced\">Advanced Topics for Groovy</a>). </p>  <h3 id=\"pigserver\">PigServer Interface</h3> <p>Currently, <a href=\"http://pig.apache.org/docs/r0.13.0/api/org/apache/pig/PigServer.html\">PigServer</a> is the main interface for embedding Pig in Java. PigServer can now be instantiated from multiple threads. (In the past, PigServer contained references to static data that prevented multiple instances of the object to be created from different threads within your application.) Please note that PigServer is NOT thread safe; the same object can't be shared across multiple threads. </p>  <h3 id=\"Usage+Examples-N1020F\">Usage Examples</h3> <p> <strong>Local Mode</strong> </p> <p>From your current working directory, compile the program. (Note that idlocal.class is written to your current working directory. Include “.” in the class path when you run the program.) </p> <pre class=\"code\">\n$ javac -cp pig.jar idlocal.java\n</pre>  <p>From your current working directory, run the program. To view the results, check the output file, id.out.</p> <pre class=\"code\">\nUnix:    $ java -cp pig.jar:. idlocal\nWindows: $ java –cp .;pig.jar idlocal\n</pre> <p>idlocal.java - The sample code is based on Pig Latin statements that extract all user IDs from the /etc/passwd file. Copy the /etc/passwd file to your local working directory.</p> <pre class=\"code\">\nimport java.io.IOException;\nimport org.apache.pig.PigServer;\npublic class idlocal{ \n    public static void main(String[] args) {\n        try {\n            PigServer pigServer = new PigServer(\"local\");\n            runIdQuery(pigServer, \"passwd\");\n        }\n        catch(Exception e) {\n        }\n    }\n    public static void runIdQuery(PigServer pigServer, String inputFile) throws IOException {\n        pigServer.registerQuery(\"A = load '\" + inputFile + \"' using PigStorage(':');\");\n        pigServer.registerQuery(\"B = foreach A generate $0 as id;\");\n        pigServer.store(\"B\", \"id.out\");\n    }\n}\n</pre>  <p> <strong>Mapreduce Mode</strong> </p> <p>Point $HADOOPDIR to the directory that contains the hadoop-site.xml file. Example: </p> <pre class=\"code\">\n$ export HADOOPDIR=/yourHADOOPsite/conf \n</pre> <p>From your current working directory, compile the program. (Note that idmapreduce.class is written to your current working directory. Include “.” in the class path when you run the program.) </p> <pre class=\"code\">\n$ javac -cp pig.jar idmapreduce.java\n</pre>  <p>From your current working directory, run the program. To view the results, check the idout directory on your Hadoop system. </p> <pre class=\"code\">\nUnix:   $ java -cp pig.jar:.:$HADOOPDIR idmapreduce\nCygwin: $ java –cp '.;pig.jar;$HADOOPDIR' idmapreduce\n</pre> <p>idmapreduce.java - The sample code is based on Pig Latin statements that extract all user IDs from the /etc/passwd file. Copy the /etc/passwd file to your home directory on the HDFS.</p> <pre class=\"code\">\nimport java.io.IOException;\nimport org.apache.pig.PigServer;\npublic class idmapreduce{\n    public static void main(String[] args) {\n        try {\n            PigServer pigServer = new PigServer(\"mapreduce\");\n            runIdQuery(pigServer, \"passwd\");\n        }\n        catch(Exception e) {\n        }\n    }\n    public static void runIdQuery(PigServer pigServer, String inputFile) throws IOException {\n        pigServer.registerQuery(\"A = load '\" + inputFile + \"' using PigStorage(':');\")\n        pigServer.registerQuery(\"B = foreach A generate $0 as id;\");\n        pigServer.store(\"B\", \"idout\");\n    }\n}\n</pre> </div>   <h2 id=\"macros\">Pig Macros</h2> <div class=\"section\"> <p>Pig Latin supports the definition, expansion, and import of macros.</p>  <h3 id=\"define-macros\">DEFINE (macros)</h3> <p>Defines a Pig macro.</p>  <h4 id=\"Syntax\">Syntax</h4> <p>Define Macro</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DEFINE macro_name (param [, param ...]) RETURNS {void | alias [, alias ...]} { pig_latin_fragment }; </p> </td> </tr> </table>  <p id=\"expand-macros\">Expand Macro</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias [, alias ...] = macro_name (param [, param ...]) ; </p> </td> </tr> </table>  <h4 id=\"Terms\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>macro_name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the macro. Macro names are global.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>param</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(optional) A comma-separated list of one or more parameters, including IN aliases (Pig relations), enclosed in parentheses, that are referenced in the Pig Latin fragment.</p> <p>Unlike user defined functions (UDFs), which only allow quoted strings as its parameters, Pig macros support four types of parameters:</p> <ul> <li>alias (IDENTIFIER)</li> <li>integer </li> <li>float</li> <li>string literal (quoted string)</li> </ul> <p>Note that type is NOT part of parameter definition. It is your responsibility to document the types of the parameters in a macro.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>void</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If the macro has no return alias, then void must be specified.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(optional) A comma-separated list of one or more return aliases (Pig relations) that are referenced in the Pig Latin fragment. The alias must exist in the macro in the form $&lt;alias&gt;.</p> <p>If the macro has no return alias, then void must be specified.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>pig_latin_fragment</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>One or more Pig Latin statements, enclosed in curly brackets.</p> </td> </tr> </table>  <h4 id=\"Usage\">Usage</h4> <p> <strong>Macro Definition</strong> </p> <p>A macro definition can appear anywhere in a Pig script as long as it appears prior to the first use. A macro definition can include references to other macros as long as the referenced macros are defined prior to the macro definition. Recursive references are not allowed. </p> <p>Note the following restrictions:</p> <ul> <li>Macros are not allowed inside a <a href=\"basic#nested-block\">FOREACH</a> nested block.</li> <li>Macros cannot contain <a href=\"cmds#shell-cmds\">Grunt shell commands</a>.</li> <li>Macros cannot include a user-defined schema that has a name collision with an alias in the macro.</li> </ul>  <p>In this example the macro is named my_macro. Note that only aliases A and C are visible from the outside; alias B is not visible from the outside.</p> <pre class=\"code\">\n DEFINE my_macro(A, sortkey) RETURNS C {\n    B = FILTER $A BY my_filter(*);\n    $C = ORDER B BY $sortkey;\n}\n</pre>  <p> <strong>Macro Expansion</strong> </p> <p>A macro can be expanded inline using the macro expansion syntax. Note the following:</p> <ul> <li>Any alias in the macro which isn't visible from the outside will be prefixed with a macro name and suffixed with an instance id to avoid namespace collision. </li> <li>Macro expansion is not a complete replacement for function calls. Recursive expansions are not supported. </li> </ul>  <p>In this example my_macro (defined above) is expanded. Because alias B is not visible from the outside it is renamed macro_my_macro_B_0.</p> <pre class=\"code\">\n/* These statements ... */\n\nX = LOAD 'users' AS (user, address, phone);\nY = my_macro(X, user);\nSTORE Y into 'bar';\n\n/* Are expanded into these statements ... */\n\nX = LOAD 'users' AS (user, address, phone);\nmacro_my_macro_B_0 = FILTER X BY my_filter(*);\nY = ORDER macro_my_macro_B_0  BY user;\nSTORE Y INTO 'output';\n</pre> <p> <strong>Macro Import</strong> </p> <p>A macro can be imported from another Pig script (see <a href=\"#import-macros\">IMPORT (macros)</a>). Splitting your macros from your main Pig script is useful for making reusable code.</p>  <h4 id=\"Examples\">Examples</h4> <p>In this example no parameters are passed to the macro.</p> <pre class=\"code\">\nDEFINE my_macro() returns B {\n   D = LOAD 'data' AS (a0:int, a1:int, a2:int);   \n   $B = FILTER D BY ($1 == 8) OR (NOT ($0+$2 &gt; $1));\n};\n\nX = my_macro();\nSTORE X INTO 'output';\n</pre> <p>In this example parameters are passed and returned.</p> <pre class=\"code\">\nDEFINE group_and_count (A, group_key, reducers) RETURNS B {\n   D = GROUP $A BY $group_key PARALLEL $reducers;\n   $B = FOREACH D GENERATE group, COUNT($A);\n};\n\nX = LOAD 'users' AS (user, age, zip);\nY = group_and_count (X, user, 20);\nZ = group_and_count (X, age, 30);\nSTORE Y into 'byuser';\nSTORE Z into 'byage';\n</pre> <p>In this example the macro does not have a return alias; thus, void must be specified.</p> <pre class=\"code\">\nDEFINE my_macro(A, sortkey) RETURNS void {     \n      B = FILTER $A BY my_filter(*);     \n      C = ORDER B BY $sortkey;\n      STORE C INTO 'my_output';  \n};\n\n/* To expand this macro, use the following */\n\nmy_macro(alpha, 'user');\n</pre> <p> In this example a name collision will occur. Here letter B is used as alias name and as name in user-defined schema. Pig will throw an exception when name collision is detected.</p> <pre class=\"code\">\nDEFINE my_macro(A, sortkey) RETURNS E {     \n      B = FILTER $A BY my_filter(*);     \n      C = ORDER B BY $sortkey;\n      D = LOAD 'in' as (B:bag{});\n   $E = FOREACH D GENERATE COUNT(B); \n   };\n</pre> <p>This example demonstrates the importance of knowing parameter types before using them in a macro script. Notice that when pass parameter $outfile to my_macro1 inside my_macro2, it must be quoted.</p> <pre class=\"code\">\n-- A: an alias\n-- outfile: output file path (quoted string)\nDEFINE my_macro1(A, outfile) RETURNS void {     \n       STORE $A INTO '$outfile'; \n   };\n\n-- A: an alias\n-- sortkey: column name (quoted string)\n-- outfile: output file path (quoted string)\nDEFINE my_macro2(A, sortkey, outfile) RETURNS void {     \n      B = FILTER $A BY my_filter(*);     \n      C = ORDER B BY $sortkey;\n      my_macro1(C, '$outfile');\n   };\n\n   alpha = Load 'input' as (user, age, gpa);\n   my_macro2(alpha, 'age', 'order_by_age.txt');\n</pre> <p>In this example a macro (group_with_parallel) refers to another macro (foreach_count).</p> <pre class=\"code\">\nDEFINE foreach_count(A, C) RETURNS B {\n   $B = FOREACH $A GENERATE group, COUNT($C);\n};\n\nDEFINE group_with_parallel (A, group_key, reducers) RETURNS B {\n   C = GROUP $A BY $group_key PARALLEL $reducers;\n   $B = foreach_count(C, $A);\n};\n       \n/* These statements ... */\n \nX = LOAD 'users' AS (user, age, zip);\nY = group_with_parallel (X, user, 23);\nSTORE Y INTO 'byuser';\n\n/* Are expanded into these statements ... */\n\nX = LOAD 'users' AS (user, age, zip);\nmacro_group_with_parallel_C_0 = GROUP X by (user) PARALLEL 23;\nY = FOREACH macro_group_with_parallel_C_0 GENERATE group, COUNT(X);\nSTORE Y INTO 'byuser';\n</pre>  <h3 id=\"import-macros\">IMPORT (macros)</h3> <p>Import macros defined in a separate file.</p>  <h4 id=\"Syntax-N103B4\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>IMPORT 'file-with-macro';</p> </td> </tr> </table>  <h4 id=\"Terms-N103C9\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>file-with-macro</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a file (enclosed in single quotes) that contains one or more macro definitions; for example, 'my_macro.pig' or 'mypath/my_macro.pig'.</p>  <p>Macro names are global and all macros share the same name space. While the file can contain more than one macro definition, having two macros with the same name in your execution context will result in an error.</p>  <p>Files are imported based on either (1) the given file path or (2) the import path specified via the Pig property pig.import.search.path. If a file path is given, whether absolute or relative to the current directory (starting with . or ..), the import path will be ignored. </p>  </td> </tr> </table>  <h4 id=\"Usage-N103F2\">Usage</h4> <p>Use the IMPORT command to import a macro defined in a separate file into your Pig script. </p> <p>IMPORT adds the macro definitions to the Pig Latin namespace; these macros can then be invoked as if they were defined in the same file.</p> <p>Macros can only contain Pig Latin statements; Grunt shell commands are not supported. REGISTER statements and parameter definitions with %default or %declare are both valid however. Your macro file also IMPORT other macro files, so long as these imports are not recursive.</p> <p>See also: <a href=\"#define-macros\">DEFINE (macros)</a> </p>  <h4 id=\"Example\">Example</h4> <p>In this example, because a path is not given, Pig will use the import path specified in <span class=\"codefrag\">pig.import.search.path</span>.</p> <pre class=\"code\">\n/* myscript.pig */\n...\n...\nIMPORT 'my_macro.pig';\n...\n...\n</pre> </div>   <h2 id=\"Parameter-Sub\">Parameter Substitution</h2> <div class=\"section\">  <h3 id=\"Description\">Description</h3> <p>Substitute values for parameters at run time.</p>  <h4 id=\"Syntax%3A+Specifying+Parameters+Using+the+Pig+Command+Line\">Syntax: Specifying Parameters Using the Pig Command Line</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>pig {-param param_name = param_value | -param_file file_name} [-debug | -dryrun] script</p> </td> </tr> </table>  <h4 id=\"Syntax%3A+Specifying+Parameters+Using+Preprocessor+Statements+in+a+Pig+Script\">Syntax: Specifying Parameters Using Preprocessor Statements in a Pig Script</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>{%declare | %default} param_name param_value</p> </td> </tr> </table>  <h4 id=\"Terms-N10458\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>pig</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> <p>Note: exec, run, and explain also support parameter substitution.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-param</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Flag. Use this option when the parameter is included in the command line.</p> <p>Multiple parameters can be specified. If the same parameter is specified multiple times, the last value will be used and a warning will be generated.</p> <p>Command line parameters and parameter files can be combined with command line parameters taking precedence. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>param_name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the parameter.</p> <p>The parameter name has the structure of a standard language identifier: it must start with a letter or underscore followed by any number of letters, digits, and underscores. </p> <p>Parameter names are case insensitive. </p> <p>If you pass a parameter to a script that the script does not use, this parameter is silently ignored. If the script has a parameter and no value is supplied or substituted, an error will result.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>param_value</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The value of the parameter. </p> <p>A parameter value can take two forms:</p> <ul> <li> <p>A sequence of characters enclosed in single or double quotes. In this case the unquoted version of the value is used during substitution. Quotes within the value can be escaped with the backslash character ( \\ ). Single word values that don't use special characters such as % or = don't have to be quoted. </p> </li> <li> <p>A command enclosed in back ticks. </p> </li> </ul> <p>The value of a parameter, in either form, can be expressed in terms of other parameters as long as the values of the dependent parameters are already defined.</p> <p>There are no hard limits on the size except that parameters need to fit into memory.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-param_file</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Flag. Use this option when the parameter is included in a file. </p> <p>Multiple files can be specified. If the same parameter is present multiple times in the file, the last value will be used and a warning will be generated. If a parameter present in multiple files, the value from the last file will be used and a warning will be generated.</p> <p>Command line parameters and parameter files can be combined with command line parameters taking precedence. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>file_name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a file containing one or more parameters.</p> <p>A parameter file will contain one line per parameter. Empty lines are allowed. Perl-style (#) comment lines are also allowed. Comments must take a full line and # must be the first character on the line. Each parameter line will be of the form: param_name = param_value. White spaces around = are allowed but are optional.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-debug</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Flag. With this option, the script is run and a fully substituted Pig script is produced in the current working directory named original_script_name.substituted </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-dryrun</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Flag. With this option, the script is not run and a fully substituted Pig script is produced in the current working directory named original_script_name.substituted</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>script</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A pig script. The pig script must be the last element in the Pig command line.</p> <ul> <li> <p>If parameters are specified in the Pig command line or in a parameter file, the script should include a $param_name for each para_name included in the command line or parameter file.</p> </li> <li> <p>If parameters are specified using the preprocessor statements, the script should include either %declare or %default.</p> </li> <li> <p>In the script, parameter names can be escaped with the backslash character ( \\ ) in which case substitution does not take place.</p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>%declare</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Preprocessor statement included in a Pig script.</p> <p>Use to describe one parameter in terms of other parameters.</p> <p>The declare statement is processed prior to running the Pig script. </p> <p>The scope of a parameter value defined using declare is all the lines following the declare statement until the next declare statement that defines the same parameter is encountered.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>%default</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Preprocessor statement included in a Pig script.</p> <p>Use to provide a default value for a parameter. The default value has the lowest priority and is used if a parameter value has not been defined by other means.</p> <p>The default statement is processed prior to running the Pig script. </p> <p>The scope is the same as for %declare.</p> </td> </tr> </table>  <h3 id=\"Usage-N1058F\">Usage</h3> <p>Parameter substitution enables you to write Pig scripts that include parameters and to supply values for these parameters at run time. For instance, suppose you have a job that needs to run every day using the current day's data. You can create a Pig script that includes a parameter for the date. Then, when you run this script you can specify or supply a value for the date parameter using one of the supported methods. </p>  <h4 id=\"Specifying+Parameters\">Specifying Parameters </h4> <p>You can specify parameter names and parameter values as follows:</p> <ul> <li> <p>As part of a command line.</p> </li> <li> <p>In parameter file, as part of a command line.</p> </li> <li> <p>With the declare statement, as part of Pig script.</p> </li> <li> <p>With default statement, as part of a Pig script.</p> </li> </ul>  <p>Parameter substitution may be used inside of macros, but it is the responsibility of the user to ensure that there are no conflicts between names of parameters defined at the top level and names of arguments or return values for a macro. A simple way to ensure this is to use ALL_CAPS for top-level parameters and lower_case for macro-level parameters. See <a href=\"#define-macros\">DEFINE (macros)</a>.</p>  <h4 id=\"Precedence\">Precedence</h4> <p>Precedence for parameters is as follows, from highest to lowest:</p> <ol> <li> <p>Parameters defined using the declare statement</p> </li> <li> <p>Parameters defined in the command line using -param</p> </li> <li> <p>Parameters defined in parameter files specified by -param_file</p> </li> <li> <p>Parameters defined using the default statement</p> </li> </ol>  <h4 id=\"Processing+Order+and+Precedence\">Processing Order and Precedence</h4> <p>Parameters are processed as follows:</p> <ul> <li> <p>Command line parameters are scanned in the order they are specified on the command line. </p> </li> <li> <p>Parameter files are scanned in the order they are specified on the command line. Within each file, the parameters are processed in the order they are listed. </p> </li> <li> <p>Declare and default preprocessors statements are processed in the order they appear in the Pig script. </p> </li> </ul>  <h3 id=\"Examples-N1060C\">Examples</h3>  <h4 id=\"Specifying+Parameters+in+the+Command+Line\">Specifying Parameters in the Command Line</h4> <p>Suppose we have a data file called 'mydata' and a pig script called 'myscript.pig'.</p> <p>mydata </p> <pre class=\"code\">\n1       2       3\n4       2       1\n8       3       4\n</pre> <p>myscript.pig</p> <pre class=\"code\">\nA = LOAD '$data' USING PigStorage() AS (f1:int, f2:int, f3:int);\nDUMP A;\n</pre> <p>In this example the parameter (data) and the parameter value (mydata) are specified in the command line. If the parameter name in the command line (data) and the parameter name in the script ($data) do not match, the script will not run. If the value for the parameter (mydata) is not found, an error is generated.</p> <pre class=\"code\">\n$ pig -param data=mydata myscript.pig\n\n(1,2,3)\n(4,2,1)\n(8,3,4)\n</pre>  <h4 id=\"Specifying+parameters+Using+a+Parameter+File\">Specifying parameters Using a Parameter File</h4> <p>Suppose we have a parameter file called 'myparams.'</p> <pre class=\"code\">\n# my parameters\ndata1 = mydata1\ncmd = `generate_name`\n</pre> <p>In this example the parameters and values are passed to the script using the parameter file.</p> <pre class=\"code\">\n$ pig -param_file myparams script2.pig\n</pre>  <h4 id=\"Specifying+Parameters+Using+the+Declare+Statement\">Specifying Parameters Using the Declare Statement</h4> <p>In this example the command is executed and its stdout is used as the parameter value.</p> <pre class=\"code\">\n%declare CMD `generate_date`;\nA = LOAD '/data/mydata/$CMD';\nB = FILTER A BY $0&gt;'5';\n\n<em>etc ... </em>\n\n</pre>  <h4 id=\"Specifying+Parameters+Using+the+Default+Statement\">Specifying Parameters Using the Default Statement</h4> <p>In this example the parameter (DATE) and value ('20090101') are specified in the Pig script using the default statement. If a value for DATE is not specified elsewhere, the default value 20090101 is used.</p> <pre class=\"code\">\n%default DATE '20090101';\nA = load '/data/mydata/$DATE';\n\n<em>etc ... </em>\n\n</pre>  <h4 id=\"Specifying+Parameter+Values+as+a+sequence+of+Characters\">Specifying Parameter Values as a sequence of Characters</h4> <p>In this example the characters (in this case, Joe's URL) can be enclosed in single or double quotes, and quotes within the sequence of characters can be escaped. </p> <pre class=\"code\">\n%declare DES 'Joe\\'s URL';\nA = LOAD 'data' AS (name, description, url);\nB = FILTER A BY description == '$DES';\n \n<em>etc ... </em>\n\n</pre> <p>In this example single word values that don't use special characters (in this case, mydata) don't have to be enclosed in quotes.</p> <pre class=\"code\">\n$ pig -param data=mydata myscript.pig\n</pre>  <h4 id=\"Specifying+Parameter+Values+as+a+Command\">Specifying Parameter Values as a Command</h4> <p>In this example the command is enclosed in back ticks. First, the parameters mycmd and date are substituted when the declare statement is encountered. Then the resulting command is executed and its stdout is placed in the path before the load statement is run.</p> <pre class=\"code\">\n%declare CMD `$mycmd $date`;\nA = LOAD '/data/mydata/$CMD';\nB = FILTER A BY $0&gt;'5';\n \n<em>etc ... </em>\n\n</pre> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/cont.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/cont.html</a>\n  </p>\n</div>\n","udf":"<h1>User Defined Functions</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#udfs\">Introduction</a> </li> <li> <a href=\"#udf-java\">Writing Java UDFs</a> <ul class=\"minitoc\"> <li> <a href=\"#eval-functions\">Eval Functions</a> </li> <li> <a href=\"#load-store-functions\"> Load/Store Functions</a> </li> <li> <a href=\"#use-short-names\">Using Short Names</a> </li> <li> <a href=\"#Advanced+Topics\">Advanced Topics</a> </li> </ul> </li> <li> <a href=\"#jython-udfs\">Writing Jython UDFs</a> <ul class=\"minitoc\"> <li> <a href=\"#register-jython\">Registering the UDF</a> </li> <li> <a href=\"#python_decorators\">Decorators and Schemas</a> </li> <li> <a href=\"#Example+Scripts\">Example Scripts</a> </li> <li> <a href=\"#jython-advanced\">Advanced Topics</a> </li> </ul> </li> <li> <a href=\"#js-udfs\">Writing JavaScript UDFs</a> <ul class=\"minitoc\"> <li> <a href=\"#register-js\">Registering the UDF</a> </li> <li> <a href=\"#schema-return-types\">Return Types and Schemas</a> </li> <li> <a href=\"#js-example\">Example Scripts</a> </li> <li> <a href=\"#js-advanced\">Advanced Topics</a> </li> </ul> </li> <li> <a href=\"#jruby-udfs\">Writing Ruby UDFs</a> <ul class=\"minitoc\"> <li> <a href=\"#write-jruby\">Writing a Ruby UDF</a> </li> <li> <a href=\"#jruby-schema-return-types\">Return Types and Schemas</a> </li> <li> <a href=\"#register-jruby\">Registering the UDF</a> </li> <li> <a href=\"#jruby-example\">Example Scripts</a> </li> <li> <a href=\"#jruby-advanced\">Advanced Topics</a> </li> </ul> </li> <li> <a href=\"#groovy-udfs\">Writing Groovy UDFs</a> <ul class=\"minitoc\"> <li> <a href=\"#register-groovy\">Registering the UDF</a> </li> <li> <a href=\"#groovy-schema-return-types\">Return Types and Schemas</a> </li> <li> <a href=\"#groovy-type-conversions\">Type Conversions</a> </li> <li> <a href=\"#groovy-advanced\">Advanced Topics</a> </li> </ul> </li> <li> <a href=\"#python-udfs\">Writing Python UDFs</a> <ul class=\"minitoc\"> <li> <a href=\"#register-python\">Registering the UDF</a> </li> <li> <a href=\"#decorators\">Decorators and Schemas</a> </li> </ul> </li> <li> <a href=\"#piggybank\">Piggy Bank</a> <ul class=\"minitoc\"> <li> <a href=\"#piggbank-access\">Accessing Functions</a> </li> <li> <a href=\"#piggybank-contribute\">Contributing Functions</a> </li> </ul> </li> </ul> </div> </div>  <h2 id=\"udfs\">Introduction</h2> <div class=\"section\"> <p> Pig provides extensive support for user defined functions (UDFs) as a way to specify custom processing. Pig UDFs can currently be implemented in six languages: Java, Jython, Python, JavaScript, Ruby and Groovy. </p> <p> The most extensive support is provided for Java functions. You can customize all parts of the processing including data load/store, column transformation, and aggregation. Java functions are also more efficient because they are implemented in the same language as Pig and because additional interfaces are supported such as the <a href=\"#algebraic-interface\">Algebraic Interface</a> and the <a href=\"#accumulator-interface\">Accumulator Interface</a>. </p> <p> Limited support is provided for Jython, Python, JavaScript, Ruby and Groovy functions. These functions are new, still evolving, additions to the system. Currently only the basic interface is supported; load/store functions are not supported. Furthermore, JavaScript, Ruby and Groovy are provided as experimental features because they did not go through the same amount of testing as Java or Jython. At runtime note that Pig will automatically detect the usage of a scripting UDF in the Pig script and will automatically ship the corresponding scripting jar, either Jython, Rhino, JRuby or Groovy-all, to the backend. Python does not require any runtime engine since it invoke python command line and stream data in and out of it.</p>  <p> Pig also provides support for Piggy Bank, a repository for JAVA UDFs. Through Piggy Bank you can access Java UDFs written by other users and contribute Java UDFs that you have written. </p> </div>    <h2 id=\"udf-java\">Writing Java UDFs</h2> <div class=\"section\">  <h3 id=\"eval-functions\">Eval Functions</h3>  <h4 id=\"eval-functions-use\">How to Use a Simple Eval Function</h4> <p>Eval is the most common type of function. It can be used in <span class=\"codefrag\">FOREACH</span> statements as shown in this script: </p> <pre class=\"code\">\n-- myscript.pig\nREGISTER myudfs.jar;\nA = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);\nB = FOREACH A GENERATE myudfs.UPPER(name);\nDUMP B;\n</pre> <p>The command below can be used to run the script. Note that all examples in this document run in local mode for simplicity but the examples can also run in Tez local/Mapreduce/ Tez mode. For more information on how to run Pig, please see the PigTutorial. </p> <pre class=\"code\">\npig -x local myscript.pig\n</pre> <p>The first line of the script provides the location of the <span class=\"codefrag\">jar file</span> that contains the UDF. (Note that there are no quotes around the jar file. Having quotes would result in a syntax error.) To locate the jar file, Pig first checks the <span class=\"codefrag\">classpath</span>. If the jar file can't be found in the classpath, Pig assumes that the location is either an absolute path or a path relative to the location from which Pig was invoked. If the jar file can't be found, an error will be printed: <span class=\"codefrag\">java.io.IOException: Can't read jar file: myudfs.jar</span>. </p> <p>Multiple <span class=\"codefrag\">register</span> commands can be used in the same script. If the same fully-qualified function is present in multiple jars, the first occurrence will be used consistently with Java semantics. </p> <p>The name of the UDF has to be fully qualified with the package name or an error will be reported: <span class=\"codefrag\">java.io.IOException: Cannot instantiate:UPPER</span>. Also, the function name is case sensitive (UPPER and upper are not the same). A UDF can take one or more parameters. The exact signature of the function should be clear from its documentation. </p> <p>The function provided in this example takes an ASCII string and produces its uppercase version. If you are familiar with column transformation functions in SQL, you will recognize that UPPER fits this concept. However, as we will see later in the document, eval functions in Pig go beyond column transformation functions and include aggregate and filter functions. </p> <p>If you are just a user of UDFs, this is most of what you need to know about UDFs to use them in your code. </p>  <h4 id=\"eval-functions-write\"> How to Write a Simple Eval Function</h4> <p>Let's now look at the implementation of the <span class=\"codefrag\">UPPER</span> UDF. </p> <pre class=\"code\">\n1  package myudfs;\n2  import java.io.IOException;\n3  import org.apache.pig.EvalFunc;\n4  import org.apache.pig.data.Tuple;\n5\n6  public class UPPER extends EvalFunc&lt;String&gt;\n7  {\n8    public String exec(Tuple input) throws IOException {\n9        if (input == null || input.size() == 0 || input.get(0) == null)\n10            return null;\n11        try{\n12            String str = (String)input.get(0);\n13           return str.toUpperCase();\n14        }catch(Exception e){\n15            throw new IOException(\"Caught exception processing input row \", e);\n16        }\n17    }\n18  }\n</pre> <p>Line 1 indicates that the function is part of the <span class=\"codefrag\">myudfs</span> package. The UDF class extends the <span class=\"codefrag\">EvalFunc</span> class which is the base class for all eval functions. It is parameterized with the return type of the UDF which is a Java <span class=\"codefrag\">String</span> in this case. We will look into the <span class=\"codefrag\">EvalFunc</span> class in more detail later, but for now all we need to do is to implement the <span class=\"codefrag\">exec</span> function. This function is invoked on every input tuple. The input into the function is a tuple with input parameters in the order they are passed to the function in the Pig script. In our example, it will contain a single string field corresponding to the student name. </p> <p>The first thing to decide is what to do with invalid data. This depends on the format of the data. If the data is of type <span class=\"codefrag\">bytearray</span> it means that it has not yet been converted to its proper type. In this case, if the format of the data does not match the expected type, a NULL value should be returned. If, on the other hand, the input data is of another type, this means that the conversion has already happened and the data should be in the correct format. This is the case with our example and that's why it throws an error (line 15.) </p> <p>Also, note that lines 9-10 check if the input data is null or empty and if so returns null. </p> <p>The actual function implementation is on lines 12-13 and is self-explanatory. </p> <p>Now that we have the function implemented, it needs to be compiled and included in a jar. You will need to build <span class=\"codefrag\">pig.jar</span> to compile your UDF. You can use the following set of commands to checkout the code from SVN repository and create pig.jar: </p> <pre class=\"code\">\nsvn co http://svn.apache.org/repos/asf/pig/trunk\ncd trunk\nant\n</pre> <p>You should see <span class=\"codefrag\">pig.jar</span> in your current working directory. The set of commands below first compiles the function and then creates a jar file that contains it. </p> <pre class=\"code\">\ncd myudfs\njavac -cp pig.jar UPPER.java\ncd ..\njar -cf myudfs.jar myudfs\n</pre> <p>You should now see <span class=\"codefrag\">myudfs.jar</span> in your current working directory. You can use this jar with the script described in the previous section. </p>  <h4 id=\"aggregate-functions\">Aggregate Functions</h4> <p>Aggregate functions are another common type of eval function. Aggregate functions are usually applied to grouped data, as shown in this script: </p> <pre class=\"code\">\n-- myscript2.pig\nA = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);\nB = GROUP A BY name;\nC = FOREACH B GENERATE group, COUNT(A);\nDUMP C;\n</pre> <p>The script above uses the <span class=\"codefrag\">COUNT</span> function to count the number of students with the same name. There are a couple of things to note about this script. First, even though we are using a function, there is no <span class=\"codefrag\">register</span> command. Second, the function is not qualified with the package name. The reason for both is that <span class=\"codefrag\">COUNT</span> is a <span class=\"codefrag\">builtin</span> function meaning that it comes with the Pig distribution. These are the only two differences between builtins and UDFs. Builtins are discussed in more detail later in this document. </p>  <h4 id=\"algebraic-interface\">Algebraic Interface</h4> <p>An aggregate function is an eval function that takes a bag and returns a scalar value. One interesting and useful property of many aggregate functions is that they can be computed incrementally in a distributed fashion. We call these functions <span class=\"codefrag\">algebraic</span>. <span class=\"codefrag\">COUNT</span> is an example of an algebraic function because we can count the number of elements in a subset of the data and then sum the counts to produce a final output. In the Hadoop world, this means that the partial computations can be done by the map and combiner, and the final result can be computed by the reducer. </p> <p>It is very important for performance to make sure that aggregate functions that are algebraic are implemented as such. Let's look at the implementation of the COUNT function to see what this means. (Error handling and some other code is omitted to save space. The full code can be accessed <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/builtin/COUNT.java?view=markup\"> here</a>.)</p> <pre class=\"code\">\npublic class COUNT extends EvalFunc&lt;Long&gt; implements Algebraic{\n    public Long exec(Tuple input) throws IOException {return count(input);}\n    public String getInitial() {return Initial.class.getName();}\n    public String getIntermed() {return Intermed.class.getName();}\n    public String getFinal() {return Final.class.getName();}\n    static public class Initial extends EvalFunc&lt;Tuple&gt; {\n        public Tuple exec(Tuple input) throws IOException {return TupleFactory.getInstance().newTuple(count(input));}\n    }\n    static public class Intermed extends EvalFunc&lt;Tuple&gt; {\n        public Tuple exec(Tuple input) throws IOException {return TupleFactory.getInstance().newTuple(sum(input));}\n    }\n    static public class Final extends EvalFunc&lt;Long&gt; {\n        public Tuple exec(Tuple input) throws IOException {return sum(input);}\n    }\n    static protected Long count(Tuple input) throws ExecException {\n        Object values = input.get(0);\n        if (values instanceof DataBag) return ((DataBag)values).size();\n        else if (values instanceof Map) return new Long(((Map)values).size());\n    }\n    static protected Long sum(Tuple input) throws ExecException, NumberFormatException {\n        DataBag values = (DataBag)input.get(0);\n        long sum = 0;\n        for (Iterator (Tuple) it = values.iterator(); it.hasNext();) {\n            Tuple t = it.next();\n            sum += (Long)t.get(0);\n        }\n        return sum;\n    }\n}\n</pre> <p> <span class=\"codefrag\">COUNT</span> implements <span class=\"codefrag\">Algebraic</span> interface which looks like this: </p> <pre class=\"code\">\npublic interface Algebraic{\n    public String getInitial();\n    public String getIntermed();\n    public String getFinal();\n}\n</pre> <p>For a function to be algebraic, it needs to implement <span class=\"codefrag\">Algebraic</span> interface that consist of definition of three classes derived from <span class=\"codefrag\">EvalFunc</span>. The contract is that the <span class=\"codefrag\">exec</span> function of the <span class=\"codefrag\">Initial</span> class is called once and is passed the original input tuple. Its output is a tuple that contains partial results. The <span class=\"codefrag\">exec</span> function of the <span class=\"codefrag\">Intermed</span> class can be called zero or more times and takes as its input a tuple that contains partial results produced by the <span class=\"codefrag\">Initial</span> class or by prior invocations of the <span class=\"codefrag\">Intermed</span> class and produces a tuple with another partial result. Finally, the <span class=\"codefrag\">exec</span> function of the <span class=\"codefrag\">Final</span> class is called and produces the final result as a scalar type. </p> <p>Here's the way to think about this in the Hadoop world. The <span class=\"codefrag\">exec</span> function of the <span class=\"codefrag\">Initial</span> class is invoked once for each input tuple by the <span class=\"codefrag\">map</span> process and produces partial results. The <span class=\"codefrag\">exec</span> function of the <span class=\"codefrag\">Intermed</span> class is invoked once by each <span class=\"codefrag\">combiner</span> invocation (which can happen zero or more times) and also produces partial results. The <span class=\"codefrag\">exec</span> function of the <span class=\"codefrag\">Final</span> class is invoked once by the reducer and produces the final result. </p> <p>Take a look at the <span class=\"codefrag\">COUNT</span> implementation to see how this is done. Note that the <span class=\"codefrag\">exec</span> function of the <span class=\"codefrag\">Initial</span> and <span class=\"codefrag\">Intermed</span> classes is parameterized with <span class=\"codefrag\">Tuple</span> and the <span class=\"codefrag\">exec</span> of the <span class=\"codefrag\">Final</span> class is parameterized with the real type of the function, which in the case of the <span class=\"codefrag\">COUNT</span> is <span class=\"codefrag\">Long</span>. Also, note that the fully-qualified name of the class needs to be returned from <span class=\"codefrag\">getInitial</span>, <span class=\"codefrag\">getIntermed</span>, and <span class=\"codefrag\">getFinal</span> methods. </p>  <h4 id=\"accumulator-interface\">Accumulator Interface</h4> <p>In Pig, problems with memory usage can occur when data, which results from a group or cogroup operation, needs to be placed in a bag and passed in its entirety to a UDF.</p> <p>This problem is partially addressed by Algebraic UDFs that use the combiner and can deal with data being passed to them incrementally during different processing phases (map, combiner, and reduce). However, there are a number of UDFs that are not Algebraic, don't use the combiner, but still don’t need to be given all data at once. </p> <p>The new Accumulator interface is designed to decrease memory usage by targeting such UDFs. For the functions that implement this interface, Pig guarantees that the data for the same key is passed continuously but in small increments. To work with incremental data, here is the interface a UDF needs to implement:</p> <pre class=\"code\">\npublic interface Accumulator &lt;T&gt; {\n   /**\n    * Process tuples. Each DataBag may contain 0 to many tuples for current key\n    */\n    public void accumulate(Tuple b) throws IOException;\n    /**\n     * Called when all tuples from current key have been passed to the accumulator.\n     * @return the value for the UDF for this key.\n     */\n    public T getValue();\n    /**\n     * Called after getValue() to prepare processing for next key. \n     */\n    public void cleanup();\n}\n</pre> <p>There are several things to note here:</p> <ol> <li>Each UDF must extend the EvalFunc class and implement all necessary functions there.</li> <li>If a function is algebraic but can be used in a FOREACH statement with accumulator functions, it needs to implement the Accumulator interface in addition to the Algebraic interface.</li> <li>The interface is parameterized with the return type of the function.</li> <li>The accumulate function is guaranteed to be called one or more times, passing one or more tuples in a bag, to the UDF. (Note that the tuple that is passed to the accumulator has the same content as the one passed to exec – all the parameters passed to the UDF – one of which should be a bag.)</li> <li>The getValue function is called after all the tuples for a particular key have been processed to retrieve the final value.</li> <li>The cleanup function is called after getValue but before the next value is processed.</li> </ol> <p>Here us a code snippet of the integer version of the MAX function that implements the interface:</p> <pre class=\"code\">\npublic class IntMax extends EvalFunc&lt;Integer&gt; implements Algebraic, Accumulator&lt;Integer&gt; {\n    …….\n    /* Accumulator interface */\n    \n    private Integer intermediateMax = null;\n    \n    @Override\n    public void accumulate(Tuple b) throws IOException {\n        try {\n            Integer curMax = max(b);\n            if (curMax == null) {\n                return;\n            }\n            /* if bag is not null, initialize intermediateMax to negative infinity */\n            if (intermediateMax == null) {\n                intermediateMax = Integer.MIN_VALUE;\n            }\n            intermediateMax = java.lang.Math.max(intermediateMax, curMax);\n        } catch (ExecException ee) {\n            throw ee;\n        } catch (Exception e) {\n            int errCode = 2106;\n            String msg = \"Error while computing max in \" + this.getClass().getSimpleName();\n            throw new ExecException(msg, errCode, PigException.BUG, e);           \n        }\n    }\n\n    @Override\n    public void cleanup() {\n        intermediateMax = null;\n    }\n\n    @Override\n    public Integer getValue() {\n        return intermediateMax;\n    }\n}\n</pre>  <h4 id=\"filter-functions\"> Filter Functions</h4> <p>Filter functions are eval functions that return a <span class=\"codefrag\">boolean</span> value. Filter functions can be used anywhere a Boolean expression is appropriate, including the <span class=\"codefrag\">FILTER</span> operator or <span class=\"codefrag\">bincond</span> expression. </p> <p>The example below uses the <span class=\"codefrag\">IsEmpy</span> builtin filter function to implement joins. </p> <pre class=\"code\">\n-- inner join\nA = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);\nB = LOAD 'voter_data' AS (name: chararray, age: int, registration: chararay, contributions: float);\nC = COGROUP A BY name, B BY name;\nD = FILTER C BY not IsEmpty(A);\nE = FILTER D BY not IsEmpty(B);\nF = FOREACH E GENERATE flatten(A), flatten(B);\nDUMP F;\n</pre> <p>Note that, even if filtering is omitted, the same results will be produced because the <span class=\"codefrag\">foreach</span> results is a cross product and cross products get rid of empty bags. However, doing up-front filtering is more efficient since it reduces the input of the cross product. </p> <pre class=\"code\">\n-- full outer join\nA = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);\nB = LOAD 'voter_data' AS (name: chararray, age: int, registration: chararay, contributions: float);\nC = COGROUP A BY name, B BY name;\nD = FOREACH C GENERATE group, flatten((IsEmpty(A) ? null : A)), flatten((IsEmpty(B) ? null : B));\ndump D;\n</pre> <p>The implementation of the <span class=\"codefrag\">IsEmpty</span> function looks like this: </p> <pre class=\"code\">\nimport java.io.IOException;\nimport java.util.Map;\n\nimport org.apache.pig.FilterFunc;\nimport org.apache.pig.PigException;\nimport org.apache.pig.backend.executionengine.ExecException;\nimport org.apache.pig.data.DataBag;\nimport org.apache.pig.data.Tuple;\nimport org.apache.pig.data.DataType;\n\n/**\n * Determine whether a bag or map is empty.\n */\npublic class IsEmpty extends FilterFunc {\n\n    @Override\n    public Boolean exec(Tuple input) throws IOException {\n        try {\n            Object values = input.get(0);\n            if (values instanceof DataBag)\n                return ((DataBag)values).size() == 0;\n            else if (values instanceof Map)\n                return ((Map)values).size() == 0;\n            else {\n                int errCode = 2102;\n                String msg = \"Cannot test a \" +\n                DataType.findTypeName(values) + \" for emptiness.\";\n                throw new ExecException(msg, errCode, PigException.BUG);\n            }\n        } catch (ExecException ee) {\n            throw ee;\n        }\n    }\n} \n</pre>  <h4 id=\"udf_simulation\">Implement UDF by Simulation</h4> <p>When implementing more advanced types of EvalFuncs, the simpler implementations can be automatically provided by Pig. Thus if your UDF implements <a href=\"#algebraic-interface\">Algebraic</a> then you will get the Accumulator interface and basic the basic EvalFunc exec method for free. Similarly, if your UDF implements <a href=\"#accumulator-interface\">Accumulator Interface</a> you will get the basic EvalFunc exec method for free. You will not get the Algebraic implemenation. Note that these free implementations are based on simulation, which might not be the most efficient. If you wish to ensure the efficiency of your Accumulator of EvalFunc exec method, you may still implement them yourself and your implementations will be used.</p>  <h4 id=\"pig-types\"> Pig Types and Native Java Types</h4> <p>The main thing to know about Pig's type system is that Pig uses native Java types for almost all of its types, as shown in this table. </p> <table class=\"ForrestTable\"> <tr> <th colspan=\"1\" rowspan=\"1\"> Pig Type </th> <th colspan=\"1\" rowspan=\"1\"> Java Class </th> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> DataByteArray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> String </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Integer </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Long </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Float </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Double </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Boolean </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> datetime </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> DateTime </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> BigDecimal </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> BigInteger </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Tuple </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> DataBag </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Map&lt;Object, Object&gt; </p> </td> </tr> </table> <p>All Pig-specific classes are available <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/data/\"> here</a>. </p> <p> <span class=\"codefrag\">Tuple</span> and <span class=\"codefrag\">DataBag</span> are different in that they are not concrete classes but rather interfaces. This enables users to extend Pig with their own versions of tuples and bags. As a result, UDFs cannot directly instantiate bags or tuples; they need to go through factory classes: <span class=\"codefrag\">TupleFactory</span> and <span class=\"codefrag\">BagFactory</span>. </p> <p>The builtin <span class=\"codefrag\">TOKENIZE</span> function shows how bags and tuples are created. A function takes a text string as input and returns a bag of words from the text. (Note that currently Pig bags always contain tuples.) </p> <pre class=\"code\">\npackage org.apache.pig.builtin;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\nimport org.apache.pig.EvalFunc;\nimport org.apache.pig.data.BagFactory;\nimport org.apache.pig.data.DataBag;\nimport org.apache.pig.data.Tuple;\nimport org.apache.pig.data.TupleFactory;\n\npublic class TOKENIZE extends EvalFunc&lt;DataBag&gt; {\n    TupleFactory mTupleFactory = TupleFactory.getInstance();\n    BagFactory mBagFactory = BagFactory.getInstance();\n\n    public DataBag exec(Tuple input) throws IOException \n        try {\n            DataBag output = mBagFactory.newDefaultBag();\n            Object o = input.get(0);\n            if (!(o instanceof String)) {\n                throw new IOException(\"Expected input to be chararray, but  got \" + o.getClass().getName());\n            }\n            StringTokenizer tok = new StringTokenizer((String)o, \" \\\",()*\", false);\n            while (tok.hasMoreTokens()) output.add(mTupleFactory.newTuple(tok.nextToken()));\n            return output;\n        } catch (ExecException ee) {\n            // error handling goes here\n        }\n    }\n}\n</pre>  <h4 id=\"schemas\">Schemas and Java UDFs</h4> <p>Pig uses type information for validation and performance. It is important for UDFs to participate in type propagation. Our UDFs generally make no effort to communicate their output schema to Pig. This is because Pig can usually figure out this information by using Java's <a href=\"http://www.oracle.com/technetwork/articles/java/javareflection-1536171.html\"> Reflection</a>. If your UDF returns a scalar or a map, no work is required. However, if your UDF returns a tuple or a bag (of tuples), it needs to help Pig figure out the structure of the tuple. </p> <p>If a UDF returns a tuple or a bag and schema information is not provided, Pig assumes that the tuple contains a single field of type bytearray. If this is not the case, then not specifying the schema can cause failures. We look at this next. </p> <p>Let's assume that we have UDF <span class=\"codefrag\">Swap</span> that, given a tuple with two fields, swaps their order. Let's assume that the UDF does not specify a schema and look at the scripts below: </p> <pre class=\"code\">\nregister myudfs.jar;\nA = load 'student_data' as (name: chararray, age: int, gpa: float);\nB = foreach A generate flatten(myudfs.Swap(name, age)), gpa;\nC = foreach B generate $2;\nD = limit B 20;\ndump D;\n</pre> <p>This script will result in the following error cause by line 4 ( <span class=\"codefrag\">C = foreach B generate $2;</span>). </p> <pre class=\"code\">\njava.io.IOException: Out of bound access. Trying to access non-existent column: 2. Schema {bytearray,gpa: float} has 2 column(s).\n</pre> <p>This is because Pig is only aware of two columns in B while line 4 is requesting the third column of the tuple. (Column indexing in Pig starts with 0.) </p> <p>The function, including the schema, looks like this: </p> <pre class=\"code\">\npackage myudfs;\nimport java.io.IOException;\nimport org.apache.pig.EvalFunc;\nimport org.apache.pig.data.Tuple;\nimport org.apache.pig.data.TupleFactory;\nimport org.apache.pig.impl.logicalLayer.schema.Schema;\nimport org.apache.pig.data.DataType;\n\npublic class Swap extends EvalFunc&lt;Tuple&gt; {\n    public Tuple exec(Tuple input) throws IOException {\n        if (input == null || input.size()   2\n            return null;\n        try{\n            Tuple output = TupleFactory.getInstance().newTuple(2);\n            output.set(0, input.get(1));\n            output.set(1, input.get(0));\n            return output;\n        } catch(Exception e){\n            System.err.println(\"Failed to process input; error - \" + e.getMessage());\n            return null;\n        }\n    }\n    public Schema outputSchema(Schema input) {\n        try{\n            Schema tupleSchema = new Schema();\n            tupleSchema.add(input.getField(1));\n            tupleSchema.add(input.getField(0));\n            return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input),tupleSchema, DataType.TUPLE));\n        }catch (Exception e){\n                return null;\n        }\n    }\n}\n</pre> <p>The function creates a schema with a single field (of type <span class=\"codefrag\">FieldSchema) of type tuple</span>. The name of the field is constructed using the <span class=\"codefrag\">getSchemaName</span> function of the <span class=\"codefrag\">EvalFunc</span> class. The name consists of the name of the UDF function, the first parameter passed to it, and a sequence number to guarantee uniqueness. In the previous script, if you replace <span class=\"codefrag\">dump D;</span> with <span class=\"codefrag\">describe B;</span> , you will see the following output: </p> <pre class=\"code\">\nB: {myudfs.swap_age_3::age: int,myudfs.swap_age_3::name: chararray,gpa: float}\n</pre> <p>The second parameter to the <span class=\"codefrag\">FieldSchema</span> constructor is the schema representing this field, which in this case is a tuple with two fields. The third parameter represents the type of the schema, which in this case is a <span class=\"codefrag\">TUPLE</span>. All supported schema types are defined in the <span class=\"codefrag\">org.apache.pig.data.DataType</span> class. </p> <pre class=\"code\">\npublic class DataType {\n    public static final byte UNKNOWN   =   0;\n    public static final byte NULL      =   1;\n    public static final byte BOOLEAN   =   5; // internal use only\n    public static final byte BYTE      =   6; // internal use only\n    public static final byte INTEGER   =  10;\n    public static final byte LONG      =  15;\n    public static final byte FLOAT     =  20;\n    public static final byte DOUBLE    =  25;\n    public static final byte DATETIME  =  30;\n    public static final byte BYTEARRAY =  50;\n    public static final byte CHARARRAY =  55;\n    public static final byte BIGINTEGER =  65;\n    public static final byte BIGDECIMAL =  70;\n    public static final byte MAP       = 100;\n    public static final byte TUPLE     = 110;\n    public static final byte BAG       = 120;\n    public static final byte ERROR     =  -1;\n    // more code here\n}\n</pre> <p>You need to import the <span class=\"codefrag\">org.apache.pig.data.DataType</span> class into your code to define schemas. You also need to import the schema class <span class=\"codefrag\">org.apache.pig.impl.logicalLayer.schema.Schema</span>. </p> <p>The example above shows how to create an output schema for a tuple. Doing this for a bag is very similar. Let's extend the <span class=\"codefrag\">TOKENIZE</span> function to do that.</p> <p>As you can see in the example below, this is very similar to the output schema definition in the <span class=\"codefrag\">Swap</span> function. One difference is that instead of reusing input schema, we create a brand new field schema to represent the tokens stored in the bag. The other difference is that the type of the schema created is BAG (not TUPLE). </p> <pre class=\"code\">\npackage org.apache.pig.builtin;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\nimport org.apache.pig.EvalFunc;\nimport org.apache.pig.data.BagFactory;\nimport org.apache.pig.data.DataBag;\nimport org.apache.pig.data.Tuple;\nimport org.apache.pig.data.TupleFactory;\nimport org.apache.pig.impl.logicalLayer.schema.Schema;\nimport org.apache.pig.data.DataType;\n\npublic class TOKENIZE extends EvalFunc&lt;DataBag&gt; {\n    TupleFactory mTupleFactory = TupleFactory.getInstance();\n    BagFactory mBagFactory = BagFactory.getInstance();\n    public DataBag exec(Tuple input) throws IOException {\n        try {\n            DataBag output = mBagFactory.newDefaultBag();\n            Object o = input.get(0);\n            if ((o instanceof String)) {\n                throw new IOException(\"Expected input to be chararray, but  got \" + o.getClass().getName());\n            }\n            StringTokenizer tok = new StringTokenizer((String)o, \" \\\",()*\", false);\n            while (tok.hasMoreTokens()) output.add(mTupleFactory.newTuple(tok.nextToken()));\n            return output;\n        } catch (ExecException ee) {\n            // error handling goes here\n        }\n    }\n    public Schema outputSchema(Schema input) {\n         try{\n             Schema bagSchema = new Schema();\n             bagSchema.add(new Schema.FieldSchema(\"token\", DataType.CHARARRAY));\n\n             return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input),\n                                                    bagSchema, DataType.BAG));\n         }catch (Exception e){\n            return null;\n         }\n    }\n}\n</pre>  <p>One more note about schemas and UDFs. Users have requested the ability to examine the input schema of the data before processing the data via a UDF. For example, they would like to know how to convert an input tuple to a map such that the keys in the map are the names of the input columns. Currently there is no way to do this. This is a feature we would like to support in the future.</p>  <h4 id=\"error-handling\"> Error Handling</h4> <p>There are several types of errors that can occur in a UDF: </p> <ol> <li> <p>An error that affects a particular row but is not likely to impact other rows. An example of such an error would be a malformed input value or divide by zero problem. A reasonable handling of this situation would be to emit a warning and return a null value. <span class=\"codefrag\">ABS</span> function in the next section demonstrates this approach. The current approach is to write the warning to <span class=\"codefrag\">stderr</span>. Eventually we would like to pass a logger to the UDFs. Note that returning a NULL value only makes sense if the malformed value is of type <span class=\"codefrag\">bytearray</span>. Otherwise the proper type has been already created and should have an appropriate value. If this is not the case, it is an internal error and should cause the system to fail. Both cases can be seen in the implementation of the <span class=\"codefrag\">ABS</span> function in the next section. </p> </li> <li> <p>An error that affects the entire processing but can succeed on retry. An example of such a failure is the inability to open a lookup file because the file could not be found. This could be a temporary environmental issue that can go away on retry. A UDF can signal this to Pig by throwing an <span class=\"codefrag\">IOException</span> as with the case of the <span class=\"codefrag\">ABS</span> function below. </p> </li> <li> <p>An error that affects the entire processing and is not likely to succeed on retry. An example of such a failure is the inability to open a lookup file because of file permission problems. Pig currently does not have a way to handle this case. Hadoop does not have a way to handle this case either. It will be handled the same way as 2 above. </p> </li> </ol>  <h4 id=\"function-overloading\">Function Overloading</h4> <p>Before the type system was available in Pig, all values for the purpose of arithmetic calculations were assumed to be doubles as the safest choice. However, this is not very efficient if the data is actually of type integer or long. (We saw about a 2x slowdown of a query when using double where integer could be used.) Now that Pig supports types we can take advantage of the type information and choose the function that is most efficient for the provided operands. </p> <p>UDF writers are encouraged to provide type-specific versions of a function if this can result in better performance. On the other hand, we don't want the users of the functions to worry about different functions - the right thing should just happen. Pig allows for this via a function table mechanism as shown in the next example. </p> <p>This example shows the implementation of the <span class=\"codefrag\">ABS</span> function that returns the absolute value of a numeric value passed to it as input. </p> <pre class=\"code\">\nimport java.io.IOException;\nimport java.util.List;\nimport java.util.ArrayList;\nimport org.apache.pig.EvalFunc;\nimport org.apache.pig.FuncSpec;\nimport org.apache.pig.data.Tuple;\nimport org.apache.pig.impl.logicalLayer.FrontendException;\nimport org.apache.pig.impl.logicalLayer.schema.Schema;\nimport org.apache.pig.data.DataType;\n\npublic class ABS extends EvalFunc&lt;Double&gt; {\n    public Double exec(Tuple input) throws IOException {\n        if (input == null || input.size() == 0)\n            return null;\n        Double d;\n        try{\n            d = DataType.toDouble(input.get(0));\n        } catch (NumberFormatException nfe){\n            System.err.println(\"Failed to process input; error - \" + nfe.getMessage());\n            return null;\n        } catch (Exception e){\n            throw new IOException(\"Caught exception processing input row \", e);\n        }\n        return Math.abs(d);\n    }\n    public List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {\n        List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt;();\n        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.BYTEARRAY))));\n        funcList.add(new FuncSpec(DoubleAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.DOUBLE))));\n        funcList.add(new FuncSpec(FloatAbs.class.getName(),   new Schema(new Schema.FieldSchema(null, DataType.FLOAT))));\n        funcList.add(new FuncSpec(IntAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.INTEGER))));\n        funcList.add(new FuncSpec(LongAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.LONG))));\n        return funcList;\n    }\n}\n</pre> <p>The main thing to notice in this example is the <span class=\"codefrag\">getArgToFuncMapping()</span> method. This method returns a list that contains a mapping from the input schema to the class that should be used to handle it. In this example the main class handles the <span class=\"codefrag\">bytearray</span> input and outsources the rest of the work to other classes implemented in separate files in the same package. The example of one such class is below. This class handles integer input values. </p> <pre class=\"code\">\nimport java.io.IOException;\nimport org.apache.pig.EvalFunc;\nimport org.apache.pig.data.Tuple;\n\npublic class IntAbs extends EvalFunc&lt;Integer&gt; {\n    public Integer exec(Tuple input) throws IOException {\n        if (input == null || input.size() == 0)\n            return null;\n        Integer d;\n        try{\n            d = (Integer)input.get(0);\n        } catch (Exception e){\n            throw new IOException(\"Caught exception processing input row \", e);\n        }\n        return Math.abs(d);\n    }\n}\n</pre> <p>A note on error handling. The <span class=\"codefrag\">ABS</span> class covers the case of the <span class=\"codefrag\">bytearray</span> which means the data has not been converted yet to its actual type. This is why a null value is returned when <span class=\"codefrag\">NumberFormatException</span> is encountered. However, the <span class=\"codefrag\">IntAbs</span> function is only called if the data is already of type <span class=\"codefrag\">Integer</span> which means it has already been converted to the real type and bad format has been dealt with. This is why an exception is thrown if the input can't be cast to <span class=\"codefrag\">Integer</span>. </p> <p>The example above covers a reasonably simple case where the UDF only takes one parameter and there is a separate function for each parameter type. However, this will not always be the case. If Pig can't find an <span class=\"codefrag\">exact match</span> it tries to do a <span class=\"codefrag\">best match</span>. The rule for the best match is to find the most efficient function that can be used safely. This means that Pig must find the function that, for each input parameter, provides the smallest type that is equal to or greater than the input type. The type progression rules are: <span class=\"codefrag\">int&gt;long&gt;float&gt;double</span>. </p> <p>For instance, let's consider function <span class=\"codefrag\">MAX</span> which is part of the <span class=\"codefrag\">piggybank</span> described later in this document. Given two values, the function returns the larger value. The function table for <span class=\"codefrag\">MAX</span> looks like this: </p> <pre class=\"code\">\npublic List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {\n    List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt;();\n    Util.addToFunctionList(funcList, IntMax.class.getName(), DataType.INTEGER);\n    Util.addToFunctionList(funcList, DoubleMax.class.getName(), DataType.DOUBLE);\n    Util.addToFunctionList(funcList, FloatMax.class.getName(), DataType.FLOAT);\n    Util.addToFunctionList(funcList, LongMax.class.getName(), DataType.LONG);\n\n    return funcList;\n}\n</pre> <p>The <span class=\"codefrag\">Util.addToFunctionList</span> function is a helper function that adds an entry to the list as the first argument, with the key of the class name passed as the second argument, and the schema containing two fields of the same type as the third argument. </p> <p>Let's now see how this function can be used in a Pig script: </p> <pre class=\"code\">\nREGISTER piggybank.jar\nA = LOAD 'student_data' AS (name: chararray, gpa1: float, gpa2: double);\nB = FOREACH A GENERATE name, org.apache.pig.piggybank.evaluation.math.MAX(gpa1, gpa2);\nDUMP B;\n</pre> <p>In this example, the function gets one parameter of type <span class=\"codefrag\">float</span> and another of type <span class=\"codefrag\">double</span>. The best fit will be the function that takes two double values. Pig makes this choice on the user's behalf by inserting implicit casts for the parameters. Running the script above is equivalent to running the script below: </p> <pre class=\"code\">\nA = LOAD 'student_data' AS (name: chararray, gpa1: float, gpa2: double);\nB = FOREACH A GENERATE name, org.apache.pig.piggybank.evaluation.math.MAX((double)gpa1, gpa2);\nDUMP B;\n</pre> <p>A special case of the <span class=\"codefrag\">best fit</span> approach is handling data without a schema specified. The type for this data is interpreted as <span class=\"codefrag\">bytearray</span>. Since the type of the data is not known, there is no way to choose a best fit version. The only time a cast is performed is when the function table contains only a single entry. This works well to maintain backward compatibility. </p> <p>Let's revisit the <span class=\"codefrag\">UPPER</span> function from our first example. As it is written now, it would only work if the data passed to it is of type <span class=\"codefrag\">chararray</span>. To make it work with data whose type is not explicitly set, a function table with a single entry needs to be added: </p> <pre class=\"code\">\npackage myudfs;\nimport java.io.IOException;\nimport org.apache.pig.EvalFunc;\nimport org.apache.pig.data.Tuple;\n\npublic class UPPER extends EvalFunc&lt;String&gt;\n{\n    public String exec(Tuple input) throws IOException {\n        if (input == null || input.size() == 0)\n            return null;\n        try{\n            String str = (String)input.get(0);\n            return str.toUpperCase();\n        }catch(Exception e){\n            System.err.println(\"WARN: UPPER: failed to process input; error - \" + e.getMessage());\n            return null;\n        }\n    }\n    public List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {\n        List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt;();\n        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY))));\n        return funcList;\n    }\n}\n</pre> <p>Now the following script will ran: </p> <pre class=\"code\">\n-- this is myscript.pig\nREGISTER myudfs.jar;\nA = LOAD 'student_data' AS (name, age, gpa);\nB = FOREACH A GENERATE myudfs.UPPER(name);\nDUMP B;\n</pre> <p>Variable-length arguments: </p> <p>The last input schema field in <span class=\"codefrag\">getArgToFuncMapping()</span> can be marked as vararg, which enables UDF writers to create UDFs that take variable length arguments. This is done by overriding the <span class=\"codefrag\">getSchemaType()</span> method: </p> <pre class=\"code\">\n@Override\npublic SchemaType getSchemaType() {\n    return SchemaType.VARARG;\n}\n</pre> <p>For an example see <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/builtin/CONCAT.java?view=markup\">CONCAT</a>.</p>  <h4 id=\"counters\">Using Counters</h4> <p>Hadoop counters are easily accessible within EvalFunc by using PigStatusReporter object. Here is one example:</p> <pre class=\"code\">\npublic class UPPER extends EvalFunc&lt;String&gt;\n{\n        public String exec(Tuple input) throws IOException {\n                if (input == null || input.size() == 0) {\n                    PigStatusReporter reporter = PigStatusReporter.getInstance();\n                    if (reporter != null) {\n                       reporter.incrCounter(PigWarning.UDF_WARNING_1, 1);\n                    }\n                    return null;\n                }\n                try{\n                        String str = (String)input.get(0);\n                        return str.toUpperCase();\n                }catch(Exception e){\n                    throw new IOException(\"Caught exception processing input row \", e);\n                }\n        }\n}\n</pre>  <h4 id=\"access-input-schema\">Access input schema inside EvalFunc</h4> <p>Not only inside outputSchema at compile time, input schema is also accessible in exec at runtime. For example:</p> <pre class=\"code\">\npublic class AddSchema extends EvalFunc&lt;String&gt;\n{\n        public String exec(Tuple input) throws IOException {\n                if (input == null || input.size() == 0)\n                    return null;\n                String result = \"\";\n                for (int i=0;i&lt;input.size();i++) {\n                    result += getInputSchema().getFields().get(i).alias;\n                    result += \":\";\n                    result += input.get(i);\n                }\n                return result;\n        }\n}\n</pre>  <h4 id=\"reporting-progress\">Reporting Progress</h4> <p>A challenge of running a large shared system is to make sure system resources are used efficiently. One aspect of this challenge is detecting runaway processes that are no longer making progress. Pig uses a heartbeat mechanism for this purpose. If any of the tasks stops sending a heartbeat, the system assumes that it is dead and kills it. </p> <p>Most of the time, single-tuple processing within a UDF is very short and does not require a UDF to heartbeat. The same is true for aggregate functions that operate on large bags because bag iteration code takes care of it. However, if you have a function that performs a complex computation that can take an order of minutes to execute, you should add a progress indicator to your code. This is very easy to accomplish. The <span class=\"codefrag\">EvalFunc</span> class provides a <span class=\"codefrag\">progress</span> function that you need to call in your <span class=\"codefrag\">exec</span> method. </p> <p>For instance, the <span class=\"codefrag\">UPPER</span> function would now look as follows: </p> <pre class=\"code\">\npublic class UPPER extends EvalFunc&lt;String&gt;\n{\n        public String exec(Tuple input) throws IOException {\n                if (input == null || input.size() == 0)\n                return null;\n                try{\n                        progress();\n                        String str = (String)input.get(0);\n                        return str.toUpperCase();\n                }catch(Exception e){\n                    throw new IOException(\"Caught exception processing input row \", e);\n                }\n        }\n}\n</pre>  <h4 id=\"distributed-cache\">Using Distributed Cache</h4> <p>Use getCacheFiles or getShipFiles to return a list of HDFS files or local files that need to be shipped to distributed cache. Inside exec method, you can assume that these files already exist in distributed cache. For example:</p> <pre class=\"code\">\npublic class Udfcachetest extends EvalFunc&lt;String&gt; { \n\n    public String exec(Tuple input) throws IOException { \n        String concatResult = \"\";\n        FileReader fr = new FileReader(\"./smallfile1\"); \n        BufferedReader d = new BufferedReader(fr);\n        concatResult +=d.readLine();\n        fr = new FileReader(\"./smallfile2\");\n        d = new BufferedReader(fr);\n        concatResult +=d.readLine();\n        return concatResult;\n    } \n\n    public List&lt;String&gt; getCacheFiles() { \n        List&lt;String&gt; list = new ArrayList&lt;String&gt;(1); \n        list.add(\"/user/pig/tests/data/small#smallfile1\");  // This is hdfs file\n        return list; \n    } \n\n    public List&lt;String&gt; getShipFiles() {\n        List&lt;String&gt; list = new ArrayList&lt;String&gt;(1);\n        list.add(\"/home/hadoop/pig/smallfile2\");  // This local file\n        return list;\n    }\n} \n\na = load '1.txt'; \nb = foreach a generate Udfcachetest(*); \ndump b;\n</pre>  <h4 id=\"compile-time-eval\">Compile time evaluation</h4> <p>If the parameters of the EvalFunc are all constants, Pig could evaluate the result at compile time. The benefit of evaluating at compile time is performance optimization, and enable certain other optimizations at front end (such as partition pruning, which only allow constant not UDF in filter condition). By default, compile time evaluation is disabled in EvalFunc to prevent potential side effect. To enable it, override allowCompileTimeCalculation. For example:</p> <pre class=\"code\">\npublic class CurrentTime extends EvalFunc&lt;DateTime&gt; {\n    public String exec(Tuple input) throws IOException {\n        return new DateTime();\n    }\n    @Override\n    public boolean allowCompileTimeCalculation() {\n        return true;\n    }\n}\n</pre>  <h4 id=\"tez-jvm-reuse\">Clean up static variable in Tez</h4> <p>In Tez, jvm could reuse for other tasks. It is important to cleanup static variable to make sure there is no side effect. Here is one example:</p> <pre class=\"code\">\npublic class UPPER extends EvalFunc&lt;String&gt;\n{\n        static boolean initialized = false;\n        static {\n            JVMReuseManager.getInstance().registerForStaticDataCleanup(UPPER.class);\n        }\n        public String exec(Tuple input) throws IOException {\n            if (!initialized) {\n                init();\n                initialized = true;\n            }\n            ......\n        }\n        @StaticDataCleanup\n        public static void staticDataCleanup() {\n            initialized = false;\n        }\n}\n</pre>  <h3 id=\"load-store-functions\"> Load/Store Functions</h3> <p>The load/store UDFs control how data goes into Pig and comes out of Pig. Often, the same function handles both input and output but that does not have to be the case. </p> <p> The Pig load/store API is aligned with Hadoop's InputFormat and OutputFormat classes. This enables you to create new LoadFunc and StoreFunc implementations based on existing Hadoop InputFormat and OutputFormat classes with minimal code. The complexity of reading the data and creating a record lies in the InputFormat while the complexity of writing the data lies in the OutputFormat. This enables Pig to easily read/write data in new storage formats as and when an Hadoop InputFormat and OutputFormat is available for them. </p> <p> <strong>Note:</strong> Both the LoadFunc and StoreFunc implementations should use the Hadoop 20 API based classes (InputFormat/OutputFormat and related classes) under the <strong>new</strong> org.apache.hadoop.mapreduce package instead of the old org.apache.hadoop.mapred package. </p>  <h4 id=\"load-functions\"> Load Functions</h4>  <p id=\"loadfunc\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadFunc.java?view=markup\">LoadFunc</a> abstract class has three main methods for loading data and for most use cases it would suffice to extend it. There are three other optional interfaces which can be implemented to achieve extended functionality: </p> <ul>  <li id=\"LoadMetadata\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadMetadata.java?view=markup\">LoadMetadata</a> has methods to deal with metadata - most implementation of loaders don't need to implement this unless they interact with some metadata system. The getSchema() method in this interface provides a way for loader implementations to communicate the schema of the data back to pig. If a loader implementation returns data comprised of fields of real types (rather than DataByteArray fields), it should provide the schema describing the data returned through the getSchema() method. The other methods are concerned with other types of metadata like partition keys and statistics. Implementations can return null return values for these methods if they are not applicable for that implementation.</li>  <li id=\"LoadPushDown\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadPushDown.java?view=markup\">LoadPushDown</a> has methods to push operations from Pig runtime into loader implementations. Currently only the pushProjection() method is called by Pig to communicate to the loader the exact fields that are required in the Pig script. The loader implementation can choose to honor the request (return only those fields required by Pig script) or not honor the request (return all fields in the data). If the loader implementation can efficiently honor the request, it should implement LoadPushDown to improve query performance. (Irrespective of whether the implementation can or cannot honor the request, if the implementation also implements getSchema(), the schema returned in getSchema() should describe the entire tuple of data.) <ul>  <li id=\"pushprojection\">pushProjection(): This method tells LoadFunc which fields are required in the Pig script, thus enabling LoadFunc to optimize performance by loading only those fields that are needed. pushProjection() takes a requiredFieldList. requiredFieldList is read only and cannot be changed by LoadFunc. requiredFieldList includes a list of requiredField: each requiredField indicates a field required by the Pig script; each requiredField includes index, alias, type (which is reserved for future use), and subFields. Pig will use the column index requiredField.index to communicate with the LoadFunc about the fields required by the Pig script. If the required field is a map, Pig will optionally pass requiredField.subFields which contains a list of keys that the Pig script needs for the map. For example, if the Pig script needs two keys for the map, \"key1\" and \"key2\", the subFields for that map will contain two requiredField; the alias field for the first RequiredField will be \"key1\" and the alias for the second requiredField will be \"key2\". LoadFunc will use requiredFieldResponse.requiredFieldRequestHonored to indicate whether the pushProjection() request is honored. </li> </ul> </li>  <li id=\"loadcaster\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadCaster.java?view=markup\">LoadCaster</a> has methods to convert byte arrays to specific types. A loader implementation should implement this if casts (implicit or explicit) from DataByteArray fields to other types need to be supported. </li>  <li id=\"loadpredicatepushdown\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadPredicatePushdown.java?view=markup\">LoadPredicatePushdown</a> has the methods to push predicates to the loader. It is different than LoadMetadata.setPartitionFilter in that loader may load records which does not satisfy the predicates. In other words, predicates is only a hint. Note this interface is still in development and might change in next version. Currently only OrcStorage implements this interface.</li> </ul> <p>The LoadFunc abstract class is the main class to extend for implementing a loader. The methods which need to be overridden are explained below:</p> <ul>  <li id=\"getInputFormat\">getInputFormat(): This method is called by Pig to get the InputFormat used by the loader. The methods in the InputFormat (and underlying RecordReader) are called by Pig in the same manner (and in the same context) as by Hadoop in a MapReduce java program. If the InputFormat is a Hadoop packaged one, the implementation should use the new API based one under org.apache.hadoop.mapreduce. If it is a custom InputFormat, it should be implemented using the new API in org.apache.hadoop.mapreduce.<br> <br> If a custom loader using a text-based InputFormat or a file-based InputFormat would like to read files in all subdirectories under a given input directory recursively, then it should use the PigTextInputFormat and PigFileInputFormat classes provided in org.apache.pig.backend.hadoop.executionengine.mapReduceLayer. The Pig InputFormat classes work around a current limitation in the Hadoop TextInputFormat and FileInputFormat classes which only read one level down from the provided input directory. For example, if the input in the load statement is 'dir1' and there are subdirs 'dir2' and 'dir2/dir3' beneath dir1, the Hadoop TextInputFormat and FileInputFormat classes read the files under 'dir1' only. Using PigTextInputFormat or PigFileInputFormat (or by extending them), the files in all the directories can be read. </li>  <li id=\"setLocation\">setLocation(): This method is called by Pig to communicate the load location to the loader. The loader should use this method to communicate the same information to the underlying InputFormat. This method is called multiple times by pig - implementations should bear this in mind and should ensure there are no inconsistent side effects due to the multiple calls. </li>  <li id=\"prepareToRead\">prepareToRead(): Through this method the RecordReader associated with the InputFormat provided by the LoadFunc is passed to the LoadFunc. The RecordReader can then be used by the implementation in getNext() to return a tuple representing a record of data back to pig. </li>  <li id=\"getnext\">getNext(): The meaning of getNext() has not changed and is called by Pig runtime to get the next tuple in the data - in this method the implementation should use the underlying RecordReader and construct the tuple to return. </li> </ul>  <p id=\"loadfunc-default\">The following methods have default implementations in LoadFunc and should be overridden only if needed: </p> <ul>  <li id=\"setUdfContextSignature\">setUdfContextSignature(): This method will be called by Pig both in the front end and back end to pass a unique signature to the Loader. The signature can be used to store into the UDFContext any information which the Loader needs to store between various method invocations in the front end and back end. A use case is to store RequiredFieldList passed to it in LoadPushDown.pushProjection(RequiredFieldList) for use in the back end before returning tuples in getNext(). The default implementation in LoadFunc has an empty body. This method will be called before other methods. </li>  <li id=\"relativeToAbsolutePath\">relativeToAbsolutePath(): Pig runtime will call this method to allow the Loader to convert a relative load location to an absolute location. The default implementation provided in LoadFunc handles this for FileSystem locations. If the load source is something else, loader implementation may choose to override this.</li>  <li id=\"getCacheFiles\">getCacheFiles(): Return a list of hdfs files to ship to distributed cache.</li>  <li id=\"getShipFiles\">getShipFiles(): Return a list of local files to ship to distributed cache.</li> </ul> <p> <strong>Example Implementation</strong> </p> <p> The loader implementation in the example is a loader for text data with line delimiter as '\\n' and '\\t' as default field delimiter (which can be overridden by passing a different field delimiter in the constructor) - this is similar to current PigStorage loader in Pig. The implementation uses an existing Hadoop supported Inputformat - TextInputFormat - as the underlying InputFormat. </p> <pre class=\"code\">\npublic class SimpleTextLoader extends LoadFunc {\n    protected RecordReader in = null;\n    private byte fieldDel = '\\t';\n    private ArrayList&lt;Object&gt; mProtoTuple = null;\n    private TupleFactory mTupleFactory = TupleFactory.getInstance();\n    private static final int BUFFER_SIZE = 1024;\n\n    public SimpleTextLoader() {\n    }\n\n    /**\n     * Constructs a Pig loader that uses specified character as a field delimiter.\n     *\n     * @param delimiter\n     *            the single byte character that is used to separate fields.\n     *            (\"\\t\" is the default.)\n     */\n    public SimpleTextLoader(String delimiter) {\n        this();\n        if (delimiter.length() == 1) {\n            this.fieldDel = (byte)delimiter.charAt(0);\n        } else if (delimiter.length() &gt;  1 &amp; &amp; delimiter.charAt(0) == '\\\\') {\n            switch (delimiter.charAt(1)) {\n            case 't':\n                this.fieldDel = (byte)'\\t';\n                break;\n\n            case 'x':\n               fieldDel =\n                    Integer.valueOf(delimiter.substring(2), 16).byteValue();\n               break;\n\n            case 'u':\n                this.fieldDel =\n                    Integer.valueOf(delimiter.substring(2)).byteValue();\n                break;\n\n            default:\n                throw new RuntimeException(\"Unknown delimiter \" + delimiter);\n            }\n        } else {\n            throw new RuntimeException(\"PigStorage delimeter must be a single character\");\n        }\n    }\n\n    @Override\n    public Tuple getNext() throws IOException {\n        try {\n            boolean notDone = in.nextKeyValue();\n            if (notDone) {\n                return null;\n            }\n            Text value = (Text) in.getCurrentValue();\n            byte[] buf = value.getBytes();\n            int len = value.getLength();\n            int start = 0;\n\n            for (int i = 0; i &lt; len; i++) {\n                if (buf[i] == fieldDel) {\n                    readField(buf, start, i);\n                    start = i + 1;\n                }\n            }\n            // pick up the last field\n            readField(buf, start, len);\n\n            Tuple t =  mTupleFactory.newTupleNoCopy(mProtoTuple);\n            mProtoTuple = null;\n            return t;\n        } catch (InterruptedException e) {\n            int errCode = 6018;\n            String errMsg = \"Error while reading input\";\n            throw new ExecException(errMsg, errCode,\n                    PigException.REMOTE_ENVIRONMENT, e);\n        }\n\n    }\n\n    private void readField(byte[] buf, int start, int end) {\n        if (mProtoTuple == null) {\n            mProtoTuple = new ArrayList&lt;Object&gt;();\n        }\n\n        if (start == end) {\n            // NULL value\n            mProtoTuple.add(null);\n        } else {\n            mProtoTuple.add(new DataByteArray(buf, start, end));\n        }\n    }\n\n    @Override\n    public InputFormat getInputFormat() {\n        return new TextInputFormat();\n    }\n\n    @Override\n    public void prepareToRead(RecordReader reader, PigSplit split) {\n        in = reader;\n    }\n\n    @Override\n    public void setLocation(String location, Job job)\n            throws IOException {\n        FileInputFormat.setInputPaths(job, location);\n    }\n}\n</pre>  <h4 id=\"store-functions\"> Store Functions</h4>  <p id=\"storefunc\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/StoreFunc.java?view=markup\">StoreFunc</a> abstract class has the main methods for storing data and for most use cases it should suffice to extend it. There is an optional interface which can be implemented to achieve extended functionality: </p> <ul>  <li id=\"storemetadata\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/StoreMetadata.java?view=markup\">StoreMetadata:</a> This interface has methods to interact with metadata systems to store schema and store statistics. This interface is optional and should only be implemented if metadata needs to stored. </li>  <li id=\"storeresources\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/StoreResources.java?view=markup\">StoreResources:</a> This interface has methods to put hdfs files or local files to distributed cache. </li>  <li id=\"errorhandling\"> <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/ErrorHandling.java?view=markup\">ErrorHandling:</a> This interface allow you to skip bad records in the storer so the storer will not throw exception and terminate the job. You can implement your own error handler by overriding <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/ErrorHandler.java?view=markup\">ErrorHandler</a> interface, or use predefined error handler: <a href=\"http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/CounterBasedErrorHandler.java?view=markup\">CounterBasedErrorHandler</a>. ErrorHandling can be turned on by setting the property pig.error-handling.enabled to true in pig.properties. Default is false. CounterBasedErrorHandler uses two settings - pig.error-handling.min.error.records (the minimum number of errors to trigger error handling) and pig.error-handling.error.threshold (percentage of the number of records as a fraction exceeding which error is thrown).</li> </ul>  <p id=\"storefunc-override\">The methods which need to be overridden in StoreFunc are explained below: </p> <ul>  <li id=\"getOutputFormat\">getOutputFormat(): This method will be called by Pig to get the OutputFormat used by the storer. The methods in the OutputFormat (and underlying RecordWriter and OutputCommitter) will be called by pig in the same manner (and in the same context) as by Hadoop in a map-reduce java program. If the OutputFormat is a hadoop packaged one, the implementation should use the new API based one under org.apache.hadoop.mapreduce. If it is a custom OutputFormat, it should be implemented using the new API under org.apache.hadoop.mapreduce. The checkOutputSpecs() method of the OutputFormat will be called by pig to check the output location up-front. This method will also be called as part of the Hadoop call sequence when the job is launched. So implementations should ensure that this method can be called multiple times without inconsistent side effects. </li>  <li id=\"setStoreLocation\">setStoreLocation(): This method is called by Pig to communicate the store location to the storer. The storer should use this method to communicate the same information to the underlying OutputFormat. This method is called multiple times by pig - implementations should bear in mind that this method is called multiple times and should ensure there are no inconsistent side effects due to the multiple calls. </li>  <li id=\"prepareToWrite\">prepareToWrite(): Writing of the data is through the OutputFormat provided by the StoreFunc. In prepareToWrite() the RecordWriter associated with the OutputFormat provided by the StoreFunc is passed to the StoreFunc. The RecordWriter can then be used by the implementation in putNext() to write a tuple representing a record of data in a manner expected by the RecordWriter. </li>  <li id=\"putNext\">putNext(): This method is called by Pig runtime to write the next tuple of data - this is the method wherein the implementation will use the underlying RecordWriter to write the Tuple out.</li> </ul>  <p id=\"storefunc-default\">The following methods have default implementations in StoreFunc and should be overridden only if necessary: </p> <ul>  <li id=\"setStoreFuncUDFContextSignature\">setStoreFuncUDFContextSignature(): This method will be called by Pig both in the front end and back end to pass a unique signature to the Storer. The signature can be used to store into the UDFContext any information which the Storer needs to store between various method invocations in the front end and back end. The default implementation in StoreFunc has an empty body. This method will be called before other methods. </li>  <li id=\"relToAbsPathForStoreLocation\">relToAbsPathForStoreLocation(): Pig runtime will call this method to allow the Storer to convert a relative store location to an absolute location. An implementation is provided in StoreFunc which handles this for FileSystem based locations. </li>  <li id=\"checkschema\">checkSchema(): A Store function should implement this function to check that a given schema describing the data to be written is acceptable to it. The default implementation in StoreFunc has an empty body. This method will be called before any calls to setStoreLocation(). </li> </ul> <p> <strong>Example Implementation</strong> </p> <p> The storer implementation in the example is a storer for text data with line delimiter as '\\n' and '\\t' as default field delimiter (which can be overridden by passing a different field delimiter in the constructor) - this is similar to current PigStorage storer in Pig. The implementation uses an existing Hadoop supported OutputFormat - TextOutputFormat as the underlying OutputFormat. </p> <pre class=\"code\">\npublic class SimpleTextStorer extends StoreFunc {\n    protected RecordWriter writer = null;\n\n    private byte fieldDel = '\\t';\n    private static final int BUFFER_SIZE = 1024;\n    private static final String UTF8 = \"UTF-8\";\n    public PigStorage() {\n    }\n\n    public PigStorage(String delimiter) {\n        this();\n        if (delimiter.length() == 1) {\n            this.fieldDel = (byte)delimiter.charAt(0);\n        } else if (delimiter.length() &gt; 1delimiter.charAt(0) == '\\\\') {\n            switch (delimiter.charAt(1)) {\n            case 't':\n                this.fieldDel = (byte)'\\t';\n                break;\n\n            case 'x':\n               fieldDel =\n                    Integer.valueOf(delimiter.substring(2), 16).byteValue();\n               break;\n            case 'u':\n                this.fieldDel =\n                    Integer.valueOf(delimiter.substring(2)).byteValue();\n                break;\n\n            default:\n                throw new RuntimeException(\"Unknown delimiter \" + delimiter);\n            }\n        } else {\n            throw new RuntimeException(\"PigStorage delimeter must be a single character\");\n        }\n    }\n\n    ByteArrayOutputStream mOut = new ByteArrayOutputStream(BUFFER_SIZE);\n\n    @Override\n    public void putNext(Tuple f) throws IOException {\n        int sz = f.size();\n        for (int i = 0; i &lt; sz; i++) {\n            Object field;\n            try {\n                field = f.get(i);\n            } catch (ExecException ee) {\n                throw ee;\n            }\n\n            putField(field);\n\n            if (i != sz - 1) {\n                mOut.write(fieldDel);\n            }\n        }\n        Text text = new Text(mOut.toByteArray());\n        try {\n            writer.write(null, text);\n            mOut.reset();\n        } catch (InterruptedException e) {\n            throw new IOException(e);\n        }\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void putField(Object field) throws IOException {\n        //string constants for each delimiter\n        String tupleBeginDelim = \"(\";\n        String tupleEndDelim = \")\";\n        String bagBeginDelim = \"{\";\n        String bagEndDelim = \"}\";\n        String mapBeginDelim = \"[\";\n        String mapEndDelim = \"]\";\n        String fieldDelim = \",\";\n        String mapKeyValueDelim = \"#\";\n\n        switch (DataType.findType(field)) {\n        case DataType.NULL:\n            break; // just leave it empty\n\n        case DataType.BOOLEAN:\n            mOut.write(((Boolean)field).toString().getBytes());\n            break;\n\n        case DataType.INTEGER:\n            mOut.write(((Integer)field).toString().getBytes());\n            break;\n\n        case DataType.LONG:\n            mOut.write(((Long)field).toString().getBytes());\n            break;\n\n        case DataType.FLOAT:\n            mOut.write(((Float)field).toString().getBytes());\n            break;\n\n        case DataType.DOUBLE:\n            mOut.write(((Double)field).toString().getBytes());\n            break;\n\n        case DataType.BYTEARRAY: {\n            byte[] b = ((DataByteArray)field).get();\n            mOut.write(b, 0, b.length);\n            break;\n                                 }\n\n        case DataType.CHARARRAY:\n            // oddly enough, writeBytes writes a string\n            mOut.write(((String)field).getBytes(UTF8));\n            break;\n\n        case DataType.MAP:\n            boolean mapHasNext = false;\n            Map&lt;String, Object&gt; m = (Map&lt;String, Object&gt;)field;\n            mOut.write(mapBeginDelim.getBytes(UTF8));\n            for(Map.Entry&lt;String, Object&gt; e: m.entrySet()) {\n                if(mapHasNext) {\n                    mOut.write(fieldDelim.getBytes(UTF8));\n                } else {\n                    mapHasNext = true;\n                }\n                putField(e.getKey());\n                mOut.write(mapKeyValueDelim.getBytes(UTF8));\n                putField(e.getValue());\n            }\n            mOut.write(mapEndDelim.getBytes(UTF8));\n            break;\n\n        case DataType.TUPLE:\n            boolean tupleHasNext = false;\n            Tuple t = (Tuple)field;\n            mOut.write(tupleBeginDelim.getBytes(UTF8));\n            for(int i = 0; i &lt; t.size(); ++i) {\n                if(tupleHasNext) {\n                    mOut.write(fieldDelim.getBytes(UTF8));\n                } else {\n                    tupleHasNext = true;\n                }\n                try {\n                    putField(t.get(i));\n                } catch (ExecException ee) {\n                    throw ee;\n                }\n            }\n            mOut.write(tupleEndDelim.getBytes(UTF8));\n            break;\n\n        case DataType.BAG:\n            boolean bagHasNext = false;\n            mOut.write(bagBeginDelim.getBytes(UTF8));\n            Iterator&lt;Tuple&gt; tupleIter = ((DataBag)field).iterator();\n            while(tupleIter.hasNext()) {\n                if(bagHasNext) {\n                    mOut.write(fieldDelim.getBytes(UTF8));\n                } else {\n                    bagHasNext = true;\n                }\n                putField((Object)tupleIter.next());\n            }\n            mOut.write(bagEndDelim.getBytes(UTF8));\n            break;\n\n        default: {\n            int errCode = 2108;\n            String msg = \"Could not determine data type of field: \" + field;\n            throw new ExecException(msg, errCode, PigException.BUG);\n        }\n\n        }\n    }\n\n    @Override\n    public OutputFormat getOutputFormat() {\n        return new TextOutputFormat&lt;WritableComparable, Text&gt;();\n    }\n\n    @Override\n    public void prepareToWrite(RecordWriter writer) {\n        this.writer = writer;\n    }\n\n    @Override\n    public void setStoreLocation(String location, Job job) throws IOException {\n        job.getConfiguration().set(\"mapred.textoutputformat.separator\", \"\");\n        FileOutputFormat.setOutputPath(job, new Path(location));\n        if (location.endsWith(\".bz2\")) {\n            FileOutputFormat.setCompressOutput(job, true);\n            FileOutputFormat.setOutputCompressorClass(job,  BZip2Codec.class);\n        }  else if (location.endsWith(\".gz\")) {\n            FileOutputFormat.setCompressOutput(job, true);\n            FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n        }\n    }\n}\n</pre>  <h3 id=\"use-short-names\">Using Short Names</h3> <p>There are two ways to call a Java UDF using a short name. One way is specifying the package to an import list via Java property, and the other is defining an alias of the UDF by DEFINE statement.</p>  <h4 id=\"import-lists\">Import Lists</h4> <p>An import list allows you to specify the package to which a UDF or a group of UDFs belong, eliminating the need to qualify the UDF on every call. An import list can be specified via the udf.import.list Java property on the Pig command line: </p> <pre class=\"code\">\npig -Dudf.import.list=com.yahoo.yst.sds.ULT\n</pre> <p>You can supply multiple locations as well: </p> <pre class=\"code\">\npig -Dudf.import.list=com.yahoo.yst.sds.ULT:org.apache.pig.piggybank.evaluation\n</pre> <p>To make use of import scripts, do the following:</p> <pre class=\"code\">\nmyscript.pig:\nA = load '/data/SDS/data/searcg_US/20090820' using ULTLoader as (s, m, l);\n....\n\ncommand:\npig -cp sds.jar -Dudf.import.list=com.yahoo.yst.sds.ULT myscript.pig \n</pre>  <h4 id=\"define-alias\">Defining Aliases</h4> <p>You can define an alias for a function using <a href=\"basic#define-udfs\">DEFINE statement</a>:</p> <pre class=\"code\">\nREGISTER piggybank.jar\nDEFINE MAXNUM org.apache.pig.piggybank.evaluation.math.MAX;\nA = LOAD 'student_data' AS (name: chararray, gpa1: float, gpa2: double);\nB = FOREACH A GENERATE name, MAXNUM(gpa1, gpa2);\nDUMP B;\n</pre> <p>The first parameter of DEFINE statement is an alias of the function. The second parameter is the fully-qualified name of the function. After the statement, you can call the function using the alias instead of the fually-qualified name.</p>  <h3 id=\"Advanced+Topics\">Advanced Topics</h3>  <h4 id=\"udf-interfaces\">UDF Interfaces</h4> <p>Java UDFs can be invoked multiple ways. The simplest UDF can just extend EvalFunc, which requires only the exec function to be implemented (see <a href=\"#eval-functions-write\"> How to Write a Simple Eval Function</a>). Every eval UDF must implement this. Additionally, if a function is algebraic, it can implement <span class=\"codefrag\">Algebraic</span> interface to significantly improve query performance in the cases when combiner can be used (see <a href=\"#algebraic-interface\">Algebraic Interface</a>). Finally, a function that can process tuples in an incremental fashion can also implement the Accumulator interface to improve query memory consumption (see <a href=\"#accumulator-interface\">Accumulator Interface</a>). </p> <p>The optimizer selects the exact method by which a UDF is invoked based on the UDF type and the query. Note that only a single interface is used at any given time. The optimizer tries to find the most efficient way to execute the function. If a combiner is used and the function implements the Algebraic interface then this interface will be used to invoke the function. If the combiner is not invoked but the accumulator can be used and the function implements Accumulator interface then that interface is used. If neither of the conditions is satisfied then the exec function is used to invoke the UDF. </p>  <h4 id=\"function-instantiation\">Function Instantiation</h4> <p>One problem that users run into is when they make assumption about how many times a constructor for their UDF is called. For instance, they might be creating side files in the store function and doing it in the constructor seems like a good idea. The problem with this approach is that in most cases Pig instantiates functions on the client side to, for instance, examine the schema of the data. </p> <p>Users should not make assumptions about how many times a function is instantiated; instead, they should make their code resilient to multiple instantiations. For instance, they could check if the files exist before creating them. </p>  <h4 id=\"udf-configurations\">Passing Configurations to UDFs</h4> <p>The singleton UDFContext class provides two features to UDF writers. First, on the backend, it allows UDFs to get access to the JobConf object, by calling getJobConf. This is only available on the backend (at run time) as the JobConf has not yet been constructed on the front end (during planning time).</p> <p>Second, it allows UDFs to pass configuration information between instantiations of the UDF on the front and backends. UDFs can store information in a configuration object when they are constructed on the front end, or during other front end calls such as checkSchema. They can then read that information on the backend when exec (for EvalFunc) or getNext (for LoadFunc) is called. Note that information will not be passed between instantiations of the function on the backend. The communication channel only works from front end to back end.</p> <p>To store information, the UDF calls getUDFProperties. This returns a Properties object which the UDF can record the information in or read the information from. To avoid name space conflicts UDFs are required to provide a signature when obtaining a Properties object. This can be done in two ways. The UDF can provide its Class object (via this.getClass()). In this case, every instantiation of the UDF will be given the same Properties object. The UDF can also provide its Class plus an array of Strings. The UDF can pass its constructor arguments, or some other identifying strings. This allows each instantiation of the UDF to have a different properties object thus avoiding name space collisions between instantiations of the UDF.</p>  <h4 id=\"udf-monitoring\">Monitoring Long-Running UDFs</h4> <p>Sometimes one may discover that a UDF that executes very quickly in the vast majority of cases turns out to run exceedingly slowly on occasion. This can happen, for example, if a UDF uses complex regular expressions to parse free-form strings, or if a UDF uses some external service to communicate with. As of version 0.8, Pig provides a facility for monitoring the length of time a UDF is executing for every invocation, and terminating its execution if it runs too long. This facility can be turned on using a simple Java annotation:</p> <pre class=\"code\">\n\timport org.apache.pig.builtin.MonitoredUDF;\n\t\n\t@MonitoredUDF\n\tpublic class MyUDF extends EvalFunc&lt;Integer&gt; {\n\t  /* implementation goes here */\n\t}\n</pre> <p>Simply annotating your UDF in this way will cause Pig to terminate the UDF's exec() method if it runs for more than 10 seconds, and return the default value of null. The duration of the timeout and the default value can be specified in the annotation, if desired:</p> <pre class=\"code\">\n\timport org.apache.pig.builtin.MonitoredUDF;\n\t\n\t@MonitoredUDF(timeUnit = TimeUnit.MILLISECONDS, duration = 100, intDefault = 10)\n\tpublic class MyUDF extends EvalFunc&lt;Integer&gt; {\n\t  /* implementation goes here */\n\t}\n</pre> <p>intDefault, longDefault, doubleDefault, floatDefault, and stringDefault can be specified in the annotation; the correct default will be chosen based on the return type of the UDF. Custom defaults for tuples and bags are not supported at this time.</p> <p>If desired, custom logic can also be implemented for error handling by creating a subclass of MonitoredUDFExecutor.ErrorCallback, and overriding its handleError and/or handleTimeout methods. Both of those methods are static, and are passed in the instance of the EvalFunc that produced an exception, as well as an exception, so you may use any state you have in the UDF to process the errors as desired. The default behavior is to increment Hadoop counters every time an error is encountered. Once you have an implementation of the ErrorCallback that performs your custom logic, you can provide it in the annotation:</p> <pre class=\"code\">\n\timport org.apache.pig.builtin.MonitoredUDF;\n\n\t@MonitoredUDF(errorCallback=MySpecialErrorCallback.class)\n\tpublic class MyUDF extends EvalFunc&lt;Integer&gt; {\n\t  /* implementation goes here */\n\t}\n</pre> <p>Currently the MonitoredUDF annotation works with regular and Algebraic UDFs, but has no effect on UDFs that run in the Accumulator mode.</p> </div>   <h2 id=\"jython-udfs\">Writing Jython UDFs</h2> <div class=\"section\">  <h3 id=\"register-jython\">Registering the UDF</h3> <p>You can register a Jython script as shown here. This example uses org.apache.pig.scripting.jython.JythonScriptEngine to interpret the Jython script. You can develop and use custom script engines to support multiple programming languages and ways to interpret them. Currently, Pig identifies jython as a keyword and ships the required scriptengine (jython) to interpret it. </p> <pre class=\"code\">\nRegister 'test.py' using jython as myfuncs;\n</pre> <p>The following syntax is also supported, where myfuncs is the namespace created for all the functions inside test.py.</p> <pre class=\"code\">\nregister 'test.py' using org.apache.pig.scripting.jython.JythonScriptEngine as myfuncs;\n</pre> <p>A typical test.py looks like this:</p> <pre class=\"code\">\n@outputSchema(\"word:chararray\")\ndef helloworld():  \n  return 'Hello, World'\n\n@outputSchema(\"word:chararray,num:long\")\ndef complex(word):\n  return str(word),len(word)\n\n@outputSchemaFunction(\"squareSchema\")\ndef square(num):\n  return ((num)*(num))\n\n@schemaFunction(\"squareSchema\")\ndef squareSchema(input):\n  return input\n\n# No decorator - bytearray\ndef concat(str):\n  return str+str\n</pre> <p>The register statement above registers the Jython functions defined in test.py in Pig’s runtime within the defined namespace (myfuncs here). They can then be referred later on in the pig script as myfuncs.helloworld(), myfuncs.complex(), and myfuncs.square(). An example usage is:</p> <pre class=\"code\">\nb = foreach a generate myfuncs.helloworld(), myfuncs.square(3);\n</pre>  <h3 id=\"python_decorators\">Decorators and Schemas</h3> <p>To annotate a Jython script so that Pig can identify return types, use Jython decorators to define output schema for the script UDF. </p> <ul>  <li id=\"outputschema\">outputSchema - Defines schema for a script UDF in a format that Pig understands and is able to parse. </li>  <li id=\"outputfunctionschema\">outputFunctionSchema - Defines a script delegate function that defines schema for this function depending upon the input type. This is needed for functions that can accept generic types and perform generic operations on these types. A simple example is square which can accept multiple types. SchemaFunction for this type is a simple identity function (same schema as input). </li>  <li id=\"schemafunction\">schemaFunction - Defines delegate function and is not registered to Pig. </li> </ul> <p>When no decorator is specified, Pig assumes the output datatype as bytearray and converts the output generated by script function to bytearray. This is consistent with Pig's behavior in case of Java UDFs. </p> <p>Sample Schema String - y:{t:(word:chararray,num:long)}, variable names inside a schema string are not used anywhere, they just make the syntax identifiable to the parser. </p>  <h3 id=\"Example+Scripts\">Example Scripts</h3> <p>Simple tasks like string manipulation, mathematical computations, and reorganizing data types can be easily done using Jython scripts without having to develop long and complex UDFs in Java. The overall overhead of using scripting language is much less and development cost is almost negligible. The following UDFs, developed in Jython, can be used with Pig. </p> <pre class=\"code\">\n mySampleLib.py\n ---------------------\n #/usr/bin/python\n \n ##################\n # Math functions #\n ##################\n #square - Square of a number of any data type\n @outputSchemaFunction(\"squareSchema\")\n def square(num):\n   return ((num)*(num))\n @schemaFunction(\"squareSchema\")\n def squareSchema(input):\n   return input\n \n #Percent- Percentage\n @outputSchema(\"percent:double\")\n def percent(num, total):\n   return num * 100 / total\n \n ####################\n # String Functions #\n ####################\n #commaFormat- format a number with commas, 12345-&gt; 12,345\n @outputSchema(\"numformat:chararray\")\n def commaFormat(num):\n   return '{:,}'.format(num)\n \n #concatMultiple- concat multiple words\n @outputSchema(\"onestring:chararray\")\n def concatMult4(word1, word2, word3, word4):\n   return word1 word2 word3 word4\n \n #######################\n # Data Type Functions #\n #######################\n #collectBag- collect elements of a bag into other bag\n #This is useful UDF after group operation\n @outputSchema(\"y:bag{t:tuple(len:int,word:chararray)}\") \n def collectBag(bag):\n   outBag = []\n   for word in bag:\n     tup=(len(bag), word[1])\n     outBag.append(tup)\n   return outBag\n \n # Few comments- \n # Pig mandates that a bag should be a bag of tuples, Jython UDFs should follow this pattern.\n # Tuples in Jython are immutable, appending to a tuple is not possible.\n </pre>  <h3 id=\"jython-advanced\">Advanced Topics</h3>  <h4 id=\"import-modules\">Importing Modules</h4> <p>You can import Jython modules in your Jython script. Pig resolves Jython dependencies recursively, which means Pig will automatically ship all dependent Jython modules to the backend. Jython modules should be found in the jython search path: JYTHON_HOME, JYTHONPATH, or current directory.</p>  <h4 id=\"combined-scripts\">Combined Scripts</h4> <p>UDFs and Pig scripts are generally stored in separate files. For testing purposes you can combine the code in a single file - a \"combined\" script. Note, however, if you then decide to embed this \"combined\" script in a host language, the language of the UDF must match the host language.</p> <p>This example combines Jython and Pig. This \"combined\" script can only be embedded in Jython.</p> <p>With Jython you MUST use the <span class=\"codefrag\">if __name__ == '__main__': </span> construct to separate UDFs and control flow. Otherwise the script will result in an error.</p> <pre class=\"code\">\n #!/usr/bin/jython\nfrom org.apache.pig.scripting import *\n\n@outputSchema(\"word:chararray\")\ndef helloworld():  \n   return 'Hello, World'\n  \nif __name__ == '__main__':\n       P = Pig.compile(\"\"\"a = load '1.txt' as (a0, a1);\n                          b = foreach a generate helloworld();\n                          store b into 'myoutput'; \"\"\")\n\nresult = P.bind().runSingle();\n </pre> </div>   <h2 id=\"js-udfs\">Writing JavaScript UDFs</h2> <div class=\"section\"> <p> <strong>Note:</strong> <em>JavaScript UDFs are an experimental feature.</em> </p>  <h3 id=\"register-js\">Registering the UDF</h3> <p>You can register JavaScript as shown here. This example uses org.apache.pig.scripting.js.JsScriptEngine to interpret JavaScript. You can develop and use custom script engines to support multiple programming languages and ways to interpret them. Currently, Pig identifies js as a keyword and ships the required scriptengine (Rhino) to interpret it. </p> <pre class=\"code\">\n register 'test.js' using javascript as myfuncs;\n </pre> <p>The following syntax is also supported, where myfuncs is the namespace created for all the functions inside test.js.</p> <pre class=\"code\">\n register 'test.js' using org.apache.pig.scripting.js.JsScriptEngine as myfuncs;\n </pre> <p>The register statement above registers the js functions defined in test.js in Pig’s runtime within the defined namespace (myfuncs here). They can then be referred later on in the pig script as myfuncs.helloworld(), myfuncs.complex(), and myfuncs.square(). An example usage is:</p> <pre class=\"code\">\n b = foreach a generate myfuncs.helloworld(), myfuncs.complex($0);\n </pre>  <h3 id=\"schema-return-types\">Return Types and Schemas</h3> <p>Since JavaScript functions are first class objects, you can annotate them by adding attributes. Add an outputSchema attribute to your function so that Pig can identify return types for the script UDF. </p> <ul> <li>outputSchema - Defines schema for a script udf in a format that Pig understands and is able to parse. </li> <li>Sample Schema String - y:{t:(word:chararray,num:long)} <br>Variable names inside a schema string are used for type conversion between Pig and JavaScript. Tuples are converted to Objects using the names and vice versa</li> </ul>  <h3 id=\"js-example\">Example Scripts</h3> <p>A simple JavaScript UDF (udf.js) is shown here.</p> <pre class=\"code\">\nhelloworld.outputSchema = \"word:chararray\";\nfunction helloworld() {\n    return 'Hello, World';\n}\n    \ncomplex.outputSchema = \"(word:chararray,num:long)\";\nfunction complex(word){\n    return {word:word, num:word.length};\n}\n</pre> <p>This Pig script registers the JavaScript UDF (udf.js).</p> <pre class=\"code\">\nregister 'udf.js' using javascript as myfuncs; \nA = load 'data' as (a0:chararray, a1:int);\nB = foreach A generate myfuncs.helloworld(), myfuncs.complex(a0);\n... ... \n</pre>  <h3 id=\"js-advanced\">Advanced Topics</h3> <p>UDFs and Pig scripts are generally stored in separate files. For testing purposes you can combine the code in a single file - a \"combined\" script. Note, however, if you then decide to embed this \"combined\" script in a host language, the language of the UDF and the host language must match.</p> <p>This example combines JavaScript and Pig. This \"combined\" script can only be embedded in JavaScript.</p> <p>With JavaScript, the control flow MUST be defined in the main function. Otherwise the script will result in an error.</p> <pre class=\"code\">\nimportPackage(Packages.org.apache.pig.scripting.js)\npig = org.apache.pig.scripting.js.JSPig;\n\nhelloworld.outputSchema = \"word:chararray\" \nfunction helloworld() { \n    return 'Hello, World'; \n}\n\nfunction main() {\n  var P = pig.compile(\" a = load '1.txt' as (a0, a1);”+\n       “b = foreach a generate helloworld();”+\n           “store b into 'myoutput';\");\n\n  var result = P.bind().runSingle();\n}\n </pre> </div>   <h2 id=\"jruby-udfs\">Writing Ruby UDFs</h2> <div class=\"section\"> <p> <strong>Note:</strong> <em>Ruby UDFs are an experimental feature.</em> </p>  <h3 id=\"write-jruby\">Writing a Ruby UDF</h3> <p>You must extend PigUdf and define your Ruby UDFs in the class. </p> <pre class=\"code\">\n require 'pigudf'\n class Myudfs &lt; PigUdf\n     def square num\n         return nil if num.nil?\n         num**2\n     end\n end\n </pre>  <h3 id=\"jruby-schema-return-types\">Return Types and Schemas</h3> <p>You have two ways to define the return schema:</p> <p>outputSchema - Defines the schema for a UDF in a format that Pig understands. </p> <pre class=\"code\">\n outputSchema \"word:chararray\"\n </pre> <pre class=\"code\">\n outputSchema \"t:(m:[], t:(name:chararray, age:int, gpa:double), b:{t:(name:chararray, age:int, gpa:double)})\"\n </pre> <p>Schema function</p> <pre class=\"code\">\n outputSchemaFunction :squareSchema\n def squareSchema input\n     input\n end\n </pre> <p>You need to put outputSchema/outputSchemaFunction statement right before your UDF. The schema function itself can be defined anywhere inside the class.</p>  <h3 id=\"register-jruby\">Registering the UDF</h3> <p>You can register a Ruby UDF as shown here. </p> <pre class=\"code\">\n register 'test.rb' using jruby as myfuncs;\n </pre> <p> This is a shortcut to the complete syntax:</p> <pre class=\"code\">\n register 'test.rb' using org.apache.pig.scripting.jruby.JrubyScriptEngine as myfuncs;\n </pre> <p>The <span class=\"codefrag\">register</span> statement above registers the Ruby functions defined in test.rb in Pig’s runtime within the defined namespace (myfuncs in this example). They can then be referred later on in the Pig Latin script as <span class=\"codefrag\">myfuncs.square()</span>. An example usage is:</p> <pre class=\"code\">\n b = foreach a generate myfuncs.concat($0, $1);\n </pre>  <h3 id=\"jruby-example\">Example Scripts</h3> <p>Here are two complete Ruby UDF samples.</p> <pre class=\"code\">\n require 'pigudf'\n class Myudfs &lt; PigUdf\n outputSchema \"word:chararray\"\n     def concat *input\n         input.inject(:+)\n     end\n end\n </pre> <pre class=\"code\">\n require 'pigudf'\n class Myudfs &lt; PigUdf\n outputSchemaFunction :squareSchema\n     def square num\n         return nil if num.nil?\n         num**2\n     end\n     def squareSchema input\n         input\n     end\n end\n </pre>  <h3 id=\"jruby-advanced\">Advanced Topics</h3> <p> You can also write Algebraic and Accumulator UDFs using Ruby. You need to extend your class from <span class=\"codefrag\">AlgebraicPigUdf</span> and <span class=\"codefrag\">AccumulatorPigUdf</span> respectively. For an Algebraic UDF, define <span class=\"codefrag\">initial</span>, <span class=\"codefrag\">intermed</span>, and <span class=\"codefrag\">final</span> methods in the class. For an Accumulator UDF, define <span class=\"codefrag\">exec</span> and <span class=\"codefrag\">get</span> methods in the class. Below are example for each type of UDF: </p> <pre class=\"code\">\n class Count &lt; AlgebraicPigUdf\n     output_schema Schema.long\n     def initial t\n          t.nil? ? 0 : 1\n     end\n     def intermed t\n          return 0 if t.nil?\n          t.flatten.inject(:+)\n     end\n     def final t\n         intermed(t)\n     end\n end\n </pre> <pre class=\"code\">\n class Sum &lt; AccumulatorPigUdf\n     output_schema { |i| i.in.in[0] }\n     def exec items\n         @sum ||= 0\n         @sum += items.flatten.inject(:+)\n     end\n     def get\n         @sum\n     end\n end\n </pre> </div>   <h2 id=\"groovy-udfs\">Writing Groovy UDFs</h2> <div class=\"section\"> <p> <strong>Note:</strong> <em>Groovy UDFs are an experimental feature.</em> </p>  <h3 id=\"register-groovy\">Registering the UDF</h3> <p>You can register a Groovy script as shown here. This example uses org.apache.pig.scripting.groovy.GroovyScriptEngine to interpret the Groovy script. You can develop and use custom script engines to support multiple programming languages and ways to interpret them. Currently, Pig identifies groovy as a keyword and ships the required scriptengine (groovy-all) to interpret it. </p> <pre class=\"code\">\nRegister 'test.groovy' using groovy as myfuncs;\n</pre> <p>The following syntax is also supported, where myfuncs is the namespace created for all the functions inside test.groovy.</p> <pre class=\"code\">\nregister 'test.groovy' using org.apache.pig.scripting.groovy.GroovyScriptEngine as myfuncs;\n</pre> <p>A registered script can contain multiple UDFs. UDFs can be static or instance methods, an instance of the enclosing class will be created as needed. Only methods for which a return schema is defined will be available in Pig.</p>  <h3 id=\"groovy-schema-return-types\">Return Types and Schemas</h3> <p>You have two ways to define the return schema, both use annotations:</p> <p>@OutputSchema annotation - Defines the schema for a UDF in a format that Pig understands.</p> <pre class=\"code\">\nimport org.apache.pig.builtin.OutputSchema;\n\nclass GroovyUDFs {\n  @OutputSchema('x:long')\n  long square(long x) {\n    return x*x;\n  }\n}\n</pre> <pre class=\"code\">\noutputSchema \"t:(m:[], t:(name:chararray, age:int, gpa:double), b:{t:(name:chararray, age:int, gpa:double)})\"\n</pre> <p>@OutputSchemaFunction annotation - Defines the name of a function which will return the schema at runtime according to the input schema.</p> <pre class=\"code\">\nimport org.apache.pig.scripting.groovy.OutputSchemaFunction;\n\nclass GroovyUDFs {\n  @OutputSchemaFunction('squareSchema')\n  public static square(x) {\n    return x * x;\n  }\n\n  public static squareSchema(input) {          \n    return input;\n  }\n}        \n\n</pre> <p>Only methods annotated with either @OutputSchema or @OutputSchemaFunction will be exposed to Pig as UDFs. In the example above, squareSchema will not be available in Pig as a UDF.</p>  <h3 id=\"groovy-type-conversions\">Type Conversions</h3> <p>The data passed back and forth between Pig and Groovy goes through a conversion process. The following conversions rules are applied:</p> <p>Pig to Groovy</p> <ul> <li>Tuple: groovy.lang.Tuple</li> <li>DataBag: groovy.lang.Tuple containing the bag's size and an iterator on its content</li> <li>org.joda.time.DateTime: org.joda.time.DateTime</li> <li>Map: java.util.Map</li> <li>int/long/float/double: as is</li> <li>chararray: String</li> <li>bytearray: byte[] (content is copied)</li> <li>boolean: boolean</li> <li>biginteger: BigInteger</li> <li>bigdecimal: BigDecimal</li> <li>null: null </li> </ul> <p>Anything else raises an exception</p> <p>Groovy to Pig</p> <ul> <li>Object[]: Tuple</li> <li>groovy.lang.Tuple: Tuple</li> <li>org.apache.pig.data.Tuple: Tuple</li> <li>org.apache.pig.data.DataBag: DataBag</li> <li>org.joda.time.DateTime: org.joda.time.DateTime</li> <li>java.util.Map: Map</li> <li>java.util.List: DataBag</li> <li>Byte/Short/Integer: int</li> <li>Long: long</li> <li>Float: float</li> <li>Double: double</li> <li>String: chararray</li> <li>byte[]: DataByteArray (content is copied)</li> <li>Boolean: boolean</li> <li>BigInteger: biginteger</li> <li>BigDecimal: bigdecimal</li> <li>null: null</li> </ul> <p>Anything else raises an exception</p>  <h3 id=\"groovy-advanced\">Advanced Topics</h3> <p>You can also write Algebraic and Accumulator UDFs using Groovy. Both types of UDFs are declared using annotations, a single Groovy file can therefore enclose several Algebraic/Accumulator UDFs, all mixed with regular UDFs.</p> <p>Algebraic UDFs are declared using three annotations, @AlgebraicInitial, @AlgebraicIntermed and @AlgebraicFinal which are to annotate methods that correspond to the initial, intermed and final steps of an Algebraic UDF. Those annotations have a single parameter which is the name of the Algebraic UDF that will be available in Pig. The methods annotated with @AlgebraicInitial and @AlgebraicIntermed accept a Tuple as parameter and return a Tuple. The return type of the method annotated with @AlgebraicFinal will determine the return type of the Algebraic UDF. Here is an example of an Algebraic UDF named 'sum' defined in Groovy:</p> <pre class=\"code\">\nimport org.apache.pig.scripting.groovy.AlgebraicInitial;\nimport org.apache.pig.scripting.groovy.AlgebraicIntermed;\nimport org.apache.pig.scripting.groovy.AlgebraicFinal;\n\nclass GroovyUDFs {\n  @AlgebraicFinal('sum')\n  public static long algFinal(Tuple t) {\n    long x = 0;\n    for (Object o: t[1]) {\n      x = x + o;\n    }\n    return x;\n  }\n  @AlgebraicInitial('sum')\n  public static Tuple algInitial(Tuple t) {\n    long x = 0;\n    for (Object o: t[1]) {\n      x = x + o[0];\n    }\n    return [x];\n  }\n  @AlgebraicIntermed('sum')\n  public static Tuple algIntermed(Tuple t) {\n    long x = 0;\n    for (Object o: t[1]) {\n      x = x + o;\n    }\n    return [x];\n  }\n}\n </pre> <p>Similarly, Accumulator UDFs are declared using the three annotations @AccumulatorAccumulate, @AccumulatorGetValue and @AccumulatorCleanup which are to annotate methods that correspond to the methods accumulate, getValue and cleanup of a Java Accumulator UDF. Those annotations have a single parameter which is the name of the Accumulator UDF that will be available in Pig. The methods annotated with @AccumulatorAccumulate and @AccumulatorCleanup return void. The methods annotated with @AccumulatorGetValue and @AccumulatorCleanup take no parameters. The method annotated with @AccumulatorAccumulate takes a Tuple as parameter. The return schema of the Accumulator UDF is determined by the @OutputSchema or @OutputSchemaFunction annotation used on the method annotated with @AccumulatorGetValue. Note that even though a method annotated with @AccumulatorGetValue has an @OutputSchema or @OutputSchemaFunction annotation, it will not be available in Pig, only the Accumulator UDF to which it belongs will.</p> <p>Since Accumulator UDFs maintain state, the methods annotated with the @AccumulatorXXX annotations cannot be static. A single instance of the enclosing class will be used when invoking them, thus enabling them to access a single state.</p> <p>The following example defines an Accumulator UDF named 'sumacc':</p> <pre class=\"code\">\nimport org.apache.pig.builtin.OutputSchema;\nimport org.apache.pig.scripting.groovy.AccumulatorAccumulate;\nimport org.apache.pig.scripting.groovy.AccumulatorGetValue;\nimport org.apache.pig.scripting.groovy.AccumulatorCleanup;\n\nclass GroovyUDFs {\n  private int sum = 0;\n  @AccumulatorAccumulate('sumacc')\n  public void accuAccumulate(Tuple t) {\n    for (Object o: t[1]) {\n      sum += o[0]\n    }\n  }\n  @AccumulatorGetValue('sumacc')\n  @OutputSchema('sum: long')\n  public long accuGetValue() {\n    return this.sum;\n  }\n  @AccumulatorCleanup('sumacc')\n  public void accuCleanup() {\n    this.sum = 0L;\n  }\n}  \n </pre> </div>   <h2 id=\"python-udfs\">Writing Python UDFs</h2> <div class=\"section\"> <p>Here Python UDFs means C Python UDFs. It uses python command line to run the Python UDFs. It is different than Jython, which relies on Jython library. Instead, it streams the data in and out of the python process. The implementation mechanism is completely different than Jython.</p>  <h3 id=\"register-python\">Registering the UDF</h3> <p>You can register a Python script as shown here. </p> <pre class=\"code\">\nRegister 'test.py' using streaming_python as myfuncs;\n</pre> <p>The following syntax is also supported, where myfuncs is the namespace created for all the functions inside test.py.</p> <pre class=\"code\">\nregister 'test.py' using org.apache.pig.scripting.streaming.python.PythonScriptEngine as myfuncs;\n</pre> <p>A typical test.py looks like this:</p> <pre class=\"code\">\nfrom pig_util import outputSchema\n\n@outputSchema(\"as:int\")\ndef square(num):\n    if num == None:\n        return None\n    return ((num) * (num))\n\n@outputSchema(\"word:chararray\")\ndef concat(word):\n    return word + word\n</pre> <p>The register statement above registers the Python functions defined in test.py in Pig’s runtime within the defined namespace (myfuncs here). They can then be referred later on in the pig script as myfuncs.square(), myfuncs.concat(). An example usage is:</p> <pre class=\"code\">\nb = foreach a generate myfuncs.concat('hello', 'world'), myfuncs.square(3);\n</pre>  <h3 id=\"decorators\">Decorators and Schemas</h3> <p>To annotate a Python script so that Pig can identify return types, use Python decorators to define output schema for the script UDF. </p> <ul> <li>outputSchema - Defines schema for a script UDF in a format that Pig understands and is able to parse. </li> </ul> <p>When no decorator is specified, Pig assumes the output datatype as bytearray and converts the output generated by script function to bytearray. This is consistent with Pig's behavior in case of Java UDFs. </p> <p>Sample Schema String - words:{(word:chararray)}, variable names inside a schema string are not used anywhere, they just make the syntax identifiable to the parser. </p> </div>    <h2 id=\"piggybank\">Piggy Bank</h2> <div class=\"section\"> <p>Piggy Bank is a place for Pig users to share the Java UDFs they have written for use with Pig. The functions are contributed \"as-is.\" If you find a bug in a function, take the time to fix it and contribute the fix to Piggy Bank. If you don't find the UDF you need, take the time to write and contribute the function to Piggy Bank. </p> <p> <strong>Note:</strong> Piggy Bank currently supports Java UDFs. Support for Jython and JavaScript UDFs will be added at a later date.</p>  <h3 id=\"piggbank-access\">Accessing Functions</h3> <p>The Piggy Bank functions are currently distributed in source form. Users are required to checkout the code and build the package themselves. No binary distributions or nightly builds are available at this time. </p> <p>To build a jar file that contains all available UDFs, follow these steps: </p> <ul> <li>Checkout UDF code: <span class=\"codefrag\">svn co http://svn.apache.org/repos/asf/pig/trunk/contrib/piggybank</span> </li> <li>Add pig.jar to your ClassPath: <span class=\"codefrag\">export CLASSPATH=$CLASSPATH:/path/to/pig.jar</span> </li> <li>Build the jar file: from directory<span class=\"codefrag\">trunk/contrib/piggybank/java</span> run <span class=\"codefrag\">ant</span>. This will generate <span class=\"codefrag\">piggybank.jar</span> in the same directory. </li> </ul>  <p>To obtain <span class=\"codefrag\">javadoc</span> description of the functions run <span class=\"codefrag\">ant javadoc</span> from directory <span class=\"codefrag\">trunk/contrib/piggybank/java</span>. The documentation is generate in directory <span class=\"codefrag\">trunk/contrib/piggybank/java/build/javadoc</span>.</p> <p>To use a function, you need to determine which package it belongs to. The top level packages correspond to the function type and currently are: </p> <ul> <li>org.apache.pig.piggybank.comparison - for custom comparator used by ORDER operator </li> <li>org.apache.pig.piggybank.evaluation - for eval functions like aggregates and column transformations </li> <li>org.apache.pig.piggybank.filtering - for functions used in FILTER operator </li> <li>org.apache.pig.piggybank.grouping - for grouping functions</li> <li>org.apache.pig.piggybank.storage - for load/store functions </li> </ul>  <p>(The exact package of the function can be seen in the javadocs or by navigating the source tree.) </p> <p>For example, to use the UPPER function: </p> <pre class=\"code\">\nREGISTER /public/share/pig/contrib/piggybank/java/piggybank.jar ;\nTweetsInaug = FILTER Tweets BY org.apache.pig.piggybank.evaluation.string.UPPER(text) \n    MATCHES '.*(INAUG|OBAMA|BIDEN|CHENEY|BUSH).*' ;\nSTORE TweetsInaug INTO 'meta/inaug/tweets_inaug' ;\n</pre>  <h3 id=\"piggybank-contribute\">Contributing Functions</h3> <p>To contribute a Java function that you have written, do the following:</p> <ol> <li>Check the existing javadoc to make sure that the function does not already exist as described in <a href=\"#piggbank-access\">Accessing Functions</a>. </li> <li>Checkout the UDF code as described in <a href=\"#piggbank-access\">Accessing Functions</a>. </li> <li>Place your java code in the directory that makes sense for your function. The directory structure currently has two levels: (1) function type, as described in <a href=\"#piggbank-access\">Accessing Functions</a>, and (2) function subtype, for some of the types (like math or string for eval functions). If you think your function requires a new subtype, feel free to add one. </li> <li>Make sure that your function is well documented and uses the <a href=\"http://docs.oracle.com/javase/6/docs/technotes/tools/solaris/javadoc.html\">javadoc</a> style of documentation. </li> <li>Make sure that your code follows Pig coding conventions described in <a href=\"https://cwiki.apache.org/confluence/display/PIG/HowToContribute\">How to Contribute to Pig</a>.</li> <li>Make sure that for each function, you add a corresponding test class in the test part of the tree. </li> <li>Submit your patch following the process described in <a href=\"https://cwiki.apache.org/confluence/display/PIG/HowToContribute\">How to Contribute to Pig</a>. </li> </ol> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/udf.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/udf.html</a>\n  </p>\n</div>\n","perf":"<h1>Performance and Efficiency</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#tez-mode\">Tez mode</a> <ul class=\"minitoc\"> <li> <a href=\"#enable-tez\">How to enable Tez</a> </li> <li> <a href=\"#tez-dag\">Tez DAG generation</a> </li> <li> <a href=\"#container-reuse\">Tez session/container reuse</a> </li> <li> <a href=\"#auto-parallelism\">Automatic parallelism</a> </li> <li> <a href=\"#api-change\">API change</a> </li> <li> <a href=\"#known-issue\">Known issues</a> </li> </ul> </li> <li> <a href=\"#profiling\">Timing your UDFs</a> </li> <li> <a href=\"#combiner\">Combiner</a> <ul class=\"minitoc\"> <li> <a href=\"#When+the+Combiner+is+Used\">When the Combiner is Used</a> </li> <li> <a href=\"#When+the+Combiner+is+Not+Used\">When the Combiner is Not Used</a> </li> </ul> </li> <li> <a href=\"#hash-based-aggregation\">Hash-based Aggregation in Map Task</a> </li> <li> <a href=\"#memory-management\">Memory Management</a> </li> <li> <a href=\"#reducer-estimation\">Reducer Estimation</a> </li> <li> <a href=\"#multi-query-execution\">Multi-Query Execution</a> <ul class=\"minitoc\"> <li> <a href=\"#Turning+it+On+or+Off\">Turning it On or Off</a> </li> <li> <a href=\"#How+it+Works\">How it Works</a> </li> <li> <a href=\"#store-dump\">Store vs. Dump</a> </li> <li> <a href=\"#error-handling\">Error Handling</a> </li> <li> <a href=\"#backward-compatibility\">Backward Compatibility</a> </li> <li> <a href=\"#Implicit-Dependencies\">Implicit Dependencies</a> </li> </ul> </li> <li> <a href=\"#optimization-rules\">Optimization Rules</a> <ul class=\"minitoc\"> <li> <a href=\"#PartitionFilterOptimizer\">PartitionFilterOptimizer</a> </li> <li> <a href=\"#PredicatePushdownOptimizer\">PredicatePushdownOptimizer</a> </li> <li> <a href=\"#ConstantCalculator\">ConstantCalculator</a> </li> <li> <a href=\"#SplitFilter\">SplitFilter</a> </li> <li> <a href=\"#PushUpFilter\">PushUpFilter</a> </li> <li> <a href=\"#MergeFilter\">MergeFilter</a> </li> <li> <a href=\"#PushDownForEachFlatten\">PushDownForEachFlatten</a> </li> <li> <a href=\"#LimitOptimizer\">LimitOptimizer</a> </li> <li> <a href=\"#ColumnMapKeyPrune\">ColumnMapKeyPrune</a> </li> <li> <a href=\"#AddForEach\">AddForEach</a> </li> <li> <a href=\"#MergeForEach\">MergeForEach</a> </li> <li> <a href=\"#GroupByConstParallelSetter\">GroupByConstParallelSetter</a> </li> </ul> </li> <li> <a href=\"#performance-enhancers\">Performance Enhancers</a> <ul class=\"minitoc\"> <li> <a href=\"#Use+Optimization\">Use Optimization</a> </li> <li> <a href=\"#types\">Use Types</a> </li> <li> <a href=\"#projection\">Project Early and Often </a> </li> <li> <a href=\"#filter\">Filter Early and Often</a> </li> <li> <a href=\"#pipeline\">Reduce Your Operator Pipeline</a> </li> <li> <a href=\"#algebraic-interface\">Make Your UDFs Algebraic</a> </li> <li> <a href=\"#accumulator-interface\">Use the Accumulator Interface</a> </li> <li> <a href=\"#nulls\">Drop Nulls Before a Join</a> </li> <li> <a href=\"#join-optimizations\">Take Advantage of Join Optimizations</a> </li> <li> <a href=\"#parallel\">Use the Parallel Features</a> </li> <li> <a href=\"#limit\">Use the LIMIT Operator</a> </li> <li> <a href=\"#distinct\">Prefer DISTINCT over GROUP BY/GENERATE</a> </li> <li> <a href=\"#compression\">Compress the Results of Intermediate Jobs</a> </li> <li> <a href=\"#combine-files\">Combine Small Input Files</a> </li> <li> <a href=\"#direct-fetch\">Direct Fetch</a> </li> <li> <a href=\"#auto-local-mode\">Auto Local Mode</a> </li> <li> <a href=\"#user-jar-cache\">User Jar Cache</a> </li> </ul> </li> <li> <a href=\"#specialized-joins\">Specialized Joins</a> <ul class=\"minitoc\"> <li> <a href=\"#replicated-joins\">Replicated Joins</a> </li> <li> <a href=\"#skewed-joins\">Skewed Joins</a> </li> <li> <a href=\"#merge-joins\">Merge Joins</a> </li> <li> <a href=\"#merge-sparse-joins\">Merge-Sparse Joins</a> </li> <li> <a href=\"#specialized-joins-performance\">Performance Considerations</a> </li> </ul> </li> </ul> </div> </div>  <h2 id=\"tez-mode\">Tez mode</h2> <div class=\"section\"> <p> <a href=\"http://tez.apache.org\">Apache Tez</a> provides an alternative execution engine than MapReduce focusing on performance. By using optimized job flow, edge semantics and container reuse, we see consistent performance boost for both large job and small job. </p>  <h3 id=\"enable-tez\">How to enable Tez</h3> <p>To run Pig in tez mode, simply add \"-x tez\" in pig command line. Alternatively, you can add \"exectype=tez\" to conf/pig.properties to change the default exec type to Tez. Java system property \"-Dexectype=tez\" is also good to trigger the Tez mode.</p> <p>Prerequisite: Tez requires the tez tarball to be available in hdfs while running a job on the cluster and a tez-site.xml with tez.lib.uris setting pointing to that hdfs location in classpath. Copy the tez tarball to hdfs and add the tez conf directory($TEZ_HOME/conf) containing tez-site.xml to environmental variable \"PIG_CLASSPATH\" if pig on tez fails with \"tez.lib.uris is not defined\". This is required by the Apache Pig distribution.</p> <pre class=\"code\">\n  &lt;property&gt;\n    &lt;name&gt;tez.lib.uris&lt;/name&gt;\n    &lt;value&gt;${fs.default.name}/apps/tez/tez-0.5.2.tar.gz&lt;/value&gt;\n  &lt;/property&gt;\n</pre>  <h3 id=\"tez-dag\">Tez DAG generation</h3> <p>Every Pig script will be compiled into 1 or more Tez DAG (typically 1). Every Tez DAG consists of a number of vertices and and edges connecting vertices. For example, a simple join involves 1 DAG which consists of 3 vertices: load left input, load right input and join. Do an <a href=\"test#explain\">explain</a> in Tez mode will show you the DAG Pig script compiled into.</p>  <h3 id=\"container-reuse\">Tez session/container reuse</h3> <p>One downside of MapReduce is the startup cost for a job is very high. That hurts the performance especially for small job. Tez alleviate the problem by using session and container reuse, so it is not necessary to start an application master for every job, and start a JVM for every task. By default, session/container reuse is on and we usually shall not turn it off. JVM reuse might cause some side effect if static variable is used since static variable might live across different jobs. So if static variable is used in EvalFunc/LoadFunc/StoreFunc, be sure to implement a cleanup function and register with <a href=\"http://pig.apache.org/docs/r0.16.0/api/org/apache/pig/JVMReuseManager.html\">JVMReuseManager</a>.</p>  <h3 id=\"auto-parallelism\">Automatic parallelism</h3> <p>Just like MapReduce, if user specify \"parallel\" in their Pig statement, or user define default_parallel in Tez mode, Pig will honor it (the only exception is if user specify a parallel which is apparently too low, Pig will override it) </p> <p>If user specify neither \"parallel\" or \"default_parallel\", Pig will use automatic parallelism. In MapReduce, Pig submit one MapReduce job a time and before submiting a job, Pig has chance to automatically set reduce parallelism based on the size of input file. On the contrary, Tez submit a DAG as a unit and automatic parallelism is managed in three parts</p> <ul> <li>Before submiting a DAG, Pig estimate parallelism of each vertex statically based on the input file size of the DAG and the complexity of the pipeline of each vertex</li> <li>When DAG progress, Pig adjust the parallelism of vertexes with the best knowledge available at that moment (Pig grace paralellism)</li> <li>At runtime, Tez adjust vertex parallelism dynamically based on the input data volume of the vertex. Note currently Tez can only decrease the parallelism dynamically not increase. So in step 1 and 2, Pig overestimate the parallelism</li> </ul> <p>The following parameter control the behavior of automatic parallelism in Tez (share with MapReduce):</p> <pre class=\"code\">\npig.exec.reducers.bytes.per.reducer\npig.exec.reducers.max\n</pre>  <h3 id=\"api-change\">API change</h3> <p>If invoking Pig in Java, there is change in PigStats and PigProgressNotificationListener if using PigRunner.run(), check <a href=\"test#pig-statistics\">Pig Statistics</a> and <a href=\"test#ppnl\">Pig Progress Notification Listener</a> </p>  <h3 id=\"known-issue\">Known issues</h3> <p>Currently known issue in Tez mode includes:</p> <ul> <li>Tez local mode is not stable, we see job hang in some cases</li> <li>Tez specific GUI is not available yet, there is no GUI to track task progress. However, log message is available in GUI</li> </ul> </div>  <h2 id=\"profiling\">Timing your UDFs</h2> <div class=\"section\"> <p>The first step to improving performance and efficiency is measuring where the time is going. Pig provides a light-weight method for approximately measuring how much time is spent in different user-defined functions (UDFs) and Loaders. Simply set the pig.udf.profile property to true. This will cause new counters to be tracked for all Map-Reduce jobs generated by your script: approx_microsecs measures the approximate amount of time spent in a UDF, and approx_invocations measures the approximate number of times the UDF was invoked. In addition, the frequency of profiling can be configured via the pig.udf.profile.frequency (by default, every 100th invocation). Note that this may produce a large number of counters (two per UDF). Excessive amounts of counters can lead to poor JobTracker performance, so use this feature carefully, and preferably on a test cluster.</p> </div>    <h2 id=\"combiner\">Combiner</h2> <div class=\"section\"> <p>The Pig combiner is an optimizer that is invoked when the statements in your scripts are arranged in certain ways. The examples below demonstrate when the combiner is used and not used. Whenever possible, make sure the combiner is used as it frequently yields an order of magnitude improvement in performance. </p>  <h3 id=\"When+the+Combiner+is+Used\">When the Combiner is Used</h3> <p>The combiner is generally used in the case of non-nested foreach where all projections are either expressions on the group column or expressions on algebraic UDFs (see <a href=\"#algebraic-interface\">Make Your UDFs Algebraic</a>).</p> <p>Example:</p> <pre class=\"code\">\nA = load 'studenttab10k' as (name, age, gpa);\nB = group A by age;\nC = foreach B generate ABS(SUM(A.gpa)), COUNT(org.apache.pig.builtin.Distinct(A.name)), (MIN(A.gpa) + MAX(A.gpa))/2, group.age;\nexplain C;\n</pre>  <p>In the above example:</p> <ul> <li>The GROUP statement can be referred to as a whole or by accessing individual fields (as in the example). </li> <li>The GROUP statement and its elements can appear anywhere in the projection. </li> </ul> <p>In the above example, a variety of expressions can be applied to algebraic functions including:</p> <ul> <li>A column transformation function such as ABS can be applied to an algebraic function SUM.</li> <li>An algebraic function (COUNT) can be applied to another algebraic function (Distinct), but only the inner function is computed using the combiner. </li> <li>A mathematical expression can be applied to one or more algebraic functions. </li> </ul>  <p>You can check if the combiner is used for your query by running <a href=\"test#explain\">EXPLAIN</a> on the FOREACH alias as shown above. You should see the combine section in the MapReduce part of the plan:</p> <pre class=\"code\">\n.....\nCombine Plan\nB: Local Rearrange[tuple]{bytearray}(false) - scope-42\n| |\n| Project[bytearray][0] - scope-43\n|\n|---C: New For Each(false,false,false)[bag] - scope-28\n| |\n| Project[bytearray][0] - scope-29\n| |\n| POUserFunc(org.apache.pig.builtin.SUM$Intermediate)[tuple] - scope-30\n| |\n| |---Project[bag][1] - scope-31\n| |\n| POUserFunc(org.apache.pig.builtin.Distinct$Intermediate)[tuple] - scope-32\n| |\n| |---Project[bag][2] - scope-33\n|\n|---POCombinerPackage[tuple]{bytearray} - scope-36--------\n.....\n</pre> <p>The combiner is also used with a nested foreach as long as the only nested operation used is DISTINCT (see <a href=\"basic#foreach\">FOREACH</a> and <a href=\"basic#nestedblock\">Example: Nested Block</a>). </p> <pre class=\"code\">\nA = load 'studenttab10k' as (name, age, gpa);\nB = group A by age;\nC = foreach B { D = distinct (A.name); generate group, COUNT(D);}\n</pre>  <p>Finally, use of the combiner is influenced by the surrounding environment of the GROUP and FOREACH statements.</p>  <h3 id=\"When+the+Combiner+is+Not+Used\">When the Combiner is Not Used</h3> <p>The combiner is generally not used if there is any operator that comes between the GROUP and FOREACH statements in the execution plan. Even if the statements are next to each other in your script, the optimizer might rearrange them. In this example, the optimizer will push FILTER above FOREACH which will prevent the use of the combiner:</p> <pre class=\"code\">\nA = load 'studenttab10k' as (name, age, gpa);\nB = group A by age;\nC = foreach B generate group, COUNT (A);\nD = filter C by group.age &lt;30;\n</pre>  <p>Please note that the script above can be made more efficient by performing filtering before the GROUP statement:</p> <pre class=\"code\">\nA = load 'studenttab10k' as (name, age, gpa);\nB = filter A by age &lt;30;\nC = group B by age;\nD = foreach C generate group, COUNT (B);\n</pre>  <p> <strong>Note:</strong> One exception to the above rule is LIMIT. Starting with Pig 0.9, even if LIMIT comes between GROUP and FOREACH, the combiner will still be used. In this example, the optimizer will push LIMIT above FOREACH but this will not prevent the use of the combiner.</p> <pre class=\"code\">\nA = load 'studenttab10k' as (name, age, gpa);\nB = group A by age;\nC = foreach B generate group, COUNT (A);\nD = limit C 20;\n</pre>  <p>The combiner is also not used in the case where multiple FOREACH statements are associated with the same GROUP:</p> <pre class=\"code\">\nA = load 'studenttab10k' as (name, age, gpa);\nB = group A by age;\nC = foreach B generate group, COUNT (A);\nD = foreach B generate group, MIN (A.gpa). MAX(A.gpa);\n.....\n</pre> <p>Depending on your use case, it might be more efficient (improve performance) to split your script into multiple scripts.</p> </div>    <h2 id=\"hash-based-aggregation\">Hash-based Aggregation in Map Task</h2> <div class=\"section\"> <p> To improve performance, hash-based aggregation will aggregate records in the map task before sending them to the combiner. This optimization reduces the serializing/deserializing costs of the combiner by sending it fewer records.</p> <p> <strong>Turning On Off</strong> </p> <p>Hash-based aggregation has been shown to improve the speed of group-by operations by up to 50%. However, since this is a very new feature, it is currently turned OFF by default. To turn it ON, set the property pig.exec.mapPartAgg to true.</p> <p> <strong>Configuring</strong> </p> <p>If the group-by keys used for grouping don't result in a sufficient reduction in the number of records, the performance might be worse with this feature turned ON. To prevent this from happening, the feature turns itself off if the reduction in records sent to combiner is not more than a configurable threshold. This threshold can be set using the property pig.exec.mapPartAgg.minReduction. It is set to a default value of 10, which means that the number of records that get sent to the combiner should be reduced by a factor of 10 or more.</p> </div>    <h2 id=\"memory-management\">Memory Management</h2> <div class=\"section\"> <p>Pig allocates a fix amount of memory to store bags and spills to disk as soon as the memory limit is reached. This is very similar to how Hadoop decides when to spill data accumulated by the combiner. </p>  <p id=\"memory-bags\">The amount of memory allocated to bags is determined by pig.cachedbag.memusage; the default is set to 20% (0.2) of available memory. Note that this memory is shared across all large bags used by the application.</p> </div>    <h2 id=\"reducer-estimation\">Reducer Estimation</h2> <div class=\"section\"> <p> By default Pig determines the number of reducers to use for a given job based on the size of the input to the map phase. The input data size is divided by the pig.exec.reducers.bytes.per.reducer parameter value (default 1GB) to determine the number of reducers. The maximum number of reducers for a job is limited by the pig.exec.reducers.max parameter (default 999). </p> <p> The default reducer estimation algorithm described above can be overridden by setting the pig.exec.reducer.estimator parameter to the fully qualified class name of an implementation of <a href=\"http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigReducerEstimator.java\">org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigReducerEstimator</a>(MapReduce) or <a href=\"http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/TezOperDependencyParallelismEstimator.java\">org.apache.pig.backend.hadoop.executionengine.tez.plan.optimizer.TezOperDependencyParallelismEstimator</a>(Tez). The class must exist on the classpath of the process submitting the Pig job. If the pig.exec.reducer.estimator.arg parameter is set, the value will be passed to a constructor of the implementing class that takes a single String. </p>  </div>    <h2 id=\"multi-query-execution\">Multi-Query Execution</h2> <div class=\"section\"> <p>With multi-query execution Pig processes an entire script or a batch of statements at once.</p>  <h3 id=\"Turning+it+On+or+Off\">Turning it On or Off</h3> <p>Multi-query execution is turned on by default. To turn it off and revert to Pig's \"execute-on-dump/store\" behavior, use the \"-M\" or \"-no_multiquery\" options. </p> <p>To run script \"myscript.pig\" without the optimization, execute Pig as follows: </p> <pre class=\"code\">\n$ pig -M myscript.pig\nor\n$ pig -no_multiquery myscript.pig\n</pre>  <h3 id=\"How+it+Works\">How it Works</h3> <p>Multi-query execution introduces some changes:</p> <ul> <li> <p>For batch mode execution, the entire script is first parsed to determine if intermediate tasks can be combined to reduce the overall amount of work that needs to be done; execution starts only after the parsing is completed (see the <a href=\"test#explain\">EXPLAIN</a> operator and the <a href=\"cmds#run\">run</a> and <a href=\"cmds#exec\">exec</a> commands). </p> </li> <li> <p>Two run scenarios are optimized, as explained below: explicit and implicit splits, and storing intermediate results.</p> </li> </ul>  <h4 id=\"splits\">Explicit and Implicit Splits</h4> <p>There might be cases in which you want different processing on separate parts of the same data stream.</p> <p>Example 1:</p> <pre class=\"code\">\nA = LOAD ...\n...\nSPLIT A' INTO B IF ..., C IF ...\n...\nSTORE B' ...\nSTORE C' ...\n</pre> <p>Example 2:</p> <pre class=\"code\">\nA = LOAD ...\n...\nB = FILTER A' ...\nC = FILTER A' ...\n...\nSTORE B' ...\nSTORE C' ...\n</pre> <p>In prior Pig releases, Example 1 will dump A' to disk and then start jobs for B' and C'. Example 2 will execute all the dependencies of B' and store it and then execute all the dependencies of C' and store it. Both are equivalent, but the performance will be different. </p> <p>Here's what the multi-query execution does to increase the performance: </p> <ul> <li> <p>For Example 2, adds an implicit split to transform the query to Example 1. This eliminates the processing of A' multiple times.</p> </li> <li> <p>Makes the split non-blocking and allows processing to continue. This helps reduce the amount of data that has to be stored right at the split. </p> </li> <li> <p>Allows multiple outputs from a job. This way some results can be stored as a side-effect of the main job. This is also necessary to make the previous item work. </p> </li> <li> <p>Allows multiple split branches to be carried on to the combiner/reducer. This reduces the amount of IO again in the case where multiple branches in the split can benefit from a combiner run. </p> </li> </ul>  <h4 id=\"data-store-performance\">Storing Intermediate Results</h4> <p>Sometimes it is necessary to store intermediate results. </p> <pre class=\"code\">\nA = LOAD ...\n...\nSTORE A'\n...\nSTORE A''\n</pre> <p>If the script doesn't re-load A' for the processing of A the steps above A' will be duplicated. This is a special case of Example 2 above, so the same steps are recommended. With multi-query execution, the script will process A and dump A' as a side-effect.</p>  <h3 id=\"store-dump\">Store vs. Dump</h3> <p>With multi-query exection, you want to use <a href=\"basic#store\">STORE</a> to save (persist) your results. You do not want to use <a href=\"test#dump\">DUMP</a> as it will disable multi-query execution and is likely to slow down execution. (If you have included DUMP statements in your scripts for debugging purposes, you should remove them.) </p> <p>DUMP Example: In this script, because the DUMP command is interactive, the multi-query execution will be disabled and two separate jobs will be created to execute this script. The first job will execute A &gt; B &gt; DUMP while the second job will execute A &gt; B &gt; C &gt; STORE.</p> <pre class=\"code\">\nA = LOAD 'input' AS (x, y, z);\nB = FILTER A BY x &gt; 5;\nDUMP B;\nC = FOREACH B GENERATE y, z;\nSTORE C INTO 'output';\n</pre> <p>STORE Example: In this script, multi-query optimization will kick in allowing the entire script to be executed as a single job. Two outputs are produced: output1 and output2.</p> <pre class=\"code\">\nA = LOAD 'input' AS (x, y, z);\nB = FILTER A BY x &gt; 5;\nSTORE B INTO 'output1';\nC = FOREACH B GENERATE y, z;\nSTORE C INTO 'output2';\t\n</pre>  <h3 id=\"error-handling\">Error Handling</h3> <p>With multi-query execution Pig processes an entire script or a batch of statements at once. By default Pig tries to run all the jobs that result from that, regardless of whether some jobs fail during execution. To check which jobs have succeeded or failed use one of these options. </p> <p>First, Pig logs all successful and failed store commands. Store commands are identified by output path. At the end of execution a summary line indicates success, partial failure or failure of all store commands. </p> <p>Second, Pig returns different code upon completion for these scenarios:</p> <ul> <li> <p>Return code 0: All jobs succeeded</p> </li> <li> <p>Return code 1: <em>Used for retrievable errors</em> </p> </li> <li> <p>Return code 2: All jobs have failed </p> </li> <li> <p>Return code 3: Some jobs have failed </p> </li> </ul>  <p>In some cases it might be desirable to fail the entire script upon detecting the first failed job. This can be achieved with the \"-F\" or \"-stop_on_failure\" command line flag. If used, Pig will stop execution when the first failed job is detected and discontinue further processing. This also means that file commands that come after a failed store in the script will not be executed (this can be used to create \"done\" files). </p> <p>This is how the flag is used: </p> <pre class=\"code\">\n$ pig -F myscript.pig\nor\n$ pig -stop_on_failure myscript.pig\n</pre>  <h3 id=\"backward-compatibility\">Backward Compatibility</h3> <p>Most existing Pig scripts will produce the same result with or without the multi-query execution. There are cases though where this is not true. Path names and schemes are discussed here.</p> <p>Any script is parsed in it's entirety before it is sent to execution. Since the current directory can change throughout the script any path used in LOAD or STORE statement is translated to a fully qualified and absolute path.</p> <p>In map-reduce mode, the following script will load from \"hdfs://&lt;host&gt;:&lt;port&gt;/data1\" and store into \"hdfs://&lt;host&gt;:&lt;port&gt;/tmp/out1\". </p> <pre class=\"code\">\ncd /;\nA = LOAD 'data1';\ncd tmp;\nSTORE A INTO 'out1';\n</pre> <p>These expanded paths will be passed to any LoadFunc or Slicer implementation. In some cases this can cause problems, especially when a LoadFunc/Slicer is not used to read from a dfs file or path (for example, loading from an SQL database). </p> <p>Solutions are to either: </p> <ul> <li> <p>Specify \"-M\" or \"-no_multiquery\" to revert to the old names</p> </li> <li> <p>Specify a custom scheme for the LoadFunc/Slicer </p> </li> </ul> <p>Arguments used in a LOAD statement that have a scheme other than \"hdfs\" or \"file\" will not be expanded and passed to the LoadFunc/Slicer unchanged.</p> <p>In the SQL case, the SQLLoader function is invoked with 'sql://mytable'. </p> <pre class=\"code\">\nA = LOAD 'sql://mytable' USING SQLLoader();\n</pre>  <h3 id=\"Implicit-Dependencies\">Implicit Dependencies</h3> <p>If a script has dependencies on the execution order outside of what Pig knows about, execution may fail. </p>  <h4 id=\"Example\">Example</h4> <p>In this script, MYUDF might try to read from out1, a file that A was just stored into. However, Pig does not know that MYUDF depends on the out1 file and might submit the jobs producing the out2 and out1 files at the same time.</p> <pre class=\"code\">\n...\nSTORE A INTO 'out1';\nB = LOAD 'data2';\nC = FOREACH B GENERATE MYUDF($0,'out1');\nSTORE C INTO 'out2';\n</pre> <p>To make the script work (to ensure that the right execution order is enforced) add the exec statement. The exec statement will trigger the execution of the statements that produce the out1 file. </p> <pre class=\"code\">\n...\nSTORE A INTO 'out1';\nEXEC;\nB = LOAD 'data2';\nC = FOREACH B GENERATE MYUDF($0,'out1');\nSTORE C INTO 'out2';\n</pre>  <h4 id=\"Example-N10299\">Example</h4> <p>In this script, the STORE/LOAD operators have different file paths; however, the LOAD operator depends on the STORE operator.</p> <pre class=\"code\">\nA = LOAD '/user/xxx/firstinput' USING PigStorage();\nB = group ....\nC = .... agrregation function\nSTORE C INTO '/user/vxj/firstinputtempresult/days1';\n..\nAtab = LOAD '/user/xxx/secondinput' USING  PigStorage();\nBtab = group ....\nCtab = .... agrregation function\nSTORE Ctab INTO '/user/vxj/secondinputtempresult/days1';\n..\nE = LOAD '/user/vxj/firstinputtempresult/' USING  PigStorage();\nF = group ....\nG = .... aggregation function\nSTORE G INTO '/user/vxj/finalresult1';\n\nEtab =LOAD '/user/vxj/secondinputtempresult/' USING  PigStorage();\nFtab = group ....\nGtab = .... aggregation function\nSTORE Gtab INTO '/user/vxj/finalresult2';\n</pre> <p>To make the script work, add the exec statement. </p> <pre class=\"code\">\nA = LOAD '/user/xxx/firstinput' USING PigStorage();\nB = group ....\nC = .... agrregation function\nSTORE C INTO '/user/vxj/firstinputtempresult/days1';\n..\nAtab = LOAD '/user/xxx/secondinput' USING  PigStorage();\nBtab = group ....\nCtab = .... agrregation function\nSTORE Ctab INTO '/user/vxj/secondinputtempresult/days1';\n\nEXEC;\n\nE = LOAD '/user/vxj/firstinputtempresult/' USING  PigStorage();\nF = group ....\nG = .... aggregation function\nSTORE G INTO '/user/vxj/finalresult1';\n..\nEtab =LOAD '/user/vxj/secondinputtempresult/' USING  PigStorage();\nFtab = group ....\nGtab = .... aggregation function\nSTORE Gtab INTO '/user/vxj/finalresult2';\n</pre> <p>If the STORE and LOAD both had exact matching file paths, Pig will recognize the implicit dependency and launch two different mapreduce jobs/Tez DAGs with the second job depending on the output of the first one. exec is not required to be specified in that case.</p> </div>    <h2 id=\"optimization-rules\">Optimization Rules</h2> <div class=\"section\"> <p>Pig supports various optimization rules, all of which are enabled by default. To disable all or specific optimizations, use one or more of the following methods. Note some optimization rules are mandatory and cannot be disabled.</p> <ul> <li>The <span class=\"codefrag\">pig.optimizer.rules.disabled</span> <a href=\"start#properties\">pig property</a>, which accepts a comma-separated list of optimization rules to disable; the <span class=\"codefrag\">all</span> keyword disables all non-mandatory optimizations. (e.g.: <span class=\"codefrag\">set pig.optimizer.rules.disabled 'ColumnMapKeyPrune';</span>)</li> <li>The <span class=\"codefrag\">-t, -optimizer_off</span> command-line options. (e.g.: <span class=\"codefrag\">pig -optimizer_off [opt_rule | all]</span>)</li> </ul> <p> <span class=\"codefrag\">FilterLogicExpressionSimplifier</span> is an exception to the above. The rule is disabled by default, and enabled by setting the <span class=\"codefrag\">pig.exec.filterLogicExpressionSimplifier</span> pig property to true.</p>  <h3 id=\"PartitionFilterOptimizer\">PartitionFilterOptimizer</h3> <p>Push the filter condition to loader.</p> <pre class=\"code\">\nA = LOAD 'input' as (dt, state, event) using HCatLoader();\nB = FILTER A BY dt=='201310' AND state=='CA';\n</pre> <p>Filter condition will be pushed to loader if loader supports (Usually the loader is partition aware, such as HCatLoader)</p> <pre class=\"code\">\nA = LOAD 'input' as (dt, state, event) using HCatLoader();\n--Filter is removed\n</pre> <p>Loader will be instructed to loader the partition with dt=='201310' and state=='CA'</p>  <h3 id=\"PredicatePushdownOptimizer\">PredicatePushdownOptimizer</h3> <p>Push the filter condition to loader. Different than PartitionFilterOptimizer, the filter condition will be evaluated in Pig. In other words, the filter condition pushed to the loader is a hint. Loader might still load records which does not satisfy filter condition.</p> <pre class=\"code\">\nA = LOAD 'input' using OrcStorage();\nB = FILTER A BY dt=='201310' AND state=='CA';\n</pre> <p>Filter condition will be pushed to loader if loader supports</p> <pre class=\"code\">\nA = LOAD 'input' using OrcStorage();  -- Filter condition push to loader\nB = FILTER A BY dt=='201310' AND state=='CA';  -- Filter evaluated in Pig again\n</pre>  <h3 id=\"ConstantCalculator\">ConstantCalculator</h3> <p>This rule evaluates constant expression.</p> <pre class=\"code\">\n1) Constant pre-calculation \n\nB = FILTER A BY a0 &gt; 5+7; \nis simplified to \nB = FILTER A BY a0 &gt; 12; \n\n2) Evaluate UDF\n\nB = FOREACH A generate UPPER(CONCAT('a', 'b'));\nis simplified to \nB = FOREACH A generate 'AB';\n\n</pre>  <h3 id=\"SplitFilter\">SplitFilter</h3> <p>Split filter conditions so that we can push filter more aggressively.</p> <pre class=\"code\">\nA = LOAD 'input1' as (a0, a1);\nB = LOAD 'input2' as (b0, b1);\nC = JOIN A by a0, B by b0;\nD = FILTER C BY a1&gt;0 and b1&gt;0;\n</pre> <p>Here D will be splitted into:</p> <pre class=\"code\">\nX = FILTER C BY a1&gt;0;\nD = FILTER X BY b1&gt;0;\n</pre> <p>So \"a1&gt;0\" and \"b1&gt;0\" can be pushed up individually.</p>  <h3 id=\"PushUpFilter\">PushUpFilter</h3> <p>The objective of this rule is to push the FILTER operators up the data flow graph. As a result, the number of records that flow through the pipeline is reduced. </p> <pre class=\"code\">\nA = LOAD 'input';\nB = GROUP A BY $0;\nC = FILTER B BY $0 &lt; 10;\n</pre>  <h3 id=\"MergeFilter\">MergeFilter</h3> <p>Merge filter conditions after PushUpFilter rule to decrease the number of filter statements.</p>  <h3 id=\"PushDownForEachFlatten\">PushDownForEachFlatten</h3> <p>The objective of this rule is to reduce the number of records that flow through the pipeline by moving FOREACH operators with a FLATTEN down the data flow graph. In the example shown below, it would be more efficient to move the foreach after the join to reduce the cost of the join operation.</p> <pre class=\"code\">\nA = LOAD 'input' AS (a, b, c);\nB = LOAD 'input2' AS (x, y, z);\nC = FOREACH A GENERATE FLATTEN($0), B, C;\nD = JOIN C BY $1, B BY $1;\n</pre>  <h3 id=\"LimitOptimizer\">LimitOptimizer</h3> <p>The objective of this rule is to push the LIMIT operator up the data flow graph (or down the tree for database folks). In addition, for top-k (ORDER BY followed by a LIMIT) the LIMIT is pushed into the ORDER BY.</p> <pre class=\"code\">\nA = LOAD 'input';\nB = ORDER A BY $0;\nC = LIMIT B 10;\n</pre>  <h3 id=\"ColumnMapKeyPrune\">ColumnMapKeyPrune</h3> <p>Prune the loader to only load necessary columns. The performance gain is more significant if the corresponding loader support column pruning and only load necessary columns (See LoadPushDown.pushProjection). Otherwise, ColumnMapKeyPrune will insert a ForEach statement right after loader.</p> <pre class=\"code\">\nA = load 'input' as (a0, a1, a2);\nB = ORDER A by a0;\nC = FOREACH B GENERATE a0, a1;\n</pre> <p>a2 is irrelevant in this query, so we can prune it earlier. The loader in this query is PigStorage and it supports column pruning. So we only load a0 and a1 from the input file.</p> <p>ColumnMapKeyPrune also prunes unused map keys:</p> <pre class=\"code\">\nA = load 'input' as (a0:map[]);\nB = FOREACH A generate a0#'key1';\n</pre>  <h3 id=\"AddForEach\">AddForEach</h3> <p>Prune unused column as soon as possible. In addition to prune the loader in ColumnMapKeyPrune, we can prune a column as soon as it is not used in the rest of the script</p> <pre class=\"code\">\n-- Original code: \n\nA = LOAD 'input' AS (a0, a1, a2); \nB = ORDER A BY a0;\nC = FILTER B BY a1&gt;0;\n</pre> <p>We can only prune a2 from the loader. However, a0 is never used after \"ORDER BY\". So we can drop a0 right after \"ORDER BY\" statement.</p> <pre class=\"code\">\n-- Optimized code: \n\nA = LOAD 'input' AS (a0, a1, a2); \nB = ORDER A BY a0;\nB1 = FOREACH B GENERATE a1;  -- drop a0\nC = FILTER B1 BY a1&gt;0;\n</pre>  <h3 id=\"MergeForEach\">MergeForEach</h3> <p>The objective of this rule is to merge together two feach statements, if these preconditions are met:</p> <ul> <li>The foreach statements are consecutive.</li> <li>The first foreach statement does not contain flatten.</li> <li>The second foreach is not nested.</li> </ul> <pre class=\"code\">\n-- Original code: \n\nA = LOAD 'file.txt' AS (a, b, c); \nB = FOREACH A GENERATE a+b AS u, c-b AS v; \nC = FOREACH B GENERATE $0+5, v; \n\n-- Optimized code: \n\nA = LOAD 'file.txt' AS (a, b, c); \nC = FOREACH A GENERATE a+b+5, c-b;\n</pre>  <h3 id=\"GroupByConstParallelSetter\">GroupByConstParallelSetter</h3> <p>Force parallel \"1\" for \"group all\" statement. That's because even if we set parallel to N, only 1 reducer will be used in this case and all other reducer produce empty result.</p> <pre class=\"code\">\nA = LOAD 'input';\nB = GROUP A all PARALLEL 10;\n</pre> </div>    <h2 id=\"performance-enhancers\">Performance Enhancers</h2> <div class=\"section\">  <h3 id=\"Use+Optimization\">Use Optimization</h3> <p>Pig supports various <a href=\"perf#optimization-rules\">optimization rules</a> which are turned on by default. Become familiar with these rules.</p>  <h3 id=\"types\">Use Types</h3> <p>If types are not specified in the load statement, Pig assumes the type of =double= for numeric computations. A lot of the time, your data would be much smaller, maybe, integer or long. Specifying the real type will help with speed of arithmetic computation. It has an additional advantage of early error detection. </p> <pre class=\"code\">\n--Query 1\nA = load 'myfile' as (t, u, v);\nB = foreach A generate t + u;\n\n--Query 2\nA = load 'myfile' as (t: int, u: int, v);\nB = foreach A generate t + u;\n</pre> <p>The second query will run more efficiently than the first. In some of our queries with see 2x speedup. </p>  <h3 id=\"projection\">Project Early and Often </h3> <p>Pig does not (yet) determine when a field is no longer needed and drop the field from the row. For example, say you have a query like: </p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nB = load 'myotherfile' as (x, y, z);\nC = join A by t, B by x;\nD = group C by u;\nE = foreach D generate group, COUNT($1);\n</pre> <p>There is no need for v, y, or z to participate in this query. And there is no need to carry both t and x past the join, just one will suffice. Changing the query above to the query below will greatly reduce the amount of data being carried through the map and reduce phases by pig. </p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nA1 = foreach A generate t, u;\nB = load 'myotherfile' as (x, y, z);\nB1 = foreach B generate x;\nC = join A1 by t, B1 by x;\nC1 = foreach C generate t, u;\nD = group C1 by u;\nE = foreach D generate group, COUNT($1);\n</pre> <p>Depending on your data, this can produce significant time savings. In queries similar to the example shown here we have seen total time drop by 50%.</p>  <h3 id=\"filter\">Filter Early and Often</h3> <p>As with early projection, in most cases it is beneficial to apply filters as early as possible to reduce the amount of data flowing through the pipeline. </p> <pre class=\"code\">\n-- Query 1\nA = load 'myfile' as (t, u, v);\nB = load 'myotherfile' as (x, y, z);\nC = filter A by t == 1;\nD = join C by t, B by x;\nE = group D by u;\nF = foreach E generate group, COUNT($1);\n\n-- Query 2\nA = load 'myfile' as (t, u, v);\nB = load 'myotherfile' as (x, y, z);\nC = join A by t, B by x;\nD = group C by u;\nE = foreach D generate group, COUNT($1);\nF = filter E by C.t == 1;\n</pre> <p>The first query is clearly more efficient than the second one because it reduces the amount of data going into the join. </p> <p>One case where pushing filters up might not be a good idea is if the cost of applying filter is very high and only a small amount of data is filtered out. </p>  <h3 id=\"pipeline\">Reduce Your Operator Pipeline</h3> <p>For clarity of your script, you might choose to split your projects into several steps for instance: </p> <pre class=\"code\">\nA = load 'data' as (in: map[]);\n-- get key out of the map\nB = foreach A generate in#'k1' as k1, in#'k2' as k2;\n-- concatenate the keys\nC = foreach B generate CONCAT(k1, k2);\n.......\n</pre> <p>While the example above is easier to read, you might want to consider combining the two foreach statements to improve your query performance: </p> <pre class=\"code\">\nA = load 'data' as (in: map[]);\n-- concatenate the keys from the map\nB = foreach A generate CONCAT(in#'k1', in#'k2');\n....\n</pre> <p>The same goes for filters. </p>  <h3 id=\"algebraic-interface\">Make Your UDFs Algebraic</h3> <p>Queries that can take advantage of the combiner generally ran much faster (sometimes several times faster) than the versions that don't. The latest code significantly improves combiner usage; however, you need to make sure you do your part. If you have a UDF that works on grouped data and is, by nature, algebraic (meaning their computation can be decomposed into multiple steps) make sure you implement it as such. For details on how to write algebraic UDFs, see <a href=\"udf#algebraic-interface\">Algebraic Interface</a>.</p> <pre class=\"code\">\nA = load 'data' as (x, y, z)\nB = group A by x;\nC = foreach B generate group, MyUDF(A);\n....\n</pre> <p>If <span class=\"codefrag\">MyUDF</span> is algebraic, the query will use combiner and run much faster. You can run <span class=\"codefrag\">explain</span> command on your query to make sure that combiner is used. </p>  <h3 id=\"accumulator-interface\">Use the Accumulator Interface</h3> <p> If your UDF can't be made Algebraic but is able to deal with getting input in chunks rather than all at once, consider implementing the Accumulator interface to reduce the amount of memory used by your script. If your function <em>is</em> Algebraic and can be used on conjunction with Accumulator functions, you will need to implement the Accumulator interface as well as the Algebraic interface. For more information, see <a href=\"udf#accumulator-interface\">Accumulator Interface</a>.</p> <p> <strong>Note:</strong> Pig automatically chooses the interface that it expects to provide the best performance: Algebraic &gt; Accumulator &gt; Default. </p>  <h3 id=\"nulls\">Drop Nulls Before a Join</h3> <p>With the introduction of nulls, join and cogroup semantics were altered to work with nulls. The semantic for cogrouping with nulls is that nulls from a given input are grouped together, but nulls across inputs are not grouped together. This preserves the semantics of grouping (nulls are collected together from a single input to be passed to aggregate functions like COUNT) and the semantics of join (nulls are not joined across inputs). Since flattening an empty bag results in an empty row (and no output), in a standard join the rows with a null key will always be dropped. </p> <p>This join</p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nB = load 'myotherfile' as (x, y, z);\nC = join A by t, B by x;\n</pre> <p>is rewritten by Pig to </p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nB = load 'myotherfile' as (x, y, z);\nC1 = cogroup A by t INNER, B by x INNER;\nC = foreach C1 generate flatten(A), flatten(B);\n</pre> <p>Since the nulls from A and B won't be collected together, when the nulls are flattened we're guaranteed to have an empty bag, which will result in no output. So the null keys will be dropped. But they will not be dropped until the last possible moment. </p> <p>If the query is rewritten to </p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nB = load 'myotherfile' as (x, y, z);\nA1 = filter A by t is not null;\nB1 = filter B by x is not null;\nC = join A1 by t, B1 by x;\n</pre> <p>then the nulls will be dropped before the join. Since all null keys go to a single reducer, if your key is null even a small percentage of the time the gain can be significant. In one test where the key was null 7% of the time and the data was spread across 200 reducers, we saw a about a 10x speed up in the query by adding the early filters. </p>  <h3 id=\"join-optimizations\">Take Advantage of Join Optimizations</h3> <p> <strong>Regular Join Optimizations</strong> </p> <p>Optimization for regular joins ensures that the last table in the join is not brought into memory but streamed through instead. Optimization reduces the amount of memory used which means you can avoid spilling the data and also should be able to scale your query to larger data volumes. </p> <p>To take advantage of this optimization, make sure that the table with the largest number of tuples per key is the last table in your query. In some of our tests we saw 10x performance improvement as the result of this optimization.</p> <pre class=\"code\">\nsmall = load 'small_file' as (t, u, v);\nlarge = load 'large_file' as (x, y, z);\nC = join small by t, large by x;\n</pre> <p> <strong>Specialized Join Optimizations</strong> </p> <p>Optimization can also be achieved using fragment replicate joins, skewed joins, and merge joins. For more information see <a href=\"perf#specialized-joins\">Specialized Joins</a>.</p>  <h3 id=\"parallel\">Use the Parallel Features</h3> <p>You can set the number of reduce tasks for the MapReduce jobs generated by Pig using two parallel features. (The parallel features only affect the number of reduce tasks. Map parallelism is determined by the input file, one map for each HDFS block.)</p> <p> <strong>You Set the Number of Reducers</strong> </p> <p>Use the <a href=\"cmds#set\">set default parallel</a> command to set the number of reducers at the script level.</p> <p>Alternatively, use the PARALLEL clause to set the number of reducers at the operator level. (In a script, the value set via the PARALLEL clause will override any value set via \"set default parallel.\") You can include the PARALLEL clause with any operator that starts a reduce phase: <a href=\"basic#cogroup\">COGROUP</a>, <a href=\"basic#cross\">CROSS</a>, <a href=\"basic#distinct\">DISTINCT</a>, <a href=\"basic#group\">GROUP</a>, <a href=\"basic#join-inner\">JOIN (inner)</a>, <a href=\"basic#join-outer\">JOIN (outer)</a>, and <a href=\"basic#order-by\">ORDER BY</a>. PARALLEL clause can also be used with <a href=\"basic#union\">UNION</a> if Tez is the execution mode. It will turn off the union optimization and introduce an extra reduce step. Though it will have slightly degraded performance due to the extra step, it is very useful for controlling the number of output files. </p> <p>The number of reducers you need for a particular construct in Pig that forms a MapReduce boundary depends entirely on (1) your data and the number of intermediate keys you are generating in your mappers and (2) the partitioner and distribution of map (combiner) output keys. In the best cases we have seen that a reducer processing about 1 GB of data behaves efficiently.</p> <p> <strong>Let Pig Set the Number of Reducers</strong> </p> <p>If neither \"set default parallel\" nor the PARALLEL clause are used, Pig sets the number of reducers using a heuristic based on the size of the input data. You can set the values for these properties:</p> <ul> <li>pig.exec.reducers.bytes.per.reducer - Defines the number of input bytes per reduce; default value is 1000*1000*1000 (1GB).</li> <li>pig.exec.reducers.max - Defines the upper bound on the number of reducers; default is 999. </li> </ul>  <p>The formula, shown below, is very simple and will improve over time. The computed value takes all inputs within the script into account and applies the computed value to all the jobs within Pig script.</p> <p> <span class=\"codefrag\">#reducers = MIN (pig.exec.reducers.max, total input size (in bytes) / bytes per reducer) </span> </p> <p> <strong>Examples</strong> </p> <p>In this example PARALLEL is used with the GROUP operator. </p> <pre class=\"code\">\nA = LOAD 'myfile' AS (t, u, v);\nB = GROUP A BY t PARALLEL 18;\n...\n</pre> <p>In this example all the MapReduce jobs that get launched use 20 reducers.</p> <pre class=\"code\">\nSET default_parallel 20;\nA = LOAD 'myfile.txt' USING PigStorage() AS (t, u, v);\nB = GROUP A BY t;\nC = FOREACH B GENERATE group, COUNT(A.t) as mycount;\nD = ORDER C BY mycount;\nSTORE D INTO 'mysortedcount' USING PigStorage();\n</pre>  <h3 id=\"limit\">Use the LIMIT Operator</h3> <p>Often you are not interested in the entire output but rather a sample or top results. In such cases, using LIMIT can yield a much better performance as we push the limit as high as possible to minimize the amount of data travelling through the pipeline. </p> <p>Sample: </p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nB = limit A 500;\n</pre> <p>Top results: </p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nB = order A by t;\nC = limit B 500;\n</pre>  <h3 id=\"distinct\">Prefer DISTINCT over GROUP BY/GENERATE</h3> <p>To extract unique values from a column in a relation you can use DISTINCT or GROUP BY/GENERATE. DISTINCT is the preferred method; it is faster and more efficient.</p> <p>Example using GROUP BY - GENERATE:</p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nB = foreach A generate u;\nC = group B by u;\nD = foreach C generate group as uniquekey;\ndump D; \n</pre> <p>Example using DISTINCT:</p> <pre class=\"code\">\nA = load 'myfile' as (t, u, v);\nB = foreach A generate u;\nC = distinct B;\ndump C; \n</pre>  <h3 id=\"compression\">Compress the Results of Intermediate Jobs</h3> <p>If your Pig script generates a sequence of MapReduce jobs, you can compress the output of the intermediate jobs using LZO compression. (Use the <a href=\"test#EXPLAIN\">EXPLAIN</a> operator to determine if your script produces multiple MapReduce Jobs.)</p> <p>By doing this, you will save HDFS space used to store the intermediate data used by PIG and potentially improve query execution speed. In general, the more intermediate data that is generated, the more benefits in storage and speed that result.</p> <p>You can set the value for these properties:</p> <ul> <li>pig.tmpfilecompression - Determines if the temporary files should be compressed or not (set to false by default).</li> <li>pig.tmpfilecompression.codec - Specifies which compression codec to use. Currently, Pig accepts \"gz\" and \"lzo\" as possible values. However, because LZO is under GPL license (and disabled by default) you will need to configure your cluster to use the LZO codec to take advantage of this feature. For details, see http://code.google.com/p/hadoop-gpl-compression/wiki/FAQ.</li> </ul>  <p>On the non-trivial queries (one ran longer than a couple of minutes) we saw significant improvements both in terms of query latency and space usage. For some queries we saw up to 96% disk saving and up to 4x query speed up. Of course, the performance characteristics are very much query and data dependent and testing needs to be done to determine gains. We did not see any slowdown in the tests we peformed which means that you are at least saving on space while using compression.</p> <p>With gzip we saw a better compression (96-99%) but at a cost of 4% slowdown. Thus, we don't recommend using gzip. </p> <p> <strong>Example</strong> </p> <pre class=\"code\">\n-- launch Pig script using lzo compression \n\njava -cp $PIG_HOME/pig.jar \n-Djava.library.path=&lt;path to the lzo library&gt; \n-Dpig.tmpfilecompression=true \n-Dpig.tmpfilecompression.codec=lzo org.apache.pig.Main  myscript.pig \n</pre>  <h3 id=\"combine-files\">Combine Small Input Files</h3> <p>Processing input (either user input or intermediate input) from multiple small files can be inefficient because a separate map has to be created for each file. Pig can now combined small files so that they are processed as a single map.</p> <p>You can set the values for these properties:</p> <ul> <li>pig.maxCombinedSplitSize – Specifies the size, in bytes, of data to be processed by a single map. Smaller files are combined untill this size is reached. </li> <li>pig.splitCombination – Turns combine split files on or off (set to “true” by default).</li> </ul>  <p>This feature works with <a href=\"func#pigstorage\">PigStorage</a>. However, if you are using a custom loader, please note the following:</p> <ul> <li>If your loader implementation makes use of the PigSplit object passed through the prepareToRead method, then you may need to rebuild the loader since the definition of PigSplit has been modified. </li> <li>The loader must be stateless across the invocations to the prepareToRead method. That is, the method should reset any internal states that are not affected by the RecordReader argument.</li> <li>If a loader implements IndexableLoadFunc, or implements OrderedLoadFunc and CollectableLoadFunc, its input splits won't be subject to possible combinations.</li> </ul>   <h3 id=\"direct-fetch\">Direct Fetch</h3> <p>When the <a href=\"test#dump\">DUMP</a> operator is used to execute Pig Latin statements, Pig can take the advantage to minimize latency by directly reading data from HDFS rather than launching MapReduce jobs.</p> <p> The result is fetched if the query contains any of the following operators: <a href=\"basic#filter\">FILTER</a>, <a href=\"basic#foreach\">FOREACH</a>, <a href=\"basic#limit\">LIMIT</a>, <a href=\"basic#stream\">STREAM</a>, <a href=\"basic#union\">UNION</a>. <br> Fetching will be disabled in case of: </p> <ul> <li>the presence of other operators, <a href=\"http://pig.apache.org/docs/r0.13.0/api/org/apache/pig/impl/builtin/SampleLoader.html\">sample loaders</a> and scalar expressions</li> <li>no <a href=\"basic#limit\">LIMIT</a> operator</li> <li>implicit splits</li> </ul> <p> Also note that direct-fetch doesn't support UDFs that interact with the distributed cache. You can check if the query can be fetched by running EXPLAIN. You should see \"No MR jobs. Fetch only.\" in the MapReduce part of the plan. </p> <p> Direct fetch is turned on by default. To turn it off set the property opt.fetch to false or start Pig with the \"-N\" or \"-no_fetch\" option. </p>  <h3 id=\"auto-local-mode\">Auto Local Mode</h3> <p>Processing small mapreduce jobs on hadoop cluster could be slow as it has overhead of job startup and job scheduling. For jobs with small input data, pig can convert them to run them as in-process mapreduce with hadoop's local mode. If pig.auto.local.enabled flag is set to true, pig will convert mapreduce jobs with input data less than pig.auto.local.input.maxbytes (100MB by default) to run in local mode, provided the number of reducers required by the job are less than or equal to 1. Note, jobs converted to run in local mode load and store data from HDFS, so any job in the pig workflow(dag) could be converted to run in local mode without affecting its downstream jobs.</p> <p>You can set the values for these properties in order to configure the behavior:</p> <ul> <li>pig.auto.local.enabled - Turns on/off auto local mode feature (false by default).</li> <li>pig.auto.local.input.maxbytes - Controls the max threshold size (in bytes) to convert jobs to run in local mode (100MB by default).</li> </ul>  <p> Sometimes, you may want change job configuration for jobs that are converted to run in local mode (eg- change io.sort.mb for small jobs). To do so, you can use pig.local. prefix to any configuration and configuration will be set on converted jobs. For example, set pig.local.io.sort.mb 100 will change io.sort.mb value to 100 for jobs converted to run in local mode. </p>  <h3 id=\"user-jar-cache\">User Jar Cache</h3> <p>Jars required for user defined functions (UDFs) are copied to distributed cache by pig to make them available on task nodes. To put these jars on distributed cache, pig clients copy these jars to HDFS under a temporary location. For scheduled jobs, these jars do not change frequently. Also, creating a lot of small jar files on HDFS is not HDFS friendly. To avoid copying these small jar files to HDFS again and again, pig allows users to configure a user level jar cache (readable only to the user for security reasons). If pig.user.cache.enabled flag is set to true, UDF jars are copied to jar cache location (configurable) under a directory named with the hash (SHA) of the jar. Hash of the jar is used to identify the existence of the jar in subsequent uses of the jar by the user. If a jar with same hash and filename is found in the cache, it is used avoiding copy of the jar to hdfs.</p> <p>You can set the values for these properties in order to configure the jar cache:</p> <ul> <li>pig.user.cache.enabled - Turn on/off user jar cache feature (false by default).</li> <li>pig.user.cache.location - Path on HDFS that will be used a staging directory for the user jar cache (defaults to pig.temp.dir or /tmp).</li> </ul>  <p> User jar cache feature is fail safe. If jars cannot be copied to jar cache due to any permission/configuration problems, pig will default old behavior. </p> </div>    <h2 id=\"specialized-joins\">Specialized Joins</h2> <div class=\"section\">  <h3 id=\"replicated-joins\">Replicated Joins</h3> <p>Fragment replicate join is a special type of join that works well if one or more relations are small enough to fit into main memory. In such cases, Pig can perform a very efficient join because all of the hadoop work is done on the map side. In this type of join the large relation is followed by one or more small relations. The small relations must be small enough to fit into main memory; if they don't, the process fails and an error is generated.</p>  <h4 id=\"Usage\">Usage</h4> <p>Perform a replicated join with the USING clause (see <a href=\"basic#join-inner\">JOIN (inner)</a> and <a href=\"basic#join-outer\">JOIN (outer)</a>). In this example, a large relation is joined with two smaller relations. Note that the large relation comes first followed by the smaller relations; and, all small relations together must fit into main memory, otherwise an error is generated. </p> <pre class=\"code\">\nbig = LOAD 'big_data' AS (b1,b2,b3);\n\ntiny = LOAD 'tiny_data' AS (t1,t2,t3);\n\nmini = LOAD 'mini_data' AS (m1,m2,m3);\n\nC = JOIN big BY b1, tiny BY t1, mini BY m1 USING 'replicated';\n</pre>  <h4 id=\"Conditions\">Conditions</h4> <p>Fragment replicate joins are experimental; we don't have a strong sense of how small the small relation must be to fit into memory. In our tests with a simple query that involves just a JOIN, a relation of up to 100 M can be used if the process overall gets 1 GB of memory. Please share your observations and experience with us.</p> <p>In order to avoid replicated joins on large relations, we fail if size of relation(s) to be replicated (in bytes) is greater than pig.join.replicated.max.bytes (default = 1GB).</p>  <h3 id=\"skewed-joins\">Skewed Joins</h3> <p> Parallel joins are vulnerable to the presence of skew in the underlying data. If the underlying data is sufficiently skewed, load imbalances will swamp any of the parallelism gains. In order to counteract this problem, skewed join computes a histogram of the key space and uses this data to allocate reducers for a given key. Skewed join does not place a restriction on the size of the input keys. It accomplishes this by splitting the left input on the join predicate and streaming the right input. The left input is sampled to create the histogram. </p> <p> Skewed join can be used when the underlying data is sufficiently skewed and you need a finer control over the allocation of reducers to counteract the skew. It should also be used when the data associated with a given key is too large to fit in memory. </p>  <h4 id=\"Usage-N10691\">Usage</h4> <p>Perform a skewed join with the USING clause (see <a href=\"basic#join-inner\">JOIN (inner)</a> and <a href=\"basic#join-outer\">JOIN (outer)</a>). </p> <pre class=\"code\">\nA = LOAD 'skewed_data' AS (a1,a2,a3);\nB = LOAD 'data' AS (b1,b2,b3);\nC = JOIN A BY a1, B BY b1 USING 'skewed';\n</pre>  <h4 id=\"Conditions-N106A7\">Conditions</h4> <p> Skewed join will only work under these conditions: </p> <ul> <li>Skewed join works with two-table inner and outer join. Currently we do not support more than two tables for skewed join. Specifying three-way (or more) joins will fail validation. For such joins, we rely on you to break them up into two-way joins.</li> <li>The skewed table must be specified as the left table. Pig samples on that table and determines the number of reducers per key.</li> <li>The pig.skewedjoin.reduce.memusage Java parameter specifies the fraction of heap available for the reducer to perform the join. A low fraction forces Pig to use more reducers but increases copying cost. We have seen good performance when we set this value in the range 0.1 - 0.4. However, note that this is hardly an accurate range. Its value depends on the amount of heap available for the operation, the number of columns in the input and the skew. An appropriate value is best obtained by conducting experiments to achieve a good performance. The default value is 0.5. </li> <li>Skewed join does not address (balance) uneven data distribution across reducers. However, in most cases, skewed join ensures that the join will finish (however slowly) rather than fail. </li> </ul>  <h3 id=\"merge-joins\">Merge Joins</h3> <p> Often user data is stored such that both inputs are already sorted on the join key. In this case, it is possible to join the data in the map phase of a MapReduce job. This provides a significant performance improvement compared to passing all of the data through unneeded sort and shuffle phases. </p> <p> Pig has implemented a merge join algorithm, or sort-merge join. It works on pre-sorted data, and does not sort data for you. See Conditions, below, for restrictions that apply when using this join algorithm. Pig implements the merge join algorithm by selecting the left input of the join to be the input file for the map phase, and the right input of the join to be the side file. It then samples records from the right input to build an index that contains, for each sampled record, the key(s) the filename and the offset into the file the record begins at. This sampling is done in the first MapReduce job. A second MapReduce job is then initiated, with the left input as its input. Each map uses the index to seek to the appropriate record in the right input and begin doing the join. </p>  <h4 id=\"Usage-N106D2\">Usage</h4> <p>Perform a merge join with the USING clause (see <a href=\"basic#join-inner\">JOIN (inner)</a> and <a href=\"basic#join-outer\">JOIN (outer)</a>). </p> <pre class=\"code\">\nC = JOIN A BY a1, B BY b1, C BY c1 USING 'merge';\n</pre>  <h4 id=\"Conditions-N106E8\">Conditions</h4> <p> <strong>Condition A</strong> </p> <p>Inner merge join (between two tables) will only work under these conditions: </p> <ul> <li>Data must come directly from either a Load or an Order statement.</li> <li>There may be filter statements and foreach statements between the sorted data source and the join statement. The foreach statement should meet the following conditions: <ul> <li>The foreach statement should not change the position of the join keys. </li> <li>There should be no transformation on the join keys which will change the sort order.</li> <li>UDFs also have to adhere to the previous condition and should not transform the JOIN keys in a way that would change the sort order.</li> </ul> </li> <li>Data must be sorted on join keys in ascending (ASC) order on both sides.</li> <li>If sort is provided by the loader, rather than an explicit Order operation, the right-side loader must implement either the {OrderedLoadFunc} interface or {IndexableLoadFunc} interface.</li> <li>Type information must be provided for the join key in the schema.</li> </ul>  <p>The PigStorage loader satisfies all of these conditions.</p>  <p> <strong>Condition B</strong> </p> <p>Outer merge join (between two tables) and inner merge join (between three or more tables) will only work under these conditions: </p> <ul> <li>No other operations can be done between the load and join statements. </li> <li>Data must be sorted on join keys in ascending (ASC) order on both sides. </li> <li>Left-most loader must implement {CollectableLoader} interface as well as {OrderedLoadFunc}. </li> <li>All other loaders must implement {IndexableLoadFunc}. </li> <li>Type information must be provided for the join key in the schema.</li> </ul>  <p>Pig does not provide a loader that supports outer merge joins. You will need to build your own loader to take advantage of this feature.</p>  <h3 id=\"merge-sparse-joins\">Merge-Sparse Joins</h3> <p>Merge-Sparse join is a specialization of merge join. Merge-sparse join is intended for use when one of the tables is very sparse, meaning you expect only a small number of records to be matched during the join. In tests this join performed well for cases where less than 1% of the data was matched in the join.</p>  <h4 id=\"Usage-N10749\">Usage</h4> <p>Perform a merge-sparse join with the USING clause (see <a href=\"basic#join-inner\">JOIN (inner)</a>). </p> <pre class=\"code\">\na = load 'sorted_input1' using org.apache.pig.piggybank.storage.IndexedStorage('\\t', '0');\nb = load 'sorted_input2' using org.apache.pig.piggybank.storage.IndexedStorage('\\t', '0');\nc = join a by $0, b by $0 using 'merge-sparse';\nstore c into 'results';\n</pre>  <h4 id=\"Conditions-N1075B\">Conditions</h4> <p>Merge-sparse join only works for inner joins and is not currently implemented for outer joins.</p> <p>For inner joins, the preconditions are the same as for merge join with the exception of constrains on the right-side loader. For sparse-merge joins the loader must implement IndexedLoadFunc or the join will fail.</p> <p>Piggybank now contains a load function called org.apache.pig.piggybank.storage.IndexedStorage that is a derivation of PigStorage and implements IndexedLoadFunc. This is the only loader included in the standard Pig distribution that can be used for merge-sparse join.</p>  <h3 id=\"specialized-joins-performance\">Performance Considerations</h3> <p>Note the following:</p> <ul> <li>If one of the data sets is small enough to fit into memory, a Replicated Join is very likely to provide better performance.</li> <li>You will also see better performance if the data in the left table is partitioned evenly across part files (no significant skew and each part file contains at least one full block of data).</li> </ul> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/perf.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/perf.html</a>\n  </p>\n</div>\n","func":"<h1>Built In Functions</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#built-in-functions\">Introduction</a> </li> <li> <a href=\"#dynamic-invokers\">Dynamic Invokers</a> </li> <li> <a href=\"#eval-functions\">Eval Functions</a> <ul class=\"minitoc\"> <li> <a href=\"#avg\">AVG</a> </li> <li> <a href=\"#bagtostring\">BagToString</a> </li> <li> <a href=\"#bloom\">Bloom</a> </li> <li> <a href=\"#concat\">CONCAT</a> </li> <li> <a href=\"#count\">COUNT</a> </li> <li> <a href=\"#count-star\">COUNT_STAR</a> </li> <li> <a href=\"#diff\">DIFF</a> </li> <li> <a href=\"#isempty\">IsEmpty</a> </li> <li> <a href=\"#max\">MAX</a> </li> <li> <a href=\"#min\">MIN</a> </li> <li> <a href=\"#plucktuple\">PluckTuple</a> </li> <li> <a href=\"#size\">SIZE</a> </li> <li> <a href=\"#subtract\">SUBTRACT</a> </li> <li> <a href=\"#sum\">SUM</a> </li> <li> <a href=\"#tokenize\">TOKENIZE</a> </li> </ul> </li> <li> <a href=\"#load-store-functions\">Load/Store Functions</a> <ul class=\"minitoc\"> <li> <a href=\"#handling-compression\">Handling Compression</a> </li> <li> <a href=\"#binstorage\">BinStorage</a> </li> <li> <a href=\"#jsonloadstore\">JsonLoader, JsonStorage</a> </li> <li> <a href=\"#pigdump\">PigDump</a> </li> <li> <a href=\"#pigstorage\">PigStorage</a> </li> <li> <a href=\"#textloader\">TextLoader</a> </li> <li> <a href=\"#HBaseStorage\">HBaseStorage</a> </li> <li> <a href=\"#AvroStorage\">AvroStorage</a> </li> <li> <a href=\"#TrevniStorage\">TrevniStorage</a> </li> <li> <a href=\"#AccumuloStorage\">AccumuloStorage</a> </li> <li> <a href=\"#OrcStorage\">OrcStorage</a> </li> </ul> </li> <li> <a href=\"#math-functions\">Math Functions</a> <ul class=\"minitoc\"> <li> <a href=\"#abs\">ABS</a> </li> <li> <a href=\"#acos\">ACOS</a> </li> <li> <a href=\"#asin\">ASIN</a> </li> <li> <a href=\"#atan\">ATAN</a> </li> <li> <a href=\"#cbrt\">CBRT</a> </li> <li> <a href=\"#ceil\">CEIL</a> </li> <li> <a href=\"#cos\">COS</a> </li> <li> <a href=\"#cosh\">COSH</a> </li> <li> <a href=\"#exp\">EXP</a> </li> <li> <a href=\"#floor\">FLOOR</a> </li> <li> <a href=\"#log\">LOG</a> </li> <li> <a href=\"#log10\">LOG10</a> </li> <li> <a href=\"#random\">RANDOM</a> </li> <li> <a href=\"#round\">ROUND</a> </li> <li> <a href=\"#round_to\">ROUND_TO</a> </li> <li> <a href=\"#sin\">SIN</a> </li> <li> <a href=\"#sinh\">SINH</a> </li> <li> <a href=\"#sqrt\">SQRT</a> </li> <li> <a href=\"#tan\">TAN</a> </li> <li> <a href=\"#tanh\">TANH</a> </li> </ul> </li> <li> <a href=\"#string-functions\">String Functions</a> <ul class=\"minitoc\"> <li> <a href=\"#endswith\">ENDSWITH</a> </li> <li> <a href=\"#equalsignorecase\">EqualsIgnoreCase</a> </li> <li> <a href=\"#indexof\">INDEXOF</a> </li> <li> <a href=\"#last-index-of\">LAST_INDEX_OF</a> </li> <li> <a href=\"#lcfirst\">LCFIRST</a> </li> <li> <a href=\"#lower\">LOWER</a> </li> <li> <a href=\"#ltrim\">LTRIM</a> </li> <li> <a href=\"#regex-extract\">REGEX_EXTRACT </a> </li> <li> <a href=\"#regex-extract-all\">REGEX_EXTRACT_ALL </a> </li> <li> <a href=\"#replace\">REPLACE</a> </li> <li> <a href=\"#rtrim\">RTRIM</a> </li> <li> <a href=\"#sprintf\">SPRINTF</a> </li> <li> <a href=\"#startswith\">STARTSWITH</a> </li> <li> <a href=\"#strsplit\">STRSPLIT</a> </li> <li> <a href=\"#strsplittobag\">STRSPLITTOBAG</a> </li> <li> <a href=\"#substring\">SUBSTRING</a> </li> <li> <a href=\"#trim\">TRIM</a> </li> <li> <a href=\"#ucfirst\">UCFIRST</a> </li> <li> <a href=\"#upper\">UPPER</a> </li> <li> <a href=\"#uniqueid\">UniqueID</a> </li> </ul> </li> <li> <a href=\"#datetime-functions\">Datetime Functions</a> <ul class=\"minitoc\"> <li> <a href=\"#add-duration\">AddDuration</a> </li> <li> <a href=\"#current-time\">CurrentTime</a> </li> <li> <a href=\"#days-between\">DaysBetween</a> </li> <li> <a href=\"#get-day\">GetDay</a> </li> <li> <a href=\"#get-hour\">GetHour</a> </li> <li> <a href=\"#get-milli-second\">GetMilliSecond</a> </li> <li> <a href=\"#get-minute\">GetMinute</a> </li> <li> <a href=\"#get-month\">GetMonth</a> </li> <li> <a href=\"#get-second\">GetSecond</a> </li> <li> <a href=\"#get-week\">GetWeek</a> </li> <li> <a href=\"#get-week-year\">GetWeekYear</a> </li> <li> <a href=\"#get-year\">GetYear</a> </li> <li> <a href=\"#hours-between\">HoursBetween</a> </li> <li> <a href=\"#milli-seconds-between\">MilliSecondsBetween</a> </li> <li> <a href=\"#minutes-between\">MinutesBetween</a> </li> <li> <a href=\"#months-between\">MonthsBetween</a> </li> <li> <a href=\"#seconds-between\">SecondsBetween</a> </li> <li> <a href=\"#subtract-duration\">SubtractDuration</a> </li> <li> <a href=\"#to-date\">ToDate</a> </li> <li> <a href=\"#to-milli-seconds\">ToMilliSeconds</a> </li> <li> <a href=\"#to-string\">ToString</a> </li> <li> <a href=\"#to-unix-time\">ToUnixTime</a> </li> <li> <a href=\"#weeks-between\">WeeksBetween</a> </li> <li> <a href=\"#years-between\">YearsBetween</a> </li> </ul> </li> <li> <a href=\"#bag-tuple-functions\">Tuple, Bag, Map Functions</a> <ul class=\"minitoc\"> <li> <a href=\"#totuple\">TOTUPLE</a> </li> <li> <a href=\"#tobag\">TOBAG</a> </li> <li> <a href=\"#tomap\">TOMAP</a> </li> <li> <a href=\"#topx\">TOP</a> </li> </ul> </li> <li> <a href=\"#hive-udf\">Hive UDF</a> <ul class=\"minitoc\"> <li> <a href=\"#Syntax-N1305A\">Syntax</a> </li> <li> <a href=\"#Terms-N13072\">Terms</a> </li> <li> <a href=\"#Example-N130A2\">Example</a> </li> </ul> </li> </ul> </div> </div>  <h2 id=\"built-in-functions\">Introduction</h2> <div class=\"section\"> <p> Pig comes with a set of built in functions (the eval, load/store, math, string, bag and tuple functions). Two main properties differentiate built in functions from <a href=\"udf\">user defined functions</a> (UDFs). First, built in functions don't need to be registered because Pig knows where they are. Second, built in functions don't need to be qualified when they are used because Pig knows where to find them. </p> </div>    <h2 id=\"dynamic-invokers\">Dynamic Invokers</h2> <div class=\"section\"> <p>Often you may need to use a simple function that is already provided by standard Java libraries, but for which a <a href=\"udf\">user defined functions</a> (UDF) has not been written. Dynamic invokers allow you to refer to Java functions without having to wrap them in custom UDFs, at the cost of doing some Java reflection on every function call. </p> <pre class=\"code\">\n...\nDEFINE UrlDecode InvokeForString('java.net.URLDecoder.decode', 'String String'); \nencoded_strings = LOAD 'encoded_strings.txt' as (encoded:chararray); \ndecoded_strings = FOREACH encoded_strings GENERATE UrlDecode(encoded, 'UTF-8'); \n...\n</pre> <p>Currently, dynamic invokers can be used for any static function that: </p> <ul> <li>Accepts no arguments or accepts some combination of strings, ints, longs, doubles, floats, or arrays with these same types </li> <li>Returns a string, an int, a long, a double, or a float</li> </ul> <p>Only primitives can be used for numbers; no capital-letter numeric classes can be used as arguments. Depending on the return type, a specific kind of invoker must be used: InvokeForString, InvokeForInt, InvokeForLong, InvokeForDouble, or InvokeForFloat. </p> <p>The <a href=\"basic#define\">DEFINE</a> statement is used to bind a keyword to a Java method, as above. The first argument to the InvokeFor* constructor is the full path to the desired method. The second argument is a space-delimited ordered list of the classes of the method arguments. This can be omitted or an empty string if the method takes no arguments. Valid class names are string, long, float, double, and int. Invokers can also work with array arguments, represented in Pig as DataBags of single-tuple elements. Simply refer to string[], for example. Class names are not case sensitive. </p> <p>The ability to use invokers on methods that take array arguments makes methods like those in org.apache.commons.math.stat.StatUtils available (for processing the results of grouping your datasets, for example). This is helpful, but a word of caution: the resulting UDF will not be optimized for Hadoop, and the very significant benefits one gains from implementing the Algebraic and Accumulator interfaces are lost here. Be careful if you use invokers this way.</p> </div>    <h2 id=\"eval-functions\">Eval Functions</h2> <div class=\"section\">  <h3 id=\"avg\">AVG</h3> <p>Computes the average of the numeric values in a single-column bag. </p>  <h4 id=\"Syntax\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AVG(expression)</p> </td> </tr> </table>  <h4 id=\"Terms\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Any expression whose result is a bag. The elements of the bag should be data type int, long, float, double, bigdecimal, biginteger or bytearray.</p> </td> </tr> </table>  <h4 id=\"Usage\">Usage</h4> <p>Use the AVG function to compute the average of the numeric values in a single-column bag. AVG requires a preceding GROUP ALL statement for global averages and a GROUP BY statement for group averages.</p> <p>The AVG function ignores NULL values. </p>  <h4 id=\"Example\">Example</h4> <p>In this example the average GPA for each student is computed (see the <a href=\"basic#group\">GROUP</a> operator for information about the field names in relation B).</p> <pre class=\"code\">\nA = LOAD 'student.txt' AS (name:chararray, term:chararray, gpa:float);\n\nDUMP A;\n(John,fl,3.9F)\n(John,wt,3.7F)\n(John,sp,4.0F)\n(John,sm,3.8F)\n(Mary,fl,3.8F)\n(Mary,wt,3.9F)\n(Mary,sp,4.0F)\n(Mary,sm,4.0F)\n\nB = GROUP A BY name;\n\nDUMP B;\n(John,{(John,fl,3.9F),(John,wt,3.7F),(John,sp,4.0F),(John,sm,3.8F)})\n(Mary,{(Mary,fl,3.8F),(Mary,wt,3.9F),(Mary,sp,4.0F),(Mary,sm,4.0F)})\n\nC = FOREACH B GENERATE A.name, AVG(A.gpa);\n\nDUMP C;\n({(John),(John),(John),(John)},3.850000023841858)\n({(Mary),(Mary),(Mary),(Mary)},3.925000011920929)\n</pre>  <h4 id=\"Types+Tables\">Types Tables</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AVG </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal *</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal *</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as double </p> </td> </tr> </table> <p>* Average values for datatypes bigdecimal and biginteger have precision setting <a href=\"http://docs.oracle.com/javase/7/docs/api/java/math/MathContext.html#DECIMAL128\">java.math.MathContext.DECIMAL128</a>.</p>  <h3 id=\"bagtostring\">BagToString</h3> <p>Concatenate the elements of a Bag into a chararray string, placing an optional delimiter between each value.</p>  <h4 id=\"Syntax-N10162\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BagToString(vals:bag [, delimiter:chararray])</p> </td> </tr> </table>  <h4 id=\"Terms-N10176\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>vals</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A bag of arbitrary values. They will each be cast to chararray if they are not already.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>delimiter</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A chararray value to place between elements of the bag; defaults to underscore <span class=\"codefrag\">'_'</span>.</p> </td> </tr> </table>  <h4 id=\"Usage-N101A1\">Usage</h4> <p>BagToString creates a single string from the elements of a bag, similar to SQL's <span class=\"codefrag\">GROUP_CONCAT</span> function. Keep in mind the following:</p> <ul> <li>Bags can be of arbitrary size, while strings in Java cannot: you will either exhaust available memory or exceed the maximum number of characters (about 2 billion). One of the worst features a production job can have is thresholding behavior: everything will seem nearly fine until the data size of your largest bag grows from nearly-too-big to just-barely-too-big.</li> <li>Bags are disordered unless you explicitly apply a nested <span class=\"codefrag\">ORDER BY</span> operation as demonstrated below. A nested <span class=\"codefrag\">FOREACH</span> will preserve ordering, letting you order by one combination of fields then project out just the values you'd like to concatenate.</li> <li>The default string conversion is applied to each element. If the bags contents are not atoms (tuple, map, etc), this may be not be what you want. Use a nested <span class=\"codefrag\">FOREACH</span> to format values and then compose them with BagToString as shown below</li> </ul> <p>Examples:</p> <table class=\"ForrestTable\"> <tr> <th colspan=\"1\" rowspan=\"1\">vals</th> <th colspan=\"1\" rowspan=\"1\">delimiter</th> <th colspan=\"1\" rowspan=\"1\">BagToString(vals, delimiter)</th> <th colspan=\"1\" rowspan=\"1\">Notes</th> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">{('BOS'),('NYA'),('BAL')}</span></td> <td colspan=\"1\" rowspan=\"1\"></td> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">BOS_NYA_BAL</span></td> <td colspan=\"1\" rowspan=\"1\">If only one argument is given, the field is delimited with underscore characters</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">{('BOS'),('NYA'),('BAL')}</span></td> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">'|'</span></td> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">BOS|NYA|BAL</span></td> <td colspan=\"1\" rowspan=\"1\">But you can supply your own delimiter</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">{('BOS'),('NYA'),('BAL')}</span></td> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">''</span></td> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">BOSNYABAL</span></td> <td colspan=\"1\" rowspan=\"1\">Use an explicit empty string to just smush everything together</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">{(1),(2),(3)}</span></td> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">'|'</span></td> <td colspan=\"1\" rowspan=\"1\"><span class=\"codefrag\">1|2|3</span></td> <td colspan=\"1\" rowspan=\"1\">Elements are type-converted for you (but see examples below)</td> </tr> </table>  <h4 id=\"Examples\">Examples</h4> <p>Simple delimited strings are simple:</p> <pre class=\"code\">\nteam_parks = LOAD 'team_parks' AS (team_id:chararray, park_id:chararray, years:bag{(year_id:int)});\n\n-- BOS     BOS07   {(1995),(1997),(1996),(1998),(1999)}\n-- NYA     NYC16   {(1995),(1999),(1998),(1997),(1996)}\n-- NYA     NYC17   {(1998)}\n-- SDN     HON01   {(1997)}\n-- SDN     MNT01   {(1996),(1999)}\n-- SDN     SAN01   {(1999),(1997),(1998),(1995),(1996)}\n\nteam_parkslist = FOREACH (GROUP team_parks BY team_id) GENERATE\n  group AS team_id, BagToString(team_parks.park_id, ';');\n\n-- BOS     BOS07\n-- NYA     NYC17;NYC16\n-- SDN     SAN01;MNT01;HON01\n</pre> <p>The default handling of complex elements works, but probably isn't what you want.</p> <pre class=\"code\">\nteam_parkyearsugly = FOREACH (GROUP team_parks BY team_id) GENERATE\n  group AS team_id,\n  BagToString(team_parks.(park_id, years));\n\n-- BOS     BOS07_{(1995),(1997),(1996),(1998),(1999)}\n-- NYA     NYC17_{(1998)}_NYC16_{(1995),(1999),(1998),(1997),(1996)}\n-- SDN     SAN01_{(1999),(1997),(1998),(1995),(1996)}_MNT01_{(1996),(1999)}_HON01_{(1997)}\n</pre> <p>Instead, assemble it in pieces. In step 2, we sort on one field but process another; it remains in the sorted order.</p> <pre class=\"code\">\nteam_park_yearslist = FOREACH team_parks {\n  years_o = ORDER years BY year_id;\n  GENERATE team_id, park_id, SIZE(years_o) AS n_years, BagToString(years_o, '/') AS yearslist;\n};\nteam_parkyearslist = FOREACH (GROUP team_park_yearslist BY team_id) {\n  tpy_o = ORDER team_park_yearslist BY n_years DESC, park_id ASC;\n  tpy_f = FOREACH tpy_o GENERATE CONCAT(park_id, ':', yearslist);\n  GENERATE group AS team_id, BagToString(tpy_f, ';');\n  };\n\n-- BOS     BOS07:1995/1996/1997/1998/1999\n-- NYA     NYC16:1995/1996/1997/1998/1999;NYC17:1998\n-- SDN     SAN01:1995/1996/1997/1998/1999;MNT01:1996/1999;HON01:1997\n</pre>  <h3 id=\"bloom\">Bloom</h3> <p>Bloom filters are a common way to select a limited set of records before moving data for a join or other heavy weight operation.</p>  <h4 id=\"Syntax-N10268\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BuildBloom(String hashType, String mode, String vectorSize, String nbHash)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Bloom(String filename)</p> </td> </tr> </table>  <h4 id=\"Terms-N10287\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>hashtype</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The type of hash function to use. Valid values for the hash functions are 'jenkins' and 'murmur'.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>mode</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Will be ignored, though by convention it should be \"fixed\" or \"fixedsize\"</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>vectorSize</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The number of bits in the bloom filter.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>nbHash</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The number of hash functions used in constructing the bloom filter.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>filename</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>File containing the serialized Bloom filter.</p> </td> </tr> </table> <p>See <a href=\"http://en.wikipedia.org/wiki/Bloom_filter\">Bloom Filter</a> for a discussion of how to select the number of bits and the number of hash functions. </p>  <h4 id=\"Usage-N102E3\">Usage</h4> <p>Bloom filters are a common way to select a limited set of records before moving data for a join or other heavy weight operation. For example, if one wanted to join a very large data set L with a smaller set S, and it was known that the number of keys in L that will match with S is small, building a bloom filter on S and then applying it to L before the join can greatly reduce the number of records from L that have to be moved from the map to the reduce, thus speeding the join. </p> <p>The implementation uses Hadoop's bloom filters (org.apache.hadoop.util.bloom.BloomFilter) internally. </p>  <h4 id=\"Examples-N102F0\">Examples</h4> <pre class=\"code\">\n  define bb BuildBloom('128', '3', 'jenkins');\n  small = load 'S' as (x, y, z);\n  grpd = group small all;\n  fltrd = foreach grpd generate bb(small.x);\n  store fltrd in 'mybloom';\n  exec;\n  define bloom Bloom('mybloom');\n  large = load 'L' as (a, b, c);\n  flarge = filter large by bloom(L.a);\n  joined = join small by x, flarge by a;\n  store joined into 'results';\n</pre>  <h3 id=\"concat\">CONCAT</h3> <p>Concatenates two or more expressions of identical type.</p>  <h4 id=\"Syntax-N10307\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>CONCAT (expression, expression, [...expression])</p> </td> </tr> </table>  <h4 id=\"Terms-N1031C\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Any expression.</p> </td> </tr> </table>  <h4 id=\"Usage-N10339\">Usage</h4> <p>Use the CONCAT function to concatenate two or more expressions. The result values of the expressions must have identical types.</p> <p>If any subexpression is null, the resulting expression is null.</p>  <h4 id=\"Example-N10346\">Example</h4> <p>In this example, fields f1, an underscore string literal, f2 and f3 are concatenated.</p> <pre class=\"code\">\nA = LOAD 'data' as (f1:chararray, f2:chararray, f3:chararray);\n\nDUMP A;\n(apache,open,source)\n(hadoop,map,reduce)\n(pig,pig,latin)\n\nX = FOREACH A GENERATE CONCAT(f1, '_', f2,f3);\n\nDUMP X;\n(apache_opensource)\n(hadoop_mapreduce)\n(pig_piglatin)\n</pre>  <h3 id=\"count\">COUNT</h3> <p>Computes the number of elements in a bag. </p>  <h4 id=\"Syntax-N10360\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>COUNT(expression) </p> </td> </tr> </table>  <h4 id=\"Terms-N10374\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with data type bag.</p> </td> </tr> </table>  <h4 id=\"Usage-N10390\">Usage</h4> <p>Use the COUNT function to compute the number of elements in a bag. COUNT requires a preceding GROUP ALL statement for global counts and a GROUP BY statement for group counts.</p> <p> The COUNT function follows syntax semantics and ignores nulls. What this means is that a tuple in the bag will not be counted if the FIRST FIELD in this tuple is NULL. If you want to include NULL values in the count computation, use <a href=\"#count-star\">COUNT_STAR</a>. </p> <p> Note: You cannot use the tuple designator (*) with COUNT; that is, COUNT(*) will not work. </p>  <h4 id=\"Example-N103A4\">Example</h4> <p>In this example the tuples in the bag are counted (see the <a href=\"basic#group\">GROUP</a> operator for information about the field names in relation B).</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int,f2:int,f3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n\nB = GROUP A BY f1;\n\nDUMP B;\n(1,{(1,2,3)})\n(4,{(4,2,1),(4,3,3)})\n(7,{(7,2,5)})\n(8,{(8,3,4),(8,4,3)})\n\nX = FOREACH B GENERATE COUNT(A);\n\nDUMP X;\n(1L)\n(2L)\n(1L)\n(2L)\n</pre>  <h4 id=\"Types+Tables-N103B6\">Types Tables</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>COUNT </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> </tr> </table>  <h3 id=\"count-star\">COUNT_STAR</h3> <p>Computes the number of elements in a bag. </p>  <h4 id=\"Syntax-N10440\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>COUNT_STAR(expression) </p> </td> </tr> </table>  <h4 id=\"Terms-N10455\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with data type bag.</p> </td> </tr> </table>  <h4 id=\"Usage-N10472\">Usage</h4> <p>Use the COUNT_STAR function to compute the number of elements in a bag. COUNT_STAR requires a preceding GROUP ALL statement for global counts and a GROUP BY statement for group counts.</p> <p>COUNT_STAR includes NULL values in the count computation (unlike <a href=\"#count\">COUNT</a>, which ignores NULL values). </p>  <h4 id=\"Example-N10483\">Example</h4> <p>In this example COUNT_STAR is used to count the tuples in a bag.</p> <pre class=\"code\">\nX = FOREACH B GENERATE COUNT_STAR(A);\n</pre>  <h3 id=\"diff\">DIFF</h3> <p>Compares two fields in a tuple.</p>  <h4 id=\"Syntax-N1049D\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DIFF (expression, expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N104B1\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with any data type.</p> </td> </tr> </table>  <h4 id=\"Usage-N104CE\">Usage</h4> <p>The DIFF function takes two bags as arguments and compares them. Any tuples that are in one bag but not the other are returned in a bag. If the bags match, an empty bag is returned. If the fields are not bags then they will be wrapped in tuples and returned in a bag if they do not match, or an empty bag will be returned if the two records match. The implementation assumes that both bags being passed to the DIFF function will fit entirely into memory simultaneously. If this is not the case the UDF will still function but it will be VERY slow.</p>  <h4 id=\"Example-N104D8\">Example</h4> <p>In this example DIFF compares the tuples in two bags.</p> <pre class=\"code\">\nA = LOAD 'bag_data' AS (B1:bag{T1:tuple(t1:int,t2:int)},B2:bag{T2:tuple(f1:int,f2:int)});\n\nDUMP A;\n({(8,9),(0,1)},{(8,9),(1,1)})\n({(2,3),(4,5)},{(2,3),(4,5)})\n({(6,7),(3,7)},{(2,2),(3,7)})\n\nDESCRIBE A;\na: {B1: {T1: (t1: int,t2: int)},B2: {T2: (f1: int,f2: int)}}\n\nX = FOREACH A GENERATE DIFF(B1,B2);\n\ngrunt&gt; dump x;\n({(0,1),(1,1)})\n({})\n({(6,7),(2,2)})\n</pre>  <h3 id=\"isempty\">IsEmpty</h3> <p>Checks if a bag or map is empty.</p>  <h4 id=\"Syntax-N104F1\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>IsEmpty(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N10505\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with any data type.</p> </td> </tr> </table>  <h4 id=\"Usage-N10521\">Usage</h4> <p>The IsEmpty function checks if a bag or map is empty (has no data). The function can be used to filter data.</p>  <h4 id=\"Example-N1052A\">Example</h4> <p>In this example all students with an SSN but no name are located.</p> <pre class=\"code\">\nSSN = load 'ssn.txt' using PigStorage() as (ssn:long);\n\nSSN_NAME = load 'students.txt' using PigStorage() as (ssn:long, name:chararray);\n\n/* do a cogroup of SSN with SSN_Name */\nX = COGROUP SSN by ssn, SSN_NAME by ssn;\n\n/* only keep those ssn's for which there is no name */\nY = filter X by IsEmpty(SSN_NAME);\n</pre>  <h3 id=\"max\">MAX</h3> <p>Computes the maximum of the numeric values or chararrays in a single-column bag. MAX requires a preceding GROUP ALL statement for global maximums and a GROUP BY statement for group maximums.</p>  <h4 id=\"Syntax-N10543\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>MAX(expression) </p> </td> </tr> </table>  <h4 id=\"Terms-N10557\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with data types int, long, float, double, bigdecimal, biginteger, chararray, datetime or bytearray.</p> </td> </tr> </table>  <h4 id=\"Usage-N10573\">Usage</h4> <p>Use the MAX function to compute the maximum of the numeric values or chararrays in a single-column bag.</p> <p>The MAX function ignores NULL values.</p>  <h4 id=\"Example-N10580\">Example</h4> <p>In this example the maximum GPA for all terms is computed for each student (see the GROUP operator for information about the field names in relation B).</p> <pre class=\"code\">\nA = LOAD 'student' AS (name:chararray, session:chararray, gpa:float);\n\nDUMP A;\n(John,fl,3.9F)\n(John,wt,3.7F)\n(John,sp,4.0F)\n(John,sm,3.8F)\n(Mary,fl,3.8F)\n(Mary,wt,3.9F)\n(Mary,sp,4.0F)\n(Mary,sm,4.0F)\n\nB = GROUP A BY name;\n\nDUMP B;\n(John,{(John,fl,3.9F),(John,wt,3.7F),(John,sp,4.0F),(John,sm,3.8F)})\n(Mary,{(Mary,fl,3.8F),(Mary,wt,3.9F),(Mary,sp,4.0F),(Mary,sm,4.0F)})\n\nX = FOREACH B GENERATE group, MAX(A.gpa);\n\nDUMP X;\n(John,4.0F)\n(Mary,4.0F)\n</pre>  <h4 id=\"Types+Tables-N1058E\">Types Tables</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>datetime </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>MAX </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>datetime </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as double</p> </td> </tr> </table>  <h3 id=\"min\">MIN</h3> <p>Computes the minimum of the numeric values or chararrays in a single-column bag. MIN requires a preceding GROUP… ALL statement for global minimums and a GROUP … BY statement for group minimums.</p>  <h4 id=\"Syntax-N10648\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>MIN(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N1065C\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with data types int, long, float, double, bigdecimal, biginteger, chararray, datetime or bytearray.</p> </td> </tr> </table>  <h4 id=\"Usage-N10678\">Usage</h4> <p>Use the MIN function to compute the minimum of a set of numeric values or chararrays in a single-column bag.</p> <p>The MIN function ignores NULL values.</p>  <h4 id=\"Example-N10685\">Example</h4> <p>In this example the minimum GPA for all terms is computed for each student (see the GROUP operator for information about the field names in relation B).</p> <pre class=\"code\">\nA = LOAD 'student' AS (name:chararray, session:chararray, gpa:float);\n\nDUMP A;\n(John,fl,3.9F)\n(John,wt,3.7F)\n(John,sp,4.0F)\n(John,sm,3.8F)\n(Mary,fl,3.8F)\n(Mary,wt,3.9F)\n(Mary,sp,4.0F)\n(Mary,sm,4.0F)\n\nB = GROUP A BY name;\n\nDUMP B;\n(John,{(John,fl,3.9F),(John,wt,3.7F),(John,sp,4.0F),(John,sm,3.8F)})\n(Mary,{(Mary,fl,3.8F),(Mary,wt,3.9F),(Mary,sp,4.0F),(Mary,sm,4.0F)})\n\nX = FOREACH B GENERATE group, MIN(A.gpa);\n\nDUMP X;\n(John,3.7F)\n(Mary,3.8F)\n</pre>  <h4 id=\"Types+Tables-N10693\">Types Tables</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>datetime </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>MIN </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>datetime </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as double</p> </td> </tr> </table>  <h3 id=\"plucktuple\">PluckTuple</h3> <p>Allows the user to specify a string prefix, and then filter for the columns in a relation that begin with that prefix or match that regex pattern. Optionally, include flag 'false' to filter for columns that do not match that prefix or match that regex pattern</p>  <h4 id=\"Syntax-N1074B\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DEFINE pluck PluckTuple(expression1)</p> <p>DEFINE pluck PluckTuple(expression1,expression3)</p> <p>pluck(expression2)</p> </td> </tr> </table>  <h4 id=\"Terms-N10766\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A prefix to pluck by or an regex pattern to pluck by</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The fields to apply the pluck to, usually '*'</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression3</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A boolean flag to indicate whether to include or exclude matching columns</p> </td> </tr> </table>  <h4 id=\"Usage-N107A9\">Usage</h4> <p>Example:</p> <pre class=\"code\">\na = load 'a' as (x, y);\nb = load 'b' as (x, y);\nc = join a by x, b by x;\nDEFINE pluck PluckTuple('a::');\nd = foreach c generate FLATTEN(pluck(*));\ndescribe c;\nc: {a::x: bytearray,a::y: bytearray,b::x: bytearray,b::y: bytearray}\ndescribe d;\nd: {plucked::a::x: bytearray,plucked::a::y: bytearray}\nDEFINE pluckNegative PluckTuple('a::','false');\nd = foreach c generate FLATTEN(pluckNegative(*));\ndescribe d;\nd: {plucked::b::x: bytearray,plucked::b::y: bytearray}\n</pre>  <h3 id=\"size\">SIZE</h3> <p>Computes the number of elements based on any Pig data type. </p>  <h4 id=\"Syntax-N107C3\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SIZE(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N107D7\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with any data type.</p> </td> </tr> </table>  <h4 id=\"Usage-N107F3\">Usage</h4> <p>Use the SIZE function to compute the number of elements based on the data type (see the Types Tables below). SIZE includes NULL values in the size computation. SIZE is not algebraic.</p> <p>If the tested object is null, the SIZE function returns null.</p>  <h4 id=\"Example-N10800\">Example</h4> <p>In this example the number of characters in the first field is computed.</p> <pre class=\"code\">\nA = LOAD 'data' as (f1:chararray, f2:chararray, f3:chararray);\n(apache,open,source)\n(hadoop,map,reduce)\n(pig,pig,latin)\n\nX = FOREACH A GENERATE SIZE(f1);\n\nDUMP X;\n(6L)\n(6L)\n(3L)\n</pre>  <h4 id=\"Types+Tables-N1080E\">Types Tables</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns 1 </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns 1 </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns 1 </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns 1 </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns number of characters in the array </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns number of bytes in the array </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns number of fields in the tuple</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns number of tuples in bag </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>returns number of key/value pairs in map </p> </td> </tr> </table>  <h3 id=\"subtract\">SUBTRACT</h3> <p>Bags subtraction, SUBTRACT(bag1, bag2) = bags composed of bag1 elements not in bag2</p>  <h4 id=\"Syntax-N108CD\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SUBTRACT(expression, expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N108E1\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with data type bag.</p> </td> </tr> </table>  <h4 id=\"Usage-N108FE\">Usage</h4> <p>SUBTRACT takes two bags as arguments and returns a new bag composed of the tuples of first bag are not in the second bag.</p> <p>If null, bag arguments are replaced by empty bags.<br>If arguments are not bags, an IOException is thrown.</p> <p>The implementation assumes that both bags being passed to the SUBTRACT function will fit <strong>entirely into memory</strong> simultaneously, if this is not the case, SUBTRACT will still function but will be <strong>very</strong> slow.</p>  <h4 id=\"Example-N10916\">Example</h4> <p>In this example, SUBTRACT creates a new bag composed of B1 elements that are not in B2.</p> <pre class=\"code\">\nA = LOAD 'bag_data' AS (B1:bag{T1:tuple(t1:int,t2:int)},B2:bag{T2:tuple(f1:int,f2:int)});\n\nDUMP A;\n({(8,9),(0,1),(1,2)},{(8,9),(1,1)})\n({(2,3),(4,5)},{(2,3),(4,5)})\n({(6,7),(3,7),(3,7)},{(2,2),(3,7)})\n\nDESCRIBE A;\nA: {B1: {T1: (t1: int,t2: int)},B2: {T2: (f1: int,f2: int)}}\n\nX = FOREACH A GENERATE SUBTRACT(B1,B2);\n\nDUMP X;\n({(0,1),(1,2)})\n({})\n({(6,7)})\n</pre>  <h3 id=\"sum\">SUM</h3> <p>Computes the sum of the numeric values in a single-column bag. SUM requires a preceding GROUP ALL statement for global sums and a GROUP BY statement for group sums.</p>  <h4 id=\"Syntax-N1092F\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SUM(expression) </p> </td> </tr> </table>  <h4 id=\"Terms-N10943\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with data types int, long, float, double, bigdecimal, biginteger or bytearray cast as double.</p> </td> </tr> </table>  <h4 id=\"Usage-N1095F\">Usage</h4> <p>Use the SUM function to compute the sum of a set of numeric values in a single-column bag.</p> <p>The SUM function ignores NULL values.</p>  <h4 id=\"Example-N1096C\">Example</h4> <p>In this example the number of pets is computed. (see the GROUP operator for information about the field names in relation B).</p> <pre class=\"code\">\nA = LOAD 'data' AS (owner:chararray, pet_type:chararray, pet_num:int);\n\nDUMP A;\n(Alice,turtle,1)\n(Alice,goldfish,5)\n(Alice,cat,2)\n(Bob,dog,2)\n(Bob,cat,2) \n\nB = GROUP A BY owner;\n\nDUMP B;\n(Alice,{(Alice,turtle,1),(Alice,goldfish,5),(Alice,cat,2)})\n(Bob,{(Bob,dog,2),(Bob,cat,2)})\n\nX = FOREACH B GENERATE group, SUM(A.pet_num);\nDUMP X;\n(Alice,8L)\n(Bob,4L)\n</pre>  <h4 id=\"Types+Tables-N1097A\">Types Tables</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SUM </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as double </p> </td> </tr> </table>  <h3 id=\"tokenize\">TOKENIZE</h3> <p>Splits a string and outputs a bag of words. </p>  <h4 id=\"Syntax-N10A24\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TOKENIZE(expression [, 'field_delimiter']) </p> </td> </tr> </table>  <h4 id=\"Terms-N10A38\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with data type chararray.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'field_delimiter'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An optional field delimiter (in single quotes).</p> <p>If field_delimiter is null or not passed, the following will be used as delimiters: space [ ], double quote [ \" ], coma [ , ] parenthesis [ () ], star [ * ].</p> </td> </tr> </table>  <h4 id=\"Usage-N10A6A\">Usage</h4> <p>Use the TOKENIZE function to split a string of words (all words in a single tuple) into a bag of words (each word in a single tuple). </p>  <h4 id=\"Example-N10A74\">Example</h4> <p>In this example the strings in each row are split.</p> <pre class=\"code\">\nA  = LOAD 'data' AS (f1:chararray);\n\nDUMP A;\n(Here is the first string.)\n(Here is the second string.)\n(Here is the third string.)\n\nX = FOREACH A GENERATE TOKENIZE(f1);\n\nDUMP X;\n({(Here),(is),(the),(first),(string.)})\n({(Here),(is),(the),(second),(string.)})\n({(Here),(is),(the),(third),(string.)})\n</pre> <p>In this example a field delimiter is specified.</p> <pre class=\"code\">\n{code}\nA = LOAD 'data' AS (f1:chararray);\nB = FOREACH A TOKENIZE (f1,'||');\nDUMP B;\n{code} \n</pre> </div>   <h2 id=\"load-store-functions\">Load/Store Functions</h2> <div class=\"section\"> <p>Load/store functions determine how data goes into Pig and comes out of Pig. Pig provides a set of built-in load/store functions, described in the sections below. You can also write your own load/store functions (see <a href=\"udf\">User Defined Functions</a>).</p>  <h3 id=\"handling-compression\">Handling Compression</h3> <p>Support for compression is determined by the load/store function. PigStorage and TextLoader support gzip and bzip compression for both read (load) and write (store). BinStorage does not support compression.</p> <p>To work with gzip compressed files, input/output files need to have a .gz extension. Gzipped files cannot be split across multiple maps; this means that the number of maps created is equal to the number of part files in the input location.</p> <pre class=\"code\">\nA = load 'myinput.gz';\nstore A into 'myoutput.gz';\n</pre> <p>To work with bzip compressed files, the input/output files need to have a .bz or .bz2 extension. Because the compression is block-oriented, bzipped files can be split across multiple maps.</p> <pre class=\"code\">\nA = load 'myinput.bz';\nstore A into 'myoutput.bz';\n</pre> <p>Note: PigStorage and TextLoader correctly read compressed files as long as they are NOT CONCATENATED bz/bz2 FILES generated in this manner: </p> <ul> <li> <p>cat *.bz &gt; text/concat.bz </p> </li> <li> <p>cat *.bz2 &gt; text/concat.bz2</p> </li> </ul>  <p>If you use concatenated bzip files with your Pig jobs, you will NOT see a failure but the results will be INCORRECT.</p>   <h3 id=\"binstorage\">BinStorage</h3> <p>Loads and stores data in machine-readable format.</p>  <h4 id=\"Syntax-N10AD6\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BinStorage() </p> </td> </tr> </table>  <h4 id=\"Terms-N10AEA\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>none</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>no parameters</p> </td> </tr> </table>  <h4 id=\"Usage-N10B06\">Usage</h4> <p>Pig uses BinStorage to load and store the temporary data that is generated between multiple MapReduce jobs.</p> <ul> <li>BinStorage works with data that is represented on disk in machine-readable format. BinStorage does NOT support <a href=\"#handling-compression\">compression</a>.</li> <li>BinStorage supports multiple locations (files, directories, globs) as input.</li> </ul>  <p>Occasionally, users use BinStorage to store their own data. However, because BinStorage is a proprietary binary format, the original data is never in BinStorage - it is always a derivation of some other data.</p> <p>We have seen several examples of users doing something like this:</p> <pre class=\"code\">\na = load 'b.txt' as (id, f);\nb = group a by id;\nstore b into 'g' using BinStorage();\n</pre> <p>And then later:</p> <pre class=\"code\">\na = load 'g/part*' using BinStorage() as (id, d:bag{t:(v, s)});\nb = foreach a generate (double)id, flatten(d);\ndump b;\n</pre> <p>There is a problem with this sequence of events. The first script does not define data types and, as the result, the data is stored as a bytearray and a bag with a tuple that contains two bytearrays. The second script attempts to cast the bytearray to double; however, since the data originated from a different loader, it has no way to know the format of the bytearray or how to cast it to a different type. To solve this problem, Pig:</p> <ul> <li>Sends an error message when the second script is executed: \"ERROR 1118: Cannot cast bytes loaded from BinStorage. Please provide a custom converter.\"</li>  <li id=\"custom-converter\">Allows you to use a custom converter to perform the casting. <br> <pre class=\"code\">\na = load 'g/part*' using BinStorage('Utf8StorageConverter') as (id, d:bag{t:(v, s)});\nb = foreach a generate (double)id, flatten(d);\ndump b;\n</pre> </li> </ul>  <h4 id=\"Examples-N10B43\">Examples</h4> <p>In this example BinStorage is used with the LOAD and STORE functions.</p> <pre class=\"code\">\nA = LOAD 'data' USING BinStorage();\n\nSTORE X into 'output' USING BinStorage(); \n</pre> <p>In this example BinStorage is used to load multiple locations.</p> <pre class=\"code\">\nA = LOAD 'input1.bin, input2.bin' USING BinStorage();\n</pre> <p>BinStorage does not track data lineage. When Pig uses BinStorage to move data between MapReduce jobs, Pig can figure out the correct cast function to use and apply it. However, as shown in the example below, when you store data using BinStorage and then use a separate Pig Latin script to read data (thus loosing the type information), it is your responsibility to correctly cast the data before storing it using BinStorage. </p> <pre class=\"code\">\nraw = load 'sampledata' using BinStorage() as (col1,col2, col3);\n--filter out null columns\nA = filter raw by col1#'bcookie' is not null;\n\nB = foreach A generate col1#'bcookie'  as reqcolumn;\ndescribe B;\n--B: {regcolumn: bytearray}\nX = limit B 5;\ndump X;\n(36co9b55onr8s)\n(36co9b55onr8s)\n(36hilul5oo1q1)\n(36hilul5oo1q1)\n(36l4cj15ooa8a)\n\nB = foreach A generate (chararray)col1#'bcookie'  as convertedcol;\ndescribe B;\n--B: {convertedcol: chararray}\nX = limit B 5;\ndump X; \n()\n()\n()\n()\n()\n</pre>  <h3 id=\"jsonloadstore\">JsonLoader, JsonStorage</h3> <p>Load or store JSON data.</p>  <h4 id=\"Syntax-N10B6B\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>JsonLoader( ['schema'] ) </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>JsonStorage( ) </p> </td> </tr> </table>  <h4 id=\"Terms-N10B8A\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>schema</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An optional Pig schema, in single quotes.</p> </td> </tr> </table>  <h4 id=\"Usage-N10BA6\">Usage</h4> <p>Use JsonLoader to load JSON data. </p> <p>Use JsonStorage to store JSON data.</p> <p>Note that there is no concept of delimit in JsonLoader or JsonStorage. The data is encoded in standard JSON format. JsonLoader optionally takes a schema as the construct argument.</p>  <h4 id=\"Examples-N10BB6\">Examples</h4> <p>In this example data is loaded with a schema. </p> <pre class=\"code\">\na = load 'a.json' using JsonLoader('a0:int,a1:{(a10:int,a11:chararray)},a2:(a20:double,a21:bytearray),a3:[chararray]');  \n</pre> <p>In this example data is loaded without a schema; it assumes there is a .pig_schema (produced by JsonStorage) in the input directory. </p> <pre class=\"code\">\na = load 'a.json' using JsonLoader(); \n</pre>  <h3 id=\"pigdump\">PigDump</h3> <p>Stores data in UTF-8 format.</p>  <h4 id=\"Syntax-N10BD6\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PigDump() </p> </td> </tr> </table>  <h4 id=\"Terms-N10BEA\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>none</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>no parameters</p> </td> </tr> </table>  <h4 id=\"Usage-N10C06\">Usage</h4> <p>PigDump stores data as tuples in human-readable UTF-8 format. </p>  <h4 id=\"Example-N10C0F\">Example</h4> <p>In this example PigDump is used with the STORE function.</p> <pre class=\"code\">\nSTORE X INTO 'output' USING PigDump();\n</pre>  <h3 id=\"pigstorage\">PigStorage</h3> <p>Loads and stores data as structured text files.</p>  <h4 id=\"Syntax-N10C28\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PigStorage( [field_delimiter] , ['options'] ) </p> </td> </tr> </table>  <h4 id=\"Terms-N10C3C\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"field-delimiter\">field_delimiter</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The default field delimiter is tab ('\\t'). </p> <p>You can specify other characters as field delimiters; however, be sure to encase the characters in single quotes.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"pigstorage-options\">'options'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A string that contains space-separated options ('optionA optionB optionC')</p> <p>Currently supported options are:</p> <ul> <li>('schema') - Stores the schema of the relation using a hidden JSON file.</li> <li>('noschema') - Ignores a stored schema during the load.</li> <li>('tagsource') - (deprecated, Use tagPath instead) Add a first column indicates the input file of the record.</li> <li>('tagPath') - Add a first column indicates the input path of the record.</li> <li>('tagFile') - Add a first column indicates the input file name of the record.</li> </ul> </td> </tr> </table>  <h4 id=\"Usage-N10C85\">Usage</h4> <p>PigStorage is the default function used by Pig to load/store the data. PigStorage supports structured text files (in human-readable UTF-8 format) in compressed or uncompressed form (see <a href=\"#handling-compression\">Handling Compression</a>). All Pig <a href=\"basic#data-types\">data types</a> (both simple and complex) can be read/written using this function. The input data to the load can be a file, a directory or a glob.</p> <p> <strong>Load/Store Statements</strong> </p> <p>Load statements – PigStorage expects data to be formatted using field delimiters, either the tab character ('\\t') or other specified character.</p> <p>Store statements – PigStorage outputs data using field delimiters, either the tab character ('\\t') or other specified character, and the line feed record delimiter ('\\n'). </p> <p> <strong>Field/Record Delimiters</strong> </p> <p>Field Delimiters – For load and store statements the default field delimiter is the tab character ('\\t'). You can use other characters as field delimiters, but separators such as ^A or Ctrl-A should be represented in Unicode (\\u0001) using UTF-16 encoding (see Wikipedia <a href=\"http://en.wikipedia.org/wiki/ASCII\">ASCII</a>, <a href=\"http://en.wikipedia.org/wiki/Unicode\">Unicode</a>, and <a href=\"http://en.wikipedia.org/wiki/UTF-16\">UTF-16</a>).</p> <p>Record Deliminters – For load statements Pig interprets the line feed ( '\\n' ), carriage return ( '\\r' or CTRL-M) and combined CR + LF ( '\\r\\n' ) characters as record delimiters (do not use these characters as field delimiters). For store statements Pig uses the line feed ('\\n') character as the record delimiter.</p> <p> <strong>Schemas</strong> </p> <p>If the schema option is specified, a hidden \".pig_schema\" file is created in the output directory when storing data. It is used by PigStorage (with or without -schema) during loading to determine the field names and types of the data without the need for a user to explicitly provide the schema in an as clause, unless <span class=\"codefrag\">noschema</span> is specified. No attempt to merge conflicting schemas is made during loading. The first schema encountered during a file system scan is used. </p> <p>Additionally, if the schema option is specified, a \".pig_headers\" file is created in the output directory. This file simply lists the delimited aliases. This is intended to make export to tools that can read files with header lines easier (just cat the header to your data). </p> <p>If the schema option is NOT specified, a schema will not be written when storing data.</p> <p>If the noschema option is NOT specified, and a schema is found, it gets loaded when loading data.</p> <p>Note that regardless of whether or not you store the schema, you always need to specify the correct delimiter to read your data. If you store using delimiter \"#\" and then load using the default delimiter, your data will not be parsed correctly.</p> <p> <strong>Record Provenance</strong> </p> <p>If tagPath or tagFile option is specified, PigStorage will add a pseudo-column INPUT_FILE_PATH or INPUT_FILE_NAME respectively to the beginning of the record. As the name suggests, it is the input file path/name containing this particular record. Please note tagsource is deprecated.</p> <p> <strong>Complex Data Types</strong> </p> <p>The formats for complex data types are shown here:</p> <ul> <li> <a href=\"basic#tuple\">Tuple</a>: enclosed by (), items separated by \",\" <ul> <li>Non-empty tuple: (item1,item2,item3)</li> <li>Empty tuple is valid: ()</li> </ul> </li> <li> <a href=\"basic#bag\">Bag</a>: enclosed by {}, tuples separated by \",\" <ul> <li>Non-empty bag: {code}{(tuple1),(tuple2),(tuple3)}{code}</li> <li>Empty bag is valid: {}</li> </ul> </li> <li> <a href=\"basic#map\">Map</a>: enclosed by [], items separated by \",\", key and value separated by \"#\" <ul> <li>Non-empty map: [key1#value1,key2#value2]</li> <li>Empty map is valid: []</li> </ul> </li> </ul> <p>If load statement specify a schema, Pig will convert the complex type according to schema. If conversion fails, the affected item will be null (see <a href=\"basic#nulls\">Nulls and Pig Latin</a>). </p>  <h4 id=\"Examples-N10D12\">Examples</h4> <p>In this example PigStorage expects input.txt to contain tab-separated fields and newline-separated records. The statements are equivalent.</p> <pre class=\"code\">\nA = LOAD 'student' USING PigStorage('\\t') AS (name: chararray, age:int, gpa: float); \n\nA = LOAD 'student' AS (name: chararray, age:int, gpa: float);\n</pre> <p>In this example PigStorage stores the contents of X into files with fields that are delimited with an asterisk ( * ). The STORE statement specifies that the files will be located in a directory named output and that the files will be named part-nnnnn (for example, part-00000).</p> <pre class=\"code\">\nSTORE X INTO  'output' USING PigStorage('*');\n</pre> <p>In this example, PigStorage loads data with complex data type, a bag of map and double.</p> <pre class=\"code\">\na = load '1.txt' as (a0:{t:(m:map[int],d:double)});\n\n{([foo#1,bar#2],34.0),([white#3,yellow#4],45.0)} : valid\n{([foo#badint],baddouble)} : conversion fail for badint/baddouble, get {([foo#],)}\n{} : valid, empty bag\n</pre>  <h3 id=\"textloader\">TextLoader</h3> <p>Loads unstructured data in UTF-8 format.</p>  <h4 id=\"Syntax-N10D3A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TextLoader()</p> </td> </tr> </table>  <h4 id=\"Terms-N10D4F\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>none</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>no parameters</p> </td> </tr> </table>  <h4 id=\"Usage-N10D6B\">Usage</h4> <p>TextLoader works with unstructured data in UTF8 format. Each resulting tuple contains a single field with one line of input text. TextLoader also supports <a href=\"#handling-compression\">compression</a>.</p> <p>Currently, TextLoader support for compression is limited.</p> <p>TextLoader cannot be used to store data.</p>  <h4 id=\"Example-N10D7F\">Example</h4> <p>In this example TextLoader is used with the LOAD function.</p> <pre class=\"code\">\nA = LOAD 'data' USING TextLoader();\n</pre>  <h3 id=\"HBaseStorage\">HBaseStorage</h3> <p>Loads and stores data from an HBase table.</p>  <h4 id=\"Syntax-N10D98\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>HBaseStorage('columns', ['options'])</p> </td> </tr> </table>  <h4 id=\"Terms-N10DAD\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>columns</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A list of qualified HBase columns to read data from or store data to. The column family name and column qualifier are seperated by a colon (:). Only the columns used in the Pig script need to be specified. Columns are specified in one of three different ways as described below.</p> <ul> <li>Explicitly specify a column family and column qualifier (e.g., user_info:id). This will produce a scalar in the resultant tuple.</li> <li>Specify a column family and a portion of column qualifier name as a prefix followed by an asterisk (i.e., user_info:address_*). This approach is used to read one or more columns from the same column family with a matching descriptor prefix. The datatype for this field will be a map of column descriptor name to field value. Note that combining this style of prefix with a long list of fully qualified column descriptor names could cause perfomance degredation on the HBase scan. This will produce a Pig map in the resultant tuple with column descriptors as keys.</li> <li>Specify all the columns of a column family using the column family name followed by an asterisk (i.e., user_info:*). This will produce a Pig map in the resultant tuple with column descriptors as keys.</li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'options'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A string that contains space-separated options (‘-optionA=valueA -optionB=valueB -optionC=valueC’)</p> <p>Currently supported options are:</p> <ul> <li>-loadKey=(true|false) Load the row key as the first value in every tuple returned from HBase (default=false)</li> <li>-gt=minKeyVal Return rows with a rowKey greater than minKeyVal</li> <li>-lt=maxKeyVal Return rows with a rowKey less than maxKeyVal</li> <li>-regex=regex Return rows with a rowKey that match this regex on KeyVal</li> <li>-gte=minKeyVal Return rows with a rowKey greater than or equal to minKeyVal</li> <li>-lte=maxKeyVal Return rows with a rowKey less than or equal to maxKeyVal</li> <li>-limit=numRowsPerRegion Max number of rows to retrieve per region</li> <li>-caching=numRows Number of rows to cache (faster scans, more memory)</li> <li>-delim=delimiter Column delimiter in columns list (default is whitespace)</li> <li>-ignoreWhitespace=(true|false) When delim is set to something other than whitespace, ignore spaces when parsing column list (default=true)</li> <li>-caster=(HBaseBinaryConverter|Utf8StorageConverter) Class name of Caster to use to convert values (default=Utf8StorageConverter). The default caster can be overridden with the pig.hbase.caster config param. Casters must implement LoadStoreCaster.</li> <li>-noWAL=(true|false) During storage, sets the write ahead to false for faster loading into HBase (default=false). To be used with extreme caution since this could result in data loss (see <a href=\"http://hbase.apache.org/book.html#perf.hbase.client.putwal\">http://hbase.apache.org/book.html#perf.hbase.client.putwal</a>).</li> <li>-minTimestamp=timestamp Return cell values that have a creation timestamp greater or equal to this value</li> <li>-maxTimestamp=timestamp Return cell values that have a creation timestamp less than this value</li> <li>-timestamp=timestamp Return cell values that have a creation timestamp equal to this value</li> <li>-includeTimestamp=Record will include the timestamp after the rowkey on store (rowkey, timestamp, ...)</li> <li>-includeTombstone=Record will include a tombstone marker on store after the rowKey and timestamp (if included) (rowkey, [timestamp,] tombstone, ...)</li> </ul> </td> </tr> </table>  <h4 id=\"Usage-N10E26\">Usage</h4> <p>HBaseStorage stores and loads data from HBase. The function takes two arguments. The first argument is a space seperated list of columns. The second optional argument is a space seperated list of options. Column syntax and available options are listed above. Note that HBaseStorage always disable split combination.</p>  <h4 id=\"Load+Example\">Load Example</h4> <p>In this example HBaseStorage is used with the LOAD function with an explicit schema.</p> <pre class=\"code\">\nraw = LOAD 'hbase://SomeTableName'\n      USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(\n      'info:first_name info:last_name tags:work_* info:*', '-loadKey=true -limit=5') AS\n      (id:bytearray, first_name:chararray, last_name:chararray, tags_map:map[], info_map:map[]);\n</pre> <p>The datatypes of the columns are declared with the \"AS\" clause. The first_name and last_name columns are specified as fully qualified column names with a chararray datatype. The third specification of tags:work_* requests a set of columns in the tags column family that begin with \"work_\". There can be zero, one or more columns of that type in the HBase table. The type is specified as tags_map:map[]. This indicates that the set of column values returned will be accessed as a map, where the key is the column name and the value is the cell value of the column. The fourth column specification is also a map of column descriptors to cell values.</p> <p>When the type of the column is specified as a map in the \"AS\" clause, the map keys are the column descriptor names and the data type is chararray. The datatype of the columns values can be declared explicitly as shown in the examples below:</p> <ul> <li>tags_map[chararray] - In this case, the column values are all declared to be of type chararray</li> <li>tags_map[int] - In this case, the column values are all declared to be of type int.</li> </ul>  <h4 id=\"Store+Example\">Store Example</h4> <p>In this example HBaseStorage is used to store a relation into HBase.</p> <pre class=\"code\">\nA = LOAD 'hdfs_users' AS (id:bytearray, first_name:chararray, last_name:chararray);\nSTORE A INTO 'hbase://users_table' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(\n    'info:first_name info:last_name');\n</pre> <p>In the example above relation A is loaded from HDFS and stored in HBase. Note that the schema of relation A is a tuple of size 3, but only two column descriptor names are passed to the HBaseStorage constructor. This is because the first entry in the tuple is used as the HBase rowKey.</p>  <h3 id=\"AvroStorage\">AvroStorage</h3> <p>Loads and stores data from Avro files.</p>  <h4 id=\"Syntax-N10E6A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AvroStorage(['schema|record name'], ['options'])</p> </td> </tr> </table>  <h4 id=\"Terms-N10E7F\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>schema</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A JSON string specifying the Avro schema for the input. You may specify an explicit schema when storing data or when loading data. When you manually provide a schema, Pig will use the provided schema for serialization and deserialization. This means that you can provide an explicit schema when saving data to simplify the output (for example by removing nullable unions), or rename fields. This also means that you can provide an explicit schema when reading data to only read a subset of the fields in each record.</p> <p>See <a href=\"http://avro.apache.org/docs/current/spec.html\"> the Apache Avro Documentation</a> for more details on how to specify a valid schema.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>record name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>When storing a bag of tuples with AvroStorage, if you do not want to specify the full schema, you may specify the avro record name instead. (AvroStorage will determine that the argument isn't a valid schema definition and use it as a variable name instead.)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'options'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A string that contains space-separated options (‘-optionA valueA -optionB valueB -optionC ’)</p> <p>Currently supported options are:</p> <ul> <li>-namespace nameSpace or -n nameSpace Explicitly specify the namespace field in Avro records when storing data</li> <li>-schemfile schemaFile or -f schemaFile Specify the input (or output) schema from an external file. Pig assumes that the file is located on the default filesystem, but you may use an explicity URL to unambigously specify the location. (For example, if the data was on your local file system in /stuff/schemafile.avsc, you could specify \"-f file:///stuff/schemafile.avsc\" to specify the location. If the data was on HDFS under /yourdirectory/schemafile.avsc, you could specify \"-f hdfs:///yourdirectory/schemafile.avsc\"). Pig expects this to be a text file, containing a valid avro schema.</li> <li>-examplefile exampleFile or -e exampleFile Specify the input (or output) schema using another Avro file as an example. Pig assumes that the file is located on the default filesystem, but you may use and explicity URL to specify the location. Pig expects this to be an Avro data file.</li> <li>-allowrecursive or -r Specify whether to allow recursive schema definitions (the default is to throw an exception if Pig encounters a recursive schema). When reading objects with recursive definitions, Pig will translate Avro records to schema-less tuples; the Pig Schema for the object may not match the data exactly.</li> <li>-doublecolons or -d Specify how to handle Pig schemas that contain double-colons when writing data in Avro format. (When you join two bags in Pig, Pig will automatically label the fields in the output Tuples with names that contain double-colons). If you select this option, AvroStorage will translate names with double colons into names with double underscores. </li> </ul> </td> </tr> </table>  <h4 id=\"Usage-N10EDE\">Usage</h4> <p>AvroStorage stores and loads data from Avro files. Often, you can load and store data using AvroStorage without knowing much about the Avros serialization format. AvroStorage will attempt to automatically translate a pig schema and pig data to avro data, or avro data to pig data.</p> <p>By default, when you use AvoStorage to load data, AvroStorage will use depth first search to find a valid Avro file on the input path, then use the schema from that file to load the data. When you use AvroStorage to store data, AvroStorage will attempt to translate the Pig schema to an equivalent Avro schema. You can manually specify the schema by providing an explicit schema in Pig, loading a schema from an external schema file, or explicitly telling Pig to read the schema from a specific avro file.</p> <p>To compress your output with AvroStorage, you need to use the correct Avro properties for compression. For example, to enable compression using deflate level 5, you would specify</p> <pre class=\"code\">\nSET avro.output.codec 'deflate'\nSET avro.mapred.deflate.level 5\n</pre> <p>Valid values for avro.output.codec include deflate, snappy, and null.</p> <p>There are a few key differences between Avro and Pig data, and in some cases it helps to understand the differences between the Avro and Pig data models. Before writing Pig data to Avro (or creating Avro files to use in Pig), keep in mind that there might not be an equivalent Avro Schema for every Pig Schema (and vice versa):</p> <ul> <li> <strong>Recursive schema definitions</strong> You cannot define schemas recursively in Pig, but you can define schemas recursively in Avro.</li> <li> <strong>Allowed characters</strong> Pig schemas may sometimes contain characters like colons (\":\") that are illegal in Avro names.</li> <li> <strong>Unions</strong> In Avro, you can define an object that may be one of several different types (including complex types such as records). In Pig, you cannot.</li> <li> <strong>Enums</strong> Avro allows you to define enums to efficiently and abstractly represent categorical variable, but Pig does not.</li> <li> <strong>Fixed Length Byte Arrays</strong> Avro allows you to define fixed length byte arrays, but Pig does not.</li> <li> <strong>Nullable values</strong> In Pig, all types are nullable. In Avro, they are not. </li> </ul> <p>Here is how AvroStorage translates Pig values to Avro:</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"></td> <td colspan=\"1\" rowspan=\"1\">Original Pig Type</td> <td colspan=\"1\" rowspan=\"1\">Translated Avro Type</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Integers</td> <td colspan=\"1\" rowspan=\"1\">int</td> <td colspan=\"1\" rowspan=\"1\">[\"int\",\"null\"]</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Longs</td> <td colspan=\"1\" rowspan=\"1\">long</td> <td colspan=\"1\" rowspan=\"1\">[\"long,\"null\"]</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Floats</td> <td colspan=\"1\" rowspan=\"1\">float</td> <td colspan=\"1\" rowspan=\"1\">[\"float\",\"null\"]</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Doubles</td> <td colspan=\"1\" rowspan=\"1\">double</td> <td colspan=\"1\" rowspan=\"1\">[\"double\",\"null\"]</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Strings</td> <td colspan=\"1\" rowspan=\"1\">chararray</td> <td colspan=\"1\" rowspan=\"1\">[\"string\",\"null\"]</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Bytes</td> <td colspan=\"1\" rowspan=\"1\">bytearray</td> <td colspan=\"1\" rowspan=\"1\">[\"bytes\",\"null\"]</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Booleans</td> <td colspan=\"1\" rowspan=\"1\">boolean</td> <td colspan=\"1\" rowspan=\"1\">[\"boolean\",\"null\"]</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Tuples</td> <td colspan=\"1\" rowspan=\"1\">tuple</td> <td colspan=\"1\" rowspan=\"1\">The Pig Tuple schema will be translated to an union of and Avro record with an equivalent schem and null.</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Bags of Tuples</td> <td colspan=\"1\" rowspan=\"1\">bag</td> <td colspan=\"1\" rowspan=\"1\">The Pig Tuple schema will be translated to a union of an array of records with an equivalent schema and null.</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Maps</td> <td colspan=\"1\" rowspan=\"1\">map</td> <td colspan=\"1\" rowspan=\"1\">The Pig Tuple schema will be translated to a union of a map of records with an equivalent schema and null.</td> </tr> </table> <p>Here is how AvroStorage translates Avro values to Pig:</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"></td> <td colspan=\"1\" rowspan=\"1\">Original Avro Types</td> <td colspan=\"1\" rowspan=\"1\">Translated Pig Type</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Integers</td> <td colspan=\"1\" rowspan=\"1\">[\"int\",\"null\"] or \"int\"</td> <td colspan=\"1\" rowspan=\"1\">int</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Longs</td> <td colspan=\"1\" rowspan=\"1\">[\"long,\"null\"] or \"long\"</td> <td colspan=\"1\" rowspan=\"1\">long</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Floats</td> <td colspan=\"1\" rowspan=\"1\">[\"float\",\"null\"] or \"float\"</td> <td colspan=\"1\" rowspan=\"1\">float</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Doubles</td> <td colspan=\"1\" rowspan=\"1\">[\"double\",\"null\"] or \"double\"</td> <td colspan=\"1\" rowspan=\"1\">double</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Strings</td> <td colspan=\"1\" rowspan=\"1\">[\"string\",\"null\"] or \"string\"</td> <td colspan=\"1\" rowspan=\"1\">chararray</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Enums</td> <td colspan=\"1\" rowspan=\"1\">Either an enum or a union of an enum and null</td> <td colspan=\"1\" rowspan=\"1\">chararray</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Bytes</td> <td colspan=\"1\" rowspan=\"1\">[\"bytes\",\"null\"] or \"bytes\"</td> <td colspan=\"1\" rowspan=\"1\">bytearray</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Fixes</td> <td colspan=\"1\" rowspan=\"1\">Either a fixed length byte array, or a union of a fixed length array and null</td> <td colspan=\"1\" rowspan=\"1\">bytearray</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Booleans</td> <td colspan=\"1\" rowspan=\"1\">[\"boolean\",\"null\"] or \"boolean\"</td> <td colspan=\"1\" rowspan=\"1\">boolean</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Tuples</td> <td colspan=\"1\" rowspan=\"1\">Either a record type, or a union or a record and null</td> <td colspan=\"1\" rowspan=\"1\">tuple</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Bags of Tuples</td> <td colspan=\"1\" rowspan=\"1\">Either an array, or a union of an array and null</td> <td colspan=\"1\" rowspan=\"1\">bag</td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">Maps</td> <td colspan=\"1\" rowspan=\"1\">Either a map, or a union of a map and null</td> <td colspan=\"1\" rowspan=\"1\">map</td> </tr> </table> <p> In many cases, AvroStorage will automatically translate your data correctly and you will not need to provide any more information to AvroStorage. But sometimes, it may be convenient to manually provide a schema to AvroStorge. See the example selection below for examples on manually specifying a schema with AvroStorage. </p>  <h4 id=\"Load+Examples\">Load Examples</h4> <p>Suppose that you were provided with a file of avro data (located in 'stuff') with the following schema:</p> <pre class=\"code\">\n{\"type\" : \"record\",\n \"name\" : \"stuff\",\n \"fields\" : [\n   {\"name\" : \"label\", \"type\" : \"string\"}, \n   {\"name\" : \"value\", \"type\" : \"int\"},\n   {\"name\" : \"marketingPlans\", \"type\" : [\"string\", \"bytearray\", \"null\"]}\n  ]\n}\n</pre> <p>Additionally, suppose that you don't need the value of the field \"marketingPlans.\" (That's a good thing, because AvroStorage doesn't know how to translate that Avro schema to a Pig schema). To load only the fieds \"label\" and \"value\" into Pig, you can manually specify the schema passed to AvroStorage:</p> <pre class=\"code\">\nmeasurements = LOAD 'stuff' USING AvroStorage(\n  '{\"type\":\"record\",\"name\":\"measurement\",\"fields\":[{\"name\":\"label\",\"type\":\"string\"},{\"name\":\"value\",\"type\":\"int\"}]}'\n  );\n</pre>  <h4 id=\"Store+Examples\">Store Examples</h4> <p>Suppose that you are saving a bag called measurements with the schema:</p> <pre class=\"code\">\nmeasurements:{measurement:(label:chararray,value:int)}\n</pre> <p>To store this bag into a file called \"measurements\", you can use a statement like:</p> <pre class=\"code\">\nSTORE measurements INTO 'measurements' USING AvroStorage('measurement');\n</pre> <p>AvroStorage will translate this to the Avro schema</p> <pre class=\"code\">\n{\"type\":\"record\", \n \"name\":\"measurement\",\n \"fields\" : [\n   {\"name\" : \"label\", \"type\" : [\"string\", \"null\"]}, \n   {\"name\" : \"value\", \"type\" : [\"int\", \"null\"]}\n  ]\n} \n</pre> <p>But suppose that you knew that the label and value fields would never be null. You could define a more precise schema manually using a statement like:</p> <pre class=\"code\">\nSTORE measurements INTO 'measurements' USING AvroStorage(\n  '{\"type\":\"record\",\"name\":\"measurement\",\"fields\":[{\"name\":\"label\",\"type\":\"string\"},{\"name\":\"value\",\"type\":\"int\"}]}'\n  );\n</pre>  <h3 id=\"TrevniStorage\">TrevniStorage</h3> <p>Loads and stores data from Trevni files.</p>  <h4 id=\"Syntax-N1111A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TrevniStorage(['schema|record name'], ['options'])</p> </td> </tr> </table> <p>Trevni is a column-oriented storage format that is part of the Apache Avro project. Trevni is closely related to Avro.</p> <p>Likewise, TrevniStorage is very closely related to AvroStorage, and shares the same options as AvroStorage. See <a href=\"#AvroStorage\">AvroStorage</a> for a detailed description of the arguments for TrevniStorage.</p>  <h3 id=\"AccumuloStorage\">AccumuloStorage</h3> <p>Loads or stores data from an Accumulo table. The first element in a Tuple is equivalent to the \"row\" from the Accumulo Key, while the columns in that row are can be grouped in various static or wildcarded ways. Basic wildcarding functionality exists to group various columns families/qualifiers into a Map for LOADs, or serialize a Map into some group of column families or qualifiers on STOREs. </p>  <h4 id=\"Syntax-N11145\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AccumuloStorage(['columns'[, 'options']])</p> </td> </tr> </table>  <h4 id=\"Arguments\">Arguments</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'columns'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A comma-separated list of \"columns\" to read data from to write data to. Each of these columns can be considered one of three different types: </p> <ol> <li>Literal</li> <li>Column family prefix</li> <li>Column qualifier prefix</li> </ol> <p> <strong>Literal:</strong> this is the simplest specification which is a colon-delimited string that maps to a column family and column qualifier. This will read/write a simple scalar from/to Accumulo. </p> <p> <strong>Column family prefix:</strong> When reading data, this will fetch data from Accumulo Key-Values in the current row whose column family match the given prefix. This will result in a Map being placed into the Tuple. When writing data, a Map is also expected at the given offset in the Tuple whose Keys will be appended to the column family prefix, an empty column qualifier is used, and the Map value will be placed in the Accumulo Value. A valid column family prefix is a literal asterisk (*) in which case the Map Key will be equivalent to the Accumulo column family. </p> <p> <strong>Column qualifier prefix:</strong> Similar to the column family prefix except it operates on the column qualifier. On reads, Accumulo Key-Values in the same row that match the given column family and column qualifier prefix will be placed into a single Map. On writes, the provided column family from the column specification will be used, the Map key will be appended to the column qualifier provided in the specification, and the Map Value will be the Accumulo Value. </p> <p>When \"columns\" is not provided or is a blank String, it is treated equivalently to \"*\". This is to say that when a column specification string is not provided, for reads, all columns in the given Accumulo row will be placed into a single Map (with the Map keys being colon delimited to preserve the column family/qualifier from Accumulo). For writes, the Map keys will be placed into the column family and the column qualifier will be empty. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'options'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A string that contains space-separated options (\"optionA valueA -optionB valueB -optionC valueC\")</p> <p>The currently supported options are:</p> <ul> <li>(-c|--caster) LoadStoreCasterImpl An implementation of a LoadStoreCaster to use when serializing types into Accumulo, usually AccumuloBinaryConverter or UTF8StringConverter, defaults to UTF8StorageConverter. </li> <li>(-auths|--authorizations) auth1,auth2... A comma-separated list of Accumulo authorizations to use when reading data from Accumulo. Defaults to the empty set of authorizations (none). </li> <li>(-s|--start) start_row The Accumulo row to begin reading from, inclusive</li> <li>(-e|--end) end_row The Accumulo row to read until, inclusive</li> <li>(-buff|--mutation-buffer-size) num_bytes The number of bytes to buffer when writing data to Accumulo. A higher value requires more memory</li> <li>(-wt|--write-threads) num_threads The number of threads used to write data to Accumulo.</li> <li>(-ml|--max-latency) milliseconds Maximum time in milliseconds before data is flushed to Accumulo.</li> <li>(-sep|--separator) str The separator character used when parsing the column specification, defaults to comma (,)</li> <li>(-iw|--ignore-whitespace) (true|false) Should whitespace be stripped from the column specification, defaults to true</li> </ul> </td> </tr> </table>  <h4 id=\"Usage-N111C9\">Usage</h4> <p>AccumuloStorage has the functionality to store or fetch data from Accumulo. Its goal is to provide a simple, widely applicable table schema compatible with Pig's API. Each Tuple contains some subset of the columns stored within one row of the Accumulo table, which depends on the columns provided as an argument to the function. If '*' is provided, all columns in the table will be returned. The second argument provides control over a variety of options that can be used to change various properties.</p> <p>When invoking Pig Scripts that use AccumuloStorage, it's important to ensure that Pig has the Accumulo jars on its classpath. This is easily achieved using the ACCUMULO_HOME environment variable. </p> <pre class=\"code\">\nPIG_CLASSPATH=\"$ACCUMULO_HOME/lib/*:$PIG_CLASSPATH\" pig my_script.pig\n</pre>  <h4 id=\"Load+Example-N111DA\">Load Example</h4> <p>It is simple to fetch all columns from Airport codes that fall between Boston and San Francisco that can be viewed with 'auth1' and/or 'auth2' Accumulo authorizations.</p> <pre class=\"code\">\nraw = LOAD 'accumulo://airports?instance=accumulo&amp;user=root&amp;password=passwd&amp;zookeepers=localhost'\n      USING org.apache.pig.backend.hadoop.accumulo.AccumuloStorage(\n      '*', '-a auth1,auth2 -s BOS -e SFO') AS\n      (code:chararray, all_columns:map[]);\n</pre> <p>The datatypes of the columns are declared with the \"AS\" clause. In this example, the row key, which is the unique airport code is assigned to the \"code\" variable while all of the other columns are placed into the map. When there is a non-empty column qualifier, the key in that map will have a colon which separates which portion of the key came from the column family and which portion came from the column qualifier. The Accumulo value is placed in the Map value.</p> <p>Most times, it is not necessary, nor desired for performance reasons, to fetch all columns.</p> <pre class=\"code\">\nraw = LOAD 'accumulo://airports?instance=accumulo&amp;user=root&amp;password=passwd&amp;zookeepers=localhost'\n      USING org.apache.pig.backend.hadoop.accumulo.AccumuloStorage(\n      'name,building:num_terminals,carrier*,reviews:transportation*') AS\n      (code:chararray name:bytearray carrier_map:map[] transportion_reviews_map:map[]);\n</pre> <p>An asterisk can be used when requesting columns to group a collection of columns into a single Map instead of enumerating each column.</p>  <h4 id=\"Store+Example-N111F5\">Store Example</h4> <p>Data can be easily stored into Accumulo.</p> <pre class=\"code\">\nA = LOAD 'flights.txt' AS (id:chararray, carrier_name:chararray, src_airport:chararray, dest_airport:chararray, tail_number:int);\nSTORE A INTO 'accumulo://flights?instance=accumulo&amp;user=root&amp;password=passwd&amp;zookeepers=localhost' USING \n    org.apache.pig.backend.hadoop.accumulo.AccumuloStorage('carrier_name,src_airport,dest_airport,tail_number');\n</pre> <p>Here, we read the file 'flights.txt' out of HDFS and store the results into the relation A. We extract a unique ID for the flight, its source and destination and the tail number from the given file. When STORE'ing back into Accumulo, we specify the column specifications (in this case, just a column family). It is also important to note that four elements are provided as columns because the first element in the Tuple is used as the row in Accumulo. </p>  <h3 id=\"OrcStorage\">OrcStorage</h3> <p>Loads from or stores data to Orc file.</p>  <h4 id=\"Syntax-N11210\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>OrcStorage(['options'])</p> </td> </tr> </table>  <h4 id=\"Options\">Options</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>A string that contains space-separated options (‘-optionA valueA -optionB valueB -optionC ’). Current options are only applicable with STORE operation and not for LOAD.</p> <p>Currently supported options are:</p> <ul> <li>--stripeSize or -s Set the stripe size for the file. Default is 268435456(256 MB).</li> <li>--rowIndexStride or -r Set the distance between entries in the row index. Default is 10000.</li> <li>--bufferSize or -b Set the size of the memory buffers used for compressing and storing the stripe in memory. Default is 262144 (256K).</li> <li>--blockPadding or -p Sets whether the HDFS blocks are padded to prevent stripes from straddling blocks. Default is true.</li> <li>--compress or -c Sets the generic compression that is used to compress the data. Valid codecs are: NONE, ZLIB, SNAPPY, LZO. Default is ZLIB.</li> <li>--version or -v Sets the version of the file that will be written</li> </ul> </td> </tr> </table>  <h4 id=\"Example-N11252\">Example</h4> <p>OrcStorage as a StoreFunc.</p> <pre class=\"code\">\nA = LOAD 'student.txt' as (name:chararray, age:int, gpa:double);\nstore A into 'student.orc' using OrcStorage('-c SNAPPY'); -- store student.txt into data.orc with SNAPPY compression\n</pre> <p>OrcStorage as a LoadFunc.</p> <pre class=\"code\">\nA = LOAD 'student.orc' USING OrcStorage();\ndescribe A; -- See the schema of student.orc\nB = filter A by age &gt; 25 and gpa &lt; 3; -- filter condition will be pushed up to loader\ndump B; -- dump the content of student.orc\n</pre>  <h4 id=\"Data+types\">Data types</h4> <p>Most Orc data type has one to one mapping to Pig data type. Several exceptions are:</p> <p>Loader side:</p> <ul> <li>Orc STRING/CHAR/VARCHAR all map to Pig varchar</li> <li>Orc BYTE/BINARY all map to Pig bytearray</li> <li>Orc TIMESTAMP/DATE all maps to Pig datetime</li> <li>Orc DECIMAL maps to Pig bigdecimal</li> </ul> <p>Storer side:</p> <ul> <li>Pig chararray maps to Orc STRING</li> <li>Pig datetime maps to Orc TIMESTAMP</li> <li>Pig bigdecimal/biginteger all map to Orc DECIMAL</li> <li>Pig bytearray maps to Orc BINARY</li> </ul>  <h4 id=\"Predicate+pushdown\">Predicate pushdown</h4> <p>If there is a filter statement right after OrcStorage, Pig will push the filter condition to the loader. OrcStorage will prune file/stripe/row group which does not satisfy the condition entirely. For the file/stripe/row group contains data that satisfies the filter condition, OrcStorage will load the file/stripe/row group and Pig will evaluate the filter condition again to remove additional data which does not satisfy the filter condition.</p> <p>OrcStorage predicate pushdown currently support all primitive data types but none of the complex data types. For example, map condition cannot push into OrcStorage:</p> <pre class=\"code\">\nA = LOAD 'student.orc' USING OrcStorage();\nB = filter A by info#'age' &gt; 25; -- map condition cannot push to OrcStorage\ndump B;\n</pre> <p>Currently, the following expressions in filter condition are supported in OrcStorage predicate pushdown: &gt;, &gt;=, &lt;, &lt;=, ==, !=, between, in, and, or, not. The missing expressions are: is null, is not null, matches.</p> </div>     <h2 id=\"math-functions\">Math Functions</h2> <div class=\"section\"> <p>For general information about these functions, see the <a href=\"http://docs.oracle.com/javase/6/docs/api/\">Java API Specification</a>, <a href=\"http://docs.oracle.com/javase/6/docs/api/java/lang/Math.html\">Class Math</a>. Note the following:</p> <ul> <li> <p>Pig function names are case sensitive and UPPER CASE.</p> </li> <li> <p>Pig may process results differently than as stated in the Java API Specification:</p> <ul> <li> <p>If the result value is null or empty, Pig returns null.</p> </li> <li> <p>If the result value is not a number (NaN), Pig returns null.</p> </li> <li> <p>If Pig is unable to process the expression, Pig returns an exception.</p> </li> </ul> </li> </ul>  <h3 id=\"abs\">ABS</h3> <p>Returns the absolute value of an expression.</p>  <h4 id=\"Syntax-N112F1\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ABS(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11305\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Any expression whose result is type int, long, float, or double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11321\">Usage</h4> <p> Use the ABS function to return the absolute value of an expression. If the result is not negative (x ≥ 0), the result is returned. If the result is negative (x &lt; 0), the negation of the result is returned. </p>  <h3 id=\"acos\">ACOS</h3> <p>Returns the arc cosine of an expression.</p>  <h4 id=\"Syntax-N11337\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ACOS(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N1134B\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11368\">Usage</h4> <p> Use the ACOS function to return the arc cosine of an expression. </p>  <h3 id=\"asin\">ASIN</h3> <p>Returns the arc sine of an expression.</p>  <h4 id=\"Syntax-N1137E\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ASIN(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11393\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N113B0\">Usage</h4> <p> Use the ASIN function to return the arc sine of an expression. </p>  <h3 id=\"atan\">ATAN</h3> <p>Returns the arc tangent of an expression.</p>  <h4 id=\"Syntax-N113C6\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ATAN(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N113DA\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N113F7\">Usage</h4> <p> Use the ATAN function to return the arc tangent of an expression. </p>  <h3 id=\"cbrt\">CBRT</h3> <p>Returns the cube root of an expression.</p>  <h4 id=\"Syntax-N1140D\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>CBRT(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11421\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N1143E\">Usage</h4> <p> Use the CBRT function to return the cube root of an expression. </p>  <h3 id=\"ceil\">CEIL</h3> <p>Returns the value of an expression rounded up to the nearest integer. </p>  <h4 id=\"Syntax-N11454\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>CEIL(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11468\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11485\">Usage</h4> <p> Use the CEIL function to return the value of an expression rounded up to the nearest integer. This function never decreases the result value. </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>x</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>CEIL(x)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 4.6</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 5</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 4</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 2.4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 3</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>1.0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>1</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-1.0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-1</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-2.4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-3</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-4.6</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-4</p> </td> </tr> </table>  <h3 id=\"cos\">COS</h3> <p>Returns the trigonometric cosine of an expression. </p>  <h4 id=\"Syntax-N11549\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>COS(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N1155D\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression (angle) whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N1157A\">Usage</h4> <p> Use the COS function to return the trigonometric cosine of an expression. </p>  <h3 id=\"cosh\">COSH</h3> <p>Returns the hyperbolic cosine of an expression. </p>  <h4 id=\"Syntax-N11590\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>COSH(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N115A4\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N115C1\">Usage</h4> <p> Use the COSH function to return the hyperbolic cosine of an expression. </p>  <h3 id=\"exp\">EXP</h3> <p>Returns Euler's number e raised to the power of x. </p>  <h4 id=\"Syntax-N115D7\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>EXP(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N115EB\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11608\">Usage</h4> <p> Use the EXP function to return the value of Euler's number e raised to the power of x (where x is the result value of the expression). </p>  <h3 id=\"floor\">FLOOR</h3> <p>Returns the value of an expression rounded down to the nearest integer. </p>  <h4 id=\"Syntax-N1161E\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>FLOOR(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11632\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N1164F\">Usage</h4> <p> Use the FLOOR function to return the value of an expression rounded down to the nearest integer. This function never increases the result value. </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>x</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>FLOOR(x)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 4.6</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 4</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 3</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 2.4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>1.0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>1</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-1.0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-1</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-2.4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-3</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-4</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-4.6</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-5</p> </td> </tr> </table>  <h3 id=\"log\">LOG</h3> <p>Returns the natural logarithm (base e) of an expression.</p>  <h4 id=\"Syntax-N11713\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LOG(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11727\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11744\">Usage</h4> <p> Use the LOG function to return the natural logarithm (base e) of an expression. </p>  <h3 id=\"log10\">LOG10</h3> <p>Returns the base 10 logarithm of an expression.</p>  <h4 id=\"Syntax-N1175A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LOG10(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N1176E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type double.</p> </td> </tr> </table>  <h4 id=\"Usage-N1178B\">Usage</h4> <p> Use the LOG10 function to return the base 10 logarithm of an expression. </p>  <h3 id=\"random\">RANDOM</h3> <p>Returns a pseudo random number.</p>  <h4 id=\"Syntax-N117A1\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>RANDOM( )</p> </td> </tr> </table>  <h4 id=\"Terms-N117B5\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>N/A</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>No terms.</p> </td> </tr> </table>  <h4 id=\"Usage-N117D2\">Usage</h4> <p> Use the RANDOM function to return a pseudo random number (type double) greater than or equal to 0.0 and less than 1.0. </p>  <h3 id=\"round\">ROUND</h3> <p>Returns the value of an expression rounded to an integer.</p>  <h4 id=\"Syntax-N117E8\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ROUND(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N117FC\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type float or double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11819\">Usage</h4> <p> Use the ROUND function to return the value of an expression rounded to an integer (if the result type is float) or rounded to a long (if the result type is double). </p> <p> Values are rounded towards positive infinity: <span class=\"codefrag\">round(x) = floor(x + 0.5)</span>. </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>x</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>ROUND(x)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 4.6</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 5</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 4</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 2.4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>1.0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>1</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-1.0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-1</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-2.4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-3</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-4.6</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-5</p> </td> </tr> </table>  <h3 id=\"round_to\">ROUND_TO</h3> <p>Returns the value of an expression rounded to a fixed number of decimal digits.</p>  <h4 id=\"Syntax-N118E3\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ROUND_TO(val, digits [, mode])</p> </td> </tr> </table>  <h4 id=\"Terms-N118F7\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>val</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type float or double: the value to round.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>digits</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is type int: the number of digits to preserve.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>mode</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> An optional int specifying the <a href=\"https://en.wikipedia.org/wiki/Rounding#Tie-breaking\">rounding mode</a>, according to the <a href=\"http://docs.oracle.com/javase/7/docs/api/constant-values.html#java.math\">constants Java provides</a>. </p> </td> </tr> </table>  <h4 id=\"Usage-N11942\">Usage</h4> <p> Use the ROUND function to return the value of an expression rounded to a fixed number of digits. Given a float, its result is a float; given a double its result is a double. </p> <p> The result is a multiple of the <span class=\"codefrag\">digits</span>-th power of ten: 0 leads to no fractional digits; a negative value zeros out correspondingly many places to the left of the decimal point. </p> <p> When <span class=\"codefrag\">mode</span> is omitted or has the value 6 (<a href=\"http://docs.oracle.com/javase/7/docs/api/java/math/RoundingMode.html#HALF_EVEN\"><span class=\"codefrag\">RoundingMode.HALF_EVEN</span></a>), the result is rounded towards the nearest neighbor, and ties are <a href=\"https://en.wikipedia.org/wiki/Rounding#Round_half_to_even\">rounded to the nearest even digit</a>. This mode minimizes cumulative error and tends to preserve the average of a set of values. </p> <p> When <span class=\"codefrag\">mode</span> has the value 4 (<a href=\"http://docs.oracle.com/javase/7/docs/api/java/math/RoundingMode.html#HALF_UP\"><span class=\"codefrag\">RoundingMode.HALF_UP</span></a>), the result is rounded towards the nearest neighbor, and ties are <a href=\"https://en.wikipedia.org/wiki/Rounding#Round_half_away_from_zero\">rounded away from zero</a>. This mode matches the behavior of most SQL systems. </p> <p> For other rounding modes, consult <a href=\"http://docs.oracle.com/javase/7/docs/api/java/math/RoundingMode.html\">Java's documentation</a>. There is no rounding mode that matches <span class=\"codefrag\">Math.round</span>'s behavior (i.e. round towards positive infinity) -- blame Java, not Pig. </p> <table class=\"ForrestTable\"> <tr> <th colspan=\"1\" rowspan=\"1\"> <p>val</p> </th> <th colspan=\"1\" rowspan=\"1\"> <p>digits</p> </th> <th colspan=\"1\" rowspan=\"1\"> <p>mode</p> </th> <th colspan=\"1\" rowspan=\"1\"> <p>ROUND_TO(val, digits)</p> </th> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 8</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 4</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-1</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1230.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-3</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1000.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 1234.1789</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-4</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.25000001</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 3.3</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.25</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 3.2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> -3.25</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> -3.2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.15</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 3.2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> -3.15</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> -3.2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.25</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 3.3</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> -3.25</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> -3.3</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 4.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> -3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> -4.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 2.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> 2.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> -2.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> -2.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 4.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> -3.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> -4.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> 2.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 3.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> -2.5</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> 0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>4</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> -3.0</p> </td> </tr> </table>  <h3 id=\"sin\">SIN</h3> <p>Returns the sine of an expression.</p>  <h4 id=\"Syntax-N11BF9\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SIN(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11C0D\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11C2A\">Usage</h4> <p> Use the SIN function to return the sine of an expession. </p>  <h3 id=\"sinh\">SINH</h3> <p>Returns the hyperbolic sine of an expression.</p>  <h4 id=\"Syntax-N11C40\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SINH(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11C54\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11C71\">Usage</h4> <p> Use the SINH function to return the hyperbolic sine of an expression. </p>  <h3 id=\"sqrt\">SQRT</h3> <p>Returns the positive square root of an expression.</p>  <h4 id=\"Syntax-N11C87\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SQRT(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11C9B\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11CB8\">Usage</h4> <p> Use the SQRT function to return the positive square root of an expression. </p>  <h3 id=\"tan\">TAN</h3> <p>Returns the trignometric tangent of an angle.</p>  <h4 id=\"Syntax-N11CCE\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TAN(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11CE2\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression (angle) whose result is double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11CFF\">Usage</h4> <p> Use the TAN function to return the trignometric tangent of an angle. </p>  <h3 id=\"tanh\">TANH</h3> <p>Returns the hyperbolic tangent of an expression. </p>  <h4 id=\"Syntax-N11D15\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TANH(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11D29\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is double.</p> </td> </tr> </table>  <h4 id=\"Usage-N11D46\">Usage</h4> <p> Use the TANH function to return the hyperbolic tangent of an expression. </p> </div>      <h2 id=\"string-functions\">String Functions</h2> <div class=\"section\"> <p>For general information about these functions, see the <a href=\"http://docs.oracle.com/javase/6/docs/api/\">Java API Specification</a>, <a href=\"http://docs.oracle.com/javase/6/docs/api/java/lang/String.html\">Class String</a>. Note the following:</p> <ul> <li> <p>Pig function names are case sensitive and UPPER CASE.</p> </li> <li> <p>Pig string functions have an extra, first parameter: the string to which all the operations are applied.</p> </li> <li> <p>Pig may process results differently than as stated in the Java API Specification. If any of the input parameters are null or if an insufficient number of parameters are supplied, NULL is returned.</p> </li> </ul>  <h3 id=\"endswith\">ENDSWITH</h3> <p>Tests inputs to determine if the first argument ends with the string in the second. </p>  <h4 id=\"Syntax-N11D8B\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ENDSWITH(string, testAgainst)</p> </td> </tr> </table>  <h4 id=\"Terms-N11D9F\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to be tested.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>testAgainst</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to test against.</p> </td> </tr> </table>  <h4 id=\"Usage-N11DCF\">Usage</h4> <p> Use the ENDSWITH function to determine if the first argument ends with the string in the second. </p> <p> For example, ENDSWITH ('foobar', 'foo') will false, whereas ENDSWITH ('foobar', 'bar') will return true. </p>  <h3 id=\"equalsignorecase\">EqualsIgnoreCase</h3> <p>Compares two Strings ignoring case considerations. </p>  <h4 id=\"Syntax-N11DE8\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>EqualsIgnoreCase(string1, string2)</p> </td> </tr> </table>  <h4 id=\"Terms-N11DFC\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The source string.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to compare against.</p> </td> </tr> </table>  <h4 id=\"Usage-N11E2C\">Usage</h4> <p> Use the EqualsIgnoreCase function to determine if two string are equal ignoring case. </p>  <h3 id=\"indexof\">INDEXOF</h3> <p>Returns the index of the first occurrence of a character in a string, searching forward from a start index. </p>  <h4 id=\"Syntax-N11E42\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>INDEXOF(string, 'character', startIndex)</p> </td> </tr> </table>  <h4 id=\"Terms-N11E57\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to be searched.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'character'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The character being searched for, in quotes. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>startIndex</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The index from which to begin the forward search. </p> <p>The string index begins with zero (0).</p> </td> </tr> </table>  <h4 id=\"Usage-N11E9D\">Usage</h4> <p> Use the INDEXOF function to determine the index of the first occurrence of a character in a string. The forward search for the character begins at the designated start index. </p>  <h3 id=\"last-index-of\">LAST_INDEX_OF</h3> <p>Returns the index of the last occurrence of a character in a string, searching backward from the end of the string. </p>  <h4 id=\"Syntax-N11EB3\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LAST_INDEX_OF(string, 'character')</p> </td> </tr> </table>  <h4 id=\"Terms-N11EC8\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to be searched.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'character'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The character being searched for, in quotes.</p> </td> </tr> </table>  <h4 id=\"Usage-N11EF8\">Usage</h4> <p> Use the LAST_INDEX_OF function to determine the index of the last occurrence of a character in a string. The backward search for the character begins at the end of the string. </p>  <h3 id=\"lcfirst\">LCFIRST</h3> <p>Converts the first character in a string to lower case. </p>  <h4 id=\"Syntax-N11F0E\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LCFIRST(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11F23\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result type is chararray.</p> </td> </tr> </table>  <h4 id=\"Usage-N11F40\">Usage</h4> <p> Use the LCFIRST function to convert only the first character in a string to lower case. </p>  <h3 id=\"lower\">LOWER</h3> <p>Converts all characters in a string to lower case. </p>  <h4 id=\"Syntax-N11F56\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LOWER(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11F6B\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result type is chararray.</p> </td> </tr> </table>  <h4 id=\"Usage-N11F88\">Usage</h4> <p> Use the LOWER function to convert all characters in a string to lower case. </p>  <h3 id=\"ltrim\">LTRIM</h3> <p>Returns a copy of a string with only leading white space removed.</p>  <h4 id=\"Syntax-N11F9E\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LTRIM(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N11FB2\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is chararray. </p> </td> </tr> </table>  <h4 id=\"Usage-N11FCF\">Usage</h4> <p> Use the LTRIM function to remove leading white space from a string. </p>  <h3 id=\"regex-extract\">REGEX_EXTRACT </h3> <p>Performs regular expression matching and extracts the matched group defined by an index parameter. </p>  <h4 id=\"Syntax-N11FE5\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>REGEX_EXTRACT (string, regex, index)</p> </td> </tr> </table>  <h4 id=\"Terms-N11FFA\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string in which to perform the match.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>regex</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The regular expression.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>index</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The index of the matched group to return.</p> </td> </tr> </table>  <h4 id=\"Usage-N1203D\">Usage</h4> <p> Use the REGEX_EXTRACT function to perform regular expression matching and to extract the matched group defined by the index parameter (where the index is a 1-based parameter.) The function uses Java regular expression form. </p> <p> The function returns a string that corresponds to the matched group in the position specified by the index. If there is no matched expression at that position, NULL is returned. </p>  <h4 id=\"Example-N1204A\">Example</h4> <p> This example will return the string '192.168.1.5'. </p> <pre class=\"code\">\nREGEX_EXTRACT('192.168.1.5:8020', '(.*):(.*)', 1);\n</pre>  <h3 id=\"regex-extract-all\">REGEX_EXTRACT_ALL </h3> <p>Performs regular expression matching and extracts all matched groups.</p>  <h4 id=\"Syntax-N12064\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>REGEX_EXTRACT_ALL (string, regex)</p> </td> </tr> </table>  <h4 id=\"Terms-N12079\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string in which to perform the match.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>regex</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The regular expression.</p> </td> </tr> </table>  <h4 id=\"Usage-N120A9\">Usage</h4> <p> Use the REGEX_EXTRACT_ALL function to perform regular expression matching and to extract all matched groups. The function uses Java regular expression form. </p> <p> The function returns a tuple where each field represents a matched expression. If there is no match, an empty tuple is returned. </p>  <h4 id=\"Example-N120B6\">Example</h4> <p> This example will return the tuple (192.168.1.5,8020). </p> <pre class=\"code\">\nREGEX_EXTRACT_ALL('192.168.1.5:8020', '(.*)\\:(.*)');\n</pre>  <h3 id=\"replace\">REPLACE</h3> <p>Replaces existing characters in a string with new characters.</p>  <h4 id=\"Syntax-N120D0\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>REPLACE(string, 'regExp', 'newChar');</p> </td> </tr> </table>  <h4 id=\"Terms-N120E5\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to be updated.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'regExp'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The regular expression to which the string is to be matched, in quotes.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'newChar'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The new characters replacing the existing characters, in quotes.</p> </td> </tr> </table>  <h4 id=\"Usage-N12128\">Usage</h4> <p> Use the REPLACE function to replace existing characters in a string with new characters. </p> <p> For example, to change \"open source software\" to \"open source wiki\" use this statement: REPLACE(string,'software','wiki') </p> <p> Note that the REPLACE function is internally implemented using <a href=\"http://docs.oracle.com/javase/6/docs/api/java/lang/String.html#replaceAll(java.lang.String,%20java.lang.String)\"> java.string.replaceAll(String regex, String replacement)</a> where 'regExp' and 'newChar' are passed as the 1st and 2nd argument respectively. If you want to replace <a href=\"http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html#bs\"> special characters</a> such as '[' in the string literal, it is necessary to escape them in 'regExp' by prefixing them with double backslashes (e.g. '\\\\['). </p>  <h3 id=\"rtrim\">RTRIM</h3> <p>Returns a copy of a string with only trailing white space removed.</p>  <h4 id=\"Syntax-N1214C\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>RTRIM(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N12160\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is chararray. </p> </td> </tr> </table>  <h4 id=\"Usage-N1217D\">Usage</h4> <p> Use the RTRIM function to remove trailing white space from a string. </p>  <h3 id=\"sprintf\">SPRINTF</h3> <p>Formats a set of values according to a printf-style template, using the <a href=\"http://docs.oracle.com/javase/7/docs/api/java/util/Formatter.html\">native Java Formatter</a> library.</p>  <h4 id=\"Syntax-N12197\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SPRINTF(format, [...vals])</p> </td> </tr> </table>  <h4 id=\"Terms-N121AB\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>format</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The printf-style string describing the template.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>vals</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> The values to place in the template. There must be a tuple element for each formatting placeholder, and it must have the correct type: <span class=\"codefrag\">int</span> or <span class=\"codefrag\">long</span> for integer formats such as <span class=\"codefrag\">%d</span>; <span class=\"codefrag\">float</span> or <span class=\"codefrag\">double</span> for decimal formats such as <span class=\"codefrag\">%f</span>; and <span class=\"codefrag\">long</span> for date/time formats such as <span class=\"codefrag\">%t</span>. </p> </td> </tr> </table>  <h4 id=\"Usage-N121F3\">Usage</h4> <p> Use the SPRINTF function to format a string according to a template. For example, SPRINTF(\"part-%05d\", 69) will return 'part-00069'. </p> <table class=\"ForrestTable\"> <tr> <th colspan=\"1\" rowspan=\"1\"> <p>String format specification</p> </th> <th colspan=\"1\" rowspan=\"1\"> <p>arg1</p> </th> <th colspan=\"1\" rowspan=\"1\"> <p>arg2</p> </th> <th colspan=\"1\" rowspan=\"1\"> <p>arg3</p> </th> <th colspan=\"1\" rowspan=\"1\"> <p>SPRINTF(format, arg1, arg2)</p> </th> <th colspan=\"1\" rowspan=\"1\"> <p>notes</p> </th> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'%8s|%8d|%-8s'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'yay'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">' 1234567| 1234567|yay '</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Format strings with %s, integers with %d. Types are converted for you where reasonable (here, int -&gt; string).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>(null value)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'yay'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(null value)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Returns null (no error or warning) with a null format string.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'%8s|%8d|%-8s'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(null value)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'yay'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(null value)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Returns null (no error or warning) if any single argument is null.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'%8.3f|%6x'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">123.14159</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">665568</span> </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">' 123.142| a27e0'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Format floats/doubles with %f, hexadecimal integers with %x (there are others besides -- see the <a href=\"http://docs.oracle.com/javase/7/docs/api/java/util/Formatter.html\">Java docs</a>)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'%,+10d|%(06d'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">-123</span> </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'+1,234,567|(0123)'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Numerics take a prefix modifier: <span class=\"codefrag\">,</span> for locale-specific thousands-delimiting, 0 for zero-padding; <span class=\"codefrag\">+</span> to always show a plus sign for positive numbers; space  to allow a space preceding positive numbers; <span class=\"codefrag\">(</span> to indicate negative numbers with parentheses (accountant-style).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'%2$5d: %3$6s %1$3s %2$4x (%&lt;4X)'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'the'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">48879</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'wheres'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'48879: wheres the beef (BEEF)'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Refer to args positionally and as many times as you like using <span class=\"codefrag\">%(pos)$...</span>. Use <span class=\"codefrag\">%&lt;...</span> to refer to the previously-specified arg.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'Launch Time: %14d %s'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">ToMilliSeconds(CurrentTime())</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">ToString(CurrentTime(), 'yyyy-MM-dd HH:mm:ss Z')</span> </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'Launch Time: 1400164132000 2014-05-15 09:28:52 -0500'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Instead use ToString to format the date/time portions and SPRINTF to layout the results.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'%8s|%-8s'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">MissingFormatArgumentException: Format specifier '%-8s' </span> </p> </td>\n<td colspan=\"1\" rowspan=\"1\"> <p>You must supply arguments for all specifiers</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'%8s'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'ignored'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\">'also'</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <span class=\"codefrag\"> 1234567</span> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>It's OK to supply too many, though</p> </td> </tr> </table> <p> <em>Note: although the Java formatter (and thus this function) offers the <span class=\"codefrag\">%t</span> specifier for date/time elements, it's best avoided: it's cumbersome, the output and timezone handling may differ from what you expect, and it doesn't accept datetime objects from pig. Instead, just prepare dates usint the ToString UDF as shown.</em> </p>  <h3 id=\"startswith\">STARTSWITH</h3> <p>Tests inputs to determine if the first argument starts with the string in the second. </p>  <h4 id=\"Syntax-N123C0\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>STARTSWITH(string, testAgainst)</p> </td> </tr> </table>  <h4 id=\"Terms-N123D4\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to be tested.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>testAgainst</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to test against.</p> </td> </tr> </table>  <h4 id=\"Usage-N12404\">Usage</h4> <p> Use the STARTSWITH function to determine if the first argument starts with the string in the second. </p> <p> For example, STARTSWITH ('foobar', 'foo') will true, whereas STARTSWITH ('foobar', 'bar') will return false. </p>  <h3 id=\"strsplit\">STRSPLIT</h3> <p>Splits a string around matches of a given regular expression. </p>  <h4 id=\"Syntax-N1241D\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>STRSPLIT(string, regex, limit)</p> </td> </tr> </table>  <h4 id=\"Terms-N12431\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to be split.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>regex</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The regular expression.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>limit</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If the value is positive, the pattern (the compiled representation of the regular expression) is applied at most limit-1 times, therefore the value of the argument means the maximum length of the result tuple. The last element of the result tuple will contain all input after the last match.</p> <p>If the value is negative, no limit is applied for the length of the result tuple.</p> <p>If the value is zero, no limit is applied for the length of the result tuple too, and trailing empty strings (if any) will be removed.</p> </td> </tr> </table>  <h4 id=\"Usage-N1247A\">Usage</h4> <p> Use the STRSPLIT function to split a string around matches of a given regular expression. </p> <p> For example, given the string (open:source:software), STRSPLIT (string, ':',2) will return ((open,source:software)) and STRSPLIT (string, ':',3) will return ((open,source,software)). </p>  <h3 id=\"strsplittobag\">STRSPLITTOBAG</h3> <p>Splits a string around matches of a given regular expression and returns a databag</p>  <h4 id=\"Syntax-N12493\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>STRSPLITTOBAG(string, regex, limit)</p> </td> </tr> </table>  <h4 id=\"Terms-N124A8\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string to be split.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>regex</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The regular expression.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>limit</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If the value is positive, the pattern (the compiled representation of the regular expression) is applied at most limit-1 times, therefore the value of the argument means the maximum size of the result bag. The last tuple of the result bag will contain all input after the last match. </p> <p>If the value is negative, no limit is applied to the size of the result bag.</p> <p>If the value is zero, no limit is applied to the size of the result bag too, and trailing empty strings (if any) will be removed. </p> </td> </tr> </table>  <h4 id=\"Usage-N124F1\">Usage</h4> <p> Use the STRSPLITTOBAG function to split a string around matches of a given regular expression. </p> <p> For example, given the string (open:source:software), STRSPLITTOBAG (string, ':',2) will return {(open),(source:software)} and STRSPLITTOBAG (string, ':',3) will return {(open),(source),(software)}. </p>  <h3 id=\"substring\">SUBSTRING</h3> <p>Returns a substring from a given string. </p>  <h4 id=\"Syntax-N1250A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SUBSTRING(string, startIndex, stopIndex)</p> </td> </tr> </table>  <h4 id=\"Terms-N1251E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The string from which a substring will be extracted.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>startIndex</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The index (type integer) of the first character of the substring.</p> <p>The index of a string begins with zero (0).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>stopIndex</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The index (type integer) of the character <em>following</em> the last character of the substring.</p> </td> </tr> </table>  <h4 id=\"Usage-N12567\">Usage</h4> <p> Use the SUBSTRING function to return a substring from a given string. </p> <p> Given a field named alpha whose value is ABCDEF, to return substring BCD use this statement: SUBSTRING(alpha,1,4). Note that 1 is the index of B (the first character of the substring) and 4 is the index of E (the character <em>following</em> the last character of the substring). </p>  <h3 id=\"trim\">TRIM</h3> <p>Returns a copy of a string with leading and trailing white space removed.</p>  <h4 id=\"Syntax-N12583\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TRIM(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N12597\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result is chararray. </p> </td> </tr> </table>  <h4 id=\"Usage-N125B4\">Usage</h4> <p> Use the TRIM function to remove leading and trailing white space from a string. </p>  <h3 id=\"ucfirst\">UCFIRST</h3> <p>Returns a string with the first character converted to upper case. </p>  <h4 id=\"Syntax-N125CA\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>UCFIRST(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N125DE\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result type is chararray.</p> </td> </tr> </table>  <h4 id=\"Usage-N125FB\">Usage</h4> <p> Use the UCFIRST function to convert only the first character in a string to upper case. </p>  <h3 id=\"upper\">UPPER</h3> <p>Returns a string converted to upper case. </p>  <h4 id=\"Syntax-N12611\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>UPPER(expression)</p> </td> </tr> </table>  <h4 id=\"Terms-N12625\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression whose result type is chararray. </p> </td> </tr> </table>  <h4 id=\"Usage-N12642\">Usage</h4> <p> Use the UPPER function to convert all characters in a string to upper case. </p>  <h3 id=\"uniqueid\">UniqueID</h3> <p>Returns a unique id string for each record in the alias. </p>  <h4 id=\"Usage-N12656\">Usage</h4> <p> UniqueID generates a unique id for each records. The id takes form \"taskindex-sequence\" </p> </div>      <h2 id=\"datetime-functions\">Datetime Functions</h2> <div class=\"section\"> <p> For general information about datetime type operations, see the <a href=\"http://docs.oracle.com/javase/6/docs/api/\">Java API Specification</a>, <a href=\"http://docs.oracle.com/javase/6/docs/api/java/util/Date.html\">Java Date class</a>, and <a href=\"http://joda-time.sourceforge.net/apidocs/index.html\">JODA DateTime class</a>. And for the information of ISO date and time formats, please refer to <a href=\"http://www.w3.org/TR/NOTE-datetime\">Date and Time Formats</a>. </p>  <h3 id=\"add-duration\">AddDuration</h3> <p>Returns the result of a DateTime object plus a <a href=\"http://en.wikipedia.org/wiki/ISO_8601#Durations\">Duration object</a>.</p>  <h4 id=\"Syntax-N12692\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AddDuration(datetime, duration)</p> </td> </tr> </table>  <h4 id=\"Terms-N126A7\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>duration</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The duration string in <a href=\"http://en.wikipedia.org/wiki/ISO_8601#Durations\">ISO 8601 format</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N126DB\">Usage</h4> <p> Use the AddDuration function to created a new datetime object by add some duration to a given datetime object. </p>  <h3 id=\"current-time\">CurrentTime</h3> <p>Returns the DateTime object of the current time.</p>  <h4 id=\"Syntax-N126F1\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>CurrentTime()</p> </td> </tr> </table>  <h4 id=\"Usage-N12706\">Usage</h4> <p> Use the CurrentTime function to generate a datetime object of current timestamp with millisecond accuracy. </p>  <h3 id=\"days-between\">DaysBetween</h3> <p>Returns the number of days between two DateTime objects.</p>  <h4 id=\"Syntax-N1271C\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DaysBetween(datetime1, datetime2)</p> </td> </tr> </table>  <h4 id=\"Terms-N12731\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Another datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12761\">Usage</h4> <p> Use the DaysBetween function to get the number of days between the two given datetime objects. </p>  <h3 id=\"get-day\">GetDay</h3> <p>Returns the day of a month from a DateTime object.</p>  <h4 id=\"Syntax-N12777\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetDay(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N1278C\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N127A9\">Usage</h4> <p> Use the GetDay function to extract the day of a month from the given datetime object. </p>  <h3 id=\"get-hour\">GetHour</h3> <p>Returns the hour of a day from a DateTime object.</p>  <h4 id=\"Syntax-N127BF\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetHour(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N127D4\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N127F1\">Usage</h4> <p> Use the GetHour function to extract the hour of a day from the given datetime object. </p>  <h3 id=\"get-milli-second\">GetMilliSecond</h3> <p>Returns the millisecond of a second from a DateTime object.</p>  <h4 id=\"Syntax-N12807\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetMilliSecond(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N1281C\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12839\">Usage</h4> <p> Use the GetMilliSecond function to extract the millsecond of a second from the given datetime object. </p>  <h3 id=\"get-minute\">GetMinute</h3> <p>Returns the minute of a hour from a DateTime object.</p>  <h4 id=\"Syntax-N1284F\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetMinute(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N12864\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12881\">Usage</h4> <p> Use the GetMinute function to extract the minute of a hour from the given datetime object. </p>  <h3 id=\"get-month\">GetMonth</h3> <p>Returns the month of a year from a DateTime object.</p>  <h4 id=\"Syntax-N12897\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetMonth(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N128AC\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N128C9\">Usage</h4> <p> Use the GetMonth function to extract the month of a year from the given datetime object. </p>  <h3 id=\"get-second\">GetSecond</h3> <p>Returns the second of a minute from a DateTime object.</p>  <h4 id=\"Syntax-N128DF\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetSecond(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N128F4\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12911\">Usage</h4> <p> Use the GetSecond function to extract the second of a minute from the given datetime object. </p>  <h3 id=\"get-week\">GetWeek</h3> <p>Returns the week of a week year from a DateTime object.</p>  <h4 id=\"Syntax-N12927\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetWeek(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N1293C\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12959\">Usage</h4> <p> Use the GetWeek function to extract the week of a week year from the given datetime object. Note that week year may be different from year. </p>  <h3 id=\"get-week-year\">GetWeekYear</h3> <p>Returns the week year from a DateTime object.</p>  <h4 id=\"Syntax-N1296F\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetWeekYear(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N12984\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N129A1\">Usage</h4> <p> Use the GetWeekYear function to extract the week year from the given datetime object. Note that week year may be different from year. </p>  <h3 id=\"get-year\">GetYear</h3> <p>Returns the year from a DateTime object.</p>  <h4 id=\"Syntax-N129B7\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>GetYear(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N129CC\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N129E9\">Usage</h4> <p> Use the GetYear function to extract the year from the given datetime object. </p>  <h3 id=\"hours-between\">HoursBetween</h3> <p>Returns the number of hours between two DateTime objects.</p>  <h4 id=\"Syntax-N129FF\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>HoursBetween(datetime1, datetime2)</p> </td> </tr> </table>  <h4 id=\"Terms-N12A14\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Another datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12A44\">Usage</h4> <p> Use the HoursBetween function to get the number of hours between the two given datetime objects. </p>  <h3 id=\"milli-seconds-between\">MilliSecondsBetween</h3> <p>Returns the number of milliseconds between two DateTime objects.</p>  <h4 id=\"Syntax-N12A5A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>MilliSecondsBetween(datetime1, datetime2)</p> </td> </tr> </table>  <h4 id=\"Terms-N12A6F\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Another datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12A9F\">Usage</h4> <p> Use the MilliSecondsBetween function to get the number of millseconds between the two given datetime objects. </p>  <h3 id=\"minutes-between\">MinutesBetween</h3> <p>Returns the number of minutes between two DateTime objects.</p>  <h4 id=\"Syntax-N12AB5\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>MinutesBetween(datetime1, datetime2)</p> </td> </tr> </table>  <h4 id=\"Terms-N12ACA\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Another datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12AFA\">Usage</h4> <p> Use the MinutsBetween function to get the number of minutes between the two given datetime objects. </p>  <h3 id=\"months-between\">MonthsBetween</h3> <p>Returns the number of months between two DateTime objects.</p>  <h4 id=\"Syntax-N12B10\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>MonthsBetween(datetime1, datetime2)</p> </td> </tr> </table>  <h4 id=\"Terms-N12B25\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Another datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12B55\">Usage</h4> <p> Use the MonthsBetween function to get the number of months between the two given datetime objects. </p>  <h3 id=\"seconds-between\">SecondsBetween</h3> <p>Returns the number of seconds between two DateTime objects.</p>  <h4 id=\"Syntax-N12B6B\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SecondsBetween(datetime1, datetime2)</p> </td> </tr> </table>  <h4 id=\"Terms-N12B80\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Another datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12BB0\">Usage</h4> <p> Use the SecondsBetween function to get the number of seconds between the two given datetime objects. </p>  <h3 id=\"subtract-duration\">SubtractDuration</h3> <p>Returns the result of a DateTime object minus a <a href=\"http://en.wikipedia.org/wiki/ISO_8601#Durations\">Duration object</a>.</p>  <h4 id=\"Syntax-N12BCA\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SubtractDuration(datetime, duration)</p> </td> </tr> </table>  <h4 id=\"Terms-N12BDF\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>duration</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The duration string in <a href=\"http://en.wikipedia.org/wiki/ISO_8601#Durations\">ISO 8601 format</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N12C13\">Usage</h4> <p> Use the AddDuration function to created a new datetime object by add some duration to a given datetime object. </p>  <h3 id=\"to-date\">ToDate</h3> <p>Returns a DateTime object according to parameters.</p>  <h4 id=\"Syntax-N12C29\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ToDate(milliseconds)</p> <p>ToDate(iosstring)</p> <p>ToDate(userstring, format)</p> <p>ToDate(userstring, format, timezone)</p> </td> </tr> </table>  <h4 id=\"Terms-N12C47\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>millseconds</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The offset from 1970-01-01T00:00:00.000Z in terms of the number milliseconds (either positive or negative).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>isostring</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The datetime string in the <a href=\"http://www.w3.org/TR/NOTE-datetime\">ISO 8601 format</a>.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>userstring</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The datetime string in the user defined format.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>format</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The date time format pattern string (see <a href=\"http://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html\">Java SimpleDateFormat class</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>timezone</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The timezone string. Either the UTC offset and the location based format can be used as a parameter, while internally the timezone will be converted to the UTC offset format.</p> <p>Please see <a href=\"http://joda-time.sourceforge.net/timezones.html\">the Joda-Time doc</a> for available timezone IDs.</p> </td> </tr> </table>  <h4 id=\"Usage-N12CBF\">Usage</h4> <p> Use the ToDate function to generate a DateTime object. Note that if the timezone is not specified with the ISO datetime string or by the timezone parameter, the default timezone will be used. </p>  <h3 id=\"to-milli-seconds\">ToMilliSeconds</h3> <p> Returns the number of milliseconds elapsed since January 1, 1970, 00:00:00.000 GMT for a DateTime object. </p>  <h4 id=\"Syntax-N12CD5\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ToMilliSeconds(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N12CEA\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12D07\">Usage</h4> <p> Use the ToMilliSeconds function to convert the DateTime to the number of milliseconds that have passed since January 1, 1970 00:00:00.000 GMT. </p>  <h3 id=\"to-string\">ToString</h3> <p> ToString converts the DateTime object to the ISO or the customized string. </p>  <h4 id=\"Syntax-N12D1D\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ToString(datetime [, format string])</p> </td> </tr> </table>  <h4 id=\"Terms-N12D32\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>format string</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The date time format pattern string (see <a href=\"http://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html\">Java SimpleDateFormat class</a>).</p> </td> </tr> </table>  <h4 id=\"Usage-N12D66\">Usage</h4> <p> Use the ToString function to convert the DateTime to the customized string. </p>  <h3 id=\"to-unix-time\">ToUnixTime</h3> <p> Returns the Unix Time as long for a DateTime object. UnixTime is the number of seconds elapsed since January 1, 1970, 00:00:00.000 GMT. </p>  <h4 id=\"Syntax-N12D7C\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ToUnixTime(datetime)</p> </td> </tr> </table>  <h4 id=\"Terms-N12D91\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12DAE\">Usage</h4> <p> Use the ToUnixTime function to convert the DateTime to Unix Time. </p>  <h3 id=\"weeks-between\">WeeksBetween</h3> <p>Returns the number of weeks between two DateTime objects.</p>  <h4 id=\"Syntax-N12DC4\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>WeeksBetween(datetime1, datetime2)</p> </td> </tr> </table>  <h4 id=\"Terms-N12DD9\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Another datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12E09\">Usage</h4> <p> Use the WeeksBetween function to get the number of weeks between the two given datetime objects. </p>  <h3 id=\"years-between\">YearsBetween</h3> <p>Returns the number of years between two DateTime objects.</p>  <h4 id=\"Syntax-N12E1F\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>YearsBetween(datetime1, datetime2)</p> </td> </tr> </table>  <h4 id=\"Terms-N12E34\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A datetime object.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Another datetime object.</p> </td> </tr> </table>  <h4 id=\"Usage-N12E64\">Usage</h4> <p> Use the YearsBetween function to get the number of years between the two given datetime objects. </p> </div>      <h2 id=\"bag-tuple-functions\">Tuple, Bag, Map Functions</h2> <div class=\"section\">  <h3 id=\"totuple\">TOTUPLE</h3> <p>Converts one or more expressions to type tuple. </p>  <h4 id=\"Syntax-N12E89\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TOTUPLE(expression [, expression ...])</p> </td> </tr> </table>  <h4 id=\"Terms-N12E9E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression of any datatype.</p> </td> </tr> </table>  <h4 id=\"Usage-N12EBB\">Usage</h4> <p>Use the TOTUPLE function to convert one or more expressions to a tuple.</p> <p>See also: <a href=\"basic#tuple\">Tuple</a> data type and <a href=\"basic#type-construction\">Type Construction Operators</a> </p>  <h4 id=\"Example-N12ECF\">Example</h4> <p> In this example, fields f1, f2 and f3 are converted to a tuple. </p> <pre class=\"code\">\na = LOAD 'student' AS (f1:chararray, f2:int, f3:float);\nDUMP a;\n\n(John,18,4.0)\n(Mary,19,3.8)\n(Bill,20,3.9)\n(Joe,18,3.8)\n\nb = FOREACH a GENERATE TOTUPLE(f1,f2,f3);\nDUMP b;\n\n((John,18,4.0))\n((Mary,19,3.8))\n((Bill,20,3.9))\n((Joe,18,3.8))\n</pre>  <h3 id=\"tobag\">TOBAG</h3> <p>Converts one or more expressions to type bag. </p>  <h4 id=\"Syntax-N12EE9\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TOBAG(expression [, expression ...])</p> </td> </tr> </table>  <h4 id=\"Terms-N12EFE\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression with any data type.</p> </td> </tr> </table>  <h4 id=\"Usage-N12F1B\">Usage</h4> <p>Use the TOBAG function to convert one or more expressions to individual tuples which are then placed in a bag.</p> <p>See also: <a href=\"basic#bag\">Bag</a> data type and <a href=\"basic#type-construction\">Type Construction Operators</a> </p>  <h4 id=\"Example-N12F2F\">Example</h4> <p> In this example, fields f1 and f3 are converted to tuples that are then placed in a bag. </p> <pre class=\"code\">\na = LOAD 'student' AS (f1:chararray, f2:int, f3:float);\nDUMP a;\n\n(John,18,4.0)\n(Mary,19,3.8)\n(Bill,20,3.9)\n(Joe,18,3.8)\n\nb = FOREACH a GENERATE TOBAG(f1,f3);\nDUMP b;\n\n({(John),(4.0)})\n({(Mary),(3.8)})\n({(Bill),(3.9)})\n({(Joe),(3.8)})\n</pre>  <h3 id=\"tomap\">TOMAP</h3> <p>Converts key/value expression pairs into a map. </p>  <h4 id=\"Syntax-N12F49\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TOMAP(key-expression, value-expression [, key-expression, value-expression ...])</p> </td> </tr> </table>  <h4 id=\"Terms-N12F5E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>key-expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression of type chararray.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>value-expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression of any type supported by a map.</p> </td> </tr> </table>  <h4 id=\"Usage-N12F8E\">Usage</h4> <p>Use the TOMAP function to convert pairs of expressions into a map. Note the following:</p> <ul> <li>You must supply an even number of expressions as parameters</li> <li>The elements must comply with map type rules: <ul> <li>Every odd element (key-expression) must be a chararray since only chararrays can be keys into the map</li> <li>Every even element (value-expression) can be of any type supported by a map. </li> </ul> </li> </ul>  <p>See also: <a href=\"basic#map\">Map</a> data type and <a href=\"basic#type-construction\">Type Construction Operators</a> </p>  <h4 id=\"Example-N12FB6\">Example</h4> <p> In this example, student names (type chararray) and student GPAs (type float) are used to create three maps. </p> <pre class=\"code\">\nA = load 'students' as (name:chararray, age:int, gpa:float);\nB = foreach A generate TOMAP(name, gpa);\nstore B into 'results';\n\nInput (students)\njoe smith 20 3.5\namy chen 22 3.2\nleo allen 18 2.1\n\nOutput (results)\n[joe smith#3.5]\n[amy chen#3.2]\n[leo allen#2.1]\n</pre>  <h3 id=\"topx\">TOP</h3> <p>Returns the top-n tuples from a bag of tuples.</p>  <h4 id=\"Syntax-N12FD0\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>TOP(topN,column,relation)</p> </td> </tr> </table>  <h4 id=\"Terms-N12FE5\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>topN</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The number of top tuples to return (type integer).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>column</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The tuple column whose values are being compared, note 0 denotes the first column.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>relation</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The relation (bag of tuples) containing the tuple column.</p> </td> </tr> </table>  <h4 id=\"Usage-N13028\">Usage</h4> <p> TOP function returns a bag containing top N tuples from the input bag where N is controlled by the first parameter to the function. The tuple comparison is performed based on a single column from the tuple. The column position is determined by the second parameter to the function. The function assumes that all tuples in the bag contain an element of the same type in the compared column. </p> <p> By default, TOP function uses descending order. But it can be configured via DEFINE statement. </p> <pre class=\"code\">\nDEFINE asc TOP('ASC'); -- ascending order\nDEFINE desc TOP('DESC'); -- descending order\n</pre>  <h4 id=\"Example-N13039\">Example</h4> <p> In this example the top 10 occurrences are returned. </p> <pre class=\"code\">\nDEFINE asc TOP('ASC'); -- ascending order\nDEFINE desc TOP('DESC'); -- descending order\n\nA = LOAD 'data' as (first: chararray, second: chararray);\nB = GROUP A BY (first, second);\nC = FOREACH B generate FLATTEN(group), COUNT(A) as count;\nD = GROUP C BY first; -- again group by first\ntopResults = FOREACH D {\n    result = asc(10, 1, C); -- and retain top 10 (in ascending order) occurrences of 'second' in first  \n    GENERATE FLATTEN(result);\n}\n\nbottomResults = FOREACH D {\n    result = desc(10, 1, C); -- and retain top 10  (in descending order) occurrences of 'second' in first  \n    GENERATE FLATTEN(result);\n}\n</pre> </div>      <h2 id=\"hive-udf\">Hive UDF</h2> <div class=\"section\"> <p>Pig invokes all types of Hive UDF, including UDF, GenericUDF, UDAF, GenericUDAF and GenericUDTF. Depending on the Hive UDF you want to use, you need to declare it in Pig with HiveUDF(handles UDF and GenericUDF), HiveUDAF(handles UDAF and GenericUDAF), HiveUDTF(handles GenericUDTF).</p>  <h3 id=\"Syntax-N1305A\">Syntax</h3> <p>HiveUDF, HiveUDAF, HiveUDTF share the same syntax.</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>HiveUDF(name[, constant parameters])</p> </td> </tr> </table>  <h3 id=\"Terms-N13072\">Terms</h3> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Hive UDF name. This can be a fully qualified class name of the Hive UDF/UDTF/UDAF class, or a registered short name in Hive FunctionRegistry (most Hive builtin UDF does that)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>constant parameters</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Optional tuple representing constant parameters of a Hive UDF/UDTF/UDAF. If Hive UDF requires a constant parameter, there is no other way Pig can pass that information to Hive, since Pig schema does not carry the information whether a parameter is constant or not. Null item in the tuple means this field is not a constant. Non-null item represents a constant field. Data type for the item is determined by Pig contant parser.</p> </td> </tr> </table>  <h3 id=\"Example-N130A2\">Example</h3> <p>HiveUDF</p> <pre class=\"code\">\ndefine sin HiveUDF('sin');\nA = LOAD 'student' as (name:chararray, age:int, gpa:double);\nB = foreach A generate sin(gpa);\n  </pre> <p>HiveUDTF</p> <pre class=\"code\">\ndefine explode HiveUDTF('explode');\nA = load 'mydata' as (a0:{(b0:chararray)});\nB = foreach A generate flatten(explode(a0));\n  </pre> <p>HiveUDAF</p> <pre class=\"code\">\ndefine avg HiveUDAF('avg');\nA = LOAD 'student' as (name:chararray, age:int, gpa:double);\nB = group A by name;\nC = foreach B generate group, avg(A.age);\n  </pre> <p>HiveUDAF with constant parameter</p> <pre class=\"code\">\ndefine in_file HiveUDF('in_file', '(null, \"names.txt\")');\nA = load 'student' as (name:chararray, age:long, gpa:double);\nB = foreach A generate in_file(name, 'names.txt');\n</pre> <p>In this example, we pass (null, \"names.txt\") to the construct of UDF in_file, meaning the first parameter is regular, the second parameter is a constant. names.txt can be double quoted (unlike other Pig syntax), or quoted in \\'. Note we need to pass 'names.txt' again in line 3. This looks stupid but we need to do this to fill the semantic gap between Pig and Hive. We need to pass the constant in the data pipeline in line 3, which is similar Pig UDF. Initialization code in Hive UDF takes ObjectInspector, which capture the data type and whether or not the parameter is a constant. However, initialization code in Pig takes schema, which only capture the former. We need to use additional mechanism (construct parameter) to convey the later.</p> <p>Note: A few Hive 0.14 UDF contains bug which affects Pig and are fixed in Hive 1.0. Here is a list: compute_stats, context_ngrams, count, ewah_bitmap, histogram_numeric, collect_list, collect_set, ngrams, case, in, named_struct, stack, percentile_approx.</p> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/func.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/func.html</a>\n  </p>\n</div>\n","basic":"<h1>Pig Latin Basics</h1> <div id=\"front-matter\"> <div id=\"minitoc-area\"> <ul class=\"minitoc\"> <li> <a href=\"#Conventions\">Conventions</a> </li> <li> <a href=\"#reserved-keywords\">Reserved Keywords</a> </li> <li> <a href=\"#case-sensitivity\">Case Sensitivity</a> </li> <li> <a href=\"#Data+Types+and+More\">Data Types and More</a> <ul class=\"minitoc\"> <li> <a href=\"#identifiers\">Identifiers</a> </li> <li> <a href=\"#relations\">Relations, Bags, Tuples, Fields</a> </li> <li> <a href=\"#data-types\">Data Types</a> </li> <li> <a href=\"#nulls\">Nulls and Pig Latin</a> </li> <li> <a href=\"#constants\">Constants</a> </li> <li> <a href=\"#expressions\">Expressions</a> </li> <li> <a href=\"#schemas\">Schemas</a> </li> </ul> </li> <li> <a href=\"#artichmetic-ops\">Arithmetic Operators and More</a> <ul class=\"minitoc\"> <li> <a href=\"#arithmetic\">Arithmetic Operators</a> </li> <li> <a href=\"#boolops\">Boolean Operators</a> </li> <li> <a href=\"#cast\">Cast Operators</a> </li> <li> <a href=\"#comparison\">Comparison Operators</a> </li> <li> <a href=\"#type-construction\">Type Construction Operators</a> </li> <li> <a href=\"#deref\">Dereference Operators</a> </li> <li> <a href=\"#disambiguate\">Disambiguate Operator</a> </li> <li> <a href=\"#flatten\">Flatten Operator</a> </li> <li> <a href=\"#null_operators\">Null Operators</a> </li> <li> <a href=\"#sign\">Sign Operators</a> </li> </ul> </li> <li> <a href=\"#Relational+Operators\">Relational Operators</a> <ul class=\"minitoc\"> <li> <a href=\"#assert\">ASSERT</a> </li> <li> <a href=\"#cogroup\">COGROUP</a> </li> <li> <a href=\"#cross\">CROSS</a> </li> <li> <a href=\"#cube\">CUBE</a> </li> <li> <a href=\"#define\">DEFINE</a> </li> <li> <a href=\"#distinct\">DISTINCT </a> </li> <li> <a href=\"#filter\">FILTER </a> </li> <li> <a href=\"#foreach\">FOREACH</a> </li> <li> <a href=\"#group\">GROUP</a> </li> <li> <a href=\"#import\">IMPORT</a> </li> <li> <a href=\"#join-inner\">JOIN (inner) </a> </li> <li> <a href=\"#join-outer\">JOIN (outer) </a> </li> <li> <a href=\"#limit\">LIMIT </a> </li> <li> <a href=\"#load\">LOAD </a> </li> <li> <a href=\"#mapreduce\">MAPREDUCE</a> </li> <li> <a href=\"#order-by\">ORDER BY</a> </li> <li> <a href=\"#rank\">RANK</a> </li> <li> <a href=\"#sample\">SAMPLE</a> </li> <li> <a href=\"#SPLIT\">SPLIT</a> </li> <li> <a href=\"#store\">STORE </a> </li> <li> <a href=\"#stream\">STREAM</a> </li> <li> <a href=\"#union\">UNION</a> </li> </ul> </li> <li> <a href=\"#udf-statements\">UDF Statements</a> <ul class=\"minitoc\"> <li> <a href=\"#define-udfs\">DEFINE (UDFs, streaming)</a> </li> <li> <a href=\"#register-jar\">REGISTER (a jar/script)</a> </li> <li> <a href=\"#register-artifact\">REGISTER (an artifact)</a> </li> </ul> </li> </ul> </div> </div>   <h2 id=\"Conventions\">Conventions</h2> <div class=\"section\"> <p>Conventions for the syntax and code examples in the Pig Latin Reference Manual are described here.</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Convention</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Description</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Example</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>( )</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Parentheses enclose one or more items.</p> <p>Parentheses are also used to indicate the tuple data type.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Multiple items:</p> <p>(1, abc, (2,4,6) )</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>[ ]</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Straight brackets enclose one or more optional items.</p> <p>Straight brackets are also used to indicate the map data type. In this case &lt;&gt; is used to indicate optional items.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Optional items:</p> <p>[INNER | OUTER]</p>  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>{ }</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Curly brackets enclose two or more items, one of which is required. </p> <p>Curly brackets also used to indicate the bag data type. In this case &lt;&gt; is used to indicate required items.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Two items, one required:</p> <p>{ block | nested_block }</p>  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>…</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Horizontal ellipsis points indicate that you can repeat a portion of the code.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Pig Latin syntax statement:</p> <p>cat path [path …]</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>UPPERCASE</p>  <p>lowercase</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>In general, uppercase type indicates elements the system supplies.</p> <p>In general, lowercase type indicates elements that you supply.</p> <p>(These conventions are not strictly adherered to in all examples.)</p> <p>See <a href=\"#case-sensitivity\">Case Sensitivity</a> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Pig Latin statement:</p> <p>a = LOAD 'data' AS (f1:int);</p>  <ul> <li> <p>LOAD, AS - Pig keywords</p> </li> <li> <p>a, f1 - aliases you supply</p> </li> <li> <p>'data' - data source you supply</p> </li> </ul> </td> </tr> </table> </div>   <h2 id=\"reserved-keywords\">Reserved Keywords</h2> <div class=\"section\"> <p>Pig reserved keywords are listed here.</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- A </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>assert, and, any, all, arrange, as, asc, AVG</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- B </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bag, BinStorage, by, bytearray, BIGINTEGER, BIGDECIMAL</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- C </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cache, CASE, cat, cd, chararray, cogroup, CONCAT, copyFromLocal, copyToLocal, COUNT, cp, cross</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- D </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>datetime, %declare, %default, define, dense, desc, describe, DIFF, distinct, double, du, dump</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- E </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>e, E, eval, exec, explain</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- F </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>f, F, filter, flatten, float, foreach, full</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- G </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>generate, group</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- H </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>help</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- I </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>if, illustrate, import, inner, input, int, into, is</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- J </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>join</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- K </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>kill</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- L </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>l, L, left, limit, load, long, ls</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- M </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>map, matches, MAX, MIN, mkdir, mv </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- N </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not, null</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- O </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>onschema, or, order, outer, output</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- P </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>parallel, pig, PigDump, PigStorage, pwd</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- Q </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>quit</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- R </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>register, returns, right, rm, rmf, rollup, run</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- S </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>sample, set, ship, SIZE, split, stderr, stdin, stdout, store, stream, SUM</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- T </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>TextLoader, TOKENIZE, through, tuple</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- U </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>union, using</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>-- V, W, X, Y, Z </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>void</p> </td> </tr> </table> </div>    <h2 id=\"case-sensitivity\">Case Sensitivity</h2> <div class=\"section\"> <p>The names (aliases) of relations and fields are case sensitive. The names of Pig Latin functions are case sensitive. The names of parameters (see <a href=\"cont#Parameter-Sub\">Parameter Substitution</a>) and all other Pig Latin keywords (see <a href=\"#reserved-keywords\">Reserved Keywords</a>) are case insensitive.</p> <p>In the example below, note the following:</p> <ul> <li> <p>The names (aliases) of relations A, B, and C are case sensitive.</p> </li> <li> <p>The names (aliases) of fields f1, f2, and f3 are case sensitive.</p> </li> <li> <p>Function names PigStorage and COUNT are case sensitive.</p> </li> <li> <p>Keywords LOAD, USING, AS, GROUP, BY, FOREACH, GENERATE, and DUMP are case insensitive. They can also be written as load, using, as, group, by, etc.</p> </li> <li> <p>In the FOREACH statement, the field in relation B is referred to by positional notation ($0).</p> </li> </ul>  <pre class=\"code\">\ngrunt&gt; A = LOAD 'data' USING PigStorage() AS (f1:int, f2:int, f3:int);\ngrunt&gt; B = GROUP A BY f1;\ngrunt&gt; C = FOREACH B GENERATE COUNT ($0);\ngrunt&gt; DUMP C;\n</pre> </div>    <h2 id=\"Data+Types+and+More\">Data Types and More</h2> <div class=\"section\">  <h3 id=\"identifiers\">Identifiers</h3> <p>Identifiers include the names of relations (aliases), fields, variables, and so on. In Pig, identifiers start with a letter and can be followed by any number of letters, digits, or underscores.</p> <p>Valid identifiers:</p> <pre class=\"code\">\nA\nA123\nabc_123_BeX_\n</pre>  <p>Invalid identifiers: </p> <pre class=\"code\">\n_A123\nabc_$\nA!B\n</pre>  <h3 id=\"relations\">Relations, Bags, Tuples, Fields</h3> <p> <a href=\"start#pl-statements\">Pig Latin statements</a> work with relations. A relation can be defined as follows:</p> <ul> <li> <p>A relation is a bag (more specifically, an outer bag).</p> </li> <li> <p>A bag is a collection of tuples. </p> </li> <li> <p>A tuple is an ordered set of fields.</p> </li> <li> <p>A field is a piece of data.</p> </li> </ul>  <p>A Pig relation is a bag of tuples. A Pig relation is similar to a table in a relational database, where the tuples in the bag correspond to the rows in a table. Unlike a relational table, however, Pig relations don't require that every tuple contain the same number of fields or that the fields in the same position (column) have the same type.</p> <p>Also note that relations are unordered which means there is no guarantee that tuples are processed in any particular order. Furthermore, processing may be parallelized in which case tuples are not processed according to any total ordering.</p>  <h4 id=\"ref-relation\">Referencing Relations</h4> <p>Relations are referred to by name (or alias). Names are assigned by you as part of the Pig Latin statement. In this example the name (alias) of the relation is A.</p> <pre class=\"code\">\nA = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);\nDUMP A;\n(John,18,4.0F)\n(Mary,19,3.8F)\n(Bill,20,3.9F)\n(Joe,18,3.8F)\n</pre> <p>You an assign an alias to another alias. The new alias can be used in the place of the original alias to refer the original relation. </p> <pre class=\"code\">\n  A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);\n  B = A;\n  DUMP B;\n  </pre>  <h4 id=\"ref-field\">Referencing Fields</h4> <p>Fields are referred to by positional notation or by name (alias). </p> <ul> <li> <p>Positional notation is generated by the system. Positional notation is indicated with the dollar sign ($) and begins with zero (0); for example, $0, $1, $2. </p> </li> <li> <p>Names are assigned by you using schemas (or, in the case of the GROUP operator and some functions, by the system). You can use any name that is not a Pig keyword (see <a href=\"#identifiers\">Identifiers</a> for valid name examples).</p> </li> </ul> <p>Given relation A above, the three fields are separated out in this table. </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>First Field</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Second Field</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Third Field </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Data type</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Positional notation (generated by system) </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>$0</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>$1</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>$2</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Possible name (assigned by you using a schema)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>name</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>age</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>gpa</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Field value (for the first tuple)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>John</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>18</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>4.0</p> </td> </tr> </table> <p>As shown in this example when you assign names to fields (using the AS schema clause) you can still refer to the fields using positional notation. However, for debugging purposes and ease of comprehension, it is better to use field names.</p> <pre class=\"code\">\nA = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);\nX = FOREACH A GENERATE name,$2;\nDUMP X;\n(John,4.0F)\n(Mary,3.8F)\n(Bill,3.9F)\n(Joe,3.8F)\n</pre> <p>In this example an error is generated because the requested column ($3) is outside of the declared schema (positional notation begins with $0). Note that the error is caught before the statements are executed.</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int,f2:int,f3:int);\nB = FOREACH A GENERATE $3;\nDUMP B;\n2009-01-21 23:03:46,715 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: \nOut of bound access. Trying to access non-existent  : 3. Schema {f1: bytearray,f2: bytearray,f3: bytearray} has 3 column(s). \n<em>etc ... </em>\n</pre>  <h4 id=\"ref-field-complex\">Referencing Fields that are Complex Data Types</h4> <p>As noted, the fields in a tuple can be any data type, including the complex data types: bags, tuples, and maps. </p> <ul> <li> <p>Use the schemas for complex data types to name fields that are complex data types. </p> </li> <li> <p>Use the dereference operators to reference and work with fields that are complex data types.</p> </li> </ul> <p>In this example the data file contains tuples. A schema for complex data types (in this case, tuples) is used to load the data. Then, dereference operators (the dot in t1.t1a and t2.$0) are used to access the fields in the tuples. Note that when you assign names to fields you can still refer to these fields using positional notation.</p> <pre class=\"code\">\ncat data;\n(3,8,9) (4,5,6)\n(1,4,7) (3,7,5)\n(2,5,8) (9,5,8)\n\nA = LOAD 'data' AS (t1:tuple(t1a:int, t1b:int,t1c:int),t2:tuple(t2a:int,t2b:int,t2c:int));\n\nDUMP A;\n((3,8,9),(4,5,6))\n((1,4,7),(3,7,5))\n((2,5,8),(9,5,8))\n\nX = FOREACH A GENERATE t1.t1a,t2.$0;\n\nDUMP X;\n(3,4)\n(1,3)\n(2,9)\n</pre>  <h3 id=\"data-types\">Data Types</h3>  <h4 id=\"Simple+and+Complex\">Simple and Complex</h4>  <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Simple Types</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Description</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Example </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Signed 32-bit integer</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>10</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Signed 64-bit integer</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Data: 10L or 10l </p> <p>Display: 10L </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>32-bit floating point</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Data: 10.5F or 10.5f or 10.5e2f or 10.5E2F</p> <p>Display: 10.5F or 1050.0F</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>64-bit floating point</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Data: 10.5 or 10.5e2 or 10.5E2</p> <p>Display: 10.5 or 1050.0</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Character array (string) in Unicode UTF-8 format</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>hello world</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Byte array (blob)</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>boolean</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>true/false (case insensitive)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>1970-01-01T00:00:00.000+00:00</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Java BigInteger</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>200000000000</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Java BigDecimal</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>33.456783321323441233442</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Complex Types</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An ordered set of fields.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(19,2)</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An collection of tuples.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>{(19,2), (18,1)}</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A set of key value pairs.</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>[open#apache]</p> </td> </tr> </table> <p>Note the following general observations about data types:</p> <ul> <li> <p>Use schemas to assign types to fields. If you don't assign types, fields default to type bytearray and implicit conversions are applied to the data depending on the context in which that data is used. For example, in relation B, f1 is converted to integer because 5 is integer. In relation C, f1 and f2 are converted to double because we don't know the type of either f1 or f2.</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1,f2,f3);\nB = FOREACH A GENERATE f1 + 5;\nC = FOREACH A generate f1 + f2;\n</pre> </li> </ul> <ul> <li> <p>If a schema is defined as part of a load statement, the load function will attempt to enforce the schema. If the data does not conform to the schema, the loader will generate a null value or an error.</p> <pre class=\"code\">\nA = LOAD 'data' AS (name:chararray, age:int, gpa:float);\n</pre> </li> </ul>  <ul> <li> <p>If an explicit cast is not supported, an error will occur. For example, you cannot cast a chararray to int.</p> <pre class=\"code\">\nA = LOAD 'data' AS (name:chararray, age:int, gpa:float);\nB = FOREACH A GENERATE (int)name;\n\nThis will cause an error …</pre> </li> </ul>  <ul> <li> <p>If Pig cannot resolve incompatible types through implicit casts, an error will occur. For example, you cannot add chararray and float (see the Types Table for addition and subtraction).</p> <pre class=\"code\">\nA = LOAD 'data' AS (name:chararray, age:int, gpa:float);\nB = FOREACH A GENERATE name + gpa;\n\nThis will cause an error …</pre> </li> </ul>  <p>All data types have corresponding <a href=\"#schemas\">schemas</a>.</p>  <h4 id=\"tuple\">Tuple</h4> <p>A tuple is an ordered set of fields.</p>  <h5 id=\"Syntax\">Syntax </h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>( field [, field …] ) </p> </td> </tr> </table>  <h5 id=\"Terms\">Terms</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>( )</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A tuple is enclosed in parentheses ( ).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>field</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A piece of data. A field can be any data type (including tuple and bag).</p> </td> </tr> </table>  <h5 id=\"Usage\">Usage</h5> <p>You can think of a tuple as a row with one or more fields, where each field can be any data type and any field may or may not have data. If a field has no data, then the following happens:</p> <ul> <li> <p>In a load statement, the loader will inject null into the tuple. The actual value that is substituted for null is loader specific; for example, PigStorage substitutes an empty field for null.</p> </li> <li> <p>In a non-load statement, if a requested field is missing from a tuple, Pig will inject null.</p> </li> </ul>  <p>Also see <a href=\"#tuple-schema\">tuple schemas</a>.</p>  <h5 id=\"Example\">Example</h5> <p>In this example the tuple contains three fields.</p> <pre class=\"code\">(John,18,4.0F)</pre>  <h4 id=\"bag\">Bag</h4> <p>A bag is a collection of tuples.</p>  <h5 id=\"Syntax%3A+Inner+bag\">Syntax: Inner bag</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>{ tuple [, tuple …] }</p> </td> </tr> </table>  <h5 id=\"Terms-N106F7\">Terms</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>{ }</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An inner bag is enclosed in curly brackets { }.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A tuple.</p> </td> </tr> </table>  <h5 id=\"Usage-N10727\">Usage </h5> <p>Note the following about bags:</p> <ul> <li> <p>A bag can have duplicate tuples.</p> </li> <li> <p>A bag can have tuples with differing numbers of fields. However, if Pig tries to access a field that does not exist, a null value is substituted.</p> </li> <li> <p>A bag can have tuples with fields that have different data types. However, for Pig to effectively process bags, the schemas of the tuples within those bags should be the same. For example, if half of the tuples include chararray fields and while the other half include float fields, only half of the tuples will participate in any kind of computation because the chararray fields will be converted to null.</p>  <p>Bags have two forms: outer bag (or relation) and inner bag.</p> </li> </ul>  <p>Also see <a href=\"#bag-schema\">bag schemas</a>.</p>  <h5 id=\"Example%3A+Outer+Bag\">Example: Outer Bag</h5> <p>In this example A is a relation or bag of tuples. You can think of this bag as an outer bag.</p> <pre class=\"code\">\nA = LOAD 'data' as (f1:int, f2:int, f3:int);\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n</pre>  <h5 id=\"Example%3A+Inner+Bag\">Example: Inner Bag</h5> <p>Now, suppose we group relation A by the first field to form relation X. </p> <p>In this example X is a relation or bag of tuples. The tuples in relation X have two fields. The first field is type int. The second field is type bag; you can think of this bag as an inner bag.</p> <pre class=\"code\">\nX = GROUP A BY f1;\nDUMP X;\n(1,{(1,2,3)})\n(4,{(4,2,1),(4,3,3)})\n(8,{(8,3,4)})\n</pre>  <h4 id=\"map\">Map</h4> <p>A map is a set of key/value pairs.</p>  <h5 id=\"Syntax+%28%3C%3E+denotes+optional%29\">Syntax (&lt;&gt; denotes optional)</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>[ key#value &lt;, key#value …&gt; ]</p> </td> </tr> </table>  <h5 id=\"Terms-N10792\">Terms</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>[ ]</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Maps are enclosed in straight brackets [ ].</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>#</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Key value pairs are separated by the pound sign #.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>key</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Must be chararray data type. Must be a unique value.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>value</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Any data type (the defaults to bytearray).</p> </td> </tr> </table>  <h5 id=\"Usage-N107E8\">Usage</h5> <p>Key values within a relation must be unique.</p> <p>Also see <a href=\"#map-schema\">map schemas</a>.</p>  <h5 id=\"Example-N107F9\">Example</h5> <p>In this example the map includes two key value pairs.</p> <pre class=\"code\">[name#John,phone#5551212]</pre>  <h3 id=\"nulls\">Nulls and Pig Latin</h3> <p>In Pig Latin, nulls are implemented using the SQL definition of null as unknown or non-existent. Nulls can occur naturally in data or can be the result of an operation. </p>  <h4 id=\"nulls-ops\">Nulls, Operators, and Functions</h4> <p>Pig Latin operators and functions interact with nulls as shown in this table.</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operator </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Interaction </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Comparison operators:</p> <p>==, !=</p> <p>&gt;, &lt;</p> <p>&gt;=, &lt;=</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If either subexpression is null, the result is null.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Comparison operator:</p> <p>matches </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If either the string being matched against or the string defining the match is null, the result is null.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Arithmetic operators:</p> <p> + , -, *, /</p> <p>% modulo</p> <p>? : bincond</p> <p>CASE : case</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If either subexpression is null, the resulting expression is null.</p>  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Null operator:</p> <p>is null </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If the tested value is null, returns true; otherwise, returns false (see <a href=\"#null_operators\">Null Operators</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Null operator:</p> <p>is not null</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If the tested value is not null, returns true; otherwise, returns false (see <a href=\"#null_operators\">Null Operators</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Dereference operators:</p> <p>tuple (.) or map (#)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If the de-referenced tuple or map is null, returns null.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operators:</p> <p>COGROUP, GROUP, JOIN</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>These operators handle nulls differently (see examples below).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Function:</p> <p>COUNT_STAR</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>This function counts all values, including nulls.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Cast operator</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Casting a null from one type to another type results in a null.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Functions:</p> <p>AVG, MIN, MAX, SUM, COUNT</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>These functions ignore nulls. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Function:</p> <p>CONCAT</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If either subexpression is null, the resulting expression is null.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Function:</p> <p>SIZE</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>If the tested object is null, returns null.</p> </td> </tr> </table> <p>For Boolean subexpressions, note the results when nulls are used with these operators:</p> <ul> <li> <p>FILTER operator – If a filter expression results in null value, the filter does not pass them through (if X is null, !X is also null, and the filter will reject both).</p> </li> <li> <p>Bincond operator – If a Boolean subexpression results in null value, the resulting expression is null (see the interactions above for Arithmetic operators)</p> </li> </ul>  <h4 id=\"nulls-constants\">Nulls and Constants</h4> <p>Nulls can be used as constant expressions in place of expressions of any type.</p> <p>In this example a and null are projected.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a, b, c).\nB = FOREACH A GENERATE a, null;\n</pre> <p>In this example of an outer join, if the join key is missing from a table it is replaced by null.</p> <pre class=\"code\">\nA = LOAD 'student' AS (name: chararray, age: int, gpa: float);\nB = LOAD 'votertab10k' AS (name: chararray, age: int, registration: chararray, donation: float);\nC = COGROUP A BY name, B BY name;\nD = FOREACH C GENERATE FLATTEN((IsEmpty(A) ? null : A)), FLATTEN((IsEmpty(B) ? null : B));\n</pre> <p>Like any other expression, null constants can be implicitly or explicitly cast. </p> <p>In this example both a and null will be implicitly cast to double.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a, b, c).\nB = FOREACH A GENERATE a + null;\n</pre> <p>In this example both a and null will be cast to int, a implicitly, and null explicitly.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a, b, c).\nB = FOREACH A GENERATE a + (int)null;\n</pre>  <h4 id=\"nulls-ops-produce\">Operations That Produce Nulls</h4> <p>As noted, nulls can be the result of an operation. These operations can produce null values: </p> <ul> <li> <p>Division by zero</p> </li> <li> <p>Returns from user defined functions (UDFs) </p> </li> <li> <p>Dereferencing a field that does not exist.</p> </li> <li> <p>Dereferencing a key that does not exist in a map. For example, given a map, info, containing [name#john, phone#5551212] if a user tries to use info#address a null is returned.</p> </li> <li> <p>Accessing a field that does not exist in a tuple.</p> </li> </ul>  <h5 id=\"Example%3A+Accessing+a+field+that+does+not+exist+in+a+tuple\">Example: Accessing a field that does not exist in a tuple</h5> <p>In this example nulls are injected if fields do not have data.</p> <pre class=\"code\">\ncat data;\n    2   3\n4   \n7   8   9\n\nA = LOAD 'data' AS (f1:int,f2:int,f3:int)\n\nDUMP A;\n(,2,3)\n(4,,)\n(7,8,9)\n\nB = FOREACH A GENERATE f1,f2;\n\nDUMP B;\n(,2)\n(4,)\n(7,8)\n</pre>  <h4 id=\"nulls-load\">Nulls and Load Functions</h4> <p>As noted, nulls can occur naturally in the data. If nulls are part of the data, it is the responsibility of the load function to handle them correctly. Keep in mind that what is considered a null value is loader-specific; however, the load function should always communicate null values to Pig by producing Java nulls.</p> <p>The Pig Latin load functions (for example, PigStorage and TextLoader) produce null values wherever data is missing. For example, empty strings (chararrays) are not loaded; instead, they are replaced by nulls.</p> <p>PigStorage is the default load function for the LOAD operator. In this example the is not null operator is used to filter names with null values.</p> <pre class=\"code\">\nA = LOAD 'student' AS (name, age, gpa); \nB = FILTER A BY name is not null;\n</pre>  <h4 id=\"nulls_group\">Nulls and GROUP/COGROUP Operators</h4> <p>When using the GROUP operator with a single relation, records with a null group key are grouped together.</p> <pre class=\"code\">\nA = load 'student' as (name:chararray, age:int, gpa:float);\ndump A;\n(joe,18,2.5)\n(sam,,3.0)\n(bob,,3.5)\n\nX = group A by age;\ndump X;\n(18,{(joe,18,2.5)})\n(,{(sam,,3.0),(bob,,3.5)})\n   </pre> <p>When using the GROUP (COGROUP) operator with multiple relations, records with a null group key from different relations are considered different and are grouped separately. In the example below note that there are two tuples in the output corresponding to the null group key: one that contains tuples from relation A (but not relation B) and one that contains tuples from relation B (but not relation A).</p> <pre class=\"code\">\nA = load 'student' as (name:chararray, age:int, gpa:float);\nB = load 'student' as (name:chararray, age:int, gpa:float);\ndump B;\n(joe,18,2.5)\n(sam,,3.0)\n(bob,,3.5)\n\nX = cogroup A by age, B by age;\ndump X;\n(18,{(joe,18,2.5)},{(joe,18,2.5)})\n(,{(sam,,3.0),(bob,,3.5)},{})\n(,{},{(sam,,3.0),(bob,,3.5)})\n</pre>  <h4 id=\"nulls_join\">Nulls and JOIN Operator</h4> <p>The JOIN operator - when performing inner joins - adheres to the SQL standard and disregards (filters out) null values. (See also <a href=\"perf#nulls\">Drop Nulls Before a Join</a>.)</p> <pre class=\"code\">\nA = load 'student' as (name:chararray, age:int, gpa:float);\nB = load 'student' as (name:chararray, age:int, gpa:float);\ndump B;\n(joe,18,2.5)\n(sam,,3.0)\n(bob,,3.5)\n  \nX = join A by age, B by age;\ndump X;\n(joe,18,2.5,joe,18,2.5)\n</pre>  <h3 id=\"constants\">Constants</h3> <p>Pig provides constant representations for all data types except bytearrays.</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Constant Example</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Notes</strong> </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Simple Data Types</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>19</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>19L</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>19.2F or 1.92e2f</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>19.2 or 1.92e2</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>'hello world'</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>Not applicable.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>boolean</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>true/false</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Case insensitive.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>Complex Data Types</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(19, 2, 1)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A constant in this form creates a tuple.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>{ (19, 2), (1, 2) }</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A constant in this form creates a bag.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>[ 'name' # 'John', 'ext' # 5555 ]</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A constant in this form creates a map.</p> </td> </tr> </table>  <p>Please note the following:</p> <ul> <li> <p>On UTF-8 systems you can specify string constants consisting of printable ASCII characters such as 'abc'; you can specify control characters such as '\\t'; and, you can specify a character in Unicode by starting it with '\\u', for instance, '\\u0001' represents Ctrl-A in hexadecimal (see Wikipedia <a href=\"http://en.wikipedia.org/wiki/ASCII\">ASCII</a>, <a href=\"http://en.wikipedia.org/wiki/Unicode\">Unicode</a>, and <a href=\"http://en.wikipedia.org/wiki/UTF-8\">UTF-8</a>). In theory, you should be able to specify non-UTF-8 constants on non-UTF-8 systems but as far as we know this has not been tested.</p> </li> <li> <p>To specify a long constant, l or L must be appended to the number (for example, 12345678L). If the l or L is not specified, but the number is too large to fit into an int, the problem will be detected at parse time and the processing is terminated. </p> </li> <li> <p>Any numeric constant with decimal point (for example, 1.5) and/or exponent (for example, 5e+1) is treated as double unless it ends with f or F in which case it is assigned type float (for example, 1.5f). </p> </li> <li> <p>There is no native constant type for datetime field. You can use a ToDate udf with chararray constant as argument to generate a datetime value. </p> </li> </ul>  <p>The data type definitions for tuples, bags, and maps apply to constants:</p> <ul> <li> <p>A tuple can contain fields of any data type</p> </li> <li> <p>A bag is a collection of tuples</p> </li> <li> <p>A map key must be a chararray; a map value can be any data type</p> </li> </ul>  <p>Complex constants (either with or without values) can be used in the same places scalar constants can be used; that is, in FILTER and GENERATE statements.</p> <pre class=\"code\">\nA = LOAD 'data' USING MyStorage() AS (T: tuple(name:chararray, age: int));\nB = FILTER A BY T == ('john', 25);\nD = FOREACH B GENERATE T.name, [25#5.6], {(1, 5, 18)};\n</pre>  <h3 id=\"expressions\">Expressions</h3> <p>In Pig Latin, expressions are language constructs used with the FILTER, FOREACH, GROUP, and SPLIT operators as well as the eval functions.</p> <p>Expressions are written in conventional mathematical infix notation and are adapted to the UTF-8 character set. Depending on the context, expressions can include:</p> <ul> <li> <p>Any Pig data type (simple data types, complex data types)</p> </li> <li> <p>Any Pig operator (arithmetic, comparison, null, boolean, dereference, sign, and cast)</p> </li> <li> <p>Any Pig built in function.</p> </li> <li> <p>Any user defined function (UDF) written in Java. </p> </li> </ul>  <p>In Pig Latin,</p> <ul> <li> <p>An arithmetic expression could look like this:</p> <pre class=\"code\">\nX = GROUP A BY f2*f3;\n</pre> </li> <li>  <p>A string expression could look like this, where a and b are both chararrays:</p> <pre class=\"code\">\nX = FOREACH A GENERATE CONCAT(a,b);\n</pre> </li> <li>  <p>A boolean expression could look like this:</p> <pre class=\"code\">\nX = FILTER A BY (f1==8) OR (NOT (f2+f3 &gt; f1));\n</pre> </li> </ul>  <h4 id=\"fexp\">Field Expressions</h4> <p>Field expressions represent a field or a <a href=\"#deref\">dereference operator</a> applied to a field.</p>  <h4 id=\"sexp\">Star Expressions</h4> <p>Star expressions ( * ) can be used to represent all the fields of a tuple. It is equivalent to writing out the fields explicitly. In the following example the definition of B and C are exactly the same, and MyUDF will be invoked with exactly the same arguments in both cases.</p> <pre class=\"code\">\nA = LOAD 'data' USING MyStorage() AS (name:chararray, age: int);\nB = FOREACH A GENERATE *, MyUDF(name, age);\nC = FOREACH A GENERATE name, age, MyUDF(*);\n          </pre> <p>A common error when using the star expression is shown below. In this example, the programmer really wants to count the number of elements in the bag in the second field: COUNT($1).</p> <pre class=\"code\">\nG = GROUP A BY $0;\nC = FOREACH G GENERATE COUNT(*)\n          </pre> <p>There are some restrictions on use of the star expression when the input schema is unknown (null):</p> <ul> <li>For GROUP/COGROUP, you can't include a star expression in a GROUP BY column. </li> <li>For ORDER BY, if you have project-star as ORDER BY column, you can’t have any other ORDER BY column in that statement. </li> </ul>  <h4 id=\"prexp\">Project-Range Expressions</h4> <p>Project-range ( .. ) expressions can be used to project a range of columns from input. For example:</p> <ul> <li>.. $x : projects columns $0 through $x, inclusive </li> <li>$x .. : projects columns through end, inclusive </li> <li>$x .. $y : projects columns through $y, inclusive </li> </ul>  <p>If the input relation has a schema, you can refer to columns by alias rather than by column position. You can also combine aliases and column positions in an expression; for example, \"col1 .. $5\" is valid. </p> <p>Project-range can be used in all cases where the <a href=\"#sexp\">star expression</a> ( * ) is allowed.</p> <p>Project-range can be used in the following statements: <a href=\"#foreach\">FOREACH</a>, <a href=\"#join-inner\">JOIN</a>, <a href=\"#group\">GROUP</a>, <a href=\"#cogroup\">COGROUP</a>, and <a href=\"#order-by\">ORDER BY</a> (also when ORDER BY is used within a nested FOREACH block).</p> <p>A few examples are shown here:</p> <pre class=\"code\">\n..... \ngrunt&gt; F = foreach IN generate (int)col0, col1 .. col3; \ngrunt&gt; describe F; \nF: {col0: int,col1: bytearray,col2: bytearray,col3: bytearray} \n..... \n..... \ngrunt&gt; SORT = order IN by col2 .. col3, col0, col4 ..; \n..... \n..... \nJ = join IN1 by $0 .. $3, IN2 by $0 .. $3; \n..... \n..... \ng = group l1 by b .. c; \n..... \n</pre> <p>There are some restrictions on the use of project-to-end form of project-range (eg \"x .. \") when the input schema is unknown (null): </p> <ul> <li>For GROUP/COGROUP, the project-to-end form of project-range is not allowed.</li> <li>For ORDER BY, the project-to-end form of project-range is supported only as the last sort column. <pre class=\"code\">\n..... \ngrunt&gt; describe IN; \nSchema for IN unknown. \n\n/* This statement is supported */\nSORT = order IN by $2 .. $3, $6 ..; \n\n/* This statement is NOT supported */ \nSORT = order IN by $2 .. $3, $6 ..; \n..... \n</pre> </li> </ul>  <h4 id=\"bexp\">Boolean Expressions</h4> <p>Boolean expressions can be made up of UDFs that return a boolean value or boolean operators (see <a href=\"#boolops\">Boolean Operators</a>). </p>  <h4 id=\"texp\">Tuple Expressions</h4> <p>Tuple expressions form subexpressions into tuples. The tuple expression has the form (expression [, expression …]), where expression is a general expression. The simplest tuple expression is the star expression, which represents all fields. </p>  <h4 id=\"gexp\">General Expressions</h4> <p>General expressions can be made up of UDFs and almost any operator. Since Pig does not consider boolean a base type, the result of a general expression cannot be a boolean. Field expressions are the simpliest general expressions. </p>  <h3 id=\"schemas\">Schemas</h3> <p>Schemas enable you to assign names to fields and declare types for fields. Schemas are optional but we encourage you to use them whenever possible; type declarations result in better parse-time error checking and more efficient code execution.</p> <p>Schemas for <a href=\"#schema-simple\">simple types</a> and <a href=\"#schema-complex\">complex types</a> can be used anywhere a schema definition is appropriate.</p> <p>Schemas are defined with the <a href=\"#load\">LOAD</a>, <a href=\"#stream\">STREAM</a>, and <a href=\"#foreach\">FOREACH</a> operators using the AS clause. If you define a schema using the LOAD operator, then it is the load function that enforces the schema (see <a href=\"#load\">LOAD</a> and <a href=\"udf\">User Defined Functions</a> for more information).</p>  <p> <strong>Known Schema Handling</strong> </p> <p>Note the following:</p> <ul> <li>You can define a schema that includes both the field name and field type.</li> <li>You can define a schema that includes the field name only; in this case, the field type defaults to bytearray.</li> <li>You can choose not to define a schema; in this case, the field is un-named and the field type defaults to bytearray.</li> </ul> <p>If you assign a name to a field, you can refer to that field using the name or by positional notation. If you don't assign a name to a field (the field is un-named) you can only refer to the field using positional notation.</p> <p>If you assign a type to a field, you can subsequently change the type using the cast operators. If you don't assign a type to a field, the field defaults to bytearray; you can change the default type using the cast operators.</p>   <p id=\"unknown-schema\"> <strong>Unknown Schema Handling</strong> </p> <p>Note the following:</p> <ul> <li>When you JOIN/COGROUP/CROSS multiple relations, if any relation has an unknown schema (or no defined schema, also referred to as a null schema), the schema for the resulting relation is null. </li> <li>If you FLATTEN a bag with empty inner schema, the schema for the resulting relation is null.</li> <li>If you UNION two relations with incompatible schema, the schema for resulting relation is null.</li> <li>If the schema is null, Pig treats all fields as bytearray (in the backend, Pig will determine the real type for the fields dynamically) </li> </ul> <p>See the examples below. If a field's data type is not specified, Pig will use bytearray to denote an unknown type. If the number of fields is not known, Pig will derive an unknown schema.</p> <pre class=\"code\">\n/* The field data types are not specified ... */\na = load '1.txt' as (a0, b0);\na: {a0: bytearray,b0: bytearray}\n\n/* The number of fields is not known ... */\na = load '1.txt';\na: Schema for a unknown\n</pre>  <p> <strong>How Pig Handles Schema</strong> </p> <p>As shown above, with a few exceptions Pig can infer the schema of a relationship up front. You can examine the schema of particular relation using <a href=\"test#describe\">DESCRIBE</a>. Pig enforces this computed schema during the actual execution by casting the input data to the expected data type. If the process is successful the results are returned to the user; otherwise, a warning is generated for each record that failed to convert. Note that Pig does not know the actual types of the fields in the input data prior to the execution; rather, Pig determines the data types and performs the right conversions on the fly.</p> <p>Having a deterministic schema is very powerful; however, sometimes it comes at the cost of performance. Consider the following example:</p> <pre class=\"code\">\nA = load 'input' as (x, y, z);\nB = foreach A generate x+y;\n</pre> <p>If you do <a href=\"test#describe\">DESCRIBE</a> on B, you will see a single column of type double. This is because Pig makes the safest choice and uses the largest numeric type when the schema is not know. In practice, the input data could contain integer values; however, Pig will cast the data to double and make sure that a double result is returned.</p> <p>If the schema of a relation can’t be inferred, Pig will just use the runtime data as is and propagate it through the pipeline.</p>  <h4 id=\"schema-load\">Schemas with LOAD and STREAM </h4> <p>With LOAD and STREAM operators, the schema following the AS keyword must be enclosed in parentheses.</p> <p>In this example the LOAD statement includes a schema definition for simple data types.</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int, f2:int);\n</pre>  <h4 id=\"schemaforeach\">Schemas with FOREACH </h4> <p>With FOREACH operators, the schema following the AS keyword must be enclosed in parentheses when the FLATTEN operator is used. Otherwise, the schema should not be enclosed in parentheses.</p> <p>In this example the FOREACH statement includes FLATTEN and a schema for simple data types.</p> <pre class=\"code\">\nX = FOREACH C GENERATE FLATTEN(B) AS (f1:int, f2:int, f3:int), group;\n</pre> <p>In this example the FOREACH statement includes a schema for simple expression.</p> <pre class=\"code\">\nX = FOREACH A GENERATE f1+f2 AS x1:int;\n</pre> <p>In this example the FOREACH statement includes a schemas for multiple fields.</p> <pre class=\"code\">\nX = FOREACH A GENERATE f1 as user, f2 as age, f3 as gpa;\n</pre>  <h4 id=\"schema-simple\">Schemas for Simple Data Types</h4> <p>Simple data types include int, long, float, double, chararray, bytearray, boolean, datetime, biginteger and bigdecimal.</p>  <h5 id=\"Syntax-N10D83\">Syntax</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>(alias[:type]) [, (alias[:type]) …] )</p> </td> </tr> </table>  <h5 id=\"Terms-N10D97\">Terms</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name assigned to the field.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>type</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(Optional) The simple data type assigned to the field.</p> <p>The alias and type are separated by a colon ( : ).</p> <p>If the type is omitted, the field defaults to type bytearray.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>( , )</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Multiple fields are enclosed in parentheses and separated by commas.</p> </td> </tr> </table>  <h5 id=\"Examples\">Examples</h5> <p>In this example the schema defines multiple types.</p> <pre class=\"code\">\ncat student;\nJohn\t18\t4.0\nMary\t19   \t3.8\nBill\t20   \t3.9\nJoe\t18   \t3.8\n\nA = LOAD 'student' AS (name:chararray, age:int, gpa:float);\n\nDESCRIBE A;\nA: {name: chararray,age: int,gpa: float}\n\nDUMP A;\n(John,18,4.0F)\n(Mary,19,3.8F)\n(Bill,20,3.9F)\n(Joe,18,3.8F)\n</pre> <p>In this example field \"gpa\" will default to bytearray because no type is declared. </p> <pre class=\"code\">\ncat student;\nJohn\t18\t4.0\nMary\t19\t3.8\nBill\t20\t3.9\nJoe\t18\t3.8\n\nA = LOAD 'data' AS (name:chararray, age:int, gpa);\n\nDESCRIBE A;\nA: {name: chararray,age: int,gpa: bytearray}\n\nDUMP A;\n(John,18,4.0)\n(Mary,19,3.8)\n(Bill,20,3.9)\n(Joe,18,3.8)\n</pre>  <h4 id=\"schema-complex\">Schemas for Complex Data Types</h4> <p>Complex data types include tuples, bags, and maps.</p>  <h4 id=\"tuple-schema\">Tuple Schemas</h4> <p>A tuple is an ordered set of fields.</p>  <h5 id=\"Syntax-N10E0A\">Syntax</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias[:tuple] (alias[:type]) [, (alias[:type]) …] )</p> </td> </tr> </table>  <h5 id=\"Terms-N10E1F\">Terms</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name assigned to the tuple.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>:tuple</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(Optional) The data type, tuple (case insensitive).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>( )</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The designation for a tuple, a set of parentheses.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias[:type]</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The constituents of the tuple, where the schema definition rules for the corresponding type applies to the constituents of the tuple:</p> <ul> <li> <p>alias – the name assigned to the field</p> </li> <li> <p>type (optional) – the simple or complex data type assigned to the field</p> </li> </ul> </td> </tr> </table>  <h5 id=\"Examples-N10E84\">Examples</h5> <p>In this example the schema defines one tuple. The load statements are equivalent.</p> <pre class=\"code\">\ncat data;\n(3,8,9)\n(1,4,7)\n(2,5,8)\n\nA = LOAD 'data' AS (T: tuple (f1:int, f2:int, f3:int));\nA = LOAD 'data' AS (T: (f1:int, f2:int, f3:int));\n\nDESCRIBE A;\nA: {T: (f1: int,f2: int,f3: int)}\n\nDUMP A;\n((3,8,9))\n((1,4,7))\n((2,5,8))\n</pre> <p>In this example the schema defines two tuples.</p> <pre class=\"code\">\ncat data;\n(3,8,9) (mary,19)\n(1,4,7) (john,18)\n(2,5,8) (joe,18)\n\nA = LOAD data AS (F:tuple(f1:int,f2:int,f3:int),T:tuple(t1:chararray,t2:int));\n\nDESCRIBE A;\nA: {F: (f1: int,f2: int,f3: int),T: (t1: chararray,t2: int)}\n\nDUMP A;\n((3,8,9),(mary,19))\n((1,4,7),(john,18))\n((2,5,8),(joe,18))\n</pre>  <h4 id=\"bag-schema\">Bag Schemas</h4> <p>A bag is a collection of tuples.</p>  <h5 id=\"Syntax-N10EA4\">Syntax</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias[:bag] {tuple} </p> </td> </tr> </table>  <h5 id=\"Terms-N10EB9\">Terms</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name assigned to the bag.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>:bag</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(Optional) The data type, bag (case insensitive).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>{ }</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The designation for a bag, a set of curly brackets.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A tuple (see Tuple Schema).</p> </td> </tr> </table>  <h5 id=\"Examples-N10F0F\">Examples</h5> <p>In this example the schema defines a bag. The two load statements are equivalent.</p> <pre class=\"code\">\ncat data;\n{(3,8,9)}\n{(1,4,7)}\n{(2,5,8)}\n\nA = LOAD 'data' AS (B: bag {T: tuple(t1:int, t2:int, t3:int)});\nA = LOAD 'data' AS (B: {T: (t1:int, t2:int, t3:int)});\n\nDESCRIBE A:\nA: {B: {T: (t1: int,t2: int,t3: int)}}\n\nDUMP A;\n({(3,8,9)})\n({(1,4,7)})\n({(2,5,8)})\n</pre>  <h4 id=\"map-schema\">Map Schemas</h4> <p>A map is a set of key value pairs.</p>  <h5 id=\"Syntax+%28%3C%3E+demotes+optional%29\">Syntax (&lt;&gt; demotes optional)</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias&lt;:map&gt; [ &lt;type&gt; ] </p> </td> </tr> </table>  <h5 id=\"Terms-N10F3C\">Terms</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name assigned to the map.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>:map</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(Optional) The data type, map (case insensitive).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>[ ]</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The designation for a map, a set of straight brackets [ ].</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>type</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(Optional) The datatype (all types allowed, bytearray is the default).</p> <p>The type applies to the map value only; the map key is always type chararray (see <a href=\"#map\">Map</a>).</p> <p>If a type is declared then ALL values in the map must be of this type.</p> </td> </tr> </table>  <h5 id=\"Examples-N10F8D\">Examples</h5> <p>In this example the schema defines an untyped map (the map values default to bytearray). The load statements are equivalent.</p> <pre class=\"code\">\ncat data;\n[open#apache]\n[apache#hadoop]\n\nA = LOAD 'data' AS (M:map []);\nA = LOAD 'data' AS (M:[]);\n\nDESCRIBE A;\na: {M: map[ ]}\n\nDUMP A;\n([open#apache])\n([apache#hadoop])\n</pre> <p>This example shows the use of a typed maps.</p> <pre class=\"code\">\n/* Map types are declared*/\na = load '1.txt' as(map[int]); --Map value is int\nb = foreach a generate (map[(i:int)])a0; -- Map value is tuple\nb = stream a through `cat` as (m:map[{(i:int,j:chararray)}]); -- Map value is bag\n\n/* The MapLookup of a typed map will result in a datatype of the map value */\na = load '1.txt' as(map[int]);\nb = foreach a generate $0#'key';\n\n/* Schema for b */\nb: {int}\n\n</pre>  <h4 id=\"schema-multi\">Schemas for Multiple Types</h4> <p>You can define schemas for data that includes multiple types.</p>  <h5 id=\"Example-N10FAD\">Example</h5> <p>In this example the schema defines a tuple, bag, and map.</p> <pre class=\"code\">\nA = LOAD 'mydata' AS (T1:tuple(f1:int, f2:int), B:bag{T2:tuple(t1:float,t2:float)}, M:map[] );\n\nA = LOAD 'mydata' AS (T1:(f1:int, f2:int), B:{T2:(t1:float,t2:float)}, M:[] );\n</pre>  <h5 id=\"previous-relation-shortcut\">Previous Relation Shortcut</h5> <p>There is a shortcut form to reference the relation on the previous line of a pig script or grunt session:</p> <pre class=\"code\">\na = load 'thing' as (x:int);\nb = foreach @ generate x;\nc = foreach @ generate x;\nd = foreach @ generate x;\n</pre> </div>    <h2 id=\"artichmetic-ops\">Arithmetic Operators and More</h2> <div class=\"section\">  <h3 id=\"arithmetic\">Arithmetic Operators</h3>  <h4 id=\"Description\">Description</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operator</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Symbol</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Notes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>addition </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>+</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>subtraction </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>-</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>multiplication </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>*</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>division </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>/</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>modulo </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>%</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Returns the remainder of a divided by b (a%b).</p> <p>Works with integral numbers (int, long). </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bincond </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>? :</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(condition ? value_if_true : value_if_false) </p> <p>The bincond should be enclosed in parenthesis. </p> <p>The schemas for the two conditional outputs of the bincond should match.</p> <p>Use expressions only (relational operators are not allowed).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>case</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>CASE WHEN THEN ELSE END</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>CASE expression [ WHEN value THEN value ]+ [ ELSE value ]? END</p> <p>CASE [ WHEN condition THEN value ]+ [ ELSE value ]? END</p> <p>Case operator is equivalent to nested bincond operators.</p> <p>The schemas for all the outputs of the when/else branches should match.</p> <p>Use expressions only (relational operators are not allowed).</p> </td> </tr> </table>  <h5 id=\"Examples-N110D1\">Examples </h5> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int, f2:int, B:bag{T:tuple(t1:int,t2:int)});\n\nDUMP A;\n(10,1,{(2,3),(4,6)})\n(10,3,{(2,3),(4,6)})\n(10,6,{(2,3),(4,6),(5,7)})\n</pre> <p>In this example the modulo operator is used with fields f1 and f2.</p> <pre class=\"code\">\nX = FOREACH A GENERATE f1, f2, f1%f2;\n\nDUMP X;\n(10,1,0)\n(10,3,1)\n(10,6,4)\n</pre> <p>In this example the bincond operator is used with fields f2 and B. The condition is \"f2 equals 1\"; if the condition is true, return 1; if the condition is false, return the count of the number of tuples in B.</p> <pre class=\"code\">\nX = FOREACH A GENERATE f2, (f2==1?1:COUNT(B));\n\nDUMP X;\n(1,1L)\n(3,2L)\n(6,3L)\n</pre> <p>In this example the case operator is used with field f2. The expression is \"f2 % 2\"; if the expression is equal to 0, return 'even'; if the expression is equal to 1, return 'odd'.</p> <pre class=\"code\">\nX = FOREACH A GENERATE f2, (\n  CASE f2 % 2\n    WHEN 0 THEN 'even'\n    WHEN 1 THEN 'odd'\n  END\n);\nDUMP X;\n(1,odd)\n(3,odd)\n(6,even)\n</pre> <p>This can be also written as follows:</p> <pre class=\"code\">\nX = FOREACH A GENERATE f2, (\n  CASE\n    WHEN f2 % 2 == 0 THEN 'even'\n    WHEN f2 % 2 == 1 THEN 'odd'\n  END\n);\nDUMP X;\n(1,odd)\n(3,odd)\n(6,even)\n</pre>  <h5 id=\"types-table-add\"> Types Table: addition (+) and subtraction (-) operators</h5> <p>* bytearray cast as this data type</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as int </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as long </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as float </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as double </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as double </p> </td> </tr> </table>  <h5 id=\"types-table-mult\">Types Table: multiplication (*) and division (/) operators</h5> <p>* bytearray cast as this data type</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not yet </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as int </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as long </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as float </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as double </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as double </p> </td> </tr> </table>  <h5 id=\"Types+Table%3A+modulo+%28%25%29+operator\">Types Table: modulo (%) operator</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as int </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as long </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> </table>  <h3 id=\"boolops\">Boolean Operators</h3>  <h4 id=\"Description-N117E1\">Description</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operator</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Symbol</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Notes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AND </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>and</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>OR </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>or</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>IN</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>in</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>IN operator is equivalent to nested OR operators.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>NOT</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>not</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> </table> <p>The result of a boolean expression (an expression that includes boolean and comparison operators) is always of type boolean (true or false).</p>  <h5 id=\"Example-N11871\">Example</h5> <pre class=\"code\">\nX = FILTER A BY (f1==8) OR (NOT (f2+f3 &gt; f1)) OR (f1 IN (9, 10, 11));\n</pre>  <h3 id=\"cast\">Cast Operators</h3>  <h4 id=\"Description-N11884\">Description</h4> <p>Pig Latin supports casts as shown in this table. </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p> <strong>from / to</strong> </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>yes</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> </table>  <h5 id=\"Syntax+%C2%A0\">Syntax </h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>{(data_type) | (tuple(data_type)) | (bag{tuple(data_type)}) | (map[]) } field</p> </td> </tr> </table>  <h5 id=\"Terms-N11C85\">Terms</h5> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>(data_type)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The data type you want to cast to, enclosed in parentheses. You can cast to any data type except bytearray (see the table above).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>field</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The field whose type you want to change. </p> <p>The field can be represented by positional notation or by name (alias). For example, if f1 is the first field and type int, you can cast to type long using (long)$0 or (long)f1.</p> </td> </tr> </table>  <h5 id=\"Usage-N11CB8\">Usage</h5> <p>Cast operators enable you to cast or convert data from one type to another, as long as conversion is supported (see the table above). For example, suppose you have an integer field, myint, which you want to convert to a string. You can cast this field from int to chararray using (chararray)myint.</p> <p>Please note the following:</p> <ul> <li> <p>A field can be explicitly cast. Once cast, the field remains that type (it is not automatically cast back). In this example $0 is explicitly cast to int.</p> <pre class=\"code\">\nB = FOREACH A GENERATE (int)$0 + 1;\n</pre> </li> </ul>  <ul> <li> <p>Where possible, Pig performs implicit casts. In this example $0 is cast to int (regardless of underlying data) and $1 is cast to double.</p> <pre class=\"code\">\nB = FOREACH A GENERATE $0 + 1, $1 + 1.0\n</pre> </li> </ul> <ul> <li> <p>When two bytearrays are used in arithmetic expressions or a bytearray expression is used with built in aggregate functions (such as SUM) they are implicitly cast to double. If the underlying data is really int or long, you’ll get better performance by declaring the type or explicitly casting the data.</p> </li> <li> <p>Downcasts may cause loss of data. For example casting from long to int may drop bits.</p> </li> </ul>  <h4 id=\"Examples-N11CF3\">Examples</h4> <p>In this example an int is cast to type chararray (see relation X).</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int,f2:int,f3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n\nB = GROUP A BY f1;\n\nDUMP B;\n(1,{(1,2,3)})\n(4,{(4,2,1),(4,3,3)})\n(7,{(7,2,5)})\n(8,{(8,3,4),(8,4,3)})\n\nDESCRIBE B;\nB: {group: int,A: {f1: int,f2: int,f3: int}}\n\nX = FOREACH B GENERATE group, (chararray)COUNT(A) AS total;\n(1,1)\n(4,2)\n(7,1)\n(8,2)\n\nDESCRIBE X;\nX: {group: int,total: chararray}\n</pre> <p>In this example a bytearray (fld in relation A) is cast to type tuple.</p> <pre class=\"code\">\ncat data;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n\nA = LOAD 'data' AS fld:bytearray;\n\nDESCRIBE A;\na: {fld: bytearray}\n\nDUMP A;\n((1,2,3))\n((4,2,1))\n((8,3,4))\n\nB = FOREACH A GENERATE (tuple(int,int,float))fld;\n\nDESCRIBE B;\nb: {(int,int,float)}\n\nDUMP B;\n((1,2,3))\n((4,2,1))\n((8,3,4))\n</pre> <p>In this example a bytearray (fld in relation A) is cast to type bag.</p> <pre class=\"code\">\ncat data;\n{(4829090493980522200L)}\n{(4893298569862837493L)}\n{(1297789302897398783L)}\n\nA = LOAD 'data' AS fld:bytearray;\n\nDESCRIBE A;\nA: {fld: bytearray}\n\nDUMP A;\n({(4829090493980522200L)})\n({(4893298569862837493L)})\n({(1297789302897398783L)})\n\nB = FOREACH A GENERATE (bag{tuple(long)})fld; \n\nDESCRIBE B;\nB: {{(long)}}\n\nDUMP B;\n({(4829090493980522200L)})\n({(4893298569862837493L)})\n({(1297789302897398783L)})\n</pre> <p>In this example a bytearray (fld in relation A) is cast to type map.</p> <pre class=\"code\">\ncat data;\n[open#apache]\n[apache#hadoop]\n[hadoop#pig]\n[pig#grunt]\n\nA = LOAD 'data' AS fld:bytearray;\n\nDESCRIBE A;\nA: {fld: bytearray}\n\nDUMP A;\n([open#apache])\n([apache#hadoop])\n([hadoop#pig])\n([pig#grunt])\n\nB = FOREACH A GENERATE ((map[])fld;\n\nDESCRIBE B;\nB: {map[ ]}\n\nDUMP B;\n([open#apache])\n([apache#hadoop])\n([hadoop#pig])\n([pig#grunt])\n</pre>  <h4 id=\"cast-relations\">Casting Relations to Scalars</h4> <p>Pig allows you to cast the elements of a single-tuple relation into a scalar value. The tuple can be a single-field or multi-field tulple. If the relation contains more than one tuple, however, a runtime error is generated: \"Scalar has more than one row in the output\". </p> <p>The cast relation can be used in any place where an expression of the type would make sense, including FOREACH, FILTER, and SPLIT. Note that if an explicit cast is not used an implict cast will be inserted according to Pig rules. Also, when the schema can't be inferred bytearray is used.</p> <p>The primary use case for casting relations to scalars is the ability to use the values of global aggregates in follow up computations. </p> <p>In this example the percentage of clicks belonging to a particular user are computed. For the FOREACH statement, an explicit cast is used. If the SUM is not given a name, a position can be used as well (userid, clicks/(double)C.$0). </p> <pre class=\"code\">\nA = load 'mydata' as (userid, clicks); \nB = group A all; \nC = foreach B genertate SUM(A.clicks) as total; \nD = foreach A generate userid, clicks/(double)C.total; \ndump D;\n</pre> <p>In this example a multi-field tuple is used. For the FILTER statement, Pig performs an implicit cast. For the FOREACH statement, an explicit cast is used.</p> <pre class=\"code\">\nA = load 'mydata' as (userid, clicks); \nB = group A all; \nC = foreach B genertate SUM(A.clicks) as total, COUNT(A) as cnt; \nD = FILTER A by clicks &gt; C.total/3 \nE = foreach D generate userid, clicks/(double)C.total, cnt; \ndump E; \n</pre>  <h3 id=\"comparison\">Comparison Operators</h3>  <h4 id=\"Description-N11D3F\">Description</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operator</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Symbol</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Notes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>equal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>==</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>not equal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>!=</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>less than </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>&lt;</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>greater than </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>&gt;</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>less than or equal to </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>&lt;=</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>greater than or equal to</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>&gt;=</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>pattern matching </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>matches</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Takes an expression on the left and a string constant on the right.</p> <p> <em>expression</em> matches <em>string-constant</em> </p> <p>Use the Java <a href=\"http://docs.oracle.com/javase/1.5.0/docs/api/java/util/regex/Pattern.html\">format</a> for regular expressions.</p> </td> </tr> </table> <p>Use the comparison operators with numeric and string data.</p>  <h4 id=\"Examples-N11E2B\">Examples</h4> <p> <strong>Numeric Example</strong> </p> <pre class=\"code\">\nX = FILTER A BY (f1 == 8);\n</pre> <p> <strong>String Example</strong> </p> <pre class=\"code\">\nX = FILTER A BY (f2 == 'apache');\n</pre> <p> <strong>Matches Example</strong> </p> <pre class=\"code\">\nX = FILTER A BY (f1 matches '.*apache.*');\n</pre>  <h4 id=\"types-table-equal\">Types Table: equal (==) operator</h4>  <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>datetime </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> <p>(see Note 1) </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> <p>(see Note 2)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>cast as boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean</p> </td> </tr> </table> <p>Note 1: boolean (Tuple A is equal to tuple B if they have the same size s, and for all 0 &lt;= i &lt; s A[i] == B[i])</p> <p>Note 2: boolean (Map A is equal to map B if A and B have the same number of entries, and for every key k1 in A with a value of v1, there is a key k2 in B with a value of v2, such that k1 == k2 and v1 == v2)</p>  <h4 id=\"types-table-not-equal\">Types Table: not equal (!=) operator</h4>  <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>datetime </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean (bytearray cast as int) </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean (bytearray cast as long) </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean (bytearray cast as float) </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean (bytearray cast as double) </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean (bytearray cast as chararray) </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> </tr> </table>  <h4 id=\"types-table-matches\">Types Table: matches operator</h4> <p>*Cast as chararray (the second argument must be chararray)</p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\">  </td> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray* </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>boolean </p> </td> </tr> </table>  <h3 id=\"type-construction\">Type Construction Operators</h3>  <h4 id=\"Description-N12ACA\">Description</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operator</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Symbol</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Notes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple constructor </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> ( ) </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to construct a tuple from the specified elements. Equivalent to <a href=\"func#totuple\">TOTUPLE</a>.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag constructor</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> { }</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to construct a bag from the specified elements. Equivalent to <a href=\"func#tobag\">TOBAG</a>.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map constructor</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> [ ]</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to construct a map from the specified elements. Equivalent to <a href=\"func#tomap\">TOMAP</a>.</p> </td> </tr> </table>  <p>Note the following:</p> <ul> <li>These operators can be used anywhere where the expression of the corresponding type is acceptable including FOREACH GENERATE, FILTER, etc.</li> <li>A single element enclosed in parens ( ) like (5) is not considered to be a tuple but rather an arithmetic operator.</li> <li>For bags, every element is put in the bag; if the element is not a tuple Pig will create a tuple for it: <ul> <li> Given this {$1, $2} Pig creates this {($1), ($2)} a bag with two tuples <p>... neither $1 and $2 are tuples so Pig creates a tuple around each item</p>  </li> <li> Given this {($1), $2} Pig creates this {($1), ($2)} a bag with two tuples <p>... since ($1) is treated as $1 (one cannot create a single element tuple using this syntax), {($1), $2} becomes {$1, $2} and Pig creates a tuple around each item</p>  </li> <li> Given this {($1, $2)} Pig creates this {($1, $2)} a bag with a single tuple <p>... Pig creates a tuple ($1, $2) and then puts this tuple into the bag</p>  </li> </ul> </li> </ul>  <h4 id=\"Examples-N12B78\">Examples</h4> <p> <strong>Tuple Construction</strong> </p> <pre class=\"code\">\nA = load 'students' as (name:chararray, age:int, gpa:float);\nB = foreach A generate (name, age);\nstore B into 'results';\n\nInput (students):\njoe smith  20  3.5\namy chen   22  3.2\nleo allen  18  2.1\n\nOutput (results):\n(joe smith,20)\n(amy chen,22)\n(leo allen,18)\n</pre> <p> <strong>Bag Construction</strong> </p> <pre class=\"code\">\nA = load 'students' as (name:chararray, age:int, gpa:float);\nB = foreach A generate {(name, age)}, {name, age};\nstore B into 'results';\n\nInput (students):\njoe smith  20  3.5\namy chen   22  3.2\nleo allen  18  2.1\n\nOutput (results):\n{(joe smith,20)}   {(joe smith),(20)}\n{(amy chen,22)}    {(amy chen),(22)}\n{(leo allen,18)}   {(leo allen),(18)}\n</pre> <p> <strong>Map Construction</strong> </p> <pre class=\"code\">\nA = load 'students' as (name:chararray, age:int, gpa:float);\nB = foreach A generate [name, gpa];\nstore B into 'results';\n\nInput (students):\njoe smith  20  3.5\namy chen   22  3.2\nleo allen  18  2.1\n\nOutput (results):\n[joe smith#3.5]\n[amy chen#3.2]\n[leo allen#2.1]\n</pre>  <h3 id=\"deref\">Dereference Operators</h3>  <h4 id=\"Description-N12BA4\">Description</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operator</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Symbol</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Notes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple dereference </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>tuple.id or tuple.(id,…)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Tuple dereferencing can be done by name (tuple.field_name) or position (mytuple.$0). If a set of fields are dereferenced (tuple.(name1, name2) or tuple.($0, $1)), the expression represents a tuple composed of the specified fields. Note that if the dot operator is applied to a bytearray, the bytearray will be assumed to be a tuple.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag dereference</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bag.id or bag.(id,…)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Bag dereferencing can be done by name (bag.field_name) or position (bag.$0). If a set of fields are dereferenced (bag.(name1, name2) or bag.($0, $1)), the expression represents a bag composed of the specified fields.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map dereference</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>map#'key'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Map dereferencing must be done by key (field_name#key or $0#key). If the pound operator is applied to a bytearray, the bytearray is assumed to be a map. If the key does not exist, the empty string is returned.</p> </td> </tr> </table>  <h4 id=\"Examples-N12C1C\">Examples</h4> <p> <strong>Tuple Example</strong> </p> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = LOAD 'data' as (f1:int, f2:tuple(t1:int,t2:int,t3:int));\n\nDUMP A;\n(1,(1,2,3))\n(2,(4,5,6))\n(3,(7,8,9))\n(4,(1,4,7))\n(5,(2,5,8))\n</pre> <p>In this example dereferencing is used to retrieve two fields from tuple f2.</p> <pre class=\"code\">\nX = FOREACH A GENERATE f2.t1,f2.t3;\n\nDUMP X;\n(1,3)\n(4,6)\n(7,9)\n(1,7)\n(2,8)\n</pre> <p> <strong>Bag Example</strong> </p> <p>Suppose we have relation B, formed by grouping relation A (see the GROUP operator for information about the field names in relation B).</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int, f2:int,f3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n\nB = GROUP A BY f1;\n\nDUMP B;\n(1,{(1,2,3)})\n(4,{(4,2,1),(4,3,3)})\n(7,{(7,2,5)})\n(8,{(8,3,4),(8,4,3)})\n\nILLUSTRATE B;\n<em>etc …</em>\n----------------------------------------------------------\n| b   | group: int | a: bag({f1: int,f2: int,f3: int}) |\n----------------------------------------------------------\n</pre> <p>In this example dereferencing is used with relation X to project the first field (f1) of each tuple in the bag (a).</p> <pre class=\"code\">\nX = FOREACH B GENERATE a.f1;\n\nDUMP X;\n({(1)})\n({(4),(4)})\n({(7)})\n({(8),(8)})\n</pre> <p> <strong>Tuple/Bag Example</strong> </p> <p>Suppose we have relation B, formed by grouping relation A (see the GROUP operator for information about the field names in relation B).</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int, f2:int, f3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n\nB = GROUP A BY (f1,f2);\n\nDUMP B;\n((1,2),{(1,2,3)})\n((4,2),{(4,2,1)})\n((4,3),{(4,3,3)})\n((7,2),{(7,2,5)})\n((8,3),{(8,3,4)})\n((8,4),{(8,4,3)})\n\nILLUSTRATE B;\n<em>etc …</em>\n-------------------------------------------------------------------------------\n| b     | group: tuple({f1: int,f2: int}) | a: bag({f1: int,f2: int,f3: int}) |\n-------------------------------------------------------------------------------\n|       | (8, 3)                                | {(8, 3, 4), (8, 3, 4)} |\n-------------------------------------------------------------------------------\n</pre> <p>In this example dereferencing is used to project a field (f1) from a tuple (group) and a field (f1) from a bag (a).</p> <pre class=\"code\">\nX = FOREACH B GENERATE group.f1, a.f1;\n\nDUMP X;\n(1,{(1)})\n(4,{(4)})\n(4,{(4)})\n(7,{(7)})\n(8,{(8)})\n(8,{(8)})\n</pre> <p> <strong>Map Example</strong> </p> <p>Suppose we have relation A. </p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int, f2:map[]);\n\nDUMP A;\n(1,[open#apache])\n(2,[apache#hadoop])\n(3,[hadoop#pig])\n(4,[pig#grunt])\n</pre> <p>In this example dereferencing is used to look up the value of key 'open'.</p> <pre class=\"code\">\nX = FOREACH A GENERATE f2#'open';\n\nDUMP X;\n(apache)\n()\n()\n()\n</pre>  <h3 id=\"disambiguate\">Disambiguate Operator</h3> <p>Use the disambiguate operator ( :: ) to identify field names after JOIN, COGROUP, CROSS, or FLATTEN operators.</p> <p>In this example, to disambiguate y, use A::y or B::y. In cases where there is no ambiguity, such as z, the :: is not necessary but is still supported.</p> <pre class=\"code\">\nA = load 'data1' as (x, y);\nB = load 'data2' as (x, y, z);\nC = join A by x, B by x;\nD = foreach C generate y; -- which y?\n</pre>  <h3 id=\"flatten\">Flatten Operator</h3> <p>The FLATTEN operator looks like a UDF syntactically, but it is actually an operator that changes the structure of tuples and bags in a way that a UDF cannot. Flatten un-nests tuples as well as bags. The idea is the same, but the operation and result is different for each type of structure.</p> <p>For tuples, flatten substitutes the fields of a tuple in place of the tuple. For example, consider a relation that has a tuple of the form (a, (b, c)). The expression GENERATE $0, flatten($1), will cause that tuple to become (a, b, c).</p> <p>For bags, the situation becomes more complicated. When we un-nest a bag, we create new tuples. If we have a relation that is made up of tuples of the form ({(b,c),(d,e)}) and we apply GENERATE flatten($0), we end up with two tuples (b,c) and (d,e). When we remove a level of nesting in a bag, sometimes we cause a cross product to happen. For example, consider a relation that has a tuple of the form (a, {(b,c), (d,e)}), commonly produced by the GROUP operator. If we apply the expression GENERATE $0, flatten($1) to this tuple, we will create new tuples: (a, b, c) and (a, d, e).</p> <p>Also note that the flatten of empty bag will result in that row being discarded; no output is generated. (See also <a href=\"perf#nulls\">Drop Nulls Before a Join</a>.) </p> <pre class=\"code\">\ngrunt&gt; cat empty.bag\n{}      1\ngrunt&gt; A = LOAD 'empty.bag' AS (b : bag{}, i : int);\ngrunt&gt; B = FOREACH A GENERATE flatten(b), i;\ngrunt&gt; DUMP B;\ngrunt&gt;\n</pre> <p>For examples using the FLATTEN operator, see <a href=\"#flatten-example\">FOREACH</a>.</p>  <h3 id=\"null_operators\">Null Operators</h3>  <h4 id=\"Description-N12CB7\">Description</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operator</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Symbol</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Notes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"is-null\">is null </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>is null</p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"is-not-null\">is not null </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>is not null </p> </td> <td colspan=\"1\" rowspan=\"1\">  </td> </tr> </table>  <p>For a detailed discussion of nulls see <a href=\"#nulls\">Nulls and Pig Latin</a>.</p>  <h4 id=\"Examples-N12D1B\">Examples</h4> <p>In this example, values that are not null are obtained.</p> <pre class=\"code\">\nX = FILTER A BY f1 is not null;\n</pre>  <h4 id=\"types-table-nulls\">Types Table</h4> <p>The null operators can be applied to all data types (see <a href=\"#nulls\">Nulls and Pig Latin</a>). </p>  <h3 id=\"sign\">Sign Operators</h3>  <h4 id=\"Description-N12D40\">Description</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>Operator</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Symbol</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Notes</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>positive </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>+</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Has no effect.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>negative (negation)</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> -</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p> Changes the sign of a positive or negative number.</p> </td> </tr> </table>  <h4 id=\"Examples-N12D9B\">Examples</h4> <p>In this example, the negation operator is applied to the \"x\" values.</p> <pre class=\"code\">\nA = LOAD 'data' as (x, y, z);\n\nB = FOREACH A GENERATE -x, y;\n</pre>  <h4 id=\"types-table-negative\">Types Table: negative ( - ) operator</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bag </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>tuple </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>map </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>int </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>long </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>float </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>chararray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bytearray </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>double (as double) </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>datetime </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>error </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>biginteger </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>bigdecimal </p> </td> </tr> </table> </div>    <h2 id=\"Relational+Operators\">Relational Operators</h2> <div class=\"section\">  <h3 id=\"assert\">ASSERT</h3> <p>Assert a condition on the data.</p>  <h4 id=\"Syntax-N12EAE\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ASSERT alias BY expression [, message];</p> </td> </tr> </table>  <h4 id=\"Terms-N12EC3\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BY</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Required keyword.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A boolean expression.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>message</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Error message when assertion fails.</p> </td> </tr> </table>  <h4 id=\"Usage-N12F19\">Usage</h4> <p>Use assert to ensure a condition is true on your data. Processing fails if any of the records voilate the condition.</p>  <h4 id=\"Examples-N12F23\">Examples</h4> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a0:int,a1:int,a2:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n</pre> <p>Now, you can assert that a0 column in your data is &gt;0, fail if otherwise</p> <pre class=\"code\">\nASSERT A by a0 &gt; 0, 'a0 should be greater than 0';\n</pre>  <h3 id=\"cogroup\">COGROUP</h3> <p>See the <a href=\"#group\">GROUP</a> operator.</p>  <h3 id=\"cross\">CROSS</h3> <p>Computes the cross product of two or more relations.</p>  <h4 id=\"Syntax-N12F53\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = CROSS alias, alias [, alias …] [PARTITION BY partitioner] [PARALLEL n];</p> </td> </tr> </table>  <h4 id=\"Terms-N12F68\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"partition-by-cross\">PARTITION BY partitioner</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p> <ul> <li> <p>For more details, see <a href=\"http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html\">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a> </p> </li> <li> <p>For usage, see <a href=\"#partitionby\">Example: PARTITION BY</a> </p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PARALLEL n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Increase the parallelism of a job by specifying the number of reduce tasks, n. </p> <p>For more information, see <a href=\"perf#parallel\">Use the Parallel Features</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N12FC7\">Usage</h4> <p>Use the CROSS operator to compute the cross product (Cartesian product) of two or more relations.</p> <p>CROSS is an expensive operation and should be used sparingly. </p>  <h4 id=\"Example-N12FD4\">Example</h4> <p>Suppose we have relations A and B.</p> <pre class=\"code\">\nA = LOAD 'data1' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n\nB = LOAD 'data2' AS (b1:int,b2:int);\n\nDUMP B;\n(2,4)\n(8,9)\n(1,3)\n</pre> <p>In this example the cross product of relation A and B is computed.</p> <pre class=\"code\">\nX = CROSS A, B;\n\nDUMP X;\n(1,2,3,2,4)\n(1,2,3,8,9)\n(1,2,3,1,3)\n(4,2,1,2,4)\n(4,2,1,8,9)\n(4,2,1,1,3)\n</pre>  <h3 id=\"cube\">CUBE</h3> <p>Performs cube/rollup operations.</p>  <h4 id=\"Cube+operation\">Cube operation</h4> <p>Cube operation computes aggregates for all possbile combinations of specified group by dimensions. The number of group by combinations generated by cube for n dimensions will be 2^n.</p>  <h4 id=\"Rollup+operation\">Rollup operation</h4> <p>Rollup operations computes multiple levels of aggregates based on hierarchical ordering of specified group by dimensions. Rollup is useful when there is hierarchical ordering on the dimensions. The number of group by combinations generated by rollup for n dimensions will be n+1.</p>  <h4 id=\"Syntax-N13008\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = CUBE alias BY { CUBE expression | ROLLUP expression }, [ CUBE expression | ROLLUP expression ] [PARALLEL n];</p> </td> </tr> </table>  <h4 id=\"Terms-N1301D\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>CUBE</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BY</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Projections (dimensions) of the relation. Supports field, star and project-range expressions.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ROLLUP</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PARALLEL n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Increase the parallelism of a job by specifying the number of reduce tasks, n.</p> <p>For more information, see <a href=\"perf#parallel\">Use the Parallel Features</a>.</p> </td> </tr> </table>  <h4 id=\"Example-N130A0\">Example</h4>  <h4 id=\"Basic+usage+of+CUBE+operation\">Basic usage of CUBE operation</h4> <pre class=\"code\">\nsalesinp = LOAD '/pig/data/salesdata' USING PigStorage(',') AS\n    (product:chararray, year:int, region:chararray, state:chararray, city:chararray, sales:long);\ncubedinp = CUBE salesinp BY CUBE(product,year);\nresult = FOREACH cubedinp GENERATE FLATTEN(group), SUM(cube.sales) AS totalsales;</pre> <p>For a sample input tuple (car, 2012, midwest, ohio, columbus, 4000), the above query with cube operation will output</p> <pre class=\"code\">\n(car,2012,4000)\n(car,,4000)\n(,2012,4000)\n(,,4000)</pre>  <h4 id=\"Output+schema\">Output schema</h4> <pre class=\"code\">\ngrunt&gt; describe cubedinp;\ncubedinp: {group: (product: chararray,year: int),cube: {(product: chararray,year: int,region: chararray,\nstate: chararray,city: chararray,sales: long)}}</pre> <p>Note the second column, ‘cube’ field which is a bag of all tuples that belong to ‘group’. Also note that the measure attribute ‘sales’ along with other unused dimensions in load statement are pushed down so that it can be referenced later while computing aggregates on the measure, like in this case SUM(cube.sales).</p>  <h4 id=\"Basic+usage+of+ROLLUP+operation\">Basic usage of ROLLUP operation</h4> <pre class=\"code\">\nsalesinp = LOAD '/pig/data/salesdata' USING PigStorage(',') AS\n    (product:chararray, year:int, region:chararray, state:chararray, city:chararray, sales:long);\nrolledup = CUBE salesinp BY ROLLUP(region,state,city);\nresult = FOREACH rolledup GENERATE FLATTEN(group), SUM(cube.sales) AS totalsales;</pre> <p>For a sample input tuple (car, 2012, midwest, ohio, columbus, 4000), the above query with rollup operation will output</p> <pre class=\"code\">\n(midwest,ohio,columbus,4000)\n(midwest,ohio,,4000)\n(midwest,,,4000)\n(,,,4000)</pre>  <h4 id=\"Output+schema-N130D9\">Output schema</h4> <pre class=\"code\">\ngrunt&gt; describe rolledup;\nrolledup: {group: (region: chararray,state: chararray,city: chararray),cube: {(region: chararray,\nstate: chararray,city: chararray,product: chararray,year: int,sales: long)}}</pre>  <h4 id=\"Basic+usage+of+CUBE+and+ROLLUP+operation+combined\">Basic usage of CUBE and ROLLUP operation combined</h4> <p>If CUBE and ROLLUP operations are used together, the output groups will be the cross product of all groups generated by cube and rollup operation. If there are m dimensions in cube operations and n dimensions in rollup operation then overall number of combinations will be (2^m) * (n+1).</p> <pre class=\"code\">\nsalesinp = LOAD '/pig/data/salesdata' USING PigStorage(',') AS\n    (product:chararray, year:int, region:chararray, state:chararray, city:chararray, sales:long);\ncubed_and_rolled = CUBE salesinp BY CUBE(product,year), ROLLUP(region, state, city);\nresult = FOREACH cubed_and_rolled GENERATE FLATTEN(group), SUM(cube.sales) AS totalsales;</pre> <p>For a sample input tuple (car, 2012, midwest, ohio, columbus, 4000), the above query with cube and rollup operation will output</p> <pre class=\"code\">\n(car,2012,midwest,ohio,columbus,4000)\n(car,2012,midwest,ohio,,4000)\n(car,2012,midwest,,,4000)\n(car,2012,,,,4000)\n(car,,midwest,ohio,columbus,4000)\n(car,,midwest,ohio,,4000)\n(car,,midwest,,,4000)\n(car,,,,,4000)\n(,2012,midwest,ohio,columbus,4000)\n(,2012,midwest,ohio,,4000)\n(,2012,midwest,,,4000)\n(,2012,,,,4000)\n(,,midwest,ohio,columbus,4000)\n(,,midwest,ohio,,4000)\n(,,midwest,,,4000)\n(,,,,,4000)</pre>  <h4 id=\"Output+schema-N130F9\">Output schema</h4> <pre class=\"code\">\ngrunt&gt; describe cubed_and_rolled;\ncubed_and_rolled: {group: (product: chararray,year: int,region: chararray,\nstate: chararray,city: chararray),cube: {(product: chararray,year: int,region: chararray,\nstate: chararray,city: chararray,sales: long)}}</pre>  <h4 id=\"Handling+null+values+in+dimensions\">Handling null values in dimensions</h4> <p>Since null values are used to represent subtotals in cube and rollup operation, in order to differentiate the legitimate null values that already exists as dimension values, CUBE operator converts any null values in dimensions to \"unknown\" value before performing cube or rollup operation. For example, for CUBE(product,location) with a sample tuple (car,) the output will be </p> <pre class=\"code\">\n(car,unknown)\n(car,)\n(,unknown)\n(,)</pre>  <h3 id=\"define\">DEFINE</h3> <p>See:</p> <ul> <li> <a href=\"basic#define-udfs\">DEFINE (UDFs, streaming)</a> </li> <li> <a href=\"cont#define-macros\">DEFINE (macros)</a> </li> </ul>  <h3 id=\"distinct\">DISTINCT </h3> <p>Removes duplicate tuples in a relation.</p>  <h4 id=\"Syntax-N13137\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = DISTINCT alias [PARTITION BY partitioner] [PARALLEL n]; </p> </td> </tr> </table>  <h4 id=\"Terms-N1314C\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"partition-by-distinct\">PARTITION BY partitioner</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p> <ul> <li> <p>For more details, see <a href=\"http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html\">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a> </p> </li> <li> <p>For usage, see <a href=\"#partitionby\">Example: PARTITION BY</a>.</p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PARALLEL n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Increase the parallelism of a job by specifying the number of reduce tasks, n.</p> <p>For more information, see <a href=\"perf#parallel\">Use the Parallel Features</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N131AD\">Usage</h4> <p>Use the DISTINCT operator to remove duplicate tuples in a relation. DISTINCT does not preserve the original order of the contents (to eliminate duplicates, Pig must first sort the data). You cannot use DISTINCT on a subset of fields; to do this, use FOREACH and a nested block to first select the fields and then apply DISTINCT (see <a href=\"#nestedblock\">Example: Nested Block</a>).</p>  <h4 id=\"Example-N131BB\">Example</h4> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(8,3,4)\n(1,2,3)        \n(4,3,3)        \n(4,3,3)        \n(1,2,3) \n</pre> <p>In this example all duplicate tuples are removed.</p> <pre class=\"code\">\nX = DISTINCT A;\n\nDUMP X;\n(1,2,3)\n(4,3,3)\n(8,3,4)\n</pre>  <h3 id=\"filter\">FILTER </h3> <p>Selects tuples from a relation based on some condition.</p>  <h4 id=\"Syntax-N131DB\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = FILTER alias BY expression;</p> </td> </tr> </table>  <h4 id=\"Terms-N131EF\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BY</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Required keyword.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A boolean expression.</p> </td> </tr> </table>  <h4 id=\"Usage-N13232\">Usage</h4> <p>Use the FILTER operator to work with tuples or rows of data (if you want to work with columns of data, use the FOREACH...GENERATE operation).</p> <p>FILTER is commonly used to select the data that you want; or, conversely, to filter out (remove) the data you don’t want.</p>  <h4 id=\"Examples-N1323F\">Examples</h4> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n</pre> <p>In this example the condition states that if the third field equals 3, then include the tuple with relation X.</p> <pre class=\"code\">\nX = FILTER A BY f3 == 3;\n\nDUMP X;\n(1,2,3)\n(4,3,3)\n(8,4,3)\n</pre> <p>In this example the condition states that if the first field equals 8 or if the sum of fields f2 and f3 is not greater than first field, then include the tuple relation X.</p> <pre class=\"code\">\nX = FILTER A BY (f1 == 8) OR (NOT (f2+f3 &gt; f1));\n\nDUMP X;\n(4,2,1)\n(8,3,4)\n(7,2,5)\n(8,4,3)\n</pre>  <h3 id=\"foreach\">FOREACH</h3> <p>Generates data transformations based on columns of data.</p>  <h4 id=\"Syntax-N13266\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = FOREACH { block | nested_block };</p> </td> </tr> </table>  <h4 id=\"Terms-N1327B\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of relation (outer bag).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>block</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>FOREACH…GENERATE block used with a relation (outer bag). Use this syntax:</p>  <p>alias = FOREACH alias GENERATE expression [AS schema] [expression [AS schema]….];</p> <p>See <a href=\"#schemas\">Schemas</a> </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"nested-block\">nested_block</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Nested FOREACH...GENERATE block used with a inner bag. Use this syntax:</p>  <p>alias = FOREACH nested_alias {</p> <p> alias = {nested_op | nested_exp}; [{alias = {nested_op | nested_exp}; …]</p> <p> GENERATE expression [AS schema] [expression [AS schema]….]</p> <p>};</p>  <p>Where:</p> <p>The nested block is enclosed in opening and closing brackets { … }. </p> <p>The GENERATE keyword must be the last statement within the nested block.</p> <p>See <a href=\"#schemas\">Schemas</a> </p> <p>Macros are NOT alllowed inside a nested block.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>nested_alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the inner bag.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>nested_op</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Allowed operations are CROSS, DISTINCT, FILTER, FOREACH, LIMIT, and ORDER BY. </p> <p>Note: FOREACH statements can be nested to two levels only. FOREACH statements that are nested to three or more levels will result in a grammar error.</p> <p>You can also perform projections within the nested block.</p> <p>For examples, see <a href=\"#nestedblock\">Example: Nested Block</a>.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>nested_exp</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Any arbitrary, supported expression.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AS</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>schema</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A schema using the AS keyword (see <a href=\"#schemas\">Schemas</a>).</p> <ul> <li> <p>If the <a href=\"#flatten\">FLATTEN</a> operator is used, enclose the schema in parentheses.</p> </li> <li> <p>If the FLATTEN operator is not used, don't enclose the schema in parentheses.</p> </li> </ul> </td> </tr> </table>  <h4 id=\"Usage-N13381\">Usage</h4> <p>Use the FOREACH…GENERATE operation to work with columns of data (if you want to work with tuples or rows of data, use the FILTER operation).</p> <p>FOREACH...GENERATE works with relations (outer bags) as well as inner bags:</p> <ul> <li> <p>If A is a relation (outer bag), a FOREACH statement could look like this.</p> <pre class=\"code\">\nX = FOREACH A GENERATE f1;\n</pre> </li> <li> <p>If A is an inner bag, a FOREACH statement could look like this.</p> <pre class=\"code\">\nX = FOREACH B {\n        S = FILTER A BY 'xyz';\n        GENERATE COUNT (S.$0);\n}\n</pre> </li> </ul>  <h4 id=\"projection\">Example: Projection</h4> <p>In this example the asterisk (*) is used to project all fields from relation A to relation X. Relation A and X are identical.</p> <pre class=\"code\">\nX = FOREACH A GENERATE *;\n\nDUMP X;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n</pre> <p>In this example two fields from relation A are projected to form relation X. </p> <pre class=\"code\">\nX = FOREACH A GENERATE a1, a2;\n\nDUMP X;\n(1,2)\n(4,2)\n(8,3)\n(4,3)\n(7,2)\n(8,4)\n</pre>  <h4 id=\"Example%3A+Nested+Projection\">Example: Nested Projection</h4> <p>In this example if one of the fields in the input relation is a tuple, bag or map, we can perform a projection on that field (using a deference operator).</p> <pre class=\"code\">\nX = FOREACH C GENERATE group, B.b2;\n\nDUMP X;\n(1,{(3)})\n(4,{(6),(9)})\n(8,{(9)})\n</pre> <p>In this example multiple nested columns are retained.</p> <pre class=\"code\">\nX = FOREACH C GENERATE group, A.(a1, a2);\n\nDUMP X;\n(1,{(1,2)})\n(4,{(4,2),(4,3)})\n(8,{(8,3),(8,4)})\n</pre>  <h4 id=\"Example%3A+Schema\">Example: Schema</h4> <p>In this example two fields in relation A are summed to form relation X. A schema is defined for the projected field.</p> <pre class=\"code\">\nX = FOREACH A GENERATE a1+a2 AS f1:int;\n\nDESCRIBE X;\nx: {f1: int}\n\nDUMP X;\n(3)\n(6)\n(11)\n(7)\n(9)\n(12)\n\nY = FILTER X BY f1 &gt; 10;\n\nDUMP Y;\n(11)\n(12)\n</pre>  <h4 id=\"Example%3A+Applying+Functions\">Example: Applying Functions</h4> <p>In this example the built in function SUM() is used to sum a set of numbers in a bag.</p> <pre class=\"code\">\nX = FOREACH C GENERATE group, SUM (A.a1);\n\nDUMP X;\n(1,1)\n(4,8)\n(8,16)\n</pre>  <h4 id=\"flatten-example\">Example: Flatten</h4> <p>In this example the <a href=\"#flatten\">FLATTEN</a> operator is used to eliminate nesting. </p> <pre class=\"code\">\nX = FOREACH C GENERATE group, FLATTEN(A);\n\nDUMP X;\n(1,1,2,3)\n(4,4,2,1)\n(4,4,3,3)\n(8,8,3,4)\n(8,8,4,3)\n</pre> <p>Another FLATTEN example.</p> <pre class=\"code\">\nX = FOREACH C GENERATE GROUP, FLATTEN(A.a3);\n\nDUMP X;\n(1,3)\n(4,1)\n(4,3)\n(8,4)\n(8,3)\n</pre> <p>Another FLATTEN example. Note that for the group '4' in C, there are two tuples in each bag. Thus, when both bags are flattened, the cross product of these tuples is returned; that is, tuples (4, 2, 6), (4, 3, 6), (4, 2, 9), and (4, 3, 9).</p> <pre class=\"code\">\nX = FOREACH C GENERATE FLATTEN(A.(a1, a2)), FLATTEN(B.$1);\n\nDUMP X;\n(1,2,3)\n(4,2,6)\n(4,2,9)\n(4,3,6)\n(4,3,9)\n(8,3,9)\n(8,4,9)\n</pre> <p>Another FLATTEN example. Here, relations A and B both have a column x. When forming relation E, you need to use the :: operator to identify which column x to use - either relation A column x (A::x) or relation B column x (B::x). This example uses relation A column x (A::x).</p> <pre class=\"code\">\nA = LOAD 'data' AS (x, y);\nB = LOAD 'data' AS (x, z);\nC = COGROUP A BY x, B BY x;\nD = FOREACH C GENERATE flatten(A), flatten(b);\nE = GROUP D BY A::x;\n……\n</pre>  <h4 id=\"nestedblock\">Example: Nested Block</h4> <p>In this example a CROSS is performed within the nested block.</p> <pre class=\"code\">\nuser = load 'user' as (uid, age, gender, region);\nsession = load 'session' as (uid, region);\nC = cogroup user by uid, session by uid;\nD = foreach C {\n    crossed = cross user, session;\n    generate crossed;\n}\ndump D;  \n</pre> <p>In this example FOREACH is nested to the second level.</p> <pre class=\"code\">\na = load '1.txt' as (a0, a1:chararray, a2:chararray); \nb = group a by a0; \nc = foreach b { \n    c0 = foreach a generate TOMAP(a1,a2); \n    generate c0; \n} \ndump c; \n</pre> <p>This example shows a CROSS and FOREACH nested to the second level.</p> <pre class=\"code\">\na = load '1.txt' as (a0, a1, a2); \nb = load '2.txt' as (b0, b1); \nc = cogroup a by a0, b by b0; \nd = foreach c { \n    d0 = cross a, b; \n    d1 = foreach d0 generate a1+b1; \n    generate d1; \n} \ndump d;\n</pre> <p>Suppose we have relations A and B. Note that relation B contains an inner bag.</p> <pre class=\"code\">\nA = LOAD 'data' AS (url:chararray,outlink:chararray);\n\nDUMP A;\n(www.ccc.com,www.hjk.com)\n(www.ddd.com,www.xyz.org)\n(www.aaa.com,www.cvn.org)\n(www.www.com,www.kpt.net)\n(www.www.com,www.xyz.org)\n(www.ddd.com,www.xyz.org)\n\nB = GROUP A BY url;\n\nDUMP B;\n(www.aaa.com,{(www.aaa.com,www.cvn.org)})\n(www.ccc.com,{(www.ccc.com,www.hjk.com)})\n(www.ddd.com,{(www.ddd.com,www.xyz.org),(www.ddd.com,www.xyz.org)})\n(www.www.com,{(www.www.com,www.kpt.net),(www.www.com,www.xyz.org)})\n</pre> <p>In this example we perform two of the operations allowed in a nested block, FILTER and DISTINCT. Note that the last statement in the nested block must be GENERATE. Also, note the use of projection (PA = FA.outlink;) to retrieve a field. DISTINCT can be applied to a subset of fields (as opposed to a relation) only within a nested block.</p> <pre class=\"code\">\nX = FOREACH B {\n        FA= FILTER A BY outlink == 'www.xyz.org';\n        PA = FA.outlink;\n        DA = DISTINCT PA;\n        GENERATE group, COUNT(DA);\n}\n\nDUMP X;\n(www.aaa.com,0)\n(www.ccc.com,0)\n(www.ddd.com,1)\n(www.www.com,1)\n</pre>  <h3 id=\"group\">GROUP</h3> <p>Groups the data in one or more relations.</p> <p>Note: The GROUP and COGROUP operators are identical. Both operators work with one or more relations. For readability GROUP is used in statements involving one relation and COGROUP is used in statements involving two or more relations. You can COGROUP up to but no more than 127 relations at a time.</p>  <h4 id=\"Syntax-N1344A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = GROUP alias { ALL | BY expression} [, alias ALL | BY expression …] [USING 'collected' | 'merge'] [PARTITION BY partitioner] [PARALLEL n];</p> </td> </tr> </table>  <h4 id=\"Terms-N1345E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> <p>You can COGROUP up to but no more than 127 relations at a time.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ALL</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword. Use ALL if you want all tuples to go to a single group; for example, when doing aggregates across entire relations.</p> <p>B = GROUP A ALL;</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BY</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword. Use this clause to group the relation by field, tuple or expression.</p> <p>B = GROUP A BY f1;</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A tuple expression. This is the group key or key field. If the result of the tuple expression is a single field, the key will be the value of the first field rather than a tuple with one field. To group using multiple keys, enclose the keys in parentheses:</p> <p>B = GROUP A BY (key1,key2);</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>USING</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'collected'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use the ‘collected’ clause with the GROUP operation (works with one relation only).</p> <p>The following conditions apply:</p> <ul> <li> <p>The loader must implement the {CollectableLoader} interface.</p> </li> <li> <p>Data must be sorted on the group key.</p> </li> </ul>  <p>If your data and loaders satisfy these conditions, use the ‘collected’ clause to perform an optimized version of GROUP; the operation will execute on the map side and avoid running the reduce phase.</p>  </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'merge'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use the ‘merge’ clause with the COGROUP operation (works with two or more relations only).</p> <p>The following conditions apply:</p> <ul> <li> <p>No other operations can be done between the LOAD and COGROUP statements.</p> </li> <li> <p>Data must be sorted on the COGROUP key for all tables in ascending (ASC) order.</p> </li> <li> <p>Nulls are considered smaller than evertyhing. If data contains null keys, they should occur before anything else.</p> </li> <li> <p>Left-most loader must implement the {CollectableLoader} interface as well as {OrderedLoadFunc} interface.</p> </li> <li> <p>All other loaders must implement IndexableLoadFunc.</p> </li> <li> <p>Type information must be provided in the schema for all the loaders.</p> </li> </ul>  <p>If your data and loaders satisfy these conditions, the ‘merge’ clause to perform an optimized version of COGROUP; the operation will execute on the map side and avoid running the reduce phase.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"partition-by-group\">PARTITION BY partitioner</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p> <ul> <li> <p>For more details, see <a href=\"http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html\">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a> </p> </li> <li> <p>For usage, see <a href=\"#partitionby\">Example: PARTITION BY</a> </p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PARALLEL n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Increase the parallelism of a job by specifying the number of reduce tasks, n.</p> <p>For more information, see <a href=\"perf#Parallel\">Use the Parallel Features</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N13583\">Usage</h4> <p>The GROUP operator groups together tuples that have the same group key (key field). The key field will be a tuple if the group key has more than one field, otherwise it will be the same type as that of the group key. The result of a GROUP operation is a relation that includes one tuple per group. This tuple contains two fields: </p> <ul> <li> <p>The first field is named \"group\" (do not confuse this with the GROUP operator) and is the same type as the group key.</p> </li> <li> <p>The second field takes the name of the original relation and is type bag.</p> </li> <li> <p>The names of both fields are generated by the system as shown in the example below.</p> </li> </ul>  <p>Note the following about the GROUP/COGROUP and JOIN operators:</p> <ul> <li> <p>The GROUP and JOIN operators perform similar functions. GROUP creates a nested set of output tuples while JOIN creates a flat set of output tuples</p> </li> <li> <p>The GROUP/COGROUP and JOIN operators handle null values differently (see <a href=\"#nulls_group\">Nulls and GROUP/COGROUP Operataors</a>).</p> </li> </ul>  <h4 id=\"Example-N135BA\">Example</h4> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = load 'student' AS (name:chararray,age:int,gpa:float);\n\nDESCRIBE A;\nA: {name: chararray,age: int,gpa: float}\n\nDUMP A;\n(John,18,4.0F)\n(Mary,19,3.8F)\n(Bill,20,3.9F)\n(Joe,18,3.8F)\n</pre> <p>Now, suppose we group relation A on field \"age\" for form relation B. We can use the DESCRIBE and ILLUSTRATE operators to examine the structure of relation B. Relation B has two fields. The first field is named \"group\" and is type int, the same as field \"age\" in relation A. The second field is name \"A\" after relation A and is type bag.</p> <pre class=\"code\">\nB = GROUP A BY age;\n\nDESCRIBE B;\nB: {group: int, A: {name: chararray,age: int,gpa: float}}\n\nILLUSTRATE B;\n<em>etc ... </em>\n----------------------------------------------------------------------\n| B     | group: int | A: bag({name: chararray,age: int,gpa: float}) |\n----------------------------------------------------------------------\n|       | 18         | {(John, 18, 4.0), (Joe, 18, 3.8)}             |\n|       | 20         | {(Bill, 20, 3.9)}                             |\n----------------------------------------------------------------------\n\nDUMP B;\n(18,{(John,18,4.0F),(Joe,18,3.8F)})\n(19,{(Mary,19,3.8F)})\n(20,{(Bill,20,3.9F)})\n</pre> <p>Continuing on, as shown in these FOREACH statements, we can refer to the fields in relation B by names \"group\" and \"A\" or by positional notation.</p> <pre class=\"code\">\nC = FOREACH B GENERATE group, COUNT(A);\n\nDUMP C;\n(18,2L)\n(19,1L)\n(20,1L)\n\nC = FOREACH B GENERATE $0, $1.name;\n\nDUMP C;\n(18,{(John),(Joe)})\n(19,{(Mary)})\n(20,{(Bill)})\n</pre>  <h4 id=\"Example-N135D9\">Example</h4> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = LOAD 'data' as (f1:chararray, f2:int, f3:int);\n\nDUMP A;\n(r1,1,2)\n(r2,2,1)\n(r3,2,8)\n(r4,4,4)\n</pre> <p>In this example the tuples are grouped using an expression, f2*f3.</p> <pre class=\"code\">\nX = GROUP A BY f2*f3;\n\nDUMP X;\n(2,{(r1,1,2),(r2,2,1)})\n(16,{(r3,2,8),(r4,4,4)})\n</pre>  <h4 id=\"Example-N135EE\">Example</h4> <p>Suppose we have two relations, A and B.</p> <pre class=\"code\">\nA = LOAD 'data1' AS (owner:chararray,pet:chararray);\n\nDUMP A;\n(Alice,turtle)\n(Alice,goldfish)\n(Alice,cat)\n(Bob,dog)\n(Bob,cat)\n\nB = LOAD 'data2' AS (friend1:chararray,friend2:chararray);\n\nDUMP B;\n(Cindy,Alice)\n(Mark,Alice)\n(Paul,Bob)\n(Paul,Jane)\n</pre> <p>In this example tuples are co-grouped using field “owner” from relation A and field “friend2” from relation B as the key fields. The DESCRIBE operator shows the schema for relation X, which has three fields, \"group\", \"A\" and \"B\" (see the GROUP operator for information about the field names).</p> <pre class=\"code\">\nX = COGROUP A BY owner, B BY friend2;\n\nDESCRIBE X;\nX: {group: chararray,A: {owner: chararray,pet: chararray},B: {friend1: chararray,friend2: chararray}}\n</pre> <p>Relation X looks like this. A tuple is created for each unique key field. The tuple includes the key field and two bags. The first bag is the tuples from the first relation with the matching key field. The second bag is the tuples from the second relation with the matching key field. If no tuples match the key field, the bag is empty.</p> <pre class=\"code\">\n(Alice,{(Alice,turtle),(Alice,goldfish),(Alice,cat)},{(Cindy,Alice),(Mark,Alice)})\n(Bob,{(Bob,dog),(Bob,cat)},{(Paul,Bob)})\n(Jane,{},{(Paul,Jane)})\n</pre>  <h4 id=\"Example-N1360A\">Example</h4> <p>This example shows how to group using multiple keys.</p> <pre class=\"code\">\n A = LOAD 'allresults' USING PigStorage() AS (tcid:int, tpid:int, date:chararray, result:chararray, tsid:int, tag:chararray);\n B = GROUP A BY (tcid, tpid); \n</pre>  <h4 id=\"partitionby\">Example: PARTITION BY</h4> <p>To use the Hadoop Partitioner add PARTITION BY clause to the appropriate operator: </p> <pre class=\"code\">\nA = LOAD 'input_data'; \nB = GROUP A BY $0 PARTITION BY org.apache.pig.test.utils.SimpleCustomPartitioner PARALLEL 2;\n</pre> <p>Here is the code for SimpleCustomPartitioner:</p> <pre class=\"code\">\npublic class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable, Writable&gt; { \n     //@Override \n    public int getPartition(PigNullableWritable key, Writable value, int numPartitions) { \n        if(key.getValueAsPigType() instanceof Integer) { \n            int ret = (((Integer)key.getValueAsPigType()).intValue() % numPartitions); \n            return ret; \n       } \n       else { \n            return (key.hashCode()) % numPartitions; \n        } \n    } \n}\n</pre>  <h3 id=\"import\">IMPORT</h3> <p>See <a href=\"cont#import-macros\">IMPORT (macros)</a> </p>  <h3 id=\"join-inner\">JOIN (inner) </h3> <p>Performs an inner join of two or more relations based on common field values.</p>  <h4 id=\"Syntax-N13648\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = JOIN alias BY {expression|'('expression [, expression …]')'} (, alias BY {expression|'('expression [, expression …]')'} …) [USING 'replicated' | 'skewed' | 'merge' | 'merge-sparse'] [PARTITION BY partitioner] [PARALLEL n]; </p> </td> </tr> </table>  <h4 id=\"Terms-N1365C\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BY</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A field expression.</p> <p>Example: X = JOIN A BY fieldA, B BY fieldB, C BY fieldC;</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>USING</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'replicated'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to perform replicated joins (see <a href=\"perf#replicated-joins\">Replicated Joins</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'skewed'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to perform skewed joins (see <a href=\"perf#skewed-joins\">Skewed Joins</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'merge'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to perform merge joins (see <a href=\"perf#merge-joins\">Merge Joins</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'merge-sparse'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to perform merge-sparse joins (see <a href=\"perf#merge-sparse-joins\">Merge-Sparse Joins</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"partition-by-join-inner\">PARTITION BY partitioner</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p> <ul> <li> <p>For more details, see <a href=\"http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html\">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a> </p> </li> <li> <p>For usage, see <a href=\"#partitionby\">Example: PARTITION BY</a> </p> </li> </ul>  <p>This feature CANNOT be used with skewed joins.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PARALLEL n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Increase the parallelism of a job by specifying the number of reduce tasks, n. </p> <p>For more information, see <a href=\"perf#parallel\">Use the Parallel Features</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N13758\">Usage</h4> <p>Use the JOIN operator to perform an inner, equijoin join of two or more relations based on common field values. Inner joins ignore null keys, so it makes sense to filter them out before the join.</p> <p>Note the following about the GROUP/COGROUP and JOIN operators:</p> <ul> <li> <p>The GROUP and JOIN operators perform similar functions. GROUP creates a nested set of output tuples while JOIN creates a flat set of output tuples.</p> </li> <li> <p>The GROUP/COGROUP and JOIN operators handle null values differently (see <a href=\"#nulls_join\">Nulls and JOIN Operator</a>).</p> </li> </ul>   <p id=\"self-joins\"> <strong>Self Joins</strong> </p> <p>To perform self joins in Pig load the same data multiple times, under different aliases, to avoid naming conflicts.</p> <p>In this example the same data is loaded twice using aliases A and B.</p> <pre class=\"code\">\ngrunt&gt; A = load 'mydata';\ngrunt&gt; B = load 'mydata';\ngrunt&gt; C = join A by $0, B by $0;\ngrunt&gt; explain C;\n</pre>  <h4 id=\"Example-N13789\">Example</h4> <p>Suppose we have relations A and B.</p> <pre class=\"code\">\nA = LOAD 'data1' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n\nB = LOAD 'data2' AS (b1:int,b2:int);\n\nDUMP B;\n(2,4)\n(8,9)\n(1,3)\n(2,7)\n(2,9)\n(4,6)\n(4,9)\n</pre> <p>In this example relations A and B are joined by their first fields.</p> <pre class=\"code\">\nX = JOIN A BY a1, B BY b1;\n\nDUMP X;\n(1,2,3,1,3)\n(4,2,1,4,6)\n(4,3,3,4,6)\n(4,2,1,4,9)\n(4,3,3,4,9)\n(8,3,4,8,9)\n(8,4,3,8,9)\n</pre>  <h3 id=\"join-outer\">JOIN (outer) </h3> <p>Performs an outer join of two relations based on common field values.</p>  <h4 id=\"Syntax-N137AA\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = JOIN left-alias BY left-alias-column [LEFT|RIGHT|FULL] [OUTER], right-alias BY right-alias-column [USING 'replicated' | 'skewed' | 'merge'] [PARTITION BY partitioner] [PARALLEL n]; </p> </td> </tr> </table>  <h4 id=\"Terms-N137BF\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation. Applies to alias, left-alias and right-alias.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias-column</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the join column for the corresponding relation. Applies to left-alias-column and right-alias-column.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>BY</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LEFT</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Left outer join.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>RIGHT</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Right outer join.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>FULL</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Full outer join.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>OUTER</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>(Optional) Keyword </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>USING</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'replicated'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to perform replicated joins (see <a href=\"perf#replicated-joins\">Replicated Joins</a>).</p> <p>Only left outer join is supported for replicated joins.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'skewed'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to perform skewed joins (see <a href=\"perf#skewed-joins\">Skewed Joins</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'merge'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use to perform merge joins (see <a href=\"perf#merge-joins\">Merge Joins</a>).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"partition-by-join-outer\">PARTITION BY partitioner</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p> <ul> <li> <p>For more details, see <a href=\"http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html\">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a> </p> </li> <li> <p>For usage, see <a href=\"#partitionby\">Example: PARTITION BY</a> </p> </li> </ul>  <p>This feature CANNOT be used with skewed joins.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PARALLEL n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Increase the parallelism of a job by specifying the number of reduce tasks, n. </p> <p>For more information, see <a href=\"perf#parallel\">Use the Parallel Features</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N138F1\">Usage</h4> <p>Use the JOIN operator with the corresponding keywords to perform left, right, or full outer joins. The keyword OUTER is optional for outer joins; the keywords LEFT, RIGHT and FULL will imply left outer, right outer and full outer joins respectively when OUTER is omitted. The Pig Latin syntax closely adheres to the SQL standard.</p> <p> Please note the following: </p> <ul> <li> <p>Outer joins will only work provided the relations which need to produce nulls (in the case of non-matching keys) have schemas.</p> </li> <li> <p>Outer joins will only work for two-way joins; to perform a multi-way outer join, you will need to perform multiple two-way outer join statements.</p> </li> </ul>  <h4 id=\"Examples-N1390D\">Examples</h4> <p>This example shows a left outer join.</p> <pre class=\"code\">\nA = LOAD 'a.txt' AS (n:chararray, a:int); \nB = LOAD 'b.txt' AS (n:chararray, m:chararray);\nC = JOIN A by $0 LEFT OUTER, B BY $0;\n</pre> <p>This example shows a full outer join.</p> <pre class=\"code\">\nA = LOAD 'a.txt' AS (n:chararray, a:int); \nB = LOAD 'b.txt' AS (n:chararray, m:chararray);\nC = JOIN A BY $0 FULL, B BY $0;\n</pre> <p>This example shows a replicated left outer join.</p> <pre class=\"code\">\nA = LOAD 'large';\nB = LOAD 'tiny';\nC= JOIN A BY $0 LEFT, B BY $0 USING 'replicated';\n</pre> <p>This example shows a skewed full outer join.</p> <pre class=\"code\">\nA = LOAD 'studenttab' as (name, age, gpa);\nB = LOAD 'votertab' as (name, age, registration, contribution);\nC = JOIN A BY name FULL, B BY name USING 'skewed';\n</pre>  <h3 id=\"limit\">LIMIT </h3> <p>Limits the number of output tuples.</p>  <h4 id=\"Syntax-N1393C\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = LIMIT alias n;</p> </td> </tr> </table>  <h4 id=\"Terms-N13950\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The number of output tuples, either:</p> <ul> <li>a constant (for example, 3)</li> <li>a scalar used in an expression (for example, c.sum/100)</li> </ul>  <p>Note: The expression can consist of constants or scalars; it cannot contain any columns from the input relation.</p> <p>Note: Using a scalar instead of a constant in LIMIT automatically disables most optimizations (only push-before-foreach is performed).</p> </td> </tr> </table>  <h4 id=\"Usage-N13990\">Usage</h4> <p>Use the LIMIT operator to limit the number of output tuples.</p> <p>If the specified number of output tuples is equal to or exceeds the number of tuples in the relation, all tuples in the relation are returned.</p> <p>If the specified number of output tuples is less than the number of tuples in the relation, then n tuples are returned. There is no guarantee which n tuples will be returned, and the tuples that are returned can change from one run to the next. A particular set of tuples can be requested using the ORDER operator followed by LIMIT.</p> <p>Note: The LIMIT operator allows Pig to avoid processing all tuples in a relation. In most cases a query that uses LIMIT will run more efficiently than an identical query that does not use LIMIT. It is always a good idea to use limit if you can.</p>  <h4 id=\"Examples-N139A3\">Examples</h4> <p>In this example the limit is expressed as a scalar.</p> <pre class=\"code\">\na = load 'a.txt';\nb = group a all;\nc = foreach b generate COUNT(a) as sum;\nd = order a by $0;\ne = limit d c.sum/100;\n</pre>  <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n</pre> <p>In this example output is limited to 3 tuples. Note that there is no guarantee which three tuples will be output.</p> <pre class=\"code\">\nX = LIMIT A 3;\n\nDUMP X;\n(1,2,3)\n(4,3,3)\n(7,2,5)\n</pre> <p>In this example the ORDER operator is used to order the tuples and the LIMIT operator is used to output the first three tuples.</p> <pre class=\"code\">\nB = ORDER A BY f1 DESC, f2 ASC;\n\nDUMP B;\n(8,3,4) \n(8,4,3) \n(7,2,5) \n(4,2,1)\n(4,3,3)\n(1,2,3)\n\nX = LIMIT B 3;\n\nDUMP X;\n(8,3,4)\n(8,4,3) \n(7,2,5) \n</pre>  <h3 id=\"load\">LOAD </h3> <p>Loads data from the file system.</p>  <h4 id=\"Syntax-N139D3\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LOAD 'data' [USING function] [AS schema]; </p> </td> </tr> </table>  <h4 id=\"Terms-N139E7\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'data'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the file or directory, in single quotes.</p> <p>If you specify a directory name, all the files in the directory are loaded. </p> <p>You can use Hadoop globing to specify files at the file system or directory levels (see Hadoop <a href=\"http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/FileSystem.html#globStatus(org.apache.hadoop.fs.Path)\">globStatus</a> for details on globing syntax).</p>  <p id=\"load-glob\"> <strong>Note:</strong> Pig uses Hadoop globbing so the functionality is IDENTICAL. However, when you run from the command line using the Hadoop fs command (rather than the Pig LOAD operator), the Unix shell may do some of the substitutions; this could alter the outcome giving the impression that globing works differently for Pig and Hadoop. For example:</p> <ul> <li>This works <br>hadoop fs -ls /mydata/20110423{00,01,02,03,04,05,06,07,08,09,{10..23}}00//<strong>part </strong> </li> <li>This does not work <br>LOAD '/mydata/20110423{00,01,02,03,04,05,06,07,08,09,{10..23}}00//<strong>part </strong>'</li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>USING</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword. </p> <p>If the USING clause is omitted, the default load function PigStorage is used. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>function</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The load function. </p> <ul> <li> <p>You can use a built in function (see <a href=\"func#load-store-functions\">Load/Store Functions</a>). PigStorage is the default load function and does not need to be specified (simply omit the USING clause).</p> </li> <li> <p>You can write your own load function if your data is in a format that cannot be processed by the built in functions (see <a href=\"udf\">User Defined Functions</a>).</p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AS</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>schema</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A schema using the AS keyword, enclosed in parentheses (see <a href=\"#schemas\">Schemas</a>).</p> <p>The loader produces the data of the type specified by the schema. If the data does not conform to the schema, depending on the loader, either a null value or an error is generated.</p> <p>Note: For performance reasons the loader may not immediately convert the data to the specified format; however, you can still operate on the data assuming the specified type.</p> </td> </tr> </table>  <h4 id=\"Usage-N13A95\">Usage</h4> <p>Use the LOAD operator to load data from the file system. </p>  <h4 id=\"Examples-N13A9E\">Examples</h4> <p>Suppose we have a data file called myfile.txt. The fields are tab-delimited. The records are newline-separated.</p> <pre class=\"code\">\n1 2 3\n4 2 1\n8 3 4\n</pre> <p>In this example the default load function, PigStorage, loads data from myfile.txt to form relation A. The two LOAD statements are equivalent. Note that, because no schema is specified, the fields are not named and all fields default to type bytearray. </p> <pre class=\"code\">\nA = LOAD 'myfile.txt';\n\nA = LOAD 'myfile.txt' USING PigStorage('\\t');\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n</pre> <p>In this example a schema is specified using the AS keyword. The two LOAD statements are equivalent. You can use the DESCRIBE and ILLUSTRATE operators to view the schema. </p> <pre class=\"code\">\nA = LOAD 'myfile.txt' AS (f1:int, f2:int, f3:int);\n\nA = LOAD 'myfile.txt' USING PigStorage('\\t') AS (f1:int, f2:int, f3:int);\n\nDESCRIBE A;\na: {f1: int,f2: int,f3: int}\n\nILLUSTRATE A;\n---------------------------------------------------------\n| a     | f1: bytearray | f2: bytearray | f3: bytearray |\n---------------------------------------------------------\n|       | 4             | 2             | 1             |\n---------------------------------------------------------\n\n---------------------------------------\n| a     | f1: int | f2: int | f3: int |\n---------------------------------------\n|       | 4       | 2       | 1       |\n---------------------------------------\n</pre> <p> For examples of how to specify more complex schemas for use with the LOAD operator, see <a href=\"#schema-complex\">Schemas for Complex Data Types</a> and <a href=\"#schema-multi\">Schemas for Multiple Types</a>. </p>  <h3 id=\"mapreduce\">MAPREDUCE</h3> <p>Executes native MapReduce jobs inside a Pig script.</p>  <h4 id=\"Syntax-N13ACF\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias1 = MAPREDUCE 'mr.jar' STORE alias2 INTO 'inputLocation' USING storeFunc LOAD 'outputLocation' USING loadFunc AS schema [`params, ... `];</p> </td> </tr> </table>  <h4 id=\"Terms-N13AE4\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias1, alias2</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The names of relations.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>mr.jar</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The MapReduce jar file (enclosed in single quotes).</p> <p>You can specify any MapReduce jar file that can be run through the <span class=\"codefrag\">hadoop jar mymr.jar params</span> command. </p> <p>The values for inputLocation and outputLocation can be passed in the params. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>STORE ... INTO ... USING</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>See <a href=\"basic#store\">STORE</a> </p> <p>Store alias2 into the inputLocation using storeFunc, which is then used by the MapReduce job to read its data.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>LOAD ... USING ... AS </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>See <a href=\"basic#load\">LOAD</a> </p> <p>After running mr.jar's MapReduce job, load back the data from outputLocation into alias1 using loadFunc as schema.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>`params, ...`</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Extra parameters required for the mapreduce job (enclosed in back tics). </p> </td> </tr> </table>  <h4 id=\"Usage-N13B62\">Usage</h4> <p>Use the MAPREDUCE operator to run native MapReduce jobs from inside a Pig script.</p> <p>The input and output locations for the MapReduce program are conveyed to Pig using the STORE/LOAD clauses. Pig, however, does not pass this information (nor require that this information be passed) to the MapReduce program. If you want to pass the input and output locations to the MapReduce program you can use the params clause or you can hardcode the locations in the MapReduce program.</p>  <h4 id=\"Example-N13B6F\">Example</h4> <p>This example demonstrates how to run the wordcount MapReduce progam from Pig. Note that the files specified as input and output locations in the MAPREDUCE statement will NOT be deleted by Pig automatically. You will need to delete them manually. </p> <pre class=\"code\">\nA = LOAD 'WordcountInput.txt';\nB = MAPREDUCE 'wordcount.jar' STORE A INTO 'inputDir' LOAD 'outputDir' \n    AS (word:chararray, count: int) `org.myorg.WordCount inputDir outputDir`;\n</pre>  <h3 id=\"order-by\">ORDER BY</h3> <p>Sorts a relation based on one or more fields.</p>  <h4 id=\"Syntax-N13B89\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = ORDER alias BY { * [ASC|DESC] | field_alias [ASC|DESC] [, field_alias [ASC|DESC] …] } [PARALLEL n];</p> </td> </tr> </table>  <h4 id=\"Terms-N13B9D\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>*</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The designator for a tuple.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>field_alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A field in the relation. The field must be a simple type.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ASC</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Sort in ascending order.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DESC</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Sort in descending order.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PARALLEL n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Increase the parallelism of a job by specifying the number of reduce tasks, n.</p> <p>For more information, see <a href=\"perf#parallel\">Use the Parallel Features</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N13C1F\">Usage</h4> <p> <strong>Note:</strong> ORDER BY is NOT stable; if multiple records have the same ORDER BY key, the order in which these records are returned is not defined and is not guarantted to be the same from one run to the next.</p> <p>In Pig, relations are unordered (see <a href=\"#relations\">Relations, Bags, Tuples, Fields</a>):</p> <ul> <li> <p>If you order relation A to produce relation X (X = ORDER A BY * DESC;) relations A and X still contain the same data. </p> </li> <li> <p>If you retrieve relation X (DUMP X;) the data is guaranteed to be in the order you specified (descending).</p> </li> <li> <p>However, if you further process relation X (Y = FILTER X BY $0 &gt; 1;) there is no guarantee that the data will be processed in the order you originally specified (descending).</p> </li> </ul>  <p>Pig currently supports ordering on fields with simple types or by tuple designator (*). You cannot order on fields with complex types or by expressions. </p> <pre class=\"code\">\nA = LOAD 'mydata' AS (x: int, y: map[]);     \nB = ORDER A BY x; -- this is allowed because x is a simple type\nB = ORDER A BY y; -- this is not allowed because y is a complex type\nB = ORDER A BY y#'id'; -- this is not allowed because y#'id' is an expression\n</pre>  <h4 id=\"Examples-N13C50\">Examples</h4> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n</pre> <p>In this example relation A is sorted by the third field, f3 in descending order. Note that the order of the three tuples ending in 3 can vary.</p> <pre class=\"code\">\nX = ORDER A BY a3 DESC;\n\nDUMP X;\n(7,2,5)\n(8,3,4)\n(1,2,3)\n(4,3,3)\n(8,4,3)\n(4,2,1)\n</pre>  <h3 id=\"rank\">RANK</h3> <p>Returns each tuple with the rank within a relation.</p>  <h4 id=\"Syntax-N13C70\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = RANK alias [ BY { * [ASC|DESC] | field_alias [ASC|DESC] [, field_alias [ASC|DESC] …] } [DENSE] ];</p> </td> </tr> </table>  <h4 id=\"Terms-N13C85\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>*</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The designator for a tuple.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>field_alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A field in the relation. The field must be a simple type.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>ASC</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Sort in ascending order.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DESC</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Sort in descending order.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DENSE</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>No gap in the ranking values. </p> </td> </tr> </table>  <h4 id=\"Usage-N13D01\">Usage</h4> <p>When specifying no field to sort on, the RANK operator simply prepends a sequential value to each tuple.</p> <p>Otherwise, the RANK operator uses each field (or set of fields) to sort the relation. The rank of a tuple is one plus the number of different rank values preceding it. If two or more tuples tie on the sorting field values, they will receive the same rank.</p> <p> <strong>NOTE:</strong> When using the option <strong>DENSE</strong>, ties do not cause gaps in ranking values.</p>  <h4 id=\"Examples-N13D16\">Examples</h4> <p>Suppose we have relation A.</p> <pre class=\"code\">\nA = load 'data' AS (f1:chararray,f2:int,f3:chararray);\n   \nDUMP A;\n(David,1,N)\n(Tete,2,N)\n(Ranjit,3,M)\n(Ranjit,3,P)\n(David,4,Q)\n(David,4,Q)\n(Jillian,8,Q)\n(JaePak,7,Q)\n(Michael,8,T)\n(Jillian,8,Q)\n(Jose,10,V)\n            </pre> <p>In this example, the RANK operator does not change the order of the relation and simply prepends to each tuple a sequential value.</p> <pre class=\"code\">\nB = rank A;\n\ndump B;\n(1,David,1,N)\n(2,Tete,2,N)\n(3,Ranjit,3,M)\n(4,Ranjit,3,P)\n(5,David,4,Q)\n(6,David,4,Q)\n(7,Jillian,8,Q)\n(8,JaePak,7,Q)\n(9,Michael,8,T)\n(10,Jillian,8,Q)\n(11,Jose,10,V)\n            </pre> <p>In this example, the RANK operator works with f1 and f2 fields, and each one with different sorting order. RANK sorts the relation on these fields and prepends the rank value to each tuple. Otherwise, the RANK operator uses each field (or set of fields) to sort the relation. The rank of a tuple is one plus the number of different rank values preceding it. If two or more tuples tie on the sorting field values, they will receive the same rank.</p> <pre class=\"code\">\nC = rank A by f1 DESC, f2 ASC;\n                                \ndump C;\n(1,Tete,2,N)\n(2,Ranjit,3,M)\n(2,Ranjit,3,P)\n(4,Michael,8,T)\n(5,Jose,10,V)\n(6,Jillian,8,Q)\n(6,Jillian,8,Q)\n(8,JaePak,7,Q)\n(9,David,1,N)\n(10,David,4,Q)\n(10,David,4,Q)                \n            </pre> <p>Same example as previous, but DENSE. In this case there are no gaps in ranking values.</p> <pre class=\"code\">\nC = rank A by f1 DESC, f2 ASC DENSE;\n\ndump C;\n(1,Tete,2,N)\n(2,Ranjit,3,M)\n(2,Ranjit,3,P)\n(3,Michael,8,T)\n(4,Jose,10,V)\n(5,Jillian,8,Q)\n(5,Jillian,8,Q)\n(6,JaePak,7,Q)\n(7,David,1,N)\n(8,David,4,Q)\n(8,David,4,Q)\n            </pre>  <h3 id=\"sample\">SAMPLE</h3> <p>Selects a random sample of data based on the specified sample size.</p>  <h4 id=\"Syntax-N13D45\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SAMPLE alias size;</p> </td> </tr> </table>  <h4 id=\"Terms-N13D59\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>size</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Sample size, either</p> <ul> <li>a constant, range 0 to 1 (for example, enter 0.1 for 10%)</li> <li>a scalar used in an expression</li> </ul>  <p>Note: The expression can consist of constants or scalars; it cannot contain any columns from the input relation.</p> </td> </tr> </table>  <h4 id=\"Usage-N13D96\">Usage</h4> <p>Use the SAMPLE operator to select a random data sample with the stated sample size. SAMPLE is a probabalistic operator; there is no guarantee that the exact same number of tuples will be returned for a particular sample size each time the operator is used.</p>  <h4 id=\"Example-N13DA0\">Example</h4> <p>In this example relation X will contain 1% of the data in relation A.</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int,f2:int,f3:int);\n\nX = SAMPLE A 0.01;\n</pre> <p>In this example, a scalar expression is used (it will sample approximately 1000 records from the input).</p> <pre class=\"code\">\na = load 'a.txt';\nb = group a all;\nc = foreach b generate COUNT(a) as num_rows;\ne = sample a 1000/c.num_rows;\n</pre>  <h3 id=\"SPLIT\">SPLIT</h3> <p>Partitions a relation into two or more relations.</p>  <h4 id=\"Syntax-N13DBE\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>SPLIT alias INTO alias IF expression, alias IF expression [, alias IF expression …] [, alias OTHERWISE];</p> </td> </tr> </table>  <h4 id=\"Terms-N13DD2\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>INTO</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Required keyword.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>IF</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Required keyword.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>expression</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>An expression.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>OTHERWISE</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Optional keyword. Designates a default relation.</p> </td> </tr> </table>  <h4 id=\"Usage-N13E3A\">Usage</h4> <p>Use the SPLIT operator to partition the contents of a relation into two or more relations based on some expression. Depending on the conditions stated in the expression:</p> <ul> <li> <p>A tuple may be assigned to more than one relation.</p> </li> <li> <p>A tuple may not be assigned to any relation.</p>   </li> </ul>  <h4 id=\"Example-N13E56\">Example</h4> <p>In this example relation A is split into three relations, X, Y, and Z.</p> <pre class=\"code\">\nA = LOAD 'data' AS (f1:int,f2:int,f3:int);\n\nDUMP A;                \n(1,2,3)\n(4,5,6)\n(7,8,9)        \n\nSPLIT A INTO X IF f1&lt;7, Y IF f2==5, Z IF (f3&lt;6 OR f3&gt;6);\n\nDUMP X;\n(1,2,3)\n(4,5,6)\n\nDUMP Y;\n(4,5,6)\n\nDUMP Z;\n(1,2,3)\n(7,8,9)\n</pre>  <h4 id=\"Example-N13E64\">Example</h4> <p>In this example, the SPLIT and FILTER statements are essentially equivalent. However, because SPLIT is implemented as \"split the data stream and then apply filters\" the SPLIT statement is more expensive than the FILTER statement because Pig needs to filter and store two data streams.</p> <pre class=\"code\">\nSPLIT input_var INTO output_var IF (field1 is not null), ignored_var IF (field1 is null);  \n-- where ignored_var is not used elsewhere\n   \noutput_var = FILTER input_var BY (field1 is not null);\n   </pre>  <h3 id=\"store\">STORE </h3> <p>Stores or saves results to the file system.</p>  <h4 id=\"Syntax-N13E7E\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>STORE alias INTO 'directory' [USING function];</p> </td> </tr> </table>  <h4 id=\"Terms-N13E92\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>INTO</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Required keyword.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>'directory'</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the storage directory, in quotes. If the directory already exists, the STORE operation will fail.</p>  <p>The output data files, named part-nnnnn, are written to this directory. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>USING</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword. Use this clause to name the store function.</p> <p>If the USING clause is omitted, the default store function PigStorage is used.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>function</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The store function.</p> <ul> <li> <p>You can use a built in function (see the <a href=\"func#load-store-functions\">Load/Store Functions</a>). PigStorage is the default store function and does not need to be specified (simply omit the USING clause).</p> </li> <li> <p>You can write your own store function if your data is in a format that cannot be processed by the built in functions (see <a href=\"udf\">User Defined Functions</a>).</p> </li> </ul> </td> </tr> </table>  <h4 id=\"Usage-N13F19\">Usage</h4> <p>Use the STORE operator to run (execute) Pig Latin statements and save (persist) results to the file system. Use STORE for production scripts and batch mode processing.</p> <p>Note: To debug scripts during development, you can use <a href=\"test#dump\">DUMP</a> to check intermediate results.</p>  <h4 id=\"Examples-N13F2A\">Examples</h4> <p>In this example data is stored using PigStorage and the asterisk character (*) as the field delimiter.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n\nSTORE A INTO 'myoutput' USING PigStorage ('*');\n\nCAT myoutput;\n1*2*3\n4*2*1\n8*3*4\n4*3*3\n7*2*5\n8*4*3\n</pre> <p>In this example, the CONCAT function is used to format the data before it is stored.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n(8,3,4)\n(4,3,3)\n(7,2,5)\n(8,4,3)\n\nB = FOREACH A GENERATE CONCAT('a:',(chararray)f1), CONCAT('b:',(chararray)f2), CONCAT('c:',(chararray)f3);\n\nDUMP B;\n(a:1,b:2,c:3)\n(a:4,b:2,c:1)\n(a:8,b:3,c:4)\n(a:4,b:3,c:3)\n(a:7,b:2,c:5)\n(a:8,b:4,c:3)\n\nSTORE B INTO 'myoutput' using PigStorage(',');\n\nCAT myoutput;\na:1,b:2,c:3\na:4,b:2,c:1\na:8,b:3,c:4\na:4,b:3,c:3\na:7,b:2,c:5\na:8,b:4,c:3\n</pre>  <h3 id=\"stream\">STREAM</h3> <p>Sends data to an external script or program.</p>  <h4 id=\"Syntax-N13F4A\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = STREAM alias [, alias …] THROUGH {`command` | cmd_alias } [AS schema] ;</p> </td> </tr> </table>  <h4 id=\"Terms-N13F5E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>THROUGH</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>`command`</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A command, including the arguments, enclosed in back tics (where a command is anything that can be executed).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>cmd_alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a command created using the DEFINE operator (see <a href=\"#define-udfs\">DEFINE (UDFs, streaming)</a> for additional streaming examples).</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>AS</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Keyword.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>schema</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>A schema using the AS keyword, enclosed in parentheses (see <a href=\"#schemas\">Schemas</a>).</p> </td> </tr> </table>  <h4 id=\"Usage-N13FE1\">Usage</h4> <p>Use the STREAM operator to send data through an external script or program. Multiple stream operators can appear in the same Pig script. The stream operators can be adjacent to each other or have other operations in between.</p> <p>When used with a command, a stream statement could look like this:</p> <pre class=\"code\">\nA = LOAD 'data';\n\nB = STREAM A THROUGH `stream.pl -n 5`;\n</pre> <p>When used with a cmd_alias, a stream statement could look like this, where mycmd is the defined alias.</p> <pre class=\"code\">\nA = LOAD 'data';\n\nDEFINE mycmd `stream.pl –n 5`;\n\nB = STREAM A THROUGH mycmd;\n</pre>  <h4 id=\"About+Data+Guarantees\">About Data Guarantees</h4> <p>Data guarantees are determined based on the position of the streaming operator in the Pig script. </p> <ul> <li> <p>Unordered data – No guarantee for the order in which the data is delivered to the streaming application. </p> </li> <li> <p>Grouped data – The data for the same grouped key is guaranteed to be provided to the streaming application contiguously</p> </li> <li> <p>Grouped and ordered data – The data for the same grouped key is guaranteed to be provided to the streaming application contiguously. Additionally, the data within the group is guaranteed to be sorted by the provided secondary key.</p> </li> </ul> <p>In addition to position, data grouping and ordering can be determined by the data itself. However, you need to know the property of the data to be able to take advantage of its structure.</p>  <h4 id=\"Example%3A+Data+Guarantees\">Example: Data Guarantees</h4> <p>In this example the data is unordered.</p> <pre class=\"code\">\nA = LOAD 'data';\n\nB = STREAM A THROUGH `stream.pl`;\n</pre> <p>In this example the data is grouped.</p> <pre class=\"code\">\nA = LOAD 'data';\n\nB = GROUP A BY $1;\n\nC = FOREACH B FLATTEN(A);\n\nD = STREAM C THROUGH `stream.pl`;\n</pre> <p>In this example the data is grouped and ordered.</p> <pre class=\"code\">\nA = LOAD 'data';\n\nB = GROUP A BY $1;\n\nC = FOREACH B {\n      D = ORDER A BY ($3, $4);\n      GENERATE D;\n}\n\nE = STREAM C THROUGH `stream.pl`;\n</pre>  <h4 id=\"Example%3A+Schemas\">Example: Schemas</h4> <p>In this example a schema is specified as part of the STREAM statement.</p> <pre class=\"code\">\nX = STREAM A THROUGH `stream.pl` as (f1:int, f2:int, f3:int);\n</pre>  <h3 id=\"union\">UNION</h3> <p>Computes the union of two or more relations.</p>  <h4 id=\"Syntax-N14051\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias = UNION [ONSCHEMA] alias, alias [, alias …] [PARALLEL n];</p> </td> </tr> </table>  <h4 id=\"Terms-N14065\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of a relation.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"onschema\">ONSCHEMA </p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Use the ONSCHEMA clause to base the union on named fields (rather than positional notation). All inputs to the union must have a non-unknown (non-null) <a href=\"#schemas\">schema</a>.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>PARALLEL n</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>This is only applicable for Tez execution mode and will not work with Mapreduce mode. Specifying PARALLEL will introduce an extra reduce step that will slightly degrade performance. The primary purpose in this case is to control the number of output files.</p> <p>For more information, see <a href=\"perf#parallel\">Use the Parallel Features</a>.</p> </td> </tr> </table>  <h4 id=\"Usage-N140B4\">Usage</h4> <p>Use the UNION operator to merge the contents of two or more relations. The UNION operator:</p> <ul> <li> <p>Does not preserve the order of tuples. Both the input and output relations are interpreted as unordered bags of tuples.</p> </li> <li> <p>Does not ensure (as databases do) that all tuples adhere to the same schema or that they have the same number of fields. In a typical scenario, however, this should be the case; therefore, it is the user's responsibility to either (1) ensure that the tuples in the input relations have the same schema or (2) be able to process varying tuples in the output relation.</p> </li> <li> <p>Does not eliminate duplicate tuples.</p> </li> </ul>  <p> <strong>Schema Behavior</strong> </p> <p>The behavior of schemas for UNION (positional notation / data types) and UNION ONSCHEMA (named fields / data types) is the same, except where noted.</p> <p>Union on relations with two different sizes result in a null schema (union only): </p> <pre class=\"code\">\nA: (a1:long, a2:long) \nB: (b1:long, b2:long, b3:long) \nA union B: null \n</pre> <p>Union columns with incompatible types result in a bytearray type: </p> <pre class=\"code\">\nA: (a1:long, a2:long) \nB: (b1:(b11:long, b12:long), b2:long) \nA union B: (a1:bytearray, a2:long) \n</pre> <p>Union columns of compatible type will produce an \"escalate\" type. The priority is:</p> <ul> <li>double &gt; float &gt; long &gt; int &gt; bytearray</li> <li>tuple|bag|map|chararray &gt; bytearray</li> </ul> <pre class=\"code\">\nA: (a1:int, a2:bytearray, a3:int) \nB: (b1:float, b2:chararray, b3:bytearray) \nA union B: (a1:float, a2:chararray, a3:int) \n</pre> <p>Union of different inner types results in an empty complex type: </p> <pre class=\"code\">\nA: (a1:(a11:long, a12:int), a2:{(a21:charray, a22:int)}) \nB: (b1:(b11:int, b12:int), b2:{(b21:int, b22:int)}) \nA union B: (a1:(), a2:{()}) \n</pre> <p>The alias of the first relation is always taken as the alias of the unioned relation field. </p>  <h4 id=\"Example-N14104\">Example</h4> <p>In this example the union of relation A and B is computed.</p> <pre class=\"code\">\nA = LOAD 'data' AS (a1:int,a2:int,a3:int);\n\nDUMP A;\n(1,2,3)\n(4,2,1)\n\nB = LOAD 'data' AS (b1:int,b2:int);\n\nDUMP A;\n(2,4)\n(8,9)\n(1,3)\n\nX = UNION A, B;\n\nDUMP X;\n(1,2,3)\n(4,2,1)\n(2,4)\n(8,9)\n(1,3)\n</pre>  <h4 id=\"Example-N14112\">Example</h4> <p>This example shows the use of ONSCHEMA.</p> <pre class=\"code\">\nL1 = LOAD 'f1' USING (a : int, b : float);\nDUMP L1;\n(11,12.0)\n(21,22.0)\n\nL2 = LOAD  'f1' USING (a : long, c : chararray);\nDUMP L2;\n(11,a)\n(12,b)\n(13,c)\n\nU = UNION ONSCHEMA L1, L2;\nDESCRIBE U ;\nU : {a : long, b : float, c : chararray}\n\nDUMP U;\n(11,12.0,)\n(21,22.0,)\n(11,,a)\n(12,,b)\n(13,,c)\n</pre> </div>     <h2 id=\"udf-statements\">UDF Statements</h2> <div class=\"section\">  <h3 id=\"define-udfs\">DEFINE (UDFs, streaming)</h3> <p>Assigns an alias to a UDF or streaming command.</p>  <h4 id=\"Syntax%3A+UDF+and+streaming\">Syntax: UDF and streaming</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>DEFINE alias {function | [`command` [input] [output] [ship] [cache] [stderr] ] };</p> </td> </tr> </table>  <h4 id=\"Terms-N1414E\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>alias</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name for a UDF function or the name for a streaming command (the cmd_alias for the <a href=\"#stream\">STREAM</a> operator). </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>function</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>For use with functions.</p> <p>The name of a UDF function. </p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>`command`</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>For use with streaming.</p> <p>A command, including the arguments, enclosed in back tics (where a command is anything that can be executed).</p> <p>The clauses (input, output, ship, cache, stderr) are described below. Note the following:</p> <ul> <li>All clauses are optional.</li> <li>The clauses can be specified in any order (for example, stderr can appear before input)</li> <li>Each clause can be specified at most once (for example, multiple inputs are not allowed)</li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>input</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>For use with streaming.</p> <p>INPUT ( {stdin | 'path'} [USING serializer] [, {stdin | 'path'} [USING serializer] …] )</p> <p>Where:</p> <ul> <li> <p>INPUT – Keyword.</p> </li> <li> <p>'path' – A file path, enclosed in single quotes.</p> </li> <li> <p>USING – Keyword.</p> </li> <li> <p>serializer – PigStreaming is the default serializer. </p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>output</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>For use with streaming.</p> <p>OUTPUT ( {stdout | stderr | 'path'} [USING deserializer] [, {stdout | stderr | 'path'} [USING deserializer] …] )</p> <p>Where:</p> <ul> <li> <p>OUTPUT – Keyword.</p> </li> <li> <p>'path' – A file path, enclosed in single quotes.</p> </li> <li> <p>USING – Keyword.</p> </li> <li> <p>deserializer – PigStreaming is the default deserializer. </p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"ship\">ship</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>For use with streaming.</p> <p>SHIP('path' [, 'path' …])</p> <p>Where:</p> <ul> <li> <p>SHIP – Keyword.</p> </li> <li> <p>'path' – A file path, enclosed in single quotes.</p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\">  <p id=\"cache\">cache</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>For use with streaming.</p> <p>CACHE('dfs_path#dfs_file' [, 'dfs_path#dfs_file' …])</p> <p>Where:</p> <ul> <li> <p>CACHE – Keyword.</p> </li> <li> <p>'dfs_path#dfs_file' – A file path/file name on the distributed file system, enclosed in single quotes. Example: '/mydir/mydata.txt#mydata.txt'</p> </li> </ul> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>stderr</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>For use with streaming.</p> <p>STDERR( '/dir') or STDERR( '/dir' LIMIT n)</p> <p>Where:</p> <ul> <li>'/dir' is the log directory, enclosed in single quotes.</li> </ul> <ul> <li>(optional) LIMIT n is the error threshold where n is an integer value. If not specified, the default error threshold is unlimited.</li> </ul> </td> </tr> </table>  <h4 id=\"Usage-N14284\">Usage</h4> <p>Use the DEFINE statement to assign a name (alias) to a UDF function or to a streaming command.</p> <p>Use DEFINE to specify a UDF function when:</p> <ul> <li> <p>The function has a long package name that you don't want to include in a script, especially if you call the function several times in that script.</p> </li> <li> <p>The constructor for the function takes string parameters. If you need to use different constructor parameters for different calls to the function you will need to create multiple defines – one for each parameter set.</p> </li> </ul> <p>Use DEFINE to specify a streaming command when: </p> <ul> <li> <p>The streaming command specification is complex.</p> </li> <li> <p>The streaming command specification requires additional parameters (input, output, and so on).</p> </li> </ul>  <h5 id=\"About+Input+and+Output\">About Input and Output</h5> <p>Serialization is needed to convert data from tuples to a format that can be processed by the streaming application. Deserialization is needed to convert the output from the streaming application back into tuples. PigStreaming is the default serialization/deserialization function.</p> <p>Streaming uses the same default format as PigStorage to serialize/deserialize the data. If you want to explicitly specify a format, you can do it as show below (see more examples in the Examples: Input/Output section). </p> <pre class=\"code\">\nDEFINE CMD `perl PigStreaming.pl - nameMap` input(stdin using PigStreaming(',')) output(stdout using PigStreaming(','));\nA = LOAD 'file';\nB = STREAM B THROUGH CMD;\n</pre> <p>If you need an alternative format, you will need to create a custom serializer/deserializer by implementing the following interfaces.</p> <pre class=\"code\">\ninterface PigToStream {\n\n    /**\n     * Given a tuple, produce an array of bytes to be passed to the streaming\n     * executable.\n     */\n    public byte[] serialize(Tuple t) throws IOException;\n}\n\ninterface StreamToPig {\n\n    /**\n     *  Given a byte array from a streaming executable, produce a tuple.\n     */\n    public Tuple deserialize(byte[]) throws IOException;\n\n    /**\n     * This will be called on the front end during planning and not on the back\n     * end during execution.\n     *\n     * @return the {@link LoadCaster} associated with this object.\n     * @throws IOException if there is an exception during LoadCaster\n     */\n    public LoadCaster getLoadCaster() throws IOException;\n}\n</pre>  <h5 id=\"ship-about\">About Ship </h5> <p>Use the ship option to send streaming binary and supporting files, if any, from the client node to the compute nodes. Pig does not automatically ship dependencies; it is your responsibility to explicitly specify all the dependencies and to make sure that the software the processing relies on (for instance, perl or python) is installed on the cluster. Supporting files are shipped to the task's current working directory and only relative paths should be specified. Any pre-installed binaries should be specified in the PATH. </p> <p>Only files, not directories, can be specified with the ship option. One way to work around this limitation is to tar all the dependencies into a tar file that accurately reflects the structure needed on the compute nodes, then have a wrapper for your script that un-tars the dependencies prior to execution.</p> <p>Note that the ship option has two components: the source specification, provided in the ship( ) clause, is the view of your machine; the command specification is the view of the actual cluster. The only guarantee is that the shipped files are available in the current working directory of the launched job and that your current working directory is also on the PATH environment variable. </p> <p>Shipping files to relative paths or absolute paths is not supported since you might not have permission to read/write/execute from arbitrary paths on the clusters.</p> <p>Note the following:</p> <ul> <li> <p>It is safe only to ship files to be executed from the current working directory on the task on the cluster.</p> <pre class=\"code\">\nOP = stream IP through 'script';\nor\nDEFINE CMD 'script' ship('/a/b/script');\nOP = stream IP through CMD;\n</pre> </li> <li> <p>Shipping files to relative paths or absolute paths is undefined and mostly will fail since you may not have permissions to read/write/execute from arbitraty paths on the actual clusters. </p> </li> </ul>  <h5 id=\"cache-about\">About Cache</h5> <p>The ship option works with binaries, jars, and small datasets. However, loading larger datasets at run time for every execution can severely impact performance. Instead, use the cache option to access large files already moved to and available on the compute nodes. Only files, not directories, can be specified with the cache option.</p>  <h5 id=\"autoship\">About Auto-Ship</h5> <p>If the ship and cache options are not specified, Pig will attempt to auto-ship the binary in the following way:</p> <ul> <li> <p>If the first word on the streaming command is perl or python, Pig assumes that the binary is the first non-quoted string it encounters that does not start with dash.</p> </li> <li> <p>Otherwise, Pig will attempt to ship the first string from the command line as long as it does not come from <span class=\"codefrag\">/bin, /usr/bin, /usr/local/bin</span>. Pig will determine this by scanning the path if an absolute path is provided or by executing <span class=\"codefrag\">which</span>. The paths can be made configurable using the <a href=\"cmds#set\">set stream.skippath</a> option (you can use multiple set commands to specify more than one path to skip). </p> </li> </ul> <p>If you don't supply a DEFINE for a given streaming command, then auto-shipping is turned off.</p> <p>Note the following:</p> <ul> <li> <p>If Pig determines that it needs to auto-ship an absolute path it will not ship it at all since there is no way to ship files to the necessary location (lack of permissions and so on). </p> <pre class=\"code\">\nOP = stream IP through `/a/b/c/script`;\nor \nOP = stream IP through `perl /a/b/c/script.pl`;\n</pre> </li> <li> <p>Pig will not auto-ship files in the following system directories (this is determined by executing 'which &lt;file&gt;' command). </p> <pre class=\"code\">\n/bin /usr/bin /usr/local/bin /sbin /usr/sbin /usr/local/sbin\n</pre> </li> <li> <p>To auto-ship, the file in question should be present in the PATH. So if the file is in the current working directory then the current working directory should be in the PATH. </p> </li> </ul>  <h4 id=\"Examples%3A+Input%2FOutput\">Examples: Input/Output</h4> <p>In this example PigStreaming is the default serialization/deserialization function. The tuples from relation A are converted to tab-delimited lines that are passed to the script.</p> <pre class=\"code\">\nX = STREAM A THROUGH `stream.pl`;\n</pre> <p>In this example PigStreaming is used as the serialization/deserialization function, but a comma is used as the delimiter.</p> <pre class=\"code\">\nDEFINE Y 'stream.pl' INPUT(stdin USING PigStreaming(',')) OUTPUT (stdout USING PigStreaming(','));\n\nX = STREAM A THROUGH Y;\n</pre> <p>In this example user defined serialization/deserialization functions are used with the script.</p> <pre class=\"code\">\nDEFINE Y 'stream.pl' INPUT(stdin USING MySerializer) OUTPUT (stdout USING MyDeserializer);\n\nX = STREAM A THROUGH Y;\n</pre>  <h4 id=\"Examples%3A+Ship%2FCache\">Examples: Ship/Cache</h4> <p>In this example ship is used to send the script to the cluster compute nodes.</p> <pre class=\"code\">\nDEFINE Y 'stream.pl' SHIP('/work/stream.pl');\n\nX = STREAM A THROUGH Y;\n</pre> <p>In this example cache is used to specify a file located on the cluster compute nodes.</p> <pre class=\"code\">\nDEFINE Y 'stream.pl data.gz' SHIP('/work/stream.pl') CACHE('/input/data.gz#data.gz');\n\nX = STREAM A THROUGH Y;\n</pre>  <h4 id=\"Example%3A+DEFINE+with+STREAM\">Example: DEFINE with STREAM</h4> <p>In this example a command is defined for use with the <a href=\"#stream\">STREAM</a> operator.</p> <pre class=\"code\">\nA = LOAD 'data';\n\nDEFINE mycmd 'stream_cmd –input file.dat';\n\nB = STREAM A through mycmd;\n</pre>  <h4 id=\"Examples%3A+Logging\">Examples: Logging</h4> <p>In this example the streaming stderr is stored in the _logs/&lt;dir&gt; directory of the job's output directory. Because the job can have multiple streaming applications associated with it, you need to ensure that different directory names are used to avoid conflicts. Pig stores up to 100 tasks per streaming job.</p> <pre class=\"code\">\nDEFINE Y 'stream.pl' stderr('&lt;dir&gt;' limit 100);\n\nX = STREAM A THROUGH Y;\n</pre>  <h4 id=\"Examples%3A+DEFINE+a+function\">Examples: DEFINE a function</h4> <p>In this example a function is defined for use with the FOREACH …GENERATE operator.</p> <pre class=\"code\">\nREGISTER /src/myfunc.jar\n\nDEFINE myFunc myfunc.MyEvalfunc('foo');\n\nA = LOAD 'students';\n\nB = FOREACH A GENERATE myFunc($0);\n</pre>  <h3 id=\"register-jar\">REGISTER (a jar/script)</h3> <p>Registers a JAR file so that the UDFs in the file can be used.</p>  <h4 id=\"Syntax-N143AE\">Syntax</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>REGISTER path;</p> </td> </tr> </table>  <h4 id=\"Terms-N143C2\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>path</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The path to the JAR file (the full location URI is required). Do not place the name in quotes.</p> </td> </tr> </table>  <h4 id=\"Usage-N143DF\">Usage</h4> <p> <strong>Pig Scripts</strong> </p> <p>Use the REGISTER statement inside a Pig script to specify a JAR file or a Python/JavaScript module. Pig supports JAR files and modules stored in local file systems as well as remote, distributed file systems such as HDFS and Amazon S3 (see <a href=\"start#pig-scripts\">Pig Scripts</a>).</p>  <p id=\"register-glob\">Additionally, JAR files stored in local file systems can be specified as a glob pattern using “*”. Pig will search for matching jars in the local file system, either the relative path (relative to your working directory) or the absolute path. Pig will pick up all JARs that match the glob.</p> <p> <strong>Command Line</strong> </p> <p>You can register additional files (to use with your Pig script) via PIG_OPTS environment variable using the -Dpig.additional.jars.uris option. For more information see <a href=\"udf\">User Defined Functions</a>.</p>  <h4 id=\"Examples-N14400\">Examples</h4> <p>In this example REGISTER states that the JavaScript module, myfunc.js, is located in the /src directory.</p> <pre class=\"code\">\n/src $ java -jar pig.jar –\n\nREGISTER /src/myfunc.js;\nA = LOAD 'students';\nB = FOREACH A GENERATE myfunc.MyEvalFunc($0);\n</pre> <p>In this example additional JAR files are registered via PIG_OPTS environment variable.</p> <pre class=\"code\">\nexport PIG_OPTS=\"-Dpig.additional.jars.uris=my.jar,your.jar\"\n</pre> <p>In this example a JAR file stored in HDFS and a local JAR file are registered.</p> <pre class=\"code\">\nexport PIG_OPTS=\"-Dpig.additional.jars.uris=hdfs://nn.mydomain.com:9020/myjars/my.jar,file:///home/root/pig/your.jar\"\n</pre> <p>Note, the legacy property pig.additional.jars which use colon as separator is still supported. But we recommend to use pig.additional.jars.uris since colon is also used in URL scheme, and thus we cannot use full scheme in the list. We will deprecate pig.additional.jar in future releases.</p> <p>This example shows how to specify a glob pattern using either a relative path or an absolute path.</p> <pre class=\"code\">\nregister /homes/user/pig/myfunc*.jar\nregister count*.jar\nregister jars/*.jar\n</pre>  <h3 id=\"register-artifact\">REGISTER (an artifact)</h3> <p> Instead of figuring out the dependencies manually, downloading them and registering each jar using the above <a href=\"#register-jar\">register command</a>, you can specify the artifact's coordinates and expect pig to automatically fetch the required dependencies, download and register them. </p>  <h4 id=\"Syntax-N14438\">Syntax</h4> <p> To download an Artifact (and its dependencies), you need to specify the artifact's group, module and version following the syntax shown below. This command will download the Jar specified and all its dependencies and load it into the classpath. </p> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>REGISTER ivy://group:module:version?querystring</p> </td> </tr> </table>  <h4 id=\"Terms-N14452\">Terms</h4> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>group</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>Which module group the module comes from. Translates directly to a Maven groupId or an Ivy Organization.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>module</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The name of the module to load. Translated directly to a Maven artifactId or an Ivy artifact.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>version</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>The version of the module to use. You can specify a specific version or use \"+\" or \"*\" to use the latest version.</p> </td> </tr> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>querystring</p> </td> <td colspan=\"1\" rowspan=\"1\"> <p>This will contain \"&amp;\" separated key-value pairs to help us exclude all or specific dependencies etc.</p> </td> </tr> </table>  <h4 id=\"Usage-N144A8\">Usage</h4> <p> The Register artifact command is an extension to the above register command used to <a href=\"#register-jar\">register a jar</a>. In addition to registering a jar from a local system or from hdfs, you can now specify the coordinates of the artifact and pig will download the artifact (and its dependencies if needed) from the configured repository. </p>  <h5 id=\"Parameters+Supported+in+the+Query+String\">Parameters Supported in the Query String</h5> <ul> <li> <strong>Transitive</strong> <p> Transitive helps specifying if you need the dependencies along with the registering jar. By setting transitive to false in the querystring we can tell pig to register only the artifact without its dependencies. This will download only the artifact specified and will not download the dependencies of the jar. The default value of transitive is true. </p> <strong>Syntax</strong> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>REGISTER ivy://org:module:version?transitive=false</p> </td> </tr> </table> </li> <li> <strong>Exclude</strong> <p> While registering an artifact if you wish to exclude some dependencies you can specify them using the exclude key. Suppose you want to use a specific version of a dependent jar which doesn't match the version of the jar when automatically fetched, then you could exclude such dependencies by specifying a comma separated list of dependencies and register the dependent jar separately. </p> <strong>Syntax</strong> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>REGISTER ivy://org:module:version?exclude=org:mod,org:mod,...</p> </td> </tr> </table> </li> <li> <strong>Classifier</strong> <p> Some maven dependencies need classifiers in order to be able to resolve. You can specify them using a classifier key. </p> <strong>Syntax</strong> <table class=\"ForrestTable\"> <tr> <td colspan=\"1\" rowspan=\"1\"> <p>REGISTER ivy://org:module:version?classifier=value</p> </td> </tr> </table> </li> </ul>  <h5 id=\"Other+properties\">Other properties</h5> <ul> <li> <p> An optional pig property, pig.artifacts.download.location, can be used to configure the location where the artifacts should be downloaded. By default, they will be downloaded to ~/.groovy/grapes </p> </li> <li> <p> This command can be used or can replace the <a href=\"#register-jar\">register jar</a> command wherever used including macros.<br> </p> </li> <li> <p> Group/Organization and Version are optional fields. In such cases you can leave them blank.<br> </p> </li> <li> <p> The repositories can be configured using an ivysettings file. Pig will search for an ivysettings.xml file in the following locations in order. PIG_CONF_DIR &gt; PIG_HOME &gt; Classpath<br> </p> </li> </ul>  <h4 id=\"Examples-N1453C\">Examples</h4> <ul> <li> <p>Registering an Artifact and all its dependencies.</p> <pre class=\"code\">\n                -- Both are the same<br>\n                REGISTER ivy://org.apache.avro:avro:1.5.1<br>\n                REGISTER ivy://org.apache.avro:avro:1.5.1?transitive=true</pre> </li> <li> <p>Registering an artifact without getting its dependencies.</p> <pre class=\"code\">\n               REGISTER ivy://org.apache.avro:avro:1.5.1?transitive=false</pre> </li> <li> <p>Registering the latest artifact.</p> <pre class=\"code\">\n                -- Both of the following syntaxes work.<br>\n                REGISTER ivy://org.apache.avro:avro:+<br>\n                REGISTER ivy://org.apache.avro:avro:*</pre> </li> <li> <p>Registering an artifact by excluding specific dependencies.</p> <pre class=\"code\">\n                REGISTER ivy://org.apache.pig:pig:0.10.0?exclude=commons-cli:commons-cli,commons-codec:commons-codec</pre> </li> <li> <p>Specifying a classifier</p> <pre class=\"code\">\n                REGISTER ivy://net.sf.json-lib:json-lib:2.4?classifier=jdk15</pre> </li> <li> <p>Registering an artifact without a group or organization. Just skip them.</p> <pre class=\"code\">\n                REGISTER ivy://:module:</pre> </li> </ul> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2007&ndash;2016 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>\n    <a href=\"https://pig.apache.org/docs/r0.16.0/basic.html\" class=\"_attribution-link\">https://pig.apache.org/docs/r0.16.0/basic.html</a>\n  </p>\n</div>\n"}