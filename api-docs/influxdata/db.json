{"index":"<h1>InfluxData Documentation</h1><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/\" class=\"_attribution-link\">https://docs.influxdata.com/</a>\n  </p>\n</div>\n","telegraf/v0.13/introduction/index":"<h1>Introduction</h1>     <p>The introductory documentation includes all the information you need to get up and running with Telegraf.</p> <h2 id=\"download-https-influxdata-com-downloads-telegraf\"><a href=\"https://influxdata.com/downloads/#telegraf\">Download</a></h2> <p>Download Telegraf.</p> <h2 id=\"installation-telegraf-v0-13-introduction-installation\"><a href=\"installation/index\">Installation</a></h2> <p>Directions for installing, starting, and configuring Telegraf.</p> <h2 id=\"getting-started-telegraf-v0-13-introduction-getting-started-telegraf\"><a href=\"getting-started-telegraf/index\">Getting Started</a></h2> <p>This guide walks you through the download, installation, and configuration processes, and it shows how to use Telegraf to get data into InfluxDB.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/introduction/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/introduction/</a>\n  </p>\n</div>\n","telegraf/v0.13/introduction/getting_started/index":"<h1>Getting Started with Telegraf</h1>     <h2 id=\"getting-started-with-telegraf\">Getting Started with Telegraf</h2> <p>Telegraf is an agent written in Go for collecting metrics and writing them into InfluxDB or other possible outputs. This guide will get you up and running with Telegraf. It walks you through the download, installation, and configuration processes, and it shows how to use Telegraf to get data into InfluxDB.</p> <h2 id=\"download-and-install-telegraf\">Download and Install Telegraf</h2> <p>Follow the instructions in the Telegraf section on the <a href=\"https://influxdata.com/downloads/\">Downloads page</a>.</p> <blockquote> <p><strong>Note:</strong> Telegraf will start automatically using the default configuration when installed from a deb package.</p> </blockquote> <h2 id=\"configuration\">Configuration</h2> <h3 id=\"configuration-file-location-by-installation-type\">Configuration file location by installation type</h3> <ul> <li>OS X <a href=\"http://brew.sh/\">Homebrew</a>: <code>/usr/local/etc/telegraf.conf</code>\n</li> <li>Linux debian and RPM packages: <code>/etc/telegraf/telegraf.conf</code>\n</li> <li>Standalone Binary: see the next section for how to create a configuration file</li> </ul> <h3 id=\"creating-and-editing-the-configuration-file\">Creating and Editing the Configuration File</h3> <p>Before starting the Telegraf server you need to edit and/or create an initial configuration that specifies your desired <a href=\"../../inputs/index\">inputs</a> (where the metrics come from) and <a href=\"../../outputs/index\">outputs</a> (where the metrics go). There are <a href=\"../../administration/configuration/index\">several ways</a> to create and edit the configuration file. Here, we’ll generate a configuration file and simultaneously specify the desired inputs with the <code>-input-filter</code> flag and the desired output with the <code>-output-filter</code> flag.</p> <p>In the example below, we create a configuration file called <code>telegraf.conf</code> with two inputs: one that reads metrics about the system’s cpu usage (<code>cpu</code>) and one that reads metrics about the system’s memory usage (<code>mem</code>). <code>telegraf.conf</code> specifies InfluxDB as the desired output.</p> <pre data-language=\"bash\">telegraf -sample-config -input-filter cpu:mem -output-filter influxdb &gt; telegraf.conf\n</pre> <h2 id=\"start-the-telegraf-server\">Start the Telegraf Server</h2> <p>Start the Telegraf server and direct it to the relevant configuration file:</p> <h3 id=\"os-x-homebrew-http-brew-sh\">OS X <a href=\"http://brew.sh/\">Homebrew</a>\n</h3> <pre data-language=\"bash\">telegraf -config telegraf.conf\n</pre> <h3 id=\"linux-sysvinit-and-upstart-installations\">Linux (sysvinit and upstart installations)</h3> <pre data-language=\"bash\">sudo service telegraf start\n</pre> <h3 id=\"linux-systemd-installations\">Linux (systemd installations)</h3> <pre data-language=\"bash\">systemctl start telegraf\n</pre> <h2 id=\"results\">Results</h2> <p>Once Telegraf is up and running it’ll start collecting data and writing them to the desired output.</p> <p>Returning to our sample configuration, we show what the <code>cpu</code> and <code>mem</code> data look like in InfluxDB below. Note that we used the default input and output configuration settings to get these data.</p> <ul> <li>List all <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#measurement\">measurements</a> in the <code>telegraf</code> <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#database\">database</a>:</li> </ul> <pre data-language=\"bash\">&gt; SHOW MEASUREMENTS\nname: measurements\n------------------\nname\ncpu\nmem\n</pre> <ul> <li>List all <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#field-key\">field keys</a> by measurement:</li> </ul> <pre data-language=\"bash\">&gt; SHOW FIELD KEYS\nname: cpu\n---------\nfieldKey\nusage_guest\nusage_guest_nice\nusage_idle\nusage_iowait\nusage_irq\nusage_nice\nusage_softirq\nusage_steal\nusage_system\nusage_user\n\nname: mem\n---------\nfieldKey\navailable\navailable_percent\nbuffered\ncached\nfree\ntotal\nused\nused_percent\n</pre> <ul> <li>Select a sample of the data in the <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#field\">field</a> <code>usage_idle</code> in the measurement <code>cpu_usage_idle</code>:</li> </ul> <pre data-language=\"bash\">&gt; SELECT usage_idle FROM cpu WHERE cpu = 'cpu-total' LIMIT 5\nname: cpu\n---------\ntime\t\t\t               usage_idle\n2016-01-16T00:03:00Z\t 97.56189047261816\n2016-01-16T00:03:10Z\t 97.76305923519121\n2016-01-16T00:03:20Z\t 97.32533433320835\n2016-01-16T00:03:30Z\t 95.68857785553611\n2016-01-16T00:03:40Z\t 98.63715928982245\n</pre> <p>Notice that the timestamps occur at rounded ten second intervals (that is, <code>:00</code>, <code>:10</code>, <code>:20</code>, and so on) - this is a configurable setting.</p> <p>That’s it! You now have the foundation for using Telegraf to collect metrics and write them to your output of choice.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/introduction/getting_started/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/introduction/getting_started/</a>\n  </p>\n</div>\n","telegraf/v0.13/introduction/getting-started-telegraf/index":"<h1>Getting Started with Telegraf</h1>     <h2 id=\"getting-started-with-telegraf\">Getting Started with Telegraf</h2> <p>Telegraf is an agent written in Go for collecting metrics and writing them into InfluxDB or other possible outputs. This guide will get you up and running with Telegraf. It walks you through the download, installation, and configuration processes, and it shows how to use Telegraf to get data into InfluxDB.</p> <h2 id=\"download-and-install-telegraf\">Download and Install Telegraf</h2> <p>Follow the instructions in the Telegraf section on the <a href=\"https://influxdata.com/downloads/\">Downloads page</a>.</p> <blockquote> <p><strong>Note:</strong> Telegraf will start automatically using the default configuration when installed from a deb package.</p> </blockquote> <h2 id=\"configuration\">Configuration</h2> <h3 id=\"configuration-file-location-by-installation-type\">Configuration file location by installation type</h3> <ul> <li>OS X <a href=\"http://brew.sh/\">Homebrew</a>: <code>/usr/local/etc/telegraf.conf</code>\n</li> <li>Linux debian and RPM packages: <code>/etc/telegraf/telegraf.conf</code>\n</li> <li>Standalone Binary: see the next section for how to create a configuration file</li> </ul> <h3 id=\"creating-and-editing-the-configuration-file\">Creating and Editing the Configuration File</h3> <p>Before starting the Telegraf server you need to edit and/or create an initial configuration that specifies your desired <a href=\"../../inputs/index\">inputs</a> (where the metrics come from) and <a href=\"../../outputs/index\">outputs</a> (where the metrics go). There are <a href=\"../../administration/configuration/index\">several ways</a> to create and edit the configuration file. Here, we’ll generate a configuration file and simultaneously specify the desired inputs with the <code>-input-filter</code> flag and the desired output with the <code>-output-filter</code> flag.</p> <p>In the example below, we create a configuration file called <code>telegraf.conf</code> with two inputs: one that reads metrics about the system’s cpu usage (<code>cpu</code>) and one that reads metrics about the system’s memory usage (<code>mem</code>). <code>telegraf.conf</code> specifies InfluxDB as the desired output.</p> <pre data-language=\"bash\">telegraf -sample-config -input-filter cpu:mem -output-filter influxdb &gt; telegraf.conf\n</pre> <h2 id=\"start-the-telegraf-server\">Start the Telegraf Server</h2> <p>Start the Telegraf server and direct it to the relevant configuration file:</p> <h3 id=\"os-x-homebrew-http-brew-sh\">OS X <a href=\"http://brew.sh/\">Homebrew</a>\n</h3> <pre data-language=\"bash\">telegraf -config telegraf.conf\n</pre> <h3 id=\"linux-sysvinit-and-upstart-installations\">Linux (sysvinit and upstart installations)</h3> <pre data-language=\"bash\">sudo service telegraf start\n</pre> <h3 id=\"linux-systemd-installations\">Linux (systemd installations)</h3> <pre data-language=\"bash\">systemctl start telegraf\n</pre> <h2 id=\"results\">Results</h2> <p>Once Telegraf is up and running it’ll start collecting data and writing them to the desired output.</p> <p>Returning to our sample configuration, we show what the <code>cpu</code> and <code>mem</code> data look like in InfluxDB below. Note that we used the default input and output configuration settings to get these data.</p> <ul> <li>List all <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#measurement\">measurements</a> in the <code>telegraf</code> <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#database\">database</a>:</li> </ul> <pre data-language=\"bash\">&gt; SHOW MEASUREMENTS\nname: measurements\n------------------\nname\ncpu\nmem\n</pre> <ul> <li>List all <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#field-key\">field keys</a> by measurement:</li> </ul> <pre data-language=\"bash\">&gt; SHOW FIELD KEYS\nname: cpu\n---------\nfieldKey\nusage_guest\nusage_guest_nice\nusage_idle\nusage_iowait\nusage_irq\nusage_nice\nusage_softirq\nusage_steal\nusage_system\nusage_user\n\nname: mem\n---------\nfieldKey\navailable\navailable_percent\nbuffered\ncached\nfree\ntotal\nused\nused_percent\n</pre> <ul> <li>Select a sample of the data in the <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#field\">field</a> <code>usage_idle</code> in the measurement <code>cpu_usage_idle</code>:</li> </ul> <pre data-language=\"bash\">&gt; SELECT usage_idle FROM cpu WHERE cpu = 'cpu-total' LIMIT 5\nname: cpu\n---------\ntime\t\t\t               usage_idle\n2016-01-16T00:03:00Z\t 97.56189047261816\n2016-01-16T00:03:10Z\t 97.76305923519121\n2016-01-16T00:03:20Z\t 97.32533433320835\n2016-01-16T00:03:30Z\t 95.68857785553611\n2016-01-16T00:03:40Z\t 98.63715928982245\n</pre> <p>Notice that the timestamps occur at rounded ten second intervals (that is, <code>:00</code>, <code>:10</code>, <code>:20</code>, and so on) - this is a configurable setting.</p> <p>That’s it! You now have the foundation for using Telegraf to collect metrics and write them to your output of choice.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/introduction/getting-started-telegraf/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/introduction/getting-started-telegraf/</a>\n  </p>\n</div>\n","influxdb/v0.13/introduction/getting_started/index":"<h1>Getting Started</h1>     <p>With InfluxDB <a href=\"../installation/index\">installed</a>, you’re ready to start doing some awesome things. In this section we’ll use the <code>influx</code> <a href=\"../../tools/shell/index\">command line interface</a> (CLI), which is included in all InfluxDB packages and is a lightweight and simple way to interact with the database. The CLI communicates with InfluxDB directly by making requests to the InfluxDB HTTP API over port <code>8086</code> by default.</p> <blockquote> <p><strong>Note:</strong> The database can also be used by making raw HTTP requests. See <a href=\"../../guides/writing_data/index\">Writing Data</a> and <a href=\"../../guides/querying_data/index\">Querying Data</a> for examples with the <code>curl</code> application.</p> </blockquote> <h2 id=\"creating-a-database\">Creating a database</h2> <p>If you’ve installed InfluxDB locally, the <code>influx</code> command should be available via the command line. Executing <code>influx</code> will start the CLI and automatically connect to the local InfluxDB instance (assuming you have already started the server with <code>service influxdb start</code> or by running <code>influxd</code> directly). The output should look like this:</p> <pre data-language=\"bash\">$ influx\nConnected to http://localhost:8086 version 0.13.x\nInfluxDB shell 0.13.x\n&gt;\n</pre> <blockquote> <p><strong>Note:</strong> The InfluxDB HTTP API runs on port <code>8086</code> by default. Therefore, <code>influx</code> will connect to port <code>8086</code> and <code>localhost</code> by default. If you need to alter these defaults, run <code>influx --help</code>.</p> </blockquote> <p>The command line is now ready to take input in the form of the Influx Query Language (a.k.a InfluxQL) statements. To exit the InfluxQL shell, type <code>exit</code> and hit return.</p> <p>A fresh install of InfluxDB has no databases (apart from the system <code>_internal</code>), so creating one is our first task. You can create a database with the <code>CREATE DATABASE &lt;db-name&gt;</code> InfluxQL statement, where <code>&lt;db-name&gt;</code> is the name of the database you wish to create. Names of databases can contain any unicode character as long as the string is double-quoted. Names can also be left unquoted if they contain <em>only</em> ASCII letters, digits, or underscores and do not begin with a digit.</p> <p>Throughout this guide, we’ll use the database name <code>mydb</code>:</p> <pre data-language=\"sql\">&gt; CREATE DATABASE mydb\n&gt;\n</pre> <blockquote> <p><strong>Note:</strong> After hitting enter, a new prompt appears and nothing else is displayed. In the CLI, this means the statement was executed and there were no errors to display. There will always be an error displayed if something went wrong. No news is good news!</p> </blockquote> <p>Now that the <code>mydb</code> database is created, we’ll use the <code>SHOW DATABASES</code> statement to display all existing databases:</p> <pre data-language=\"sql\">&gt; SHOW DATABASES\nname: databases\n---------------\nname\n_internal\nmydb\n\n&gt;\n</pre> <blockquote> <p><strong>Note:</strong> The <code>_internal</code> database is created and used by InfluxDB to store internal runtime metrics. Check it out later to get an interesting look at how InfluxDB is performing under the hood.</p> </blockquote> <p>Unlike <code>SHOW DATABASES</code>, most InfluxQL statements must operate against a specific database. You may explicitly name the database with each query, but the CLI provides a convenience statement, <code>USE &lt;db-name&gt;</code>, which will automatically set the database for all future requests. For example:</p> <pre data-language=\"sql\">&gt; USE mydb\nUsing database mydb\n&gt;\n</pre> <p>Now future commands will only be run against the <code>mydb</code> database.</p> <h2 id=\"writing-and-exploring-data\">Writing and exploring data</h2> <p>Now that we have a database, InfluxDB is ready to accept queries and writes.</p> <p>First, a short primer on the datastore. Data in InfluxDB is organized by “time series”, which contain a measured value, like “cpu_load” or “temperature”. Time series have zero to many <code>points</code>, one for each discrete sample of the metric. Points consist of <code>time</code> (a timestamp), a <code>measurement</code> (“cpu_load”, for example), at least one key-value <code>field</code> (the measured value itself, e.g. “value=0.64”, or “temperature=21.2”), and zero to many key-value <code>tags</code> containing any metadata about the value (e.g. “host=server01”, “region=EMEA”, “dc=Frankfurt”).</p> <p>Conceptually you can think of a <code>measurement</code> as an SQL table, where the primary index is always time. <code>tags</code> and <code>fields</code> are effectively columns in the table. <code>tags</code> are indexed, and <code>fields</code> are not. The difference is that, with InfluxDB, you can have millions of measurements, you don’t have to define schemas up-front, and null values aren’t stored.</p> <p>Points are written to InfluxDB using the Line Protocol, which follows the following format:</p> <pre>&lt;measurement&gt;[,&lt;tag-key&gt;=&lt;tag-value&gt;...] &lt;field-key&gt;=&lt;field-value&gt;[,&lt;field2-key&gt;=&lt;field2-value&gt;...] [unix-nano-timestamp]\n</pre> <p>The following lines are all examples of points that can be written to InfluxDB:</p> <pre>cpu,host=serverA,region=us_west value=0.64\npayment,device=mobile,product=Notepad,method=credit billed=33,licenses=3i 1434067467100293230\nstock,symbol=AAPL bid=127.46,ask=127.48\ntemperature,machine=unit42,type=assembly external=25,internal=37 1434067467000000000\n</pre> <blockquote> <p><strong>Note:</strong> More information on the line protocol can be found on the <a href=\"../../write_protocols/write_syntax/index\">Write Syntax</a> page.</p> </blockquote> <p>To insert a single time-series datapoint into InfluxDB using the CLI, enter <code>INSERT</code> followed by a point:</p> <pre data-language=\"sql\">&gt; INSERT cpu,host=serverA,region=us_west value=0.64\n&gt;\n</pre> <p>A point with the measurement name of <code>cpu</code> and tag <code>host</code> has now been written to the database, with the measured <code>value</code> of <code>0.64</code>.</p> <p>Now we will query for the data we just wrote:</p> <pre data-language=\"sql\">&gt; SELECT host, region, value FROM cpu\nname: cpu\n---------\ntime\t\t    \t                     host     \tregion   value\n2015-10-21T19:28:07.580664347Z  serverA\t  us_west\t 0.64\n\n&gt;\n</pre> <blockquote> <p><strong>Note:</strong> We did not supply a timestamp when writing our point. When no timestamp is supplied for a point, InfluxDB assigns the local current timestamp when the point is ingested. That means your timestamp will be different.</p> </blockquote> <p>Let’s try storing another type of data, with two fields in the same measurement:</p> <pre data-language=\"sql\">&gt; INSERT temperature,machine=unit42,type=assembly external=25,internal=37\n&gt;\n</pre> <p>To return all fields and tags with a query, you can use the <code>*</code> operator:</p> <pre data-language=\"sql\">&gt; SELECT * FROM temperature\nname: temperature\n-----------------\ntime\t\t                        \t external\t  internal\tmachine\ttype\n2015-10-21T19:28:08.385013942Z  25\t        \t37     \t\tunit42  assembly\n\n&gt;\n</pre> <p>InfluxQL has many <a href=\"../../query_language/spec/index\">features and keywords</a> that are not covered here, including support for Go-style regex. For example:</p> <pre data-language=\"sql\">&gt; SELECT * FROM /.*/ LIMIT 1\n--\n&gt; SELECT * FROM cpu_load_short\n--\n&gt; SELECT * FROM cpu_load_short WHERE value &gt; 0.9\n</pre> <p>This is all you need to know to write data into InfluxDB and query it back. To learn more about the InfluxDB write protocol, check out the guide on <a href=\"../../guides/writing_data/index\">Writing Data</a>. To futher explore the query language, check out the guide on <a href=\"../../guides/querying_data/index\">Querying Data</a>. For more information on InfluxDB concepts, check out the <a href=\"../../concepts/key_concepts/index\">Key Concepts</a> page.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/introduction/getting_started/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/introduction/getting_started/</a>\n  </p>\n</div>\n","chronograf/v0.13/introduction/getting_started/index":"<h1>Getting Started with Chronograf</h1>     <p>This guide shows how to download, install, and start visualizing data with Chronograf.</p> <h2 id=\"download-and-install-chronograf\">Download and Install Chronograf</h2> <p>Follow the instructions in the Chronograf Downloads section on the <a href=\"https://influxdata.com/downloads/#chronograf\">Downloads page</a>.</p> <h2 id=\"start-the-chronograf-server\">Start the Chronograf Server</h2> <p>How you start Chronograf depends on how you installed it:</p> <h4 id=\"debian-or-rpm-package\">Debian or RPM package</h4> <pre>sudo service chronograf start\n</pre> <p>Note that the Chronograf startup script needs root permission to ensure that it can write to <code>/var/log</code>, but the executable runs as a normal user.</p> <h4 id=\"os-x-via-homebrew-http-brew-sh\">OS X via <a href=\"http://brew.sh/\">Homebrew</a>\n</h4> <ul> <li>\n<p>Run Chronograf manually on the command line:</p> <pre>chronograf\n</pre>\n</li> <li>\n<p>To have launchd start homebrew/binary/chronograf at login:</p> <pre>ln -sfv /usr/local/opt/chronograf/*.plist ~/Library/LaunchAgents\n</pre> <p>Then to load homebrew/binary/chronograf now:</p> <pre>launchctl load ~/Library/LaunchAgents/homebrew.mxcl.chronograf.plist\n</pre>\n</li> </ul> <h4 id=\"standalone-os-x-binary\">Standalone OS X binary</h4> <p>Assuming you’re working with Chronograf version 0.13, from the <code>chronograf-0.13/</code> directory:</p> <pre>./chronograf-0.13-darwin_amd64\n</pre> <p>Check to see that Chronograf is running at <a href=\"http://127.0.0.1:10000\">http://127.0.0.1:10000</a>. If this is the first time you’ve started Chronograf, you’ll see this:</p> <p><img src=\"https://docs.influxdata.com/img/chronograf/v0.11/add-new-server.png\" alt=\"Add new server\"></p> <blockquote> <p><strong>NOTE:</strong> By default, Chronograf runs on localhost port <code>10000</code>. Those settings are configurable; see the <a href=\"../../administration/configuration/index\">configuration file</a> to change them and to see the other configuration options.</p> </blockquote> <p>Now that you’ve got everything installed and running it’s time to start visualizing your data in Chronograf!</p> <h2 id=\"add-your-first-server\">Add your first Server</h2> <p>Direct Chronograf to your InfluxDB data by adding a server:</p> <p>1. Click the <code>Add new server</code> button.</p> <p>2. Fill out the form with the relevant information. In this step the only required fields are <code>NICKNAME</code>, <code>HOST</code>, and <code>PORT</code>.</p> <p>In the example below, we’ve called our server <code>InfluxDB-1</code> and it’s running on <code>localhost</code> on port <code>8086</code> (the default <code>HOST</code> and <code>PORT</code> for InfluxDB). <br> <br> <img src=\"https://docs.influxdata.com/img/chronograf/v0.11/add-server.png\" alt=\"Add server\"></p> <p>3. Click the <code>Add</code> button.</p> <p>In the image below, notice that Chronograf is now aware of our InfluxDB server <code>InfluxDB-1</code>. <br> <br> <img src=\"https://docs.influxdata.com/img/chronograf/v0.11/servers.png\" alt=\"Servers\"></p> <p>Click <code>Done</code> in the top left corner and move on to the next section to create your first graph. You can always return to the <code>Servers</code> page by clicking on the gear in the top right corner.</p> <h2 id=\"create-your-first-visualization\">Create your first Visualization</h2> <p>It’s time to graph your data. In the next steps, we’ll create an example graph that shows the average idle CPU percentage grouped by CPU tag and by one minute time intervals. If you’d like to follow along, see <a href=\"../../../../telegraf/v0.13/introduction/getting-started-telegraf/index\">Getting Started with Telegraf</a> to get the data we use in this section.</p> <p>1. Click <code>Add Visualization</code> and name your graph in the <code>New Graph</code> window. We’ll call our graph <code>Average idle CPU usage</code>.</p> <p>2. Click the <code>Save</code> button.</p> <p>3. Choose the server, <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#database\">database</a>, and <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#retention-policy-rp\">retention policy</a> that you want to work with: <br> <br> <img src=\"https://docs.influxdata.com/img/chronograf/v0.11/server-tray.png\" alt=\"Server Tray\"></p> <p>In this example, we’re working with the the server <code>InfluxDB-1</code>, the database <code>telegraf</code>, and the retention policy <code>default</code>.</p> <p>4. Create your query.</p> <p>To create a <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#query\">query</a>, you can either use the Query Builder or, if you’re already familiar with InfluxQL, you can manually enter the query in the text input. In this step, we manually enter the following query:</p> <pre>SELECT mean(usage_idle) FROM cpu WHERE time &gt; now() - 10m GROUP BY time(1m), cpu\n</pre> <p>Our query calculates the average of the field key <code>usage_idle</code> in the measurement <code>cpu</code>, and it <code>GROUPs BY</code> the tag <code>cpu</code> and by one minute intervals. The query asks for data that fall within the past 10 minutes. <br> <br> <img src=\"https://docs.influxdata.com/img/chronograf/v0.11/query-builder.png\" alt=\"Query Builder\"></p> <p>Click <code>Done</code> in the top right corner to complete your graph, and move on to the next section to create your first dashboard.</p> <h2 id=\"create-your-first-dashboard\">Create your first Dashboard</h2> <p>Now that we have a graph we want to add it to a dashboard.</p> <p>1. Move to the <code>DASHBOARDS</code> tab at the top of your screen.</p> <p>2. Click <code>+</code> and name your dashboard in the <code>New Dashboard</code> window. We’ll call our dashboard <code>Idle CPU usage</code>.</p> <p>Because our dashboard has no visualizations, it looks like this: <br> <br> <img src=\"https://docs.influxdata.com/img/chronograf/v0.11/add-graph-to-dash.png\" alt=\"Add Graph to Dashboard\"></p> <p>3. Click the <code>Add Visualization</code> button.</p> <p>4. We want to add the graph we made in the section above so we click <code>Add From Existing Visualizations</code>. <br> <br> <img src=\"https://docs.influxdata.com/img/chronograf/v0.11/add-from-existing-graphs.png\" alt=\"Add From Existing Graphs\"></p> <p>5. Select <code>Average idle CPU usage</code> and click the <code>Add Visualizations to Dash</code> button in the top right corner. <br> <br> <img src=\"https://docs.influxdata.com/img/chronograf/v0.13/select-graph-for-dash.png\" alt=\"Select Graph For Dash\"></p> <p>That takes us back to our new dashboard! One visualization on a dashboard isn’t spectacularly interesting, so we’ve added a couple more graphs to show you some of the possibilities: <br> <br> <img src=\"https://docs.influxdata.com/img/chronograf/v0.11/sample-dashboard.png\" alt=\"Sample Dashboard\"></p> <p>And that’s it! You now have the foundation for building beautiful data visualizations and dashboards with Chronograf.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/introduction/getting_started/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/introduction/getting_started/</a>\n  </p>\n</div>\n","chronograf/v0.13/introduction/index":"<h1>Introduction to Chronograf</h1>     <p>The introductory documentation includes all the information you need to get up and running with Chronograf.</p> <h2 id=\"download-https-influxdata-com-downloads-chronograf\"><a href=\"https://influxdata.com/downloads/#chronograf\">Download</a></h2> <p>Download Chronograf.</p> <h2 id=\"installation-chronograf-v0-13-introduction-installation\"><a href=\"installation/index\">Installation</a></h2> <p>Directions for installing, starting, and configuring Chronograf.</p> <h2 id=\"getting-started-chronograf-v0-13-introduction-getting-started\"><a href=\"getting_started/index\">Getting Started</a></h2> <p>Get started building beautiful visualizations and dashboards with Chronograf.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/introduction/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/introduction/</a>\n  </p>\n</div>\n","chronograf/v0.13/templating/index":"<h1>Templating</h1>     <h2 id=\"time-range-template-chronograf-v0-13-templating-template-time-range\"><a href=\"template_time_range/index\">Time Range Template</a></h2> <p>Easily modify the time range displayed by a visualization and/or dashboard.</p> <h2 id=\"tag-value-template-chronograf-v0-13-templating-template-tag-values\"><a href=\"template_tag_values/index\">Tag Value Template</a></h2> <p>Easily modify the <a href=\"../../../influxdb/v0.13/concepts/glossary/index#tag-value\">tag value</a> displayed by a visualization and/or dashboard.</p> <h2 id=\"templating-a-dashboard-chronograf-v0-13-templating-templating-a-dashboard\"><a href=\"templating_a_dashboard/index\">Templating a Dashboard</a></h2><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/templating/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/templating/</a>\n  </p>\n</div>\n","chronograf/v0.13/templating/template_time_range/index":"<h1>Time Range Template</h1>     <p>The following sections will show you how to create time range template on a graph. If you’d like to follow along, see <a href=\"../../../../telegraf/v0.13/introduction/getting-started-telegraf/index\">Getting Started with Telegraf</a> to get the data we use in this section.</p> <h3 id=\"template-time-range-with-tmpltime\">Template time range with <code>tmplTime()</code>\n</h3> <p>Use the <code>tmplTime()</code> function to create a template variable that controls the query’s time range. This works for queries that use the query builder as well as manually entered queries.</p> <p><strong>Example:</strong></p> <p>Once you enter the query below, you can select alternative time ranges by clicking on the dropdown in the top right corner.</p> <p><em>Query:</em></p> <pre>SELECT usage_idle FROM \"telegraf\".\"default\".\"cpu\" WHERE tmplTime()\n</pre> <p><em>Chronograf visualization with a time template variable:</em></p> <p><img src=\"https://docs.influxdata.com/img/chronograf/v0.11/time-template.gif\" alt=\"Time template\"></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/templating/template_time_range/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/templating/template_time_range/</a>\n  </p>\n</div>\n","chronograf/v0.13/templating/template_tag_values/index":"<h1>Tag Value Template</h1>     <p>The following section will show you how to create a tag value template on a graph. If you’d like to follow along, see <a href=\"../../../../telegraf/v0.13/introduction/getting-started-telegraf/index\">Getting Started with Telegraf</a> to get the data we use in this section.</p> <h3 id=\"template-tag-values-with-tmpltagvalue\">Template tag values with <code>tmplTagValue()</code>\n</h3> <p>Use the <code>tmplTagValue()</code> function to create a template variable that allows you to change the value of the query’s <a href=\"../../../../influxdb/v0.13/concepts/glossary/index#tag-key\">tag key(s)</a>.</p> <p><strong>Example:</strong></p> <p>In the <code>FILTER BY</code> section of the Query Builder:</p> <p>1. Select the tag key that you want to template (here, we choose <code>cpu</code>).</p> <p>2. Select <code>Make Variable</code> as the Tag Value.</p> <p>3. Save the title of your template variable in the <code>Make Tag Value Variable</code> window (here, we name it <code>cpu</code>).</p> <p>Now you can select alternative tag values of the tag key <code>cpu</code> in the <code>cpu</code> dropdown in the top right corner.</p> <blockquote> <p><strong>Note:</strong> If you’d like to enter the <code>tmplTagValue()</code> function manually, place in the <code>WHERE</code> clause:</p> <pre>&lt;tag_key&gt; = tmplTagValue('&lt;tag_key&gt;','&lt;variable_name&gt;')\n</pre> <p>Where <code>variable_name</code> is the title of the dropdown that appears in the top right corner.</p> </blockquote> <p><em>Chronograf visualization with a tag template variable:</em></p> <p><img src=\"https://docs.influxdata.com/img/chronograf/v0.11/tag-template.gif\" alt=\"Tag template\"></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/templating/template_tag_values/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/templating/template_tag_values/</a>\n  </p>\n</div>\n","kapacitor/v0.13/introduction/getting_started/index":"<h1>Getting Started</h1>     <p>Kapacitor is a data processing engine. It can process both stream and batch data. This guide will walk you through both workflows and teach you the basics of using and running a Kapacitor daemon.</p> <h2 id=\"what-you-will-need\">What you will need</h2> <p>Don’t worry about installing anything yet, instructions are found below.</p> <ul> <li>\n<a href=\"https://docs.influxdata.com/docs/v0.9/introduction/installation.html\">InfluxDB</a> - While Kapacitor does not require InfluxDB it is the easiest to setup and so we will use it in this guide. You will need InfluxDB &gt;= 0.13</li> <li>\n<a href=\"https://github.com/influxdb/telegraf#installation\">Telegraf</a> - We will use a specific Telegraf config to send data to InfluxDB so that the examples Kapacitor tasks have context. You will need Telegraf &gt;= 0.13</li> <li>\n<a href=\"https://github.com/influxdb/kapacitor\">Kapacitor</a> - You can get the latest Kapacitor binaries for your OS at the <a href=\"https://influxdata.com/downloads/#kapacitor\">downloads</a> page.</li> <li>Terminal - Kapacitor’s interface is via a CLI and so you will need a basic terminal to issue commands.</li> </ul> <h2 id=\"the-use-case\">The Use Case</h2> <p>For this guide we will follow the classic use case of triggering an alert for high cpu usage on a server.</p> <h2 id=\"the-process\">The Process</h2> <ol> <li>Install everything we need.</li> <li>Start InfluxDB and send it data from Telegraf.</li> <li>Configure Kapacitor.</li> <li>Start Kapacitor.</li> <li>Define and run a streaming task to trigger cpu alerts.</li> <li>Define and run a batching task to trigger cpu alerts.</li> </ol> <h2 id=\"installation\">Installation</h2> <p>Install <a href=\"../installation/index\">Kapacitor</a>, <a href=\"https://docs.influxdata.com/docs/v0.9/introduction/installation.html\">InfluxDB</a> and <a href=\"https://github.com/influxdb/telegraf#installation\">Telegraf</a> on the same host.</p> <p>All examples will assume that Kapacitor is running on <code>http://localhost:9092</code> and InfluxDB on <code>http://localhost:8086</code>.</p> <h2 id=\"influxdb-telegraf\">InfluxDB + Telegraf</h2> <p>Start InfluxDB:</p> <pre data-language=\"bash\">influxd run\n</pre> <p>The following is a simple Telegraf configuration file that will send just cpu metrics to InfluxDB:</p> <pre>[agent]\n    interval = \"1s\"\n\n[outputs]\n\n# Configuration to send data to InfluxDB.\n[outputs.influxdb]\n    urls = [\"http://localhost:8086\"]\n    database = \"kapacitor_example\"\n    user_agent = \"telegraf\"\n\n# Collect metrics about cpu usage\n[cpu]\n    percpu = false\n    totalcpu = true\n    drop = [\"cpu_time\"]\n\n</pre> <p>Put the above configuration in a file called <code>telegraf.conf</code> and start telegraf:</p> <pre data-language=\"bash\">telegraf -config telegraf.conf\n</pre> <p>OK, at this point we should have a running InfluxDB + Telegraf setup. There should be some cpu metrics in a database called <code>kapacitor_example</code>. Confirm this with this query:</p> <pre data-language=\"bash\">curl -G 'http://localhost:8086/query?db=kapacitor_example' --data-urlencode 'q=SELECT count(usage_idle) FROM cpu'\n</pre> <h2 id=\"starting-kapacitor\">Starting Kapacitor</h2> <p>First we need a valid configuration file. Run the following command to create a default configuration file:</p> <pre data-language=\"bash\">kapacitord config &gt; kapacitor.conf\n</pre> <p>The configuration is a <a href=\"https://github.com/toml-lang/toml\">toml</a> file and is very similar to the InfluxDB configuration. That is because any input that you can configure for InfluxDB also works for Kapacitor.</p> <p>Let’s start the Kapacitor server:</p> <pre data-language=\"bash\">kapacitord -config kapacitor.conf\n</pre> <p>Since InfluxDB is running on <code>http://localhost:8086</code> Kapacitor finds it during start up and creates several <a href=\"https://github.com/influxdb/influxdb/blob/master/influxql/README.md#create-subscription\">subscriptions</a> on InfluxDB. These subscriptions tell InfluxDB to send all the data it receives to Kapacitor. You should see some basic start up messages and something about listening on UDP port and starting subscriptions. At this point InfluxDB is streaming the data it is receiving from Telegraf to Kapacitor.</p> <h2 id=\"trigger-alert-from-stream-data\">Trigger Alert from Stream data</h2> <p>That was a bit of setup, but at this point it should be smooth sailing and we can get to the fun stuff of actually using Kapacitor.</p> <p>A <code>task</code> in Kapacitor represents an amount of work to do on a set of data. There are two types of tasks, <code>stream</code> and <code>batch</code> tasks. We will be using a <code>stream</code> task first, and next we will do the same thing with a <code>batch</code> task.</p> <p>Kapacitor uses a DSL called <a href=\"../../tick/index\">TICKscript</a> to define tasks. Each TICKscript defines a pipeline that tells Kapacitor which data to process and how.</p> <p>So what do we want to tell Kapacitor to do? As an example, we will trigger an alert on high cpu usage. What is high cpu usage? Let’s say when idle cpu drops below 70% we should trigger an alert.</p> <p>Now that we know what we want to do, let’s write it in a way the Kapacitor understands. Put the script below into a file called <code>cpu_alert.tick</code> in your working directory:</p> <pre data-language=\"javascript\">stream\n    // Select just the cpu measurement from our example database.\n    |from()\n        .measurement('cpu')\n    |alert()\n        .crit(lambda: \"usage_idle\" &lt;  70)\n        // Whenever we get an alert write it to a file.\n        .log('/tmp/alerts.log')\n</pre> <p>Kapacitor has an HTTP API with which all communcation happens. The binary <code>kapacitor</code> exposes the API over the command line. Now use the CLI tool to define the <code>task</code> and the databases and retention policies it can access:</p> <pre data-language=\"bash\">kapacitor define cpu_alert \\\n    -type stream \\\n    -tick cpu_alert.tick \\\n    -dbrp kapacitor_example.default\n</pre> <p>That’s it, Kapacitor now knows how to trigger our alert.</p> <p>However nothing is going to happen until we enable the task. Before we enable the task, we should test it first so we do not spam ourselves with alerts. Record the current data stream for a bit so we can use it to test our task with:</p> <pre data-language=\"bash\">kapacitor record stream -task cpu_alert -duration 20s\n</pre> <p>Since we defined the task with a database and retention policy pair, the recording knows to only record data from that database and retention policy.</p> <p>Now grab that ID that was returned and lets put it in a bash variable for easy use later (your ID will be different):</p> <pre data-language=\"bash\">rid=cd158f21-02e6-405c-8527-261ae6f26153\n</pre> <p>Let’s confirm that the recording captured some data. Run</p> <pre data-language=\"bash\">kapacitor list recordings $rid\n</pre> <p>You should see some output like:</p> <pre>ID                                      Type    Status    Size      Date\ncd158f21-02e6-405c-8527-261ae6f26153    stream  finished  1.6 MB    04 May 16 11:44 MDT\n</pre> <p>As long as the size is more than a few bytes we know we captured data. If Kapacitor is not receiving data yet, check each layer: Telegraf -&gt; InfluxDB -&gt; Kapacitor. Telegraf will log errors if it cannot communicate to InfluxDB. InfluxDB will log an error about <code>connection refused</code> if it cannot send data to Kapacitor. Run the query <code>SHOW SUBSCRIPTIONS</code> to find the endpoint that InfluxDB is using to send data to Kapacitor.</p> <p>OK, we have a snapshot of data recorded from the stream, so we can now replay that data to our task. The <code>replay</code> action replays data only to a specific task. This way we can test the task in complete isolation:</p> <pre data-language=\"bash\">kapacitor replay -recording $rid -task cpu_alert\n</pre> <p>Since we already have the data recorded, we can just replay the data as fast as possible instead of waiting for real time to pass. If <code>-real</code> was set, then the data would be replayed by waiting for the deltas between the timestamps to pass, though the result is identical whether real time passes or not. This is because time is measured on each node by the data points it receives.</p> <p>Check the log using the command below, did we get any alerts? The file should contain lines of JSON, where each line represents one alert. The JSON contains the alert level and the data that triggered the alert.</p> <pre data-language=\"bash\">cat /tmp/alerts.log\n</pre> <p>Depending on how busy the server was, maybe not. Let’s modify the task to be really sensitive so that we know the alerts are working. Change the <code>.crit(lambda: \"value\" &lt; 70)</code> line in the TICKscript to <code>.crit(lambda: \"value\" &lt; 100)</code>. Now every data point that was received during the recording will trigger an alert.</p> <p>Let’s replay it again and verify the results. Any time you want to update a task change the TICKscript and then run the <code>define</code> command again with just the <code>TASK_NAME</code> and <code>-tick</code> arguments:</p> <pre data-language=\"bash\"># edit threshold in cpu_alert.tick and redefine the task.\nkapacitor define cpu_alert -tick cpu_alert.tick\nkapacitor replay -recording $rid -task cpu_alert\n</pre> <p>Now that we know it’s working, let’s change it back to a more reasonable threshold. Are you happy with the threshold? If so, let’s <code>enable</code> the task so it can start processing the live data stream with:</p> <pre data-language=\"bash\">kapacitor enable cpu_alert\n</pre> <p>Now you can see alerts in the log in real time.</p> <p>To see that the task is receiving data and behaving as expected run the <code>show</code> command to get more information about a task:</p> <pre data-language=\"bash\">$ kapacitor show cpu_alert\nID: cpu_alert\nError:\nType: stream\nStatus: Enabled\nExecuting: true\nCreated: 04 May 16 21:01 MDT\nModified: 04 May 16 21:04 MDT\nLastEnabled: 04 May 16 21:03 MDT\nDatabases Retention Policies: [\"kapacitor_example\".\"default\"]\nTICKscript:\nstream\n    // Select just the cpu measurement from our example database.\n    |from()\n        .measurement('cpu')\n    |alert()\n        .crit(lambda: \"usage_idle\" &lt;  70)\n        // Whenever we get an alert write it to a file.\n        .log('/tmp/alerts.log')\n\nDOT:\ndigraph asdf {\ngraph [throughput=\"0.00 points/s\"];\n\nstream0 [avg_exec_time_ns=\"0\" ];\nstream0 -&gt; from1 [processed=\"12\"];\n\nfrom1 [avg_exec_time_ns=\"0\" ];\nfrom1 -&gt; alert2 [processed=\"12\"];\n\nalert2 [alerts_triggered=\"0\" avg_exec_time_ns=\"0\" ];\n}\n</pre> <p>The first part has information about the state of the task and any error it may have encounted. The <code>TICKscript</code> section displays the version of the TICKscript that Kapacitor has stored in its local db.</p> <p>The last section <code>DOT</code> is a <a href=\"http://www.graphviz.org\">graphviz dot</a> formatted string that contains information about the data processing pipeline defined by the TICKscript. The <em>key=value</em> pairs are stats about each node or edge. The <em>processed</em> key indicates the number of data points that have passed along the specified edge of the graph. For example in the above the <code>stream0</code> node (aka the <code>stream</code> var from the TICKscript) has sent 12 points to the <code>from1</code> node. The <code>from1</code> node has also sent 12 points on to the <code>alert2</code> node. Since Telegraf is configured to only send <code>cpu</code> data all 12 points match the from/measurement criteria of the <code>from1</code> node and are passed on.</p> <p>Well now that we can see the task is running with live data, here is a quick hack to use 100% of one core so you can get some cpu activity:</p> <pre data-language=\"bash\">while true; do i=0; done\n</pre> <p>Well, that was cool and all, but, just to get a simple threshold alert, there are plenty of ways to do that. Why all this pipeline TICKscript stuff? Well, it can quickly be extended to become <em>much</em> more powerful.</p> <h3 id=\"keep-the-quotes-in-mind\">Keep the quotes in mind</h3> <p>Single quotes and double quotes in kapacitor do very different things:</p> <p>Note the following example:</p> <pre data-language=\"javascript\">var data = stream\n    |from()\n        .database('telegraf')\n        .retentionPolicy('default')\n        .measurement('cpu')\n        // NOTE: Double quotes on server1\n        .where(lambda: \"host\" == \"server1\")\n</pre> <p>Result of this search will always be empty, because we used double quotes around server1. This means that we are searching for series where field “host” is equal to field “server1”. This is probably not what we were planning to do. We were probably searching for series where tag “host” has value “server1”, so we should use single quotes for that and our tick script should look like this:</p> <pre data-language=\"javascript\">var data = stream\n    |from()\n        .database('telegraf')\n        .retentionPolicy('default')\n        .measurement('cpu')\n        // NOTE: Single quotes on server1\n        .where(lambda: \"host\" == 'server1')\n</pre> <h3 id=\"extending-your-tickscripts\">Extending Your TICKscripts</h3> <p>The TICKscript below will compute the running mean and compare current values to it. It will then trigger an alert if the values are more than 3 standard deviations away from the mean. Replace the <code>cpu_alert.tick</code> script with the TICKscript below:</p> <pre data-language=\"javascript\">stream\n    |from()\n        .measurement('cpu')\n    |alert()\n        // Compare values to running mean and standard deviation\n        .crit(lambda: sigma(\"usage_idle\") &gt; 3)\n        .log('/tmp/alerts.log')\n</pre> <p>Just like that, we have a dynamic threshold, and, if cpu usage drops in the day or spikes at night, we will get an alert! Let’s try it out. Use <code>define</code> to update the task TICKscript.</p> <pre data-language=\"bash\">kapacitor define cpu_alert -tick cpu_alert.tick\n</pre> <blockquote> <p>NOTE: If a task is already enabled <code>define</code>ing the task again will automatically <code>reload</code> it. To define a task without reloading it use <code>-no-reload</code></p> </blockquote> <p>Now tail the alert log:</p> <pre data-language=\"bash\">tail -f /tmp/alerts.log\n</pre> <p>There shouldn’t be any alerts triggering just yet. Next, start a few while loops to add some load:</p> <pre data-language=\"bash\">while true; do i=0; done\n</pre> <p>You should see an alert trigger in the log once you create enough load. Leave the loops running for a few minutes. After canceling the loops, you should get another alert that cpu usage has again changed. Using this technique you can get alerts for the raising and falling edges of cpu usage, as well as any outliers.</p> <h3 id=\"a-real-world-example\">A Real-World Example</h3> <p>Now that we understand the basics, here is a more real world example. Once you get metrics from all your hosts streaming to Kapacitor, you can do something like: Aggregate and group the cpu usage for each service running in each datacenter, and then trigger an alert based off the 95th percentile. In addition to just writing the alert to a log, Kapacitor can integrate with third-party utilities: currently Slack, PagerDuty and VictorOps are supported, as well as posting the alert to a custom endpoint or executing a custom script. You can also define a custom message format so that alerts have the right context and meaning. The TICKscript for this would look like:</p> <pre data-language=\"javascript\">stream\n    |from()\n        .measurement('cpu')\n    // create a new field called 'used' which inverts the idle cpu.\n    |eval(lambda: 100.0 - \"usage_idle\")\n        .as('used')\n    |groupBy('service', 'datacenter')\n    |window()\n        .period(1m)\n        .every(1m)\n    // calculate the 95th percentile of the used cpu.\n    |percentile('used', 95.0)\n    |eval(lambda: sigma(\"percentile\"))\n        .as('sigma')\n        .keep('percentile', 'sigma')\n    |alert()\n        .id('{{ .Name }}/{{ index .Tags \"service\" }}/{{ index .Tags \"datacenter\"}}')\n        .message('{{ .ID }} is {{ .Level }} cpu-95th:{{ index .Fields \"percentile\" }}')\n        // Compare values to running mean and standard deviation\n        .warn(lambda: \"sigma\" &gt; 2.5)\n        .crit(lambda: \"sigma\" &gt; 3.0)\n        .log('/tmp/alerts.log')\n\n        // Post data to custom endpoint\n        .post('https://alerthandler.example.com')\n\n        // Execute custom alert handler script\n        .exec('/bin/custom_alert_handler.sh')\n\n        // Send alerts to slack\n        .slack()\n        .channel('#alerts')\n\n        // Sends alerts to PagerDuty\n        .pagerDuty()\n\n        // Send alerts to VictorOps\n        .victorOps()\n        .routingKey('team_rocket')\n</pre> <p>Something so simple as defining an alert can quickly be extended to apply to a much larger scope. With the above script, you will be alerted if any service in any datacenter deviates more than 3 standard deviations away from normal behavior as defined by the historical 95th percentile of cpu usage, within 1 minute!</p> <p>For more information on how the alerting works, see the <a href=\"../../nodes/alert_node/index\">AlertNode</a> docs.</p> <h2 id=\"trigger-alert-from-batch-data\">Trigger Alert from Batch data</h2> <p>Instead of just processing the data in streams, you can also tell Kapacitor to periodically query InfluxDB and then process that data in batches. While triggering an alert based off cpu usage is more suited for the streaming case, you can get the basic idea of how <code>batch</code> tasks work by following the same use case.</p> <p>This TICKscript does the same thing as the earlier stream task, but as a batch task:</p> <pre data-language=\"javascript\">batch\n    |query('''\n        SELECT mean(usage_idle)\n        FROM \"kapacitor_example\".\"default\".\"cpu\"\n    ''')\n        .period(5m)\n        .every(5m)\n        .groupBy(time(1m))\n    |alert()\n        .crit(lambda: \"mean\" &lt; 70)\n        .log('/tmp/alerts.log')\n</pre> <p>To define this task do:</p> <pre data-language=\"bash\">kapacitor define batch_cpu_alert -type batch -tick batch_cpu_alert.tick -dbrp kapacitor_example.default\n</pre> <p>You can record the result of the query in the task like so (again, your ID will differ):</p> <pre data-language=\"bash\">kapacitor record batch -task batch_cpu_alert -past 20m\n# Save the id again\nrid=b82d4034-7d5c-4d59-a252-16604f902832\n</pre> <p>This will record the last 20 minutes of batches using the query in the <code>batch_cpu_alert</code> task. In this case, since the <code>period</code> is 5 minutes, the last 4 batches will be saved in the recording.</p> <p>The batch recording can be replayed in the same way:</p> <pre data-language=\"bash\">kapacitor replay -recording $rid -task batch_cpu_alert\n</pre> <p>Check the alert log to make sure you received alerts to fire when you expected them to. You can also go back and use the <code>sigma</code> based alert for the batch data as well. Play around until you are comfortable updating, testing, and running tasks in Kapacitor.</p> <h3 id=\"what-s-next\">What’s next?</h3> <p>Take a look at the <a href=\"../../examples/index\">examples</a> page for more guides on how to use Kapacitor. These use cases demonstrate some of the more rich features of Kapacitor.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/introduction/getting_started/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/introduction/getting_started/</a>\n  </p>\n</div>\n","telegraf/v0.13/outputs/index":"<h1>Supported Outputs</h1>     <p>Telegraf allows users to specify multiple output sinks in the configuration file.</p> <h2 id=\"supported-output-plugins-list\">Supported Output Plugins List</h2> <ul> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/influxdb\">InfluxDB</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/amon\">amon</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/amqp\">AMQP</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/kinesis\">AWS Kinesis</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/cloudwatch\">AWS CloudWatch</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/datadog\">Datadog</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/file\">File</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/graphite\">Graphite</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/graylog\">Graylog</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/instrumental\">Instrumental</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/kafka\">Kafka</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/librato\">Librato</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/mqtt\">MQTT</a></li> <li><a href=\"ghttps://ithub.com/influxdata/telegraf/tree/master/plugins/outputs/nsq\">NSQ</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/opentsdb\">OpenTSDB</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/prometheus_client\">Prometheus</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/outputs/riemann\">Riemann</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/outputs/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/outputs/</a>\n  </p>\n</div>\n","telegraf/v0.13/inputs/index":"<h1>Supported Input Plugins</h1>     <p>Telegraf is entirely input driven. It gathers all metrics from the inputs specified in the configuration file.</p> <h2 id=\"usage-instructions\">Usage Instructions</h2> <p>View usage instructions for each input by running <code>telegraf -usage &lt;input-name&gt;</code>.</p> <h2 id=\"supported-input-plugins\">Supported Input Plugins</h2> <ul> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/cloudwatch\">AWS CloudWatch</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/aerospike\">Aerospike</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/apache\">Apache</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/bcache\">bcache</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/cassandra\">Cassandra</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/couchbase\">Couchbase</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/couchdb\">CouchDB</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/disque\">Disque</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/dns_query\">DNS Query Time</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/docker\">Docker</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/dovecot\">Dovecot</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/elasticsearch\">Elasticsearch</a></li> <li>\n<a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/exec\">exec</a> (generic JSON-emitting executable plugin)</li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/filestat\">filestat</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/haproxy\">HAProxy</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/http_response\">HTTP response</a></li> <li>\n<a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/httpjson\">HTTPJSON</a> (generic JSON-emitting http service plugin)</li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/influxdb\">InfluxDB</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/ipmi_sensor\">IPMI_sensor</a></li> <li>\n<a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia\">Jolokia</a> (remote JMX with JSON over HTTP)</li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/leofs\">LeoFS</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/lustre2\">Lustre2</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/mailchimp\">Mailchimp</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/memcached\">Memcached</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/mesos\">Mesos</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/mongodb\">MongoDB</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/mysql\">MySQL</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/net_response\">Net_response</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/nginx\">NGINX</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/nsq\">NSQ</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/ntpq\">NTPQ</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/phpfpm\">PHP-FPM</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/passenger\">Phusion Passenger</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/ping\">ping</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/postgresql\">PostgreSQL</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/postgresql_extensible\">PostgreSQL_extensible</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/powerdns\">PowerDNS</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/procstat\">procstat</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/prometheus\">Prometheus</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/puppetagent\">Puppet Agent</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/rabbitmq\">RabbitMQ</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/raindrops\">raindrops</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/redis\">Redis</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/rethinkdb\">RethinkDB</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/riak\">Riak</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/sensors\">sensors</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/snmp\">snmp</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/sqlserver\">SQL server (Microsoft)</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/twemproxy\">twemproxy (nutcracker)</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/trig\">trig</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/zfs\">ZFS</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/zookeeper\">Zookeeper</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/win_perf_counters\">Windows performance counters</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/sysstat\">sysstat</a></li> <li>\n<a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/system\">system</a> (cpu, mem, net, netstat, disk, diskio, swap)</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/inputs/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/inputs/</a>\n  </p>\n</div>\n","telegraf/v0.13/services/index":"<h1>Supported Service Inputs</h1>     <p>Telegraf is entirely input driven. It gathers all metrics from the inputs specified in the configuration file.</p> <h2 id=\"usage-instructions\">Usage Instructions</h2> <p>View usage instructions for each service input by running <code>telegraf -usage &lt;service-input-name&gt;</code>.</p> <h2 id=\"supported-service-plugin-list\">Supported Service Plugin List</h2> <ul> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/kafka_consumer\">Kafka Consumer</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/github_webhooks\">GitHub Webhooks</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/mqtt_consumer\">MQTT Consumer</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/nats_consumer\">NATS Consumer</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/statsd\">StatsD</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/tcp_listener\">TCP Listener</a></li> <li><a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/udp_listener\">UDP Listener</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/services/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/services/</a>\n  </p>\n</div>\n","telegraf/v0.13/administration/index":"<h1>Administration</h1>     <h2 id=\"differences-between-telegraf-0-13-and-0-12-telegraf-v0-13-administration-differences\"><a href=\"differences/index\">Differences between Telegraf 0.13 and 0.12</a></h2> <h2 id=\"upgrading-from-previous-versions-telegraf-v0-13-administration-upgrading\"><a href=\"upgrading/index\">Upgrading from Previous Versions</a></h2> <h2 id=\"configuration-telegraf-v0-13-administration-configuration\"><a href=\"configuration/index\">Configuration</a></h2><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/administration/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/administration/</a>\n  </p>\n</div>\n","telegraf/v0.13/administration/differences/index":"<h1>Differences between Telegraf 0.13 and 0.12</h1>     <p>This page aims to ease the transition from Telegraf 0.12 to Telegraf 0.13. It is not intended to be a comprehensive list of the differences between the versions. See <a href=\"https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md\">Telegraf’s Changelog</a> for detailed release notes.</p> <h3 id=\"remove-tags-from-measurements-on-inputs-and-outputs-with-tagexclude-and-taginclude\">Remove tags from measurements on inputs and outputs with <code>tagexclude</code> and <code>taginclude</code>\n</h3> <p>The <code>tagexclude</code> and <code>taginclude</code> filters can be configured per input or output.</p> <p>Examples:</p> <p>Only include the <code>cpu</code> tag in the measurements for the cpu plugin:</p> <pre>[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  taginclude = [\"cpu\"]\n</pre> <p>Exclude the <code>fstype</code> tag from the measurements for the disk plugin:</p> <pre>[[inputs.disk]]\n  tagexclude = [\"fstype\"]\n</pre> <h3 id=\"new-filestat-plugin\">New filestat plugin</h3> <p>The new filestat input plugin gathers metrics about file existence, size, and other stats. See the filestat <a href=\"https://github.com/influxdata/telegraf/tree/master/plugins/inputs/filestat\">README.md</a> for more information.</p> <h3 id=\"configuration-file-environment-variable\">Configuration file environment variable</h3> <p>With Telegraf 0.13, you can set the environment variable <code>TELEGRAF_CONFIG_PATH</code> to the path of your configuration file and start the process.</p> <p>Telegraf first checks for the <code>-config</code> command line option and then for the environment variable. If you do not supply a configuration file, Telegraf uses the internal default configuration.</p> <h3 id=\"breaking-changes\">Breaking changes</h3> <p>Telegraf 0.13 makes breaking changes to the jolokia plugin, docker plugin, and the win_perf_counters plugin. Please see <a href=\"https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md\">Telegraf’s Changelog</a> for more information .</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/administration/differences/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/administration/differences/</a>\n  </p>\n</div>\n","telegraf/v0.13/administration/upgrading/index":"<h1>Upgrading from Previous Versions</h1>     <p>Users looking to upgrade from Telegraf 0.12 to 0.13 should review the Telegraf <a href=\"https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md\">Changelog</a> available on GitHub. Telegraf 0.13 makes breaking changes to the jolokia plugin, docker plugin, and the win_perf_counters plugin.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/administration/upgrading/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/administration/upgrading/</a>\n  </p>\n</div>\n","telegraf/v0.13/administration/configuration/index":"<h1>Configuration</h1>     <h2 id=\"generating-a-configuration-file\">Generating a Configuration File</h2> <p>A default Telegraf config file can be generated using the <code>-sample-config</code> flag:</p> <pre>telegraf -sample-config &gt; telegraf.conf\n</pre> <p>To generate a file with specific inputs and outputs, you can use the <code>-input-filter</code> and <code>-output-filter</code> flags:</p> <pre>telegraf -sample-config -input-filter cpu:mem:net:swap -output-filter influxdb:kafka\n</pre> <p>You can see the latest config file with all available plugins here: <a href=\"https://github.com/influxdata/telegraf/blob/master/etc/telegraf.conf\">telegraf.conf</a>.</p> <h2 id=\"environment-variables\">Environment Variables</h2> <p>Environment variables can be used anywhere in the config file, simply prepend them with <code>$</code>. For strings the variable must be within quotes (ie, <code>\"$STR_VAR\"</code>), for numbers and booleans they should be plain (ie, <code>$INT_VAR</code>, <code>$BOOL_VAR</code>)</p> <h2 id=\"global-tags-configuration\">\n<code>[global_tags]</code> Configuration</h2> <p>Global tags can be specified in the <code>[global_tags]</code> section of the config file in key=“value” format. All metrics being gathered on this host will be tagged with the tags specified here.</p> <h2 id=\"agent-configuration\">\n<code>[agent]</code> Configuration</h2> <p>Telegraf has a few options you can configure under the <code>agent</code> section of the config.</p> <ul> <li>\n<strong>interval</strong>: Default data collection interval for all inputs</li> <li>\n<strong>round_interval</strong>: Rounds collection interval to ‘interval’ ie, if interval=“10s” then always collect on :00, :10, :20, etc.</li> <li>\n<strong>metric_batch_size</strong>: Telegraf will send metrics to output in batch of at most metric_batch_size metrics.</li> <li>\n<strong>metric_buffer_limit</strong>: Telegraf will cache metric_buffer_limit metrics for each output, and will flush this buffer on a successful write. This should be a multiple of metric_batch_size and could not be less than 2 times metric_batch_size.</li> <li>\n<strong>collection_jitter</strong>: Collection jitter is used to jitter the collection by a random amount. Each plugin will sleep for a random time within jitter before collecting. This can be used to avoid many plugins querying things like sysfs at the same time, which can have a measurable effect on the system.</li> <li>\n<strong>flush_interval</strong>: Default data flushing interval for all outputs. You should not set this below interval. Maximum flush_interval will be flush_interval + flush_jitter</li> <li>\n<strong>flush_jitter</strong>: Jitter the flush interval by a random amount. This is primarily to avoid large write spikes for users running a large number of telegraf instances. ie, a jitter of 5s and flush_interval 10s means flushes will happen every 10-15s.</li> <li>\n<strong>debug</strong>: Run telegraf in debug mode.</li> <li>\n<strong>quiet</strong>: Run telegraf in quiet mode.</li> <li>\n<strong>hostname</strong>: Override default hostname, if empty use os.Hostname().</li> </ul> <h4 id=\"measurement-filtering\">Measurement Filtering</h4> <p>Filters can be configured per input or output, see below for examples.</p> <ul> <li>\n<strong>namepass</strong>: An array of strings that is used to filter metrics generated by the current input. Each string in the array is tested as a glob match against measurement names and if it matches, the field is emitted.</li> <li>\n<strong>namedrop</strong>: The inverse of pass, if a measurement name matches, it is not emitted.</li> <li>\n<strong>fieldpass</strong>: An array of strings that is used to filter metrics generated by the current input. Each string in the array is tested as a glob match against field names and if it matches, the field is emitted. fieldpass is not available for outputs.</li> <li>\n<strong>fielddrop</strong>: The inverse of pass, if a field name matches, it is not emitted. fielddrop is not available for outputs.</li> <li>\n<strong>tagpass</strong>: tag names and arrays of strings that are used to filter measurements by the current input. Each string in the array is tested as a glob match against the tag name, and if it matches the measurement is emitted.</li> <li>\n<strong>tagdrop</strong>: The inverse of tagpass. If a tag matches, the measurement is not emitted. This is tested on measurements that have passed the tagpass test.</li> <li>\n<strong>tagexclude</strong>: tagexclude can be used to exclude a tag from measurement(s). As opposed to tagdrop, which will drop an entire measurement based on it’s tags, tagexclude simply strips the given tag keys from the measurement. This can be used on inputs &amp; outputs, but it is <em>recommended</em> to be used on inputs, as it is more efficient to filter out tags at the ingestion point.</li> <li>\n<strong>taginclude</strong>: taginclude is the inverse of tagexclude. It will only include the tag keys in the final measurement.</li> </ul> <h2 id=\"input-configuration\">Input Configuration</h2> <p>Some configuration options are configurable per input:</p> <ul> <li>\n<strong>name_override</strong>: Override the base name of the measurement. (Default is the name of the input).</li> <li>\n<strong>name_prefix</strong>: Specifies a prefix to attach to the measurement name.</li> <li>\n<strong>name_suffix</strong>: Specifies a suffix to attach to the measurement name.</li> <li>\n<strong>tags</strong>: A map of tags to apply to a specific input’s measurements.</li> <li>\n<strong>interval</strong>: How often to gather this metric. Normal plugins use a single global interval, but if one particular input should be run less or more often, you can configure that here.</li> </ul> <h4 id=\"input-configuration-examples\">Input Configuration Examples</h4> <p>This is a full working config that will output CPU data to an InfluxDB instance at 192.168.59.103:8086, tagging measurements with dc=“denver-1”. It will output measurements at a 10s interval and will collect per-cpu data, dropping any fields which begin with <code>time_</code>.</p> <pre data-language=\"toml\">[global_tags]\n  dc = \"denver-1\"\n\n[agent]\n  interval = \"10s\"\n\n# OUTPUTS\n[[outputs.influxdb]]\n  url = \"http://192.168.59.103:8086\" # required.\n  database = \"telegraf\" # required.\n  precision = \"s\"\n\n# INPUTS\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  # filter all fields beginning with 'time_'\n  fielddrop = [\"time_*\"]\n</pre> <h4 id=\"input-config-tagpass-and-tagdrop\">Input Config: tagpass and tagdrop</h4> <pre data-language=\"toml\">[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  fielddrop = [\"cpu_time\"]\n  # Don't collect CPU data for cpu6 &amp; cpu7\n  [inputs.cpu.tagdrop]\n    cpu = [ \"cpu6\", \"cpu7\" ]\n\n[[inputs.disk]]\n  [inputs.disk.tagpass]\n    # tagpass conditions are OR, not AND.\n    # If the (filesystem is ext4 or xfs) OR (the path is /opt or /home)\n    # then the metric passes\n    fstype = [ \"ext4\", \"xfs\" ]\n    # Globs can also be used on the tag values\n    path = [ \"/opt\", \"/home*\" ]\n</pre> <h4 id=\"input-config-fieldpass-and-fielddrop\">Input Config: fieldpass and fielddrop</h4> <pre data-language=\"toml\"># Drop all metrics for guest &amp; steal CPU usage\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n  fielddrop = [\"usage_guest\", \"usage_steal\"]\n\n# Only store inode related metrics for disks\n[[inputs.disk]]\n  fieldpass = [\"inodes*\"]\n</pre> <h4 id=\"input-config-namepass-and-namedrop\">Input Config: namepass and namedrop</h4> <pre data-language=\"toml\"># Drop all metrics about containers for kubelet\n[[inputs.prometheus]]\n  urls = [\"http://kube-node-1:4194/metrics\"]\n  namedrop = [\"container_*\"]\n\n# Only store rest client related metrics for kubelet\n[[inputs.prometheus]]\n  urls = [\"http://kube-node-1:4194/metrics\"]\n  namepass = [\"rest_client_*\"]\n</pre> <h4 id=\"input-config-taginclude-and-tagexclude\">Input Config: taginclude and tagexclude</h4> <pre data-language=\"toml\"># Only include the \"cpu\" tag in the measurements for the cpu plugin.\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  taginclude = [\"cpu\"]\n\n# Exclude the \"fstype\" tag from the measurements for the disk plugin.\n[[inputs.disk]]\n  tagexclude = [\"fstype\"]\n</pre> <h4 id=\"input-config-prefix-suffix-and-override\">Input config: prefix, suffix, and override</h4> <p>This plugin will emit measurements with the name <code>cpu_total</code></p> <pre data-language=\"toml\">[[inputs.cpu]]\n  name_suffix = \"_total\"\n  percpu = false\n  totalcpu = true\n</pre> <p>This will emit measurements with the name <code>foobar</code></p> <pre data-language=\"toml\">[[inputs.cpu]]\n  name_override = \"foobar\"\n  percpu = false\n  totalcpu = true\n</pre> <h4 id=\"input-config-tags\">Input config: tags</h4> <p>This plugin will emit measurements with two additional tags: <code>tag1=foo</code> and <code>tag2=bar</code></p> <p>NOTE: Order matters, the <code>[inputs.cpu.tags]</code> table must be at the <em>end</em> of the plugin definition.</p> <pre data-language=\"toml\">[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n  [inputs.cpu.tags]\n    tag1 = \"foo\"\n    tag2 = \"bar\"\n</pre> <h4 id=\"multiple-inputs-of-the-same-type\">Multiple inputs of the same type</h4> <p>Additional inputs (or outputs) of the same type can be specified, just define more instances in the config file. It is highly recommended that you utilize <code>name_override</code>, <code>name_prefix</code>, or <code>name_suffix</code> config options to avoid measurement collisions:</p> <pre data-language=\"toml\">[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  name_override = \"percpu_usage\"\n  fielddrop = [\"cpu_time*\"]\n</pre> <h2 id=\"output-configuration\">Output Configuration</h2> <p>Telegraf also supports specifying multiple output sinks to send data to, configuring each output sink is different, but examples can be found by running <code>telegraf -sample-config</code>.</p> <pre data-language=\"toml\">[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf\"\n  precision = \"s\"\n  # Drop all measurements that start with \"aerospike\"\n  namedrop = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-aerospike-data\"\n  precision = \"s\"\n  # Only accept aerospike data:\n  namepass = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-cpu0-data\"\n  precision = \"s\"\n  # Only store measurements where the tag \"cpu\" matches the value \"cpu0\"\n  [outputs.influxdb.tagpass]\n    cpu = [\"cpu0\"]\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/administration/configuration/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/administration/configuration/</a>\n  </p>\n</div>\n","chronograf/v0.13/introduction/installation/index":"<h1>Installation</h1>     <p>This page provides directions for installing, starting, and configuring Chronograf.</p> <h2 id=\"requirements\">Requirements</h2> <p>Installation of the Chronograf package may require <code>root</code> or administrator privileges in order to complete successfully.</p> <h3 id=\"networking\">Networking</h3> <p>By default, Chronograf runs on <code>localhost</code> port <code>10000</code>. The port and interface can be modified through the <a href=\"../../administration/configuration/index\">configuration file</a>.</p> <h2 id=\"installation\">Installation</h2> <p>Follow the instructions in the Chronograf Downloads section on the <a href=\"https://influxdata.com/downloads\">Downloads page</a>.</p> <h3 id=\"start-the-chronograf-service\">Start the Chronograf service</h3> <h4 id=\"mac-os-x-via-homebrew\">Mac OS X (via Homebrew)</h4> <p>To run Chronograf manually, you can specify the configuration file on the command line:</p> <pre>chronograf -config=/usr/local/etc/chronograf.toml\n</pre> <p>To have launchd start homebrew/binary/chronograf at login:</p> <pre>ln -sfv /usr/local/opt/chronograf/*.plist ~/Library/LaunchAgents\n</pre> <p>Then to load homebrew/binary/chronograf now:</p> <pre>launchctl load ~/Library/LaunchAgents/homebrew.mxcl.chronograf.plist\n</pre> <h4 id=\"linux-deb-or-rpm-package\">Linux DEB or RPM package:</h4> <pre>sudo service chronograf start\n</pre> <h4 id=\"standalone-os-x-binary\">Standalone OS X binary</h4> <p>Assuming you’re working with Chronograf version 0.13, from the <code>chronograf-0.13/</code>` directory:</p> <pre>./chronograf-0.13-darwin_amd64\n</pre> <h2 id=\"configuration\">Configuration</h2> <p>See the <a href=\"../../administration/configuration/index\">configuration documentation</a> for more information.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/introduction/installation/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/introduction/installation/</a>\n  </p>\n</div>\n","chronograf/v0.13/templating/templating_a_dashboard/index":"<h1>Templating a Dashboard</h1>     <p>Chronograf’s template variables also work on dashboards. Note that only visualizations that specify the relevant <a href=\"../template_time_range/index\"><code>tmplTime()</code></a> and/or <a href=\"../template_tag_values/index\"><code>tmplTagValue()</code></a> will work with the template variables.</p> <p><em>Chronograf dashboard with template variables:</em></p> <p><img src=\"https://docs.influxdata.com/img/chronograf/v0.11/template-dashboard.gif\" alt=\"Tag template\"></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/templating/templating_a_dashboard/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/templating/templating_a_dashboard/</a>\n  </p>\n</div>\n","chronograf/v0.13/troubleshooting/index":"<h1>Troubleshooting</h1><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/troubleshooting/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/troubleshooting/</a>\n  </p>\n</div>\n","chronograf/v0.13/troubleshooting/frequently-asked-questions/index":"<h1>Frequently Asked Questions</h1>     <h3 id=\"can-i-customize-the-y-axis-range\">Can I customize the y-axis range?</h3> <p>Yes. As of version 0.11, users can customize the y-axis range in Chronograf:</p> <p><img src=\"https://docs.influxdata.com/img/chronograf/v0.11/y-axis-customization.gif\" alt=\"Y-axis customizer GIF\"></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/troubleshooting/frequently-asked-questions/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/troubleshooting/frequently-asked-questions/</a>\n  </p>\n</div>\n","chronograf/v0.13/administration/index":"<h1>Administration</h1>     <h2 id=\"differences-between-chronograf-0-13-and-0-12-chronograf-v0-13-administration-differences\"><a href=\"differences/index\">Differences between Chronograf 0.13 and 0.12</a></h2> <h2 id=\"upgrading-from-previous-versions-chronograf-v0-13-administration-upgrading\"><a href=\"upgrading/index\">Upgrading from previous versions</a></h2> <h2 id=\"configuration-chronograf-v0-13-administration-configuration\"><a href=\"configuration/index\">Configuration</a></h2><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/administration/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/administration/</a>\n  </p>\n</div>\n","telegraf/v0.13/introduction/installation/index":"<h1>Installation</h1>     <p>This page provides directions for installing, starting, and configuring Telegraf.</p> <h2 id=\"requirements\">Requirements</h2> <p>Installation of the Telegraf package may require <code>root</code> or administrator privileges in order to complete successfully.</p> <h3 id=\"networking\">Networking</h3> <p>Telegraf offers multiple <a href=\"../../services/index\">service plugins</a> that may require custom ports. All port mappings can be modified through the configuration file which is located at <code>/etc/telegraf/telegraf.conf</code> for default installations.</p> <h2 id=\"installation\">Installation</h2> <h3 id=\"ubuntu-debian\">Ubuntu &amp; Debian</h3> <p>For instructions on how to install the Debian package from a file, please see the <a href=\"https://influxdata.com/downloads/\">downloads page</a>.</p> <p>Debian and Ubuntu users can install the latest stable version of Telegraf using the <code>apt-get</code> package manager. For Ubuntu users, you can add the InfluxData repository by using the following commands:</p> <pre data-language=\"bash\">curl -sL https://repos.influxdata.com/influxdb.key | sudo apt-key add -\nsource /etc/lsb-release\necho \"deb https://repos.influxdata.com/${DISTRIB_ID,,} ${DISTRIB_CODENAME} stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\n</pre> <p>For Debian users, you can add the InfluxData repository by using the following commands:</p> <pre data-language=\"bash\">curl -sL https://repos.influxdata.com/influxdb.key | sudo apt-key add -\nsource /etc/os-release\ntest $VERSION_ID = \"7\" &amp;&amp; echo \"deb https://repos.influxdata.com/debian wheezy stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\ntest $VERSION_ID = \"8\" &amp;&amp; echo \"deb https://repos.influxdata.com/debian jessie stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\n</pre> <p>And then to install Telegraf:</p> <pre data-language=\"bash\">sudo apt-get update &amp;&amp; sudo apt-get install telegraf\n</pre> <h3 id=\"redhat-centos\">RedHat &amp; CentOS</h3> <p>For instructions on how to install the RPM package from a file, please see the <a href=\"https://influxdata.com/downloads/\">downloads page</a>.</p> <p>RedHat and CentOS users can install the latest stable version of Telegraf using the <code>yum</code> package manager:</p> <pre data-language=\"bash\">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo\n[influxdb]\nname = InfluxDB Repository - RHEL \\$releasever\nbaseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stable\nenabled = 1\ngpgcheck = 1\ngpgkey = https://repos.influxdata.com/influxdb.key\nEOF\n</pre> <p>Once repository is added to the <code>yum</code> configuration, you can install telegraf</p> <pre data-language=\"bash\">sudo yum install telegraf\n</pre> <h3 id=\"sles-opensuse\">SLES &amp; openSUSE</h3> <p>There are RPM packages provided by openSUSE Build Service for SUSE Linux users:</p> <pre data-language=\"bash\"># add go repository\nzypper ar -f obs://devel:languages:go/ go\n# install latest telegraf\nzypper in telegraf\n</pre> <h3 id=\"freebsd-pc-bsd\">FreeBSD/PC-BSD</h3> <p>Telegraf is part of the FreeBSD package system. It can be installed by running:</p> <pre data-language=\"bash\">sudo pkg install telegraf\n</pre> <p>The configuration file is located at <code>/usr/local/etc/telegraf.conf</code> with examples in <code>/usr/local/etc/telegraf.conf.sample</code>.</p> <h3 id=\"mac-os-x\">Mac OS X</h3> <p>Users of OS X 10.8 and higher can install Telegraf using the <a href=\"http://brew.sh/\">Homebrew</a> package manager. Once <code>brew</code> is installed, you can install Telegraf by running:</p> <pre data-language=\"bash\">brew update\nbrew install telegraf\n</pre> <h3 id=\"start-the-telegraf-service\">Start the Telegraf service</h3> <h4 id=\"os-x-via-homebrew\">OS X (via Homebrew)</h4> <p>To have launchd start telegraf at login:</p> <pre>ln -sfv /usr/local/opt/telegraf/*.plist ~/Library/LaunchAgents\n</pre> <p>Then to load telegraf now:</p> <pre>launchctl load ~/Library/LaunchAgents/homebrew.mxcl.telegraf.plist\n</pre> <p>Or, if you don’t want/need launchctl, you can just run:</p> <pre>telegraf -config /usr/local/etc/telegraf.conf\n</pre> <h4 id=\"sysv-systems\">sysv systems</h4> <pre>sudo service telegraf start\n</pre> <h4 id=\"systemd-systems-such-at-ubuntu-15\">systemd systems (such at Ubuntu 15+)</h4> <pre>systemctl start Telegraf\n</pre> <h2 id=\"configuration\">Configuration</h2> <h3 id=\"create-a-configuration-file-with-default-input-and-output-plugins\">Create a configuration file with default input and output plugins.</h3> <p>Every plugin will be in the file, but most will be commented.</p> <pre>telegraf -sample-config &gt; telegraf.conf\n</pre> <h3 id=\"create-a-configuration-file-with-specific-inputs-and-outputs\">Create a configuration file with specific inputs and outputs</h3> <pre>telegraf -sample-config -input-filter &lt;pluginname&gt;[:&lt;pluginname&gt;] -output-filter &lt;outputname&gt;[:&lt;outputname&gt;] &gt; telegraf.conf\n</pre> <p>For more advanced configuration details, see the <a href=\"../../administration/configuration/index\">configuration documentation</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/introduction/installation/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/introduction/installation/</a>\n  </p>\n</div>\n","telegraf/v0.13/about_the_project/index":"<h1>About the Project</h1><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/telegraf/v0.13/about_the_project/\" class=\"_attribution-link\">https://docs.influxdata.com/telegraf/v0.13/about_the_project/</a>\n  </p>\n</div>\n","influxdb/v0.13/concepts/glossary/index":"<h1>Glossary of Terms</h1>     <h2 id=\"aggregation\">aggregation</h2> <p>An InfluxQL function that returns an aggregated value across a set of points. See <a href=\"../../query_language/functions/index#aggregations\">InfluxQL Functions</a> for a complete list of the available and upcoming aggregations.</p> <p>Related entries: <a href=\"index#function\">function</a>, <a href=\"index#selector\">selector</a>, <a href=\"index#transformation\">transformation</a></p> <h2 id=\"batch\">batch</h2> <p>A collection of points in line protocol format, separated by newlines (<code>0x0A</code>). A batch of points may be submitted to the database using a single HTTP request to the write endpoint. This makes writes via the HTTP API much more performant by drastically reducing the HTTP overhead. InfluxData recommends batch sizes of 5,000-10,000 points, although different use cases may be better served by significantly smaller or larger batches.</p> <p>Related entries: <a href=\"index#line-protocol\">line protocol</a>, <a href=\"index#point\">point</a></p> <h2 id=\"continuous-query-cq\">continuous query (CQ)</h2> <p>An InfluxQL query that runs automatically and periodically within a database. Continuous queries require a function in the <code>SELECT</code> clause and must include a <code>GROUP BY time()</code> clause. See <a href=\"../../query_language/continuous_queries/index\">Continuous Queries</a>.</p> <p>Related entries: <a href=\"index#function\">function</a></p> <h2 id=\"database\">database</h2> <p>A logical container for users, retention policies, continuous queries, and time series data.</p> <p>Related entries: <a href=\"index#continuous-query-cq\">continuous query</a>, <a href=\"index#retention-policy-rp\">retention policy</a>, <a href=\"index#user\">user</a></p> <h2 id=\"duration\">duration</h2> <p>The attribute of the retention policy that determines how long InfluxDB stores data. Data older than the duration are automatically dropped from the database. See <a href=\"../../query_language/database_management/index#create-retention-policies-with-create-retention-policy\">Database Management</a> for how to set duration.</p> <p>Related entries: <a href=\"index#retention-policy-rp\">retention policy</a></p> <h2 id=\"field\">field</h2> <p>The key-value pair in InfluxDB’s data structure that records metadata and the actual data value. Fields are required in InfluxDB’s data structure and they are not indexed - queries on field values scan all points that match the specified time range and, as a result, are not performant relative to tags.</p> <p><em>Query tip:</em> Compare fields to tags; tags are indexed.</p> <p>Related entries: <a href=\"index#field-key\">field key</a>, <a href=\"index#field-set\">field set</a>, <a href=\"index#field-value\">field value</a>, <a href=\"index#tag\">tag</a></p> <h2 id=\"field-key\">field key</h2> <p>The key part of the key-value pair that makes up a field. Field keys are strings and they store metadata.</p> <p>Related entries: <a href=\"index#field\">field</a>, <a href=\"index#field-set\">field set</a>, <a href=\"index#field-value\">field value</a>, <a href=\"index#tag-key\">tag key</a></p> <h2 id=\"field-set\">field set</h2> <p>The collection of field keys and field values on a point.</p> <p>Related entries: <a href=\"index#field\">field</a>, <a href=\"index#field-key\">field key</a>, <a href=\"index#field-value\">field value</a>, <a href=\"index#point\">point</a></p> <h2 id=\"field-value\">field value</h2> <p>The value part of the key-value pair that makes up a field. Field values are the actual data; they can be strings, floats, integers, or booleans. A field value is always associated with a timestamp.</p> <p>Field values are not indexed - queries on field values scan all points that match the specified time range and, as a result, are not performant.</p> <p><em>Query tip:</em> Compare field values to tag values; tag values are indexed.</p> <p>Related entries: <a href=\"index#field\">field</a>, <a href=\"index#field-key\">field key</a>, <a href=\"index#field-set\">field set</a>, <a href=\"index#tag-value\">tag value</a>, <a href=\"index#timestamp\">timestamp</a></p> <h2 id=\"function\">function</h2> <p>InfluxQL aggregations, selectors, and transformations. See <a href=\"../../query_language/functions/index\">InfluxQL Functions</a> for a complete list of InfluxQL functions.</p> <p>Related entries: <a href=\"index#aggregation\">aggregation</a>, <a href=\"index#selector\">selector</a>, <a href=\"index#transformation\">transformation</a></p> <h2 id=\"identifier\">identifier</h2> <p>Tokens which refer to database names, retention policy names, user names, measurement names, tag keys, and field keys. See <a href=\"../../query_language/spec/index#identifiers\">Query Language Specification</a>.</p> <p>Related entries: <a href=\"index#database\">database</a>, <a href=\"index#field-key\">field key</a>, <a href=\"index#measurement\">measurement</a>, <a href=\"index#retention-policy-rp\">retention policy</a>, <a href=\"index#tag-key\">tag key</a>, <a href=\"index#user\">user</a></p> <h2 id=\"line-protocol\">line protocol</h2> <p>The text based format for writing points to InfluxDB. See <a href=\"../../write_protocols/line/index\">Line Protocol</a>.</p> <h2 id=\"measurement\">measurement</h2> <p>The part of InfluxDB’s structure that describes the data stored in the associated fields. Measurements are strings.</p> <p>Related entries: <a href=\"index#field\">field</a>, <a href=\"index#series\">series</a></p> <h2 id=\"metastore\">metastore</h2> <p>Contains internal information about the status of the system. That includes user information, database and shard metadata, and which retention policies are enabled.</p> <p>Related entries: <a href=\"index#database\">database</a>, <a href=\"index#retention-policy-rp\">retention policy</a>, <a href=\"index#user\">user</a></p> <h2 id=\"node\">node</h2> <p>An independent <code>influxd</code> process.</p> <p>Related entries: <a href=\"index#server\">server</a></p> <h2 id=\"point\">point</h2> <p>The part of InfluxDB’s data structure that consists of a single collection of fields in a series. Each point is uniquely identified by its series and timestamp.</p> <p>You cannot store more than one point with the same timestamp in the same series. Instead, when you write a new point to the same series with the same timestamp as an existing point in that series, the field set becomes the union of the old field set and the new field set, where any ties go to the new field set. For an example, see <a href=\"../../troubleshooting/frequently_encountered_issues/index#writing-duplicate-points\">Frequently Encountered Issues</a>.</p> <p>Related entries: <a href=\"index#field-set\">field set</a>, <a href=\"index#series\">series</a>, <a href=\"index#timestamp\">timestamp</a></p> <h2 id=\"points-per-second\">points per second</h2> <p>A deprecated measurement of the rate at which data are persisted to InfluxDB. The schema allows and even encourages the recording of multiple metric vales per point, rendering points per second ambiguous.</p> <p>Write speeds are generally quoted in values per second, a more precise metric.</p> <p>Related entries: <a href=\"index#point\">point</a>, <a href=\"index#schema\">schema</a>, <a href=\"index#values-per-second\">values per second</a></p> <h2 id=\"query\">query</h2> <p>An operation that retrieves data from InfluxDB. See <a href=\"../../query_language/data_exploration/index\">Data Exploration</a>, <a href=\"../../query_language/schema_exploration/index\">Schema Exploration</a>, <a href=\"../../query_language/database_management/index\">Database Management</a>.</p> <h2 id=\"replication-factor\">replication factor</h2> <p>The attribute of the retention policy that determines how many copies of the data are stored in the cluster. InfluxDB replicates data across <code>N</code> data nodes, where <code>N</code> is the replication factor.</p> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <p>Related entries: <a href=\"index#duration\">duration</a>, <a href=\"index#node\">node</a>, <a href=\"index#retention-policy-rp\">retention policy</a></p> <h2 id=\"retention-policy-rp\">retention policy (RP)</h2> <p>The part of InfluxDB’s data structure that describes for how long InfluxDB keeps data (duration), how many copies of those data are stored in the cluster (replication factor), and the time range covered by shard groups (shard group duration). RPs are unique per database and along with the measurement and tag set define a series.</p> <p>When you create a database, InfluxDB automatically creates a retention policy called <code>default</code> with an infinite duration, a replication factor set to one, and a shard group duration set to seven days. See <a href=\"../../query_language/database_management/index#retention-policy-management\">Database Management</a> for retention policy management.</p> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <p>Related entries: <a href=\"index#duration\">duration</a>, <a href=\"index#measurement\">measurement</a>, <a href=\"index#replication-factor\">replication factor</a>, <a href=\"index#series\">series</a>, <a href=\"index#tag-set\">tag set</a></p> <h2 id=\"schema\">schema</h2> <p>How the data are organized in InfluxDB. The fundamentals of the InfluxDB schema are databases, retention policies, series, measurements, tag keys, tag values, and field keys. See <a href=\"../schema_and_data_layout/index\">Schema Design</a> for more information.</p> <p>Related entries: <a href=\"index#database\">database</a>, <a href=\"index#field-key\">field key</a>, <a href=\"index#measurement\">measurement</a>, <a href=\"index#retention-policy-rp\">retention policy</a>, <a href=\"index#series\">series</a>, <a href=\"index#tag-key\">tag key</a>, <a href=\"index#tag-value\">tag value</a></p> <h2 id=\"selector\">selector</h2> <p>An InfluxQL function that returns a single point from the range of specified points. See <a href=\"../../query_language/functions/index#selectors\">InfluxQL Functions</a> for a complete list of the available and upcoming selectors.</p> <p>Related entries: <a href=\"index#aggregation\">aggregation</a>, <a href=\"index#function\">function</a>, <a href=\"index#transformation\">transformation</a></p> <h2 id=\"series\">series</h2> <p>The collection of data in InfluxDB’s data structure that share a measurement, tag set, and retention policy.</p> <blockquote> <p><strong>Note:</strong> The field set is not part of the series identification!</p> </blockquote> <p>Related entries: <a href=\"index#field-set\">field set</a>, <a href=\"index#measurement\">measurement</a>, <a href=\"index#retention-policy-rp\">retention policy</a>, <a href=\"index#tag-set\">tag set</a></p> <h2 id=\"series-cardinality\">series cardinality</h2> <p>The count of all combinations of measurements and tags within a given data set. For example, take measurement <code>mem_available</code> with tags <code>host</code> and <code>total_mem</code>. If there are 35 different <code>host</code>s and 15 different <code>total_mem</code> values then series cardinality for that measurement is <code>35 * 15 = 525</code>. To calculate series cardinality for a database add the series cardinalities for the individual measurements together.</p> <p>Related entries: <a href=\"index#tag-set\">tag set</a>, <a href=\"index#measurement\">measurement</a>, <a href=\"index#tag-key\">tag key</a></p> <h2 id=\"server\">server</h2> <p>A machine, virtual or physical, that is running InfluxDB. There should only be one InfluxDB process per server.</p> <p>Related entries: <a href=\"index#node\">node</a></p> <h2 id=\"shard\">shard</h2> <p>A shard is represented by a TSM file on disk. Every shard belongs to one and only one shard group. Multiple shards may exist in a single shard group. Each shard contains a specific set of series. All points falling on a given series in the relevant time range will be stored in the same shard on disk.</p> <p>Related entries: <a href=\"index#series\">series</a>, <a href=\"index#shard-group\">shard group</a>, <a href=\"index#tsm\">tsm</a></p> <h2 id=\"shard-group\">shard group</h2> <p>Data on disk are organized into shard groups, logical containers organized by time and retention policy. Every retention policy that contains data has at least one associated shard group. A given shard group contains all data from that retention policy for a certain time interval. The specific interval is determined by the <code>SHARD DURATION</code> of the retention policy. See <a href=\"../../query_language/database_management/index#retention-policy-management\">Retention Policy management</a> for more information.</p> <p>For example, given a retention policy with <code>SHARD DURATION</code> set to <code>1w</code>, each shard group will span a single week and contain all points with timestamps in that week.</p> <p>Shard groups are logical containers for shards, which are actual files on disk.</p> <p>Related entries: <a href=\"index#database\">database</a>, <a href=\"index#retention-policy\">retention policy</a>, <a href=\"index#series\">series</a>, <a href=\"index#shard\">shard</a></p> <h2 id=\"tag\">tag</h2> <p>The key-value pair in InfluxDB’s data structure that records metadata. Tags are an optional part of InfluxDB’s data structure but they are useful for storing commonly-queried metadata; tags are indexed so queries on tags are performant. <em>Query tip:</em> Compare tags to fields; fields are not indexed.</p> <p>Related entries: <a href=\"index#field\">field</a>, <a href=\"index#tag-key\">tag key</a>, <a href=\"index#tag-set\">tag set</a>, <a href=\"index#tag-value\">tag value</a></p> <h2 id=\"tag-key\">tag key</h2> <p>The key part of the key-value pair that makes up a tag. Tag keys are strings and they store metadata. Tag keys are indexed so queries on tag keys are performant.</p> <p><em>Query tip:</em> Compare tag keys to field keys; field keys are not indexed.</p> <p>Related entries: <a href=\"index#field-key\">field key</a>, <a href=\"index#tag\">tag</a>, <a href=\"index#tag-set\">tag set</a>, <a href=\"index#tag-value\">tag value</a></p> <h2 id=\"tag-set\">tag set</h2> <p>The collection of tag keys and tag values on a point.</p> <p>Related entries: <a href=\"index#point\">point</a>, <a href=\"index#series\">series</a>, <a href=\"index#tag\">tag</a>, <a href=\"index#tag-key\">tag key</a>, <a href=\"index#tag-value\">tag value</a></p> <h2 id=\"tag-value\">tag value</h2> <p>The value part of the key-value pair that makes up a tag. Tag values are strings and they store metadata. Tag values are indexed so queries on tag values are performant.</p> <p>Related entries: <a href=\"index#tag\">tag</a>, <a href=\"index#tag-key\">tag key</a>, <a href=\"index#tag-set\">tag set</a></p> <h2 id=\"timestamp\">timestamp</h2> <p>The date and time associated with a point. All time in InfluxDB is UTC.</p> <p>For how to specify time when writing data, see <a href=\"../../write_protocols/write_syntax/index\">Write Syntax</a>. For how to specify time when querying data, see <a href=\"../../query_language/data_exploration/index#time-syntax-in-queries\">Data Exploration</a>.</p> <p>Related entries: <a href=\"index#point\">point</a></p> <h2 id=\"transformation\">transformation</h2> <p>An InfluxQL function that returns a value or a set of values calculated from specified points, but does not return an aggregated value across those points. See <a href=\"../../query_language/functions/index#transformations\">InfluxQL Functions</a> for a complete list of the available and upcoming aggregations.</p> <p>Related entries: <a href=\"index#aggregation\">aggregation</a>, <a href=\"index#function\">function</a>, <a href=\"index#selector\">selector</a></p> <h2 id=\"tsm-time-structured-merge-tree\">tsm (Time Structured Merge tree)</h2> <p>The purpose-built data storage format for InfluxDB. TSM allows for greater compaction and higher write and read throughput than existing B+ or LSM tree implementations. See <a href=\"http://docs.influxdata.com/influxdb/v0.13/concepts/storage_engine/\">Storage Engine</a> for more.</p> <h2 id=\"user\">user</h2> <p>There are two kinds of users in InfluxDB:</p> <ul> <li>\n<em>Admin users</em> have <code>READ</code> and <code>WRITE</code> access to all databases and full access to administrative queries and user management commands.</li> <li>\n<em>Non-admin users</em> have <code>READ</code>, <code>WRITE</code>, or <code>ALL</code> (both <code>READ</code> and <code>WRITE</code>) access per database.</li> </ul> <p>When authentication is enabled, InfluxDB only executes HTTP requests that are sent with a valid username and password. See <a href=\"../../administration/authentication_and_authorization/index\">Authentication and Authorization</a>.</p> <h2 id=\"values-per-second\">values per second</h2> <p>The preferred measurement of the rate at which data are persisted to InfluxDB. Write speeds are generally quoted in values per second.</p> <p>To calculate the values per second rate, multiply the number of points written per second by the number of values stored per point. For example, if the points have four fields each, and a batch of 5000 points is written 10 times per second, then the values per second rate is <code>4 field values per point * 5000 points per batch * 10 batches per second = 200,000 values per second</code>.</p> <p>Related entries: <a href=\"index#batch\">batch</a>, <a href=\"index#field\">field</a>, <a href=\"index#point\">point</a>, <a href=\"index#points-per-second\">points per second</a></p> <h2 id=\"wal-write-ahead-log\">wal (Write Ahead Log)</h2> <p>The temporary cache for recently written points. To reduce the frequency with which the permanent storage files are accessed, InfluxDB caches new points in the WAL until their total size or age triggers a flush to more permanent storage. This allows for efficient batching of the writes into the TSM.</p> <p>Points in the WAL can be queried, and they persist through a system reboot. On process start, all points in the WAL must be flushed before the system accepts new writes.</p> <p>Related entries: <a href=\"index#tsm\">tsm</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/glossary/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/concepts/glossary/</a>\n  </p>\n</div>\n","chronograf/v0.13/administration/differences/index":"<h1>Differences between Chronograf 0.13 and 0.12</h1>     <h3 id=\"upgrades-to-the-visualizations-page\">Upgrades to the <code>VISUALIZATIONS</code> page</h3> <p>In Chronograf 0.13, we’ve updated the <code>VISUALIZATIONS</code> page. Instead of 0.12’s visualization thumbnails, Chronograf 0.13 improves performance by displaying the only the titles of the visualizations and their associated queries.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/administration/differences/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/administration/differences/</a>\n  </p>\n</div>\n","chronograf/v0.13/administration/upgrading/index":"<h1>Upgrading from Previous Versions</h1>     <p>Users looking to upgrade from versions prior to 0.10 to version 0.13 need to take a few additional steps.</p> <h3 id=\"for-users-who-wish-to-maintain-their-dashboards-and-visualizations\">For users who wish to maintain their dashboards and visualizations</h3> <ol> <li>\n<a href=\"https://influxdata.com/downloads/\">Install</a> Chronograf 0.10 to upgrade the <code>chronograf.db</code> directory</li> <li>\n<a href=\"https://influxdata.com/downloads/\">Install</a> Chronograf 0.13</li> </ol> <h3 id=\"for-users-with-no-attachment-to-their-dashboards-and-visualizations\">For users with no attachment to their dashboards and visualizations</h3> <ol> <li>Remove the <code>chronograf.db</code> directory</li> <li>\n<a href=\"https://influxdata.com/downloads/\">Install</a> Chronograf 0.13</li> </ol> <blockquote> <p><strong>Note:</strong> Chronograf 0.11 made several changes to take into account the <a href=\"https://github.com/influxdata/influxdb/blob/master/CHANGELOG.md\">breaking API changes</a> released with InfluxDB 0.11. As a result, we do not recommend using Chronograf 0.13 with InfluxDB versions prior to 0.11. In general, we recommend maintaining version parity across the TICK stack.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/administration/upgrading/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/administration/upgrading/</a>\n  </p>\n</div>\n","chronograf/v0.13/administration/configuration/index":"<h1>Configuration</h1>     <h3 id=\"configuration-file-location-by-installation-type\">Configuration file location by installation type</h3> <blockquote> <ul> <li>Debian or RPM package: <code>/opt/chronograf/config.toml</code>\n</li> <li>OS X via Homebrew: <code>/usr/local/etc/chronograf.toml</code>\n</li> <li>Standalone OS X binary: <code>chronograf-0.x.x/chronograf.toml</code>\n</li> </ul> </blockquote> <h3 id=\"chronograf-configuration-file\">Chronograf configuration file</h3> <pre># Chronograf configuration\n\n# TCP address that Chronograf should bind to.\n# Bind to localhost by default, so that Chronograf should be inaccessible from the public internet.\n# If you do want Chronograf to be accessible to the public internet on port 10000, use the next line:\n# Bind = \"0.0.0.0:10000\" # &lt;- This will expose Chronograf to the public internet. Use with caution!\n# Can be overridden with environment variable CHRONOGRAF_BIND.\nBind = \"127.0.0.1:10000\"\n\n# Path to local database file to use or create for storing Chronograf application data.\n# Can be overridden with environment variable CHRONOGRAF_LOCAL_DATABASE.\nLocalDatabase = \"/opt/chronograf/chronograf.db\"\n\n# Maximum response size in bytes, for queries that pass through Chronograf.\n# Setting this to a reasonable value will ensure that your browser does not crash\n# if a query results in an excessively large response.\n# If you do have a query that is too large, you will see an error message about the\n# query response being too large.\n# Can be overridden with environment variable CHRONOGRAF_QUERY_RESPONSE_BYTES_LIMIT.\nQueryResponseBytesLimit = 2500000\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/chronograf/v0.13/administration/configuration/\" class=\"_attribution-link\">https://docs.influxdata.com/chronograf/v0.13/administration/configuration/</a>\n  </p>\n</div>\n","influxdb/v0.13/high_availability/relay/index":"<h1>Relay</h1>     <p>Relay adds a basic high availability layer to InfluxDB. With the right architecture and disaster recovery processes, this achieves a highly available setup.</p> <p>Please see the <a href=\"https://github.com/influxdata/influxdb-relay/blob/master/README.md\">README.md</a> on GitHub for more information.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/high_availability/relay/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/high_availability/relay/</a>\n  </p>\n</div>\n","influxdb/v0.13/introduction/installation/index":"<h1>Installation</h1>     <p>This page provides directions for installing, starting, and configuring InfluxDB.</p> <h2 id=\"requirements\">Requirements</h2> <p>Installation of the InfluxDB package may require <code>root</code> or administrator privileges in order to complete successfully.</p> <h3 id=\"networking\">Networking</h3> <p>By default, InfluxDB uses the following network ports:</p> <ul> <li>TCP port <code>8083</code> is used for InfluxDB’s <a href=\"../../tools/web_admin/index\">Admin panel</a>\n</li> <li>TCP port <code>8086</code> is used for client-server communication over InfluxDB’s HTTP API</li> </ul> <p>In addition to the ports above, InfluxDB also offers multiple plugins that may require custom ports. All port mappings can be modified through the <a href=\"../../administration/config/index\">configuration file</a>, which is located at <code>/etc/influxdb/influxdb.conf</code> for default installations.</p> <h2 id=\"installation\">Installation</h2> <p>For users who don’t want to install any software and are ready to use InfluxDB, you may want to check out our <a href=\"https://cloud.influxdata.com\">managed hosted InfluxDB offering</a>.</p> <h3 id=\"ubuntu-debian\">Ubuntu &amp; Debian</h3> <p>For instructions on how to install the Debian package from a file, please see the <a href=\"https://influxdata.com/downloads/\">downloads page</a>. Debian and Ubuntu users can install the latest stable version of InfluxDB using the <code>apt-get</code> package manager.</p> <p>For Ubuntu users, you can add the InfluxData repository by using the following commands:</p> <pre data-language=\"bash\">curl -sL https://repos.influxdata.com/influxdb.key | sudo apt-key add -\nsource /etc/lsb-release\necho \"deb https://repos.influxdata.com/${DISTRIB_ID,,} ${DISTRIB_CODENAME} stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\n</pre> <p>For Debian users, you can add the InfluxData repository by using the following commands:</p> <pre data-language=\"bash\">curl -sL https://repos.influxdata.com/influxdb.key | sudo apt-key add -\nsource /etc/os-release\ntest $VERSION_ID = \"7\" &amp;&amp; echo \"deb https://repos.influxdata.com/debian wheezy stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\ntest $VERSION_ID = \"8\" &amp;&amp; echo \"deb https://repos.influxdata.com/debian jessie stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\n</pre> <p>And then to install and start the InfluxDB service:</p> <pre data-language=\"bash\">sudo apt-get update &amp;&amp; sudo apt-get install influxdb\nsudo service influxdb start\n</pre> <p>Or if your operating system is using systemd (Ubuntu 15.04+, Debian 8+):</p> <pre data-language=\"bash\">sudo apt-get update &amp;&amp; sudo apt-get install influxdb\nsudo systemctl start influxdb\n</pre> <h3 id=\"redhat-centos\">RedHat &amp; CentOS</h3> <p>For instructions on how to install the RPM package from a file, please see the <a href=\"https://influxdata.com/downloads/\">downloads page</a>.</p> <p>RedHat and CentOS users can install the latest stable version of InfluxDB using the <code>yum</code> package manager:</p> <pre data-language=\"bash\">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo\n[influxdb]\nname = InfluxDB Repository - RHEL \\$releasever\nbaseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stable\nenabled = 1\ngpgcheck = 1\ngpgkey = https://repos.influxdata.com/influxdb.key\nEOF\n</pre> <p>Once repository is added to the <code>yum</code> configuration, you can install and start the InfluxDB service by running:</p> <pre data-language=\"bash\">sudo yum install influxdb\nsudo service influxdb start\n</pre> <p>Or if your operating system is using systemd (CentOS 7+, RHEL 7+):</p> <pre data-language=\"bash\">sudo yum install influxdb\nsudo systemctl start influxdb\n</pre> <h3 id=\"sles-opensuse\">SLES &amp; openSUSE</h3> <p>There are RPM packages provided by openSUSE Build Service for SUSE Linux users:</p> <pre data-language=\"bash\"># add go repository\nzypper ar -f obs://devel:languages:go/ go\n# install latest influxdb\nzypper in influxdb\n</pre> <h3 id=\"freebsd-pc-bsd\">FreeBSD/PC-BSD</h3> <p>InfluxDB is part of the FreeBSD package system. It can be installed by running:</p> <pre data-language=\"bash\">sudo pkg install influxdb\n</pre> <p>The configuration file is located at <code>/usr/local/etc/influxd.conf</code> with examples in <code>/usr/local/etc/influxd.conf.sample</code>.</p> <p>Start the backend by executing:</p> <pre data-language=\"bash\">sudo service influxd onestart\n</pre> <p>To have InfluxDB start at system boot, add <code>influxd_enable=\"YES\"</code> to <code>/etc/rc.conf</code>.</p> <h3 id=\"mac-os-x\">Mac OS X</h3> <p>Users of OS X 10.8 and higher can install InfluxDB using the <a href=\"http://brew.sh/\">Homebrew</a> package manager. Once <code>brew</code> is installed, you can install InfluxDB by running:</p> <pre data-language=\"bash\">brew update\nbrew install influxdb\n</pre> <p>To have <code>launchd</code> start InfluxDB at login, run:</p> <pre data-language=\"bash\">ln -sfv /usr/local/opt/influxdb/*.plist ~/Library/LaunchAgents\n</pre> <p>And then to start InfluxDB now, run:</p> <pre data-language=\"bash\">launchctl load ~/Library/LaunchAgents/homebrew.mxcl.influxdb.plist\n</pre> <p>Or, if you don’t want/need launchctl, in a separate terminal window you can just run:</p> <pre data-language=\"bash\">influxd -config /usr/local/etc/influxdb.conf\n</pre> <h2 id=\"configuration\">Configuration</h2> <p>For non-packaged installations, it is a best practice to generate a new configuration for each upgrade to ensure you have the latest features and settings. Any changes made in the old file will need to be manually ported to the newly generated file. Packaged installations will come with a configuration pre-installed, so this step may not be needed if you installed InfluxDB using a package manager (though it is handy to know either way).</p> <blockquote> <p>Note: Newly generated configuration files have no knowledge of any local customizations or settings. Please make sure to double-check any configuration changes prior to deploying them.</p> </blockquote> <p>To generate a new configuration file run the following command and edit the <code>influxdb.generated.conf</code> file to have the desired configuration settings:</p> <pre data-language=\"bash\">influxd config &gt; influxdb.generated.conf\n</pre> <p>There are two ways to launch InfluxDB with your configuration file:</p> <ul> <li>\n<p>Point the process to the correct configuration file by using the <code>-config</code> option:</p> <pre data-language=\"bash\">influxd -config influxdb.generated.conf\n</pre>\n</li> <li>\n<p>Set the environment variable <code>INFLUXDB_CONFIG_PATH</code> to the path of your configuration file and start the process. For example:</p> <pre>echo $INFLUXDB_CONFIG_PATH\n/root/influxdb.generated.conf\n\n\ninfluxd\n</pre>\n</li> </ul> <p>InfluxDB first checks for the <code>-config</code> option and then for the environment variable. If you do not supply a configuration file, InfluxDB uses an internal default configuration (equivalent to the output of <code>influxd config</code>).</p> <blockquote> <p>Note: The <code>influxd</code> command has two similarly named flags. The <code>config</code> flag prints a generated default configuration file to STDOUT but does not launch the <code>influxd</code> process. The <code>-config</code> flag takes a single argument, which is the path to the InfluxDB configuration file to use when launching the process.</p> </blockquote> <p>The <code>config</code> and <code>-config</code> flags can be combined to output the union of the internal default configuration and the configuration file passed to <code>-config</code>. The options specified in the configuration file will overwrite any internally generated configuration.</p> <pre data-language=\"bash\">influxd config -config /etc/influxdb/influxdb.partial.conf\n</pre> <p>The output will show every option configured in the <code>influxdb.partial.conf</code> file and will substitute internal defaults for any configuration options not specified in that file.</p> <p>The example configuration file shipped with the installer is for information only. It is an identical file to the internally generated configuration except that the example file has comments.</p> <h2 id=\"hosting-on-aws\">Hosting on AWS</h2> <h3 id=\"hardware\">Hardware</h3> <p>We recommend using two SSD volumes. One for the <code>influxdb/wal</code> and one for the <code>influxdb/data</code>. Depending on your load each volume should have around 1k-3k provisioned IOPS. The <code>influxdb/data</code> volume should have more disk space with lower IOPS and the <code>influxdb/wal</code> volume should have less disk space with higher IOPS.</p> <p>Each machine should have a minimum of 8G RAM.</p> <p>We’ve seen the best performance with the C3 class of machines.</p> <h3 id=\"configuring-the-instance\">Configuring the Instance</h3> <p>This example assumes that you are using two SSD volumes and that you have mounted them appropriately. This example also assumes that each of those volumes is mounted at <code>/mnt/influx</code> and <code>/mnt/db</code>. For more information on how to do that see the Amazon documentation on how to <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-add-volume-to-instance.html\">Add a Volume to Your Instance</a>.</p> <h3 id=\"config-file\">Config File</h3> <p>You’ll have to update the config file appropriately for each InfluxDB instance you have.</p> <pre>...\n\n[meta]\n  dir = \"/mnt/db/meta\"\n  ...\n\n...\n\n[data]\n  dir = \"/mnt/db/data\"\n  ...\nwal-dir = \"/mnt/influx/wal\"\n  ...\n\n...\n\n[hinted-handoff]\n    ...\ndir = \"/mnt/db/hh\"\n    ...\n</pre> <h3 id=\"permissions\">Permissions</h3> <p>When using non-standard directories for InfluxDB data and configurations, also be sure to set filesystem permissions correctly:</p> <pre data-language=\"bash\">chown influxdb:influxdb /mnt/influx\nchown influxdb:influxdb /mnt/db\n</pre> <h2 id=\"nightly-and-development-versions\">Nightly and Development Versions</h2> <p>Nightly packages are available for Linux through the InfluxData package repository by using the <code>nightly</code> channel. Other package options can be found on the <a href=\"https://influxdata.com/downloads/\">downloads page</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/introduction/installation/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/introduction/installation/</a>\n  </p>\n</div>\n","influxdb/v0.13/guides/index":"<h1>Guides</h1>     <h2 id=\"writing-data-influxdb-v0-13-guides-writing-data\"><a href=\"writing_data/index\">Writing Data</a></h2> <h2 id=\"querying-data-influxdb-v0-13-guides-querying-data\"><a href=\"querying_data/index\">Querying Data</a></h2> <h2 id=\"downsampling-and-data-retention-influxdb-v0-13-guides-downsampling-and-retention\"><a href=\"downsampling_and_retention/index\">Downsampling and Data Retention</a></h2> <h2 id=\"hardware-sizing-guidelines-http-localhost-1313-influxdb-v0-13-guides-hardware-sizing\"><a href=\"http://localhost:1313/influxdb/v0.13/guides/hardware_sizing/\">Hardware Sizing Guidelines</a></h2><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/guides/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/guides/</a>\n  </p>\n</div>\n","influxdb/v0.13/guides/querying_data/index":"<h1>Querying Data</h1>     <h2 id=\"querying-data-using-the-http-api\">Querying data using the HTTP API</h2> <p>The HTTP API is the primary means for querying data in InfluxDB (see the <a href=\"../../tools/shell/index\">command line interface</a> and <a href=\"../../clients/api/index\">client libraries</a> for alternative ways to query the database).</p> <p>To perform a query send a <code>GET</code> request to the <code>/query</code> endpoint, set the URL parameter <code>db</code> as the target database, and set the URL parameter <code>q</code> as your query. The example below uses the HTTP API to query the same database that you encountered in <a href=\"../writing_data/index\">Writing Data</a>. <br></p> <pre data-language=\"bash\">curl -GET 'http://localhost:8086/query?pretty=true' --data-urlencode \"db=mydb\" --data-urlencode \"q=SELECT value FROM cpu_load_short WHERE region='us-west'\"\n</pre> <p>InfluxDB returns JSON. The results of your query appear in the <code>\"results\"</code> array. If an error occurs, InfluxDB sets an <code>\"error\"</code> key with an explanation of the error. <br></p> <pre data-language=\"json\">{\n    \"results\": [\n        {\n            \"series\": [\n                {\n                    \"name\": \"cpu_load_short\",\n                    \"columns\": [\n                        \"time\",\n                        \"value\"\n                    ],\n                    \"values\": [\n                        [\n                            \"2015-01-29T21:55:43.702900257Z\",\n                            0.55\n                        ],\n                        [\n                            \"2015-01-29T21:55:43.702900257Z\",\n                            23422\n                        ],\n                        [\n                            \"2015-06-11T20:46:02Z\",\n                            0.64\n                        ]\n                    ]\n                }\n            ]\n        }\n    ]\n}\n</pre> <blockquote> <p><strong>Note:</strong> Appending <code>pretty=true</code> to the URL enables pretty-printed JSON output. While this is useful for debugging or when querying directly with tools like <code>curl</code>, it is not recommended for production use as it consumes unnecessary network bandwidth.</p> </blockquote> <h3 id=\"multiple-queries\">Multiple queries</h3>  <p>Send multiple queries to InfluxDB in a single API call. Simply delimit each query using a semicolon, for example:<br> <br></p> <pre data-language=\"bash\">curl -G 'http://localhost:8086/query?pretty=true' --data-urlencode \"db=mydb\" --data-urlencode \"q=SELECT value FROM cpu_load_short WHERE region='us-west';SELECT count(value) FROM cpu_load_short WHERE region='us-west'\"\n</pre> <p>returns:<br> <br></p> <pre data-language=\"json\">{\n    \"results\": [\n        {\n            \"series\": [\n                {\n                    \"name\": \"cpu_load_short\",\n                    \"columns\": [\n                        \"time\",\n                        \"value\"\n                    ],\n                    \"values\": [\n                        [\n                            \"2015-01-29T21:55:43.702900257Z\",\n                            0.55\n                        ],\n                        [\n                            \"2015-01-29T21:55:43.702900257Z\",\n                            23422\n                        ],\n                        [\n                            \"2015-06-11T20:46:02Z\",\n                            0.64\n                        ]\n                    ]\n                }\n            ]\n        },\n        {\n            \"series\": [\n                {\n                    \"name\": \"cpu_load_short\",\n                    \"columns\": [\n                        \"time\",\n                        \"count\"\n                    ],\n                    \"values\": [\n                        [\n                            \"1970-01-01T00:00:00Z\",\n                            3\n                        ]\n                    ]\n                }\n            ]\n        }\n    ]\n}\n</pre> <h3 id=\"other-options-when-querying-data\">Other options when querying data</h3>  <h4 id=\"timestamp-format\">Timestamp Format</h4> <p>Everything in InfluxDB is stored and reported in UTC. By default, timestamps are returned in RFC3339 UTC and have nanosecond precision, for example <code>2015-08-04T19:05:14.318570484Z</code>. If you want timestamps in Unix epoch format include in your request the query string parameter <code>epoch</code> where <code>epoch=[h,m,s,ms,u,ns]</code>. For example, get epoch in seconds with:<br> <br></p> <pre data-language=\"bash\">curl -G 'http://localhost:8086/query' --data-urlencode \"db=mydb\" --data-urlencode \"epoch=s\" --data-urlencode \"q=SELECT value FROM cpu_load_short WHERE region='us-west'\"\n</pre> <h4 id=\"authentication\">Authentication</h4> <p>Authentication in InfluxDB is disabled by default. See <a href=\"../../administration/authentication_and_authorization/index\">Authentication and Authorization</a> for how to enable and set up authentication.</p> <h4 id=\"maximum-row-limit\">Maximum Row Limit</h4> <p>InfluxDB will limit the maximum number of returned results to prevent itself from running out of memory while it aggregates the results. This is set to 10,000 by default and can be configured by modifying <code>max-row-limit</code> in the <code>http</code> section of the configuration file.</p> <p>The maximum row limit only applies to non-chunked queries. Chunked queries can return an unlimited number of points.</p> <h4 id=\"chunking\">Chunking</h4> <p>Chunking can be used to return results in streamed batches rather than as a single response by setting the query string parameter <code>chunked=true</code>. Responses will be chunked by series or by every 10,000 points, whichever occurs first. To change the maximum chunk size to a different value, set the query string parameter <code>chunk_size</code> to a different value. For example, get your results in batches of 20,000 points with:<br> <br></p> <pre data-language=\"bash\">curl -G 'http://localhost:8086/query' --data-urlencode \"db=deluge\" --data-urlencode=\"chunked=true\" --data-urlencode \"chunk_size=20000\" --data-urlencode \"q=SELECT * FROM liters\"\n</pre> <h3 id=\"influxql\">InfluxQL</h3>  <p>Now that you know how to query data, check out the <a href=\"../../query_language/data_exploration/index\">Data Exploration page</a> to get acquainted with InfluxQL.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/guides/querying_data/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/guides/querying_data/</a>\n  </p>\n</div>\n","influxdb/v0.13/guides/downsampling_and_retention/index":"<h1>Downsampling and Data Retention</h1>     <p>InfluxDB can handle hundreds of thousands of data points per second. Working with that much data over a long period of time can create storage concerns. A natural solution is to downsample the data; keep the high precision raw data for only a limited time, and store the lower precision, summarized data for much longer or forever.</p> <p>This guide shows how to combine two InfluxDB features – retention policies and continuous queries – to automatically downsample and expire data.</p> <h2 id=\"retention-policies\">Retention Policies</h2> <h3 id=\"definition\">Definition</h3> <p>A retention policy (RP) is the part of InfluxDB’s data structure that describes for how long InfluxDB keeps data (duration) and how many copies of those data are stored in the cluster (replication factor). A database can have several RPs and RPs are unique per database.</p> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <h3 id=\"purpose\">Purpose</h3> <p>In general, InfluxDB wasn’t built to process deletes. One of the fundamental assumptions in its architecture is that deletes are infrequent and need not be highly performant. However, InfluxDB recognizes the necessity of purging data that have outlived their usefulness - that is the purpose of RPs.</p> <h3 id=\"working-with-rps\">Working with RPs</h3> <p>When you create a database, InfluxDB automatically creates an RP called <code>default</code> with an infinite duration and a replication factor set to one. <code>default</code> also serves as the <code>DEFAULT</code> RP; if you do not supply an explicit RP when you write a point to the database, the data are subject to the <code>DEFAULT</code> RP.</p> <p>InfluxDB automatically queries from and writes to the <code>DEFAULT</code> RP on a database. To query from or write to a different RP, you must fully qualify the measurement, that is, specify the database and retention policy with the measurement name: <code>&lt;database_name&gt;.\"&lt;retention_policy&gt;\".&lt;measurement_name&gt;</code>.</p> <p>You can also create, alter, and delete you own RPs, and you can change the database’s <code>DEFAULT</code> RP. See <a href=\"../../query_language/database_management/index#retention-policy-management\">Database Management</a> for more on RP management.</p> <blockquote> <p><strong>Clarifying</strong> <code>default</code> <strong>vs.</strong> <code>DEFAULT</code></p> <p><code>default</code>: The name of the RP that InfluxDB automatically generates when you create a new database. It has an infinite duration and a replication factor set to one. It is initially the <code>DEFAULT</code> RP as well, but that can be altered.</p> <p><code>DEFAULT</code>: The RP that InfluxDB writes to if you do not supply an explicit RP in the write.</p> </blockquote> <h2 id=\"continuous-queries\">Continuous Queries</h2> <h3 id=\"definition-1\">Definition</h3> <p>A continuous query (CQ) is an InfluxQL query that runs automatically and periodically within a database. CQs require a function in the <code>SELECT</code> clause and must include a <code>GROUP BY time()</code> clause. InfluxDB stores the results of the CQ in a specified measurement.</p> <h3 id=\"purpose-1\">Purpose</h3> <p>CQs are optimal for regularly downsampling data - once you implement the CQ, InfluxDB automatically and periodically runs the query, and, instead of simply returning the results like a normal query, InfluxDB stores the results of a CQ in a measurement for future use.</p> <h3 id=\"working-with-cqs\">Working with CQs</h3> <p>The section below offers a very brief introduction to creating CQs. See <a href=\"../../query_language/continuous_queries/index\">Continuous Queries</a> for a detailed discussion on how to create and manage CQs.</p> <h2 id=\"combining-rps-and-cqs-a-casestudy\">Combining RPs and CQs - a casestudy</h2> <p>We have real-time data that track the number of food orders to a restaurant via phone and via website at 10 second intervals. In the long run, we’re only interested in the average number of orders by phone and by website at 30 minute intervals. In the next steps, we use RPs and CQs to make InfluxDB:</p> <ul> <li>automatically delete the raw 10 second level data that are older than two hours</li> <li>automatically aggregate the 10 second level data to 30 minute level data</li> <li>keep the 30 minute level data forever</li> </ul> <p>The following steps work with a fictional <a href=\"../../concepts/glossary/index#database\">database</a> called <code>food_data</code> and the <a href=\"../../concepts/glossary/index#measurement\">measurement</a> <code>orders</code>. <code>orders</code> has two <a href=\"../../concepts/glossary/index#field\">fields</a>, <code>phone</code> and <code>website</code>, which store the number of orders that arrive via each channel every 10 seconds.</p> <h3 id=\"prepare-the-database\">Prepare the database</h3> <p>Before writing the data to the database <code>food_data</code>, we perform the following steps.</p> <blockquote> <p><strong>Note:</strong> We do this before inserting any data because InfluxDB only performs CQs on new data, that is, data with timestamps that occur after the time at which we create the CQ.</p> </blockquote> <h4 id=\"create-a-new-default-rp\">Create a new <code>DEFAULT</code> RP</h4> <p>When we initially <a href=\"../../query_language/database_management/index#create-a-database-with-create-database\">created the database</a> <code>food_data</code>, InfluxDB automatically generated an RP called <code>default</code> with an infinite duration and a replication factor set to one. <code>default</code> is also the <code>DEFAULT</code> RP for <code>food_data</code>; if we do not supply an explicit RP when we write a point to the database, InfluxDB writes the point to <code>default</code> and it keeps those data forever.</p> <p>We want the <code>DEFAULT</code> RP on <code>food_data</code> to be a two hour policy. To create our new RP, we enter the following command:</p> <pre data-language=\"sql\">&gt; CREATE RETENTION POLICY two_hours ON food_data DURATION 2h REPLICATION 1 DEFAULT\n</pre> <p>That query makes the <code>two_hours</code> RP the <code>DEFAULT</code> RP in <code>food_data</code>. When we write data to the database and do not supply an RP in the write, InfluxDB automatically stores those data in the <code>two_hours</code> RP. Once those data have timestamps that are older than two hours, InfluxDB deletes those data. For a more detailed discussion on the <code>CREATE RETENTION POLICY</code> syntax, see <a href=\"../../query_language/database_management/index#retention-policy-management\">Database Management</a>.</p> <p>To clarify, we’ve included the results from the <a href=\"../../query_language/schema_exploration/index#explore-retention-policies-with-show-retention-policies\"><code>SHOW RETENTION POLICIES</code></a> query below. Notice that there are two RPs in <code>food_data</code> (<code>default</code> and <code>two_hours</code>) and that the third column identifies <code>two_hours</code> as the <code>DEFAULT</code> RP.</p> <pre data-language=\"bash\">&gt; SHOW RETENTION POLICIES ON food_data\nname\t\t      duration\t  replicaN\t  default\ndefault\t\t   0\t\t        1\t\t        false\ntwo_hours\t  2h0m0s\t\t   1\t\t        true\n</pre> <h4 id=\"create-the-cq\">Create the CQ</h4> <p>Now we create a CQ that automatically downsamples the 10 second level data to 30 minute level data:</p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY cq_30m ON food_data BEGIN SELECT mean(website) AS mean_website,mean(phone) AS mean_phone INTO food_data.\"default\".downsampled_orders FROM orders GROUP BY time(30m) END\n</pre> <p>That CQ makes InfluxDB automatically and periodically calculate the 30 minute average from the 10 second website order data and the 30 minute average from the 10 second phone order data. InfluxDB also writes the CQ’s results into the measurement <code>downsampled_orders</code> and to the RP <code>default</code>; InfluxDB stores the aggregated data in <code>downsampled_orders</code> forever.</p> <blockquote> <p><strong>Note:</strong> You must specify the RP in the <code>INTO</code> clause to write the results of the query to an RP other than the <code>DEFAULT</code> RP. In the CQ above, we write the results of the query to the infinite RP <code>default</code> by fully qualifying the measurement. To fully qualify a measurement, specify its database and RP with <code>&lt;database_name&gt;.\"&lt;retention_policy&gt;\".&lt;measurement_name&gt;</code>. If you do not fully qualify the measurement, InfluxDB writes the results of the query to the two hour RP <code>DEFAULT</code>.</p> </blockquote> <p>For a more detailed discussion on the <code>CREATE CONTINUOUS QUERY</code> syntax, see <a href=\"../../query_language/continuous_queries/index\">Continuous Queries</a>.</p> <h3 id=\"write-the-data-to-influxdb-and-see-the-results\">Write the data to InfluxDB and see the results</h3> <p>Now that we’ve prepped <code>food_data</code>, we start writing the data to InfluxDB and let things run for a bit. After a while, we see that the database has two measurements: <code>orders</code> and <code>downsampled_orders</code>.</p> <p>A sample of the oldest data in <code>orders</code> - these are the raw 10 second data subject to the <code>two_hours</code> RP:</p> <pre data-language=\"bash\">&gt; SELECT * FROM orders LIMIT 5\nname: orders\n-----------------\ntime\t\t\t\t\t\t            phone \twebsite\n2015-12-04T20:00:11Z\t 1\t     6\n2015-12-04T20:00:20Z\t\t9\t     10\n2015-12-04T20:00:30Z\t\t2\t     17\n2015-12-04T20:00:40Z\t\t3\t     10\n2015-12-04T20:00:50Z\t\t1\t     15\n</pre> <p>We submitted this query on 12/04/2015 at 22:08:19 UTC - notice that the oldest data have timestamps that are no older than around two hours ago.</p> <blockquote> <p><strong>Note:</strong> By default, InfluxDB checks to enforce an RP every 30 minutes so you may have data that are older than two hours between checks. The rate at which InfluxDB checks to enforce an RP is a configurable setting, see <a href=\"../../administration/config/index#retention\">Database Configuration</a>.</p> </blockquote> <p>A sample of the oldest data in <code>downsampled_orders</code> - these are the aggregated data subject to the <code>default</code> RP:</p> <pre data-language=\"bash\">&gt; SELECT * FROM food_data.\"default\".downsampled_orders LIMIT 5\nname: downsampled_orders\n------------------------\ntime\t\t\t               mean_phone\t\t       mean_website\n2015-12-03T22:30:00Z\t 4.318181818181818\t 9.254545454545454\n2015-12-03T23:00:00Z\t 4.266666666666667\t 9.827777777777778\n2015-12-03T23:30:00Z\t 4.766666666666667\t 9.677777777777777\n2015-12-04T00:00:00Z\t 4.405555555555556\t 8.5\n2015-12-04T00:30:00Z\t 4.788888888888889\t 9.383333333333333\n</pre> <p>Notice that the timestamps in <code>downsampled_orders</code> occur at 30 minute intervals and that the measurement has timestamps that are older than those in the <code>orders</code> measurement. The data in <code>downsampled_orders</code> aren’t subject to the <code>two_hours</code> RP.</p> <blockquote> <p><strong>Note:</strong> You must specify the RP in your query to select data that are subject to an RP other than the <code>DEFAULT</code> RP. In the second <code>SELECT</code> statement, we get the CQ results by fully qualifying the measurement. To fully qualify a measurement, specify its database and RP with <code>&lt;database_name&gt;.\"&lt;retention_policy&gt;\".&lt;measurement_name&gt;</code>.</p> </blockquote> <p>Using a combination of RPs and CQs, we’ve made InfluxDB automatically downsample data and expire old data. Now that you have a general understanding of how these features can work together, we recommend looking at the detailed documentation on <a href=\"../../query_language/continuous_queries/index\">CQs</a> and <a href=\"../../query_language/database_management/index#retention-policy-management\">RPs</a> to see all that they can do for you.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/guides/downsampling_and_retention/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/guides/downsampling_and_retention/</a>\n  </p>\n</div>\n","influxdb/v0.13/guides/hardware_sizing/index":"<h1>Hardware Sizing Guidelines</h1>     <p>This guide offers general hardware recommendations for InfluxDB and addresses some frequently asked questions about hardware sizing. The recommendations are only for the <a href=\"../../concepts/storage_engine/index#the-new-influxdb-storage-engine-from-lsm-tree-to-b-tree-and-back-again-to-create-the-time-structured-merge-tree\">Time Structured Merge</a> tree (<code>TSM</code>) storage engine, the only storage engine available with InfluxDB 0.13. Users running older versions of InfluxDB with <a href=\"https://docs.influxdata.com/influxdb/v0.10/administration/upgrading/#convert-b1-and-bz1-shards-to-tsm1\">unconverted</a> <code>b1</code> or <code>bz1</code> shards may have different performance characteristics. See the <a href=\"https://docs.influxdata.com/influxdb/v0.9/guides/hardware_sizing/\">InfluxDB 0.9 sizing guide</a> for more detail.</p> <ul> <li><a href=\"index#general-hardware-guidelines-for-a-single-node\">General hardware guidelines for a single node</a></li> <li><a href=\"index#when-do-i-need-more-ram\">When do I need more RAM?</a></li> <li><a href=\"index#what-kind-of-storage-do-i-need\">What kind of storage do I need?</a></li> <li><a href=\"index#how-much-storage-do-i-need\">How much storage do I need?</a></li> <li><a href=\"index#how-should-i-configure-my-hardware\">How should I configure my hardware?</a></li> </ul> <h2 id=\"general-hardware-guidelines-for-a-single-node\">General hardware guidelines for a single node</h2> <p>We define the load that you’ll be placing on InfluxDB by the number of writes per second, the number of queries per second, and the number of unique <a href=\"../../concepts/glossary/index#series\">series</a>. Based on your load, we make general CPU, RAM, and IOPS recommendations.</p> <table> <thead> <tr> <th>Load</th> <th>Writes per second</th> <th>Queries per second</th> <th>Unique series</th> </tr> </thead> <tbody> <tr> <td><strong>Low</strong></td> <td>&lt; 5 thousand</td> <td>&lt; 5</td> <td>&lt; 100 thousand</td> </tr> <tr> <td><strong>Moderate</strong></td> <td>&lt; 100 thousand</td> <td>&lt; 25</td> <td>&lt; 1 million</td> </tr> <tr> <td><strong>High</strong></td> <td>&gt; 100 thousand</td> <td>&gt; 25</td> <td>&gt; 1 million</td> </tr> <tr> <td><strong>Probably infeasible</strong></td> <td>&gt; 500 thousand</td> <td>&gt; 100</td> <td>&gt; 10 million</td> </tr> </tbody> </table> <h4 id=\"low-load-recommendations\">Low load recommendations</h4> <ul> <li>CPU: 2-4<br>\n</li> <li>RAM: 2-4 GB<br>\n</li> <li>IOPS: 500<br>\n</li> </ul> <h4 id=\"moderate-load-recommendations\">Moderate load recommendations</h4> <ul> <li>CPU: 4-6<br>\n</li> <li>RAM: 8-32GB<br>\n</li> <li>IOPS: 500-1000<br>\n</li> </ul> <h4 id=\"high-load-recommendations\">High load recommendations</h4> <ul> <li>CPU: 8+<br>\n</li> <li>RAM: 32+ GB<br>\n</li> <li>IOPS: 1000+<br>\n</li> </ul> <h4 id=\"probably-infeasible-load\">Probably infeasible load</h4> <p>Performance at this scale is a significant challenge and may not be achievable. Please contact us at <a href=\"mailto:sales@influxdb.com\">sales@influxdb.com</a> for assistance with tuning your systems.</p> <h2 id=\"when-do-i-need-more-ram\">When do I need more RAM?</h2> <p>In general, having more RAM helps queries return faster. There is no known downside to adding more RAM.</p> <p>The major component that affects your RAM needs is series cardinality. Series cardinality is the total number of <a href=\"../../concepts/glossary/index#series\">series</a> in a database. If you have one measurement with two tags, and each tag has 1,000 possible values then the series cardinality is 1 million. A series cardinality around or above 10 million can cause OOM failures even with large amounts of RAM. If this is the case, you can usually address the problem by redesigning your <a href=\"../../concepts/glossary/index#schema\">schema</a>.</p> <p>The increase in RAM needs relative to series cardinality is exponential where the exponent is between one and two:</p> <p><img src=\"https://docs.influxdata.com/img/influxdb/series-cardinality.png\" alt=\"Series Cardinality\"></p> <h2 id=\"what-kind-of-storage-do-i-need\">What kind of storage do I need?</h2> <p>InfluxDB is designed to run on SSDs. Performance is lower on spinning disk drives and may not function properly under increasing loads.</p> <h2 id=\"how-much-storage-do-i-need\">How much storage do I need?</h2> <p>Database names, <a href=\"../../concepts/glossary/index#measurement\">measurements</a>, <a href=\"../../concepts/glossary/index#tag-key\">tag keys</a>, <a href=\"../../concepts/glossary/index#field-key\">field keys</a>, and <a href=\"../../concepts/glossary/index#tag-value\">tag values</a> are stored only once and always as strings. Only <a href=\"../../concepts/glossary/index#field-value\">field values</a> and <a href=\"../../concepts/glossary/index#timestamp\">timestamps</a> are stored per-point.</p> <p>Non-string values require approximately three bytes. String values require variable space as determined by string compression.</p> <h2 id=\"how-should-i-configure-my-hardware\">How should I configure my hardware?</h2> <p>When running InfluxDB in a production environment the <code>wal</code> directory and the <code>data</code> directory should be on separate storage devices. This optimization significantly reduces disk contention when the system is under heavy write load. This is an important consideration if the write load is highly variable. If the write load does not vary by more than 15% the optimization is probably unneeded.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/guides/hardware_sizing/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/guides/hardware_sizing/</a>\n  </p>\n</div>\n","influxdb/v0.13/guides/writing_data/index":"<h1>Writing Data</h1>     <p>There are many ways to write data into InfluxDB including the <a href=\"../../tools/shell/index\">command line interface</a>, <a href=\"../../clients/api/index\">client libraries</a> and plugins for common data formats such as <a href=\"../../write_protocols/graphite/index\">Graphite</a>. Here we’ll show you how to create a database and write data to that database using the built-in HTTP API.</p> <h2 id=\"creating-a-database-using-the-http-api\">Creating a database using the HTTP API</h2> <p>To create a database send a <code>POST</code> request to the <code>/query</code> endpoint and set the URL parameter <code>q</code> to <code>CREATE DATABASE &lt;new_database_name&gt;</code>. The example below sends a request to InfluxDB running on <code>localhost</code> and creates the database <code>mydb</code>:<br> <br></p> <pre data-language=\"bash\">curl -POST http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE mydb\"\n</pre> <h2 id=\"writing-data-using-the-http-api\">Writing data using the HTTP API</h2> <p>The HTTP API is the primary means of putting data into InfluxDB. To write data send a <code>POST</code> request to the <code>/write</code> endpoint. The example below writes a single point to the <code>mydb</code> database. The data consist of the <a href=\"../../concepts/glossary/index#measurement\">measurement</a> <code>cpu_load_short</code>, the <a href=\"../../concepts/glossary/index#tag-key\">tag keys</a> <code>host</code> and <code>region</code> with the <a href=\"../../concepts/glossary/index#tag-value\">tag values</a> <code>server01</code> and <code>us-west</code>, the <a href=\"../../concepts/glossary/index#field-key\">field key</a> <code>value</code> with a <a href=\"../../concepts/glossary/index#field-value\">field value</a> of <code>0.64</code>, and the <a href=\"../../concepts/glossary/index#timestamp\">timestamp</a> <code>1434055562000000000</code>. <br></p> <pre data-language=\"bash\">curl -i -XPOST 'http://localhost:8086/write?db=mydb' --data-binary 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'\n</pre> <p>When writing points, you must specify an existing database in the <code>db</code> query parameter. See the <a href=\"../../write_protocols/write_syntax/index#http\">HTTP section</a> on the Write Syntax page for a complete list of the available query parameters.</p> <p>The body of the POST - we call this the <a href=\"../../concepts/glossary/index#line-protocol\">Line Protocol</a> - contains the time-series data that you wish to store. They consist of a measurement, tags, fields, and a timestamp. InfluxDB requires a measurement name. Strictly speaking, tags are optional but most series include tags to differentiate data sources and to make querying both easy and efficient. Both tag keys and tag values are strings. Field keys are required and are always strings, and, <a href=\"../../write_protocols/write_syntax/index#line-protocol\">by default</a>, field values are floats. The timestamp - supplied at the end of the line in Unix time in nanoseconds since January 1, 1970 UTC - is optional. If you do not specify a timestamp InfluxDB uses the server’s local nanosecond timestamp in Unix epoch. Anything that has to do with time in InfluxDB is always UTC.</p> <h3 id=\"writing-multiple-points\">Writing multiple points</h3>  <p>Post multiple points to multiple series at the same time by separating each point with a new line. Batching points in this manner results in much higher performance.</p> <p>The following example writes three points to the database <code>mydb</code>. The first point belongs to the series with the measurement <code>cpu_load_short</code> and tag set <code>host=server02</code> and has the server’s local timestamp. The second point belongs to the series with the measurement <code>cpu_load_short</code> and tag set <code>host=server02,region=us-west</code> and has the specified timestamp <code>1422568543702900257</code>. The third point has the same specified timestamp as the second point, but it is written to the series with the measurement <code>cpu_load_short</code> and tag set <code>direction=in,host=server01,region=us-west</code>. <br></p> <pre data-language=\"bash\">curl -i -XPOST 'http://localhost:8086/write?db=mydb' --data-binary 'cpu_load_short,host=server02 value=0.67\ncpu_load_short,host=server02,region=us-west value=0.55 1422568543702900257\ncpu_load_short,direction=in,host=server01,region=us-west value=2.0 1422568543702900257'\n</pre> <h3 id=\"writing-points-from-a-file\">Writing points from a file</h3>  <p>Write points from a file by passing <code>@filename</code> to <code>curl</code>. The data in the file should follow InfluxDB’s <a href=\"../../write_protocols/write_syntax/index\">line protocol syntax</a>.</p> <p>Example of a properly-formatted file (<code>cpu_data.txt</code>):<br> <br></p> <pre data-language=\"txt\">cpu_load_short,host=server02 value=0.67\ncpu_load_short,host=server02,region=us-west value=0.55 1422568543702900257\ncpu_load_short,direction=in,host=server01,region=us-west value=2.0 1422568543702900257\n</pre> <p>Write the data in <code>cpu_data.txt</code> to the <code>mydb</code> database with:<br> <br> <code>curl -i -XPOST 'http://localhost:8086/write?db=mydb' --data-binary @cpu_data.txt</code></p> <blockquote> <p><strong>Note:</strong> If your data file has more than 5,000 points, it may be necessary to split that file into several files in order to write your data in batches to InfluxDB. By default, the HTTP request times out after five seconds. InfluxDB will still attempt to write the points after that time out but there will be no confirmation that they were successfully written.</p> </blockquote> <h3 id=\"schemaless-design\">Schemaless Design</h3>  <p>InfluxDB is a schemaless database. You can add new measurements, tags, and fields at any time. Note that if you attempt to write data with a different type than previously used (for example, writing a string to a field that previously accepted integers), InfluxDB will reject those data.</p> <h3 id=\"a-note-on-rest\">A note on REST…</h3>  <p>InfluxDB uses HTTP solely as a convenient and widely supported data transfer protocol.</p> <p>Modern web APIs have settled on REST because it addresses a common need. As the number of endpoints grows the need for an organizing system becomes pressing. REST is the industry agreed style for organizing large numbers of endpoints. This consistency is good for those developing and consuming the API: everyone involved knows what to expect.</p> <p>REST, however, is a convention. InfluxDB makes do with three API endpoints. This simple, easy to understand system uses HTTP as a transfer system for <a href=\"../../query_language/spec/index\">InfluxQL</a>. The InfluxDB API makes no attempt to be RESTful.</p> <h3 id=\"http-response-summary\">HTTP response summary</h3>  <ul> <li>2xx: If it’s <code>HTTP 204 No Content</code>, success! If it’s <code>HTTP 200 OK</code>, InfluxDB understood the request but couldn’t complete it. The body of the response will contain additional error information.</li> <li>4xx: InfluxDB could not understand the request.</li> <li>5xx: The system is overloaded or significantly impaired.</li> </ul> <p><strong>Examples of error responses:</strong></p> <ul> <li>Writing a float to a field that previously accepted booleans:</li> </ul> <pre data-language=\"bash\">curl -i -XPOST 'http://localhost:8086/write?db=hamlet' --data-binary 'tobeornottobe booleanonly=true'  \n\ncurl -i -XPOST 'http://localhost:8086/write?db=hamlet' --data-binary 'tobeornottobe booleanonly=5'\n</pre> <p>returns:<br> <br></p> <pre data-language=\"bash\">HTTP/1.1 400 Bad Request\n[...]\nwrite failed: field type conflict: input field \"booleanonly\" on measurement \"tobeornottobe\" is type float64, already exists as type boolean\n</pre> <ul> <li>Writing a point to a database that doesn’t exist:</li> </ul> <pre data-language=\"bash\">curl -i -XPOST 'http://localhost:8086/write?db=atlantis' --data-binary 'liters value=10'\n</pre> <p>returns:<br> <br></p> <pre data-language=\"bash\">HTTP/1.1 404 Not Found\n[...]\ndatabase not found: \"atlantis\"\n</pre> <h3 id=\"next-steps\">Next steps</h3>  <p>Now that you know how to write data with the built-in HTTP API discover how to query them with the <a href=\"../querying_data/index\">Querying Data</a> guide!</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/guides/writing_data/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/guides/writing_data/</a>\n  </p>\n</div>\n","influxdb/v0.13/concepts/index":"<h1>InfluxDB Concepts</h1>     <p>Understanding the important the following concepts will help you get the most out of InfluxDB.</p> <h2 id=\"key-concepts-influxdb-v0-13-concepts-key-concepts\"><a href=\"key_concepts/index\">Key Concepts</a></h2> <p>A brief explanation of InfluxDB’s core architecture useful for new beginners.</p> <h2 id=\"glossary-of-terms-influxdb-v0-13-concepts-glossary\"><a href=\"glossary/index\">Glossary of Terms</a></h2> <p>A list of InfluxDB terms and their definitions.</p> <h2 id=\"comparison-to-sql-influxdb-v0-13-concepts-crosswalk\"><a href=\"crosswalk/index\">Comparison to SQL</a></h2> <h2 id=\"design-insights-and-tradeoffs-influxdb-v0-13-concepts-insights-tradeoffs\"><a href=\"insights_tradeoffs/index\">Design Insights and Tradeoffs</a></h2> <p>A brief treatment of some of the performance tradeoffs made during the design phase of InfluxDB</p> <h2 id=\"schema-and-data-layout-influxdb-v0-13-concepts-schema-and-data-layout\"><a href=\"schema_and_data_layout/index\">Schema and Data Layout</a></h2> <p>A useful overview of the InfluxDB time series data structure and how it affects performance.</p> <h2 id=\"http-api-endpoints-influxdb-v0-13-concepts-api\"><a href=\"api/index\">HTTP API Endpoints</a></h2> <p>The list of InfluxDB’s HTTP API endpoints and the parameters they accept.</p> <h2 id=\"storage-engine-influxdb-v0-13-concepts-storage-engine\"><a href=\"storage_engine/index\">Storage Engine</a></h2> <p>A overview of how InfluxDB to stores data on disk.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/concepts/</a>\n  </p>\n</div>\n","influxdb/v0.13/concepts/key_concepts/index":"<h1>Key Concepts</h1>     <p>Before diving into InfluxDB it’s good to get acquainted with some of the key concepts of the database. This document provides a gentle introduction to those concepts and common InfluxDB terminology. We’ve provided a list below of all the terms we’ll cover, but we recommend reading this document from start to finish to gain a more general understanding of our favorite time series database.</p> <table style=\"width:100%\"> <tr> <td><a href=\"index#database\">database</a></td> <td><a href=\"index#field-key\">field key</a></td> <td><a href=\"index#field-set\">field set</a></td> </tr> <tr> <td><a href=\"index#field-value\">field value</a></td> <td><a href=\"index#measurement\">measurement</a></td> <td><a href=\"index#point\">point</a></td> </tr> <tr> <td><a href=\"index#retention-policy\">retention policy</a></td> <td><a href=\"index#series\">series</a></td> <td><a href=\"index#tag-key\">tag key</a></td> </tr> <tr> <td><a href=\"index#tag-set\">tag set</a></td> <td><a href=\"index#tag-value\">tag value</a></td> <td><a href=\"index#timestamp\">timestamp</a></td> </tr> </table> <p>Check out the <a href=\"../glossary/index\">Glossary</a> if you prefer the cold, hard facts.</p> <h3 id=\"sample-data\">Sample data</h3> <p>The next section references the data printed out below. The data are fictional, but represent a believable setup in InfluxDB. They show the number of butterflies and honeybees counted by two scientists (<code>langstroth</code> and <code>perpetua</code>) in two locations (location <code>1</code> and location <code>2</code>) over the time period from August 18, 2015 at midnight through August 18, 2015 at 6:12 AM. Assume that the data live in a database called <code>my_database</code> and are subject to the <code>default</code> retention policy (more on databases and retention policies to come).</p> <p><em>Hint:</em> Hover over the links for tooltips to get acquainted with InfluxDB terminology and the layout.</p> <p>name: <span class=\"tooltip\" data-tooltip-text=\"Measurement\">census</span><br> -————————————<br> time <span class=\"tooltip\" data-tooltip-text=\"Field key\">butterflies</span> <span class=\"tooltip\" data-tooltip-text=\"Field key\">honeybees</span> <span class=\"tooltip\" data-tooltip-text=\"Tag key\">location</span> <span class=\"tooltip\" data-tooltip-text=\"Tag key\">scientist</span><br> 2015-08-18T00:00:00Z 12 23 1 langstroth<br> 2015-08-18T00:00:00Z 1 30 1 perpetua<br> 2015-08-18T00:06:00Z 11 28 1 langstroth<br> <span class=\"tooltip\" data-tooltip-text=\"Timestamp\">2015-08-18T00:06:00Z</span> <span class=\"tooltip\" data-tooltip-text=\"Field value\">3</span> <span class=\"tooltip\" data-tooltip-text=\"Field value\">28</span> <span class=\"tooltip\" data-tooltip-text=\"Tag value\">1</span> <span class=\"tooltip\" data-tooltip-text=\"Tag value\">perpetua</span><br> 2015-08-18T05:54:00Z 2 11 2 langstroth<br> 2015-08-18T06:00:00Z 1 10 2 langstroth<br> 2015-08-18T06:06:00Z 8 23 2 perpetua<br> 2015-08-18T06:12:00Z 7 22 2 perpetua</p> <h3 id=\"discussion\">Discussion</h3> <p>Now that you’ve seen some sample data in InfluxDB this section covers what it all means.</p> <p>InfluxDB is a time series database so it makes sense to start with what is at the root of everything we do: time. In the data above there’s a column called <code>time</code> - all data in InfluxDB have that column. <code>time</code> stores timestamps, and the <em>timestamp</em> shows the date and time, in <a href=\"https://www.ietf.org/rfc/rfc3339.txt\">RFC3339</a> UTC, associated with particular data.</p> <p>The next two columns, called <code>butterflies</code> and <code>honeybees</code>, are fields. Fields are made up of field keys and field values. <em>Field keys</em> (<code>butterflies</code> and <code>honeybees</code>) are strings and they store metadata; the field key <code>butterflies</code> tells us that the field values <code>12</code>-<code>7</code> refer to butterflies and the field key <code>honeybees</code> tells us that the field values <code>23</code>-<code>22</code> refer to, well, honeybees.</p> <p><em>Field values</em> are your data; they can be strings, floats, integers, or booleans, and, because InfluxDB is a time series database, a field value is always associated with a timestamp. The field values in the sample data are:</p> <pre>12   23\n1    30\n11   28\n3    28\n2    11\n1    10\n8    23\n7    22\n</pre> <p>In the data above, the collection of field-key and field-value pairs make up a <em>field set</em>. Here are all eight field sets in the sample data:</p> <ul> <li><code>butterflies = 12   honeybees = 23</code></li> <li><code>butterflies = 1    honeybees = 30</code></li> <li><code>butterflies = 11   honeybees = 28</code></li> <li><code>butterflies = 3    honeybees = 28</code></li> <li><code>butterflies = 2    honeybees = 11</code></li> <li><code>butterflies = 1    honeybees = 10</code></li> <li><code>butterflies = 8    honeybees = 23</code></li> <li><code>butterflies = 7    honeybees = 22</code></li> </ul> <p>Fields are a required piece of InfluxDB’s data structure - you cannot have data in InfluxDB without fields. It’s also important to note that fields are not indexed. <a href=\"../glossary/index#query\">Queries</a> that use field values as filters must scan all values that match the other conditions in the query. As a result, those queries are not performant relative to queries on tags (more on tags below). In general, fields should not contain commonly-queried metadata.</p> <p>The last two columns in the sample data, called <code>location</code> and <code>scientist</code>, are tags. Tags are made up of tag keys and tag values. Both <em>tag keys</em> and <em>tag values</em> are stored as strings and record metadata. The tag keys in the sample data are <code>location</code> and <code>scientist</code>. The tag key <code>location</code> has two tag values: <code>1</code> and <code>2</code>. The tag key <code>scientist</code> also has two tag values: <code>langstroth</code> and <code>perpetua</code>.</p> <p>In the data above, the <em>tag set</em> is the different combinations of all the tag key-value pairs. The four tag sets in the sample data are:</p> <ul> <li>\n<code>location = 1</code>, <code>scientist = langstroth</code>\n</li> <li>\n<code>location = 2</code>, <code>scientist = langstroth</code>\n</li> <li>\n<code>location = 1</code>, <code>scientist = perpetua</code>\n</li> <li>\n<code>location = 2</code>, <code>scientist = perpetua</code>\n</li> </ul> <p>Tags are optional. You don’t need to have tags in your data structure, but it’s generally a good idea to make use of them because, unlike fields, tags are indexed. This means that queries on tags are faster and that tags are ideal for storing commonly-queried metadata.</p> <blockquote> <p><strong>Why indexing matters: The schema case study</strong></p> <p>Say you notice that most of your queries focus on the values of the field keys <code>honeybees</code> and <code>butterflies</code>:</p> <p><code>SELECT * FROM census WHERE butterflies = 1</code><br> <code>SELECT * FROM census WHERE honeybees = 23</code></p> <p>Because fields aren’t indexed, InfluxDB scans every value of <code>butterflies</code> in the first query and every value of <code>honeybees</code> in the second query before it provides a response. That behavior can hurt query response times - especially on a much larger scale. To optimize your queries, it may be beneficial to rearrange your <a href=\"../glossary/index#schema\">schema</a> such that the fields (<code>butterflies</code> and <code>honeybees</code>) become the tags and the tags (<code>location</code> and <code>scientist</code>) become the fields:</p> <p>name: <span class=\"tooltip\" data-tooltip-text=\"Measurement\">census</span><br> -————————————<br> time <span class=\"tooltip\" data-tooltip-text=\"Field key\">location</span> <span class=\"tooltip\" data-tooltip-text=\"Field key\">scientist</span> <span class=\"tooltip\" data-tooltip-text=\"Tag key\">butterflies</span> <span class=\"tooltip\" data-tooltip-text=\"Tag key\">honeybees</span><br> 2015-08-18T00:00:00Z 1 langstroth 12 23<br> 2015-08-18T00:00:00Z 1 perpetua 1 30<br> 2015-08-18T00:06:00Z 1 langstroth 11 28<br> <span class=\"tooltip\" data-tooltip-text=\"Timestamp\">2015-08-18T00:06:00Z</span> <span class=\"tooltip\" data-tooltip-text=\"Field value\">1</span> <span class=\"tooltip\" data-tooltip-text=\"Field value\">perpetua</span> <span class=\"tooltip\" data-tooltip-text=\"Tag value\">3</span> <span class=\"tooltip\" data-tooltip-text=\"Tag value\">28</span><br> 2015-08-18T05:54:00Z 2 langstroth 2 11<br> 2015-08-18T06:00:00Z 2 langstroth 1 10<br> 2015-08-18T06:06:00Z 2 perpetua 8 23<br> 2015-08-18T06:12:00Z 2 perpetua 7 22</p> <p>Now that <code>butterflies</code> and <code>honeybees</code> are tags, InfluxDB won’t have to scan every one of their values when it performs the queries above - this means that your queries are even faster.</p> </blockquote> <p>The <em>measurement</em> acts as a container for tags, fields, and the <code>time</code> column, and the measurement name is the description of the data that are stored in the associated fields. Measurement names are strings, and, for any SQL users out there, a measurement is conceptually similar to a table. The only measurement in the sample data is <code>census</code>. The name <code>census</code> tells us that the field values record the number of <code>butterflies</code> and <code>honeybees</code> - not their size, direction, or some sort of happiness index.</p> <p>A single measurement can belong to different retention policies. A <em>retention policy</em> describes how long InfluxDB keeps data (<code>DURATION</code>) and how many copies of those data are stored in the cluster (<code>REPLICATION</code>). If you’re interested in reading more about retention policies, check out <a href=\"../../query_language/database_management/index#retention-policy-management\">Database Management</a>.</p> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <p>In the sample data, everything in the <code>census</code> measurement belongs to the <code>default</code> retention policy. InfluxDB automatically creates that retention policy; it has an infinite duration and a replication factor set to one.</p> <p>Now that you’re familiar with measurements, tag sets, and retention policies it’s time to discuss series. In InfluxDB, a <em>series</em> is the collection of data that share a retention policy, measurement, and tag set. The data above consist of four series:</p> <table> <thead> <tr> <th>Arbitrary series number</th> <th>Retention policy</th> <th>Measurement</th> <th>Tag set</th> </tr> </thead> <tbody> <tr> <td>series 1</td> <td><code>default</code></td> <td><code>census</code></td> <td>\n<code>location = 1</code>,<code>scientist = langstroth</code>\n</td> </tr> <tr> <td>series 2</td> <td><code>default</code></td> <td><code>census</code></td> <td>\n<code>location = 2</code>,<code>scientist = langstroth</code>\n</td> </tr> <tr> <td>series 3</td> <td><code>default</code></td> <td><code>census</code></td> <td>\n<code>location = 1</code>,<code>scientist = perpetua</code>\n</td> </tr> <tr> <td>series 4</td> <td><code>default</code></td> <td><code>census</code></td> <td>\n<code>location = 2</code>,<code>scientist = perpetua</code>\n</td> </tr> </tbody> </table> <p>Understanding the concept of a series is essential when designing your <a href=\"../glossary/index#schema\">schema</a> and when working with your data in InfluxDB.</p> <p>Finally, a <em>point</em> is the field set in the same series with the same timestamp. For example, here’s a single point:</p> <pre>name: census\n-----------------\ntime\t\t\t               butterflies\t honeybees\t location\t scientist\n2015-08-18T00:00:00Z\t 1\t\t          30\t\t       1\t\t       perpetua\n</pre> <p>The series in the example is defined by the retention policy (<code>default</code>), the measurement (<code>census</code>), and the tag set (<code>location = 1</code>, <code>scientist = perpetua</code>). The timestamp for the point is <code>2015-08-18T00:00:00Z</code>.</p> <p>All of the stuff we’ve just covered is stored in a database - the sample data are in the database <code>my_database</code>. An InfluxDB <em>database</em> is similar to traditional relational databases and serves as a logical container for users, retention policies, continuous queries, and, of course, your time series data. See <a href=\"../../administration/authentication_and_authorization/index\">Authentication and Authorization</a> and <a href=\"../../query_language/continuous_queries/index\">Continuous Queries</a> for more on those topics.</p> <p>Databases can have several users, continuous queries, retention policies, and measurements. InfluxDB is a schemaless database which means it’s easy to add new measurements, tags, and fields at any time. It’s designed to make working with time series data awesome.</p> <p>You made it! You’ve covered the fundamental concepts and terminology in InfluxDB. If you’re just starting out, we recommend taking a look at <a href=\"../../introduction/getting_started/index\">Getting Started</a> and the <a href=\"../../guides/writing_data/index\">Writing Data</a> and <a href=\"../../guides/querying_data/index\">Querying Data</a> guides. May our time series database serve you well 🕔.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/key_concepts/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/concepts/key_concepts/</a>\n  </p>\n</div>\n","influxdb/v0.13/concepts/insights_tradeoffs/index":"<h1>Design Insights and Tradeoffs in InfluxDB</h1>     <p>InfluxDB is a time-series database. Optimizing for this use-case entails some tradeoffs, primarily to increase performance at the cost of functionality. Below is a list of some of those design insights that lead to tradeoffs:</p> <ol> <li>For the time series use case, we assume that if the same data is sent multiple times, it is the exact same data that a client just sent several times. <ul> <li>\n<em>Pro:</em> Simplified <a href=\"../../troubleshooting/frequently_encountered_issues/index#writing-duplicate-points\">conflict resolution</a> increases write performance</li> <li>\n<em>Con:</em> May lose data in rare circumstances</li> </ul>\n</li> <li>Deletes are a rare occurrence. When they do occur it is almost always against large ranges of old data that are cold for writes. <ul> <li>\n<em>Pro:</em> Restricting access to deletes allows for increased query and write performance</li> <li>\n<em>Con:</em> Delete functionality is significantly restricted</li> </ul>\n</li> <li>Updates to existing data are a rare occurrence and contentious updates never happen. Time series data is predominantly new data that is never updated. <ul> <li>\n<em>Pro:</em> Restricting access to updates allows for increased query and write performance</li> <li>\n<em>Con:</em> Update functionality is significantly restricted</li> </ul>\n</li> <li>The vast majority of writes are for data with very recent timestamps and the data is added in time ascending order. <ul> <li>\n<em>Pro:</em> Adding data in time ascending order is significantly more performant</li> <li>\n<em>Con:</em> Writing points with random times or with time not in ascending order is significantly less performant</li> </ul>\n</li> <li>Scale is critical. The database must be able to handle a <em>high</em> volume of reads and writes. <ul> <li>\n<em>Pro:</em> The database can handle a <em>high</em> volume of reads and writes</li> <li>\n<em>Con:</em> The InfluxDB development team was forced to make tradeoffs to increase performance</li> </ul>\n</li> <li>Being able to write and query the data is more important than having a strongly consistent view. <ul> <li>\n<em>Pro:</em> Writing and querying the database can be done by multiple clients and at high loads</li> <li>\n<em>Con:</em> Query returns may not include the most recent points if database is under heavy load</li> </ul>\n</li> <li>Many time <a href=\"../glossary/index#series\">series</a> are ephemeral. There are often time series that appear only for a few hours and then go away, e.g. a new host that gets started and reports for a while and then gets shut down. <ul> <li>\n<em>Pro:</em> InfluxDB is good at managing discontinuous data</li> <li>\n<em>Con:</em> Schema-less design means that some database functions are not supported e.g. there are no cross table joins</li> </ul>\n</li> <li>No one point is too important. <ul> <li>\n<em>Pro:</em> InfluxDB has very powerful tools to deal with aggregate data and large data sets</li> <li>\n<em>Con:</em> Points don’t have IDs in the traditional sense, they are differentiated by timestamp and series</li> </ul>\n</li> </ol><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/insights_tradeoffs/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/concepts/insights_tradeoffs/</a>\n  </p>\n</div>\n","influxdb/v0.13/concepts/schema_and_data_layout/index":"<h1>Schema Design</h1>     <p>Every InfluxDB use case is special and your <a href=\"../glossary/index#schema\">schema</a> will reflect that uniqueness. There are, however, general guidelines to follow and pitfalls to avoid when designing your schema.</p> <h2 id=\"encouraged-schema-design\">Encouraged Schema Design</h2> <p>In no particular order, we recommend that you:</p> <ul> <li>\n<p><em>Encode meta data in tags</em></p> <p><a href=\"../glossary/index#tag\">Tags</a> are indexed and <a href=\"../glossary/index#field\">fields</a> are not indexed. This means that queries on tags are more performant than those on fields.</p> <p>In general, your queries should guide what gets stored as a tag and what gets stored as a field:</p> <ul> <li>Store data in tags if they’re commonly-queried meta data</li> <li>Store data in tags if you plan to use them with <code>GROUP BY()</code>\n</li> <li>Store data in fields if you plan to use them with an <a href=\"../../query_language/functions/index\">InfluxQL function</a>\n</li> <li>Store data in fields if you <em>need</em> them to be something other than a string - <a href=\"../glossary/index#tag-value\">tag values</a> are always interpreted as strings</li> </ul>\n</li> <li>\n<p><em>Avoid using InfluxQL Keywords as identifier names</em></p> <p>This isn’t necessary, but it simplifies writing queries; you won’t have to wrap those identifiers in double quotes. Identifiers are database names, <a href=\"../glossary/index#retention-policy-rp\">retention policy</a> names, <a href=\"../glossary/index#user\">user</a> names, <a href=\"../glossary/index#measurement\">measurement</a> names, <a href=\"../glossary/index#tag-key\">tag keys</a>, and <a href=\"../glossary/index#field-key\">field keys</a>. See <a href=\"https://github.com/influxdata/influxdb/blob/master/influxql/README.md#keywords\">InfluxQL Keywords</a> for words to avoid.</p> <p>Note that you will also need to wrap identifiers in double quotes in queries if they contain characters other than <code>[A-z,_]</code>.</p>\n</li> </ul> <h2 id=\"discouraged-schema-design\">Discouraged Schema Design</h2> <p>In no particular order, we recommend that you:</p> <ul> <li>\n<p><em>Don’t have too many series</em></p> <p>See <a href=\"../../guides/hardware_sizing/index#general-hardware-guidelines-for-a-single-node\">Hardware Sizing Guidelines</a> for <a href=\"../glossary/index#series-cardinality\">series cardinality</a> recommendations based on your hardware.</p> <p><a href=\"../glossary/index#tag\">Tags</a> that specify highly variable information like UUIDs, hashes, and random strings can increase your series cardinality to uncomfortable levels. If you need that information in your database, consider storing the high-cardinality data as a field rather than a tag (note that query performance will be slower).</p>\n</li> <li>\n<p><em>Don’t differentiate data with measurement names</em></p> <p>In general, taking this step will simplify your queries. InfluxDB queries merge data that fall within the same <a href=\"../glossary/index#measurement\">measurement</a>; it’s better to differentiate data with <a href=\"../glossary/index#tag\">tags</a> than with detailed measurement names.</p> <p>Example:</p> <table> <thead> <tr> <th>Schema 1</th> <th>Schema 2</th> </tr> </thead> <tbody> <tr> <td>\n<em>Measurement:</em> <code>blueberries.field-1.region-north</code>\n</td> <td>\n<em>Measurement:</em> <code>blueberries</code>; <em>Tags:</em> <code>field = 1</code> and <code>region = north</code>\n</td> </tr> <tr> <td>\n<em>Measurement:</em> <code>blueberries.field-2.region-midwest</code>\n</td> <td>\n<em>Measurement:</em> <code>blueberries</code>; <em>Tags:</em> <code>field = 2</code> and <code>region = midwest</code>\n</td> </tr> </tbody> </table> <p>Assume that each measurement contains a single field key called <code>value</code>. The following queries calculate the average of <code>value</code> across all fields and all regions. Notice that, even at this small scale, this is harder to do under Schema 1.</p> <p><em>Schema 1</em></p> <pre>&gt; SELECT mean(value) FROM /^blueberries/\nname: blueberries.field-1.region-north\n--------------------------------------\ntime                    mean\n1970-01-01T00:00:00Z    444\n\n\nname: blueberries.field-2.region-midwest\n----------------------------------------\ntime                    mean\n1970-01-01T00:00:00Z    33766.666666666664\n</pre> <p>Then calculate the mean yourself.</p> <p><em>Schema 2</em></p> <pre>&gt; SELECT mean(value) FROM blueberries\nname: blueberries\n-----------------\ntime                    mean\n1970-01-01T00:00:00Z    17105.333333333332\n</pre>\n</li> <li>\n<p><em>Don’t put more than one piece of information in one tag</em></p> <p>Similar to the point above, taking this step will simplify your queries. It will reduce your need for regular expressions.</p> <p>Example:</p> <table> <thead> <tr> <th>Tagset 1</th> <th>Tagset 2</th> </tr> </thead> <tbody> <tr> <td><code>location = field-1.region-north</code></td> <td>\n<code>field = 1</code> and <code>region = north</code>\n</td> </tr> <tr> <td><code>location = field-2.region-north</code></td> <td>\n<code>field = 2</code> and <code>region = north</code>\n</td> </tr> <tr> <td><code>location = field-2.region-midwest</code></td> <td>\n<code>field = 2</code> and <code>region = midwest</code>\n</td> </tr> </tbody> </table> <p>Assume that each <a href=\"../glossary/index#tag-set\">tag set</a> falls in the <a href=\"../glossary/index#measurement\">measurement</a> <code>blueberries</code> and is associated with a <a href=\"../glossary/index#field\">field</a> called <code>value</code>. The following queries calculate the average of <code>value</code> for blueberries that fall in the <code>north</code>. While both queries are relatively simple, you can imagine that the regex could get much more complicated if Schema 1 contained a more complex tag value.</p> <p><em>Schema 1</em></p> <pre>&gt; SELECT mean(value) FROM blueberries WHERE location =~ /north/\n</pre> <p><em>Schema 2</em></p> <pre>&gt; SELECT mean(value) FROM blueberries WHERE region = 'north'\n</pre>\n</li> <li>\n<p><em>Don’t use the same name for a field key and tag key</em></p> <p>You won’t be able to query the <a href=\"../glossary/index#tag-key\">tag key</a> if the tag key is the same as a <a href=\"../glossary/index#field-key\">field key</a> in your schema. Be sure to differentiate your tag keys and field keys.</p> \n<dt> See GitHub Issue <a href=\"https://github.com/influxdata/influxdb/issues/6519\">#6519</a> for more information. </dt>\n</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/schema_and_data_layout/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/concepts/schema_and_data_layout/</a>\n  </p>\n</div>\n","influxdb/v0.13/concepts/api/index":"<h1>API Reference</h1>     <p>The InfluxDB API provides a simple way interact with the database. It uses HTTP response codes, HTTP authentication, and responses are returned in JSON.</p> <p>The following sections assume your InfluxDB instance is running on <code>localhost</code> port <code>8086</code> and HTTPS is not enabled. Those settings <a href=\"../../administration/config/index\">are configurable</a>.</p> <h2 id=\"endpoints\">Endpoints</h2> <table> <thead> <tr> <th align=\"left\">Endpoint</th> <th align=\"left\">Description</th> </tr> </thead> <tbody> <tr> <td align=\"left\"><a href=\"#ping\">/ping</a></td> <td align=\"left\">Use <code>/ping</code> to check the status of your InfluxDB instance and your version of InfluxDB.</td> </tr> <tr> <td align=\"left\"><a href=\"#query\">/query</a></td> <td align=\"left\">Use <code>/query</code> to query data and manage databases, retention policies, and users.</td> </tr> <tr> <td align=\"left\"><a href=\"#write\">/write</a></td> <td align=\"left\">Use <code>/write</code> to write data to a pre-existing database.</td> </tr> </tbody> </table> <h3 id=\"ping\">/ping</h3> <p>The ping endpoint accepts both <code>GET</code> and <code>HEAD</code> HTTP requests. Use this endpoint to check the status of your InfluxDB instance and your version of InfluxDB.</p> <h4 id=\"definition\">Definition</h4> <pre>GET http://localhost:8086/ping\n</pre> <pre>HEAD http://localhost:8086/ping\n</pre> <h4 id=\"example\">Example</h4> <p>Extract the version of your InfluxDB instance in the <code>X-Influxdb-Version</code> field of the header:</p> <pre data-language=\"bash\">$ curl -sl -I localhost:8086/ping\n\nHTTP/1.1 204 No Content\nRequest-Id: 7d641f0b-e23b-11e5-8005-000000000000\nX-Influxdb-Version: 0.13.x\nDate: Fri, 04 Mar 2016 19:01:23 GMT\n</pre> <h4 id=\"status-codes-and-responses\">Status Codes and Responses</h4> <p>The response body is empty.</p> <table> <thead> <tr> <th align=\"left\">HTTP Status Code</th> <th align=\"left\">Description</th> </tr> </thead> <tbody> <tr> <td align=\"left\">204</td> <td align=\"left\">Success! Your InfluxDB instance is up and running.</td> </tr> </tbody> </table> <h3 id=\"query\">/query</h3> <p>The <code>/query</code> endpoint accepts <code>GET</code> and <code>POST</code> HTTP requests. Use this endpoint to query data and manage databases, retention policies, and users.</p> <h4 id=\"definition-1\">Definition</h4> <pre>GET http://localhost:8086/query\n</pre> <pre>POST http://localhost:8086/query\n</pre> <h4 id=\"verb-usage\">Verb usage</h4> <table> <thead> <tr> <th align=\"left\">Verb</th> <th align=\"left\">Query Type</th> </tr> </thead> <tbody> <tr> <td align=\"left\">GET</td> <td align=\"left\">Use for all queries that start with: <br><br> <a href=\"../../query_language/spec/index#select\"><code>SELECT</code></a>* <br><br> <a href=\"../../query_language/spec/index#show-continuous-queries\"><code>SHOW</code></a>\n</td> </tr> <tr> <td align=\"left\">POST</td> <td align=\"left\">Use for all queries that start with: <br><br> <a href=\"../../query_language/spec/index#alter-retention-policy\"><code>ALTER</code></a> <br><br> <a href=\"../../query_language/spec/index#create-continuous-query\"><code>CREATE</code></a> <br><br> <a href=\"../../query_language/spec/index#delete\"><code>DELETE</code></a> <br><br> <a href=\"../../query_language/spec/index#drop-continuous-query\"><code>DROP</code></a> <br><br> <a href=\"../../query_language/spec/index#grant\"><code>GRANT</code></a> <br><br> <a href=\"../../query_language/spec/index#kill-query\"><code>KILL</code></a> <br><br> <a href=\"../../query_language/spec/index#revoke\"><code>REVOKE</code></a>\n</td> </tr> </tbody> </table> <p>* The only exceptions are <code>SELECT</code> queries that include an <a href=\"../../query_language/data_exploration/index#the-into-clause\"><code>INTO</code> clause</a>. Those <code>SELECT</code> queries require a <code>POST</code> request.</p> <h4 id=\"query-string-parameters\">Query String Parameters</h4> <table> <thead> <tr> <th align=\"left\">Query String Parameter</th> <th align=\"left\">Optional/Required</th> <th align=\"left\">Definition</th> </tr> </thead> <tbody> <tr> <td align=\"left\">chunk_size=&lt;number_of_points&gt;</td> <td align=\"left\">Optional</td> <td align=\"left\">Set the number of points returned per batch. By default, InfluxDB returns points in batches of 10,000 points.</td> </tr> <tr> <td align=\"left\">db=&lt;database_name&gt;</td> <td align=\"left\">Required for database-dependent queries (most <a href=\"../../query_language/spec/index#select\"><code>SELECT</code></a> queries and <a href=\"../../query_language/spec/index#show-continuous-queries\"><code>SHOW</code></a> queries require this parameter).</td> <td align=\"left\">Sets the target <a href=\"../glossary/index#database\">database</a> for the query.</td> </tr> <tr> <td align=\"left\">epoch=[h,m,s,ms,u,ns]</td> <td align=\"left\">Optional</td> <td align=\"left\">Returns epoch timestamps with the specified precision. By default, InfluxDB returns timestamps in RFC3339 format with nanosecond precision.</td> </tr> <tr> <td align=\"left\">p=&lt;password&gt;</td> <td align=\"left\">Optional if you haven’t <a href=\"../../administration/authentication_and_authorization/index#set-up-authentication\">enabled authentication</a>. Required if you’ve enabled authentication.</td> <td align=\"left\">Sets the password for authentication if you’ve enabled authentication. Use with the query string parameter <code>u</code>.</td> </tr> <tr> <td align=\"left\">pretty=true</td> <td align=\"left\">Optional</td> <td align=\"left\">Enables pretty-printed JSON output. While this is useful for debugging it is not recommended for production use as it consumes unnecessary network bandwidth.</td> </tr> <tr> <td align=\"left\">rp=&lt;retention_policy_name&gt;</td> <td align=\"left\">Optional</td> <td align=\"left\">Sets the target <a href=\"../glossary/index#retention-policy-rp\">retention policy</a> for the query. InfluxDB queries the <code>DEFAULT</code> retention policy if you do not specify a retention policy.</td> </tr> <tr> <td align=\"left\">u=&lt;username&gt;</td> <td align=\"left\">Optional if you haven’t <a href=\"../../administration/authentication_and_authorization/index#set-up-authentication\">enabled authentication</a>. Required if you’ve enabled authentication.</td> <td align=\"left\">Sets the username for authentication if you’ve enabled authentication. The user must have read access to the database. Use with the query string parameter <code>p</code>.</td> </tr> </tbody> </table> <h4 id=\"request-body\">Request Body</h4> <pre>--data-urlencode \"q=&lt;InfluxQL query&gt;\"\n</pre> <p>All queries must be URL encoded and follow <a href=\"../../query_language/index\">InfluxQL</a> syntax. Our example shows the <code>--data-urlencode</code> parameter from <code>curl</code>, which we will use in all examples on this page.</p> <p>Delimit multiple queries with a semicolon.</p> <h4 id=\"examples\">Examples</h4> <p>Query data with a <code>SELECT</code> statement:</p> <pre>$ curl -GET 'http://localhost:8086/query?db=mydb' --data-urlencode 'q=SELECT * FROM \"mymeas\"'\n\n{\"results\":[{\"series\":[{\"name\":\"mymeas\",\"columns\":[\"time\",\"myfield\",\"mytag1\",\"mytag2\"],\"values\":[[\"2016-05-20T21:30:00Z\",12,\"1\",null],[\"2016-05-20T21:30:20Z\",11,\"2\",null],[\"2016-05-20T21:30:40Z\",18,null,\"1\"],[\"2016-05-20T21:31:00Z\",19,null,\"3\"]]}]}]}\n</pre> <p>Query data with a <code>SELECT</code> statement and an <code>INTO</code> clause:</p> <pre>$ curl -POST 'http://localhost:8086/query?db=mydb' --data-urlencode 'q=SELECT * INTO \"newmeas\" FROM \"mymeas\"'\n\n{\"results\":[{\"series\":[{\"name\":\"result\",\"columns\":[\"time\",\"written\"],\"values\":[[\"1970-01-01T00:00:00Z\",4]]}]}]}\n</pre> <p>Query data with a <code>SELECT</code> statement and return pretty-printed JSON:</p> <pre>$ curl -GET 'http://localhost:8086/query?db=mydb&amp;pretty=true' --data-urlencode 'q=SELECT * FROM \"mymeas\"'\n\n{\n    \"results\": [\n        {\n            \"series\": [\n                {\n                    \"name\": \"mymeas\",\n                    \"columns\": [\n                        \"time\",\n                        \"myfield\",\n                        \"mytag1\",\n                        \"mytag2\"\n                    ],\n                    \"values\": [\n                        [\n                            \"2016-05-20T21:30:00Z\",\n                            12,\n                            \"1\",\n                            null\n                        ],\n                        [\n                            \"2016-05-20T21:30:20Z\",\n                            11,\n                            \"2\",\n                            null\n                        ],\n                        [\n                            \"2016-05-20T21:30:40Z\",\n                            18,\n                            null,\n                            \"1\"\n                        ],\n                        [\n                            \"2016-05-20T21:31:00Z\",\n                            19,\n                            null,\n                            \"3\"\n                        ]\n                    ]\n                }\n            ]\n        }\n    ]\n}\n</pre> <p>Query data with a <code>SELECT</code> statement and return second precision epoch timestamps:</p> <pre>$ curl -GET 'http://localhost:8086/query?db=mydb&amp;epoch=s' --data-urlencode 'q=SELECT * FROM \"mymeas\"'\n\n{\"results\":[{\"series\":[{\"name\":\"mymeas\",\"columns\":[\"time\",\"myfield\",\"mytag1\",\"mytag2\"],\"values\":[[1463779800,12,\"1\",null],[1463779820,11,\"2\",null],[1463779840,18,null,\"1\"],[1463779860,19,null,\"3\"]]}]}]}\n</pre> <p>Create a database:</p> <pre>$ curl -POST 'http://localhost:8086/query' --data-urlencode 'q=CREATE DATABASE mydb'\n\n{\"results\":[{}]}\n</pre> <p>Create a database using HTTP authentication:</p> <pre>$ curl -POST 'http://localhost:8086/query?u=myusername&amp;p=mypassword' --data-urlencode 'q=CREATE DATABASE mydb'\n\n{\"results\":[{}]}\n</pre> <p>Send multiple queries:</p> <pre>$ curl -GET 'http://localhost:8086/query?db=mydb&amp;epoch=s' --data-urlencode 'q=SELECT * FROM \"mymeas\";SELECT mean(\"myfield\") FROM \"mymeas\"'\n\n{\"results\":[{\"series\":[{\"name\":\"mymeas\",\"columns\":[\"time\",\"myfield\",\"mytag1\",\"mytag2\"],\"values\":[[1463779800,12,\"1\",null],[1463779820,11,\"2\",null],[1463779840,18,null,\"1\"],[1463779860,19,null,\"3\"]]}]},{\"series\":[{\"name\":\"mymeas\",\"columns\":[\"time\",\"mean\"],\"values\":[[0,15]]}]}]}\n</pre> <h4 id=\"status-codes-and-responses-1\">Status codes and responses</h4> <p>Responses are returned in JSON. Enable pretty-print JSON by including the query string parameter <code>pretty=true</code>.</p> <h5 id=\"summary-table\">Summary Table</h5> <p><br></p> <table> <thead> <tr> <th align=\"left\">HTTP status code</th> <th align=\"left\">Description</th> </tr> </thead> <tbody> <tr> <td align=\"left\">200 OK</td> <td align=\"left\">Success! The returned JSON offers further information.</td> </tr> <tr> <td align=\"left\">400 Bad Request</td> <td align=\"left\">Unacceptable request. Can occur with a syntactically incorrect query. The returned JSON offers further information.</td> </tr> </tbody> </table> <h5 id=\"examples-1\">Examples</h5> <p><br> A successful request that returns data:</p> <pre>$ curl -i -GET 'http://localhost:8086/query?db=mydb' --data-urlencode 'q=SELECT * FROM \"mymeas\"'\n\nHTTP/1.1 200 OK\n[...]\n{\"results\":[{\"series\":[{\"name\":\"mymeas\",\"columns\":[\"time\",\"myfield\",\"mytag1\",\"mytag2\"],\"values\":[[\"2016-05-20T21:30:00Z\",12,\"1\",null],[\"2016-05-20T21:30:20Z\",11,\"2\",null],[\"2016-05-20T21:30:40Z\",18,null,\"1\"],[\"2016-05-20T21:31:00Z\",19,null,\"3\"]]}]}]}\n</pre> <p>A successful request that returns an error:</p> <pre>$ curl -i -GET 'http://localhost:8086/query?db=mydb1' --data-urlencode 'q=SELECT * FROM \"mymeas\"'\n\nHTTP/1.1 200 OK\n[...]\n{\"results\":[{\"error\":\"database not found: mydb1\"}]}\n</pre> <p>An incorrectly formatted query:</p> <pre>$ curl -i -GET 'http://localhost:8086/query?db=mydb' --data-urlencode 'q=SELECT *'\n\nHTTP/1.1 400 Bad Request\n[...]\n{\"error\":\"error parsing query: found EOF, expected FROM at line 1, char 9\"}\n</pre> <h3 id=\"write\">/write</h3> <p>The <code>/write</code> endpoint accepts <code>POST</code> HTTP requests. Use this endpoint to write data to a pre-existing database.</p> <h4 id=\"definition-2\">Definition</h4> <pre>POST http://localhost:8086/write\n</pre> <h4 id=\"query-string-parameters-1\">Query String Parameters</h4> <table> <thead> <tr> <th align=\"left\">Query String Parameter</th> <th align=\"left\">Optional/Required</th> <th align=\"left\">Description</th> </tr> </thead> <tbody> <tr> <td align=\"left\">db=&lt;database&gt;</td> <td align=\"left\">Required</td> <td align=\"left\">Sets the target <a href=\"../glossary/index#database\">database</a> for the write.</td> </tr> <tr> <td align=\"left\">p=&lt;password&gt;</td> <td align=\"left\">Optional if you haven’t <a href=\"../../administration/authentication_and_authorization/index#set-up-authentication\">enabled authentication</a>. Required if you’ve enabled authentication.</td> <td align=\"left\">Sets the password for authentication if you’ve enabled authentication. Use with the query string parameter <code>u</code>.</td> </tr> <tr> <td align=\"left\">precision=[n,u,ms,s,m,h]</td> <td align=\"left\">Optional</td> <td align=\"left\">Sets the precision for the supplied Unix time values. InfluxDB assumes that timestamps are in nanoseconds if you do not specify <code>precision</code>.</td> </tr> <tr> <td align=\"left\">rp=&lt;retention_policy_name&gt;</td> <td align=\"left\">Optional</td> <td align=\"left\">Sets the target <a href=\"../glossary/index#retention-policy-rp\">retention policy</a> for the write. InfluxDB writes to the <code>DEFAULT</code> retention policy if you do not specify a retention policy.</td> </tr> <tr> <td align=\"left\">u=&lt;username&gt;</td> <td align=\"left\">Optional if you haven’t <a href=\"../../administration/authentication_and_authorization/index#set-up-authentication\">enabled authentication</a>. Required if you’ve enabled authentication.</td> <td align=\"left\">Sets the username for authentication if you’ve enabled authentication. The user must have write access to the database. Use with the query string parameter <code>p</code>.</td> </tr> </tbody> </table> <h4 id=\"request-body-1\">Request Body</h4> <pre>--data-binary '&lt;Data in Line Protocol format&gt;'\n</pre> <p>All data must be binary encoded and in the <a href=\"../glossary/index#line-protocol\">Line Protocol</a> format. Our example shows the <code>--data-binary</code> parameter from curl, which we will use in all examples on this page. Using any encoding method other than <code>--data-binary</code> will likely lead to issues; <code>-d</code>, <code>--data-urlencode</code>, and <code>--data-ascii</code> may strip out newlines or introduce new, unintended formatting.</p> <p>Options:</p> <ul> <li>Write several points to the database with one request by separating each point by a new line.</li> <li>\n<p>Write points from a file with the <code>@</code> flag. The file should contain a batch of points in the Line Protocol format. Individual points must be on their own line and separated by newline characters (<code>\\n</code>). Files containing carriage returns will cause parser errors.</p> <p>We recommend writing points in batches of 5,000 to 10,000 points. Smaller batches, and more HTTP requests, will result in sub-optimal performance.</p>\n</li> </ul> <h4 id=\"examples-2\">Examples</h4> <p>Write a point to the database <code>mydb</code> with a nanosecond timestamp:</p> <pre>$ curl -i -POST \"http://localhost:8086/write?db=mydb\" --data-binary 'mymeas,mytag=1 myfield=90 1463683075000000000'\n</pre> <p>Write a point to the database <code>mydb</code> with the local server’s nanosecond timestamp:</p> <pre>$ curl -i -POST \"http://localhost:8086/write?db=mydb\" --data-binary 'mymeas,mytag=1 myfield=90'\n</pre> <p>Write a point to the database <code>mydb</code> with a timestamp in seconds:</p> <pre>$ curl -i -POST \"http://localhost:8086/write?db=mydb&amp;precision=s\" --data-binary 'mymeas,mytag=1 myfield=90 1463683075'\n</pre> <p>Write a point to the database <code>mydb</code> and the retention policy <code>myrp</code>:</p> <pre>$ curl -i -POST \"http://localhost:8086/write?db=mydb&amp;rp=myrp\" --data-binary 'mymeas,mytag=1 myfield=90'\n</pre> <p>Write a point to the database <code>mydb</code> using HTTP authentication:</p> <pre>$ curl -i -POST \"http://localhost:8086/write?db=mydb&amp;u=myusername&amp;p=mypassword\" --data-binary 'mymeas,mytag=1 myfield=91'\n</pre> <p>Write several points to the database <code>mydb</code> by separating points with a new line:</p> <pre>$ curl -i -POST \"http://localhost:8086/write?db=mydb\" --data-binary 'mymeas,mytag=3 myfield=89\nmymeas,mytag=2 myfield=34 1463689152000000000'\n</pre> <p>Write several points to the database <code>mydb</code> from the file <code>data.txt</code>:</p> <pre>$ curl -i -POST \"http://localhost:8086/write?db=mydb\" --data-binary @data.txt\n</pre> <p>A sample of the data in <code>data.txt</code>:</p> <pre>mymeas,mytag1=1 value=21 1463689680000000000\nmymeas,mytag1=1 value=34 1463689690000000000\nmymeas,mytag2=8 value=78 1463689700000000000\nmymeas,mytag3=9 value=89 1463689710000000000\n</pre> <h4 id=\"status-codes-and-responses-2\">Status codes and responses</h4> <p>In general, status codes of the form <code>2xx</code> indicate success, <code>4xx</code> indicate that InfluxDB could not understand the request, and <code>5xx</code> indicate that the system is overloaded or significantly impaired. Errors are returned in JSON.</p> <h5 id=\"summary-table-1\">Summary Table</h5> <p><br></p> <table> <thead> <tr> <th align=\"left\">HTTP status code</th> <th align=\"left\">Description</th> </tr> </thead> <tbody> <tr> <td align=\"left\">204 No Content</td> <td align=\"left\">Success!</td> </tr> <tr> <td align=\"left\">400 Bad Request</td> <td align=\"left\">Unacceptable request. Can occur with a Line Protocol syntax error or if a user attempts to write values to a field that previously accepted a different value type. The returned JSON offers further information.</td> </tr> <tr> <td align=\"left\">404 Not Found</td> <td align=\"left\">Unacceptable request. Can occur if a user attempts to write to a database that does not exist. The returned JSON offers further information.</td> </tr> <tr> <td align=\"left\">500 Internal Server Error</td> <td align=\"left\">The system is overloaded or significantly impaired. Can occur if a user attempts to write to a retention policy that does not exist. The returned JSON offers further information.</td> </tr> </tbody> </table> <h5 id=\"examples-3\">Examples</h5> <p><br> A successful write:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Write a point with an incorrect timestamp:</p> <pre>HTTP/1.1 400 Bad Request\n[...]\n{\"error\":\"unable to parse 'mymeas,mytag=1 myfield=91 abc123': bad timestamp\"}\n</pre> <p>Write an integer to a field that previously accepted a float:</p> <pre>HTTP/1.1 400 Bad Request\n[...]\n{\"error\":\"field type conflict: input field \\\"myfield\\\" on measurement \\\"mymeas\\\" is type int64, already exists as type float\"}\n</pre> <p>Write a point to a database that doesn’t exist:</p> <pre>HTTP/1.1 404 Not Found\n[...]\n{\"error\":\"database not found: \\\"mydb1\\\"\"}\n</pre> <p>Write a point to a retention policy that doesn’t exist:</p> <pre>HTTP/1.1 500 Internal Server Error\n[...]\n{\"error\":\"retention policy not found: myrp\"}\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/api/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/concepts/api/</a>\n  </p>\n</div>\n","influxdb/v0.13/introduction/index":"<h1>Introduction</h1>     <p>The introductory documentation includes all the information you need to get up and running with InfluxDB.</p> <h2 id=\"download-https-influxdata-com-downloads-influxdb\"><a href=\"https://influxdata.com/downloads/#influxdb\">Download</a></h2> <p>Provides the location to download the latest stable and nightly builds of InfluxDB.</p> <h2 id=\"installation-influxdb-v0-13-introduction-installation\"><a href=\"installation/index\">Installation</a></h2> <p>Provides instructions for installing InfluxDB on Ubuntu, Debian, RedHat, CentOS, and OS X.</p> <h2 id=\"getting-started-influxdb-v0-13-introduction-getting-started\"><a href=\"getting_started/index\">Getting Started</a></h2> <p>A introductory guide to reading and writing time series data using InfluxDB.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/introduction/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/introduction/</a>\n  </p>\n</div>\n","influxdb/v0.13/concepts/crosswalk/index":"<h1>Comparison to SQL</h1>     <h1 id=\"what-s-in-a-database\">What’s in a database?</h1> <p>This page gives SQL users an overview of how InfluxDB is like an SQL database and how it’s not. It highlights some of the major distinctions between the two and provides a loose crosswalk between the different database terminologies and query languages.</p> <h2 id=\"in-general\">In general…</h2> <p>InfluxDB is designed to work with time-series data. SQL databases can handle time-series but weren’t created strictly for that purpose. In short, InfluxDB is made to store a large volume of time-series data and perform real-time analysis on those data, quickly.</p> <h3 id=\"timing-is-everything\">Timing is everything</h3> <p>In InfluxDB, a timestamp identifies a single point in any given data series. This is like an SQL database table where the primary key is pre-set by the system and is always time.</p> <p>InfluxDB also recognizes that your <a href=\"../glossary/index#schema\">schema</a> preferences may change over time. In InfluxDB you don’t have to define schemas up front. Data points can have one of the fields on a measurement, all of the fields on a measurement, or any number in-between. You can add new fields to a measurement simply by writing a point for that new field. If you need an explanation of the terms measurements, tags, and fields check out the next section for an SQL database to InfluxDB terminology crosswalk.</p> <h2 id=\"terminology\">Terminology</h2> <p>The table below is a (very) simple example of a table called <code>foodships</code> in an SQL database with the unindexed column <code>#_foodships</code> and the indexed columns <code>park_id</code>, <code>planet</code>, and <code>time</code>.</p> <pre data-language=\"sql\">+---------+---------+---------------------+--------------+\n| park_id | planet  | time                | #_foodships  |\n+---------+---------+---------------------+--------------+\n|       1 | Earth   | 1429185600000000000 |            0 |\n|       1 | Earth   | 1429185601000000000 |            3 |\n|       1 | Earth   | 1429185602000000000 |           15 |\n|       1 | Earth   | 1429185603000000000 |           15 |\n|       2 | Saturn  | 1429185600000000000 |            5 |\n|       2 | Saturn  | 1429185601000000000 |            9 |\n|       2 | Saturn  | 1429185602000000000 |           10 |\n|       2 | Saturn  | 1429185603000000000 |           14 |\n|       3 | Jupiter | 1429185600000000000 |           20 |\n|       3 | Jupiter | 1429185601000000000 |           21 |\n|       3 | Jupiter | 1429185602000000000 |           21 |\n|       3 | Jupiter | 1429185603000000000 |           20 |\n|       4 | Saturn  | 1429185600000000000 |            5 |\n|       4 | Saturn  | 1429185601000000000 |            5 |\n|       4 | Saturn  | 1429185602000000000 |            6 |\n|       4 | Saturn  | 1429185603000000000 |            5 |\n+---------+---------+---------------------+--------------+\n</pre> <p>Those same data look like this in InfluxDB:</p> <pre data-language=\"sql\">name: foodships\ntags: park_id=1, planet=Earth\ntime\t\t\t              #_foodships\n----\t\t\t              ------------\n2015-04-16T12:00:00Z\t0\n2015-04-16T12:00:01Z\t3\n2015-04-16T12:00:02Z\t15\n2015-04-16T12:00:03Z\t15\n\nname: foodships\ntags: park_id=2, planet=Saturn\ntime\t\t\t              #_foodships\n----\t\t\t              ------------\n2015-04-16T12:00:00Z\t5\n2015-04-16T12:00:01Z\t9\n2015-04-16T12:00:02Z\t10\n2015-04-16T12:00:03Z\t14\n\nname: foodships\ntags: park_id=3, planet=Jupiter\ntime\t\t\t              #_foodships\n----\t\t\t              ------------\n2015-04-16T12:00:00Z\t20\n2015-04-16T12:00:01Z\t21\n2015-04-16T12:00:02Z\t21\n2015-04-16T12:00:03Z\t20\n\nname: foodships\ntags: park_id=4, planet=Saturn\ntime\t\t\t              #_foodships\n----\t\t\t              ------------\n2015-04-16T12:00:00Z\t5\n2015-04-16T12:00:01Z\t5\n2015-04-16T12:00:02Z\t6\n2015-04-16T12:00:03Z\t5\n</pre> <p>Referencing the example above, in general:</p> <ul> <li>An InfluxDB measurement (<code>foodships</code>) is similar to an SQL database table.</li> <li>InfluxDB tags ( <code>park_id</code> and <code>planet</code>) are like indexed columns in an SQL database.</li> <li>InfluxDB fields (<code>#_foodships</code>) are like unindexed columns in an SQL database.</li> <li>InfluxDB points (for example, <code>2015-04-16T12:00:00Z   5</code>) are similar to SQL rows.</li> </ul> <p>Building on this comparison of database terminology, InfluxDB’s <a href=\"../glossary/index#continuous-query-cq\">continuous queries</a> and <a href=\"../glossary/index#retention-policy-rp\">retention policies</a> are similar to stored procedures in an SQL database. They’re specified once and then performed regularly and automatically.</p> <p>Of course, there are some major disparities between SQL databases and InfluxDB. SQL <code>JOIN</code>s aren’t available for InfluxDB measurements; your schema design should reflect that difference. And, as we mentioned above, a measurement is like an SQL table where the primary index is always pre-set to time. InfluxDB timestamps must be in UNIX epoch (GMT) or formatted as a date-time string valid under RFC3339.</p> <p>For more detailed descriptions of the InfluxDB terms mentioned in this section see our <a href=\"../glossary/index\">Glossary of Terms</a>.</p> <h2 id=\"influxql-and-sql\">InfluxQL and SQL</h2> <p>InfluxQL is an SQL-like query language for interacting with InfluxDB. It has been lovingly crafted to feel familiar to those coming from other SQL or SQL-like environments while also providing features specific to storing and analyzing time series data.</p> <p>InfluxQL’s <code>SELECT</code> statement follows the form of an SQL <code>SELECT</code> statement:</p> <pre data-language=\"sql\">SELECT &lt;stuff&gt; FROM &lt;measurement_name&gt; WHERE &lt;some_conditions&gt;\n</pre> <p>where <code>WHERE</code> is optional. To get the InfluxDB output in the section above, you’d enter:</p> <pre data-language=\"sql\">SELECT * FROM foodships\n</pre> <p>If you only wanted to see data for the planet <code>Saturn</code>, you’d enter:</p> <pre data-language=\"sql\">SELECT * FROM foodships WHERE planet = 'Saturn'\n</pre> <p>If you wanted to see data for the planet <code>Saturn</code> after 12:00:01 UTC on April 16, 2015, you’d enter:</p> <pre data-language=\"sql\">SELECT * FROM foodships WHERE planet = 'Saturn' AND time &gt; '2015-04-16 12:00:01'\n</pre> <p>As shown in the example above, InfluxQL allows you to specify the time range of your query in the <code>WHERE</code> clause. You can use date-time strings wrapped in single quotes that have the format <code>YYYY-MM-DD HH:MM:SS.mmm</code> ( <code>mmm</code> is milliseconds and is optional, and you can also specify microseconds or nanoseconds). You can also use relative time with <code>now()</code> which refers to the server’s current timestamp:</p> <pre data-language=\"sql\">SELECT * FROM foodships WHERE time &gt; now() - 1h\n</pre> <p>That query outputs the data in the <code>foodships</code> measure where the timestamp is newer than the server’s current time minus one hour. The options for specifying time durations with <code>now()</code> are:</p> <table> <thead> <tr> <th align=\"center\">Letter</th> <th align=\"center\">Meaning</th> </tr> </thead> <tbody> <tr> <td align=\"center\">u</td> <td align=\"center\">microseconds</td> </tr> <tr> <td align=\"center\">s</td> <td align=\"center\">seconds</td> </tr> <tr> <td align=\"center\">m</td> <td align=\"center\">minutes</td> </tr> <tr> <td align=\"center\">h</td> <td align=\"center\">hours</td> </tr> <tr> <td align=\"center\">d</td> <td align=\"center\">days</td> </tr> <tr> <td align=\"center\">w</td> <td align=\"center\">weeks</td> </tr> </tbody> </table> <p><br></p> <p>InfluxQL also supports regular expressions, arithmetic in expressions, <code>SHOW</code> statements, and <code>GROUP BY</code> statements. See our <a href=\"../../query_language/data_exploration/index\">data exploration</a> page for an in-depth discussion of those topics. InfluxQL functions include <code>COUNT</code>, <code>MIN</code>, <code>MAX</code>, <code>MEDIAN</code>, <code>DERIVATIVE</code> and more. For a full list check out the <a href=\"../../query_language/functions/index\">functions</a> page.</p> <p>Now that you have the general idea, check out our <a href=\"../../introduction/getting_started/index\">Getting Started Guide</a>.</p> <h2 id=\"a-note-on-why-influxdb-isn-t-crud\">A note on why InfluxDB isn’t CRUD…</h2> <p>InfluxDB is a database that has been optimized for time series data. This data commonly comes from sources like distributed sensor groups, click data from large websites, or lists of financial transactions.</p> <p>One thing this data has in common is that it is more useful in the aggregate. One reading saying that your computer’s CPU is at 12% utilization at 12:38:35 UTC on a Tuesday is hard to draw conclusions from. It becomes more useful when combined with the rest of the series and visualized. This is where trends over time begin to show, and actionable insight can be drawn from the data. In addition, time series data is generally written once and rarely updated.</p> <p>The result is that InfluxDB is not a full CRUD database but more like a CR-ud, prioritizing the performance of creating and reading data over update and destroy, and preventing some update and destroy behaviors to make create and read more performant.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/crosswalk/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/concepts/crosswalk/</a>\n  </p>\n</div>\n","influxdb/v0.13/troubleshooting/statistics/index":"<h1>Server Monitoring</h1>     <p>InfluxDB can display statistical and diagnostic information about each node. This information can be very useful for troubleshooting and performance monitoring.</p> <h2 id=\"show-stats\">SHOW STATS</h2> <p>To see node stats execute the command <code>SHOW STATS</code>.</p> <p>The statistics returned by <code>SHOW STATS</code> are stored in memory only, and are reset to zero when the node is restarted.</p> <h2 id=\"show-diagnostics\">SHOW DIAGNOSTICS</h2> <p>To see node diagnostics execute the command <code>SHOW DIAGNOSTICS</code>. This returns information such as build information, uptime, hostname, server configuration, memory usage, and Go runtime diagnostics.</p> <h2 id=\"internal-monitoring\">Internal Monitoring</h2> <p>InfluxDB also writes statistical and diagnostic information to database named <code>_internal</code>, which records metrics on the internal runtime and service performance. The <code>_internal</code> database can be queried and manipulated like any other InfluxDB database. Check out the <a href=\"https://github.com/influxdb/influxdb/blob/master/monitor/README.md\">monitor service README</a> and the <a href=\"https://influxdb.com/blog/2015/09/22/monitoring_internal_show_stats.html\">internal monitoring blog post</a> for more detail.</p> <h2 id=\"useful-commands\">Useful Commands</h2> <p>Below are a collection of commands to find useful performance metrics about your InfluxDB instance.</p> <p>To find the number of points per second being written to the instance. Must have the <code>monitor</code> service enabled:</p> <pre data-language=\"bash\">$ influx -execute 'select derivative(pointReq, 1s) from \"write\" where time &gt; now() - 5m' -database '_internal' -precision 'rfc3339'\n</pre> <p>To find the number of writes separated by database since the beginnning of the log file:</p> <pre data-language=\"bash\">$ grep 'POST' /var/log/influxdb/influxd.log | awk '{ print $10 }' | sort | uniq -c\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/troubleshooting/statistics/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/troubleshooting/statistics/</a>\n  </p>\n</div>\n","influxdb/v0.13/write_protocols/line/index":"<h1>Line Protocol</h1>     <p>The line protocol is a text based format for writing points to InfluxDB. Each line defines a single point. Multiple lines must be separated by the newline character <code>\\n</code>. The format of the line consists of three parts:</p> <pre>[key] [fields] [timestamp]\n</pre> <p>Each section is separated by spaces. The minimum required point consists of a measurement name and at least one field. Points without a specified timestamp will be written using the server’s local timestamp. Timestamps are assumed to be in nanoseconds unless a <code>precision</code> value is passed in the query string.</p> <h2 id=\"key\">Key</h2> <p>The key is the measurement name and any optional tags separated by commas. Measurement names, tag keys, and tag values must escape any spaces or commas using a backslash (<code>\\</code>). For example: <code>\\</code> and <code>\\,</code>. All tag values are stored as strings and should not be surrounded in quotes.</p> <p>Tags should be sorted by key before being sent for best performance. The sort should match that from the Go <code>bytes.Compare</code> function (<a href=\"http://golang.org/pkg/bytes/#Compare\">http://golang.org/pkg/bytes/#Compare</a>).</p> <h3 id=\"examples\">Examples</h3> <pre># measurement only\ncpu\n\n# measurement and tags\ncpu,host=serverA,region=us-west\n\n# measurement with commas\ncpu\\,01,host=serverA,region=us-west\n\n# tag value with spaces\ncpu,host=server\\ A,region=us\\ west\n</pre> <h2 id=\"fields\">Fields</h2> <p>Fields are key-value metrics associated with the measurement. Every line must have at least one field. Multiple fields must be separated with commas and not spaces.</p> <p>Field keys are always strings and follow the same syntactical rules as described above for tag keys and values. Field values can be one of four types. The first value written for a given field on a given measurement defines the type of that field for all series under that measurement.</p> <p><strong>Integers</strong> are numeric values that do not include a decimal and are followed by a trailing <code>i</code> when inserted (e.g. 1i, 345i, 2015i, -10i). Note that all values <em>must</em> have a trailing <code>i</code>. If they do not they will be written as floats.</p> <p><strong>Floats</strong> are numeric values that are not followed by a trailing <code>i</code>. (e.g. 1, 1.0, -3.14, 6.0e5, 10).</p> <p><strong>Boolean</strong> values indicate true or false. Valid boolean strings for line protocol are (t, T, true, True, TRUE, f, F, false, False and FALSE).</p> <p><strong>Strings</strong> are text values. All string field values <em>must</em> be surrounded in double-quotes <code>\"</code>. If the string contains a double-quote, the double-quote must be escaped with a backslash, e.g. <code>\\\"</code>.</p> <pre># integer value\ncpu value=1i\n\n# float value\ncpu_load value=1\n\ncpu_load value=1.0\n\ncpu_load value=1.2\n\n# boolean value\nerror fatal=true\n\n# string value\nevent msg=\"logged out\"\n\n# multiple values\ncpu load=10,alert=true,reason=\"value above maximum threshold\"\n</pre> <h2 id=\"timestamp\">Timestamp</h2> <p>The timestamp section is optional but should be specified if possible. The value is an integer representing nanoseconds since the epoch. If the timestamp is not provided the point will inherit the server’s local timestamp.</p> <p>Some write APIs allow passing a lower precision. If the API supports a lower precision, the timestamp may also be an integer epoch in microseconds, milliseconds, seconds, minutes or hours. We recommend using the least precise precision possible as this can result in significant improvements in compression.</p> <h2 id=\"full-example\">Full Example</h2> <p>A full example is shown below.</p> <pre>cpu,host=server01,region=uswest value=1 1434055562000000000\ncpu,host=server02,region=uswest value=3 1434055562000010000\ntemperature,machine=unit42,type=assembly internal=32,external=100 1434055562000000035\ntemperature,machine=unit143,type=assembly internal=22,external=130 1434055562005000035\n</pre> <p>In this example the first line shows a <code>measurement</code> of “cpu”, there are two <code>tags</code> “host” and “region”, the <code>value</code> is 1.0, and the <code>timestamp</code> is 1434055562000000000. Following this is a second line, also a point in the <code>measurement</code> “cpu” but belonging to a different “host”.</p> <p>The two last lines of the example show two points in the <code>measurement</code> “temperature”. There are two <code>tags</code> “machine” and “type” with <code>fields</code> for “internal” and “external” temperatures.</p> <pre>cpu,host=server\\ 01,region=uswest value=1,msg=\"all systems nominal\"\ncpu,host=server\\ 01,region=us\\,west value_int=1i\n</pre> <p>In these examples, the “host” is set to <code>server 01</code>. The field value associated with field key <code>msg</code> is double-quoted, as it is a string. The second example shows a region of <code>us,west</code> with the comma properly escaped. In the first example <code>value</code> is written as a floating point number. In the second, <code>value_int</code> is an integer.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/write_protocols/line/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/write_protocols/line/</a>\n  </p>\n</div>\n","influxdb/v0.13/concepts/storage_engine/index":"<h1>Storage Engine</h1>     <p>The 0.9 line of InfluxDB used BoltDB as the underlying storage engine. This writeup is about the experimental version of the Time Structured Merge Tree storage engine that was released in 0.9.5. There may be small discrepancies between the current implementation of TSM and this document.</p> <p><a href=\"https://influxdata.com/blog/new-storage-engine-time-structured-merge-tree/\" target=\"_\">See the blog post announcement about the storage engine here</a>.</p> <h2 id=\"the-new-influxdb-storage-engine-from-lsm-tree-to-b-tree-and-back-again-to-create-the-time-structured-merge-tree\">The new InfluxDB storage engine: from LSM Tree to B+Tree and back again to create the Time Structured Merge Tree</h2> <p>The properties of the time series data use case make it challenging for many existing storage engines. Over the course of InfluxDB’s development we’ve tried a few of the more popular options. We started with LevelDB, an engine based on LSM Trees, which are optimized for write throughput. After that we tried BoltDB, an engine based on a memory mapped B+Tree, which is optimized for reads. Finally, we ended up building our own storage engine that is similar in many ways to LSM Trees.</p> <p>With our new storage engine we were able to achieve up to a 45x reduction in disk space usage from our B+Tree setup with even greater write throughput and compression than what we saw with LevelDB and its variants. Using batched writes, we were able to insert more than 300,000 points per second on a c3.8xlarge instance in AWS. This post will cover the details of that evolution and end with an in-depth look at our new storage engine and its inner workings.</p> <h3 id=\"properties-of-time-series-data\">Properties of Time Series Data</h3> <p>The workload of time series data is quite different from normal database workloads. There are a number of factors that conspire to make it very difficult to get it to scale and perform well:</p> <ul> <li>Billions of individual data points</li> <li>High write throughput</li> <li>High read throughput</li> <li>Large deletes to free up disk space</li> <li>Mostly an insert/append workload, very few updates</li> </ul> <p>The first and most obvious problem is one of scale. In DevOps, for instance, you can collect hundreds of millions or billions of unique data points every day.</p> <p>To prove out the numbers, let’s say we have 200 VMs or servers running, with each server collecting an average of 100 measurements every 10 seconds. Given there are 86,400 seconds in a day, a single measurement will generate 8,640 points in a day, per server. That gives us a total of 200 * 100 * 8,640 = 172,800,000 individual data points per day. We find similar or larger numbers in sensor data use cases.</p> <p>The volume of data means that the write throughput can be very high. We regularly get requests for setups than can handle hundreds of thousands of writes per second. I’ve talked to some larger companies that will only consider systems that can handle millions of writes per second.</p> <p>At the same time, time series data can be a high read throughput use case. It’s true that if you’re tracking 700,000 unique metrics or time series, you can’t hope to visualize all of them, which is what leads many people to think that you don’t actually read most of the data that goes into the database. However, other than dashboards that people have up on their screens, there are automated systems for monitoring or combining the large volume of time series data with other types of data.</p> <p>Inside InfluxDB, we have aggregates that can get calculated on the fly that can combine tens of thousands of time series into a single view. Each one of those queries does a read on each data point, which means that for InfluxDB, the read throughput is often many times higher than the write throughput.</p> <p>Given that time series is a mostly an append only workload, you might think that it’s possible to get great performance on a B+Tree. Appends in the keyspace are efficient and you can achieve greater than 100,000 per second. However, we have those appends happening in individual time series. So the inserts end up looking more like random inserts than append only inserts.</p> <p>One of the biggest problems we found with time series data is that it’s very common to delete all data after it gets past a certain age. The common pattern here is that you’ll have high precision data that is kept for a short period of time like a few hours or a few weeks. You’ll then downsample and aggregate that data into lower precision, which you’ll keep around for much longer.</p> <p>The naive implementation would be to simply delete each record once it gets past its expiration time. However, that means that once you’re up to your window of retention, you’ll be doing just as many deletes as you do writes, which is something most storage engines aren’t designed for.</p> <p>Let’s dig into the details of the two types of storage engines we tried and how these properties had a significant impact on our performance.</p> <h3 id=\"leveldb-and-log-structured-merge-trees\">LevelDB and Log Structured Merge Trees</h3> <p>Two years ago when we started InfluxDB, we picked LevelDB as the storage engine because it was what we used for time series data storage for the product that was the precursor to InfluxDB. We knew that it had great properties for write throughput and everything seemed to just work.</p> <p>LevelDB is an implementation of a Log Structured Merge Tree (or LSM Tree) that was built as an open source project at Google. It exposes an API for a key/value store where the key space is sorted. This last part is important for time series data as it would allow us to quickly go through ranges of time as long as the timestamp was in the key.</p> <p>LSM Trees are based on a log that takes writes and two structures known as Mem Tables and SSTables. These tables represent the sorted keyspace. SSTables are read only files that continuously get replaced by other SSTables that merge inserts and updates into the keyspace.</p> <p>The two biggest advantages that LevelDB had for us were high write throughput and built in compression. However, as we learned more about what people needed with time series data, we encountered a few insurmountable challenges.</p> <p>The first problem we had was that LevelDB doesn’t support hot backups. If you want to do a safe backup of the database, you have to close it and then copy it. The LevelDB variants RocksDB and HyperLevelDB fix this problem so we could have moved to them, but there was another problem that was more pressing that we didn’t think could be solved with either of them.</p> <p>We needed to give our users a way to automatically manage the data retention of their time series data. That meant that we’d have to do very large scale deletes. In LSM Trees a delete is as expensive, if not more so, than a write. A delete will write a new record known as a tombstone. After that queries will merge the result set with any tombstones to clear out the deletes. Later, a compaction will run that will remove the tombstone and the underlying record from the SSTable file.</p> <p>To get around doing deletes, we split data across what we call shards, which are contiguous blocks of time. Shards would typically hold either a day or 7 days worth of data. Each shard mapped to an underlying LevelDB. This meant that we could drop an entire day of data by just closing out the database and removing the underlying files.</p> <p>Users of RocksDB may at this point bring up a feature called ColumnFamilies. When putting time series data into Rocks, it’s common to split blocks of time into column families and then drop those when their time is up. It’s the same general idea that you create a separate area where you can just drop files instead of updating any indexes when you delete a large block of old data. Dropping a column family is a very efficient operation. However, column families are a fairly new feature and we had another use case for shards.</p> <p>Organizing data into shards meant that it could be moved within a cluster without having to examine billions of keys. At the time of this writing I don’t think it’s possible to move a column family in one RocksDB to another. Old shards are typically cold for writes so moving them around would be cheap and easy and we’d have the added benefit of having a spot in the keyspace that is cold for writes so it would be easier to do consistency checks later.</p> <p>The organization of data into shards worked great for a little while until a large amount of data went into InfluxDB. For our users that had 6 months or a year of data in large databases, they would run out of file handles. LevelDB splits the data out over many small files. Having dozens or hundreds of these databases open in a single process ended up creating a big problem. It’s not something we found with a large number of users, but for anyone that was stressing the database to its limits, they were hitting this problem and we had no fix for it. There were simply too many file handles open.</p> <h3 id=\"boltdb-and-mmap-b-trees\">BoltDB and mmap B+Trees</h3> <p>After struggling with LevelDB and its variants for a year we decided to move over to BoltDB, a pure Golang database heavily inspired by LMDB, a mmap B+Tree database written in C. It has the same API semantics as LevelDB: a key value store where the keyspace is ordered. Many of our users were surprised about this move after we posted some early testing results of the LevelDB variants vs. LMDB (a mmap B+Tree) that showed RocksDB as the best performer.</p> <p>However, there were other considerations that went into this decision outside of the pure write performance. At this point our most important goal was to get to something stable that could be run in production and backed up. BoltDB also had the advantage of being written in pure Go, which simplified our build chain immensely and made it easy to build for other OSes and platforms like Windows and ARM.</p> <p>The biggest win for us was that BoltDB used a single file as the database. At this point our most common source of bug reports were from people running out of file handles. Bolt solved the hot backup problem, made it easy to move a shard from one server to another, and the file limit problems all at the same time.</p> <p>We were willing to take a hit on write throughput if it meant that we’d have a system that was more reliable and stable that we could build on. Our reasoning was that for anyone pushing really big write loads, they’d be running a cluster anyway.</p> <p>We released versions 0.9.0 to 0.9.2 based on BoltDB. From a development perspective it was delightful. Clean API, fast and easy to build in our Go project, and reliable. However, after running for a while we found a big problem with write throughput falling over. After the database got to a certain size (over a few GB) writes would start spiking IOPS.</p> <p>Some of our users were able to get past this by putting it on big hardware with near unlimited IOPS. However, most of our users are on VMs with limited resources in the cloud. We had to figure out a way to reduce the impact of writing a bunch of points into hundreds of thousands of series at a time.</p> <p>With the 0.9.3 and 0.9.4 releases our plan was to put a write ahead log in front of Bolt. That way we could reduce the number of random insertions into the keyspace. Instead, we’d buffer up multiple writes at once that were next to each other and then flush them. However, that only served to delay the problem. High IOPS still became an issue and it showed up very quickly for anyone operating at even moderate work loads.</p> <p>However, my experience building the first WAL implementation in front of Bolt gave me the confidence I needed that the write problem could be solved. The performance of the WAL itself was fantastic, the index simply couldn’t keep up. At this point I started thinking again about how we could create something similar to an LSM Tree that could keep up with our write load.</p> <h3 id=\"the-new-influxdb-storage-engine-and-lsm-refined\">The new InfluxDB storage engine and LSM refined</h3> <p>The new InfluxDB storage engine looks very similar to a LSM Tree. It has a write ahead log, a collection of data files which are read-only indexes similar in structure to SSTables in an LSM Tree, and a few other files that keep compressed metadata.</p> <p>InfluxDB will create a shard for each block of time. For example, if you have a retention policy with an unlimited duration, shards will get created for each 7 day block of time. Each of these shards maps to an underlying storage engine database. Each of these databases has its own WAL, compressed metadata that describe which series are in the index, and the index data files.</p> <p>We’ll dig into each of these parts of the storage engine.</p> <h4 id=\"the-write-ahead-log\">The Write Ahead Log</h4> <p>The WAL is organized as a bunch of files that look like _000001.wal. The file numbers are monotonically increasing. When a file reaches 2MB in size, it is closed and a new one is opened. There is a single write lock into the WAL that keeps many goroutines from trying to write to the file at once. The lock is only obtained to write the already serialized and compressed bytes to the file.</p> <p>When a write comes in with points and optionally new series and fields defined, they are serialized, compressed using Snappy, and written to a WAL file. The file is fsync’d and the data added to an in memory index before a success is returned. This means that batching points together will be required to achieve high throughput performance.</p> <p>Each entry in the WAL follows a TLV standard with a single byte representing what the type of entry it is (points, new fields, new series, or delete), a 4 byte uint32 for the length of the compressed block, and then the compressed block.</p> <p>It’s possible to disable the persistence of the WAL, instead relying on a regular flush to the index. This would result in the best possible performance at the cost of opening up a window of data loss for unflushed writes.</p> <p>The WAL keeps an in memory cache of all data points that are written to it. The points are organized by the key, which is the measurement, tagset, and unique field. Each field is kept as its own time ordered range. The data isn’t compressed while in memory.</p> <p>Queries to the storage engine will merge data from the WAL with data from the index. The cache uses a read-write lock to enable many goroutines to access the cache. When a query happens a copy of the data is made from the cache to be processed by the query engine. This way writes that come in while a query is happening won’t change the result.</p> <p>Deletes sent to the WAL will clear out the cache for the given key, persist in the WAL file and tell the index to keep a tombstone in memory.</p> <p>The WAL exposes a few controls for flush behavior. The two most important controls are the memory limits. There is a lower bound, which will trigger a flush to the index. There is also an upper bound limit at which the WAL will start rejecting writes. This is useful if the index gets backed up flushing and we need to apply back pressure to clients writing data. It also ensures that we can keep from running out of memory if the WAL gets too big. The checks for memory thresholds occur on every write.</p> <p>The other flush controls are time based. The idle flush will have the WAL flush to the index if it hasn’t received a write within a given amount of time. The second control is to have the WAL periodically flush even when taking writes. For instance if you disabled persistence and set this to 30 seconds, you would have a 30 second window of potential data loss.</p> <p>Finally, the WAL is fully flushed on startup. If it was particularly backed up this could cause the startup time to take a bit.</p> <h4 id=\"data-files-like-sstables\">Data Files (like SSTables)</h4> <p>The storage index is a collection of read-only index files that get memory mapped. The structure of these files looks very similar to an SSTable in LevelDB or other LSM Tree variants. Each file has a structure that looks like this:</p> <p><img src=\"https://docs.influxdata.com/img/influxdb/tsm/data_file.png\" alt=\"data file\"></p> <p>The magic number at the beginning identifies the file as a data file for the PD1 storage engine. We’ll go into the details of the data and index blocks in a bit. The min time and max time are nanosecond epochs of the minimum and maximum timestamps of the points in any of the data blocks of the file. Finally, the file ends with a uint32 of the number of series contained in the data file.</p> <p>The structure of each data block looks like this:</p> <p><img src=\"https://docs.influxdata.com/img/influxdb/tsm/data_block.png\" alt=\"data block\"></p> <p>The ID is the identifier for the series and field. IDs are generated by taking the fnv64-a hash of the series key (measurement name + tagset) and the field name. We’ll discuss how we handle ID hash collisions later in this doc.</p> <p>The compressed block uses a compression scheme depending on the type of the field. Details on compression are covered later, but for now it’s important to know that the first 8 bytes of every compressed block has the minimum timestamp for all values in the block. By default, up to 1000 values are encoded in a block.</p> <p>The data blocks are always arranged in sorted order by the IDs and if there are multiple blocks for a given ID in a single file, those blocks will be arranged by time. The timestamps for values in the blocks for the same ID are increasing and non-overlapping.</p> <p>The index block in a data file looks like this:</p> <p><img src=\"https://docs.influxdata.com/img/influxdb/tsm/index_block.png\" alt=\"index block\"></p> <p>The index block contains all the IDs that have data blocks in the file and the position of their first block. All IDs are in sorted order, which is important later for finding the starting position of an ID’s block. Note that the starting position is a uint32 value, which means that data files are limited in size to a maximum of 4GB.</p> <h4 id=\"writes-to-the-index\">Writes to the Index</h4> <p>The storage index keeps a collection of data files. These files have non-overlapping time ranges. The index keeps a mapping in memory of the memory mapped data files that exist and their min and max times. The files are kept in a sorted array by their time.</p> <p>The first thing that happens when the WAL flushes a block of writes to the index is mapping the keys of the series and fields that have values to their unique ID. Most IDs should simply be the fnv64-a hash of the series key and the field name. However, we need to ensure that we keep track of any collisions.</p> <p>The index keeps a file called “names” that keeps a compressed JSON map of key to ID. When a flush happens we ensure that any of the keys coming in have an ID in the map that is equal to the fnv64-a hash. If it is present in the map and the same we move on. If it isn’t present in the map, we add it. If the ID is taken already, we keep track of the collision.</p> <p>If there are any new keys, we marshall, compress and write a new names file. The old file is removed once the new one has been written and closed. If there are any collisions, we write those into a collisions file. This is read on startup of the index so collisions can be kept in memory. We’ll talk more about how collisions are handled later.</p> <p>This name decoding scheme is fairly inefficient since we’re marshaling this entire map on every WAL flush. The alternative is to keep it in memory, but that is also expensive as there are many indexes (one per shard) in a running InfluxDB process. We could also just keep the map in memory while a given shard is hot for writes (i.e. its time range is current for now).</p> <p>In testing the cost of decoding and rewriting the names file hasn’t been a big problem and the simplicity of the scheme makes it a bit easier to work with. We’ve been testing up to 500k unique keys at this point.</p> <p>After the ID resolution is done the write will be broken up by the time of the individual data points. If no data file exists for any of the time ranges, a new one will be created and written. If a data file already exists for the given time range and it is less than a configurable size (5MB by default), then the data file will be read in and the new values merged with it and a new data file will be created.</p> <p>While this new file is getting written, all queries will hit the old data file and continue to use the WAL cache, which won’t be cleared until the write to the index is confirmed. Once the new file is written, the engine will obtain a write lock to replace the old data file with the new data file in the in memory index, remove the old file from the filesystem and then return success back to the WAL. We’ll talk about how recovery is handled in a later section of this document.</p> <p>Because of this structure, multiple ranges of time can be written simultaneously, which enables us to perform compactions on old time ranges while still accepting writes for current time ranges. We’ll cover the details of compaction in a later section. Writes will only block reads during the window of time that the in-memory sorted list of data files is modified (an operation that takes less than a few microseconds).</p> <p>The index is optimized for the append only workload that is most common in InfluxDB usage. It’s possible to update old data points, but that operation can be expensive because data files will need to be rewritten. However, historical backfill when initially setting up a database is very efficient and has a workload that looks like filling in new data as long as the backfill is done in time ascending order.</p> <p>Even during normal operation, data files continually get rewritten as the WAL flushes to the index. However, the data files are generally small at that point so the cost of rewriting them is negligible. Later, compactions will combine older data files into larger ones.</p> <h4 id=\"compaction\">Compaction</h4> <p>In the background the index continually checks for old data files that can be combined together. By default, any files with a time older than 30 minutes will be combined together. The process is the same as during a write, a new file is created, the two data files are read in together and the merged stream of sorted IDs and blocks are written out as they are read from the two files.</p> <p>Once the new file is written, the index obtains the write lock to modify the collection of data files and removes the older two files. The compaction process will only attempt to compact data files that are less than a configurable size (500MB by default and up to a max of 1GB).</p> <h4 id=\"updates\">Updates</h4> <p>Updates are held first in the WAL. If there are a number of updates to the same range, ideally the WAL will buffer them together before it gets flushed to the index. The index will simply rewrite the data file that contains the updated data. If the updates are to a recent block of time, the updates will be relatively inexpensive.</p> <h4 id=\"deletes\">Deletes</h4> <p>Deletes are handled in two phases. First, when the WAL gets a delete it will persist it in the log, clear its cache of that series, and tell the index to keep an in-memory tombstone of the delete. When a query comes into the index, it will check it against any of the in-memory tombstones and act accordingly.</p> <p>Later, when the WAL flushes to the index, it will include the delete and any data points that came into the series after the delete. The index will handle the delete first by rewriting any data files that contain that series to remove them. Then it will persist any new data (for this series or any others), remove the tombstone from memory and return success to the WAL.</p> <h4 id=\"metadata\">Metadata</h4> <p>Metadata for all the series and fields in the shard are kept in 2 files named “names” and “fields”. These two files contain a single Snappy compressed block of the serialized JSON of any names or fields in the shard.</p> <p>This means that any flush from the WAL that contains either new fields or series, will force the old file to be read in, decompressed, marshalled, merged with the new series and a new file to be written. Because the WAL buffers these new definitions, the cost is generally negligible. However, as the number of series in a shard gets very high it can become expensive. We’ve tested up to 500k unique series and this scheme still works well.</p> <h4 id=\"seeking-and-reading-a-series\">Seeking and Reading a Series</h4> <p>When a seek comes into the storage engine, it is a seek to a given time associated with a specific series key and field. First, we do a binary search on the data files to find the file that has a time range that matches the time we’re seeking to.</p> <p>Once we have the data file we need to find the position in the file for the block that contains the given time. First, we check the collisions map to see if this seek is to an ID that doesn’t match up with the hash ID. If it’s not there, we do a fnv64-a hash against the series key and field name to get its ID.</p> <p>Now that we have the ID we check the end of the file for the series count. Remember that it’s memory mapped so this should be fast. Now that we know the count of the series, we know which byte positions in the file contain the index of IDs to starting position.</p> <p>We’ll now do a binary search against the section of the index to find the ID and its position. Remember that the index is sorted by increasing IDs. This operation is very efficient and the index for a data file is in the memory map.</p> <p>Now that we have the starting position we can look at the first block’s timestamp and the next block’s timestamp. We traverse the blocks until we find the one that matches up with the timestamp we’re seeking to. This means we’re making jumps in the file by the length of the compressed blocks.</p> <p>In practice, a shard will have the data for a day or 7 days. For people with regular time series that sample every 10 seconds, they’ll have data files that contain either 8,640 data points (1 day) for each series or that times 7. That means that at most we’ll end up making 56 jumps to the last block. In practice it’s likely that 7 days of data would be split across multiple data files, reducing the number of blocks for a given series in a file.</p> <p>Once we’ve found the matching block, we decompress it and go to the specific point. As the query engine traverses the series by getting subsequent points, we go through that block, then decompress the next one, then jump to the next data file, until we ultimately reach the end of the shard.</p> <h4 id=\"compression\">Compression</h4> <p>Each block is compressed to reduce storage space and disk IO when querying. A block contains the timestamps and values for a given series and field. Each block has one byte header, followed by the compressed timestamps and then the compressed values. The timestamps and values are stored separately as two compressed parts using different encodings depending on the data type and its shape. Storing them independently allows timestamp encoding to be used with different field types more easily. It also allows for different encodings to be used depending on the type of the data. For example, some points may be able to use run-length encoding whereas other may not. Each value type also contains a 1 byte header indicating the type of compression for the remaining bytes. The four high bits store the compression type and the four low bits are used by the encoder if needed.</p> <p><img src=\"https://docs.influxdata.com/img/influxdb/tsm/compressed_block.png\" alt=\"compressed block\"></p> <p>Timestamp encoding is adaptive and based on the structure of the timestamps that are encoded. It uses a combination of delta encoding, scaling and compression using simple8b, run-length encoding as well as falling back to no compression if needed.</p> <p>Timestamp resolution can be as granular as a nanosecond and, uncompressed, require 8 bytes to store. During encoding, the values are first delta-encoded. The first value is the starting timestamp and subsequent values are the differences from the prior value. This usually converts the values into much smaller integers that are easier to compress. Many timestamps are also monotonically increasing and fall on even boundaries of time such as every 10s. When timestamps have this structure, they are scaled by the largest common divisor that Zis also a factor of 10. This has the effect of converting very large integer deltas into smaller ones that compress even better.</p> <p>Using these adjusted values, if all the deltas are the same, the time range is stored using run-length encoding. If run-length encoding is not possible and all values are less than 1 &lt;&lt; 60 - 1 (~36.5 yrs in nanosecond resolution), then the timestamps are encoded using the simple8b encoding which is a 64bit word-aligned integer encoding. This encoding packs up multiple integers into a single 64bit word. If any value exceeds the maximum values, the deltas are stored uncompressed using 8 bytes each for the block. Future encodings will use a patched scheme such as Patched Frame-Of-Reference (PFOR) to handle outlier more effectively.</p> <p>Floats are encoded using an implementation of the Facebook Gorilla paper. This encoding XORs consecutive values together which produces a small result when the values are close together. The delta is then stored using control bits to indicate how many leading and trailing zero are in the XOR value. Our version removes the timestamp encoding, as described in paper, and only encodes the float values.</p> <p>Integer encoding uses two different strategies depending on the range of values in the uncompressed data. Encoded values are first encoded using zig zag encoding which is also used for signed integers in Google Protocol Buffers. This interleaves positive and negative integers across a range of positive integers.</p> <p>For example, [-2,-1,0,1] becomes [3,1,0,2]. See <a href=\"https://developers.google.com/protocol-buffers/docs/encoding?hl=en#signed-integers\">https://developers.google.com/protocol-buffers/docs/encoding?hl=en#signed-integers</a> for more information.</p> <p>If all the zig zag encoded values less than 1 &lt;&lt; 60 - 1, they are compressed using the simple8b encoding. If any values is larger than the maximum value, then values are stored uncompressed in the block.</p> <p>Booleans are encoded using a simple bit packing strategy where each boolean uses 1 bit. The number of booleans encoded is stored using variable-byte encoding at the beginning of the block.</p> <p>Strings are encoding using Snappy compression. Each string is packed next each other in order and compressed as one larger block.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/storage_engine/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/concepts/storage_engine/</a>\n  </p>\n</div>\n","influxdb/v0.13/troubleshooting/index":"<h1>Troubleshooting</h1>     <h2 id=\"frequently-encountered-issues-influxdb-v0-13-troubleshooting-frequently-encountered-issues\"><a href=\"frequently_encountered_issues/index\">Frequently Encountered Issues</a></h2> <p>This page addresses frequent sources of confusion and places where InfluxDB behaves in an unexpected way relative to other database systems. Where applicable, it links to outstanding issues on GitHub.</p> <h2 id=\"system-monitoring-influxdb-v0-13-troubleshooting-statistics\"><a href=\"statistics/index\">System Monitoring</a></h2> <p>System Monitoring means all statistical and diagnostic information made available to the user of the InfluxDB system, about the system itself. Its purpose is to assist with troubleshooting and performance analysis of the database itself.</p> <h2 id=\"query-management-influxdb-v0-13-troubleshooting-query-management\"><a href=\"query_management/index\">Query management</a></h2> <p>With InfluxDB’s query management features, users are able to identify currently-running queries and have the ability to kill queries that are overloading their system. Additionally, users can prevent and halt the execution of inefficient queries with several configuration settings.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/troubleshooting/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/troubleshooting/</a>\n  </p>\n</div>\n","influxdb/v0.13/troubleshooting/frequently_encountered_issues/index":"<h1>Frequently Encountered Issues</h1>     <p>This page addresses frequent sources of confusion and places where InfluxDB behaves in an unexpected way relative to other database systems. Where applicable, it links to outstanding issues on GitHub.</p> <p><strong>Querying data</strong></p> <ul> <li>\n<a href=\"index#understanding-the-time-intervals-returned-from-group-by-time-queries\">Understanding the time intervals returned from <code>GROUP BY time()</code> queries</a><br>\n</li> <li>\n<a href=\"index#querying-after-now\">Querying after <code>now()</code></a><br>\n</li> <li>\n<a href=\"index#querying-a-time-range-that-spans-epoch-0\">Querying a time range that spans epoch 0</a><br>\n</li> <li>\n<a href=\"index#querying-with-booleans\">Querying with booleans</a><br>\n</li> <li><a href=\"index#working-with-really-big-or-really-small-integers\">Working with really big or really small integers</a></li> <li>\n<a href=\"index#doing-math-on-timestamps\">Doing math on timestamps</a><br>\n</li> <li>\n<a href=\"index#getting-an-unexpected-epoch-0-timestamp-in-query-returns\">Getting an unexpected epoch 0 timestamp in query returns</a><br>\n</li> <li>\n<a href=\"index#getting-large-query-returns-in-batches-when-using-the-http-api\">Getting large query returns in batches when using the HTTP API</a><br>\n</li> <li><a href=\"index#getting-the-expected-identifier-error-unexpectedly\">Getting the <code>expected identifier</code> error, unexpectedly</a></li> <li>\n<a href=\"index#identifying-write-precision-from-returned-timestamps\">Identifying write precision from returned timestamps</a><br>\n</li> <li>\n<a href=\"index#single-quoting-and-double-quoting-in-queries\">Single quoting and double quoting in queries</a><br>\n</li> <li><a href=\"index#missing-data-after-creating-a-new-default-retention-policy\">Missing data after creating a new <code>DEFAULT</code> retention policy</a></li> </ul> <p><strong>Writing data</strong></p> <ul> <li>\n<a href=\"index#writing-integers\">Writing integers</a><br>\n</li> <li>\n<a href=\"index#writing-duplicate-points\">Writing duplicate points</a><br>\n</li> <li><a href=\"index#getting-an-unexpected-error-when-sending-data-over-the-http-api\">Getting an unexpected error when sending data over the HTTP API</a></li> <li>\n<a href=\"index#words-and-characters-to-avoid\">Words and characters to avoid</a><br>\n</li> <li>\n<a href=\"index#single-quoting-and-double-quoting-when-writing-data\">Single quoting and double quoting when writing data</a><br>\n</li> </ul> <p><strong>Administration</strong></p> <ul> <li><a href=\"index#single-quoting-the-password-string\">Single quoting the password string</a></li> <li>\n<a href=\"index#escaping-the-single-quote-in-a-password\">Escaping the single quote in a password</a><br>\n</li> <li>\n<a href=\"index#identifying-your-version-of-influxdb\">Identifying your version of InfluxDB</a><br>\n</li> <li><a href=\"index#data-aren-t-dropped-after-altering-a-retention-policy\">Data aren’t dropped after altering a retention policy</a></li> </ul> <h1 id=\"querying-data\">Querying data</h1> <h2 id=\"understanding-the-time-intervals-returned-from-group-by-time-queries\">Understanding the time intervals returned from <code>GROUP BY time()</code> queries</h2> <p>With some <code>GROUP BY time()</code> queries, the returned time intervals may not reflect the time range specified in the <code>WHERE</code> clause. In the example below the first <a href=\"../../concepts/glossary/index#timestamp\">timestamp</a> in the results occurs before the lower bound of the query:</p> <p>Query with a two day <code>GROUP BY time()</code> interval: </p>\n<pre data-language=\"sh\">\n&gt; SELECT count(water_level) FROM h2o_feet WHERE time &gt;= <u><b>‘2015-08-20T00:00:00Z’</b></u> AND time &lt;= ‘2015-08-24T00:00:00Z’ AND location = ‘santa_monica’ GROUP BY time(2d)\n</pre> <p>Results:</p> <pre data-language=\"sh\">name: h2o_feet\n--------------\ntime                     count\n<u><b>2015-08-19T00:00:00Z</b></u>     240\n2015-08-21T00:00:00Z     480\n2015-08-23T00:00:00Z     241\n</pre> <p>InfluxDB queries the <code>GROUP BY time()</code> intervals that fall within the <code>WHERE time</code> clause. Default <code>GROUP BY time()</code> intervals fall on rounded calendar time boundaries. Because they’re rounded time boundaries, the start and end timestamps may appear to include more data than those covered by the query’s <code>WHERE time</code> clause.</p> <p>For the example above, InfluxDB works with two day intervals based on round number calendar days. The rounded two day buckets in August are as follows (explanation continues below):</p> <pre>August 1st-2nd\nAugust 3rd-4th\n[...]\nAugust 19th-20th\nAugust 21st-22nd\nAugust 23rd-24th\n[...]\n</pre> <p>Because InfluxDB groups together August 19th and August 20th by default, August 19th is the first timestamp to appear in the results despite not being within the query’s time range. The number in the <code>count</code> column, however, only includes data that occur on or after August 20th as that is the time range specified by the query’s <code>WHERE</code> clause.</p> <p>Users may offset the default rounded calendar time boundaries by including an <a href=\"../../query_language/data_exploration/index#configured-group-by-time-boundaries\">offset interval</a> in their query.</p> <h2 id=\"querying-after-now\">Querying after <code>now()</code>\n</h2> <p>By default, InfluxDB uses <code>now()</code> (the current nanosecond timestamp of the node that is processing the query) as the upper bound in queries. You must provide explicit directions in the <code>WHERE</code> clause to query points that occur after <code>now()</code>.</p> <p>The first query below asks InfluxDB to return everything from <code>hillvalley</code> that occurs between epoch 0 (<code>1970-01-01T00:00:00Z</code>) and <code>now()</code>. The second query asks InfluxDB to return everything from <code>hillvalley</code> that occurs between epoch 0 and 1,000 days from <code>now()</code>.</p> <p><code>SELECT * FROM hillvalley</code><br> <code>SELECT * FROM hillvalley WHERE time &lt; now() + 1000d</code></p> <h2 id=\"querying-a-time-range-that-spans-epoch-0\">Querying a time range that spans epoch 0</h2> <p>Currently, InfluxDB can return results for queries that cover either the time range before epoch 0 or the time range after epoch 0, not both. A query with a time range that spans epoch 0 returns partial results.</p> \n<dt> <a href=\"https://github.com/influxdb/influxdb/issues/2703\">GitHub Issue #2703</a> </dt> <h2 id=\"querying-with-booleans\">Querying with booleans</h2> <p>Acceptable boolean syntax differs for data writes and data queries.</p> <table> <thead> <tr> <th>Boolean syntax</th> <th>Writes</th> <th>Queries</th> </tr> </thead> <tbody> <tr> <td>\n<code>t</code>,<code>f</code>\n</td> <td>👍</td> <td>❌</td> </tr> <tr> <td>\n<code>T</code>,<code>F</code>\n</td> <td>👍</td> <td>❌</td> </tr> <tr> <td>\n<code>true</code>,<code>false</code>\n</td> <td>👍</td> <td>👍</td> </tr> <tr> <td>\n<code>True</code>,<code>False</code>\n</td> <td>👍</td> <td>👍</td> </tr> <tr> <td>\n<code>TRUE</code>,<code>FALSE</code>\n</td> <td>👍</td> <td>👍</td> </tr> </tbody> </table> <p>For example, <code>SELECT * FROM hamlet WHERE bool=True</code> returns all points with <code>bool</code> set to <code>TRUE</code>, but <code>SELECT * FROM hamlet WHERE bool=T</code> returns all points with<code>bool</code> set to <code>false</code>.</p> \n<dt> <a href=\"https://github.com/influxdb/influxdb/issues/3939\">GitHub Issue #3939</a> </dt> <h2 id=\"working-with-really-big-or-really-small-integers\">Working with really big or really small integers</h2> <p>InfluxDB stores all integers as signed int64 data types. The minimum and maximum valid values for int64 are <code>-9023372036854775808</code> and <code>9023372036854775807</code>. See <a href=\"http://golang.org/pkg/builtin/#int64\">Go builtins</a> for more information.</p> <p>Values close to but within those limits may lead to unexpected results; some functions and operators convert the int64 data type to float64 during calculation which can cause overflow issues.</p> <h2 id=\"doing-math-on-timestamps\">Doing math on timestamps</h2> <p>Currently, it is not possible to execute mathematical operators or functions against timestamp values in InfluxDB. All time calculations must be carried out by the client receiving the query results.</p> <h2 id=\"getting-an-unexpected-epoch-0-timestamp-in-query-returns\">Getting an unexpected epoch 0 timestamp in query returns</h2> <p>In InfluxDB, epoch 0 (<code>1970-01-01T00:00:00Z</code>) is often used as a null timestamp equivalent. If you request a query that has no timestamp to return, such as an aggregation function with an unbounded time range, InfluxDB returns epoch 0 as the timestamp.</p> <h2 id=\"getting-large-query-returns-in-batches-when-using-the-http-api\">Getting large query returns in batches when using the HTTP API</h2> <p>InfluxDB returns large query results in batches of 10,000 points unless you use the query string parameter <code>chunk_size</code> to explicitly set the batch size. For example, get results in batches of 20,000 points with:</p> <p><code>curl -G 'http://localhost:8086/query' --data-urlencode \"db=deluge\" --data-urlencode \"chunk_size=20000\" --data-urlencode \"q=SELECT * FROM liters\"</code></p> \n<dt> See <a href=\"https://github.com/influxdb/influxdb/issues/3242\">GitHub Issue #3242</a> for more information on the challenges that this can cause, especially with Grafana visualization. </dt> <h2 id=\"getting-the-expected-identifier-error-unexpectedly\">Getting the <code>expected identifier</code> error, unexpectedly</h2> <p>Receiving the error <code>ERR: error parsing query: found [WORD], expected identifier[, string, number, bool]</code> is often a gentle reminder that you forgot to include something in your query, as is the case in the following examples:</p> <ul> <li>\n<code>SELECT FROM logic WHERE rational = 5</code> should be <code>SELECT something FROM logic WHERE rational = 5</code><br>\n</li> <li>\n<code>SELECT * FROM WHERE rational = 5</code> should be <code>SELECT * FROM logic WHERE rational = 5</code>\n</li> </ul> <p>In other cases, your query seems complete but you receive the same error:</p> <ul> <li>\n<code>SELECT field FROM why</code><br>\n</li> <li>\n<code>SELECT * FROM why WHERE tag = '1'</code><br>\n</li> <li><code>SELECT * FROM grant WHERE why = 9</code></li> </ul> <p>In the last three queries, and in most unexpected <code>expected identifier</code> errors, at least one of the identifiers in the query is an InfluxQL keyword. Identifiers are database names, retention policy names, user names, measurement names, tag keys, and field keys. To successfully query data that use a keyword as an identifier enclose that identifier in double quotes, so the examples above become:</p> <ul> <li>\n<code>SELECT \"field\" FROM why</code><br>\n</li> <li>\n<code>SELECT * FROM why WHERE \"tag\" = '1'</code><br>\n</li> <li><code>SELECT * FROM \"grant\" WHERE why = 9</code></li> </ul> <p>While using double quotes is an acceptable workaround, we recommend that you avoid using InfluxQL keywords as identifiers for simplicity’s sake. The InfluxQL documentation has a comprehensive list of all <a href=\"https://github.com/influxdb/influxdb/blob/master/influxql/README.md#keywords\">InfluxQL keywords</a>.</p> <h2 id=\"identifying-write-precision-from-returned-timestamps\">Identifying write precision from returned timestamps</h2> <p>InfluxDB stores all timestamps as nanosecond values regardless of the write precision supplied. It is important to note that when returning query results, the database silently drops trailing zeros from timestamps which obscures the initial write precision.</p> <p>In the example below, the tags <code>precision_supplied</code> and <code>timestamp_supplied</code> show the time precision and timestamp that the user provided at the write. Because InfluxDB silently drops trailing zeros on returned timestamps, the write precision is not recognizable in the returned timestamps. <br></p> <pre data-language=\"bash\">name: trails\n-------------\ntime                  value\t precision_supplied  timestamp_supplied\n1970-01-01T01:00:00Z  3      n                   3600000000000\n1970-01-01T01:00:00Z  5      h                   1\n1970-01-01T02:00:00Z  4      n                   7200000000000\n1970-01-01T02:00:00Z  6      h                   2\n</pre> \n<dt> <a href=\"https://github.com/influxdb/influxdb/issues/2977\">GitHub Issue #2977</a> </dt> <h2 id=\"single-quoting-and-double-quoting-in-queries\">Single quoting and double quoting in queries</h2> <p>Single quote string values (for example, tag values) but do not single quote identifiers (database names, retention policy names, user names, measurement names, tag keys, and field keys).</p> <p>Double quote identifiers if they start with a digit, contain characters other than <code>[A-z,0-9,_]</code>, or if they are an <a href=\"https://github.com/influxdb/influxdb/blob/master/influxql/README.md#keywords\">InfluxQL keyword</a>. You can double quote identifiers even if they don’t fall into one of those categories but it isn’t necessary.</p> <p>Examples:</p> <p>Yes: <code>SELECT bikes_available FROM bikes WHERE station_id='9'</code></p> <p>Yes: <code>SELECT \"bikes_available\" FROM \"bikes\" WHERE \"station_id\"='9'</code></p> <p>Yes: <code>SELECT * from \"cr@zy\" where \"p^e\"='2'</code></p> <p>No: <code>SELECT 'bikes_available' FROM 'bikes' WHERE 'station_id'=\"9\"</code></p> <p>No: <code>SELECT * from cr@zy where p^e='2'</code></p> <p>Single quote date time strings. InfluxDB returns an error (<code>ERR: invalid\noperation: time and *influxql.VarRef are not compatible</code>) if you double quote a date time string.</p> <p>Examples:</p> <p>Yes: <code>SELECT water_level FROM h2o_feet WHERE time &gt; '2015-08-18T23:00:01.232000000Z' AND time &lt; '2015-09-19'</code></p> <p>No: <code>SELECT water_level FROM h2o_feet WHERE time &gt; \"2015-08-18T23:00:01.232000000Z\" AND time &lt; \"2015-09-19\"</code></p> <p>See <a href=\"../../query_language/data_exploration/index#time-syntax-in-queries\">Data Exploration</a> for more on time syntax in queries.</p> <h2 id=\"missing-data-after-creating-a-new-default-retention-policy\">Missing data after creating a new <code>DEFAULT</code> retention policy</h2> <p>When you create a new <code>DEFAULT</code> retention policy (RP) on a database, the data written to the old <code>DEFAULT</code> RP remain in the old RP. Queries that do not specify an RP automatically query the new <code>DEFAULT</code> RP so the old data may appear to be missing. To query the old data you must fully qualify the relevant data in the query.</p> <p>Example:</p> <p>All of the data in the measurement <code>fleeting</code> fall under the <code>DEFAULT</code> RP called <code>one_hour</code>:</p> <pre data-language=\"bash\">&gt; SELECT count(flounders) FROM fleeting\nname: fleeting\n--------------\ntime\t\t\t               count\n1970-01-01T00:00:00Z\t 8\n</pre> <p>We <a href=\"../../query_language/database_management/index#create-retention-policies-with-create-retention-policy\">create</a> a new <code>DEFAULT</code> RP (<code>two_hour</code>) and perform the same query:</p> <pre data-language=\"bash\">&gt; SELECT count(flounders) FROM fleeting\n&gt;\n</pre> <p>To query the old data, we must specify the old <code>DEFAULT</code> RP by fully qualifying <code>fleeting</code>:</p> <pre data-language=\"bash\">&gt; SELECT count(flounders) FROM fish.one_hour.fleeting\nname: fleeting\n--------------\ntime\t\t\t               count\n1970-01-01T00:00:00Z\t 8\n</pre> <h1 id=\"writing-data\">Writing data</h1> <h2 id=\"writing-integers\">Writing integers</h2> <p>Add a trailing <code>i</code> to the end of the field value when writing an integer. If you do not provide the <code>i</code>, InfluxDB will treat the field value as a float.</p> <p>Writes an integer: <code>value=100i</code><br> Writes a float: <code>value=100</code></p> <h2 id=\"writing-duplicate-points\">Writing duplicate points</h2> <p>In InfluxDB 0.13 a point is uniquely identified by the measurement name, <a href=\"../../concepts/glossary/index#tag-set\">tag set</a>, and timestamp. If you submit a new point with the same measurement, tag set, and timestamp as an existing point, the field set becomes the union of the old field set and the new field set, where any ties go to the new field set. This is the intended behavior.</p> <p>For example:</p> <p>Old point: <code>cpu_load,hostname=server02,az=us_west val_1=24.5,val_2=7 1234567890000000</code></p> <p>New point: <code>cpu_load,hostname=server02,az=us_west val_1=5.24 1234567890000000</code></p> <p>After you submit the new point, InfluxDB overwrites <code>val_1</code> with the new field value and leaves the field <code>val_2</code> alone:</p> <pre>&gt; SELECT * FROM cpu_load WHERE time = 1234567890000000\nname: cpu_load\n--------------\ntime\t\t\t                  az\t      hostname\t val_1\t val_2\n1970-01-15T06:56:07.89Z\t us_west\t server02\t 5.24\t  7\n</pre> <p>To store both points:</p> <ul> <li>\n<p>Introduce an arbitrary new tag to enforce uniqueness.</p> <p>Old point: <code>cpu_load,hostname=server02,az=us_west,uniq=1 val_1=24.5,val_2=7 1234567890000000</code></p> <p>New point: <code>cpu_load,hostname=server02,az=us_west,uniq=2 val_1=5.24 1234567890000000</code></p> <p>After writing the new point to InfluxDB:</p> <pre>&gt; SELECT * FROM cpu_load WHERE time = 1234567890000000\nname: cpu_load\n--------------\ntime                      az       hostname   uniq    val_1   val_2\n1970-01-15T06:56:07.89Z   us_west  server02   1       24.5    7\n1970-01-15T06:56:07.89Z   us_west  server02   2       5.24\n</pre>\n</li> <li>\n<p>Increment the timestamp by a nanosecond.</p> <p>Old point: <code>cpu_load,hostname=server02,az=us_west val_1=24.5,val_2=7 1234567890000000</code></p> <p>New point: <code>cpu_load,hostname=server02,az=us_west val_1=5.24 1234567890000001</code></p> <p>After writing the new point to InfluxDB:</p> <pre>&gt; SELECT * FROM cpu_load WHERE time &gt;= 1234567890000000 and time &lt;= 1234567890000001\nname: cpu_load\n--------------\ntime                             az       hostname   val_1  val_2\n1970-01-15T06:56:07.89Z          us_west  server02   24.5    7\n1970-01-15T06:56:07.890000001Z   us_west  server02   5.24\n</pre>\n</li> </ul> <h2 id=\"getting-an-unexpected-error-when-sending-data-over-the-http-api\">Getting an unexpected error when sending data over the HTTP API</h2> <p>First, double check your <a href=\"../../write_protocols/line/index\">line protocol</a> syntax. Second, if you continue to receive errors along the lines of <code>bad timestamp</code> or <code>unable to parse</code>, verify that your newline character is line feed (<code>\\n</code>, which is ASCII <code>0x0A</code>). InfluxDB’s line protocol relies on <code>\\n</code> to indicate the end of a line and the beginning of a new line; files or data that use a newline character other than <code>\\n</code> will encounter parsing issues. Convert the newline character and try sending the data again.</p> <blockquote> <p><strong>Note:</strong> If you generated your data file on a Windows machine, Windows uses carriage return and line feed (<code>\\r\\n</code>) as the newline character.</p> </blockquote> <h2 id=\"words-and-characters-to-avoid\">Words and characters to avoid</h2> <p>If you use any of the <a href=\"https://github.com/influxdb/influxdb/blob/master/influxql/README.md#keywords\">InfluxQL keywords</a> as an identifier you will need to double quote that identifier in every query. This can lead to <a href=\"index#getting-the-expected-identifier-error-unexpectedly\">non-intuitive errors</a>. Identifiers are database names, retention policy names, user names, measurement names, tag keys, and field keys.</p> <p>To keep regular expressions and quoting simple, avoid using the following characters in identifiers:</p> <p><code>\\</code> backslash<br> <code>^</code> circumflex accent<br> <code>$</code> dollar sign<br> <code>'</code> single quotation mark<br> <code>\"</code> double quotation mark<br> <code>,</code> comma</p> <h2 id=\"single-quoting-and-double-quoting-when-writing-data\">Single quoting and double quoting when writing data</h2> <ul> <li>Avoid single quoting and double quoting identifiers when writing data via the line protocol; see the examples below for how writing identifiers with quotes can complicate queries. Identifiers are database names, retention policy names, user names, measurement names, tag keys, and field keys. <br> <br> Write with a double-quoted measurement: <code>INSERT \"bikes\" bikes_available=3</code><br> Applicable query: <code>SELECT * FROM \"\\\"bikes\\\"\"</code> <br> <br> Write with a single-quoted measurement: <code>INSERT 'bikes' bikes_available=3</code><br> Applicable query: <code>SELECT * FROM \"\\'bikes\\'\"</code> <br> <br> Write with an unquoted measurement: <code>INSERT bikes bikes_available=3</code><br> Applicable query: <code>SELECT * FROM bikes</code> <br> <br>\n</li> <li>Double quote field values that are strings. <br> <br> Write: <code>INSERT bikes happiness=\"level 2\"</code><br> Applicable query: <code>SELECT * FROM bikes WHERE happiness='level 2'</code> <br> <br>\n</li> <li>Special characters should be escaped with a backslash and not placed in quotes. <br> <br> Write: <code>INSERT wacky va\\\"ue=4</code><br> Applicable query: <code>SELECT \"va\\\"ue\" FROM wacky</code>\n</li> </ul> <p>See the <a href=\"../../write_protocols/write_syntax/index\">Line Protocol Syntax</a> page for more information.</p> <h1 id=\"administration\">Administration</h1> <h2 id=\"single-quoting-the-password-string\">Single quoting the password string</h2> <p>The <code>CREATE USER &lt;user&gt; WITH PASSWORD '&lt;password&gt;'</code> query requires single quotation marks around the password string. Do not include the single quotes when authenticating requests.</p> <h2 id=\"escaping-the-single-quote-in-a-password\">Escaping the single quote in a password</h2> <p>For passwords that include a single quote, escape the single quote with a backslash both when creating the password and when authenticating requests.</p> <h2 id=\"identifying-your-version-of-influxdb\">Identifying your version of InfluxDB</h2> <p>There a number of ways to identify the version of InfluxDB that you’re using:</p> <ul> <li>Check the return when you <code>curl</code> the <code>/ping</code> endpoint. For example, if you’re using 0.13.0 <code>curl -i 'http://localhost:8086/ping'</code> returns:<br>\n</li> </ul> <p><code>HTTP/1.1 204 No Content</code><br> <code>Request-Id: 874101f6-e23e-11e5-8097-000000000000</code><br> ✨<code>X-Influxdb-Version: 0.13.0</code>✨<br> <code>Date: Fri, 04 Mar 2016 19:23:08 GMT</code></p> <ul> <li>Check the text that appears when you <a href=\"../../tools/shell/index\">launch</a> the CLI:</li> </ul> <p><code>Connected to http://localhost:8086</code>✨<code>version 0.13.0</code>✨<br> <code>InfluxDB shell 0.13.0</code></p> <ul> <li>Check the HTTP response in your logs:<br>\n</li> </ul> <p><code>[http] 2016/03/04 11:25:13 ::1 - - [04/Mar/2016:11:25:13 -0800] GET /query?db=&amp;epoch=ns&amp;q=show+databases HTTP/1.1 200 98 -</code> ✨<code>InfluxDBShell/0.13.0</code>✨<code>d16e7a83-e23e-11e5-80a7-000000000000 529.543µs</code></p> <h2 id=\"data-aren-t-dropped-after-altering-a-retention-policy\">Data aren’t dropped after altering a retention policy</h2> <p>After <a href=\"../../query_language/database_management/index#modify-retention-policies-with-alter-retention-policy\">shortening</a> the <code>DURATION</code> of a <a href=\"../../concepts/glossary/index#retention-policy-rp\">retention policy</a> (RP), you may notice that InfluxDB keeps some data that are older than the <code>DURATION</code> of the modified RP. This behavior is a result of the relationship between the time interval covered by a shard group and the <code>DURATION</code> of a retention policy.</p> <p>InfluxDB stores data in shard groups. A single shard group covers a specific time interval; InfluxDB determines that time interval by looking at the <code>DURATION</code> of the relevant RP. The table below outlines the relationship between the <code>DURATION</code> of an RP and the time interval of a shard group:</p> <table> <thead> <tr> <th>RP duration</th> <th>Shard group interval</th> </tr> </thead> <tbody> <tr> <td>&lt; 2 days</td> <td>1 hour</td> </tr> <tr> <td>&gt;= 2 days and &lt;= 6 months</td> <td>1 day</td> </tr> <tr> <td>&gt; 6 months</td> <td>7 days</td> </tr> </tbody> </table> <p>If you shorten the <code>DURATION</code> of an RP and the shard group interval also shrinks, InfluxDB may be forced to keep data that are older than the new <code>DURATION</code>. This happens because InfluxDB cannot divide the old, longer shard group into new, shorter shard groups; it must keep all of the data in the longer shard group even if only a small part of those data overlaps with the new <code>DURATION</code>.</p> <p><em>Example: Moving from an infinite RP to a three day RP</em></p> <p>Figure 1 shows the shard groups for our example database (<code>example_db</code>) after 11 days. The database uses the automatically generated <code>default</code> retention policy with an infinite (<code>INF</code>) <code>DURATION</code> so each shard group interval is seven days. On day 11, InfluxDB is no longer writing to <code>Shard Group 1</code> and <code>Shard Group 2</code> has four days worth of data:</p> <blockquote> <p><strong>Figure 1</strong> <img src=\"https://docs.influxdata.com/img/influxdb/fei/alter-rp-inf.png\" alt=\"Retention policy duration infinite\"></p> </blockquote> <p>On day 11, we notice that <code>example_db</code> is accruing data too fast; we want to delete, and keep deleting, all data older than three days. We do this by <a href=\"../../query_language/database_management/index#modify-retention-policies-with-alter-retention-policy\">altering</a> the retention policy: <br> <br></p> <pre>&gt; ALTER RETENTION POLICY default ON example_db DURATION 3d\n</pre> <p>At the next <a href=\"../../administration/config/index#retention\">retention policy enforcement check</a>, InfluxDB immediately drops <code>Shard Group 1</code> because all of its data are older than 3 days. InfluxDB does not drop <code>Shard Group 2</code>. This is because InfluxDB cannot divide existing shard groups and some data in <code>Shard Group 2</code> still fall within the new three day retention policy.</p> <p>Figure 2 shows the shard groups for <code>example_db</code> five days after the retention policy change. Notice that the new shard groups span one day intervals. All of the data in <code>Shard Group 2</code> remain in the database because the shard group still has data within the retention policy’s three day <code>DURATION</code>:</p> <blockquote> <p><strong>Figure 2</strong> <img src=\"https://docs.influxdata.com/img/influxdb/fei/alter-rp-3d.png\" alt=\"Retention policy duration three days\"></p> </blockquote> <p>After day 17, all data within the past 3 days will be in one day shard groups. InfluxDB will then be able to drop <code>Shard Group 2</code> and <code>example_db</code> will have only 3 days worth of data.</p> <blockquote> <p><strong>Note:</strong> The time it takes for InfluxDB to adjust to the new retention policy may be longer depending on your shard precreation configuration setting. See <a href=\"../../administration/config/index#shard-precreation\">Database Configuration</a> for more on that setting. See <a href=\"../../query_language/database_management/index#delete-a-shard-with-drop-shard\">Database Management</a> for how to delete a shard.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/troubleshooting/frequently_encountered_issues/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/troubleshooting/frequently_encountered_issues/</a>\n  </p>\n</div>\n","influxdb/v0.13/troubleshooting/query_management/index":"<h1>Query Management</h1>     <p>With InfluxDB’s query management features, users are able to identify currently-running queries and have the ability to kill queries that are overloading their system. Additionally, users can prevent and halt the execution of inefficient queries with several configuration settings.</p> <ul> <li><a href=\"index#list-currently-running-queries-with-show-queries\">List currently-running queries with <code>SHOW QUERIES</code></a></li> <li><a href=\"index#stop-currently-running-queries-with-kill-query\">Stop currently-running queries with <code>KILL QUERY</code></a></li> <li>\n<a href=\"index#configuration-settings-for-query-management\">Configuration settings for query management</a> <ul> <li><a href=\"index#max-concurrent-queries\">Limit the number of running queries with <code>max-concurrent-queries</code></a></li> <li><a href=\"index#query-timeout\">Set a query timeout with <code>query-timeout</code></a></li> <li><a href=\"index#log-queries-after\">Log queries if they run longer than specified time with <code>log-queries-after</code></a></li> <li><a href=\"index#max-select-point\">Limit the number of points that a <code>SELECT</code> statement can process with <code>max-select-point</code></a></li> <li><a href=\"index#max-select-series\">Limit the number of series that a <code>SELECT</code> statement can process with <code>max-select-series</code></a></li> <li><a href=\"index#max-select-buckets\">Limit the number of <code>GROUP BY time()</code> buckets a query can process with <code>max-select-buckets</code></a></li> </ul>\n</li> </ul> <h2 id=\"list-currently-running-queries-with-show-queries\">List currently-running queries with <code>SHOW QUERIES</code>\n</h2> <p><code>SHOW QUERIES</code> lists the query id, query text, relevant database, and duration of all currently-running queries on your InfluxDB instance.</p> <h5 id=\"syntax\">Syntax:</h5> <pre>SHOW QUERIES\n</pre> <h5 id=\"example\">Example:</h5> <p><br></p> <pre>&gt; SHOW QUERIES\nqid\t  query\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t               database\t\t  duration\n37\t   SHOW QUERIES\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                \t  100368u\n36\t   SELECT mean(myfield) FROM mymeas   mydb        3s\n</pre> <h5 id=\"explanation-of-the-output\">Explanation of the output:</h5> <p><br> <code>qid</code>: The id number of the query. Use this value with <a href=\"index#stop-currently-running-queries-with-kill-query\"><code>KILL QUERY</code></a>.<br> <code>query</code>: The query text.<br> <code>database</code>: The database targeted by the query.<br> <code>duration</code>: The length of time that the query has been running. See <a href=\"../../query_language/spec/index#durations\">Query Language Reference</a> for an explanation of InfluxDB’s time units.</p> <h2 id=\"stop-currently-running-queries-with-kill-query\">Stop currently-running queries with <code>KILL QUERY</code>\n</h2> <p><code>KILL QUERY</code> tells InfluxDB to stop running the relevant query.</p> <h5 id=\"syntax-1\">Syntax:</h5> <p>Where <code>qid</code> is the id of the query from the <a href=\"index#list-currently-running-queries-with-show-queries\"><code>SHOW QUERIES</code></a> output:</p> <pre>KILL QUERY &lt;qid&gt;\n</pre> <h5 id=\"example-1\">Example:</h5> <p><br></p> <pre>&gt; KILL QUERY 36\n&gt;\n</pre> <p>A successful <code>KILL QUERY</code> query returns no results.</p> <h2 id=\"configuration-settings-for-query-management\">Configuration settings for query management</h2> <p>The following configuration settings are in the <a href=\"../../administration/config/index#cluster\">[cluster]</a> section of the configuration file.</p> <h3 id=\"max-concurrent-queries\">max-concurrent-queries</h3> <p>The maximum number of running queries allowed on your instance. The default setting (<code>0</code>) allows for an unlimited number of queries.</p> <p>If you exceed <code>max-concurrent-queries</code>, InfluxDB does not execute the query and outputs the following error:</p> <pre>ERR: max concurrent queries reached\n</pre> <h3 id=\"query-timeout\">query-timeout</h3> <p>The maximum time for which a query can run on your instance before InfluxDB kills the query. The default setting (<code>\"0\"</code>) allows queries to run with no time restrictions. This setting is a <a href=\"../../query_language/spec/index#durations\">duration literal</a>.</p> <p>If your query exceeds the query timeout, InfluxDB kills the query and outputs the following error:</p> <pre>ERR: query timeout reached\n</pre> <h3 id=\"log-queries-after\">log-queries-after</h3> <p>The maximum time a query can run after which InfluxDB logs the query with a <code>Detected slow query</code> message. The default setting (<code>\"0\"</code>) will never tell InfluxDB to log the query. This setting is a <a href=\"../../query_language/spec/index#durations\">duration literal</a>.</p> <p>Example log output with <code>log-queries-after</code> set to <code>\"1s\"</code>:</p> <pre>[query] 2016/04/28 14:11:31 Detected slow query: SELECT mean(usage_idle) FROM cpu WHERE time &gt;= 0 GROUP BY time(20s) (qid: 3, database: telegraf, threshold: 1s)\n</pre> <p><code>qid</code> is the id number of the query. Use this value with <a href=\"index#stop-currently-running-queries-with-kill-query\"><code>KILL QUERY</code></a>.</p> <h3 id=\"max-select-point\">max-select-point</h3> <p>The maximum number of <a href=\"../../concepts/glossary/index#point\">points</a> that a <code>SELECT</code> statement can process. The default setting (<code>0</code>) allows the <code>SELECT</code> statement to process an unlimited number of points.</p> <p>If your query exceeds <code>max-select-point</code>, InfluxDB kills the query and outputs the following error:</p> <pre>ERR: max number of points reached\n</pre> <h3 id=\"max-select-series\">max-select-series</h3> <p>The maximum number of <a href=\"../../concepts/glossary/index#series\">series</a> that a <code>SELECT</code> statement can process. The default setting (<code>0</code>) allows the <code>SELECT</code> statement to process an unlimited number of series.</p> <p>If your query exceeds <code>max-select-series</code>, InfluxDB does not execute the query and outputs the following error:</p> <pre>ERR: max select series count exceeded: &lt;query_series_count&gt; series\n</pre> <h3 id=\"max-select-buckets\">max-select-buckets</h3> <p>The maximum number of <code>GROUP BY time()</code> buckets that a query can process. The default setting (<code>0</code>) allows a query to process an unlimited number of buckets.</p> <p>If your query exceeds <code>max-select-buckets</code>, InfluxDB does not execute the query and outputs the following error:</p> <pre>ERR: max select bucket count exceeded: &lt;query_bucket_count&gt; buckets\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/troubleshooting/query_management/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/troubleshooting/query_management/</a>\n  </p>\n</div>\n","influxdb/v0.13/write_protocols/index":"<h1>Write Protocols</h1>     <h2 id=\"line-protocol-influxdb-v0-13-write-protocols-line\"><a href=\"line/index\">Line Protocol</a></h2> <p>The line protocol is a text based format for writing points to InfluxDB.</p> <h2 id=\"write-syntax-influxdb-v0-13-write-protocols-write-syntax\"><a href=\"write_syntax/index\">Write Syntax</a></h2> <p>Syntax reference for Line Protocol</p> <h2 id=\"json-protocol-removed-influxdb-v0-13-write-protocols-json\"><a href=\"json/index\">JSON Protocol(REMOVED)</a></h2> <h2 id=\"udp-influxdb-v0-13-write-protocols-udp\"><a href=\"udp/index\">UDP</a></h2> <p>InfluxDB accepts writes over UDP. To configure InfluxDB to support writes over UDP you must adjust your config file.</p> <h2 id=\"graphite-influxdb-v0-13-write-protocols-graphite\"><a href=\"graphite/index\">Graphite</a></h2> <p>InfluxDB provides an easy way to hook up Graphite as an input source.</p> <h2 id=\"collectd-influxdb-v0-13-write-protocols-collectd\"><a href=\"collectd/index\">CollectD</a></h2> <p>InfluxDB provides an easy way to hook up CollectD as an input source.</p> <h2 id=\"opentsdb-influxdb-v0-13-write-protocols-opentsdb\"><a href=\"opentsdb/index\">OpenTSDB</a></h2> <p>InfluxDB provides an easy way to hook up OpenTSDB as an input source.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/write_protocols/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/write_protocols/</a>\n  </p>\n</div>\n","influxdb/v0.13/write_protocols/write_syntax/index":"<h1>Write Syntax</h1>     <p>Syntax is always a challenge to remember, so here’s a reference</p> <h2 id=\"line-protocol\">Line Protocol</h2> <p>The syntax for the line protocol is</p> <p><code>measurement[,tag_key1=tag_value1...] field_key=field_value[,field_key2=field_value2] [timestamp]</code></p> <p>For example:</p> <pre data-language=\"bash\">measurement,tkey1=tval1,tkey2=tval2 fkey=fval,fkey2=fval2 1234567890000000000\n</pre> <h3 id=\"whitespace\">Whitespace</h3> <p>A space must exist between the measurement and the field(s), or between the tag(s) and the field(s) if tags are provided. The measurement and tags must be separated by a single comma <code>,</code> with no whitespace.</p> <p>There must also be whitespace between the field(s) and the timestamp, if one is provided.</p> <p>Valid (<code>value</code> and <code>otherval</code> are fields, <code>foo</code> and <code>bat</code> are tags)</p> <pre data-language=\"bash\">measurement value=12\nmeasurement value=12 1439587925\nmeasurement,foo=bar value=12\nmeasurement,foo=bar value=12 1439587925\nmeasurement,foo=bar,bat=baz value=12,otherval=21 1439587925\n</pre> <p>Invalid</p> <pre data-language=\"bash\">measurement,value=12\nmeasurement value=12,1439587925\nmeasurement foo=bar value=12\nmeasurement,foo=bar,value=12 1439587925\nmeasurement,foo=bar\nmeasurement,foo=bar 1439587925\n</pre> <h3 id=\"timestamps\">Timestamps</h3> <p>Timestamps are not required. When no timestamp is provided the server will insert the point with the local server timestamp. If a timestamp is provided it must be separated from the field(s) by a space. Timestamps must be in Unix time and are assumed to be in nanoseconds. A different precision can be specified, see the <a href=\"index#http\">HTTP syntax</a> for details. We recommend using the least precise precision possible as this can result in significant improvements in compression.</p> <h3 id=\"key-value-separator\">Key-value Separator</h3> <p>Tag keys and values, and field keys and values must be separated by the equals sign <code>=</code> without spaces.</p> <h3 id=\"escaping-characters\">Escaping Characters</h3> <p>If a tag key, tag value, or field key contains a space , comma <code>,</code>, or an equals sign <code>=</code> it must be escaped using the backslash character <code>\\</code>. Backslash characters do not need to be escaped. Commas <code>,</code> and spaces will also need to be escaped for measurements, though equals signs <code>=</code> do not.</p> <h3 id=\"comments\">Comments</h3> <p><code>#</code> at the beginning of the line is a valid comment character for the line protocol. All subsequent characters are ignored.</p> <h3 id=\"data-types\">Data Types</h3> <p>Measurements, tag keys, tag values, and field keys are always stored as strings in the database.</p> <p><code>string</code> values have a length limit of 64 KB. All Unicode characters should be valid, although commas and spaces require escaping. Backslash characters do not require escaping, but may not be used directly preceding a comma or space. (Note that <code>string</code> field values have different quoting and escaping rules than the measurement, tag, and field name <code>string</code> syntax.) The field <code>location=\"us-west\"</code> stores a string value.</p> <p>Field values may be stored as <code>float64</code>, <code>int64</code>, <code>boolean</code>, or <code>string</code>. All subsequent field values must match the type of the first point written to given measurement.</p> <p><code>float64</code> values are the default numerical type. <code>1</code> is a float, <code>1i</code> is an integer.</p> <p><code>int64</code> values must have a trailing <code>i</code>. The field <code>bikes_present=15i</code> stores an integer and the field <code>bikes_present=15</code> stores a float.</p> <p><code>boolean</code> values are <code>t</code>, <code>T</code>, <code>true</code>, <code>True</code>, or <code>TRUE</code> for TRUE, and <code>f</code>, <code>F</code>, <code>false</code>, <code>False</code>, or <code>FALSE</code> for FALSE</p> <p><code>string</code> values for field values must be double-quoted. Double-quotes contained within the string must be escaped. All other characters are supported without escaping.</p> <h3 id=\"examples\">Examples</h3> <h4 id=\"simplest-valid-point-measurement-field\">Simplest Valid Point (measurement + field)</h4> <pre>disk_free value=442221834240i\n</pre> <h4 id=\"with-timestamp\">With Timestamp</h4> <pre>disk_free value=442221834240i 1435362189575692182\n</pre> <h4 id=\"with-tags\">With Tags</h4> <pre>disk_free,hostname=server01,disk_type=SSD value=442221834240i\n</pre> <h4 id=\"with-tags-with-timestamp\">With Tags, With Timestamp</h4> <pre>disk_free,hostname=server01,disk_type=SSD value=442221834240i 1435362189575692182\n</pre> <h4 id=\"multiple-fields\">Multiple Fields</h4> <pre>disk_free free_space=442221834240i,disk_type=\"SSD\" 1435362189575692182\n</pre> <h4 id=\"escaping-commas-and-spaces\">Escaping Commas and Spaces</h4> <pre>total\\ disk\\ free,volumes=/net\\,/home\\,/ value=442221834240i 1435362189575692182\n</pre> <p>In the above example, the measurement is written as <code>total disk free</code> and the tag key <code>volumes</code> has a tag value of <code>/net,/home,/</code></p> <h4 id=\"escaping-equals-signs\">Escaping Equals Signs</h4> <pre>disk_free,a\\=b=y\\=z value=442221834240i\n</pre> <p>In the above example, the tag key <code>a=b</code> has a tag value of <code>y=z</code></p> <h4 id=\"with-backslash-in-tag-value\">With Backslash in Tag Value</h4> <pre>disk_free,path=C:\\Windows value=442221834240i\n</pre> <p>Backslashes do not need to be escaped when used in strings. Unless followed by a comma, space, or equals sign backslashes are treated as a normal character.</p> <h4 id=\"escaping-field-key\">Escaping Field Key</h4> <pre>disk_free value=442221834240i,working\\ directories=\"C:\\My Documents\\Stuff for examples,C:\\My Documents\"\n</pre> <p>In the above example, the second field key is <code>working directories</code> and the corresponding field value is <code>C:\\My Documents\\Stuff for examples,C:\\My Documents</code>.</p> <h4 id=\"showing-all-escaping-and-quoting-behavior\">Showing all escaping and quoting behavior</h4> <pre>\"measurement\\ with\\ quotes\",tag\\ key\\ with\\ spaces=tag\\,value\\,with\"commas\" field_key\\\\\\\\=\"string field value, only \\\" need be quoted\"\n</pre> <p>In the above example, the measurement is <code>\"measurement with quotes\"</code>, the tag key is <code>tag key with spaces</code>, the tag value is <code>tag,value,with\"commas\"</code>, the field key is <code>field_key\\\\\\\\</code> and the field value is <code>string field value, only \" need be quoted</code>.</p> <h3 id=\"caveats\">Caveats</h3> <p>If you write points in a batch all points without explicit timestamps will receive the same timestamp when inserted. Since a point is defined only by its measurement, tag set, and timestamp, that can lead to duplicate points. When InfluxDB encounters a duplicate point, the <a href=\"../../concepts/glossary/index#field-set\">field set</a> becomes the union of the old field set and the new field set, where any ties go to the new field set. It is a best practice to provide explicit timestamps with all points.</p> <p>Measurements, tag keys, tag values, and field keys are never quoted. Spaces and commas must be escaped. Field values that are stored as strings must always be double-quoted. Only double-quotes should be escaped.</p> <p>Querying measurements or tags that contain double-quotes <code>\"</code> can be difficult, since double-quotes are also the syntax for an identifier. It’s possible to work around the limitations with regular expressions but it’s not easy.</p> <p>Avoid using Keywords as identifiers (database names, retention policy names, measurement names, tag keys, or field keys) whenever possible. Keywords in InfluxDB are referenced on the <a href=\"../../query_language/spec/index#keywords\">InfluxQL Syntax</a> page. There is no need to quote or escape keywords in the write syntax.</p> <p>All values in InfluxDB are case-sensitive: <code>MyDB</code> != <code>mydb</code> != <code>MYDB</code>. The exception is Keywords, which are case-insensitive.</p> <h2 id=\"cli\">CLI</h2> <p>To write points using the command line interface, use the <code>insert</code> command.</p> <h4 id=\"write-a-point-with-the-cli\">Write a Point with the CLI</h4> <pre data-language=\"bash\">&gt; insert disk_free,hostname=server01 value=442221834240i 1435362189575692182\n</pre> <p>The CLI will return nothing on success and should give an informative parser error if the point cannot be written. See <a href=\"../../tools/shell/index#import-data-from-a-file-with-import\">InfluxDB CLI/Shell</a> for how to import data from a file using the CLI.</p> <h2 id=\"http\">HTTP</h2> <p>To write points using HTTP, POST to the <code>/write</code> endpoint at port <code>8086</code>. You must specify the target database in the query string using <code>db=&lt;target_database&gt;</code>.</p> <p>You may optionally provide a target retention policy, specify the precision of any supplied timestamps, and pass any required authentication in the query string.</p> <p>Successful writes will return a <code>204</code> HTTP Status Code. Writes will return a <code>400</code> for invalid syntax.</p> <h3 id=\"query-string-parameters-for-writes\">Query String Parameters for Writes</h3> <p>The following query string parameters can be passed as part of the GET string when using the HTTP API:</p> <ul> <li>\n<code>db=&lt;database&gt;</code> REQUIRED - sets the target database for the write</li> <li>\n<code>rp=&lt;retention_policy&gt;</code> - sets the target retention policy for the write. If not present the default retention policy is used</li> <li>\n<code>u=&lt;username&gt;</code>, <code>p=&lt;password&gt;</code> - if authentication is enabled, you must authenticate as a user with write permissions to the target database</li> <li>\n<code>precision=[n,u,ms,s,m,h]</code> - sets the precision of the supplied Unix time values. If not present timestamps are assumed to be in nanoseconds</li> </ul> <h4 id=\"write-a-point-with-curl\">Write a Point with <code>curl</code>\n</h4> <pre data-language=\"bash\">curl -X POST 'http://localhost:8086/write?db=mydb' --data-binary 'disk_free,hostname=server01 value=442221834240i 1435362189575692182'\n</pre> <h4 id=\"write-a-point-to-a-non-default-retention-policy\">Write a Point to a non-default Retention Policy</h4> <p>Use the <code>rp=&lt;retention_policy</code> query string parameter to supply a target retention policy. If not specified, the default retention policy for the target database will be used.</p> <pre data-language=\"bash\">curl -X POST 'http://localhost:8086/write?db=mydb&amp;rp=six_month_rollup' --data-binary 'disk_free,hostname=server01 value=442221834240i 1435362189575692182'\n</pre> <h4 id=\"write-a-point-using-authentication\">Write a Point Using Authentication</h4> <p>Use the <code>u=&lt;user&gt;</code> and <code>p=&lt;password&gt;</code> to pass the authentication details, if required.</p> <pre data-language=\"bash\">curl -X POST 'http://localhost:8086/write?db=mydb&amp;u=root&amp;p=123456' --data-binary 'disk_free,hostname=server01 value=442221834240i 1435362189575692182'\n</pre> <h4 id=\"specify-non-nanosecond-timestamps\">Specify Non-nanosecond Timestamps</h4> <p>Use the <code>precision=[n,u,ms,s,m,h]</code> query string parameter to supply a precision for the timestamps.</p> <p>All timestamps are assumed to be Unix nanoseconds unless otherwise specified. If you provide timestamps in any unit other than nanoseconds, you must supply the appropriate precision in the URL query string. Use <code>n</code>, <code>u</code>, <code>ms</code>, <code>s</code>, <code>m</code>, and <code>h</code> for nanoseconds, microseconds, milliseconds, seconds, minutes, and hours, respectively.</p> <pre data-language=\"bash\">curl -X POST 'http://localhost:8086/write?db=mydb&amp;precision=ms' --data-binary 'disk_free value=442221834240i 1435362189575'\n</pre> <h4 id=\"write-a-batch-of-points-with-curl\">Write a Batch of Points with <code>curl</code>\n</h4> <p>You can also pass a file using the <code>@</code> flag. The file can contain a batch of points, one per line. Points must be separated by newline characters <code>\\n</code>. We recommend writing points in batches of 5,000 to 10,000 points. Smaller batches, and more HTTP requests, will result in sub-optimal performance.</p> <p><code>curl -X POST 'http://localhost:8086/write?db=&lt;database&gt;' --data-binary @&lt;filename&gt;</code></p> <h3 id=\"caveats-1\">Caveats</h3> <p>Use <code>curl</code>’s <code>--data-binary</code> encoding method for all writes in the line protocol format. Using any encoding method other than <code>--data-binary</code> is likely to lead to issues with writing points. <code>-d</code>, <code>--data-urlencode</code>, and <code>--data-ascii</code> may all strip out newlines or introduce new unintended formatting.</p> <p>When passing a file to <code>curl</code>, the points must be separated by newline characters only (<code>\\n</code>). Files containing carriage returns will cause parser errors.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/write_protocols/write_syntax/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/write_protocols/write_syntax/</a>\n  </p>\n</div>\n","influxdb/v0.13/write_protocols/udp/index":"<h1>Service - UDP</h1>     <p>InfluxDB accepts writes over UDP. By default, no ports are open to UDP. To configure InfluxDB to support writes over UDP you must adjust your config file.</p> <h2 id=\"a-note-on-udp-ip-os-buffer-sizes\">A note on UDP/IP OS Buffer sizes</h2> <p>Some OSes (most notably, Linux) place very restricive limits on the performance of UDP protocols. Recent versions of FreeBSD, OSX, and Windows do not have this problem. It is <em>highly</em> recommended that you increase these OS limits to 8MB before trying to run large amounts of UDP traffic to your instance. 8MB is a starting recommendation, and should be adjusted to be in line with your <code>read-buffer</code> plugin setting.</p> <h3 id=\"linux\">Linux</h3> <p>Check the current UDP/IP read buffer limit by typing the following commands:</p> <pre>sysctl net.core.rmem_max\n</pre> <p>If the value is less than 8388608 bytes you should add the following lines to the /etc/sysctl.conf file:</p> <pre>net.core.rmem_max=8388608\n</pre> <p>Changes to /etc/sysctl.conf do not take effect until reboot. To update the values immediately, type the following commands as root:</p> <pre>sysctl -w net.core.rmem_max=8388608\n</pre> <h3 id=\"bsd-darwin\">BSD/Darwin</h3> <p>On BSD/Darwin systems you need to add about a 15% padding to the kernel limit socket buffer. Meaning if you want an 8MB buffer (8388608 bytes) you need to set the kernel limit to <code>8388608*1.15 = 9646900</code>. This is not documented anywhere but happens <a href=\"https://github.com/freebsd/freebsd/blob/master/sys/kern/uipc_sockbuf.c#L63-L64\">in the kernel here.</a></p> <p>Check the current UDP/IP buffer limit by typing the following command:</p> <pre>sysctl kern.ipc.maxsockbuf\n</pre> <p>If the value is less than 8388608 bytes you should add the following lines to the /etc/sysctl.conf file (create it if necessary):</p> <pre>kern.ipc.maxsockbuf=8388608\n</pre> <p>Changes to /etc/sysctl.conf do not take effect until reboot. To update the values immediately, type the following commands as root:</p> <pre>sysctl -w kern.ipc.maxsockbuf=8388608\n</pre> <p>See <a href=\"https://access.redhat.com/documentation/en-US/JBoss_Enterprise_Web_Platform/5/html/Administration_And_Configuration_Guide/jgroups-perf-udpbuffer.html\">here</a> for instructions on other OSes.</p> <h3 id=\"using-the-read-buffer-option-for-the-udp-listener\">Using the read-buffer option for the UDP listener</h3> <p>The <code>read-buffer</code> option allows users to set the buffer size for the UDP listener. It Sets the size of the operating system’s receive buffer associated with the UDP traffic. Keep in mind that the OS must be able to handle the number set here or the UDP listener will error and exit.</p> <p><code>read-buffer = 0</code> means to use the OS default, which is usually too small for high UDP performance.</p> <h2 id=\"config-file\">Config File</h2> <p>The target database and listening port for all UDP writes must be specified in the configuration file.</p> <pre>...\n\n[[udp]]\n  enabled = true\n  bind-address = \":8089\" # the bind address\n  database = \"foo\" # Name of the database that will be written to\n  batch-size = 1000 # will flush if this many points get buffered\n  batch-timeout = \"1s\" # will flush at least this often even if the batch-size is not reached\n  batch-pending = 5 # number of batches that may be pending in memory\n  read-buffer = 0 # UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.\n...\n</pre> <p>Multiple configurations can be specified to support multiple listening ports or multiple target databases. For example:</p> <pre>...\n[[udp]]\n  # Default UDP for Telegraf\n  enabled = true\n  bind-address = \":8089\" # the bind address\n  database = \"telegraf\" # Name of the database that will be written to\n  batch-size = 5000 # will flush if this many points get buffered\n  batch-timeout = \"1s\" # will flush at least this often even if the batch-size is not reached\n  batch-pending = 10 # number of batches that may be pending in memory\n  read-buffer = 0 # UDP read buffer size, 0 means to use OS default\n\n[[udp]]\n  # High-traffic UDP\n  enabled = true\n  bind-address = \":8090\" # the bind address\n  database = \"mymetrics\" # Name of the database that will be written to\n  batch-size = 5000 # will flush if this many points get buffered\n  batch-timeout = \"1s\" # will flush at least this often even if the batch-size is not reached\n  batch-pending = 100 # number of batches that may be pending in memory\n  read-buffer = 8388608 # (8*1024*1024) UDP read buffer size\n...\n</pre> <h2 id=\"writing-points\">Writing Points</h2> <p>To write, just send newline separated <a href=\"../line/index\">line protocol</a> over UDP. For better performance send batches of points rather than multiple single points.</p> <pre data-language=\"bash\">$ echo \"cpu value=1\"&gt; /dev/udp/localhost/8089\n</pre> <h2 id=\"more-information\">More Information</h2> <p>For more information about the UDP plugin, please see the UDP plugin <a href=\"https://github.com/influxdb/influxdb/blob/master/services/udp/README.md\">README</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/write_protocols/udp/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/write_protocols/udp/</a>\n  </p>\n</div>\n","influxdb/v0.13/write_protocols/graphite/index":"<h1>Service - Graphite</h1>     <p>InfluxDB provides an easy way to hook up Graphite as an input source. See the <a href=\"https://github.com/influxdata/influxdb/blob/master/services/graphite/README.md\">README on GitHub</a> for more information.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/write_protocols/graphite/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/write_protocols/graphite/</a>\n  </p>\n</div>\n","influxdb/v0.13/write_protocols/collectd/index":"<h1>Service - CollectD</h1>     <p>InfluxDB provides an easy way to hook up CollectD as an input source. See the <a href=\"https://github.com/influxdata/influxdb/blob/master/services/collectd/README.md\">README on GitHub</a> for more information.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/write_protocols/collectd/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/write_protocols/collectd/</a>\n  </p>\n</div>\n","influxdb/v0.13/write_protocols/opentsdb/index":"<h1>Service - OpenTSDB</h1>     <p>InfluxDB provides an easy way to hook up OpenTSDB as an input source. See the <a href=\"https://github.com/influxdb/influxdb/blob/master/services/opentsdb/README.md\">README on GitHub</a> for more information.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/write_protocols/opentsdb/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/write_protocols/opentsdb/</a>\n  </p>\n</div>\n","influxdb/v0.13/query_language/index":"<h1>Query Language</h1>     <h2 id=\"data-exploration-influxdb-v0-13-query-language-data-exploration\"><a href=\"data_exploration/index\">Data Exploration</a></h2> <p>Query language basics for InfluxQL.</p> <h2 id=\"schema-exploration-influxdb-v0-13-query-language-schema-exploration\"><a href=\"schema_exploration/index\">Schema Exploration</a></h2> <p>Schema manipulation basics for InfluxQL.</p> <h2 id=\"database-management-influxdb-v0-13-query-language-database-management\"><a href=\"database_management/index\">Database Management</a></h2> <p>Manage databases and retention polices with InfluxQL.</p> <h2 id=\"functions-influxdb-v0-13-query-language-functions\"><a href=\"functions/index\">Functions</a></h2> <p>Functions and their usage in InfluxQL.</p> <h2 id=\"continuous-queries-influxdb-v0-13-query-language-continuous-queries\"><a href=\"continuous_queries/index\">Continuous Queries</a></h2> <p>Proper syntax for continuous queries.</p> <h2 id=\"influxdb-query-language-reference-influxdb-v0-13-query-language-spec\"><a href=\"spec/index\">InfluxDB Query Language Reference</a></h2> <p>Syntax specification for InfluxQL.</p> <h2 id=\"mathematical-operators-influxdb-v0-13-query-language-math-operators\"><a href=\"math_operators/index\">Mathematical Operators</a></h2> <p>Use of mathematical operators in InfluxQL.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/query_language/</a>\n  </p>\n</div>\n","influxdb/v0.13/write_protocols/json/index":"<h1>JSON Protocol</h1>     <p>The JSON write protocol is no longer a valid write protocol for InfluxDB.</p> <p>The protocol was deprecated as of InfluxDB 0.9.1 and was <a href=\"https://github.com/influxdata/influxdb/pull/5512\">disabled</a> by default in InfluxDB 0.11. <a href=\"../line/index\">Line Protocol</a> is the only recommended write protocol for InfluxDB 0.13.</p> <p>For reasons behind its removal, please see the comments on the line protocol pull request, particularly the comments on JSON serialization <a href=\"https://github.com/influxdb/influxdb/pull/2696#issuecomment-106968181\">CPU costs</a> and on the <a href=\"https://github.com/influxdb/influxdb/pull/2696#issuecomment-107043910\">ease of use</a> concerns.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/write_protocols/json/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/write_protocols/json/</a>\n  </p>\n</div>\n","influxdb/v0.13/query_language/math_operators/index":"<h1>Mathematical Operators</h1>     <p>Mathematical operators follow the standard order of operations. That is, <em>parentheses</em> take precedence to <em>division</em> and <em>multiplication</em>, which takes precedence to <em>addition</em> and <em>substraction</em>. For example <code>5 / 2 + 3 * 2 =  (5 / 2) + (3 * 2)</code> and <code>5 + 2 * 3 - 2 = 5 + (2 * 3) - 2</code>.</p> <h2 id=\"supported-operators\">Supported Operators</h2> <h3 id=\"addition\">Addition</h3> <p>You can add a constant.</p> <pre data-language=\"sql\">SELECT A + 5 FROM add\n</pre> <pre data-language=\"sql\">SELECT * FROM add WHERE A + 5 &gt; 10\n</pre> <p>You can add together other field keys.</p> <pre data-language=\"sql\">SELECT A + B FROM add\n</pre> <pre data-language=\"sql\">SELECT * FROM add WHERE A + B &gt;= 10\n</pre> <h3 id=\"subtraction\">Subtraction</h3> <p>You can subtract a constant.</p> <pre data-language=\"sql\">SELECT 1 - A FROM sub\n</pre> <pre data-language=\"sql\">SELECT * FROM sub WHERE 1 - A &lt;= 3\n</pre> <p>You can subtract one field key from another field key.</p> <pre data-language=\"sql\">SELECT A - B FROM sub\n</pre> <pre data-language=\"sql\">SELECT * FROM sub WHERE A - B &lt;= 1\n</pre> <h3 id=\"multiplication\">Multiplication</h3> <p>You can multiply by a constant.</p> <pre data-language=\"sql\">SELECT 10 * A FROM mult\n</pre> <pre data-language=\"sql\">SELECT * FROM mult WHERE A * 10 &gt;= 20\n</pre> <p>You can multiply by other field keys.</p> <pre data-language=\"sql\">SELECT A * B * C FROM mult\n</pre> <pre data-language=\"sql\">SELECT * FROM mult WHERE A * B &lt;= 80\n</pre> <p>Multiplication distributes across other operators</p> <pre data-language=\"sql\">SELECT 10 * (A + B + C) FROM mult\n</pre> <pre data-language=\"sql\">SELECT 10 * (A - B - C) FROM mult\n</pre> <pre data-language=\"sql\">SELECT 10 * (A + B - C) FROM mult\n</pre> <h3 id=\"division\">Division</h3> <p>You can divide by a constant.</p> <pre data-language=\"sql\">SELECT 10 / A FROM div\n</pre> <pre data-language=\"sql\">SELECT * FROM div WHERE A / 10 &lt;= 2\n</pre> <p>You can divide by other field keys.</p> <pre data-language=\"sql\">SELECT A / B FROM div\n</pre> <pre data-language=\"sql\">SELECT * FROM div WHERE A / B &gt;= 10\n</pre> <p>Division distributes across other operators</p> <pre data-language=\"sql\">SELECT 10 / (A + B + C) FROM mult\n</pre> <h2 id=\"operators-with-functions\">Operators with Functions</h2> <p>The use of mathematical operators inside of function calls is currently unsupported. Note that InfluxDB only allows functions in the <code>SELECT</code> clause.</p> <p>For example</p> <pre data-language=\"sql\">SELECT 10 * mean(value) FROM cpu\n</pre> <p>will work, however</p> <pre data-language=\"sql\">SELECT mean(10 * value) FROM cpu\n</pre> <p>will yield a parse error.</p> <h2 id=\"unsupported-operators\">Unsupported Operators</h2> <h3 id=\"inequalities\">Inequalities</h3> <p>Using any of <code>=</code>,<code>!=</code>,<code>&lt;</code>,<code>&gt;</code>,<code>&lt;=</code>,<code>&gt;=</code>,<code>&lt;&gt;</code> in the <code>SELECT</code> clause yields empty results for all types. See GitHub issue <a href=\"https://github.com/influxdb/influxdb/issues/3525\">3525</a>.</p> <h3 id=\"miscellaneous\">Miscellaneous</h3> <p>Using any of <code>%</code>, <code>^</code> will yield a parse error. If you would like to see support for these operators open an <a href=\"https://github.com/influxdb/influxdb/issues/new\">issue</a>.</p> <h2 id=\"logical-operators-are-unsupported\">Logical Operators are Unsupported</h2> <p>Using any of <code>&amp;</code>,<code>|</code>,<code>!|</code>,<code>NAND</code>,<code>XOR</code>,<code>NOR</code> will yield parse error.</p> <p>Additionally using <code>AND</code>, <code>OR</code> in the <code>SELECT</code> clause of a query will not behave as mathematical operators and simply yield empty results, as they are tokens in InfluxQL.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/math_operators/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/query_language/math_operators/</a>\n  </p>\n</div>\n","influxdb/v0.13/high_availability/index":"<h1>High Availability</h1>     <h2 id=\"relay-influxdb-v0-13-high-availability-relay\"><a href=\"relay/index\">Relay</a></h2> <p>Relay adds a basic high availability layer to InfluxDB. With the right architecture and disaster recovery processes, this achieves a highly available setup.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/high_availability/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/high_availability/</a>\n  </p>\n</div>\n","influxdb/v0.13/query_language/database_management/index":"<h1>Database Management</h1>     <p>InfluxQL offers a full suite of administrative commands.</p> <ul> <li><p><a href=\"index#data-management\">Data management</a><br> ◦ <a href=\"index#create-a-database-with-create-database\">Create a database with <code>CREATE DATABASE</code></a><br> ◦ <a href=\"index#delete-a-database-with-drop-database\">Delete a database with <code>DROP DATABASE</code></a><br> ◦ <a href=\"index#drop-series-from-the-index-with-drop-series\">Drop series from the index with <code>DROP SERIES</code></a><br> ◦ <a href=\"index#delete-series-with-delete\">Delete series with <code>DELETE</code></a><br> ◦ <a href=\"index#delete-measurements-with-drop-measurement\">Delete measurements with <code>DROP MEASUREMENT</code></a><br> ◦ <a href=\"index#delete-a-shard-with-drop-shard\">Delete a shard with <code>DROP SHARD</code></a></p></li> <li><p><a href=\"index#retention-policy-management\">Retention policy management</a><br> ◦ <a href=\"index#create-retention-policies-with-create-retention-policy\">Create retention policies with <code>CREATE RETENTION POLICY</code></a><br> ◦ <a href=\"index#modify-retention-policies-with-alter-retention-policy\">Modify retention policies with <code>ALTER RETENTION POLICY</code></a><br> ◦ <a href=\"index#delete-retention-policies-with-drop-retention-policy\">Delete retention policies with <code>DROP RETENTION POLICY</code></a></p></li> </ul> <p>If you’re looking for <code>SHOW</code> queries (for example, <code>SHOW DATABASES</code> or <code>SHOW RETENTION POLICIES</code>), see <a href=\"../schema_exploration/index\">Schema Exploration</a>.</p> <p>The examples in the sections below use InfluxDB’s <a href=\"../../introduction/getting_started/index\">Command Line Interface (CLI)</a>. You can also execute the commands using the HTTP API; simply send a <code>GET</code> request to the <code>/query</code> endpoint and include the command in the URL parameter <code>q</code>. See the <a href=\"../../guides/querying_data/index\">Querying Data</a> guide for more on using the HTTP API.</p> <blockquote> <p><strong>Note:</strong> When authentication is enabled, only admin users can execute most of the commands listed on this page. See the documentation on <a href=\"../../administration/authentication_and_authorization/index\">authentication and authorization</a> for more information.</p> </blockquote> <h2 id=\"data-management\">Data Management</h2> <h3 id=\"create-a-database-with-create-database\">Create a database with CREATE DATABASE</h3>  <p>The <code>CREATE DATABASE</code> query takes the following form:</p> <pre data-language=\"sql\">CREATE DATABASE [IF NOT EXISTS] &lt;database_name&gt; [WITH [DURATION &lt;duration&gt;] [REPLICATION &lt;n&gt;] [SHARD DURATION &lt;duration&gt;] [NAME &lt;retention-policy-name&gt;]]\n</pre> <blockquote> <p><strong>Note:</strong> The <code>IF NOT EXISTS</code> clause does nothing, is deprecated, and will be removed in InfluxDB version 1.0.</p> </blockquote> <p>Create the database <code>NOAA_water_database</code>:</p> <pre data-language=\"bash\">&gt; CREATE DATABASE NOAA_water_database\n&gt;\n</pre> <p>Create the database <code>NOAA_water_database</code> with a new <code>DEFAULT</code> retention policy called <code>liquid</code>:</p> <pre data-language=\"bash\">&gt; CREATE DATABASE NOAA_water_database WITH DURATION 3d REPLICATION 3 SHARD DURATION 30m NAME liquid\n&gt;\n</pre> <p>When specifying a retention policy you can include one or more of the attributes <code>DURATION</code>, <code>REPLICATION</code>, <code>SHARD DURATION</code>, and <code>NAME</code>. For more on retention policies, see <a href=\"index#retention-policy-management\">Retention Policy Management</a></p> <p>A successful <code>CREATE DATABASE</code> query returns an empty result. If you attempt to create a database that already exists, InfluxDB does not return an error.</p> <h3 id=\"delete-a-database-with-drop-database\">Delete a database with DROP DATABASE</h3>  <p>The <code>DROP DATABASE</code> query deletes all of the data, measurements, series, continuous queries, and retention policies from the specified database. The query takes the following form:</p> <pre data-language=\"sql\">DROP DATABASE [IF EXISTS] &lt;database_name&gt;\n</pre> <blockquote> <p><strong>Note:</strong> The <code>IF EXISTS</code> clause does nothing, is deprecated, and will be removed in InfluxDB version 1.0.</p> </blockquote> <p>Drop the database NOAA_water_database:</p> <pre data-language=\"bash\">&gt; DROP DATABASE NOAA_water_database\n&gt;\n</pre> <p>A successful <code>DROP DATABASE</code> query returns an empty result. If you attempt to drop a database that does not exist, InfluxDB does not return an error.</p> <h3 id=\"drop-series-from-the-index-with-drop-series\">Drop series from the index with DROP SERIES</h3>  <p>The <code>DROP SERIES</code> query deletes all points from a <a href=\"../../concepts/glossary/index#series\">series</a> in a database, and it drops the series from the index.</p> <blockquote> <p><strong>Note:</strong> <code>DROP SERIES</code> does not support time intervals in the <code>WHERE</code> clause. See <a href=\"index#delete-series-with-delete\"><code>DELETE</code></a> for that functionality.</p> </blockquote> <p>The query takes the following form, where you must specify either the <code>FROM</code> clause or the <code>WHERE</code> clause:</p> <pre data-language=\"sql\">DROP SERIES FROM &lt;measurement_name[,measurement_name]&gt; WHERE &lt;tag_key&gt;='&lt;tag_value&gt;'\n</pre> <p>Drop all series from a single measurement:</p> <pre data-language=\"sql\">&gt; DROP SERIES FROM h2o_feet\n</pre> <p>Drop series with a specific tag pair from a single measurement:</p> <pre data-language=\"sql\">&gt; DROP SERIES FROM h2o_feet WHERE location = 'santa_monica'\n</pre> <p>Drop all points in the series that have a specific tag pair from all measurements in the database:</p> <pre data-language=\"sql\">&gt; DROP SERIES WHERE location = 'santa_monica'\n</pre> <p>A successful <code>DROP SERIES</code> query returns an empty result.</p> <h3 id=\"delete-series-with-delete\">Delete series with DELETE</h3>  <p>The <code>DELETE</code> query deletes all points from a <a href=\"../../concepts/glossary/index#series\">series</a> in a database. Unlike <a href=\"index#drop-series-from-the-index-with-drop-series\"><code>DROP SERIES</code></a>, it does not drop the series from the index and it supports time intervals in the <code>WHERE</code> clause.</p> <p>The query takes the following form where you must include either the <code>FROM</code> clause or the <code>WHERE</code> clause, or both:</p> <pre>DELETE FROM &lt;measurement_name&gt; WHERE [&lt;tag_key&gt;='&lt;tag_value&gt;'] | [&lt;time interval&gt;]\n</pre> <p>Delete all data associated with the measurement <code>h2o_feet</code>:</p> <pre>&gt; DELETE FROM h2o_feet\n</pre> <p>Delete all data associated with the measurement <code>h2o_quality</code> and where the tag <code>randtag</code> equals <code>3</code>:</p> <pre>&gt; DELETE FROM h2o_quality WHERE randtag = '3'\n</pre> <p>Delete all data in the database that occur before January 01, 2016:</p> <pre>&gt; DELETE WHERE time &lt; '2016-01-01'\n</pre> <p>A successful <code>DELETE</code> query returns an empty result.</p> <p>Things to note about <code>DELETE</code>:</p> <ul> <li>\n<code>DELETE</code> supports <a href=\"../data_exploration/index#regular-expressions-in-queries\">regular expressions</a> in the <code>FROM</code> clause when specifying measurement names and in the <code>WHERE</code> clause when specifying tag values.</li> <li>\n<code>DELETE</code> does not support <a href=\"../../concepts/glossary/index#field\">fields</a> in the <code>WHERE</code> clause.</li> </ul> <h3 id=\"delete-measurements-with-drop-measurement\">Delete measurements with DROP MEASUREMENT</h3>  <p>The <code>DROP MEASUREMENT</code> query deletes all data and series from the specified <a href=\"../../concepts/glossary/index#measurement\">measurement</a> and deletes the measurement from the index.</p> <p>The query takes the following form:</p> <pre data-language=\"sql\">DROP MEASUREMENT &lt;measurement_name&gt;\n</pre> <p>Delete the measurement <code>h2o_feet</code>:</p> <pre data-language=\"sql\">&gt; DROP MEASUREMENT h2o_feet\n</pre> <blockquote> <p><strong>Note:</strong> <code>DROP MEASUREMENT</code> drops all data and series in the measurement. It does not drop the associated continuous queries.</p> </blockquote> <p>A successful <code>DROP MEASUREMENT</code> query returns an empty result.</p> \n<dt> Currently, InfluxDB does not support regular expressions with <code>DROP MEASUREMENTS</code>. See GitHub Issue <a href=\"https://github.com/influxdb/influxdb/issues/4275\">#4275</a> for more information. </dt> <h3 id=\"delete-a-shard-with-drop-shard\">Delete a shard with DROP SHARD</h3>  <p>The <code>DROP SHARD</code> query deletes a shard. It also drops the shard from the <a href=\"../../concepts/glossary/index#metastore\">metastore</a>. The query takes the following form:</p> <pre data-language=\"sql\">DROP SHARD &lt;shard_id_number&gt;\n</pre> <p>Delete the shard with the id <code>1</code>:</p> <pre>&gt; DROP SHARD 1\n&gt;\n</pre> <p>A successful <code>DROP SHARD</code> query returns an empty result. InfluxDB does not return an error if you attempt to drop a shard that does not exist.</p> <h2 id=\"retention-policy-management\">Retention Policy Management</h2> <p>The following sections cover how to create, alter, and delete retention policies. Note that when you create a database, InfluxDB automatically creates a retention policy named <code>default</code> which has infinite retention. You may disable that auto-creation in the configuration file.</p> <h3 id=\"create-retention-policies-with-create-retention-policy\">Create retention policies with CREATE RETENTION POLICY</h3>  <p>The <code>CREATE RETENTION POLICY</code> query takes the following form:</p> <pre data-language=\"sql\">CREATE RETENTION POLICY &lt;retention_policy_name&gt; ON &lt;database_name&gt; DURATION &lt;duration&gt; REPLICATION &lt;n&gt; [SHARD DURATION &lt;duration&gt;] [DEFAULT]\n</pre> <ul> <li>\n<p><code>DURATION</code> determines how long InfluxDB keeps the data. The options for specifying the duration of the retention policy are listed below. Note that the minimum retention period is one hour.<br> <code>m</code> minutes<br> <code>h</code> hours<br> <code>d</code> days<br> <code>w</code> weeks<br> <code>INF</code> infinite</p> \n<dt> Currently, the <code>DURATION</code> attribute supports only single units. For example, you cannot express the duration <code>7230m</code> as <code>120h 30m</code>. See GitHub Issue <a href=\"https://github.com/influxdb/influxdb/issues/3634\">#3634</a> for more information. </dt>\n</li> <li><p><code>REPLICATION</code> determines how many independent copies of each point are stored in the cluster, where <code>n</code> is the number of data nodes.</p></li> </ul> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <ul> <li>\n<code>SHARD DURATION</code> determines the time range covered by a shard group. The options for specifying the duration of the shard group are listed below. The default shard group duration depends on your retention policy’s <code>DURATION</code>.<br> <code>u</code> microseconds<br> <code>ms</code> milliseconds<br> <code>s</code> seconds<br> <code>m</code> minutes<br> <code>h</code> hours<br> <code>d</code> days<br> <code>w</code> weeks</li> </ul> \n<dt> Currently, the <code>SHARD DURATION</code> attribute supports only single units. For example, you cannot express the duration <code>7230m</code> as <code>120h 30m</code>. </dt> <ul> <li>\n<code>DEFAULT</code> sets the new retention policy as the default retention policy for the database.</li> </ul> <p>Create a retention policy called <code>one_day_only</code> for the database <code>NOAA_water_database</code> with a one day duration and a replication factor of one:</p> <pre data-language=\"sql\">&gt; CREATE RETENTION POLICY one_day_only ON NOAA_water_database DURATION 1d REPLICATION 1\n&gt;\n</pre> <p>Create the same retention policy as the one in the example above, but set it as the default retention policy for the database.</p> <pre data-language=\"sql\">&gt; CREATE RETENTION POLICY one_day_only ON NOAA_water_database DURATION 1d REPLICATION 1 DEFAULT\n&gt;\n</pre> <p>A successful <code>CREATE RETENTION POLICY</code> query returns an empty response. If you attempt to create a retention policy that already exists, InfluxDB does not return an error.</p> <blockquote> <p><strong>Note:</strong> You can also specify a new retention policy in the <code>CREATE DATABASE</code> query. See <a href=\"index#create-a-database-with-create-database\">Create a database with CREATE DATABASE</a>.</p> </blockquote> <h3 id=\"modify-retention-policies-with-alter-retention-policy\">Modify retention policies with ALTER RETENTION POLICY</h3>  <p>The <code>ALTER RETENTION POLICY</code> query takes the following form, where you must declare at least one of the retention policy attributes <code>DURATION</code>, <code>REPLICATION</code>, <code>SHARD DURATION</code>, or <code>DEFAULT</code>:</p> <pre data-language=\"sql\">ALTER RETENTION POLICY &lt;retention_policy_name&gt; ON &lt;database_name&gt; DURATION &lt;duration&gt; REPLICATION &lt;n&gt; SHARD DURATION &lt;duration&gt; DEFAULT\n</pre> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <p>First, create the retention policy <code>what_is_time</code> with a <code>DURATION</code> of two days:</p> <pre data-language=\"sql\">&gt; CREATE RETENTION POLICY what_is_time ON NOAA_water_database DURATION 2d REPLICATION 1\n&gt;\n</pre> <p>Modify <code>what_is_time</code> to have a three week <code>DURATION</code>, a 30 minute shard group duration, and make it the <code>DEFAULT</code> retention policy for <code>NOAA_water_database</code>.</p> <pre data-language=\"sql\">&gt; ALTER RETENTION POLICY what_is_time ON NOAA_water_database DURATION 3w SHARD DURATION 30m DEFAULT\n&gt;\n</pre> <p>In the last example, <code>what_is_time</code> retains its original replication factor of 1.</p> <p>A successful <code>ALTER RETENTION POLICY</code> query returns an empty result.</p> <h3 id=\"delete-retention-policies-with-drop-retention-policy\">Delete retention policies with DROP RETENTION POLICY</h3> <p>Delete all measurements and data in a specific retention policy with:</p> <pre data-language=\"sql\">DROP RETENTION POLICY &lt;retention_policy_name&gt; ON &lt;database_name&gt;\n</pre> <p>Delete the retention policy <code>what_is_time</code> in the <code>NOAA_water_database</code> database:</p> <pre data-language=\"bash\">&gt; DROP RETENTION POLICY what_is_time ON NOAA_water_database\n&gt;\n</pre> <p>A successful <code>DROP RETENTION POLICY</code> query returns an empty result. If you attempt to drop a retention policy that does not exist, InfluxDB does not return an error.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/database_management/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/query_language/database_management/</a>\n  </p>\n</div>\n","influxdb/v0.13/clients/index":"<h1>InfluxDB Clients</h1>     <h2 id=\"v0-13-compatible-libraries-influxdb-v0-13-clients-api\"><a href=\"api/index\">v0.13.* Compatible Libraries</a></h2><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/clients/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/clients/</a>\n  </p>\n</div>\n","influxdb/v0.13/clients/api/index":"<h1>HTTP API Client Libraries</h1>     <p>This is a list of the client libraries which have some support for InfluxDB version 0.9. Most should be fully compatible with InfluxDB version 0.13. Functionality will vary, and there are, as yet, no standard features that all libraries must implement in order to be listed here.</p> <h2 id=\"erlang-udp\">Erlang UDP</h2> <ul> <li>\n<a href=\"https://github.com/palkan/influx_udp\">InfluxDB Erlang UDP</a> by <a href=\"https://github.com/palkan\">palkan</a>\n</li> </ul> <h2 id=\"go\">Go</h2> <ul> <li>\n<a href=\"https://github.com/influxdb/influxdb/blob/master/client/README.md\">InfluxDB Go</a> by <a href=\"https://github.com/influxdata\">InfluxData</a>\n</li> </ul> <h2 id=\"haskell\">Haskell</h2> <ul> <li>\n<a href=\"https://github.com/maoe/influxdb-haskell\">InfluxDB Haskell</a> by <a href=\"https://github.com/maoe\">maoe</a>\n</li> </ul> <h2 id=\"java\">Java</h2> <ul> <li>\n<a href=\"https://github.com/influxdb/influxdb-java\">InfluxDB Java</a> by <a href=\"https://github.com/majst01\">majst01</a>\n</li> </ul> <h2 id=\"javascript-node-js\">JavaScript/Node.js</h2> <ul> <li>\n<a href=\"https://github.com/node-influx/node-influx\">node-influx</a> by <a href=\"https://github.com/node-influx\">node-influx</a>\n</li> <li>\n<a href=\"https://github.com/gobwas/influent\">Influent</a> by <a href=\"https://github.com/gobwas\">gobwas</a>\n</li> </ul> <h2 id=\"lisp\">Lisp</h2> <ul> <li>\n<a href=\"https://github.com/mmaul/cl-influxdb\">cl-influxdb</a> by <a href=\"https://github.com/mmaul\">mmaul</a>\n</li> </ul> <h2 id=\"net\">.Net</h2> <ul> <li>\n<a href=\"https://github.com/AdysTech/InfluxDB.Client.Net\">InfluxDB.Client.Net</a> by <a href=\"https://github.com/mvadu\">mvadu</a>\n</li> <li>\n<a href=\"https://github.com/danielwertheim/myinfluxdbclient\">MyInfluxDbClient</a> by <a href=\"https://github.com/danielwertheim\">danielwertheim</a>\n</li> <li>\n<a href=\"https://github.com/pootzko/InfluxData.Net\">InfluxData.Net</a> by <a href=\"https://github.com/pootzko\">pootzko</a>\n</li> <li>\n<a href=\"https://github.com/MikaelGRA/InfluxDB.Client\">InfluxDB Client for .NET</a> by <a href=\"https://github.com/MikaelGRA\">MikaelGRA</a>\n</li> </ul> <h2 id=\"perfmon\">Perfmon</h2> <ul> <li>\n<a href=\"https://github.com/willemdh/naf_windows_perfmon_to_influxdb\">naf_windows_perfmon_to_influxdb</a> by <a href=\"https://github.com/willemdh\">willemdh</a>\n</li> </ul> <h2 id=\"perl\">Perl</h2> <ul> <li>\n<a href=\"https://github.com/ajgb/anyevent-influxdb\">AnyEvent::InfluxDB</a> by <a href=\"https://github.com/ajgb\">ajgb</a>\n</li> <li>\n<a href=\"http://search.cpan.org/~domm/InfluxDB-LineProtocol-1.001/\">InfluxDB::LineProtocol</a> by <a href=\"http://search.cpan.org/~domm/\">domm</a>\n</li> <li>\n<a href=\"https://github.com/raphaelthomas/InfluxDB-HTTP\">InfluxDB::HTTP</a> by <a href=\"https://github.com/raphaelthomas\">Raphael Seebacher</a>\n</li> </ul> <h2 id=\"php\">PHP</h2> <ul> <li>\n<a href=\"https://github.com/influxdb/influxdb-php\">InfluxDB PHP</a> by <a href=\"https://github.com/thecodeassassin\">thecodeassassin</a>\n</li> <li>\n<a href=\"https://github.com/crodas/InfluxPHP\">InfluxPHP</a> by <a href=\"https://github.com/crodas\">crodas</a>\n</li> <li>\n<a href=\"https://github.com/corley/influxdb-php-sdk\">InfluxDB PHP SDK</a> by <a href=\"https://github.com/corley\">corley</a>\n</li> </ul> <h2 id=\"python\">Python</h2> <ul> <li><a href=\"https://github.com/influxdb/influxdb-python\">InfluxDB Python</a></li> </ul> <h2 id=\"r\">R</h2> <ul> <li>\n<a href=\"https://github.com/dleutnant/influxdbr\">influxdbr</a> by <a href=\"https://github.com/dleutnant\">dleutnant</a>\n</li> </ul> <h2 id=\"ruby\">Ruby</h2> <ul> <li><a href=\"https://github.com/influxdb/influxdb-ruby\">InfluxDB Ruby</a></li> <li>\n<a href=\"https://github.com/palkan/influxer\">Influxer</a> by <a href=\"https://github.com/palkan\">palkan</a>\n</li> </ul> <h2 id=\"rust\">Rust</h2> <ul> <li>\n<a href=\"https://github.com/gobwas/influent.rs\">Influent.rs</a> by <a href=\"https://github.com/gobwas\">gobwas</a>\n</li> </ul> <h2 id=\"scala\">Scala</h2> <ul> <li>\n<a href=\"https://github.com/paulgoldbaum/scala-influxdb-client\">scala-influxdb-client</a> by <a href=\"https://github.com/paulgoldbaum\">paulgoldbaum</a>\n</li> </ul> <h2 id=\"sensu\">Sensu</h2> <ul> <li>\n<a href=\"https://github.com/jhrv/sensu-influxdb-extension\">sensu-influxdb-extension</a> by <a href=\"https://github.com/jhrv\">jhrv</a>\n</li> </ul> <h2 id=\"snmp\">SNMP</h2> <ul> <li>\n<a href=\"https://github.com/paulstuart/influxsnmp\">influxsnmp</a> by <a href=\"https://github.com/paulstuart\">paulstuart</a>\n</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/clients/api/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/clients/api/</a>\n  </p>\n</div>\n","influxdb/v0.13/query_language/schema_exploration/index":"<h1>Schema Exploration</h1>     <p>InfluxQL is an SQL-like query language for interacting with data in InfluxDB. The following sections cover useful query syntax for exploring your schema (that is, how you set up your time series data):</p> <ul> <li><a href=\"index#see-all-databases-with-show-databases\">See all databases with <code>SHOW DATABASES</code></a></li> <li><a href=\"index#explore-retention-policies-with-show-retention-policies\">Explore retention policies with <code>SHOW RETENTION POLICIES</code></a></li> <li><a href=\"index#explore-series-with-show-series\">Explore series with <code>SHOW SERIES</code></a></li> <li><a href=\"index#explore-measurements-with-show-measurements\">Explore measurements with <code>SHOW MEASUREMENTS</code></a></li> <li><a href=\"index#explore-tag-keys-with-show-tag-keys\">Explore tag keys with <code>SHOW TAG KEYS</code></a></li> <li><a href=\"index#explore-tag-values-with-show-tag-values\">Explore tag values with <code>SHOW TAG VALUES</code></a></li> <li><a href=\"index#explore-field-keys-with-show-field-keys\">Explore field keys with <code>SHOW FIELD KEYS</code></a></li> </ul> <p>The examples below query data using <a href=\"../../tools/shell/index\">InfluxDB’s Command Line Interface (CLI)</a>. See the <a href=\"../../guides/querying_data/index\">Querying Data</a> guide for how to directly query data with the HTTP API.</p> <p><strong>Sample data</strong></p> <p>This document uses the same sample data as the <a href=\"../data_exploration/index\">Data Exploration</a> page. The data are described and are available for download on the <a href=\"https://docs.influxdata.com/influxdb/v0.13/sample_data/data_download/\">Sample Data</a> page.</p> <h2 id=\"see-all-databases-with-show-databases\">See all databases with <code>SHOW DATABASES</code>\n</h2> <p>Get a list of all the databases in your system by entering:</p> <pre data-language=\"sql\">SHOW DATABASES\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; SHOW DATABASES\nname: databases\n---------------\nname\nNOAA_water_database\n</pre> <h2 id=\"explore-retention-policies-with-show-retention-policies\">Explore retention policies with <code>SHOW RETENTION POLICIES</code>\n</h2> <p>The <code>SHOW RETENTION POLICIES</code> query lists the existing <a href=\"../../concepts/glossary/index#retention-policy-rp\">retention policies</a> on a given database, and it takes the following form:</p> <pre data-language=\"sql\">SHOW RETENTION POLICIES ON &lt;database_name&gt;\n</pre> <p>CLI example:</p> <pre data-language=\"sql\">&gt; SHOW RETENTION POLICIES ON NOAA_water_database\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name\t    duration\t replicaN\t default\ndefault\t 0\t\t       1\t\t       true\n</pre> <p>The first column of the output contains the names of the different retention policies in the specified database. The second column shows the <a href=\"../../concepts/glossary/index#duration\">duration</a> and the third column shows the <a href=\"../../concepts/glossary/index#replication-factor\">replication factor</a> of the retention policy. The fourth column specifies if the retention policy is the <code>DEFAULT</code> retention policy for the database.</p> <p>The following example shows a hypothetical CLI response where there are four different retention policies in the database, and where the <code>DEFAULT</code> retention policy is <code>three_days_only</code>:</p> <pre data-language=\"bash\">name\t\t           duration\t replicaN\t default\ndefault\t\t        0\t\t       1\t\t       false\ntwo_days_only\t   48h0m0s\t\t 1\t\t       false\none_day_only\t    24h0m0s\t\t 1\t\t       false\nthree_days_only\t 72h0m0s\t\t 1\t\t       true\n</pre> <h2 id=\"explore-series-with-show-series\">Explore series with <code>SHOW SERIES</code>\n</h2> <p>The <code>SHOW SERIES</code> query returns the distinct <a href=\"../../concepts/glossary/index#series\">series</a> in your database and takes the following form, where the <code>FROM</code> and <code>WHERE</code> clauses are optional:</p> <pre data-language=\"sql\">SHOW SERIES [FROM &lt;measurement_name&gt; [WHERE &lt;tag_key&gt;='&lt;tag_value&gt;']]\n</pre> <p>Return all series in the database <code>NOAA_water_database</code>:</p> <pre data-language=\"sql\">&gt; SHOW SERIES\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">key\naverage_temperature,location=coyote_creek\naverage_temperature,location=santa_monica\nh2o_feet,location=coyote_creek\nh2o_feet,location=santa_monica\nh2o_pH,location=coyote_creek\nh2o_pH,location=santa_monica\nh2o_quality,location=coyote_creek,randtag=1\nh2o_quality,location=coyote_creek,randtag=2\nh2o_quality,location=coyote_creek,randtag=3\nh2o_quality,location=santa_monica,randtag=1\nh2o_quality,location=santa_monica,randtag=2\nh2o_quality,location=santa_monica,randtag=3\nh2o_temperature,location=coyote_creek\nh2o_temperature,location=santa_monica\n</pre> <p><code>SHOW SERIES</code> organizes its output similar to the <a href=\"../../write_protocols/line/index\">line protocol</a> format. Everything before the first comma is the <a href=\"../../concepts/glossary/index#measurement\">measurement</a> name. Everything after the first comma is either a <a href=\"../../concepts/glossary/index#tag-key\">tag key</a> or a <a href=\"../../concepts/glossary/index#tag-value\">tag value</a>.</p> <p>From the output above you can see that the data in the database <code>NOAA_water_database</code> have five different measurements and 14 different series. The measurements are <code>average_temperature</code>, <code>h2o_feet</code>, <code>h2o_pH</code>, <code>h2o_quality</code>, and <code>h2o_temperature</code>. Every measurement has the tag key <code>location</code> with the tag values <code>coyote_creek</code> and <code>santa_monica</code> - that makes 10 series. The measurement <code>h2o_quality</code> has the additional tag key <code>randtag</code> with the tag values <code>1</code>,<code>2</code>, and <code>3</code> - that makes 14 series.</p> <p>Return series for a specific measurement:</p> <pre data-language=\"sql\">&gt; SHOW SERIES FROM h2o_quality\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">key\nh2o_quality,location=coyote_creek,randtag=1\nh2o_quality,location=coyote_creek,randtag=2\nh2o_quality,location=coyote_creek,randtag=3\nh2o_quality,location=santa_monica,randtag=1\nh2o_quality,location=santa_monica,randtag=2\nh2o_quality,location=santa_monica,randtag=3\n</pre> <p>Return series for a specific measurement and tag set:</p> <pre data-language=\"sql\">&gt; SHOW SERIES FROM h2o_quality WHERE location = 'coyote_creek'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">key\nh2o_quality,location=coyote_creek,randtag=1\nh2o_quality,location=coyote_creek,randtag=2\nh2o_quality,location=coyote_creek,randtag=3\n</pre> <h2 id=\"explore-measurements-with-show-measurements\">Explore measurements with <code>SHOW MEASUREMENTS</code>\n</h2> <p>The <code>SHOW MEASUREMENTS</code> query returns the <a href=\"../../concepts/glossary/index#measurement\">measurements</a> in your database and it takes the following form:</p> <pre data-language=\"sql\">SHOW MEASUREMENTS [WITH MEASUREMENT =~ &lt;regular_expression&gt;] [WHERE &lt;tag_key&gt;=&lt;'tag_value'&gt;]\n</pre> <p>Return all measurements in the <code>NOAA_water_database</code> database:</p> <pre data-language=\"sql\">&gt; SHOW MEASUREMENTS\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: measurements\n------------------\nname\naverage_temperature\nh2o_feet\nh2o_pH\nh2o_quality\nh2o_temperature\n</pre> <p>From the output you can see that the database <code>NOAA_water_database</code> has five measurements: <code>average_temperature</code>, <code>h2o_feet</code>, <code>h2o_pH</code>, <code>h2o_quality</code>, and <code>h2o_temperature</code>.</p> <p>Return measurements where the tag key <code>randtag</code> equals <code>1</code>:</p> <pre data-language=\"sql\">&gt; SHOW MEASUREMENTS WHERE randtag = '1'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: measurements\n------------------\nname\nh2o_quality\n</pre> <p>Only the measurement <code>h2o_quality</code> contains the tag set <code>randtag = 1</code>.</p> <p>Use a regular expression to return measurements where the tag key <code>randtag</code> is a digit:</p> <pre data-language=\"sql\">SHOW MEASUREMENTS WHERE randtag =~ /\\d/\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: measurements\n------------------\nname\nh2o_quality\n</pre> <p>Use a regular expression with <code>WITH MEASUREMENT</code> to return all measurements that start with <code>h2o</code>:</p> <pre data-language=\"sql\">&gt; SHOW MEASUREMENTS WITH MEASUREMENT =~ /h2o.*/\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: measurements\n------------------\nname\nh2o_feet\nh2o_pH\nh2o_quality\nh2o_temperature\n</pre> <h2 id=\"explore-tag-keys-with-show-tag-keys\">Explore tag keys with SHOW TAG KEYS</h2> <p><code>SHOW TAG KEYS</code> returns the <a href=\"../../concepts/glossary/index#tag-key\">tag keys</a> associated with each measurement and takes the following form, where the <code>FROM</code> clause is optional:</p> <pre data-language=\"sql\">SHOW TAG KEYS [FROM &lt;measurement_name&gt;]\n</pre> <p>Return all tag keys that are in the database <code>NOAA_water_database</code>:</p> <pre data-language=\"sql\">&gt; SHOW TAG KEYS\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: average_temperature\n-------------------------\ntagKey\nlocation\n\nname: h2o_feet\n--------------\ntagKey\nlocation\n\nname: h2o_pH\n------------\ntagKey\nlocation\n\nname: h2o_quality\n-----------------\ntagKey\nlocation\nrandtag\n\nname: h2o_temperature\n---------------------\ntagKey\nlocation\n</pre> <p>InfluxDB organizes the output by measurement. Notice that each of the five measurements has the tag key <code>location</code> and that the measurement <code>h2o_quality</code> also has the tag key <code>randtag</code>.</p> <p>Return the tag keys for a specific measurement:</p> <pre data-language=\"sql\">&gt; SHOW TAG KEYS FROM h2o_temperature\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_temperature\n---------------------\ntagKey\nlocation\n</pre> <h2 id=\"explore-tag-values-with-show-tag-values\">Explore tag values with SHOW TAG VALUES</h2> <p>The <code>SHOW TAG VALUES</code> query returns the set of <a href=\"../../concepts/glossary/index#tag-value\">tag values</a> for a specific tag key across all measurements in the database. Syntax for specifying a single tag key:</p> <pre data-language=\"sql\">SHOW TAG VALUES [FROM &lt;measurement_name&gt;] WITH KEY = \"&lt;tag_key&gt;\"\n</pre> <p>Syntax for specifying more than one tag key:</p> <pre data-language=\"sql\">SHOW TAG VALUES [FROM &lt;measurement_name&gt;] WITH KEY IN (\"&lt;tag_key1&gt;\",\"&lt;tag_key2\")\n</pre> <p>Return the tag values for a single tag key (<code>randtag</code>) across all measurements in the database <code>NOAA_water_database</code>:</p> <pre data-language=\"sql\">&gt; SHOW TAG VALUES WITH KEY = \"randtag\"\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_quality\n-----------------\nkey\t     value\nrandtag\t 1\nrandtag\t 3\nrandtag\t 2\n</pre> <p>Return the tag values for two tag keys (<code>location</code> and <code>randtag</code>) across all measurements in the database <code>NOAA_water_database</code>:</p> <pre data-language=\"sql\">&gt; SHOW TAG VALUES WITH KEY IN (\"location\",\"randtag\")\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: average_temperature\n-------------------------\nkey\t\t     value\nlocation\t coyote_creek\nlocation\t santa_monica\n\n\nname: h2o_feet\n--------------\nkey\t\t     value\nlocation\t coyote_creek\nlocation\t santa_monica\n\n\nname: h2o_pH\n------------\nkey\t\t     value\nlocation\t coyote_creek\nlocation\t santa_monica\n\n\nname: h2o_quality\n-----------------\nkey\t\t     value\nlocation\t coyote_creek\nrandtag\t\t 1\nrandtag\t\t 3\nrandtag\t\t 2\nlocation\t santa_monica\n\n\nname: h2o_temperature\n---------------------\nkey\t\t     value\nlocation\t coyote_creek\nlocation\t santa_monica\n</pre> <p>Return the tag values for the tag key <code>randtag</code> for a specific measurement in the <code>NOAA_water_database</code> database:</p> <pre data-language=\"sql\">&gt; SHOW TAG VALUES FROM average_temperature WITH KEY = \"randtag\"\n</pre> <p>CLI response:</p>  <p>The measurement <code>average_temperature</code> doesn’t have the tag key <code>randtag</code> so InfluxDB returns nothing.</p> <h2 id=\"explore-field-keys-with-show-field-keys\">Explore field keys with <code>SHOW FIELD KEYS</code>\n</h2> <p>The <code>SHOW FIELD KEYS</code> query returns the <a href=\"../../concepts/glossary/index#field-key\">field keys</a> across each measurement in the database. It takes the following form, where the <code>FROM</code> clause is optional:</p> <pre data-language=\"sql\">SHOW FIELD KEYS [FROM &lt;measurement_name&gt;]\n</pre> <p>Return the field keys across all measurements in the database <code>NOAA_water_database</code>:</p> <pre data-language=\"sql\">&gt; SHOW FIELD KEYS\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: average_temperature\n-------------------------\nfieldKey\ndegrees\n\nname: h2o_feet\n--------------\nfieldKey\nlevel description\nwater_level\n\nname: h2o_pH\n------------\nfieldKey\npH\n\nname: h2o_quality\n-----------------\nfieldKey\nindex\n\nname: h2o_temperature\n---------------------\nfieldKey\ndegrees\n</pre> <p>Return the field keys in the measurement <code>h2o_feet</code> in the database <code>NOAA_water_database</code>:</p> <pre data-language=\"sql\">&gt; SHOW FIELD KEYS FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\nfieldKey\nlevel description\nwater_level\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/schema_exploration/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/query_language/schema_exploration/</a>\n  </p>\n</div>\n","influxdb/v0.13/query_language/continuous_queries/index":"<h1>Continuous Queries</h1>     <p>When writing large amounts of data to InfluxDB, you may often want to downsample the raw data, that is, use <code>GROUP BY time()</code> with an InfluxQL <a href=\"../functions/index\">function</a> to change the high frequency data into lower frequency data. Repeatedly running the same queries by hand can be tedious. InfluxDB’s continuous queries (CQ) simplify the downsampling process; CQs run automatically and write the query results to another measurement.</p> <ul> <li><a href=\"index#cq-definition\">CQ definition</a></li> <li>\n<a href=\"index#influxql-for-creating-a-cq\">InfluxQL for creating a CQ</a><br> ◦ <a href=\"index#the-create-continuous-query-statement\">The <code>CREATE CONTINUOUS QUERY</code> statement</a><br> ◦ <a href=\"index#cqs-with-backreferencing\">CQs with backreferencing</a><br>\n</li> <li><a href=\"index#list-cqs-with-show\">List CQs with <code>SHOW</code></a></li> <li><a href=\"index#delete-cqs-with-drop\">Delete CQs with <code>DROP</code></a></li> <li><a href=\"index#backfilling\">Backfilling</a></li> <li><a href=\"index#further-reading\">Further reading</a></li> </ul> <h2 id=\"cq-definition\">CQ definition</h2> <p>A CQ is an InfluxQL query that the system runs automatically and periodically within a database. InfluxDB stores the results of the CQ in a specified <a href=\"../../concepts/glossary/index#measurement\">measurement</a>. CQs require a function in the <code>SELECT</code> clause and must include a <code>GROUP BY time()</code> clause.</p> <p>CQs do not maintain any state. Each execution of a CQ is a standalone query that resamples all points in the database matching the conditions of the query.</p> <p>The time ranges of the CQ results have round-number boundaries that are set internally by the database. There is currently no way for users to alter the start or end times of the intervals.</p> <p>Only admin users are allowed to work with continuous queries. For more on user privileges, see <a href=\"../../administration/authentication_and_authorization/index#user-types-and-their-privileges\">Authentication and Authorization</a>.</p> <blockquote> <p><strong>Note:</strong> CQs only execute on data received after the CQ’s creation. If you’d like to downsample data written to InfluxDB before the CQ was created, see the examples in <a href=\"../data_exploration/index#downsample-data\">Data Exploration</a>.</p> </blockquote> <h2 id=\"influxql-for-creating-a-cq\">InfluxQL for creating a CQ</h2> <h3 id=\"the-create-continuous-query-statement\">The <code>CREATE CONTINUOUS QUERY</code> statement</h3> <pre data-language=\"sql\">CREATE CONTINUOUS QUERY &lt;cq_name&gt; ON &lt;database_name&gt; [RESAMPLE [EVERY &lt;interval&gt;] [FOR &lt;interval&gt;]] BEGIN SELECT &lt;function&gt;(&lt;stuff&gt;)[,&lt;function&gt;(&lt;stuff&gt;)] INTO &lt;different_measurement&gt; FROM &lt;current_measurement&gt; [WHERE &lt;stuff&gt;] GROUP BY time(&lt;interval&gt;)[,&lt;stuff&gt;] END\n</pre> <p>The <code>CREATE CONTINUOUS QUERY</code> statement is essentially an InfluxQL query surrounded by <code>CREATE CONTINUOUS QUERY [...] BEGIN</code> and <code>END</code>. The following discussion breaks the CQ statement into its meta portion (everything between <code>CREATE</code> and <code>BEGIN</code>) and query portion (everything between <code>BEGIN</code> and <code>END</code>).</p> <p>A successful <code>CREATE CONTINUOUS QUERY</code> statement returns an empty response. If you attempt to create a continuous query that already exists, InfluxDB does not return an error.</p> <h4 id=\"meta-syntax\">Meta syntax:</h4> <pre>CREATE CONTINUOUS QUERY ON &lt;database_name&gt; [RESAMPLE [EVERY &lt;interval&gt;] [FOR &lt;interval&gt;]]\n</pre> <p>A CQ belongs to a database. Specify the database where you want the CQ to live with <code>ON &lt;database_name&gt;</code>.</p> <p>The optional <code>RESAMPLE</code> clause determines how often InfluxDB runs the CQ (<code>EVERY &lt;interval&gt;</code>) and the time range over which InfluxDB runs the CQ (<code>FOR &lt;interval&gt;</code>). If included, the <code>RESAMPLE</code> clause must specify either <code>EVERY</code>, or <code>FOR</code>, or both. Without the <code>RESAMPLE</code> clause, InfluxDB runs the CQ at the same interval as the <code>GROUP BY time()</code> interval and it calculates the query for the most recent <code>GROUP BY time()</code> interval (that is, where time is between <code>now()</code> and <code>now()</code> minus the <code>GROUP BY time()</code> interval).</p> <h4 id=\"query-syntax\">Query syntax:</h4> <pre>BEGIN SELECT &lt;function&gt;(&lt;stuff&gt;)[,&lt;function&gt;(&lt;stuff&gt;)] INTO &lt;different_measurement&gt; FROM &lt;current_measurement&gt; [WHERE &lt;stuff&gt;] GROUP BY time(&lt;interval&gt;)[,&lt;stuff&gt;] END\n</pre> <p>The query portion of the statement differs from a typical <code>SELECT [...] GROUP BY (time)</code> statement in two ways:</p> <ol> <li><p>The <code>INTO</code> clause: This is where you specify the destination measurement for the query results.</p></li> <li><p>The optional <code>WHERE</code> clause: Because CQs run on regularly incremented time intervals you don’t need to (and shouldn’t!) specify a time range in the <code>WHERE</code> clause. When included, the CQ’s <code>WHERE</code> clause should filter information about tags.</p></li> </ol> <blockquote> <p><strong>Note:</strong> If you include a tag in the CQ’s <code>SELECT</code> clause, InfluxDB changes the tag in <code>&lt;current_measurement&gt;</code> to a field in <code>&lt;different_measurement&gt;</code>. To preserve a tag in <code>&lt;different_measurement&gt;</code>, only include the tag key in the CQ’s <code>GROUP BY</code> clause.</p> <p>If you specify a tag in the CQ’s <code>SELECT</code> clause <strong>and</strong> in the CQ’s <code>GROUP BY</code> clause, you will not be able to query the data in <code>&lt;different_measurement&gt;</code>. See GitHub Issue <a href=\"https://github.com/influxdata/influxdb/issues/4630\">#4630</a> for more information.</p> </blockquote> <h4 id=\"cq-examples\">CQ examples:</h4> <ul> <li>\n<p>Create a CQ with one function:</p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY minnie ON world BEGIN SELECT min(mouse) INTO min_mouse FROM zoo GROUP BY time(30m) END\n</pre> <p>Once executed, InfluxDB automatically calculates the 30 minute minimum of the field <code>mouse</code> in the measurement <code>zoo</code>, and it writes the results to the measurement <code>min_mouse</code>. Note that the CQ <code>minnie</code> only exists in the database <code>world</code>.</p>\n</li> <li>\n<p>Create a CQ with one function and write the results to another <a href=\"../../concepts/glossary/index#retention-policy-rp\">retention policy</a>:</p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY minnie_jr ON world BEGIN SELECT min(mouse) INTO world.\"7days\".min_mouse FROM world.\"1day\".zoo GROUP BY time(30m) END\n</pre> <p>The CQ <code>minnie_jr</code> acts in the same way as the CQ <code>minnie</code>, however, InfluxDB calculates the 30 minute minimum of the field <code>mouse</code> in the measurement <code>zoo</code> and under the retention policy <code>1day</code>, and it automatically writes the results of the query to the measurement <code>min_mouse</code> under the retention policy <code>7days</code>.</p> <p>Combining CQs and retention policies provides a useful way to automatically downsample data and expire the unnecessary raw data. For a complete discussion on this topic, see <a href=\"../../guides/downsampling_and_retention/index\">Downsampling and Data Retention</a>.</p>\n</li> <li>\n<p>Create a CQ with two functions:</p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY minnie_maximus ON world BEGIN SELECT min(mouse),max(imus) INTO min_max_mouse FROM zoo GROUP BY time(30m) END\n</pre> <p>The CQ <code>minnie_maximus</code> automatically calculates the 30 minute minimum of the field <code>mouse</code> and the 30 minute maximum of the field <code>imus</code> (both fields are in the measurement <code>zoo</code>), and it writes the results to the measurement <code>min_max_mouse</code>.</p>\n</li> <li>\n<p>Create a CQ with two functions and personalize the <a href=\"../../concepts/glossary/index#field-key\">field keys</a> in the results:</p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY minnie_maximus_1 ON world BEGIN SELECT min(mouse) AS minuscule,max(imus) AS monstrous INTO min_max_mouse FROM zoo GROUP BY time(30m) END\n</pre> <p>The CQ <code>minnie_maximus_1</code> acts in the same way as <code>minnie_maximus</code>, however, InfluxDB names field keys <code>miniscule</code> and <code>monstrous</code> in the destination measurement instead of <code>min</code> and <code>max</code>. For more on <code>AS</code>, see <a href=\"../functions/index#rename-the-output-column-s-title-with-as\">Functions</a>.</p>\n</li> <li>\n<p>Create a CQ with a 30 minute <code>GROUP BY time()</code> interval that runs every 15 minutes:</p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY vampires ON transylvania RESAMPLE EVERY 15m BEGIN SELECT count(dracula) INTO vampire_populations FROM raw_vampires GROUP BY time(30m) END\n</pre> <p>Without <code>RESAMPLE EVERY 15m</code>, <code>vampires</code> would run every 30 minutes - the same interval as the <code>GROUP BY time()</code> interval.</p>\n</li> <li>\n<p>Create a CQ with a 30 minute <code>GROUP BY time()</code> interval that runs every 30 minutes and computes the query for all <code>GROUP BY time()</code> intervals within the last hour:</p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY vampires_1 ON transylvania RESAMPLE FOR 60m BEGIN SELECT count(dracula) INTO vampire_populations_1 FROM raw_vampires GROUP BY time(30m) END\n</pre> <p>InfluxDB runs <code>vampires_1</code> every 30 minutes (the same interval as the <code>GROUP BY time()</code> interval) and it computes two queries per run: one where time is between <code>now()</code> and <code>now() - 30m</code> and one where time is between <code>now() - 30m</code> and <code>now() - 60m</code>. Without the <code>RESAMPLE</code> clause, InfluxDB would compute the query for only one 30 minute interval, that is, where time is between <code>now()</code> and <code>now() - 30m</code>.</p>\n</li> <li>\n<p>Create a CQ with a 30 minute <code>GROUP BY time()</code> interval that runs every 15 minutes and computes the query for all <code>GROUP BY time()</code> intervals within the last hour:</p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY vampires_2 ON transylvania RESAMPLE EVERY 15m FOR 60m BEGIN SELECT count(dracula) INTO vampire_populations_2 FROM raw_vampires GROUP BY time(30m) END\n</pre> <p><code>vampires_2</code> runs every 15 minutes and computes two queries per run: one where time is between <code>now()</code> and <code>now() - 30m</code> and one where time is between <code>now() - 30m</code> and <code>now() - 60m</code></p>\n</li> </ul> <h3 id=\"cqs-with-backreferencing\">CQs with backreferencing</h3> <p>Use <code>:MEASUREMENT</code> in the <code>INTO</code> statement to backreference measurement names:</p> <pre data-language=\"sql\">CREATE CONTINUOUS QUERY &lt;cq_name&gt; ON &lt;database_name&gt; BEGIN SELECT &lt;function&gt;(&lt;stuff&gt;)[,&lt;function&gt;(&lt;stuff&gt;)] INTO &lt;database_name&gt;.&lt;retention_policy&gt;.:MEASUREMENT FROM &lt;/relevant_measurement(s)/&gt; [WHERE &lt;stuff&gt;] GROUP BY time(&lt;interval&gt;)[,&lt;stuff&gt;] END\n</pre> <p><em>CQ backreferencing example:</em> <br> <br></p> <pre data-language=\"sql\">&gt; CREATE CONTINUOUS QUERY elsewhere ON fantasy BEGIN SELECT mean(value) INTO reality.\"default\".:MEASUREMENT FROM /elf/ GROUP BY time(10m) END\n</pre> <p>The CQ <code>elsewhere</code> automatically calculates the 10 minute average of the field <code>value</code> in each <code>elf</code> measurement in the database <code>fantasy</code>. It writes the results to the already-existing database <code>reality</code>, preserving all of the measurement names in <code>fantasy</code>.</p> <p>A sample of the data in <code>fantasy</code>:</p> <pre data-language=\"bash\">&gt; SHOW MEASUREMENTS\nname: measurements\n------------------\nname\nelf1\nelf2\nwizard\n&gt;\n&gt; SELECT * FROM elf1\nname: cpu_usage_idle\n--------------------\ntime\t\t\t               value\n2015-12-19T01:15:30Z\t 97.76333874796951\n2015-12-19T01:15:40Z\t 98.3129217695576\n[...]\n2015-12-19T01:36:00Z\t 94.71778221778222\n2015-12-19T01:35:50Z\t 87.8\n</pre> <p>A sample of the data in <code>reality</code> after <code>elsewhere</code> runs for a bit:</p> <pre data-language=\"bash\">&gt; SHOW MEASUREMENTS\nname: measurements\n------------------\nname\nelf1\nelf2\n&gt;\n&gt; SELECT * FROM elf1\nname: elf1\n--------------------\ntime\t\t\t               mean\n2015-12-19T01:10:00Z\t 97.11668879244841\n2015-12-19T01:20:00Z\t 94.50035091670394\n2015-12-19T01:30:00Z\t 95.99739053789172\n</pre> <h2 id=\"list-cqs-with-show\">List CQs with <code>SHOW</code>\n</h2> <p>List every CQ by database with:</p> <pre data-language=\"sql\">SHOW CONTINUOUS QUERIES\n</pre> <p><em>Example:</em></p> <pre data-language=\"bash\">&gt; SHOW CONTINUOUS QUERIES\nname: reality\n-------------\nname\tquery\n\nname: fantasy\n-------------\nname\t\t     query\nelsewhere\t CREATE CONTINUOUS QUERY elsewhere ON fantasy BEGIN SELECT mean(value) INTO reality.\"default\".:MEASUREMENT FROM fantasy.\"default\"./cpu/ WHERE cpu = 'cpu-total' GROUP BY time(10m) END\n</pre> <p>The output shows that the database <code>reality</code> has no CQs and the database <code>fantasy</code> has one CQ called <code>elsewhere</code>.</p> <h2 id=\"delete-cqs-with-drop\">Delete CQs with <code>DROP</code>\n</h2> <p>Delete a CQ from a specific database with:</p> <pre data-language=\"sql\">DROP CONTINUOUS QUERY &lt;cq_name&gt; ON &lt;database_name&gt;\n</pre> <p><em>Example:</em></p> <pre data-language=\"bash\">&gt; DROP CONTINUOUS QUERY elsewhere ON fantasy\n&gt;\n</pre> <p>A successful <code>DROP CONTINUOUS QUERY</code> returns an empty response.</p> <h2 id=\"backfilling\">Backfilling</h2> <p>CQs do not backfill data, that is, they do not compute results for data written to the database before the CQ existed. Instead, users can backfill data with the <code>INTO</code> clause. Unlike CQs, backfill queries require a <code>WHERE</code> clause with a <code>time</code> restriction.</p> <h3 id=\"examples\">Examples</h3> <p>Here is a basic backfill example:</p> <pre data-language=\"sql\">&gt; SELECT min(temp) as min_temp, max(temp) as max_temp INTO \"reading.minmax.5m\" FROM reading\nWHERE time &gt;= '2015-12-14 00:05:20' AND time &lt; '2015-12-15 00:05:20'\nGROUP BY time(5m)\n</pre> <p>Tags (<code>sensor_id</code> in the example below) can be used optionally in the same way as in CQs:</p> <pre data-language=\"sql\">&gt; SELECT min(temp) as min_temp, max(temp) as max_temp INTO \"reading.minmax.5m\" FROM reading\nWHERE time &gt;= '2015-12-14 00:05:20' AND time &lt; '2015-12-15 00:05:20'\nGROUP BY time(5m), sensor_id\n</pre> <p>To prevent the backfill from creating a huge number of “empty” points containing only <code>null</code> values, <a href=\"../data_exploration/index#the-group-by-clause-and-fill\">fill()</a> can be used at the end of the query:</p> <pre data-language=\"sql\">&gt; SELECT min(temp) as min_temp, max(temp) as max_temp INTO \"reading.minmax.5m\" FROM reading\nWHERE time &gt;= '2015-12-14 00:05:20' AND time &lt; '2015-12-15 00:05:20'\nGROUP BY time(5m), fill(none)\n</pre> <p>If you would like to further break down the queries and run them with even more control, you can add additional <code>WHERE</code> clauses:</p> <pre data-language=\"sql\">&gt; SELECT min(temp) as min_temp, max(temp) as max_temp INTO \"reading.minmax.5m\" FROM reading\nWHERE sensor_id=\"EG-21442\" AND time &gt;= '2015-12-14 00:05:20' AND time &lt; '2015-12-15 00:05:20'\nGROUP BY time(5m)\n</pre> <h2 id=\"further-reading\">Further reading</h2> <p>Now that you know how to create CQs with InfluxDB, check out <a href=\"../../guides/downsampling_and_retention/index\">Downsampling and Data Retention</a> for how to combine CQs with retention policies to automatically downsample data and expire unnecessary data.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/continuous_queries/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/query_language/continuous_queries/</a>\n  </p>\n</div>\n","influxdb/v0.13/administration/index":"<h1>Database Administration</h1>     <p>The administration documentation contains all the information needed to administer a working InfluxDB installation.</p> <h2 id=\"authentication-and-authorization-influxdb-v0-13-administration-authentication-and-authorization\"><a href=\"authentication_and_authorization/index\">Authentication and Authorization</a></h2> <p>This section covers setting up and managing authentication and authorization in InfluxDB.</p> <h2 id=\"logs-influxdb-v0-13-administration-logs\"><a href=\"logs/index\">Logs</a></h2> <p>Information on how to direct InfluxDB log output.</p> <h2 id=\"backup-and-restore-influxdb-v0-13-administration-backup-and-restore\"><a href=\"backup_and_restore/index\">Backup and Restore</a></h2> <p>Procedures to backup data created by InfluxDB and to restore from a backup.</p> <h2 id=\"differences-between-influxdb-0-13-and-0-12-influxdb-v0-13-administration-012-vs-013\"><a href=\"012_vs_013/index\">Differences between InfluxDB 0.13 and 0.12</a></h2> <h2 id=\"differences-between-influxdb-0-13-and-versions-prior-to-0-12-http-localhost-1313-influxdb-v0-13-administration-013-vs-previous\"><a href=\"http://localhost:1313/influxdb/v0.13/administration/013_vs_previous/\">Differences between InfluxDB 0.13 and versions prior to 0.12</a></h2> <h2 id=\"upgrading-influxdb-v0-13-administration-upgrading\"><a href=\"upgrading/index\">Upgrading</a></h2> <p>Information about upgrading from previous versions of InfluxDB</p> <h2 id=\"database-configuration-influxdb-v0-13-administration-config\"><a href=\"config/index\">Database Configuration</a></h2> <p>Information about the config file <code>influx.conf</code></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/administration/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/administration/</a>\n  </p>\n</div>\n","influxdb/v0.13/administration/logs/index":"<h1>Logs</h1>     <p>InfluxDB writes log output, by default, to <code>stderr</code>. Depending on your use case, this log information can be written to another location.</p> <h2 id=\"running-influxdb-directly\">Running InfluxDB directly</h2> <p>If you run InfluxDB directly, using <code>influxd</code>, all logs will be written to <code>stderr</code>. You may redirect this log output as you would any output to <code>stderr</code> like so:</p> <pre data-language=\"bash\">influxd 2&gt;$HOME/my_log_file\n</pre> <h2 id=\"launched-as-a-service\">Launched as a service</h2> <p>If InfluxDB was installed using a pre-built package, and then launched as a service, <code>stderr</code> is redirected to <code>/var/log/influxdb/influxd.log</code>, and all log data will be written to that file. You can override this location by setting the variable <code>STDERR</code> in the file <code>/etc/default/influxdb</code>.</p> <blockquote> <p><strong>Note:</strong> On OS X the logs, by default, are stored in the file <code>/usr/local/var/log/influxdb.log</code></p> </blockquote> <p>For example, if <code>/etc/default/influxdb</code> contains:</p> <pre data-language=\"bash\">STDERR=/dev/null\n</pre> <p>all log data will be discarded. You can similarly direct output to <code>stdout</code> by setting <code>STDOUT</code> in the same file. Output to <code>stdout</code> is sent to <code>/dev/null</code> by default when InfluxDB is launched as a service.</p> <p>InfluxDB must be restarted to pick up any changes to <code>/etc/default/influxdb</code>.</p> <h2 id=\"using-logrotate\">Using logrotate</h2> <p>You can use <a href=\"http://manpages.ubuntu.com/manpages/hardy/man8/logrotate.8.html\">logrotate</a> to rotate the log files generated by InfluxDB. If using the package install, the config file for logrotate is installed in <code>/etc/logrotate.d</code>. You can view the file <a href=\"https://github.com/influxdb/influxdb/blob/master/scripts/logrotate\">here</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/administration/logs/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/administration/logs/</a>\n  </p>\n</div>\n","influxdb/v0.13/administration/backup_and_restore/index":"<h1>Backup and Restore</h1>     <p>InfluxDB has the ability to snapshot a single data node at a point-in-time and restore it.</p> <h2 id=\"backups\">Backups</h2> <h3 id=\"backing-up-the-metastore\">Backing up the Metastore</h3> <p>InfluxDB’s metastore contains internal information about the status of the system, including: user information, database/shard metadata, and which retention policies are enabled. While a node is running, you can create a backup of your instance’s metastore by running the command:</p> <pre>influxd backup /path/to/backup\n</pre> <p>Where <code>/path/to/backup</code> can be replaced with the directory where you would like the backup to be written to. Without any other arguments, the backup will only record the current state of the system metastore. For example, the command:</p> <pre data-language=\"bash\">$ influxd backup /tmp/backup\n2016/02/01 17:15:03 backing up metastore to /tmp/backup/meta.00\n2016/02/01 17:15:03 backup complete\n</pre> <p>Will create a metastore backup in the directory <code>/tmp/backup</code> (the directory will be created if it doesn’t already exist).</p> <h3 id=\"backing-up-a-database\">Backing up a Database</h3> <p>To backup a database, you will need to add the <code>-database</code> flag:</p> <pre data-language=\"bash\">influxd backup -database mydatabase /path/to/backup\n</pre> <p>Where <code>mydatabase</code> is the name of the database you would like to backup, and <code>/path/to/backup</code> is where the backup data should be stored. Optional flags also include:</p> <ul> <li><p><code>-retention &lt;retention policy name&gt;</code> - This flag can be used to backup a specific retention policy. For more information on retention policies, please see <a href=\"../../query_language/database_management/index#retention-policy-management\">here</a>. If not specified, all retention policies will be backed up.</p></li> <li><p><code>-shard &lt;shard ID&gt;</code> - This flag can be used to backup a specific shard ID. To see which shards are available, you can run the command <code>SHOW SHARDS</code> using the InfluxDB query language. If not specified, all shards will be backed up.</p></li> <li><p><code>-since &lt;date&gt;</code> - This flag can be used to create a backup <em>since</em> a specific date, where the date must be in <a href=\"https://www.ietf.org/rfc/rfc3339.txt\">RFC3339</a> format (for example, <code>2015-12-24T08:12:23Z</code>). This flag is important if you would like to take incremental backups of your database. If not specified, all timeranges within the database will be backed up.</p></li> </ul> <blockquote> <p><strong>Note:</strong> Metastore backups are also included in per-database backups</p> </blockquote> <p>As a real-world example, you can take a backup of the <code>default</code> retention policy for the <code>telegraf</code> database since midnight UTC on February 1st, 2016 by using the command:</p> <pre>$ influxd backup -database telegraf -retention default -since 2016-02-01T00:00:00Z /tmp/backup\n2016/02/01 18:02:36 backing up rp=default since 2016-02-01 00:00:00 +0000 UTC\n2016/02/01 18:02:36 backing up metastore to /tmp/backup/meta.01\n2016/02/01 18:02:36 backing up db=telegraf rp=default shard=2 to /tmp/backup/telegraf.default.00002.01 since 2016-02-01 00:00:00 +0000 UTC\n2016/02/01 18:02:36 backup complete\n</pre> <p>Which will send the resulting backup to <code>/tmp/backup</code>, where it can then be compressed and sent to long-term storage.</p> <h3 id=\"remote-backups\">Remote Backups</h3> <p>To capture a backup from a remote node, specify the host and port of the remote instance using the <code>-host</code> configuration switch:</p> <pre data-language=\"bash\">$ influxd backup -database mydatabase -host 10.0.0.1:8088 /tmp/mysnapshot\n</pre> <p>Where all of the flags above still apply to remote hosts.</p> <h2 id=\"restore\">Restore</h2> <p>To restore a backup, you will need to use the <code>influxd restore</code> command.</p> <blockquote> <p><strong>Note:</strong> Restoring from backup is only supported while the InfluxDB daemon is stopped.</p> </blockquote> <p>In order to restore from a backup, you will need to provide the path to the backup. The command:</p> <pre>influxd restore /tmp/backup\n</pre> <p>Will attempt to restore the contents of the <code>/tmp/backup</code>. The optional flags for restoring a backup are:</p> <ul> <li><p><code>-metadir &lt;path to meta directory&gt;</code> - This is the path to the meta directory where you would like the metastore backup recovered to. For packaged installations, this should be specified as <code>/var/lib/influxdb/meta</code>.</p></li> <li><p><code>-datadir &lt;path to data directory&gt;</code> - This is the path to the data directory where you would like the database backup recovered to. For packaged installations, this should be specified as <code>/var/lib/influxdb/data</code>.</p></li> <li><p><code>-database &lt;database&gt;</code> - This is the database that you would like to restore the data to. This option is required if no <code>-metadir</code> option is provided.</p></li> <li><p><code>-retention &lt;retention policy&gt;</code> - This is the target retention policy for the stored data to be restored to.</p></li> <li><p><code>-shard &lt;shard id&gt;</code> - This is the shard data that should be restored. If specified, <code>-database</code> and <code>-retention</code> must also be set.</p></li> </ul> <p>Following the backup example above, the backup can be restored in two steps. First, the metastore needs to be restored so that InfluxDB knows which databases exist:</p> <pre>$ influxd restore -metadir /var/lib/influxdb/meta /tmp/backup\nUsing metastore snapshot: /tmp/backup/meta.00\n</pre> <p>Once the metastore has been restored, we can now recover the backed up data. In the real-world example above, we backed up the <code>telegraf</code> database to <code>/tmp/backup</code>, so let’s restore that same dataset. To restore the <code>telegraf</code> database:</p> <pre>$ influxd restore -database telegraf -datadir /var/lib/influxdb/data /tmp/backup                                                                         \nRestoring from backup /tmp/backup/telegraf.*\nunpacking /var/lib/influxdb/data/telegraf/default/2/000000004-000000003.tsm\nunpacking /var/lib/influxdb/data/telegraf/default/2/000000005-000000001.tsm\n</pre> <blockquote> <p><strong>Note:</strong> Once the backed up data has been recovered, the permissions on the shards may no longer be accurate. To ensure the file permissions are correct, please run:</p> <p><code>$ sudo chown -R influxdb:influxdb /var/lib/influxdb</code></p> </blockquote> <p>Once the data and metastore are recovered, it’s time to start the database:</p> <pre data-language=\"bash\">$ service influxdb start\n</pre> <p>As a quick check, we can verify the database is known to the metastore by running a <code>SHOW DATABASES</code> command:</p> <pre>influx -execute 'show databases'\nname: databases\n---------------\nname\n_internal\ntelegraf\n</pre> <p>The database has now been successfully restored!</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/administration/backup_and_restore/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/administration/backup_and_restore/</a>\n  </p>\n</div>\n","influxdb/v0.13/administration/012_vs_013/index":"<h1>Differences Between InfluxDB 0.13 and 0.12</h1>     <p>This page aims to ease the transition from InfluxDB 0.12 to InfluxDB 0.13. It is not intended to be a comprehensive list of the differences between the versions. See <a href=\"https://github.com/influxdata/influxdb/blob/master/CHANGELOG.md\">InfluxDB’s Changelog</a> for detailed release notes.</p> <h4 id=\"sections\">Sections:</h4> <ul> <li><a href=\"index#collectd-and-opentsdb-support-multiple-listeners\">collectd and OpenTSDB support multiple listeners</a></li> <li><a href=\"index#deprecated-query-endpoint-request-behavior\">Deprecated <code>/query</code> endpoint request behavior</a></li> <li><a href=\"index#delete-functionality\"><code>DELETE</code> functionality</a></li> <li><a href=\"index#configure-group-by-time-interval-boundaries\">Configure <code>GROUP BY time()</code> interval boundaries</a></li> <li><a href=\"index#configuration-file-environment-variable\">Configuration file environment variable</a></li> <li><a href=\"index#new-query-management-features\">New query management features</a></li> <li><a href=\"index#new-functions\">New functions</a></li> </ul> <h3 id=\"collectd-and-opentsdb-support-multiple-listeners\">collectd and OpenTSDB support multiple listeners</h3> <p>InfluxDB 0.13 supports multiple listeners for the collectd and OpenTSDB inputs. Because of changes to the configuration file (the headers <code>[collectd]</code> and <code>[opentsdb]</code> are now <code>[[collectd]]</code> and <code>[[opentsdb]]</code>), we recommend that users <a href=\"../config/index#using-configuration-files\">export</a> a default configuration file and manually migrate customizations into it before launching version 0.13.</p> <p>See Database Configuration for the <a href=\"../config/index#collectd\">collectd</a> and <a href=\"../config/index#opentsdb\">OpenTSDB</a> settings.</p> <h3 id=\"deprecated-query-endpoint-request-behavior\">Deprecated /query endpoint request behavior</h3> <p>Starting with InfluxDB 0.13, all queries sent to the <code>/query</code> endpoint that are not <code>SELECT</code> or <code>SHOW</code> queries should be <code>POST</code> requests to the <code>/query</code> endpoint. non-<code>SELECT</code> and non-<code>SHOW</code> queries will continue to work with <code>GET</code> requests to the <code>/query</code> endpoint but this behavior is deprecated in version 0.13 and will no longer work in version 1.0.</p> <h3 id=\"delete-functionality\">DELETE functionality</h3> <p>InfluxDB 0.13 introduces the <code>DELETE</code> query. The query deletes all points from series in a database. Unlike <a href=\"../../query_language/database_management/index#drop-series-from-the-index-with-drop-series\"><code>DROP SERIES</code></a> , it does not drop series from the index and it supports time intervals in the <code>WHERE</code> clause.</p> <p>The query takes the following form where you must specify either the <code>FROM</code> clause or the <code>WHERE</code> clause:</p> <pre>DELETE FROM &lt;measurement_name&gt; WHERE [&lt;tag_key&gt;='&lt;tag_value&gt;'] | [&lt;time interval&gt;]\n</pre> <p>See <a href=\"../../query_language/database_management/index#delete-series-with-delete\">Database Management</a> for more on the <code>DELETE</code> syntax and sample queries.</p> <h3 id=\"configure-group-by-time-interval-boundaries\">Configure GROUP BY time() interval boundaries</h3> <p>With InfluxDB 0.13, users can alter <code>GROUP BY time()</code>’s default rounded calendar time boundaries by including an offset interval. The new syntax is:</p> <pre>SELECT [...] GROUP BY time(&lt;time_interval&gt;[,&lt;offset_interval&gt;])\n</pre> <p>For an in-depth discussion of the <code>offset_interval</code>, see the discussion in <a href=\"../../query_language/data_exploration/index#configured-group-by-time-boundaries\">Data Exploration</a>.</p> <h3 id=\"configuration-file-environment-variable\">Configuration file environment variable</h3> <p>With InfluxDB 0.13, users can set the environment variable <code>INFLUXDB_CONFIG_PATH</code> to the path of their configuration file.</p> <p>For more information see <a href=\"../config/index#using-configuration-files\">Using Configuration Files</a>.</p> <h3 id=\"new-query-management-features\">New query management features</h3> <p>Version 0.13 offers a new configuration setting for query management. The setting <code>log-queries-after</code> tells InfluxDB to log a query if the query runs longer than a given time period.</p> <p>See the <a href=\"../../troubleshooting/query_management/index#log-queries-after\">Query Management</a> page for more information.</p> <h3 id=\"new-functions\">New functions</h3> <p>InfluxDB 0.13 provides one new InfluxQL function. <code>ELAPSED()</code> returns the difference between subsequent timestamps in a single field. See <a href=\"../../query_language/functions/index#elapsed\">Functions</a> for the syntax and sample queries.</p> <p>InfluxDB 0.13 also updates the behavior of <a href=\"../../query_language/functions/index#selectors\">selector functions</a> to return the relevant timestamp:</p> <p>Example:</p> <pre>&gt; SELECT FIRST(water_level) FROM h2o_feet\n</pre> <p>0.12 return format: 0.13 return format:</p> <pre>name: h2o_feet                              name: h2o_feet\n--------------                              --------------\ntime\t\t\t               first                 time\t\t\t               first\n1970-01-01T00:00:00Z\t 8.12                  2015-08-18T00:00:00Z\t 8.12\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/administration/012_vs_013/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/administration/012_vs_013/</a>\n  </p>\n</div>\n","influxdb/v0.13/administration/upgrading/index":"<h1>Upgrading from previous versions</h1>     <p>This page outlines process for upgrading from:</p> <ul> <li><a href=\"index#upgrading-from-0-12-to-0-13\">Version 0.12 to 0.13</a></li> <li><a href=\"index#upgrading-from-0-10-or-0-11-to-0-13\">Version 0.10 or 0.11 to 0.13</a></li> </ul> <h2 id=\"upgrading-from-0-12-to-0-13\">Upgrading from 0.12 to 0.13</h2> <ol> <li><p><a href=\"https://influxdata.com/downloads/#influxdb\">Download</a> InfluxDB version 0.13</p></li> <li>\n<p>Update the configuration file</p> <p><a href=\"../config/index#using-configuration-files\">Export</a> a default configuration file and manually migrate customizations into it before launching version 0.13.</p>\n</li> <li><p>Check out the new features outlined in <a href=\"../012_vs_013/index\">Differences between InfluxDB 0.13 and 0.12</a></p></li> </ol> <h2 id=\"upgrading-from-0-10-or-0-11-to-0-13\">Upgrading from 0.10 or 0.11 to 0.13</h2> <blockquote> <p><strong>Note:</strong> 0.10 users will need to <a href=\"https://docs.influxdata.com/influxdb/v0.10/administration/upgrading/#convert-b1-and-bz1-shards-to-tsm1\">convert</a> any remaining <code>b1</code> and <code>bz1</code> shards to <code>TSM</code> format before following the instructions below. InfluxDB 0.13 cannot read non-<code>TSM</code> shards. Check for non-<code>TSM</code> shards in your data directory:</p> <ul> <li>Non-<code>TSM</code> shards are files of the form: <code>data/&lt;database&gt;/&lt;retention_policy&gt;/&lt;shard_id&gt;</code>`</li> <li>\n<code>TSM</code> shards are files of the form: <code>data/&lt;database&gt;/&lt;retention_policy&gt;/&lt;shard_id&gt;/&lt;file&gt;.tsm</code>\n</li> </ul> </blockquote> <p>In versions prior to 0.12, InfluxDB stores <a href=\"../../concepts/glossary/index#metastore\">metastore</a> information in <code>raft.db</code> via the raft services. In versions 0.12+, InfluxDB stores metastore information in <code>meta.db</code>, a binary protobuf file.</p> <p>The following steps outline how to transfer metastore information to the new format. They also outline when to upgrade the binary to 0.13 and when to generate a new configuration file.</p> <p>To start out, you must be working with version 0.10 or 0.11 (don’t upgrade the <code>influxd</code> binary yet!). If you’ve already upgraded the binary to 0.13, <a href=\"https://docs.influxdata.com/influxdb/v0.12/administration/upgrading/#urls-for-influxdb-0-11\">reinstall 0.11</a>; InfluxDB 0.13 will yield an error (<code>run: create server: detected /var/lib/influxdb/meta/raft.db. [...]</code>) if you attempt to start the process without completing the steps below. The examples below assume you are working with a version of linux.</p> <blockquote> <p>Before you start, we recommend making a copy of the entire 0.10 or 0.11 <code>meta</code> directory in case you experience problems with the upgrade. The upgrade process removes the <code>raft.db</code> and <code>node.json</code> files from the <code>meta</code> directory:</p> <pre>cp -r &lt;path_to_meta_directory&gt; &lt;path_to_011_meta_directory_backup&gt;\n</pre> <p>Example:</p> <p>Create a copy of the 0.10 or 0.11 <code>meta</code> directory in <code>backups/</code>:</p> <pre>~# cp -r /var/lib/influxdb/meta backups/\n</pre> </blockquote> <p><strong>1.</strong> While still running 0.10 or 0.11, export the metastore data to a different directory:</p> <pre>influxd backup &lt;path_to_metastore_backup&gt;\n</pre> <p>The directory will be created if it doesn’t already exist.</p> <p>Example:</p> <p>Export the 0.10 or 0.11 metastore to <code>/tmp/backup</code>:</p> <pre>~# influxd backup /tmp/backup/\n2016/04/01 15:33:35 backing up metastore to /tmp/backup/meta.00\n2016/04/01 15:33:35 backup complete\n</pre> <p><strong>2.</strong> Stop the <code>influxdb</code> service:</p> <pre>sudo service influxdb stop\n</pre> <p><strong>3.</strong> <a href=\"https://influxdata.com/downloads/#influxdb\">Upgrade</a> the <code>influxd</code> binary to 0.13. but do not start the service.</p> <p><strong>4.</strong> Upgrade your metastore to the 0.13 store by performing a <code>restore</code> with the backup you created in step 1.</p> <pre>influxd restore -metadir=&lt;path_to_013_meta_directory&gt; &lt;path_to_metastore_backup&gt;\n</pre> <p>Example:</p> <p>Restore <code>/tmp/backup</code> to the meta directory in <code>/var/lib/influxdb/meta</code>:</p> <pre>~# influxd restore -metadir=/var/lib/influxdb/meta /tmp/backup\nUsing metastore snapshot: /tmp/backup/meta.00\n</pre> <p><strong>5.</strong> Generate a new configuration file.</p> <p>InfluxDB 0.13 has several new settings in the <a href=\"../config/index\">configuration file</a>.</p> <p>The <code>influxd config</code> command prints out a new TOML-formatted configuration with all the available configuration options set to their default values. On POSIX systems, a new configuration file can be generated by redirecting the output of the command to a file.</p> <pre>influxd config &gt; /etc/influxdb/influxdb_013.conf.generated\n</pre> <p>Compare your old configuration file against the newly generated <a href=\"../config/index\">InfluxDB 0.13 file</a> and manually update any defaults with your localized settings.</p> <p><strong>6.</strong> Start the 0.13 service:</p> <pre>sudo service influxdb start\n</pre> <p><strong>7.</strong> Confirm that your metastore data are present.</p> <p>The 0.13 output from the queries <code>SHOW DATABASES</code>,<code>SHOW USERS</code> and <code>SHOW RETENTION POLICIES ON &lt;database_name&gt;</code> should match the 0.10 or 0.11 output.</p> <p>If your metastore data do not appear to be present, stop the service, reinstall InfluxDB 0.10 or 0.11, restore the copy you made of the entire 0.10 or 0.11 <code>meta</code> directory to the <code>meta</code> directory, and try working through these steps again.</p> <p><strong>8.</strong> Check out the new features outlined in <a href=\"../012_vs_013/index\">Differences between InfluxDB 0.13 and 0.12</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/administration/upgrading/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/administration/upgrading/</a>\n  </p>\n</div>\n","kapacitor/v0.13/introduction/index":"<h1>Introduction</h1>     <p>The introductory documentation includes all the information you need to get up and running with Kapacitor.</p> <h2 id=\"download-https-influxdata-com-downloads-kapacitor\"><a href=\"https://influxdata.com/downloads/#kapacitor\">Download</a></h2> <p>Download Kapacitor.</p> <h2 id=\"installation-kapacitor-v0-13-introduction-installation\"><a href=\"installation/index\">Installation</a></h2> <p>Directions for installing, starting, and configuring Kapacitor.</p> <h2 id=\"getting-started-kapacitor-v0-13-introduction-getting-started\"><a href=\"getting_started/index\">Getting Started</a></h2> <p>This guide will walk you through how to process stream and batch data and teaches you the basics of using and running a Kapacitor daemon.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/introduction/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/introduction/</a>\n  </p>\n</div>\n","influxdb/v0.13/query_language/data_exploration/index":"<h1>Data Exploration</h1>     <p>InfluxQL is an SQL-like query language for interacting with data in InfluxDB. The following sections cover useful query syntax for exploring your data.</p> <p>The basics:</p> <ul> <li>\n<a href=\"index#the-select-statement-and-the-where-clause\">The <code>SELECT</code> statement and the <code>WHERE</code> clause</a><br> ◦ <a href=\"index#the-basic-select-statement\">The basic <code>SELECT</code> statement</a><br> ◦ <a href=\"index#the-select-statement-and-arithmetic\">The <code>SELECT</code> statement and arithmetic</a><br> ◦ <a href=\"index#the-where-clause\">The <code>WHERE</code> clause</a>\n</li> <li>\n<a href=\"index#the-group-by-clause\">The <code>GROUP BY</code> clause</a><br> ◦ <a href=\"index#group-by-tag-values\"><code>GROUP BY</code> tag values</a><br> ◦ <a href=\"index#group-by-time-intervals\"><code>GROUP BY</code> time intervals</a><br> ◦ <a href=\"index#group-by-tag-values-and-a-time-interval\"><code>GROUP BY</code> tag values and a time interval</a><br> ◦ <a href=\"index#the-group-by-clause-and-fill\">The <code>GROUP BY</code> clause and <code>fill()</code></a><br>\n</li> <li>\n<a href=\"index#the-into-clause\">The <code>INTO</code> clause</a><br> ◦ <a href=\"index#relocate-data\">Relocate data</a><br> ◦ <a href=\"index#downsample-data\">Downsample data</a><br>\n</li> </ul> <p>Limit and sort your results:</p> <ul> <li>\n<a href=\"index#limit-query-returns-with-limit-and-slimit\">Limit query returns with <code>LIMIT</code> and <code>SLIMIT</code></a><br> ◦ <a href=\"index#limit-the-number-of-results-returned-per-series-with-limit\">Limit results per series with <code>LIMIT</code></a><br> ◦ <a href=\"index#limit-the-number-of-series-returned-with-slimit\">Limit the number of series returned with <code>SLIMIT</code></a><br> ◦ <a href=\"index#limit-the-number-of-points-and-series-returned-with-limit-and-slimit\">Limit the number of points and series returned with <code>LIMIT</code> and <code>SLIMIT</code></a>\n</li> <li><a href=\"index#sort-query-returns-with-order-by-time-desc\">Sort query returns with <code>ORDER BY time DESC</code></a></li> <li><a href=\"index#paginate-query-returns-with-offset-and-soffset\">Paginate query returns with <code>OFFSET</code> and <code>SOFFSET</code></a></li> </ul> <p>General tips on query syntax:</p> <ul> <li><a href=\"index#multiple-statements-in-queries\">Multiple statements in queries</a></li> <li><a href=\"index#merge-series-in-queries\">Merge series in queries</a></li> <li>\n<a href=\"index#time-syntax-in-queries\">Time syntax in queries</a><br> ◦ <a href=\"index#relative-time\">Relative time</a><br> ◦ <a href=\"index#absolute-time\">Absolute time</a>\n</li> <li>\n<a href=\"index#regular-expressions-in-queries\">Regular expressions in queries</a><br> ◦ <a href=\"index#regular-expressions-and-selecting-measurements\">Regular expressions and selecting measurements</a><br> ◦ <a href=\"index#regular-expressions-and-specifying-tags\">Regular expressions and specifying tags</a>\n</li> </ul> <p>The examples below query data using <a href=\"../../tools/shell/index\">InfluxDB’s Command Line Interface (CLI)</a>. See the <a href=\"../../guides/querying_data/index\">Querying Data</a> guide for how to query data directly using the HTTP API.</p> <h4 id=\"sample-data\">Sample data</h4> <p><br> If you’d like to follow along with the queries in this document, see <a href=\"https://docs.influxdata.com/influxdb/v0.13/sample_data/data_download/\">Sample Data</a> for how to download and write the data to InfluxDB.</p> <p>This document uses publicly available data from the <a href=\"http://tidesandcurrents.noaa.gov/stations.html?type=Water+Levels\">National Oceanic and Atmospheric Administration’s (NOAA) Center for Operational Oceanographic Products and Services</a>. The data include water levels (ft) collected every six seconds at two stations (Santa Monica, CA (ID 9410840) and Coyote Creek, CA (ID 9414575)) over the period from August 18, 2015 through September 18, 2015.</p> <p>A subsample of the data in the measurement <code>h2o_feet</code>:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                level description\t      location\t       water_level\n2015-08-18T00:00:00Z\t  between 6 and 9 feet\t   coyote_creek\t   8.12\n2015-08-18T00:00:00Z\t  below 3 feet\t\t          santa_monica\t   2.064\n2015-08-18T00:06:00Z\t  between 6 and 9 feet\t   coyote_creek\t   8.005\n2015-08-18T00:06:00Z\t  below 3 feet\t\t          santa_monica\t   2.116\n2015-08-18T00:12:00Z\t  between 6 and 9 feet\t   coyote_creek\t   7.887\n2015-08-18T00:12:00Z\t  below 3 feet\t\t          santa_monica\t   2.028\n2015-08-18T00:18:00Z\t  between 6 and 9 feet\t   coyote_creek\t   7.762\n2015-08-18T00:18:00Z\t  below 3 feet\t\t          santa_monica\t   2.126\n2015-08-18T00:24:00Z\t  between 6 and 9 feet\t   coyote_creek\t   7.635\n2015-08-18T00:24:00Z\t  below 3 feet\t\t          santa_monica\t   2.041\n</pre> <p>The <a href=\"../../concepts/glossary/index#series\">series</a> are made up of the <a href=\"../../concepts/glossary/index#measurement\">measurement</a> <code>h2o_feet</code> and the <a href=\"../../concepts/glossary/index#tag-key\">tag key</a> <code>location</code> with the <a href=\"../../concepts/glossary/index#tag-value\">tag values</a> <code>santa_monica</code> and <code>coyote_creek</code>. There are two <a href=\"../../concepts/glossary/index#field\">fields</a>: <code>water_level</code> which stores floats and <code>level description</code> which stores strings. All of the data are in the <code>NOAA_water_database</code> database.</p> <blockquote> <p><strong>Disclaimer:</strong> The <code>level description</code> field isn’t part of the original NOAA data - we snuck it in there for the sake of having a field key with a special character and string <a href=\"../../concepts/glossary/index#field-value\">field values</a>.</p> </blockquote> <h2 id=\"the-select-statement-and-the-where-clause\">The SELECT statement and the <code>WHERE</code> clause</h2> <p>InfluxQL’s <code>SELECT</code> statement follows the form of an SQL <code>SELECT</code> statement where the <code>WHERE</code> clause is optional:</p> <pre data-language=\"sql\">SELECT &lt;stuff&gt; FROM &lt;measurement_name&gt; WHERE &lt;some_conditions&gt;\n</pre> <h3 id=\"the-basic-select-statement\">The basic <code>SELECT</code> statement</h3>  <p>The following three examples return everything from the measurement <code>h2o_feet</code> (see the CLI response at the end of this section). While they all return the same result, they get to that result in slightly different ways and serve to introduce some of the specifics of the <code>SELECT</code> syntax:</p> <p>Select everything from <code>h2o_feet</code> with <code>*</code>:</p> <pre data-language=\"sql\"> &gt; SELECT * FROM h2o_feet\n</pre> <p>Select everything from <code>h2o_feet</code> by specifying each tag key and field key:</p> <pre data-language=\"sql\"> &gt; SELECT \"level description\",location,water_level FROM h2o_feet\n</pre> <ul> <li><p>Separate multiple fields and tags of interest with a comma. Note that you must specify at least one field in the <code>SELECT</code> statement.</p></li> <li><p>Leave identifiers unquoted unless they start with a digit, contain characters other than <code>[A-z,0-9,_]</code>, or if they are an <a href=\"https://github.com/influxdb/influxdb/blob/master/influxql/README.md#keywords\">InfluxQL keyword</a> - then you need to double quote them. Identifiers are database names, retention policy names, user names, measurement names, tag keys, and field keys.</p></li> </ul> <p>Select everything from <code>h2o_feet</code> by fully qualifying the measurement:</p> <pre data-language=\"sql\"> &gt; SELECT * FROM NOAA_water_database.\"default\".h2o_feet\n</pre> <ul> <li>Fully qualify a measurement if you wish to query data from a different database or from a retention policy other than the default <a href=\"../../concepts/glossary/index#retention-policy-rp\">retention policy</a>. A fully qualified measurement takes the following form:<br> <code>\n\"&lt;database&gt;\".\"&lt;retention policy&gt;\".\"&lt;measurement&gt;\"\n</code>\n</li> </ul> <p>The CLI response for all three queries:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                level description\t      location\t       water_level\n2015-08-18T00:00:00Z\t  between 6 and 9 feet\t   coyote_creek\t   8.12\n2015-08-18T00:00:00Z\t  below 3 feet\t\t          santa_monica\t   2.064\n2015-08-18T00:06:00Z\t  between 6 and 9 feet\t   coyote_creek\t   8.005\n2015-08-18T00:06:00Z\t  below 3 feet\t\t          santa_monica\t   2.116\n[...]\n2015-09-18T21:24:00Z\t  between 3 and 6 feet\t   santa_monica\t   5.013\n2015-09-18T21:30:00Z\t  between 3 and 6 feet\t   santa_monica\t   5.01\n2015-09-18T21:36:00Z\t  between 3 and 6 feet\t   santa_monica\t   5.066\n2015-09-18T21:42:00Z\t  between 3 and 6 feet\t   santa_monica\t   4.938\n</pre> <h3 id=\"the-select-statement-and-arithmetic\">The <code>SELECT</code> statement and arithmetic</h3>  <p>Perform basic arithmetic operations on fields that store floats and integers.</p> <p>Add two to the field <code>water_level</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level + 2 FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\n2015-08-18T00:00:00Z\t10.12\n2015-08-18T00:00:00Z\t4.064\n[...]\n2015-09-18T21:36:00Z\t7.066\n2015-09-18T21:42:00Z\t6.938\n</pre> <p>Another example that works:</p> <pre data-language=\"sql\">&gt; SELECT (water_level * 2) + 4 from h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\n2015-08-18T00:00:00Z\t20.24\n2015-08-18T00:00:00Z\t8.128\n[...]\n2015-09-18T21:36:00Z\t14.132\n2015-09-18T21:42:00Z\t13.876\n</pre> <blockquote> <p><strong>Note:</strong> When performing arithmetic on fields that store integers be aware that InfluxDB casts those integers to floats for all mathematical operations. This can lead to <a href=\"../../troubleshooting/frequently_encountered_issues/index#working-with-really-big-or-really-small-integers\">overflow issues</a> for some numbers.</p> </blockquote> <h3 id=\"the-where-clause\">The <code>WHERE</code> clause</h3>  <p>Use a <code>WHERE</code> clause to filter your data based on tags, time ranges, and/or field values.</p> <blockquote> <p><strong>Note:</strong> The quoting syntax for queries differs from the <a href=\"../../concepts/glossary/index#line-protocol\">line protocol</a>. Please review the <a href=\"../../troubleshooting/frequently_encountered_issues/index#single-quoting-and-double-quoting-in-queries\">rules for single and double-quoting</a> in queries.</p> </blockquote> <p><strong>Tags</strong><br> Return data where the tag key <code>location</code> has the tag value <code>santa_monica</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE location = 'santa_monica'\n</pre> <ul> <li>Always single quote tag values in queries - they are strings. Note that double quotes do not work when specifying tag values and can cause queries to silently fail.</li> </ul> <blockquote> <p><strong>Note:</strong> Tags are indexed so queries on tag keys or tag values are more performant than queries on fields.</p> </blockquote> <p>Return data where the tag key <code>location</code> has no tag value (more on regular expressions <a href=\"index#regular-expressions-in-queries\">later</a>):</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE location !~ /./\n</pre> <p>Return data where the tag key <code>location</code> has a value:</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE location =~ /./\n</pre> <p><strong>Time ranges</strong><br> Return data from the past seven days:</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE time &gt; now() - 7d\n</pre> <ul> <li>\n<code>now()</code> is the Unix time of the server at the time the query is executed on that server. For more on <code>now()</code> and other ways to specify time in queries, see <a href=\"index#time-syntax-in-queries\">time syntax in queries</a>.</li> </ul> <p><strong>Field values</strong><br> Return data where the tag key <code>location</code> has the tag value <code>coyote_creek</code> and the field <code>water_level</code> is greater than 8 feet:</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE location = 'coyote_creek' AND  water_level &gt; 8\n</pre> <p>Return data where the tag key <code>location</code> has the tag value <code>santa_monica</code> and the field <code>level description</code> equals <code>'below 3 feet'</code>:</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE location = 'santa_monica' AND \"level description\" = 'below 3 feet'\n</pre> <p>Return data where the field values in <code>water_level</code> plus <code>2</code> are greater than <code>11.9</code>:</p> <pre>&gt; SELECT * FROM h2o_feet WHERE water_level + 2 &gt; 11.9\n</pre> <ul> <li>Always single quote field values that are strings. Note that double quotes do not work when specifying string field values and can cause queries to silently fail.</li> </ul> <blockquote> <p><strong>Note:</strong> Fields are not indexed; queries on fields are not as performant as those on tags.</p> </blockquote> <p>More on the <code>WHERE</code> clause in InfluxQL:</p> <ul> <li>The <code>WHERE</code> clause supports comparisons against strings, booleans, floats, integers, and against the <code>time</code> of the timestamp. It supports using regular expressions to match tags, but not to match fields.</li> <li>Chain logic together using <code>AND</code> and <code>OR</code>, and separate using <code>(</code> and <code>)</code>.</li> <li>Acceptable comparators include:<br> <code>=</code> equal to<br> <code>&lt;&gt;</code> not equal to<br> <code>!=</code> not equal to<br> <code>&gt;</code> greater than<br> <code>&lt;</code> less than<br> <code>=~</code> matches against<br> <code>!~</code> doesn’t match against<br>\n</li> </ul> <h2 id=\"the-group-by-clause\">The GROUP BY clause</h2> <p>Use the <code>GROUP BY</code> clause to group data by tags and/or time intervals. To successfully implement <code>GROUP BY</code>, append the<code>GROUP BY</code> clause to a <code>SELECT</code> statement and pair the <code>SELECT</code> statement with one of InfluxQL’s <a href=\"../functions/index\">functions</a>:</p> <blockquote> <p><strong>Note:</strong> If your query includes both a <code>WHERE</code> clause and a <code>GROUP BY</code> clause, the <code>GROUP BY</code> clause must come after the <code>WHERE</code> clause.</p> </blockquote> <h3 id=\"group-by-tag-values\">GROUP BY tag values</h3> <p>Calculate the <a href=\"../functions/index#mean\"><code>MEAN()</code></a> <code>water_level</code> for the different tag values of <code>location</code>:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">&gt; SELECT MEAN(water_level) FROM h2o_feet GROUP BY location\nname: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t               mean\n----\t\t\t               ----\n1970-01-01T00:00:00Z\t 5.359342451341401\n\n\nname: h2o_feet\ntags: location=santa_monica\ntime\t\t\t               mean\n----\t\t\t               ----\n1970-01-01T00:00:00Z\t 3.530863470081006\n</pre> <blockquote> <p><strong>Note:</strong> In InfluxDB, <a href=\"https://en.wikipedia.org/wiki/Unix_time\">epoch 0</a> (<code>1970-01-01T00:00:00Z</code>) is often used as a null timestamp equivalent. If you request a query that has no timestamp to return, such as an aggregation function with an unbounded time range, InfluxDB returns epoch 0 as the timestamp.</p> </blockquote> <p>Calculate the <a href=\"../functions/index#mean\"><code>MEAN()</code></a> <code>index</code> for every tag set in <code>h2o_quality</code>:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(index) FROM h2o_quality GROUP BY *\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_quality\ntags: location=coyote_creek, randtag=1\ntime\t\t\t               mean\n----\t\t\t               ----\n1970-01-01T00:00:00Z\t 50.55405446521169\n\n\nname: h2o_quality\ntags: location=coyote_creek, randtag=2\ntime\t\t\t               mean\n----\t\t\t               ----\n1970-01-01T00:00:00Z\t 50.49958856271162\n\n\nname: h2o_quality\ntags: location=coyote_creek, randtag=3\ntime\t\t\t               mean\n----\t\t\t               ----\n1970-01-01T00:00:00Z\t 49.5164137518956\n\n\nname: h2o_quality\ntags: location=santa_monica, randtag=1\ntime\t\t\t               mean\n----\t\t\t               ----\n1970-01-01T00:00:00Z\t 50.43829082296367\n\n\nname: h2o_quality\ntags: location=santa_monica, randtag=2\ntime\t\t\t               mean\n----\t\t\t               ----\n1970-01-01T00:00:00Z\t 52.0688508894012\n\n\nname: h2o_quality\ntags: location=santa_monica, randtag=3\ntime\t\t\t               mean\n----\t\t\t               ----\n1970-01-01T00:00:00Z\t 49.29386362086556\n</pre> <h3 id=\"group-by-time-intervals\">GROUP BY time intervals</h3> <p>Users can group data by a given time interval with <code>GROUP BY time()</code>:</p> <pre>SELECT &lt;function&gt;(&lt;field_key&gt;) FROM &lt;measurement_name&gt; WHERE &lt;time_range&gt; GROUP BY time(&lt;time_interval&gt;[,&lt;offset_interval&gt;])\n</pre> <p>InfluxQL requires a <code>WHERE</code> clause if you’re using <code>GROUP BY</code> with <code>time()</code>. Note that unless you specify a different upper and lower bound for the time range, <code>GROUP BY</code> uses <code>epoch 0</code> as the lower bound and <code>now()</code> as the upper bound for the query.</p> <p>Valid units for <code>time()</code> are:<br> <br> <code>u</code> microseconds<br> <code>ms</code> milliseconds<br> <code>s</code> seconds<br> <code>m</code> minutes<br> <code>h</code> hours<br> <code>d</code> days<br> <code>w</code> weeks</p> <h4 id=\"rounded-group-by-time-boundaries\">Rounded <code>GROUP BY time()</code> boundaries</h4> <p>By default, <code>GROUP BY time()</code> returns results that fall on rounded calendar time boundaries.</p> <p>Example:</p> <p><a href=\"../functions/index#count\"><code>COUNT()</code></a> the number of <code>water_level</code> points between August 19, 2015 at midnight and August 27 at 5:00pm at three day intervals:</p> <pre data-language=\"sql\">&gt; SELECT COUNT(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-19T00:00:00Z' AND time &lt;= '2015-08-27T17:00:00Z' AND location='coyote_creek' GROUP BY time(3d)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               count\n2015-08-18T00:00:00Z\t 480\n2015-08-21T00:00:00Z\t 720\n2015-08-24T00:00:00Z\t 720\n2015-08-27T00:00:00Z\t 171\n</pre> <p>Each timestamp represents a three day interval and the value in the <code>count</code> field is the number of <code>water_level</code> points that occur in that three day interval. You could get the same results by querying the data four times - that is, one <code>COUNT()</code> query for every three days between August 19, 2015 at midnight and August 27, 2015 at 5:00pm - but that could take a while.</p> <p>Notice that the first timestamp in the CLI response (<code>2015-08-18T00:00:00Z</code>) occurs before the lower bound of the query’s time range (<code>2015-08-19T00:00:00Z</code>). This is because default <code>GROUP BY time()</code> intervals fall on rounded calendar time boundaries. The <code>count</code> results where <code>time</code> is <code>2015-08-18T00:00:00Z</code>, however, only include data from <code>2015-08-19T00:00:00Z</code>. See <a href=\"../../troubleshooting/frequently_encountered_issues/index#understanding-the-time-intervals-returned-from-group-by-time-queries\">Frequently Encountered Issues</a> for more detailed explanation of the default <code>GROUP BY time()</code> behavior.</p> <h4 id=\"configured-group-by-time-boundaries\">Configured <code>GROUP BY time()</code> boundaries</h4> <p><code>GROUP BY time()</code> also allows you to alter the default rounded calendar time boundaries by including an offset interval.</p> <p>Examples:</p> <p><a href=\"../functions/index#count\"><code>COUNT()</code></a> the number of <code>water_level</code> points between August 19, 2015 at midnight and August 27 at 5:00pm at three day intervals, and offset the time boundary by one day:</p> <pre data-language=\"sql\">&gt; SELECT COUNT(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-19T00:00:00Z' AND time &lt;= '2015-08-27T17:00:00Z' AND location='coyote_creek' GROUP BY time(3d,1d)\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t               count\n2015-08-19T00:00:00Z\t 720\n2015-08-22T00:00:00Z\t 720\n2015-08-25T00:00:00Z\t 651\n</pre> <p>The <code>1d</code> offset interval alters the default three day time interval boundaries<br> from: to:</p> <pre>August 18 - August 20         August 19 - August 21\nAugust 21 - August 23         August 22 - August 24\nAugust 24 - August 26         August 25 - August 27\nAugust 27 - August 29         \n</pre> <p><a href=\"../functions/index#count\"><code>COUNT()</code></a> the number of <code>water_level</code> points between August 19, 2015 at midnight and August 27 at 5:00pm at three day intervals, and offset the time boundary by -2 days:</p> <pre>&gt; SELECT COUNT(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-19T00:00:00Z' AND time &lt;= '2015-08-27T17:00:00Z' AND location='coyote_creek' GROUP BY time(3d,-2d)\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t               count\n2015-08-19T00:00:00Z\t 720\n2015-08-22T00:00:00Z\t 720\n2015-08-25T00:00:00Z\t 651\n</pre> <p>The <code>-2d</code> offset interval alters the default three day time interval boundaries<br> from: to:</p> <pre>August 18 - August 20         August 16 - August 18\nAugust 21 - August 23         August 19 - August 21\nAugust 24 - August 26         August 22 - August 24\nAugust 27 - August 29         August 25 - August 27\n</pre> <p>InfluxDB does not return results for the first time interval (August 16 - August 18), because it is completely outside the time range in the query’s <code>WHERE</code> clause.</p> <h3 id=\"group-by-tag-values-and-a-time-interval\">GROUP BY tag values AND a time interval</h3> <p>Separate multiple <code>GROUP BY</code> arguments with a comma.</p> <p>Calculate the average <code>water_level</code> for the different tag values of <code>location</code> in the last two weeks at 6 hour intervals:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet WHERE time &gt; now() - 2w GROUP BY location,time(6h)\n</pre> <h3 id=\"the-group-by-clause-and-fill\">The <code>GROUP BY</code> clause and <code>fill()</code>\n</h3>  <p>By default, a <code>GROUP BY</code> interval with no data has <code>null</code> as its value in the output column. Use <code>fill()</code> to change the value reported for intervals that have no data. <code>fill()</code> options include:</p> <ul> <li>Any numerical value</li> <li>\n<code>null</code> - sets <code>null</code> as the value for intervals with no data</li> <li>\n<code>previous</code> - copies the value from the previous interval for intervals with no data</li> <li>\n<code>none</code> - skips intervals with no data to report</li> </ul> <p>Follow the ✨ in the examples below to see what <code>fill()</code> can do.</p> <p><strong>GROUP BY without fill()</strong></p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18' AND time &lt; '2015-09-24' GROUP BY time(10d)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t                 mean\n2015-08-13T00:00:00Z\t   4.306212083333323\n2015-08-23T00:00:00Z\t   4.318944629367029\n2015-09-02T00:00:00Z\t   4.363877681204781\n2015-09-12T00:00:00Z   \t4.69811470811633\n✨2015-09-22T00:00:00Z\n</pre> <p><strong>GROUP BY with fill()</strong><br> Use <code>fill()</code> with <code>-100</code>:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18' AND time &lt; '2015-09-24' GROUP BY time(10d) fill(-100)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t                 mean\n2015-08-13T00:00:00Z\t   4.306212083333323\n2015-08-23T00:00:00Z\t   4.318944629367029\n2015-09-02T00:00:00Z\t   4.363877681204781\n2015-09-12T00:00:00Z\t   4.698114708116322\n✨2015-09-22T00:00:00Z\t -100\n</pre> <p>Use <code>fill()</code> with <code>none</code>:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18' AND time &lt; '2015-09-24' GROUP BY time(10d) fill(none)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               mean\n2015-08-13T00:00:00Z\t 4.306212083333323\n2015-08-23T00:00:00Z\t 4.318944629367029\n2015-09-02T00:00:00Z\t 4.363877681204781\n2015-09-12T00:00:00Z\t 4.69811470811633\n✨\n</pre> <blockquote> <p><strong>Note:</strong> If you’re <code>GROUP(ing) BY</code> several things (for example, both tags and a time interval) <code>fill()</code> must go at the end of the <code>GROUP BY</code> clause.</p> </blockquote> <h2 id=\"the-into-clause\">The INTO clause</h2> <h3 id=\"relocate-data\">Relocate data</h3> <p>Copy data to another database, retention policy, and measurement with the <code>INTO</code> clause:</p> <pre data-language=\"sql\">SELECT &lt;field_key&gt; INTO &lt;different_measurement&gt; FROM &lt;current_measurement&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Write the field <code>water_level</code> in <code>h2o_feet</code> to a new measurement (<code>h2o_feet_copy</code>) in the same database:</p> <pre data-language=\"sql\">&gt; SELECT water_level INTO h2o_feet_copy FROM h2o_feet WHERE location = 'coyote_creek'\n</pre> <p>The CLI response shows the number of points that InfluxDB wrote to <code>h2o_feet_copy</code>:</p> <pre>name: result\n------------\ntime\t\t\t               written\n1970-01-01T00:00:00Z\t 7604\n</pre> <p>Write the field <code>water_level</code> in <code>h2o_feet</code> to a new measurement (<code>h2o_feet_copy</code>) and to the retention policy <code>default</code> in the <a href=\"../database_management/index#create-a-database-with-create-database\">already-existing</a> database <code>where_else</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level INTO where_else.\"default\".h2o_feet_copy FROM h2o_feet WHERE location = 'coyote_creek'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: result\n------------\ntime\t\t\t               written\n1970-01-01T00:00:00Z\t 7604\n</pre> <blockquote> <p><strong>Note</strong>: If you use <code>SELECT *</code> with <code>INTO</code>, the query converts tags in the current measurement to fields in the new measurement. This can cause InfluxDB to overwrite points that were previously differentiated by a tag value. Use <code>GROUP BY &lt;tag_key&gt;</code> to preserve tags as tags.</p> </blockquote> <h3 id=\"downsample-data\">Downsample data</h3> <p>Combine the <code>INTO</code> clause with an InfluxQL <a href=\"../functions/index\">function</a> and a <code>GROUP BY</code> clause to write the lower precision query results to a different measurement:</p> <pre data-language=\"sql\">SELECT &lt;function&gt;(&lt;field_key&gt;) INTO &lt;different_measurement&gt; FROM &lt;current_measurement&gt; WHERE &lt;stuff&gt; GROUP BY &lt;stuff&gt;\n</pre> <blockquote> <p><strong>Note:</strong> The <code>INTO</code> queries in this section downsample old data, that is, data that have already been written to InfluxDB. If you want InfluxDB to automatically query and downsample all future data see <a href=\"../continuous_queries/index\">Continuous Queries</a>.</p> </blockquote> <p>Calculate the average <code>water_level</code> in <code>santa_monica</code>, and write the results to a new measurement (<code>average</code>) in the same database:</p> <pre data-language=\"sql\">&gt; SELECT mean(water_level) INTO average FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m)\n</pre> <p>The CLI response shows the number of points that InfluxDB wrote to the new measurement:</p> <pre data-language=\"bash\">name: result\n------------\ntime\t\t\t               written\n1970-01-01T00:00:00Z\t 3\n</pre> <p>To see the query results, select everything from the new measurement <code>average</code> in <code>NOAA_water_database</code>:</p> <pre data-language=\"bash\">&gt; SELECT * FROM average\nname: average\n-------------\ntime\t\t\t               mean\n2015-08-18T00:00:00Z\t 2.09\n2015-08-18T00:12:00Z\t 2.077\n2015-08-18T00:24:00Z\t 2.0460000000000003\n</pre> <p>Calculate the average <code>water_level</code> and the max <code>water_level</code> in <code>santa_monica</code>, and write the results to a new measurement (<code>aggregates</code>) in a different database (<code>where_else</code>):</p> <pre data-language=\"sql\">&gt; SELECT mean(water_level), max(water_level) INTO where_else.\"default\".aggregates FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: result\n------------\ntime\t\t\t               written\n1970-01-01T00:00:00Z\t 3\n</pre> <p>Select everything from the new measurement <code>aggregates</code> in the database <code>where_else</code>:</p> <pre data-language=\"bash\">&gt; SELECT * FROM where_else.\"default\".aggregates\nname: aggregates\n----------------\ntime\t\t\t               max\t   mean\n2015-08-18T00:00:00Z\t 2.116\t 2.09\n2015-08-18T00:12:00Z\t 2.126\t 2.077\n2015-08-18T00:24:00Z\t 2.051\t 2.0460000000000003\n</pre> <p>Calculate the average <code>degrees</code> for all temperature measurements (<code>h2o_temperature</code> and <code>average_temperature</code>) in the <code>NOAA_water_database</code> and write the results to new measurements with the same names in a different database (<code>where_else</code>). <code>:MEASUREMENT</code> tells InfluxDB to write the query results to measurements with the same names as those targeted by the query:</p> <pre data-language=\"sql\">&gt; SELECT mean(degrees) INTO where_else.\"default\".:MEASUREMENT FROM /temperature/ WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: result\n------------\ntime\t\t\t               written\n1970-01-01T00:00:00Z\t 6\n</pre> <p>Select the <code>mean</code> field from all new temperature measurements in the database <code>where_else</code>.</p> <pre data-language=\"bash\">&gt; SELECT mean FROM where_else.\"default\"./temperature/\nname: average_temperature\n-------------------------\ntime\t\t\t               mean\n2015-08-18T00:00:00Z\t 78.5\n2015-08-18T00:12:00Z\t 84\n2015-08-18T00:24:00Z\t 74.75\n\nname: h2o_temperature\n---------------------\ntime\t\t\t                mean\n2015-08-18T00:00:00Z\t  63.75\n2015-08-18T00:12:00Z\t  63.5\n2015-08-18T00:24:00Z\t  63.5\n</pre> <p>More on downsampling with <code>INTO</code>:</p> <ul> <li>InfluxDB does not store null values. Depending on the frequency of your data, the query results may be missing time intervals. Use <a href=\"index#the-group-by-clause-and-fill\">fill()</a> to ensure that every time interval appears in the results.</li> <li>The number of writes in the CLI response includes one write for every time interval in the query’s time range even if there is no data for some of the time intervals.</li> </ul> <h2 id=\"limit-query-returns-with-limit-and-slimit\">Limit query returns with LIMIT and SLIMIT</h2> <p>InfluxQL supports two different clauses to limit your query results:</p> <ul> <li>\n<code>LIMIT &lt;N&gt;</code> returns the first &lt;N&gt; <a href=\"../../concepts/glossary/index#point\">points</a> from each <a href=\"../../concepts/glossary/index#series\">series</a> in the specified measurement.</li> <li>\n<code>SLIMIT &lt;N&gt;</code> returns every point from &lt;N&gt; series in the specified measurement.</li> <li>\n<code>LIMIT &lt;N&gt;</code> followed by <code>SLIMIT &lt;N&gt;</code> returns the first &lt;N&gt; points from &lt;N&gt; series in the specified measurement.</li> </ul> <h3 id=\"limit-the-number-of-results-returned-per-series-with-limit\">Limit the number of results returned per series with <code>LIMIT</code>\n</h3>  <p>Use <code>LIMIT &lt;N&gt;</code> with <code>SELECT</code> and <code>GROUP BY *</code> to return the first &lt;N&gt; points from each series.</p> <p>Return the three oldest points from each series associated with the measurement <code>h2o_feet</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet GROUP BY * LIMIT 3\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t              water_level\n----\t\t\t              -----------\n2015-08-18T00:00:00Z\t8.12\n2015-08-18T00:06:00Z\t8.005\n2015-08-18T00:12:00Z\t7.887\n\nname: h2o_feet\ntags: location=santa_monica\ntime\t\t\t              water_level\n----\t\t\t              -----------\n2015-08-18T00:00:00Z\t2.064\n2015-08-18T00:06:00Z\t2.116\n2015-08-18T00:12:00Z\t2.028\n</pre> <blockquote> <p><strong>Note:</strong> If &lt;N&gt; is greater than the number of points in the series, InfluxDB returns all points in the series.</p> </blockquote> <h3 id=\"limit-the-number-of-series-returned-with-slimit\">Limit the number of series returned with <code>SLIMIT</code>\n</h3>  <p>Use <code>SLIMIT &lt;N&gt;</code> with <code>SELECT</code> and <code>GROUP BY *</code> to return every point from &lt;N&gt; series.</p> <p>Return everything from one of the series associated with the measurement <code>h2o_feet</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet GROUP BY * SLIMIT 1\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t              water_level\n----\t\t\t              -----\n2015-08-18T00:00:00Z\t8.12\n2015-08-18T00:06:00Z\t8.005\n2015-08-18T00:12:00Z\t7.887\n[...]\n2015-09-18T16:12:00Z\t3.402\n2015-09-18T16:18:00Z\t3.314\n2015-09-18T16:24:00Z\t3.235\n</pre> <blockquote> <p><strong>Note:</strong> If &lt;N&gt; is greater than the number of series associated with the specified measurement, InfluxDB returns all points from every series.</p> </blockquote> <h3 id=\"limit-the-number-of-points-and-series-returned-with-limit-and-slimit\">Limit the number of points and series returned with <code>LIMIT</code> and <code>SLIMIT</code>\n</h3>  <p>Use <code>LIMIT &lt;N1&gt;</code> followed by <code>SLIMIT &lt;N2&gt;</code> with <code>GROUP BY *</code> to return &lt;N1&gt; points from &lt;N2&gt; series.</p> <p>Return the three oldest points from one of the series associated with the measurement <code>h2o_feet</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet GROUP BY * LIMIT 3 SLIMIT 1\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t               water_level\n----\t\t\t               -----------\n2015-08-18T00:00:00Z\t 8.12\n2015-08-18T00:06:00Z\t 8.005\n2015-08-18T00:12:00Z\t 7.887\n</pre> <blockquote> <p><strong>Note:</strong> If &lt;N1&gt; is greater than the number of points in the series, InfluxDB returns all points in the series. If &lt;N2&gt; is greater than the number of series associated with the specified measurement, InfluxDB returns points from every series.</p> </blockquote> <h2 id=\"sort-query-returns-with-order-by-time-desc\">Sort query returns with ORDER BY time DESC</h2> <p>By default, InfluxDB returns results in ascending time order - so the first points that are returned are the oldest points by timestamp. Use <code>ORDER BY time DESC</code> to see the newest points by timestamp.</p> <p>Return the oldest five points from one series <strong>without</strong> <code>ORDER BY time DESC</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE location = 'santa_monica' LIMIT 5\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n----------\ntime\t\t\twater_level\n2015-08-18T00:00:00Z\t2.064\n2015-08-18T00:06:00Z\t2.116\n2015-08-18T00:12:00Z\t2.028\n2015-08-18T00:18:00Z\t2.126\n2015-08-18T00:24:00Z\t2.041\n</pre> <p>Now include <code>ORDER BY time DESC</code> to get the newest five points from the same series:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE location = 'santa_monica' ORDER BY time DESC LIMIT 5\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n----------\ntime\t\t\twater_level\n2015-09-18T21:42:00Z\t4.938\n2015-09-18T21:36:00Z\t5.066\n2015-09-18T21:30:00Z\t5.01\n2015-09-18T21:24:00Z\t5.013\n2015-09-18T21:18:00Z\t5.072\n</pre> <p>Finally, use <code>GROUP BY</code> with <code>ORDER BY time DESC</code> to return the last five points from each series:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet GROUP BY location ORDER BY time DESC LIMIT 5\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location=santa_monica\ntime\t\t\t               water_level\n----\t\t\t               -----------\n2015-09-18T21:42:00Z\t 4.938\n2015-09-18T21:36:00Z\t 5.066\n2015-09-18T21:30:00Z\t 5.01\n2015-09-18T21:24:00Z\t 5.013\n2015-09-18T21:18:00Z\t 5.072\n\nname: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t               water_level\n----\t\t\t               -----------\n2015-09-18T16:24:00Z\t 3.235\n2015-09-18T16:18:00Z\t 3.314\n2015-09-18T16:12:00Z\t 3.402\n2015-09-18T16:06:00Z\t 3.497\n2015-09-18T16:00:00Z\t 3.599\n</pre> <h2 id=\"paginate-query-returns-with-offset-and-soffset\">Paginate query returns with OFFSET and SOFFSET</h2> <h3 id=\"use-offset-to-paginate-the-results-returned\">Use <code>OFFSET</code> to paginate the results returned</h3>  <p>For example, get the first three points written to a series:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE location = 'coyote_creek' LIMIT 3\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n----------\ntime\t\t\t               water_level\n2015-08-18T00:00:00Z\t 8.12\n2015-08-18T00:06:00Z\t 8.005\n2015-08-18T00:12:00Z\t 7.887\n</pre> <p>Then get the second three points from that same series:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE location = 'coyote_creek' LIMIT 3 OFFSET 3\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n----------\ntime\t\t\t               water_level\n2015-08-18T00:18:00Z\t 7.762\n2015-08-18T00:24:00Z\t 7.635\n2015-08-18T00:30:00Z\t 7.5\n</pre> <h3 id=\"use-soffset-to-paginate-the-series-returned\">Use <code>SOFFSET</code> to paginate the series returned</h3>  <p>For example, get the first three points from a single series:</p> <pre>&gt; SELECT water_level FROM h2o_feet GROUP BY * LIMIT 3 SLIMIT 1\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t               water_level\n----\t\t\t               -----------\n2015-08-18T00:00:00Z\t 8.12\n2015-08-18T00:06:00Z\t 8.005\n2015-08-18T00:12:00Z\t 7.887\n</pre> <p>Then get the first three points from the next series:</p> <pre>&gt; SELECT water_level FROM h2o_feet GROUP BY * LIMIT 3 SLIMIT 1 SOFFSET 1\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\ntags: location=santa_monica\ntime\t\t\t               water_level\n----\t\t\t               -----------\n2015-08-18T00:00:00Z\t 2.064\n2015-08-18T00:06:00Z\t 2.116\n2015-08-18T00:12:00Z\t 2.028\n</pre> <h2 id=\"multiple-statements-in-queries\">Multiple statements in queries</h2> <p>Separate multiple statements in a query with a semicolon. For example: <br> <br></p> <pre data-language=\"sql\">&gt; SELECT mean(water_level) FROM h2o_feet WHERE time &gt; now() - 2w GROUP BY location,time(24h) fill(none); SELECT count(water_level) FROM h2o_feet WHERE time &gt; now() - 2w GROUP BY location,time(24h) fill(80)\n</pre> <h2 id=\"merge-series-in-queries\">Merge series in queries</h2> <p>In InfluxDB, queries merge series automatically.</p> <p>The <code>NOAA_water_database</code> database has two <a href=\"../../concepts/glossary/index#series\">series</a>. The first series is made up of the measurement <code>h2o_feet</code> and the tag key <code>location</code> with the tag value <code>coyote_creek</code>. The second series is made of up the measurement <code>h2o_feet</code> and the tag key <code>location</code> with the tag value <code>santa_monica</code>.</p> <p>The following query automatically merges those two series when it calculates the <a href=\"../functions/index#mean\">average</a> <code>water_level</code>:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               mean\n1970-01-01T00:00:00Z\t 4.319097913525821\n</pre> <p>If you only want the <code>MEAN()</code> <code>water_level</code> for the first series, specify the tag set in the <code>WHERE</code> clause:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet WHERE location = 'coyote_creek'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               mean\n1970-01-01T00:00:00Z\t 5.296914449406493\n</pre> <blockquote> <p><strong>NOTE:</strong> In InfluxDB, <a href=\"https://en.wikipedia.org/wiki/Unix_time\">epoch 0</a> (<code>1970-01-01T00:00:00Z</code>) is often used as a null timestamp equivalent. If you request a query that has no timestamp to return, such as an aggregation function with an unbounded time range, InfluxDB returns epoch 0 as the timestamp.</p> </blockquote> <h2 id=\"time-syntax-in-queries\">Time syntax in queries</h2> <p>InfluxDB is a time series database so, unsurprisingly, InfluxQL has a lot to do with specifying time ranges. If you do not specify start and end times in your query, they default to epoch 0 (<code>1970-01-01T00:00:00Z</code>) and <code>now()</code>. The following sections detail how to specify different start and end times in queries.</p> <h3 id=\"relative-time\">Relative time</h3>  <p><code>now()</code> is the Unix time of the server at the time the query is executed on that server. Use <code>now()</code> to calculate a timestamp relative to the server’s current timestamp.</p> <p>Query data starting an hour ago and ending <code>now()</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE time &gt; now() - 1h\n</pre> <p>Query data that occur between epoch 0 and 1,000 days from <code>now()</code>:</p> <pre data-language=\"sql\">&gt; SELECT \"level description\" FROM h2o_feet WHERE time &lt; now() + 1000d\n</pre> <ul> <li>Note the whitespace between the operator and the time duration. Leaving that whitespace out can cause InfluxDB to return no results or an <code>error parsing query</code> error .</li> </ul> <p>The other options for specifying time durations with <code>now()</code> are listed below.<br> <code>u</code> microseconds<br> <code>ms</code> milliseconds<br> <code>s</code> seconds<br> <code>m</code> minutes<br> <code>h</code> hours<br> <code>d</code> days<br> <code>w</code> weeks</p> <h3 id=\"absolute-time\">Absolute time</h3>  <h4 id=\"date-time-strings\">Date time strings</h4> <p>Specify time with date time strings. Date time strings can take two formats: <code>YYYY-MM-DD HH:MM:SS.nnnnnnnnn</code> and <code>YYYY-MM-DDTHH:MM:SS.nnnnnnnnnZ</code>, where the second specification is <a href=\"https://www.ietf.org/rfc/rfc3339.txt\">RFC3339</a>. Nanoseconds (<code>nnnnnnnnn</code>) are optional in both formats.</p> <p>Examples:</p> <p>Query data between August 18, 2015 23:00:01.232000000 and September 19, 2015 00:00:00 with the timestamp syntax <code>YYYY-MM-DD HH:MM:SS.nnnnnnnnn</code> and <code>YYYY-MM-DDTHH:MM:SS.nnnnnnnnnZ</code>:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE time &gt; '2015-08-18 23:00:01.232000000' AND time &lt; '2015-09-19'\n</pre> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE time &gt; '2015-08-18T23:00:01.232000000Z' AND time &lt; '2015-09-19'\n</pre> <p>Query data that occur 6 minutes after September 18, 2015 21:24:00:</p> <pre data-language=\"sql\">&gt; SELECT water_level FROM h2o_feet WHERE time &gt; '2015-09-18T21:24:00Z' + 6m\n</pre> <p>Things to note about querying with date time strings:</p> <ul> <li>Single quote the date time string. InfluxDB returns as error (<code>ERR: invalid operation: time and *influxql.VarRef are not compatible</code>) if you double quote the date time string.</li> <li>If you only specify the date, InfluxDB sets the time to <code>00:00:00</code>.</li> </ul> <h4 id=\"epoch-time\">Epoch time</h4> <p>Specify time with timestamps in epoch time. Epoch time is the number of nanoseconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970. Indicate the units of the timestamp at the end of the timestamp (see the section above for a list of acceptable time units).</p> <p>Examples:</p> <p>Return all points that occur after <code>2014-01-01 00:00:00</code>:</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE time &gt; 1388534400s\n</pre> <p>Return all points that occur 6 minutes after <code>2015-09-18 21:24:00</code>:</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE time &gt; 24043524m + 6m\n</pre> <h2 id=\"regular-expressions-in-queries\">Regular expressions in queries</h2> <p>Regular expressions are surrounded by <code>/</code> characters and use <a href=\"http://golang.org/pkg/regexp/syntax/\">Golang’s regular expression syntax</a>. Use regular expressions when selecting measurements and tags.</p> <blockquote> <p><strong>Note:</strong> You cannot use regular expressions to match databases, retention policies, or fields. You can only use regular expressions to match measurements and tags.</p> </blockquote> <p>In this section we’ll be using all of the measurements in the <a href=\"index#sample-data\">sample data</a>: <code>h2o_feet</code>, <code>h2o_quality</code>, <code>h2o_pH</code>, <code>average_temperature</code>, and <code>h2o_temperature</code>. Please note that every measurement besides <code>h2o_feet</code> is fictional and contains fictional data.</p> <h3 id=\"regular-expressions-and-selecting-measurements\">Regular expressions and selecting measurements</h3>  <p>Select the oldest point from every measurement in the <code>NOAA_water_database</code> database:</p> <pre data-language=\"sql\">&gt; SELECT * FROM /.*/ LIMIT 1\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: average_temperature\n-------------------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\t randtag\t water_level\n2015-08-18T00:00:00Z\t 82\t\t\t\t\t                               coyote_creek\n\n\nname: h2o_feet\n--------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\t randtag\t water_level\n2015-08-18T00:00:00Z\t\t\t               between 6 and 9 feet\t coyote_creek\t\t\t            8.12\n\n\nname: h2o_pH\n------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\t randtag\t water_level\n2015-08-18T00:00:00Z\t\t\t\t\t\t                                  coyote_creek\t 7\n\n\nname: h2o_quality\n-----------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\t randtag\t water_level\n2015-08-18T00:00:00Z\t\t         41\t\t\t\t                       coyote_creek\t\t    1\n\n\nname: h2o_temperature\n---------------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\t randtag\t water_level\n2015-08-18T00:00:00Z\t 60\t\t\t\t\t                               coyote_creek\n</pre> <ul> <li>\n<p>Alternatively, <code>SELECT</code> all of the measurements in <code>NOAA_water_database</code> by typing them out and separating each name with a comma , but that could get tedious:</p> <pre data-language=\"sql\">&gt; SELECT * FROM average_temperature,h2o_feet,h2o_pH,h2o_quality,h2o_temperature LIMIT 1\n</pre>\n</li> </ul> <p>Select the first three points from every measurement whose name starts with <code>h2o</code>:</p> <pre data-language=\"sql\">&gt; SELECT * FROM /^h2o/ LIMIT 3\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\trandtag\twater_level\n2015-08-18T00:00:00Z\t\t\t               between 6 and 9 feet\t coyote_creek\t\t\t          8.12\n2015-08-18T00:00:00Z\t\t\t               below 3 feet\t\t        santa_monica\t\t\t          2.064\n2015-08-18T00:06:00Z\t\t\t               between 6 and 9 feet\t coyote_creek\t\t\t          8.005\n\n\nname: h2o_pH\n------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\trandtag\twater_level\n2015-08-18T00:00:00Z\t\t\t\t\t\t                                  coyote_creek\t 7\n2015-08-18T00:00:00Z\t\t\t\t\t\t                                  santa_monica\t 6\n2015-08-18T00:06:00Z\t\t\t\t\t\t                                  coyote_creek\t 8\n\n\nname: h2o_quality\n-----------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\trandtag\twater_level\n2015-08-18T00:00:00Z\t\t         99\t\t\t\t                       santa_monica\t\t   2\n2015-08-18T00:00:00Z\t\t         41\t\t\t\t                       coyote_creek\t\t   1\n2015-08-18T00:06:00Z\t\t         11\t\t\t\t                       coyote_creek\t\t   3\n\n\nname: h2o_temperature\n---------------------\ntime\t\t\t               degrees\t index\t level description\t    location\t     pH\trandtag\twater_level\n2015-08-18T00:00:00Z\t 70\t\t\t\t\t                               santa_monica\n2015-08-18T00:00:00Z\t 60\t\t\t\t\t                               coyote_creek\n2015-08-18T00:06:00Z\t 60\t\t\t\t\t                               santa_monica\n\n</pre> <p>Select the first 5 points from every measurement whose name contains <code>temperature</code>:</p> <pre data-language=\"sql\">&gt; SELECT * FROM /.*temperature.*/ LIMIT 5\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: average_temperature\n-------------------------\ntime\t\t\t              degrees\tlocation\n2015-08-18T00:00:00Z\t85\t     santa_monica\n2015-08-18T00:00:00Z\t82\t     coyote_creek\n2015-08-18T00:06:00Z\t73\t     coyote_creek\n2015-08-18T00:06:00Z\t74\t     santa_monica\n2015-08-18T00:12:00Z\t86\t     coyote_creek\n\nname: h2o_temperature\n---------------------\ntime\t\t\t              degrees\tlocation\n2015-08-18T00:00:00Z\t60\t     coyote_creek\n2015-08-18T00:00:00Z\t70\t     santa_monica\n2015-08-18T00:06:00Z\t65\t     coyote_creek\n2015-08-18T00:06:00Z\t60\t     santa_monica\n2015-08-18T00:12:00Z\t68\t     coyote_creek\n</pre> <h3 id=\"regular-expressions-and-specifying-tags\">Regular expressions and specifying tags</h3>  <p>Use regular expressions to specify tags in the <code>WHERE</code> clause. The relevant comparators include:<br> <code>=~</code> matches against<br> <code>!~</code> doesn’t match against</p> <p>Select the oldest four points from the measurement <code>h2o_feet</code> where the value of the tag <code>location</code> does not include an <code>a</code>:</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE location !~ /.*a.*/ LIMIT 4\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               level description\t    location\t     water_level\n2015-08-18T00:00:00Z\t between 6 and 9 feet \tcoyote_creek\t 8.12\n2015-08-18T00:06:00Z\t between 6 and 9 feet\t coyote_creek\t 8.005\n2015-08-18T00:12:00Z\t between 6 and 9 feet\t coyote_creek\t 7.887\n2015-08-18T00:18:00Z\t between 6 and 9 feet\t coyote_creek\t 7.762\n</pre> <p>Select the oldest four points from the measurement <code>h2o_feet</code> where the value of the tag <code>location</code> includes a <code>y</code> or an <code>m</code> and <code>water_level</code> is greater than zero:</p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE (location =~ /.*y.*/ OR location =~ /.*m.*/) AND water_level &gt; 0 LIMIT 4\n</pre> <p>or <br> <br></p> <pre data-language=\"sql\">&gt; SELECT * FROM h2o_feet WHERE location =~ /[ym]/ AND water_level &gt; 0 LIMIT 4\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t               level description\t    location\t     water_level\n2015-08-18T00:00:00Z\t between 6 and 9 feet\t coyote_creek\t 8.12\n2015-08-18T00:00:00Z\t below 3 feet\t\t        santa_monica\t 2.064\n2015-08-18T00:06:00Z\t between 6 and 9 feet\t coyote_creek\t 8.005\n2015-08-18T00:06:00Z\t below 3 feet\t\t        santa_monica\t 2.116\n</pre> <p>See <a href=\"index#the-where-clause\">the WHERE clause</a> section for an example of how to return data where a tag key has a value and an example of how to return data where a tag key has no value using regular expressions.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/data_exploration/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/query_language/data_exploration/</a>\n  </p>\n</div>\n","influxdb/v0.13/query_language/functions/index":"<h1>Functions</h1>     <p>Use InfluxQL functions to aggregate, select, and transform data.</p> <table> <thead> <tr> <th>Aggregations</th> <th>Selectors</th> <th>Transformations</th> </tr> </thead> <tbody> <tr> <td><a href=\"index#count\">COUNT()</a></td> <td><a href=\"index#bottom\">BOTTOM()</a></td> <td><a href=\"index#ceiling\">CEILING()</a></td> </tr> <tr> <td><a href=\"index#distinct\">DISTINCT()</a></td> <td><a href=\"index#first\">FIRST()</a></td> <td><a href=\"index#derivative\">DERIVATIVE()</a></td> </tr> <tr> <td><a href=\"index#integral\">INTEGRAL()</a></td> <td><a href=\"index#last\">LAST()</a></td> <td><a href=\"index#difference\">DIFFERENCE()</a></td> </tr> <tr> <td><a href=\"index#mean\">MEAN()</a></td> <td><a href=\"index#max\">MAX()</a></td> <td><a href=\"index#elapsed\">ELAPSED()</a></td> </tr> <tr> <td><a href=\"index#median\">MEDIAN()</a></td> <td><a href=\"index#min\">MIN()</a></td> <td><a href=\"index#floor\">FLOOR()</a></td> </tr> <tr> <td><a href=\"index#spread\">SPREAD()</a></td> <td><a href=\"index#percentile\">PERCENTILE()</a></td> <td><a href=\"index#histogram\">HISTOGRAM()</a></td> </tr> <tr> <td><a href=\"index#sum\">SUM()</a></td> <td><a href=\"index#top\">TOP()</a></td> <td><a href=\"index#moving-average\">MOVING_AVERAGE()</a></td> </tr> <tr> <td></td> <td></td> <td><a href=\"index#non-negative-derivative\">NON_NEGATIVE_DERIVATIVE()</a></td> </tr> <tr> <td></td> <td></td> <td><a href=\"index#stddev\">STDDEV()</a></td> </tr> </tbody> </table> <p>Useful InfluxQL for functions:</p> <ul> <li><a href=\"index#include-multiple-functions-in-a-single-query\">Include multiple functions in a single query</a></li> <li><a href=\"index#change-the-value-reported-for-intervals-with-no-data-with-fill\">Change the value reported for intervals with no data with <code>fill()</code> </a></li> <li><a href=\"index#rename-the-output-column-s-title-with-as\">Rename the output column’s title with <code>AS</code></a></li> </ul> <p>The examples below query data using <a href=\"../../tools/shell/index\">InfluxDB’s Command Line Interface (CLI)</a>. See the <a href=\"../../guides/querying_data/index\">Querying Data</a> guide for how to query data directly using the HTTP API.</p> <p><strong>Sample data</strong></p> <p>The examples in this document use the same sample data as the <a href=\"../data_exploration/index\">Data Exploration</a> page. The data are described and are available for download on the <a href=\"https://docs.influxdata.com/influxdb/v0.13/sample_data/data_download/\">Sample Data</a> page.</p> <h1 id=\"aggregations\">Aggregations</h1> <h2 id=\"count\">COUNT()</h2> <p>Returns the number of non-null values in a single <a href=\"../../concepts/glossary/index#field\">field</a>.</p> <pre data-language=\"sql\">SELECT COUNT(&lt;field_key&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Count the number of non-null field values in the <code>water_level</code> field:</li> </ul> <pre data-language=\"sql\">&gt; SELECT COUNT(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               count\n1970-01-01T00:00:00Z\t 15258\n</pre> <blockquote> <p><strong>Note:</strong> Aggregation functions return epoch 0 (<code>1970-01-01T00:00:00Z</code>) as the timestamp unless you specify a lower bound on the time range. Then they return the lower bound as the timestamp.</p> </blockquote> <ul> <li>Count the number of non-null field values in the <code>water_level</code> field at four-day intervals:</li> </ul> <pre data-language=\"sql\">&gt; SELECT COUNT(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-09-18T17:00:00Z' GROUP BY time(4d)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               count\n2015-08-17T00:00:00Z\t 1440\n2015-08-21T00:00:00Z\t 1920\n2015-08-25T00:00:00Z\t 1920\n2015-08-29T00:00:00Z\t 1920\n2015-09-02T00:00:00Z\t 1915\n2015-09-06T00:00:00Z\t 1920\n2015-09-10T00:00:00Z\t 1920\n2015-09-14T00:00:00Z\t 1920\n2015-09-18T00:00:00Z\t 335\n</pre> <blockquote> <h4 id=\"count-and-controlling-the-values-reported-for-intervals-with-no-data\">\n<code>COUNT()</code> and controlling the values reported for intervals with no data</h4> <p><br> Other InfluxQL functions report <code>null</code> values for intervals with no data, and appending <code>fill(&lt;stuff&gt;)</code> to queries with those functions replaces <code>null</code> values in the output with <code>&lt;stuff&gt;</code>. <code>COUNT()</code>, however, reports <code>0</code>s for intervals with no data, so appending <code>fill(&lt;stuff&gt;)</code> to queries with <code>COUNT()</code> replaces <code>0</code>s in the output with <code>&lt;stuff&gt;</code>.</p> <p>Example: Use <code>fill(none)</code> to suppress intervals with <code>0</code> data</p> <p><code>COUNT()</code> without <code>fill(none)</code>:</p> <pre data-language=\"bash\">&gt; SELECT COUNT(water_level) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-09-18T21:41:00Z' AND time &lt;= '2015-09-18T22:41:00Z' GROUP BY time(30m)\nname: h2o_feet\n--------------\ntime\t\t\t               count\n2015-09-18T21:30:00Z\t 1\n2015-09-18T22:00:00Z\t 0\n2015-09-18T22:30:00Z\t 0\n</pre> <p><code>COUNT()</code> with <code>fill(none)</code>:</p> <pre data-language=\"bash\">&gt; SELECT COUNT(water_level) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-09-18T21:41:00Z' AND time &lt;= '2015-09-18T22:41:00Z' GROUP BY time(30m) fill(none)\nname: h2o_feet\n--------------\ntime\t\t\t               count\n2015-09-18T21:30:00Z\t 1\n</pre> <p>For a more general discussion of <code>fill()</code>, see <a href=\"../data_exploration/index#the-group-by-clause-and-fill\">Data Exploration</a>.</p> </blockquote> <h2 id=\"distinct\">DISTINCT()</h2> <p>Returns the unique values of a single <a href=\"../../concepts/glossary/index#field\">field</a>.</p> <pre data-language=\"sql\">SELECT DISTINCT(&lt;field_key&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Select the unique field values in the <code>level description</code> field:</li> </ul> <pre data-language=\"sql\">&gt; SELECT DISTINCT(\"level description\") FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               distinct\n1970-01-01T00:00:00Z\t between 6 and 9 feet\n1970-01-01T00:00:00Z\t below 3 feet\n1970-01-01T00:00:00Z\t between 3 and 6 feet\n1970-01-01T00:00:00Z\t at or greater than 9 feet\n</pre> <p>The response shows that <code>level description</code> has four distinct field values. The timestamp reflects the first time the field value appears in the data.</p> <blockquote> <p><strong>Note:</strong> Aggregation functions return epoch 0 (<code>1970-01-01T00:00:00Z</code>) as the timestamp unless you specify a lower bound on the time range. Then they return the lower bound as the timestamp.</p> </blockquote> <ul> <li>Select the unique field values in the <code>level description</code> field grouped by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT DISTINCT(\"level description\") FROM h2o_feet GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t                distinct\n----\t\t\t                --------\n1970-01-01T00:00:00Z\t  between 6 and 9 feet\n1970-01-01T00:00:00Z\t  between 3 and 6 feet\n1970-01-01T00:00:00Z\t  below 3 feet\n1970-01-01T00:00:00Z\t  at or greater than 9 feet\n\n\nname: h2o_feet\ntags: location=santa_monica\ntime\t\t\t                distinct\n----\t\t\t                --------\n1970-01-01T00:00:00Z\t  below 3 feet\n1970-01-01T00:00:00Z\t  between 3 and 6 feet\n1970-01-01T00:00:00Z\t  between 6 and 9 feet\n</pre> <ul> <li>Nest <code>DISTINCT()</code> in <a href=\"index#count\"><code>COUNT()</code></a> to get the number of unique field values in <code>level description</code> grouped by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT COUNT(DISTINCT(\"level description\")) FROM h2o_feet GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location = coyote_creek\ntime\t\t\t               count\n----\t\t\t               -----\n1970-01-01T00:00:00Z\t 4\n\nname: h2o_feet\ntags: location = santa_monica\ntime\t\t\t               count\n----\t\t\t               -----\n1970-01-01T00:00:00Z\t 3\n</pre> <h2 id=\"integral\">INTEGRAL()</h2> <p><code>INTEGRAL()</code> is not yet functional.</p> \n<dt> See GitHub Issue <a href=\"https://github.com/influxdata/influxdb/issues/5930\">#5930</a> for more information. </dt> <h2 id=\"mean\">MEAN()</h2> <p>Returns the arithmetic mean (average) for the values in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field type must be int64 or float64.</p> <pre data-language=\"sql\">SELECT MEAN(&lt;field_key&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Calculate the average value of the <code>water_level</code> field:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               mean\n1970-01-01T00:00:00Z\t 4.286791371454075\n</pre> <blockquote> <p><strong>Notes:</strong></p> <ul> <li>Aggregation functions return epoch 0 (<code>1970-01-01T00:00:00Z</code>) as the timestamp unless you specify a lower bound on the time range. Then they return the lower bound as the timestamp.</li> <li>Executing <code>mean()</code> on the same set of float64 points may yield slightly different results. InfluxDB does not sort points before it applies the function which results in those small discrepancies.</li> </ul> </blockquote> <ul> <li>Calculate the average value in the field <code>water_level</code> at four-day intervals:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-09-18T17:00:00Z' GROUP BY time(4d)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               mean\n2015-08-17T00:00:00Z\t 4.322029861111125\n2015-08-21T00:00:00Z\t 4.251395512375667\n2015-08-25T00:00:00Z\t 4.285036458333324\n2015-08-29T00:00:00Z\t 4.469495801899061\n2015-09-02T00:00:00Z\t 4.382785378590083\n2015-09-06T00:00:00Z\t 4.28849666349042\n2015-09-10T00:00:00Z\t 4.658127604166656\n2015-09-14T00:00:00Z\t 4.763504687500006\n2015-09-18T00:00:00Z\t 4.232829850746268\n</pre> <h2 id=\"median\">MEDIAN()</h2> <p>Returns the middle value from the sorted values in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field values must be of type int64 or float64.</p> <pre data-language=\"sql\">SELECT MEDIAN(&lt;field_key&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <blockquote> <p><strong>Note:</strong> <code>MEDIAN()</code> is nearly equivalent to <a href=\"index#percentile\"><code>PERCENTILE(field_key, 50)</code></a>, except <code>MEDIAN()</code> returns the average of the two middle values if the field contains an even number of points.</p> </blockquote> <p>Examples:</p> <ul> <li>Select the median value in the field <code>water_level</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MEDIAN(water_level) from h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               median\n1970-01-01T00:00:00Z\t 4.124\n</pre> <blockquote> <p><strong>Note:</strong> Aggregation functions return epoch 0 (<code>1970-01-01T00:00:00Z</code>) as the timestamp unless you specify a lower bound on the time range. Then they return the lower bound as the timestamp.</p> </blockquote> <ul> <li>Select the median value of <code>water_level</code> between August 18, 2015 at 00:00:00 and August 18, 2015 at 00:30:00 grouped by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MEDIAN(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-08-18T00:36:00Z' GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location = coyote_creek\ntime\t\t\t               median\n----\t\t\t               ------\n2015-08-18T00:00:00Z\t 7.8245\n\nname: h2o_feet\ntags: location = santa_monica\ntime\t\t\t               median\n----\t\t\t               ------\n2015-08-18T00:00:00Z\t 2.0575\n</pre> <h2 id=\"spread\">SPREAD()</h2> <p>Returns the difference between the minimum and maximum values of a <a href=\"../../concepts/glossary/index#field\">field</a>. The field must be of type int64 or float64.</p> <pre data-language=\"sql\">SELECT SPREAD(&lt;field_key&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Calculate the difference between the minimum and maximum values across all values in the <code>water_level</code> field:</li> </ul> <pre data-language=\"sql\">&gt; SELECT SPREAD(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                spread\n1970-01-01T00:00:00Z\t  10.574\n</pre> <blockquote> <p><strong>Notes:</strong></p> <ul> <li>Aggregation functions return epoch 0 (<code>1970-01-01T00:00:00Z</code>) as the timestamp unless you specify a lower bound on the time range. Then they return the lower bound as the timestamp.</li> <li>Executing <code>spread()</code> on the same set of float64 points may yield slightly different results. InfluxDB does not sort points before it applies the function which results in those small discrepancies.</li> </ul> </blockquote> <ul> <li>Calculate the difference between the minimum and maximum values in the field <code>water_level</code> for a specific tag and time range and at 30 minute intervals:</li> </ul> <pre data-language=\"sql\">&gt; SELECT SPREAD(water_level) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-09-18T17:00:00Z' AND time &lt; '2015-09-18T20:30:00Z' GROUP BY time(30m)\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                spread\n2015-09-18T17:00:00Z\t  0.16699999999999982\n2015-09-18T17:30:00Z\t  0.5469999999999997\n2015-09-18T18:00:00Z\t  0.47499999999999964\n2015-09-18T18:30:00Z\t  0.2560000000000002\n2015-09-18T19:00:00Z\t  0.23899999999999988\n2015-09-18T19:30:00Z\t  0.1609999999999996\n2015-09-18T20:00:00Z\t  0.16800000000000015\n</pre> <h2 id=\"sum\">SUM()</h2> <p>Returns the sum of the all values in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field must be of type int64 or float64.</p> <pre data-language=\"sql\">SELECT SUM(&lt;field_key&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Calculate the sum of the values in the <code>water_level</code> field:</li> </ul> <pre data-language=\"sql\">&gt; SELECT SUM(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               sum\n1970-01-01T00:00:00Z\t 67777.66900000002\n</pre> <blockquote> <p><strong>Notes:</strong></p> <ul> <li>Aggregation functions return epoch 0 (<code>1970-01-01T00:00:00Z</code>) as the timestamp unless you specify a lower bound on the time range. Then they return the lower bound as the timestamp.</li> <li>Executing <code>sum()</code> on the same set of float64 points may yield slightly different results. InfluxDB does not sort points before it applies the function which results in those small discrepancies.</li> </ul> </blockquote> <ul> <li>Calculate the sum of the <code>water_level</code> field grouped by five-day intervals:</li> </ul> <pre data-language=\"sql\">&gt; SELECT SUM(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-09-18T17:00:00Z' GROUP BY time(5d)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">--------------\ntime\t\t\t               sum\n2015-08-18T00:00:00Z\t 10334.908999999983\n2015-08-23T00:00:00Z\t 10113.356999999995\n2015-08-28T00:00:00Z\t 10663.683000000006\n2015-09-02T00:00:00Z\t 10451.321\n2015-09-07T00:00:00Z\t 10871.817999999994\n2015-09-12T00:00:00Z\t 11459.00099999999\n2015-09-17T00:00:00Z\t 3627.762000000003\n</pre> <h1 id=\"selectors\">Selectors</h1> <h2 id=\"bottom\">BOTTOM()</h2> <p>Returns the smallest <code>N</code> values in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field type must be int64 or float64.</p> <pre data-language=\"sql\">SELECT BOTTOM(&lt;field_key&gt;[,&lt;tag_keys&gt;],&lt;N&gt;)[,&lt;tag_keys&gt;] FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Select the smallest three values of <code>water_level</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT BOTTOM(water_level,3) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               bottom\n2015-08-29T14:30:00Z\t -0.61\n2015-08-29T14:36:00Z\t -0.591\n2015-08-30T15:18:00Z\t -0.594\n</pre> <ul> <li>Select the smallest three values of <code>water_level</code> and include the relevant <code>location</code> tag in the output:</li> </ul> <pre data-language=\"sql\">&gt; SELECT BOTTOM(water_level,3),location FROM h2o_feet\n</pre> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               bottom\t location\n2015-08-29T14:30:00Z\t -0.61\t  coyote_creek\n2015-08-29T14:36:00Z\t -0.591\t coyote_creek\n2015-08-30T15:18:00Z\t -0.594\t coyote_creek\n</pre> <ul> <li>Select the smallest value of <code>water_level</code> within each tag value of <code>location</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT BOTTOM(water_level,location,2) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               bottom\t location\n2015-08-29T10:36:00Z\t -0.243\t santa_monica\n2015-08-29T14:30:00Z\t -0.61\t  coyote_creek\n</pre> <p>The output shows the bottom values of <code>water_level</code> for each tag value of <code>location</code> (<code>santa_monica</code> and <code>coyote_creek</code>).</p> <blockquote> <p><strong>Note:</strong> Queries with the syntax <code>SELECT BOTTOM(&lt;field_key&gt;,&lt;tag_key&gt;,&lt;N&gt;)</code>, where the tag has <code>X</code> distinct values, return <code>N</code> or <code>X</code> field values, whichever is smaller, and each returned point has a unique tag value. To demonstrate this behavior, see the results of the above example query where <code>N</code> equals <code>3</code> and <code>N</code> equals <code>1</code>.</p> <ul> <li>\n<code>N</code> = <code>3</code>\n</li> </ul> <pre data-language=\"sql\">SELECT BOTTOM(water_level,location,3) FROM h2o_feet\n</pre> <p>CLI response: <br> <br></p> <pre>name: h2o_feet\n--------------\ntime\t\t\t               bottom\t location\n2015-08-29T10:36:00Z\t -0.243\t santa_monica\n2015-08-29T14:30:00Z\t -0.61\t  coyote_creek\n</pre> <p>InfluxDB returns two values instead of three because the <code>location</code> tag has only two values (<code>santa_monica</code> and <code>coyote_creek</code>).</p> <ul> <li>\n<code>N</code> = <code>1</code>\n</li> </ul> <pre data-language=\"sql\">&gt; SELECT BOTTOM(water_level,location,1) FROM h2o_feet\n</pre> <p>CLI response: <br> <br></p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               bottom\t location\n2015-08-29T14:30:00Z\t -0.61\t  coyote_creek\n</pre> <p>InfluxDB compares the bottom values of <code>water_level</code> within each tag value of <code>location</code> and returns the smaller value of <code>water_level</code>.</p> </blockquote> <ul> <li>Select the smallest two values of <code>water_level</code> between August 18, 2015 at 4:00:00 and August 18, 2015 at 4:18:00 for every tag value of <code>location</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT BOTTOM(water_level,2) FROM h2o_feet WHERE time &gt;= '2015-08-18T04:00:00Z' AND time &lt; '2015-08-18T04:24:00Z' GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t               bottom\n----\t\t\t               ------\n2015-08-18T04:12:00Z\t 2.717\n2015-08-18T04:18:00Z\t 2.625\n\n\nname: h2o_feet\ntags: location=santa_monica\ntime\t\t\t               bottom\n----\t\t\t               ------\n2015-08-18T04:00:00Z\t 3.911\n2015-08-18T04:06:00Z\t 4.055\n</pre> <ul> <li>Select the smallest two values of <code>water_level</code> between August 18, 2015 at 4:00:00 and August 18, 2015 at 4:18:00 in <code>santa_monica</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT BOTTOM(water_level,2) FROM h2o_feet WHERE time &gt;= '2015-08-18T04:00:00Z' AND time &lt; '2015-08-18T04:24:00Z' AND location = 'santa_monica'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               bottom\n2015-08-18T04:00:00Z\t 3.911\n2015-08-18T04:06:00Z\t 4.055\n</pre> <p>Note that in the raw data, <code>water_level</code> equals <code>4.055</code> at <code>2015-08-18T04:06:00Z</code> and at <code>2015-08-18T04:12:00Z</code>. In the case of a tie, InfluxDB returns the value with the earlier timestamp.</p> <h2 id=\"first\">FIRST()</h2> <p>Returns the oldest value (determined by the timestamp) of a single <a href=\"../../concepts/glossary/index#field\">field</a>.</p> <pre data-language=\"sql\">SELECT FIRST(&lt;field_key&gt;)[,&lt;tag_key(s)&gt;] FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Select the oldest value of the field <code>water_level</code> where the <code>location</code> is <code>santa_monica</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT FIRST(water_level) FROM h2o_feet WHERE location = 'santa_monica'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               first\n2015-08-18T00:00:00Z\t 2.064\n</pre> <ul> <li>Select the oldest value of the field <code>water_level</code> between <code>2015-08-18T00:42:00Z</code> and <code>2015-08-18T00:54:00Z</code>, and output the relevant <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT FIRST(water_level),location FROM h2o_feet WHERE time &gt;= '2015-08-18T00:42:00Z' and time &lt;= '2015-08-18T00:54:00Z'\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t               first\t location\n2015-08-18T00:42:00Z\t 7.234\t coyote_creek\n</pre> <ul> <li>Select the oldest values of the field <code>water_level</code> grouped by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT FIRST(water_level) FROM h2o_feet GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location = coyote_creek\ntime\t\t\t               first\n----\t\t\t               -----\n2015-08-18T00:00:00Z\t 8.12\n\nname: h2o_feet\ntags: location = santa_monica\ntime\t\t\t               first\n----\t\t\t               -----\n2015-08-18T00:00:00Z\t 2.064\n</pre> <h2 id=\"last\">LAST()</h2> <p>Returns the newest value (determined by the timestamp) of a single <a href=\"../../concepts/glossary/index#field\">field</a>.</p> <pre data-language=\"sql\">SELECT LAST(&lt;field_key&gt;)[,&lt;tag_key(s)&gt;] FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Select the newest value of the field <code>water_level</code> where the <code>location</code> is <code>santa_monica</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT LAST(water_level) FROM h2o_feet WHERE location = 'santa_monica'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               last\n2015-09-18T21:42:00Z\t 4.938\n</pre> <ul> <li>Select the newest value of the field <code>water_level</code> between <code>2015-08-18T00:42:00Z</code> and <code>2015-08-18T00:54:00Z</code>, and output the relevant <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT LAST(water_level),location FROM h2o_feet WHERE time &gt;= '2015-08-18T00:42:00Z' and time &lt;= '2015-08-18T00:54:00Z'\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t               last\t  location\n2015-08-18T00:54:00Z\t 6.982\t coyote_creek\n</pre> <ul> <li>Select the newest values of the field <code>water_level</code> grouped by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT LAST(water_level) FROM h2o_feet GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location = coyote_creek\ntime\t\t\t               last\n----\t\t\t               ----\n2015-09-18T16:24:00Z\t 3.235\n\nname: h2o_feet\ntags: location = santa_monica\ntime\t\t\t               last\n----\t\t\t               ----\n2015-09-18T21:42:00Z\t 4.938\n</pre> <blockquote> <p><strong>Note:</strong> <code>LAST()</code> does not return points that occur after <code>now()</code> unless the <code>WHERE</code> clause specifies that time range. See <a href=\"../../troubleshooting/frequently_encountered_issues/index#querying-after-now\">Frequently Encountered Issues</a> for how to query after <code>now()</code>.</p> </blockquote> <h2 id=\"max\">MAX()</h2> <p>Returns the highest value in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field must be an int64, float64, or boolean.</p> <pre data-language=\"sql\">SELECT MAX(&lt;field_key&gt;)[,&lt;tag_key(s)&gt;] FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Select the maximum <code>water_level</code> in the measurement <code>h2o_feet</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MAX(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               max\n2015-08-29T07:24:00Z\t 9.964\n</pre> <ul> <li>Select the maximum <code>water_level</code> in the measurement <code>h2o_feet</code> and output the relevant <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MAX(water_level),location FROM h2o_feet\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t               max\t   location\n2015-08-29T07:24:00Z\t 9.964\t coyote_creek\n</pre> <ul> <li>Select the maximum <code>water_level</code> in the measurement <code>h2o_feet</code> between August 18, 2015 at midnight and August 18, 2015 at 00:48 grouped at 12 minute intervals and by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MAX(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-08-18T00:54:00Z' GROUP BY time(12m), location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location = coyote_creek\ntime\t\t\t                max\n----\t\t  \t              ---\n2015-08-18T00:00:00Z\t  8.12\n2015-08-18T00:12:00Z\t  7.887\n2015-08-18T00:24:00Z\t  7.635\n2015-08-18T00:36:00Z\t  7.372\n2015-08-18T00:48:00Z\t  7.11\n\nname: h2o_feet\ntags: location = santa_monica\ntime\t\t\t                max\n----\t\t  \t              ---\n2015-08-18T00:00:00Z\t  2.116\n2015-08-18T00:12:00Z\t  2.126\n2015-08-18T00:24:00Z\t  2.051\n2015-08-18T00:36:00Z\t  2.067\n2015-08-18T00:48:00Z\t  1.991\n</pre> <h2 id=\"min\">MIN()</h2> <p>Returns the lowest value in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field must be an int64, float64, or boolean.</p> <pre data-language=\"sql\">SELECT MIN(&lt;field_key&gt;)[,&lt;tag_key(s)&gt;] FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Select the minimum <code>water_level</code> in the measurement <code>h2o_feet</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MIN(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               min\n2015-08-29T14:30:00Z\t -0.61\n</pre> <ul> <li>Select the minimum <code>water_level</code> in the measurement <code>h2o_feet</code> and output the relevant <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MIN(water_level),location FROM h2o_feet\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t              min\t   location\n2015-08-29T14:30:00Z\t-0.61\t coyote_creek\n</pre> <ul> <li>Select the minimum <code>water_level</code> in the measurement <code>h2o_feet</code> between August 18, 2015 at midnight and August 18, at 00:48 grouped at 12 minute intervals and by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT MIN(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-08-18T00:54:00Z' GROUP BY time(12m), location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location = coyote_creek\ntime\t\t\t                 min\n----\t\t\t                 ---\n2015-08-18T00:00:00Z\t   8.005\n2015-08-18T00:12:00Z\t   7.762\n2015-08-18T00:24:00Z\t   7.5\n2015-08-18T00:36:00Z\t   7.234\n2015-08-18T00:48:00Z\t   7.11\n\nname: h2o_feet\ntags: location = santa_monica\ntime\t\t\t                 min\n----\t\t\t                 ---\n2015-08-18T00:00:00Z\t   2.064\n2015-08-18T00:12:00Z\t   2.028\n2015-08-18T00:24:00Z\t   2.041\n2015-08-18T00:36:00Z\t   2.057\n2015-08-18T00:48:00Z\t   1.991\n</pre> <h2 id=\"percentile\">PERCENTILE()</h2> <p>Returns the <code>N</code>th percentile value for the sorted values of a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field must be of type int64 or float64. The percentile <code>N</code> must be an integer or floating point number between 0 and 100, inclusive.</p> <pre data-language=\"sql\">SELECT PERCENTILE(&lt;field_key&gt;, &lt;N&gt;)[,&lt;tag_key(s)&gt;] FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Calculate the fifth percentile of the field <code>water_level</code> where the tag <code>location</code> equals <code>coyote_creek</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT PERCENTILE(water_level,5) FROM h2o_feet WHERE location = 'coyote_creek'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               percentile\n2015-09-09T11:42:00Z\t 1.148\n</pre> <p>The value <code>1.148</code> is larger than 5% of the values in <code>water_level</code> where <code>location</code> equals <code>coyote_creek</code>.</p> <ul> <li>Calculate the fifth percentile of the field <code>water_level</code> and output the relevant <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT PERCENTILE(water_level,5),location FROM h2o_feet\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t                  percentile\t location\n2015-08-28T12:06:00Z\t  1.122\t\t     santa_monica\n</pre> <ul> <li>Calculate the 100th percentile of the field <code>water_level</code> grouped by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT PERCENTILE(water_level, 100) FROM h2o_feet GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location = coyote_creek\ntime\t\t\t               percentile\n----\t\t\t               ----------\n2015-08-29T07:24:00Z\t 9.964\n\nname: h2o_feet\ntags: location = santa_monica\ntime\t\t\t               percentile\n----\t\t\t               ----------\n2015-08-29T03:54:00Z\t 7.205\n</pre> <p>Notice that <code>PERCENTILE(&lt;field_key&gt;,100)</code> is equivalent to <code>MAX(&lt;field_key&gt;)</code>.</p> \n<dt> Currently, <code>PERCENTILE(&lt;field_key&gt;,0)</code> is not equivalent to <code>MIN(&lt;field_key&gt;)</code>. See GitHub Issue <a href=\"https://github.com/influxdata/influxdb/issues/4418\">#4418</a> for more information. </dt> <blockquote> <p><strong>Note</strong>: <code>PERCENTILE(&lt;field_key&gt;, 50)</code> is nearly equivalent to <code>MEDIAN()</code>, except <code>MEDIAN()</code> returns the average of the two middle values if the field contains an even number of points.</p> </blockquote> <h2 id=\"top\">TOP()</h2> <p>Returns the largest <code>N</code> values in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field type must be int64 or float64.</p> <pre data-language=\"sql\">SELECT TOP(&lt;field_key&gt;[,&lt;tag_keys&gt;],&lt;N&gt;)[,&lt;tag_keys&gt;] FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Select the largest three values of <code>water_level</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT TOP(water_level,3) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               top\n2015-08-29T07:18:00Z\t 9.957\n2015-08-29T07:24:00Z\t 9.964\n2015-08-29T07:30:00Z\t 9.954\n</pre> <ul> <li>Select the largest three values of <code>water_level</code> and include the relevant <code>location</code> tag in the output:</li> </ul> <pre data-language=\"sql\">&gt; SELECT TOP(water_level,3),location FROM h2o_feet\n</pre> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               top\t   location\n2015-08-29T07:18:00Z\t 9.957\t coyote_creek\n2015-08-29T07:24:00Z\t 9.964\t coyote_creek\n2015-08-29T07:30:00Z\t 9.954\t coyote_creek\n</pre> <ul> <li>Select the largest value of <code>water_level</code> within each tag value of <code>location</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT TOP(water_level,location,2) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               top\t   location\n2015-08-29T03:54:00Z\t 7.205\t santa_monica\n2015-08-29T07:24:00Z\t 9.964\t coyote_creek\n</pre> <p>The output shows the top values of <code>water_level</code> for each tag value of <code>location</code> (<code>santa_monica</code> and <code>coyote_creek</code>).</p> <blockquote> <p><strong>Note:</strong> Queries with the syntax <code>SELECT TOP(&lt;field_key&gt;,&lt;tag_key&gt;,&lt;N&gt;)</code>, where the tag has <code>X</code> distinct values, return <code>N</code> or <code>X</code> field values, whichever is smaller, and each returned point has a unique tag value. To demonstrate this behavior, see the results of the above example query where <code>N</code> equals <code>3</code> and <code>N</code> equals <code>1</code>.</p> <ul> <li>\n<code>N</code> = <code>3</code>\n</li> </ul> <pre data-language=\"sql\">&gt; SELECT TOP(water_level,location,3) FROM h2o_feet\n</pre> <p>CLI response: <br> <br></p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               top\t   location\n2015-08-29T03:54:00Z\t 7.205\t santa_monica\n2015-08-29T07:24:00Z\t 9.964\t coyote_creek\n</pre> <p>InfluxDB returns two values instead of three because the <code>location</code> tag has only two values (<code>santa_monica</code> and <code>coyote_creek</code>).</p> <ul> <li>\n<code>N</code> = <code>1</code>\n</li> </ul> <pre data-language=\"sql\">&gt; SELECT TOP(water_level,location,1) FROM h2o_feet\n</pre> <p>CLI response: <br> <br></p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               top\t   location\n2015-08-29T07:24:00Z\t 9.964\t coyote_creek\n</pre> <p>InfluxDB compares the top values of <code>water_level</code> within each tag value of <code>location</code> and returns the larger value of <code>water_level</code>.</p> </blockquote> <ul> <li>Select the largest two values of <code>water_level</code> between August 18, 2015 at 4:00:00 and August 18, 2015 at 4:18:00 for every tag value of <code>location</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT TOP(water_level,2) FROM h2o_feet WHERE time &gt;= '2015-08-18T04:00:00Z' AND time &lt; '2015-08-18T04:24:00Z' GROUP BY location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location=coyote_creek\ntime\t\t\t               top\n----\t\t\t               ---\n2015-08-18T04:00:00Z\t 2.943\n2015-08-18T04:06:00Z\t 2.831\n\n\nname: h2o_feet\ntags: location=santa_monica\ntime\t\t\t               top\n----\t\t\t               ---\n2015-08-18T04:06:00Z\t 4.055\n2015-08-18T04:18:00Z\t 4.124\n</pre> <ul> <li>Select the largest two values of <code>water_level</code> between August 18, 2015 at 4:00:00 and August 18, 2015 at 4:18:00 in <code>santa_monica</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT TOP(water_level,2) FROM h2o_feet WHERE time &gt;= '2015-08-18T04:00:00Z' AND time &lt; '2015-08-18T04:24:00Z' AND location = 'santa_monica'\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               top\n2015-08-18T04:06:00Z\t 4.055\n2015-08-18T04:18:00Z\t 4.124\n</pre> <p>Note that in the raw data, <code>water_level</code> equals <code>4.055</code> at <code>2015-08-18T04:06:00Z</code> and at <code>2015-08-18T04:12:00Z</code>. In the case of a tie, InfluxDB returns the value with the earlier timestamp.</p> <h1 id=\"transformations\">Transformations</h1> <h2 id=\"ceiling\">CEILING()</h2> <p><code>CEILING()</code> is not yet functional.</p> \n<dt> See GitHub Issue <a href=\"https://github.com/influxdata/influxdb/issues/5930\">#5930</a> for more information. </dt> <h2 id=\"derivative\">DERIVATIVE()</h2> <p>Returns the rate of change for the values in a single <a href=\"../../concepts/glossary/index#field\">field</a> in a <a href=\"../../concepts/glossary/index#series\">series</a>. InfluxDB calculates the difference between chronological field values and converts those results into the rate of change per <code>unit</code>. The <code>unit</code> argument is optional and, if not specified, defaults to one second (<code>1s</code>).</p> <p>The basic <code>DERIVATIVE()</code> query:</p> <pre data-language=\"sql\">SELECT DERIVATIVE(&lt;field_key&gt;, [&lt;unit&gt;]) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;]\n</pre> <p>Valid time specifications for <code>unit</code> are:<br> <code>u</code> microseconds<br> <code>s</code> seconds<br> <code>m</code> minutes<br> <code>h</code> hours<br> <code>d</code> days<br> <code>w</code> weeks</p> <p><code>DERIVATIVE()</code> also works with a nested function coupled with a <code>GROUP BY time()</code> clause. For queries that include those options, InfluxDB first performs the aggregation, selection, or transformation across the time interval specified in the <code>GROUP BY time()</code> clause. It then calculates the difference between chronological field values and converts those results into the rate of change per <code>unit</code>. The <code>unit</code> argument is optional and, if not specified, defaults to the same interval as the <code>GROUP BY time()</code> interval.</p> <p>The <code>DERIVATIVE()</code> query with an aggregation function and <code>GROUP BY time()</code> clause:</p> <pre data-language=\"sql\">SELECT DERIVATIVE(AGGREGATION_FUNCTION(&lt;field_key&gt;),[&lt;unit&gt;]) FROM &lt;measurement_name&gt; WHERE &lt;stuff&gt; GROUP BY time(&lt;aggregation_interval&gt;)\n</pre> <p>Examples:</p> <p>The following examples work with the first six observations of the <code>water_level</code> field in the measurement <code>h2o_feet</code> with the tag set <code>location = santa_monica</code>:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               water_level\n2015-08-18T00:00:00Z\t 2.064\n2015-08-18T00:06:00Z\t 2.116\n2015-08-18T00:12:00Z\t 2.028\n2015-08-18T00:18:00Z\t 2.126\n2015-08-18T00:24:00Z\t 2.041\n2015-08-18T00:30:00Z\t 2.051\n</pre> <ul> <li>\n<code>DERIVATIVE()</code> with a single argument:<br> Calculate the rate of change per one second</li> </ul> <pre data-language=\"sql\">&gt; SELECT DERIVATIVE(water_level) FROM h2o_feet WHERE location = 'santa_monica' LIMIT 5\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               derivative\n2015-08-18T00:06:00Z\t 0.00014444444444444457\n2015-08-18T00:12:00Z\t -0.00024444444444444465\n2015-08-18T00:18:00Z\t 0.0002722222222222218\n2015-08-18T00:24:00Z\t -0.000236111111111111\n2015-08-18T00:30:00Z\t 2.777777777777842e-05\n</pre> <p>Notice that the first field value (<code>0.00014</code>) in the <code>derivative</code> column is <strong>not</strong> <code>0.052</code> (the difference between the first two field values in the raw data: <code>2.116</code> - <code>2.604</code> = <code>0.052</code>). Because the query does not specify the <code>unit</code> option, InfluxDB automatically calculates the rate of change per one second, not the rate of change per six minutes. The calculation of the first value in the <code>derivative</code> column looks like this: <br> <br></p> <pre>(2.116 - 2.064) / (360s / 1s)\n</pre> <p>The numerator is the difference between chronological field values. The denominator is the difference between the relevant timestamps in seconds (<code>2015-08-18T00:06:00Z</code> - <code>2015-08-18T00:00:00Z</code> = <code>360s</code>) divided by <code>unit</code> (<code>1s</code>). This returns the rate of change per second from <code>2015-08-18T00:00:00Z</code> to <code>2015-08-18T00:06:00Z</code>.</p> <ul> <li>\n<code>DERIVATIVE()</code> with two arguments:<br> Calculate the rate of change per six minutes</li> </ul> <pre data-language=\"sql\">&gt; SELECT DERIVATIVE(water_level,6m) FROM h2o_feet WHERE location = 'santa_monica' LIMIT 5\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               derivative\n2015-08-18T00:06:00Z\t 0.052000000000000046\n2015-08-18T00:12:00Z\t -0.08800000000000008\n2015-08-18T00:18:00Z\t 0.09799999999999986\n2015-08-18T00:24:00Z\t -0.08499999999999996\n2015-08-18T00:30:00Z\t 0.010000000000000231\n</pre> <p>The calculation of the first value in the <code>derivative</code> column looks like this: <br> <br></p> <pre>(2.116 - 2.064) / (6m / 6m)\n</pre> <p>The numerator is the difference between chronological field values. The denominator is the difference between the relevant timestamps in minutes (<code>2015-08-18T00:06:00Z</code> - <code>2015-08-18T00:00:00Z</code> = <code>6m</code>) divided by <code>unit</code> (<code>6m</code>). This returns the rate of change per six minutes from <code>2015-08-18T00:00:00Z</code> to <code>2015-08-18T00:06:00Z</code>.</p> <ul> <li>\n<code>DERIVATIVE()</code> with two arguments:<br> Calculate the rate of change per 12 minutes</li> </ul> <pre data-language=\"sql\">&gt; SELECT DERIVATIVE(water_level,12m) FROM h2o_feet WHERE location = 'santa_monica' LIMIT 5\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               derivative\n2015-08-18T00:06:00Z\t 0.10400000000000009\n2015-08-18T00:12:00Z\t -0.17600000000000016\n2015-08-18T00:18:00Z\t 0.19599999999999973\n2015-08-18T00:24:00Z\t -0.16999999999999993\n2015-08-18T00:30:00Z\t 0.020000000000000462\n</pre> <p>The calculation of the first value in the <code>derivative</code> column looks like this: <br> <br></p> <pre>(2.116 - 2.064 / (6m / 12m)\n</pre> <p>The numerator is the difference between chronological field values. The denominator is the difference between the relevant timestamps in minutes (<code>2015-08-18T00:06:00Z</code> - <code>2015-08-18T00:00:00Z</code> = <code>6m</code>) divided by <code>unit</code> (<code>12m</code>). This returns the rate of change per 12 minutes from <code>2015-08-18T00:00:00Z</code> to <code>2015-08-18T00:06:00Z</code>.</p> <blockquote> <p><strong>Note:</strong> Specifying <code>12m</code> as the <code>unit</code> <strong>does not</strong> mean that InfluxDB calculates the rate of change for every 12 minute interval of data. Instead, InfluxDB calculates the rate of change per 12 minutes for each interval of valid data.</p> </blockquote> <ul> <li>\n<code>DERIVATIVE()</code> with one argument, a function, and a <code>GROUP BY time()</code> clause:<br> Select the <code>MAX()</code> value at 12 minute intervals and calculate the rate of change per 12 minutes</li> </ul> <pre data-language=\"sql\">&gt; SELECT DERIVATIVE(MAX(water_level)) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-08-18T00:36:00Z' GROUP BY time(12m)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               derivative\n2015-08-18T00:12:00Z\t 0.009999999999999787\n2015-08-18T00:24:00Z\t -0.07499999999999973\n</pre> <p>To get those results, InfluxDB first aggregates the data by calculating the <code>MAX()</code> <code>water_level</code> at the time interval specified in the <code>GROUP BY time()</code> clause (<code>12m</code>). Those results look like this:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               max\n2015-08-18T00:00:00Z\t 2.116\n2015-08-18T00:12:00Z\t 2.126\n2015-08-18T00:24:00Z\t 2.051\n</pre> <p>Second, InfluxDB calculates the rate of change per <code>12m</code> (the same interval as the <code>GROUP BY time()</code> interval) to get the results in the <code>derivative</code> column above. The calculation of the first value in the <code>derivative</code> column looks like this: <br> <br></p> <pre>(2.126 - 2.116) / (12m / 12m)\n</pre> <p>The numerator is the difference between chronological field values. The denominator is the difference between the relevant timestamps in minutes (<code>2015-08-18T00:12:00Z</code> - <code>2015-08-18T00:00:00Z</code> = <code>12m</code>) divided by <code>unit</code> (<code>12m</code>). This returns rate of change per 12 minutes for the aggregated data from <code>2015-08-18T00:00:00Z</code> to <code>2015-08-18T00:12:00Z</code>.</p> <ul> <li>\n<code>DERIVATIVE()</code> with two arguments, a function, and a <code>GROUP BY time()</code> clause:<br> Aggregate the data to 18 minute intervals and calculate the rate of change per six minutes</li> </ul> <pre data-language=\"sql\">&gt; SELECT DERIVATIVE(SUM(water_level),6m) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-08-18T00:36:00Z' GROUP BY time(18m)\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               derivative\n2015-08-18T00:18:00Z\t 0.0033333333333332624\n</pre> <p>To get those results, InfluxDB first aggregates the data by calculating the <code>SUM()</code> of <code>water_level</code> at the time interval specified in the <code>GROUP BY time()</code> clause (<code>18m</code>). The aggregated results look like this:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               sum\n2015-08-18T00:00:00Z\t 6.208\n2015-08-18T00:18:00Z\t 6.218\n</pre> <p>Second, InfluxDB calculates the rate of change per <code>unit</code> (<code>6m</code>) to get the results in the <code>derivative</code> column above. The calculation of the first value in the <code>derivative</code> column looks like this: <br> <br></p> <pre>(6.218 - 6.208) / (18m / 6m)\n</pre> <p>The numerator is the difference between chronological field values. The denominator is the difference between the relevant timestamps in minutes (<code>2015-08-18T00:18:00Z</code> - <code>2015-08-18T00:00:00Z</code> = <code>18m</code>) divided by <code>unit</code> (<code>6m</code>). This returns the rate of change per six minutes for the aggregated data from <code>2015-08-18T00:00:00Z</code> to <code>2015-08-18T00:18:00Z</code>.</p> <h2 id=\"difference\">DIFFERENCE()</h2> <p>Returns the difference between consecutive chronological values in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field type must be int64 or float64.</p> <p>The basic <code>DIFFERENCE()</code> query:</p> <pre>SELECT DIFFERENCE(&lt;field_key&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;]\n</pre> <p>The <code>DIFFERENCE()</code> query with a nested function and a <code>GROUP BY time()</code> clause:</p> <pre>SELECT DIFFERENCE(&lt;function&gt;(&lt;field_key&gt;)) FROM &lt;measurement_name&gt; WHERE &lt;stuff&gt; GROUP BY time(&lt;time_interval&gt;)\n</pre> <p>Functions that work with <code>DIFFERENCE()</code> include <a href=\"index#count\"><code>COUNT()</code></a>, <a href=\"index#mean\"><code>MEAN()</code></a>, <a href=\"index#median\"><code>MEDIAN()</code></a>, <a href=\"index#sum\"><code>SUM()</code></a>, <a href=\"index#first\"><code>FIRST()</code></a>, <a href=\"index#last\"><code>LAST()</code></a>, <a href=\"index#min\"><code>MIN()</code></a>, <a href=\"index#max\"><code>MAX()</code></a>, and <a href=\"index#percentile\"><code>PERCENTILE()</code></a>.</p> <p>Examples:</p> <p>The following examples focus on the field <code>water_level</code> in <code>santa_monica</code> between <code>2015-08-18T00:00:00Z</code> and <code>2015-08-18T00:36:00Z</code>:</p> <pre>&gt; SELECT water_level FROM h2o_feet WHERE location='santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:36:00Z'\nname: h2o_feet\n--------------\ntime\t\t\t                water_level\n2015-08-18T00:00:00Z\t  2.064\n2015-08-18T00:06:00Z\t  2.116\n2015-08-18T00:12:00Z\t  2.028\n2015-08-18T00:18:00Z\t  2.126\n2015-08-18T00:24:00Z\t  2.041\n2015-08-18T00:30:00Z\t  2.051\n2015-08-18T00:36:00Z\t  2.067\n</pre> <ul> <li>Calculate the difference between <code>water_level</code> values:</li> </ul> <pre>&gt; SELECT DIFFERENCE(water_level) FROM h2o_feet WHERE location='santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:36:00Z'\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                difference\n2015-08-18T00:06:00Z\t  0.052000000000000046\n2015-08-18T00:12:00Z\t  -0.08800000000000008\n2015-08-18T00:18:00Z\t  0.09799999999999986\n2015-08-18T00:24:00Z\t  -0.08499999999999996\n2015-08-18T00:30:00Z\t  0.010000000000000231\n2015-08-18T00:36:00Z\t  0.016000000000000014\n</pre> <p>The first value in the <code>difference</code> column is <code>2.116 - 2.064</code>, and the second value in the <code>difference</code> column is <code>2.028 - 2.116</code>. Please note that the extra decimal places are the result of floating point inaccuracies.</p> <ul> <li>Select the minimum <code>water_level</code> values at 12 minute intervals and calculate the difference between those values:</li> </ul> <pre>&gt; SELECT DIFFERENCE(MIN(water_level)) FROM h2o_feet WHERE location='santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:36:00Z' GROUP BY time(12m)\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                difference\n2015-08-18T00:12:00Z\t  -0.03600000000000003\n2015-08-18T00:24:00Z\t  0.0129999999999999\n2015-08-18T00:36:00Z\t  0.026000000000000245\n</pre> <p>To get the values in the <code>difference</code> column, InfluxDB first selects the <code>MIN()</code> values at 12 minute intervals:</p> <pre>&gt; SELECT MIN(water_level) FROM h2o_feet WHERE location='santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:36:00Z' GROUP BY time(12m)\nname: h2o_feet\n--------------\ntime\t\t\t                min\n2015-08-18T00:00:00Z  \t2.064\n2015-08-18T00:12:00Z  \t2.028\n2015-08-18T00:24:00Z  \t2.041\n2015-08-18T00:36:00Z  \t2.067\n</pre> <p>It then uses those values to calculate the difference between chronological values; the first value in the <code>difference</code> column is <code>2.028 - 2.064</code>.</p> <h2 id=\"elapsed\">ELAPSED()</h2> <p>Returns the difference between subsequent timestamps in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The <code>unit</code> argument is an optional <a href=\"../spec/index#durations\">duration literal</a> and, if not specified, defaults to one nanosecond.</p> <pre>SELECT ELAPSED(&lt;field_key&gt;, &lt;unit&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Calculate the difference (in nanoseconds) between the timestamps in the field <code>h2o_feet</code>:</li> </ul> <pre>&gt; SELECT ELAPSED(water_level) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:24:00Z'\n</pre> <p>CLI Response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                elapsed\n2015-08-18T00:06:00Z\t  360000000000\n2015-08-18T00:12:00Z\t  360000000000\n2015-08-18T00:18:00Z\t  360000000000\n2015-08-18T00:24:00Z\t  360000000000\n</pre> <ul> <li>Calculate the number of one minute intervals between the timestamps in the field <code>h2o_feet</code>:</li> </ul> <pre>&gt; SELECT ELAPSED(water_level,1m) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:24:00Z'\n</pre> <p>CLI Response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                elapsed\n2015-08-18T00:06:00Z\t  6\n2015-08-18T00:12:00Z\t  6\n2015-08-18T00:18:00Z\t  6\n2015-08-18T00:24:00Z\t  6\n</pre> <blockquote> <p><strong>Note:</strong> InfluxDB returns <code>0</code> if <code>unit</code> is greater than the difference between the timestamps. For example, the timestamps in <code>h2o_feet</code> occur at six minute intervals. If the query asks for the number of one hour intervals between the timestamps, InfluxDB returns <code>0</code>:</p> <pre>&gt; SELECT ELAPSED(water_level,1h) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:24:00Z'\nname: h2o_feet\n--------------\ntime\t\t\t                elapsed\n2015-08-18T00:06:00Z\t  0\n2015-08-18T00:12:00Z\t  0\n2015-08-18T00:18:00Z\t  0\n2015-08-18T00:24:00Z\t  0\n</pre> </blockquote> <h2 id=\"floor\">FLOOR()</h2> <p><code>FLOOR()</code> is not yet functional.</p> \n<dt> See GitHub Issue <a href=\"https://github.com/influxdb/influxdb/issues/5930\">#5930</a> for more information. </dt> <h2 id=\"histogram\">HISTOGRAM()</h2> <p><code>HISTOGRAM()</code> is not yet functional.</p> \n<dt> See GitHub Issue <a href=\"https://github.com/influxdb/influxdb/issues/5930\">#5930</a> for more information. </dt> <h2 id=\"moving-average\">MOVING_AVERAGE()</h2> <p>Returns the moving average across a <code>window</code> of consecutive chronological field values for a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field type must be int64 or float64.</p> <p>The basic <code>MOVING_AVERAGE()</code> query:</p> <pre>SELECT MOVING_AVERAGE(&lt;field_key&gt;,&lt;window&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;]\n</pre> <p>The <code>MOVING_AVERAGE()</code> query with a nested function and a <code>GROUP BY time()</code> clause:</p> <pre>SELECT MOVING_AVERAGE(&lt;function&gt;(&lt;field_key&gt;),&lt;window&gt;) FROM &lt;measurement_name&gt; WHERE &lt;stuff&gt; GROUP BY time(&lt;time_interval&gt;)\n</pre> <p>Functions that work with <code>MOVING_AVERAGE()</code> include <a href=\"index#count\"><code>COUNT()</code></a>, <a href=\"index#mean\"><code>MEAN()</code></a>, <a href=\"index#median\"><code>MEDIAN()</code></a>, <a href=\"index#sum\"><code>SUM()</code></a>, <a href=\"index#first\"><code>FIRST()</code></a>, <a href=\"index#last\"><code>LAST()</code></a>, <a href=\"index#min\"><code>MIN()</code></a>, <a href=\"index#max\"><code>MAX()</code></a>, and <a href=\"index#percentile\"><code>PERCENTILE()</code></a>.</p> <p>Examples:</p> <p>The following examples focus on the field <code>water_level</code> in <code>santa_monica</code> between <code>2015-08-18T00:00:00Z</code> and <code>2015-08-18T00:36:00Z</code>:</p> <pre>&gt; SELECT water_level FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:36:00Z'\nname: h2o_feet\n--------------\ntime\t\t\t                water_level\n2015-08-18T00:00:00Z\t  2.064\n2015-08-18T00:06:00Z\t  2.116\n2015-08-18T00:12:00Z\t  2.028\n2015-08-18T00:18:00Z\t  2.126\n2015-08-18T00:24:00Z\t  2.041\n2015-08-18T00:30:00Z\t  2.051\n2015-08-18T00:36:00Z\t  2.067\n</pre> <ul> <li>Calculate the moving average across every 2 field values:</li> </ul> <pre>&gt; SELECT MOVING_AVERAGE(water_level,2) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:36:00Z'\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                moving_average\n2015-08-18T00:06:00Z\t  2.09\n2015-08-18T00:12:00Z\t  2.072\n2015-08-18T00:18:00Z\t  2.077\n2015-08-18T00:24:00Z\t  2.0835\n2015-08-18T00:30:00Z\t  2.0460000000000003\n2015-08-18T00:36:00Z\t  2.059\n</pre> <p>The first value in the <code>moving_average</code> column is the average of <code>2.064</code> and <code>2.116</code>, the second value in the <code>moving_average</code> column is the average of <code>2.116</code> and <code>2.028</code>.</p> <ul> <li>Select the minimum <code>water_level</code> at 12 minute intervals and calculate the moving average across every 2 field values:</li> </ul> <pre>&gt; SELECT MOVING_AVERAGE(MIN(water_level),2) FROM h2o_feet WHERE location = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' and time &lt;= '2015-08-18T00:36:00Z' GROUP BY time(12m)\n</pre> <p>CLI response:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                moving_average\n2015-08-18T00:12:00Z\t  2.0460000000000003\n2015-08-18T00:24:00Z\t  2.0345000000000004\n2015-08-18T00:36:00Z\t  2.0540000000000003\n</pre> <p>To get those results, InfluxDB first selects the <code>MIN()</code> <code>water_level</code> for every 12 minute interval:</p> <pre>name: h2o_feet\n--------------\ntime\t\t\t                min\n2015-08-18T00:00:00Z\t  2.064\n2015-08-18T00:12:00Z\t  2.028\n2015-08-18T00:24:00Z\t  2.041\n2015-08-18T00:36:00Z\t  2.067\n</pre> <p>It then uses those values to calculate the moving average across every 2 field values; the first result in the <code>moving_average</code> column the average of <code>2.064</code> and <code>2.028</code>, and the second result is the average of <code>2.028</code> and <code>2.041</code>.</p> <h2 id=\"non-negative-derivative\">NON_NEGATIVE_DERIVATIVE()</h2> <p>Returns the non-negative rate of change for the values in a single <a href=\"../../concepts/glossary/index#field\">field</a> in a <a href=\"../../concepts/glossary/index#series\">series</a>. InfluxDB calculates the difference between chronological field values and converts those results into the rate of change per <code>unit</code>. The <code>unit</code> argument is optional and, if not specified, defaults to one second (<code>1s</code>).</p> <p>The basic <code>NON_NEGATIVE_DERIVATIVE()</code> query:</p> <pre data-language=\"sql\">SELECT NON_NEGATIVE_DERIVATIVE(&lt;field_key&gt;, [&lt;unit&gt;]) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;]\n</pre> <p>Valid time specifications for <code>unit</code> are:<br> <code>u</code> microseconds<br> <code>s</code> seconds<br> <code>m</code> minutes<br> <code>h</code> hours<br> <code>d</code> days<br> <code>w</code> weeks</p> <p><code>NON_NEGATIVE_DERIVATIVE()</code> also works with a nested function coupled with a <code>GROUP BY time()</code> clause. For queries that include those options, InfluxDB first performs the aggregation, selection, or transformation across the time interval specified in the <code>GROUP BY time()</code> clause. It then calculates the difference between chronological field values and converts those results into the rate of change per <code>unit</code>. The <code>unit</code> argument is optional and, if not specified, defaults to the same interval as the <code>GROUP BY time()</code> interval.</p> <p>The <code>NON_NEGATIVE_DERIVATIVE()</code> query with an aggregation function and <code>GROUP BY time()</code> clause:</p> <pre data-language=\"sql\">SELECT NON_NEGATIVE_DERIVATIVE(AGGREGATION_FUNCTION(&lt;field_key&gt;),[&lt;unit&gt;]) FROM &lt;measurement_name&gt; WHERE &lt;stuff&gt; GROUP BY time(&lt;aggregation_interval&gt;)\n</pre> <p>See <a href=\"index#derivative\"><code>DERIVATIVE()</code></a> for example queries. All query results are the same for <code>DERIVATIVE()</code> and <code>NON_NEGATIVE_DERIVATIVE</code> except that <code>NON_NEGATIVE_DERIVATIVE()</code> returns only the positive values.</p> <h2 id=\"stddev\">STDDEV()</h2> <p>Returns the standard deviation of the values in a single <a href=\"../../concepts/glossary/index#field\">field</a>. The field must be of type int64 or float64.</p> <pre data-language=\"sql\">SELECT STDDEV(&lt;field_key&gt;) FROM &lt;measurement_name&gt; [WHERE &lt;stuff&gt;] [GROUP BY &lt;stuff&gt;]\n</pre> <p>Examples:</p> <ul> <li>Calculate the standard deviation for the <code>water_level</code> field in the measurement <code>h2o_feet</code>:</li> </ul> <pre data-language=\"sql\">&gt; SELECT STDDEV(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               stddev\n1970-01-01T00:00:00Z\t 2.279144584196145\n</pre> <blockquote> <p><strong>Notes:</strong></p> <ul> <li>\n<code>stddev()</code> returns epoch 0 (<code>1970-01-01T00:00:00Z</code>) as the timestamp unless you specify a lower bound on the time range. Then it returns the lower bound as the timestamp.</li> <li>Executing <code>stddev()</code> on the same set of float64 points may yield slightly different results. InfluxDB does not sort points before it applies the function which results in those small discrepancies.</li> </ul> </blockquote> <ul> <li>Calculate the standard deviation for the <code>water_level</code> field between August 18, 2015 at midnight and September 18, 2015 at noon grouped at one week intervals and by the <code>location</code> tag:</li> </ul> <pre data-language=\"sql\">&gt; SELECT STDDEV(water_level) FROM h2o_feet WHERE time &gt;= '2015-08-18T00:00:00Z' and time &lt; '2015-09-18T12:06:00Z' GROUP BY time(1w), location\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\ntags: location = coyote_creek\ntime\t\t\t               stddev\n----\t\t\t               ------\n2015-08-13T00:00:00Z\t 2.2437263080193985\n2015-08-20T00:00:00Z\t 2.121276150144719\n2015-08-27T00:00:00Z\t 3.0416122170786215\n2015-09-03T00:00:00Z\t 2.5348065025435207\n2015-09-10T00:00:00Z\t 2.584003954882673\n2015-09-17T00:00:00Z\t 2.2587514836274414\n\nname: h2o_feet\ntags: location = santa_monica\ntime\t\t\t               stddev\n----\t\t\t               ------\n2015-08-13T00:00:00Z\t 1.11156344587553\n2015-08-20T00:00:00Z\t 1.0909849279082366\n2015-08-27T00:00:00Z\t 1.9870116180096962\n2015-09-03T00:00:00Z\t 1.3516778450902067\n2015-09-10T00:00:00Z\t 1.4960573811500588\n2015-09-17T00:00:00Z\t 1.075701669442093\n</pre> <h2 id=\"include-multiple-functions-in-a-single-query\">Include multiple functions in a single query</h2> <p>Separate multiple functions in one query with a <code>,</code>.</p> <p>Calculate the <a href=\"index#min\">minimum</a> <code>water_level</code> and the <a href=\"index#max\">maximum</a> <code>water_level</code> with a single query:</p> <pre data-language=\"sql\">&gt; SELECT MIN(water_level), MAX(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               min\t   max\n1970-01-01T00:00:00Z\t -0.61\t 9.964\n</pre> <h2 id=\"change-the-value-reported-for-intervals-with-no-data-with-fill\">Change the value reported for intervals with no data with <code>fill()</code>\n</h2> <p>By default, queries with an InfluxQL function report <code>null</code> values for intervals with no data. Append <code>fill()</code> to the end of your query to alter that value. For a complete discussion of <code>fill()</code>, see <a href=\"../data_exploration/index#the-group-by-clause-and-fill\">Data Exploration</a>.</p> <blockquote> <p><strong>Note:</strong> <code>fill()</code> works differently with <code>COUNT()</code>. See <a href=\"index#count-and-controlling-the-values-reported-for-intervals-with-no-data\">the documentation on <code>COUNT()</code></a> for a function-specific use of <code>fill()</code>.</p> </blockquote> <h2 id=\"rename-the-output-column-s-title-with-as\">Rename the output column’s title with <code>AS</code>\n</h2> <p>By default, queries that include a function output a column that has the same name as that function. If you’d like a different column name change it with an <code>AS</code> clause.</p> <p>Before:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               mean\n1970-01-01T00:00:00Z\t 4.442107025822521\n</pre> <p>After:</p> <pre data-language=\"sql\">&gt; SELECT MEAN(water_level) AS dream_name FROM h2o_feet\n</pre> <p>CLI response:</p> <pre data-language=\"bash\">name: h2o_feet\n--------------\ntime\t\t\t               dream_name\n1970-01-01T00:00:00Z\t 4.442107025822521\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/functions/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/query_language/functions/</a>\n  </p>\n</div>\n","influxdb/v0.13/query_language/spec/index":"<h1>InfluxDB Query Language Reference</h1>     <h2 id=\"introduction\">Introduction</h2> <p>This is a reference for the Influx Query Language (“InfluxQL”). If you’re looking for less formal documentation see <a href=\"../data_exploration/index\">Data Exploration</a>, <a href=\"../schema_exploration/index\">Schema Exploration</a>, <a href=\"../database_management/index\">Database Management</a>, and <a href=\"../../administration/authentication_and_authorization/index\">Authentication and Authorization</a>.</p> <p>InfluxQL is a SQL-like query language for interacting with InfluxDB. It has been lovingly crafted to feel familiar to those coming from other SQL or SQL-like environments while providing features specific to storing and analyzing time series data.</p> <p>Sections:</p> <ul> <li><a href=\"index#notation\">Notation</a></li> <li><a href=\"index#query-representation\">Query representation</a></li> <li><a href=\"index#letters-and-digits\">Letters and digits</a></li> <li><a href=\"index#identifiers\">Identifiers</a></li> <li><a href=\"index#keywords\">Keywords</a></li> <li><a href=\"index#literals\">Literals</a></li> <li><a href=\"index#queries\">Queries</a></li> <li><a href=\"index#statements\">Statements</a></li> <li><a href=\"index#clauses\">Clauses</a></li> <li><a href=\"index#expressions\">Expressions</a></li> <li><a href=\"index#other\">Other</a></li> <li><a href=\"index#query-engine-internals\">Query Engine Internals</a></li> </ul> <h2 id=\"notation\">Notation</h2> <p>The syntax is specified using Extended Backus-Naur Form (“EBNF”). EBNF is the same notation used in the <a href=\"http://golang.org\">Go</a> programming language specification, which can be found <a href=\"https://golang.org/ref/spec\">here</a>. Not so coincidentally, InfluxDB is written in Go.</p> <pre>Production  = production_name \"=\" [ Expression ] \".\" .\nExpression  = Alternative { \"|\" Alternative } .\nAlternative = Term { Term } .\nTerm        = production_name | token [ \"…\" token ] | Group | Option | Repetition .\nGroup       = \"(\" Expression \")\" .\nOption      = \"[\" Expression \"]\" .\nRepetition  = \"{\" Expression \"}\" .\n</pre> <p>Notation operators in order of increasing precedence:</p> <pre>|   alternation\n()  grouping\n[]  option (0 or 1 times)\n{}  repetition (0 to n times)\n</pre> <h2 id=\"query-representation\">Query representation</h2> <h3 id=\"characters\">Characters</h3> <p>InfluxQL is Unicode text encoded in <a href=\"http://en.wikipedia.org/wiki/UTF-8\">UTF-8</a>.</p> <pre>newline             = /* the Unicode code point U+000A */ .\nunicode_char        = /* an arbitrary Unicode code point except newline */ .\n</pre> <h2 id=\"letters-and-digits\">Letters and digits</h2> <p>Letters are the set of ASCII characters plus the underscore character _ (U+005F) is considered a letter.</p> <p>Only decimal digits are supported.</p> <pre>letter              = ascii_letter | \"_\" .\nascii_letter        = \"A\" … \"Z\" | \"a\" … \"z\" .\ndigit               = \"0\" … \"9\" .\n</pre> <h2 id=\"identifiers\">Identifiers</h2> <p>Identifiers are tokens which refer to <a href=\"../../concepts/glossary/index#database\">database</a> names, <a href=\"../../concepts/glossary/index#retention-policy-rp\">retention policy</a> names, <a href=\"../../concepts/glossary/index#user\">user</a> names, <a href=\"../../concepts/glossary/index#measurement\">measurement</a> names, <a href=\"../../concepts/glossary/index#tag-key\">tag keys</a>, and <a href=\"../../concepts/glossary/index#field-key\">field keys</a>.</p> <p>The rules:</p> <ul> <li>double quoted identifiers can contain any unicode character other than a new line</li> <li>double quoted identifiers can contain escaped <code>\"</code> characters (i.e., <code>\\\"</code>)</li> <li>double quoted identifiers can contain InfluxQL <a href=\"index#keywords\">keywords</a>\n</li> <li>unquoted identifiers must start with an upper or lowercase ASCII character or “_”</li> <li>unquoted identifiers may contain only ASCII letters, decimal digits, and “_”</li> </ul> <pre>identifier          = unquoted_identifier | quoted_identifier .\nunquoted_identifier = ( letter ) { letter | digit } .\nquoted_identifier   = `\"` unicode_char { unicode_char } `\"` .\n</pre> <h4 id=\"examples\">Examples:</h4> <pre>cpu\n_cpu_stats\n\"1h\"\n\"anything really\"\n\"1_Crazy-1337.identifier&gt;NAME👍\"\n</pre> <h2 id=\"keywords\">Keywords</h2> <pre>ALL           ALTER         ANY           AS            ASC           BEGIN\nBY            CREATE        CONTINUOUS    DATABASE      DATABASES     DEFAULT\nDELETE        DESC          DESTINATIONS  DIAGNOSTICS   DISTINCT      DROP\nDURATION      END           EVERY         EXISTS        EXPLAIN       FIELD\nFOR           FORCE         FROM          GRANT         GRANTS        GROUP\nGROUPS        IF            IN            INF           INNER         INSERT\nINTO          KEY           KEYS          KILL          LIMIT         SHOW          \nMEASUREMENT   MEASUREMENTS  NAME          NOT           OFFSET        ON\nORDER         PASSWORD      POLICY        POLICIES      PRIVILEGES    QUERIES       \nQUERY         READ          REPLICATION   RESAMPLE      RETENTION     REVOKE        \nSELECT        SERIES        SET           SHARD         SHARDS        SLIMIT        \nSOFFSET       STATS         SUBSCRIPTION  SUBSCRIPTIONS TAG           TO            \nUSER          USERS         VALUES        WHERE         WITH          WRITE         \n</pre> <h2 id=\"literals\">Literals</h2> <h3 id=\"integers\">Integers</h3> <p>InfluxQL supports decimal integer literals. Hexadecimal and octal literals are not currently supported.</p> <pre>int_lit             = ( \"1\" … \"9\" ) { digit } .\n</pre> <h3 id=\"floats\">Floats</h3> <p>InfluxQL supports floating-point literals. Exponents are not currently supported.</p> <pre>float_lit           = int_lit \".\" int_lit .\n</pre> <h3 id=\"strings\">Strings</h3> <p>String literals must be surrounded by single quotes. Strings may contain <code>'</code> characters as long as they are escaped (i.e., <code>\\'</code>).</p> <pre>string_lit          = `'` { unicode_char } `'`' .\n</pre> <h3 id=\"durations\">Durations</h3> <p>Duration literals specify a length of time. An integer literal followed immediately (with no spaces) by a duration unit listed below is interpreted as a duration literal.</p> <h4 id=\"duration-units\">Duration units</h4> <table> <thead> <tr> <th>Units</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>u or µ</td> <td>microseconds (1 millionth of a second)</td> </tr> <tr> <td>ms</td> <td>milliseconds (1 thousandth of a second)</td> </tr> <tr> <td>s</td> <td>second</td> </tr> <tr> <td>m</td> <td>minute</td> </tr> <tr> <td>h</td> <td>hour</td> </tr> <tr> <td>d</td> <td>day</td> </tr> <tr> <td>w</td> <td>week</td> </tr> </tbody> </table> <pre>duration_lit        = int_lit duration_unit .\nduration_unit       = \"u\" | \"µ\" | \"ms\" | \"s\" | \"m\" | \"h\" | \"d\" | \"w\" .\n</pre> <h3 id=\"dates-times\">Dates &amp; Times</h3> <p>The date and time literal format is not specified in EBNF like the rest of this document. It is specified using Go’s date / time parsing format, which is a reference date written in the format required by InfluxQL. The reference date time is:</p> <p>InfluxQL reference date time: January 2nd, 2006 at 3:04:05 PM</p> <pre>time_lit            = \"2006-01-02 15:04:05.999999\" | \"2006-01-02\"\n</pre> <h3 id=\"booleans\">Booleans</h3> <pre>bool_lit            = TRUE | FALSE .\n</pre> <h3 id=\"regular-expressions\">Regular Expressions</h3> <pre>regex_lit           = \"/\" { unicode_char } \"/\" .\n</pre> <p><strong>Comparators:</strong><br> <code>=~</code> matches against<br> <code>!~</code> doesn’t match against</p> <blockquote> <p><strong>Note:</strong> Use regular expressions to match measurements and tags. You cannot use regular expressions to match databases, retention policies, or fields.</p> </blockquote> <h2 id=\"queries\">Queries</h2> <p>A query is composed of one or more statements separated by a semicolon.</p> <pre>query               = statement { ; statement } .\n\nstatement           = alter_retention_policy_stmt |\n                      create_continuous_query_stmt |\n                      create_database_stmt |\n                      create_retention_policy_stmt |\n                      create_subscription_stmt |\n                      create_user_stmt |\n                      delete_stmt |\n                      drop_continuous_query_stmt |\n                      drop_database_stmt |\n                      drop_measurement_stmt |\n                      drop_retention_policy_stmt |\n                      drop_series_stmt |\n                      drop_shard_stmt |\n                      drop_subscription_stmt |\n                      drop_user_stmt |\n                      grant_stmt |\n                      kill_query_statement |\n                      show_continuous_queries_stmt |\n                      show_databases_stmt |\n                      show_field_keys_stmt |\n                      show_grants_stmt |\n                      show_measurements_stmt |\n                      show_queries_stmt |\n                      show_retention_policies |\n                      show_series_stmt |\n                      show_shard_groups_stmt |\n                      show_shards_stmt |\n                      show_subscriptions_stmt|\n                      show_tag_keys_stmt |\n                      show_tag_values_stmt |\n                      show_users_stmt |\n                      revoke_stmt |\n                      select_stmt .\n</pre> <h2 id=\"statements\">Statements</h2> <h3 id=\"alter-retention-policy\">ALTER RETENTION POLICY</h3> <pre>alter_retention_policy_stmt  = \"ALTER RETENTION POLICY\" policy_name on_clause\n                               retention_policy_option\n                               [ retention_policy_option ]\n                               [ retention_policy_option ]\n                               [ retention_policy_option ] .\n</pre> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <h4 id=\"examples-1\">Examples:</h4> <pre data-language=\"sql\">-- Set default retention policy for mydb to 1h.cpu.\nALTER RETENTION POLICY \"1h.cpu\" ON mydb DEFAULT\n\n-- Change duration and replication factor.\nALTER RETENTION POLICY policy1 ON somedb DURATION 1h REPLICATION 4\n</pre> <h3 id=\"create-continuous-query\">CREATE CONTINUOUS QUERY</h3> <pre>create_continuous_query_stmt = \"CREATE CONTINUOUS QUERY\" query_name on_clause\n                               [ \"RESAMPLE\" resample_opts ]\n                               \"BEGIN\" select_stmt \"END\" .\n\nquery_name                   = identifier .\n\nresample_opts                = (every_stmt for_stmt | every_stmt | for_stmt) .\nevery_stmt                   = \"EVERY\" duration_lit\nfor_stmt                     = \"FOR\" duration_lit\n</pre> <h4 id=\"examples-2\">Examples:</h4> <pre data-language=\"sql\">-- selects from default retention policy and writes into 6_months retention policy\nCREATE CONTINUOUS QUERY \"10m_event_count\"\nON db_name\nBEGIN\n  SELECT count(value)\n  INTO \"6_months\".events\n  FROM events\n  GROUP BY time(10m)\nEND;\n\n-- this selects from the output of one continuous query in one retention policy and outputs to another series in another retention policy\nCREATE CONTINUOUS QUERY \"1h_event_count\"\nON db_name\nBEGIN\n  SELECT sum(count) as count\n  INTO \"2_years\".events\n  FROM \"6_months\".events\n  GROUP BY time(1h)\nEND;\n\n-- this customizes the resample interval so the interval is queried every 10s and intervals are resampled until 2m after their start time\n-- when resample is used, at least one of \"EVERY\" or \"FOR\" must be used\nCREATE CONTINUOUS QUERY \"cpu_mean\"\nON db_name\nRESAMPLE EVERY 10s FOR 2m\nBEGIN\n  SELECT mean(value)\n  INTO \"cpu_mean\"\n  FROM \"cpu\"\n  GROUP BY time(1m)\nEND;\n</pre> <h3 id=\"create-database\">CREATE DATABASE</h3> <pre>create_database_stmt = \"CREATE DATABASE\" [\"IF NOT EXISTS\"] db_name\n                       [ WITH\n                       [ retention_policy_duration ]\n                       [ retention_policy_replication ]\n                       [ retention_policy_shard_group_duration ]\n                       [ retention_policy_name ]\n                       ] .\n</pre> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <h4 id=\"examples-3\">Examples:</h4> <pre data-language=\"sql\">-- Create a database called foo\nCREATE DATABASE foo\n\n-- Create a database called bar with a new DEFAULT retention policy and specify the duration, replication, shard group duration, and name of that retention policy\nCREATE DATABASE bar WITH DURATION 1d REPLICATION 1 SHARD DURATION 30m NAME myrp\n\n-- Create a database called mydb with a new DEFAULT retention policy and specify the name of that retention policy\nCREATE DATABASE mydb WITH NAME myrp\n</pre> <h3 id=\"create-retention-policy\">CREATE RETENTION POLICY</h3> <pre>create_retention_policy_stmt = \"CREATE RETENTION POLICY\" policy_name on_clause\n                               retention_policy_duration\n                               retention_policy_replication\n                               [ retention_policy_shard_group_duration ]\n                               [ \"DEFAULT\" ] .\n</pre> \n<dt> Replication factors do not serve a purpose with single node instances. </dt> <h4 id=\"examples-4\">Examples</h4> <pre data-language=\"sql\">-- Create a retention policy.\nCREATE RETENTION POLICY \"10m.events\" ON somedb DURATION 60m REPLICATION 2\n\n-- Create a retention policy and set it as the DEFAULT.\nCREATE RETENTION POLICY \"10m.events\" ON somedb DURATION 60m REPLICATION 2 DEFAULT\n\n-- Create a retention policy and specify the shard group duration.\nCREATE RETENTION POLICY \"10m.events\" ON somedb DURATION 60m REPLICATION 2 SHARD DURATION 30m\n</pre> <h3 id=\"create-subscription\">CREATE SUBSCRIPTION</h3> <p>Subscriptions tell InfluxDB to send all the data it receives to <a href=\"../../../../kapacitor/v0.13/introduction/index\">Kapacitor</a>.</p> <pre>create_subscription_stmt = \"CREATE SUBSCRIPTION\" subscription_name \"ON\" db_name \".\" retention_policy \"DESTINATIONS\" (\"ANY\"|\"ALL\") host { \",\" host} .\n</pre> <h4 id=\"examples-5\">Examples:</h4> <pre data-language=\"sql\">-- Create a SUBSCRIPTION on database 'mydb' and retention policy 'default' that send data to 'example.com:9090' via UDP.\nCREATE SUBSCRIPTION sub0 ON \"mydb\".\"default\" DESTINATIONS ALL 'udp://example.com:9090'\n\n-- Create a SUBSCRIPTION on database 'mydb' and retention policy 'default' that round robins the data to 'h1.example.com:9090' and 'h2.example.com:9090'.\nCREATE SUBSCRIPTION sub0 ON \"mydb\".\"default\" DESTINATIONS ANY 'udp://h1.example.com:9090', 'udp://h2.example.com:9090'\n</pre> <h3 id=\"create-user\">CREATE USER</h3> <pre>create_user_stmt = \"CREATE USER\" user_name \"WITH PASSWORD\" password\n                   [ \"WITH ALL PRIVILEGES\" ] .\n</pre> <h4 id=\"examples-6\">Examples:</h4> <pre data-language=\"sql\">-- Create a normal database user.\nCREATE USER jdoe WITH PASSWORD '1337password'\n\n-- Create an admin user.\n-- Note: Unlike the GRANT statement, the \"PRIVILEGES\" keyword is required here.\nCREATE USER jdoe WITH PASSWORD '1337password' WITH ALL PRIVILEGES\n</pre> <blockquote> <p><strong>Note:</strong> The password string must be wrapped in single quotes.</p> </blockquote> <h3 id=\"delete\">DELETE</h3> <pre>delete_stmt = \"DELETE\" ( from_clause | where_clause | from_clause where_clause ) .\n</pre> <h4 id=\"examples-7\">Examples:</h4> <pre>DELETE FROM cpu\nDELETE FROM cpu WHERE time &lt; '2000-01-01T00:00:00Z'\nDELETE WHERE time &lt; '2000-01-01T00:00:00Z'\n</pre> <h3 id=\"drop-continuous-query\">DROP CONTINUOUS QUERY</h3> <pre>drop_continuous_query_stmt = \"DROP CONTINUOUS QUERY\" query_name \"ON\" db_name.\n</pre> <h4 id=\"example\">Example:</h4> <pre data-language=\"sql\">DROP CONTINUOUS QUERY myquery ON mydb\n</pre> <h3 id=\"drop-database\">DROP DATABASE</h3> <pre>drop_database_stmt = \"DROP DATABASE\" [\"IF EXISTS\"] db_name .\n</pre> <h4 id=\"example-1\">Example:</h4> <pre data-language=\"sql\">DROP DATABASE mydb\n</pre> <h3 id=\"drop-measurement\">DROP MEASUREMENT</h3> <pre>drop_measurement_stmt = \"DROP MEASUREMENT\" measurement .\n</pre> <h4 id=\"examples-8\">Examples:</h4> <pre data-language=\"sql\">-- drop the cpu measurement\nDROP MEASUREMENT cpu\n</pre> <h3 id=\"drop-retention-policy\">DROP RETENTION POLICY</h3> <pre>drop_retention_policy_stmt = \"DROP RETENTION POLICY\" policy_name on_clause .\n</pre> <h4 id=\"example-2\">Example:</h4> <pre data-language=\"sql\">-- drop the retention policy named 1h.cpu from mydb\nDROP RETENTION POLICY \"1h.cpu\" ON mydb\n</pre> <h3 id=\"drop-series\">DROP SERIES</h3> <pre>drop_series_stmt = \"DROP SERIES\" ( from_clause | where_clause | from_clause where_clause ) .\n</pre> <h4 id=\"example-3\">Example:</h4>  <h3 id=\"drop-shard\">DROP SHARD</h3> <pre>drop_shard_stmt = \"DROP SHARD\" ( shard_id ) .\n</pre> <h4 id=\"example-4\">Example:</h4> <pre>DROP SHARD 1\n</pre> <h3 id=\"drop-subscription\">DROP SUBSCRIPTION</h3> <pre>drop_subscription_stmt = \"DROP SUBSCRIPTION\" subscription_name \"ON\" db_name \".\" retention_policy .\n</pre> <h4 id=\"example-5\">Example:</h4> <pre data-language=\"sql\">DROP SUBSCRIPTION sub0 ON \"mydb\".\"default\"\n</pre> <h3 id=\"drop-user\">DROP USER</h3> <pre>drop_user_stmt = \"DROP USER\" user_name .\n</pre> <h4 id=\"example-6\">Example:</h4> <pre data-language=\"sql\">DROP USER jdoe\n</pre> <h3 id=\"grant\">GRANT</h3> <blockquote> <p><strong>NOTE:</strong> Users can be granted privileges on databases that do not exist.</p> </blockquote> <pre>grant_stmt = \"GRANT\" privilege [ on_clause ] to_clause\n</pre> <h4 id=\"examples-9\">Examples:</h4> <pre data-language=\"sql\">-- grant admin privileges\nGRANT ALL TO jdoe\n\n-- grant read access to a database\nGRANT READ ON mydb TO jdoe\n</pre> <h3 id=\"kill-query\">KILL QUERY</h3> <pre>kill_query_statement = \"KILL QUERY\" query_id .\n</pre> <h4 id=\"examples-10\">Examples:</h4> <pre>--- kill a query with the query_id 36\nKILL QUERY 36\n</pre> <blockquote> <p><strong>NOTE:</strong> Identify the <code>query_id</code> from the <a href=\"index#show-queries\"><code>SHOW QUERIES</code></a> output.</p> </blockquote> <h3 id=\"show-continuous-queries\">SHOW CONTINUOUS QUERIES</h3> <pre>show_continuous_queries_stmt = \"SHOW CONTINUOUS QUERIES\" .\n</pre> <h4 id=\"example-7\">Example:</h4> <pre data-language=\"sql\">-- show all continuous queries\nSHOW CONTINUOUS QUERIES\n</pre> <h3 id=\"show-databases\">SHOW DATABASES</h3> <pre>show_databases_stmt = \"SHOW DATABASES\" .\n</pre> <h4 id=\"example-8\">Example:</h4> <pre data-language=\"sql\">-- show all databases\nSHOW DATABASES\n</pre> <h3 id=\"show-field-keys\">SHOW FIELD KEYS</h3> <pre>show_field_keys_stmt = \"SHOW FIELD KEYS\" [ from_clause ] .\n</pre> <h4 id=\"examples-11\">Examples:</h4> <pre data-language=\"sql\">-- show field keys from all measurements\nSHOW FIELD KEYS\n\n-- show field keys from specified measurement\nSHOW FIELD KEYS FROM cpu\n</pre> <h3 id=\"show-grants\">SHOW GRANTS</h3> <pre>show_grants_stmt = \"SHOW GRANTS FOR\" user_name .\n</pre> <h4 id=\"example-9\">Example:</h4> <pre data-language=\"sql\">-- show grants for jdoe\nSHOW GRANTS FOR jdoe\n</pre> <h3 id=\"show-measurements\">SHOW MEASUREMENTS</h3> <pre>show_measurements_stmt = \"SHOW MEASUREMENTS\" [ with_measurement_clause ] [ where_clause ] [ limit_clause ] [ offset_clause ] .\n</pre> <pre data-language=\"sql\">-- show all measurements\nSHOW MEASUREMENTS\n\n-- show measurements where region tag = 'uswest' AND host tag = 'serverA'\nSHOW MEASUREMENTS WHERE region = 'uswest' AND host = 'serverA'\n\n-- show measurements that start with 'h2o'\nSHOW MEASUREMENTS WITH MEASUREMENT =~ /h2o.*/\n</pre> <h3 id=\"show-queries\">SHOW QUERIES</h3> <pre>show_queries_stmt = \"SHOW QUERIES\" .\n</pre> <h4 id=\"example-10\">Example:</h4> <pre data-language=\"sql\">-- show all currently-running queries\nSHOW QUERIES\n</pre> <h3 id=\"show-retention-policies\">SHOW RETENTION POLICIES</h3> <pre>show_retention_policies = \"SHOW RETENTION POLICIES\" on_clause .\n</pre> <h4 id=\"example-11\">Example:</h4> <pre data-language=\"sql\">-- show all retention policies on a database\nSHOW RETENTION POLICIES ON mydb\n</pre> <h3 id=\"show-series\">SHOW SERIES</h3> <pre>show_series_stmt = \"SHOW SERIES\" [ from_clause ] [ where_clause ] [ limit_clause ] [ offset_clause ] .\n</pre> <h4 id=\"example-12\">Example:</h4>  <h3 id=\"show-shard-groups\">SHOW SHARD GROUPS</h3> <pre>show_shard_groups_stmt = \"SHOW SHARD GROUPS\" .\n</pre> <h4 id=\"example-13\">Example:</h4> <pre data-language=\"sql\">SHOW SHARD GROUPS\n</pre> <h3 id=\"show-shards\">SHOW SHARDS</h3> <pre>show_shards_stmt = \"SHOW SHARDS\" .\n</pre> <h4 id=\"example-14\">Example:</h4> <pre data-language=\"sql\">SHOW SHARDS\n</pre> <h3 id=\"show-subscriptions\">SHOW SUBSCRIPTIONS</h3> <pre>show_subscriptions_stmt = \"SHOW SUBSCRIPTIONS\" .\n</pre> <h4 id=\"example-15\">Example:</h4> <pre data-language=\"sql\">SHOW SUBSCRIPTIONS\n</pre> <h3 id=\"show-tag-keys\">SHOW TAG KEYS</h3> <pre>show_tag_keys_stmt = \"SHOW TAG KEYS\" [ from_clause ] [ where_clause ] [ group_by_clause ]\n                     [ limit_clause ] [ offset_clause ] .\n</pre> <h4 id=\"examples-12\">Examples:</h4> <pre data-language=\"sql\">-- show all tag keys\nSHOW TAG KEYS\n\n-- show all tag keys from the cpu measurement\nSHOW TAG KEYS FROM cpu\n\n-- show all tag keys from the cpu measurement where the region key = 'uswest'\nSHOW TAG KEYS FROM cpu WHERE region = 'uswest'\n\n-- show all tag keys where the host key = 'serverA'\nSHOW TAG KEYS WHERE host = 'serverA'\n</pre> <h3 id=\"show-tag-values\">SHOW TAG VALUES</h3> <pre>show_tag_values_stmt = \"SHOW TAG VALUES\" [ from_clause ] with_tag_clause [ where_clause ]\n                       [ group_by_clause ] [ limit_clause ] [ offset_clause ] .\n</pre> <h4 id=\"examples-13\">Examples:</h4> <pre data-language=\"sql\">-- show all tag values across all measurements for the region tag\nSHOW TAG VALUES WITH KEY = \"region\"\n\n-- show tag values from the cpu measurement for the region tag\nSHOW TAG VALUES FROM cpu WITH KEY = \"region\"\n\n-- show tag values from the cpu measurement for region &amp; host tag keys where service = 'redis'\nSHOW TAG VALUES FROM cpu WITH KEY IN (\"region\", \"host\") WHERE service = 'redis'\n</pre> <h3 id=\"show-users\">SHOW USERS</h3> <pre>show_users_stmt = \"SHOW USERS\" .\n</pre> <h4 id=\"example-16\">Example:</h4> <pre data-language=\"sql\">-- show all users\nSHOW USERS\n</pre> <h3 id=\"revoke\">REVOKE</h3> <pre>revoke_stmt = \"REVOKE\" privilege [ on_clause ] \"FROM\" user_name .\n</pre> <h4 id=\"examples-14\">Examples:</h4> <pre data-language=\"sql\">-- revoke admin privileges from jdoe\nREVOKE ALL PRIVILEGES FROM jdoe\n\n-- revoke read privileges from jdoe on mydb\nREVOKE READ ON mydb FROM jdoe\n</pre> <h3 id=\"select\">SELECT</h3> <pre>select_stmt = \"SELECT\" fields from_clause [ into_clause ] [ where_clause ]\n              [ group_by_clause ] [ order_by_clause ] [ limit_clause ]\n              [ offset_clause ] [ slimit_clause ] [ soffset_clause ] .\n</pre> <h4 id=\"examples-15\">Examples:</h4> <pre data-language=\"sql\">-- select mean value from the cpu measurement where region = 'uswest' grouped by 10 minute intervals\nSELECT mean(value) FROM cpu WHERE region = 'uswest' GROUP BY time(10m) fill(0)\n\n-- select from all measurements beginning with cpu into the same measurement name in the cpu_1h retention policy\nSELECT mean(value) INTO cpu_1h.:MEASUREMENT FROM /cpu.*/\n</pre> <h2 id=\"clauses\">Clauses</h2> <pre>from_clause     = \"FROM\" measurements .\n\ngroup_by_clause = \"GROUP BY\" dimensions fill(fill_option).\n\ninto_clause     = \"INTO\" ( measurement | back_ref ).\n\nlimit_clause    = \"LIMIT\" int_lit .\n\noffset_clause   = \"OFFSET\" int_lit .\n\nslimit_clause   = \"SLIMIT\" int_lit .\n\nsoffset_clause  = \"SOFFSET\" int_lit .\n\non_clause       = \"ON\" db_name .\n\norder_by_clause = \"ORDER BY\" sort_fields .\n\nto_clause       = \"TO\" user_name .\n\nwhere_clause    = \"WHERE\" expr .\n\nwith_measurement_clause = \"WITH MEASUREMENT\" ( \"=\" measurement | \"=~\" regex_lit ) .\n\nwith_tag_clause = \"WITH KEY\" ( \"=\" tag_key | \"IN (\" tag_keys \")\" ) .\n</pre> <h2 id=\"expressions\">Expressions</h2> <pre>binary_op        = \"+\" | \"-\" | \"*\" | \"/\" | \"AND\" | \"OR\" | \"=\" | \"!=\" | \"&lt;&gt;\" | \"&lt;\" |\n                   \"&lt;=\" | \"&gt;\" | \"&gt;=\" .\n\nexpr             = unary_expr { binary_op unary_expr } .\n\nunary_expr       = \"(\" expr \")\" | var_ref | time_lit | string_lit | int_lit |\n                   float_lit | bool_lit | duration_lit | regex_lit .\n</pre> <h2 id=\"other\">Other</h2> <pre>alias            = \"AS\" identifier .\n\nback_ref         = ( policy_name \".:MEASUREMENT\" ) |\n                   ( db_name \".\" [ policy_name ] \".:MEASUREMENT\" ) .\n\ndb_name          = identifier .\n\ndimension        = expr .\n\ndimensions       = dimension { \",\" dimension } .\n\nfield_key        = identifier .\n\nfield            = expr [ alias ] .\n\nfields           = field { \",\" field } .\n\nfill_option      = \"null\" | \"none\" | \"previous\" | int_lit | float_lit .\n\nhost             = string_lit .\n\nmeasurement      = measurement_name |\n                   ( policy_name \".\" measurement_name ) |\n                   ( db_name \".\" [ policy_name ] \".\" measurement_name ) .\n\nmeasurements     = measurement { \",\" measurement } .\n\nmeasurement_name = identifier | regex_lit .\n\npassword         = string_lit .\n\npolicy_name      = identifier .\n\nprivilege        = \"ALL\" [ \"PRIVILEGES\" ] | \"READ\" | \"WRITE\" .\n\nquery_id         = int_lit .\n\nquery_name       = identifier .\n\nretention_policy = identifier .\n\nretention_policy_option      = retention_policy_duration |\n                               retention_policy_replication |\n                               retention_policy_shard_group_duration |\n                               \"DEFAULT\" .\n\nretention_policy_duration    = \"DURATION\" duration_lit .\nretention_policy_replication = \"REPLICATION\" int_lit .\nretention_policy_shard_group_duration = \"SHARD DURATION\" duration_lit .\n\nretention_policy_name = \"NAME\" identifier .\n\nseries_id        = int_lit .\n\nshard_id         = int_lit .\n\nsort_field       = field_key [ ASC | DESC ] .\n\nsort_fields      = sort_field { \",\" sort_field } .\n\nsubscription_name = identifier .\n\ntag_key          = identifier .\n\ntag_keys         = tag_key { \",\" tag_key } .\n\nuser_name        = identifier .\n\nvar_ref          = measurement .\n</pre> <h2 id=\"query-engine-internals\">Query Engine Internals</h2> <p>Once you understand the language itself, it’s important to know how these language constructs are implemented in the query engine. This gives you an intuitive sense for how results will be processed and how to create efficient queries.</p> <p>The life cycle of a query looks like this:</p> <ol> <li><p>InfluxQL query string is tokenized and then parsed into an abstract syntax tree (AST). This is the code representation of the query itself.</p></li> <li><p>The AST is passed to the <code>QueryExecutor</code> which directs queries to the appropriate handlers. For example, queries related to meta data are executed by the meta service and <code>SELECT</code> statements are executed by the shards themselves.</p></li> <li><p>The query engine then determines the shards that match the <code>SELECT</code> statement’s time range. From these shards, iterators are created for each field in the statement.</p></li> <li><p>Iterators are passed to the emitter which drains them and joins the resulting points. The emitter’s job is to convert simple time/value points into the more complex result objects that are returned to the client.</p></li> </ol> <h3 id=\"understanding-iterators\">Understanding Iterators</h3> <p>Iterators are at the heart of the query engine. They provide a simple, strongly-typed interface for looping over a set of points. Each field in a query must be a single type (float, integer, string, boolean) and combining series of different types will cause them to be cast to a single type.</p> <p>For example, this is an iterator over Float points:</p> <pre>type FloatIterator interface {\n    Next() *FloatPoint\n}\n</pre> <p>These iterators are created through the IteratorCreator interface:</p> <pre>type IteratorCreator interface {\n    CreateIterator(opt *IteratorOptions) (Iterator, error)\n}\n</pre> <p>The <code>IteratorOptions</code> provide arguments about field selection, time ranges, and dimensions that the iterator creator can use when planning an iterator. The <code>IteratorCreator</code> interface is used at many levels such as the <code>Shards</code>, <code>Shard</code>, and <code>Engine</code>. This allows optimizations to be performed when applicable such as returning a precomputed <code>COUNT()</code>.</p> <p>Iterators aren’t just for reading raw data from storage though. Iterators can be composed so that they provided additional functionality around an input iterator. For example, a <code>DistinctIterator</code> can compute the distinct values for each time window for an input iterator. Or a <code>FillIterator</code> can generate additional points that are missing from an input iterator.</p> <p>This composition also lends itself well to aggregation. For example, a statement such as this:</p> <pre>SELECT MEAN(value) FROM cpu GROUP BY time(10m)\n</pre> <p>In this case, <code>MEAN(value)</code> is a <code>MeanIterator</code> wrapping an iterator from the underlying shards. However, if we can add an additional iterator to determine the derivative of the mean:</p> <pre>SELECT DERIVATIVE(MEAN(value), 20m) FROM cpu GROUP BY time(10m)\n</pre> <h3 id=\"understanding-auxiliary-fields\">Understanding Auxiliary Fields</h3> <p>Because InfluxQL allows users to use selector functions such as <code>FIRST()</code>, <code>LAST()</code>, <code>MIN()</code>, and <code>MAX()</code>, the engine must provide a way to return related data at the same time with the selected point.</p> <p>For example, in this query:</p> <pre>SELECT FIRST(value), host FROM cpu GROUP BY time(1h)\n</pre> <p>We are selecting the first <code>value</code> that occurs every hour but we also want to retrieve the <code>host</code> associated with that point. Since the <code>Point</code> types only specify a single typed <code>Value</code> for efficiency, we push the <code>host</code> into the auxiliary fields of the point. These auxiliary fields are attached to the point until it is passed to the emitter where the fields get split off to their own iterator.</p> <h3 id=\"built-in-iterators\">Built-in Iterators</h3> <p>There are many helper iterators that let us build queries:</p> <ul> <li><p>Merge Iterator - This iterator combines one or more iterators into a single new iterator of the same type. This iterator guarantees that all points within a window will be output before starting the next window but does not provide ordering guarantees within the window. This allows for fast access for aggregate queries which do not need stronger sorting guarantees.</p></li> <li><p>Sorted Merge Iterator - This iterator also combines one or more iterators into a new iterator of the same type. However, this iterator guarantees time ordering of every point. This makes it slower than the <code>MergeIterator</code> but this ordering guarantee is required for non-aggregate queries which return the raw data points.</p></li> <li><p>Limit Iterator - This iterator limits the number of points per name/tag group. This is the implementation of the <code>LIMIT</code> &amp; <code>OFFSET</code> syntax.</p></li> <li><p>Fill Iterator - This iterator injects extra points if they are missing from the input iterator. It can provide <code>null</code> points, points with the previous value, or points with a specific value.</p></li> <li><p>Buffered Iterator - This iterator provides the ability to “unread” a point back onto a buffer so it can be read again next time. This is used extensively to provide lookahead for windowing.</p></li> <li><p>Reduce Iterator - This iterator calls a reduction function for each point in a window. When the window is complete then all points for that window are output. This is used for simple aggregate functions such as <code>COUNT()</code>.</p></li> <li><p>Reduce Slice Iterator - This iterator collects all points for a window first and then passes them all to a reduction function at once. The results are returned from the iterator. This is used for aggregate functions such as <code>DERIVATIVE()</code>.</p></li> <li><p>Transform Iterator - This iterator calls a transform function for each point from an input iterator. This is used for executing binary expressions.</p></li> <li><p>Dedupe Iterator - This iterator only outputs unique points. It is resource intensive so it is only used for small queries such as meta query statements.</p></li> </ul> <h3 id=\"call-iterators\">Call Iterators</h3> <p>Function calls in InfluxQL are implemented at two levels. Some calls can be wrapped at multiple layers to improve efficiency. For example, a <code>COUNT()</code> can be performed at the shard level and then multiple <code>CountIterators</code> can be wrapped with another CountIterator to compute the count of all shards. These iterators can be created using <code>NewCallIterator()</code>.</p> <p>Some iterators are more complex or need to be implemented at a higher level. For example, the <code>DERIVATIVE()</code> needs to retrieve all points for a window first before performing the calculation. This iterator is created by the engine itself and is never requested to be created by the lower levels.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/spec/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/query_language/spec/</a>\n  </p>\n</div>\n","influxdb/v0.13/administration/013_vs_previous/index":"<h1>Differences between InfluxDB 0.13 and versions prior to 0.12</h1>     <p>Users looking to upgrade to InfluxDB 0.13 from versions prior to 0.12 should view the following pages in our documentation.</p> <h5 id=\"0-11-users\">0.11 users:</h5> <p><a href=\"https://docs.influxdata.com/influxdb/v0.12/concepts/011_vs_012/\">Differences between InfluxDB 0.12 and InfluxDB 0.11</a></p> <h5 id=\"0-10-users\">0.10 users:</h5> <p><a href=\"https://docs.influxdata.com/influxdb/v0.11/concepts/010_vs_011/\">Differences between InfluxDB 0.11 and InfluxDB 0.10</a></p> <h5 id=\"0-9-users\">0.9 users:</h5> <p><a href=\"https://docs.influxdata.com/influxdb/v0.10/concepts/09_vs_010/\">Differences between InfluxDB 0.9 and InfluxDB 0.10</a></p> <h5 id=\"0-8-users\">0.8 users:</h5> <p><a href=\"https://docs.influxdata.com/influxdb/v0.10/concepts/08_vs_010/\">Differences between InfluxDB 0.8 and InfluxDB 0.10</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/administration/013_vs_previous/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/administration/013_vs_previous/</a>\n  </p>\n</div>\n","influxdb/v0.13/tools/index":"<h1>Tools</h1>     <h2 id=\"influxdb-shell-cli-influxdb-v0-13-tools-shell\"><a href=\"shell/index\">InfluxDB Shell(CLI)</a></h2> <p>The Influx shell is an interactive shell for InfluxDB and the recommended <em>ad hoc</em> method of using the HTTP API.</p> <h2 id=\"influxdb-web-admin-influxdb-v0-13-tools-web-admin\"><a href=\"web_admin/index\">InfluxDB Web Admin</a></h2> <p>The built-in web administration GUI is a simple way to interact with InfluxDB. For any significant use, whether writing or querying data, the HTTP API is the preferred method.</p> <h2 id=\"grafana-graphs-and-dashboards-http-docs-grafana-org-datasources-influxdb\"><a href=\"http://docs.grafana.org/datasources/influxdb/\">Grafana Graphs and Dashboards</a></h2> <p>Grafana is a convenient dashboard tool for visualizing time series data. It was originally built for Graphite, modeled after Kibana, and since been updated to support InfluxDB.</p> \n<dt> Because of the <a href=\"https://docs.influxdata.com/influxdb/v0.11/concepts/010_vs_011/#breaking-api-changes\">changes</a> to the <code>SHOW SERIES</code> and <code>SHOW TAG VALUES</code> formats in InfluxDB 0.11, InfluxDB 0.13 will not work with the Query Editor in Grafana 2.6. This issue does not affect existing queries and dashboards or users working with Grafana 3.0. </dt> <h2 id=\"chronograf-dashboards-chronograf-v0-13\"><a href=\"../../../chronograf/v0.13/index\">Chronograf Dashboards</a></h2> <h2 id=\"kapacitor-monitoring-kapacitor-v0-13\"><a href=\"../../../kapacitor/v0.13/index\">Kapacitor Monitoring</a></h2> <h2 id=\"telegraf-telegraf-v0-13\"><a href=\"../../../telegraf/v0.13/index\">Telegraf</a></h2> <p>Telegraf is an open source tool for metrics collection (e.g. CollectD) built and maintained by the InfluxDB team.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/tools/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/tools/</a>\n  </p>\n</div>\n","influxdb/v0.13/tools/web_admin/index":"<h1>Web Admin Interface</h1>     <p>The built-in web administration GUI is a simple way to interact with InfluxDB. For any significant use, whether writing or querying data, direct use of the HTTP API (<a href=\"../../guides/querying_data/index\">reading</a>, <a href=\"../../guides/writing_data/index\">writing</a>) or the <a href=\"../shell/index\">command line interface</a> are better options.</p> <h2 id=\"accessing-the-ui\">Accessing the UI</h2> <p>The Admin UI is available by default at port 8083, i.e. <a href=\"http://localhost:8083\">http://localhost:8083</a>. You can control the port in the InfluxDB config file using the <code>port</code> option in the <code>[admin]</code> section.</p> <p>You can also access remote InfluxDB instances, although you may only connect to one instance at a time. To access an instance at a location other than than <a href=\"http://localhost:8083\">http://localhost:8083</a>, click the Settings icon in the upper right and enter the proper connection settings for the target InfluxDB instance.</p> <h3 id=\"http-vs-https\">HTTP vs HTTPS</h3> <p>The Admin UI uses HTTP by default but can be configured to use HTTPS. In the InfluxDB config file, find the <code>[admin]</code> section and set <code>https-enabled = true</code>.</p> <blockquote> <p><strong>Note:</strong> If HTTPS is enabled, you must explicitly connect to the instance using <code>https://</code>, there is no redirect from <code>http</code> to <code>https</code>.</p> </blockquote> <h3 id=\"selecting-the-database\">Selecting the Database</h3> <p>Choose your target database for writes and queries from the “Databases” pull-down menu in the upper right. If you have recently created a database you will need to refresh the Admin UI page before it appears in the pull-down menu.</p> <h2 id=\"writing-data\">Writing Data</h2> <p>The Admin UI has a “Write Data” link in the top menu bar. This link pops up a modal dialog that will accept points in the <a href=\"../../write_protocols/line/index\">line protocol</a> format.</p> <h2 id=\"querying-data\">Querying Data</h2> <p>The Admin UI has a “Query” box where you can enter any valid <a href=\"../../query_language/spec/index\">InfluxQL</a> command, including database administration and schema exploration commands. The “Query Templates” pull-down menu will pre-populate the Query box with a number of common InfluxQL queries.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/tools/web_admin/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/tools/web_admin/</a>\n  </p>\n</div>\n","influxdb/v0.13/administration/authentication_and_authorization/index":"<h1>Authentication and Authorization</h1>     <p>This document covers setting up and managing authentication and authorization in InfluxDB.</p> <p><a href=\"index#authentication\">Authentication</a></p> <ul> <li><a href=\"index#set-up-authentication\">Set up authentication</a></li> <li>\n<a href=\"index#authenticating-requests\">Authenticating requests</a><br> ◦ <a href=\"index#authenticate-using-the-http-api\">Authenticate using the HTTP API</a><br> ◦ <a href=\"index#authenticate-using-the-cli\">Authenticate using the CLI</a><br>\n</li> </ul> <p><a href=\"index#authorization\">Authorization</a></p> <ul> <li>\n<a href=\"index#user-types-and-their-privileges\">User types and their privileges</a><br> ◦ <a href=\"index#admin-users\">Admin users</a><br> ◦ <a href=\"index#non-admin-users\">Non-admin users</a><br>\n</li> <li>\n<a href=\"index#user-management-commands\">User management commands</a><br> ◦ <a href=\"index#admin-user-management\">Admin user management</a><br> □ <a href=\"index#create-a-new-admin-user\"><code>CREATE</code> a new admin user</a><br> □ <a href=\"index#grant-administrative-privileges-to-an-existing-user\"><code>GRANT</code> administrative privileges to an existing user</a><br> □ <a href=\"index#revoke-administrative-privileges-from-an-admin-user\"><code>REVOKE</code> administrative privileges from an admin user</a><br> □ <a href=\"index#show-all-existing-users-and-their-admin-status\"><code>SHOW</code> all existing users and their admin status</a><br> ◦ <a href=\"index#non-admin-user-management\">Non-admin user management</a><br> □ <a href=\"index#create-a-new-non-admin-user\"><code>CREATE</code> a new non-admin user</a><br> □ <a href=\"index#grant-read-write-or-all-database-privileges-to-an-existing-user\"><code>GRANT</code> <code>READ</code>,<code>WRITE</code>, or <code>ALL</code> database privileges to an existing user</a><br> □ <a href=\"index#revoke-read-write-or-all-database-privileges-from-an-existing-user\"><code>REVOKE</code> <code>READ</code>,<code>WRITE</code>, or <code>ALL</code> database privileges from an existing user</a><br> □ <a href=\"index#show-a-user-s-database-privileges\"><code>SHOW</code> a user’s database privileges</a><br> ◦ <a href=\"index#show-a-user-s-database-privileges\">General admin and non-admin user management</a><br> □ <a href=\"index#re-set-a-user-s-password\">Re<code>SET</code> a user’s password</a><br> □ <a href=\"index#drop-a-user\"><code>DROP</code> a user</a><br>\n</li> </ul> <p><a href=\"index#authentication-and-authorization-http-errors\">Authentication and authorization HTTP errors</a></p> <blockquote> <p><strong>Note:</strong> Authentication and authorization should not be relied upon to prevent access and protect data from malicious actors. If additional security or compliance features are desired, InfluxDB should be run behind a third-party service.</p> </blockquote> <h2 id=\"authentication\">Authentication</h2> <p>InfluxDB’s HTTP API and the <a href=\"../../tools/shell/index\">command line interface</a> (CLI), which connects to the database using the API, include simple, built-in authentication based on user credentials. When you enable authentication InfluxDB only executes HTTP requests that are sent with valid credentials.</p> <blockquote> <p><strong>Note:</strong> Authentication only occurs at the HTTP request scope. Plugins do not currently have the ability to authenticate requests and service endpoints (for example, Graphite, collectd, etc.) are not authenticated.</p> </blockquote> <h3 id=\"set-up-authentication\">Set up authentication</h3>  <ol> <li>\n<p>Create at least one <a href=\"index#admin-users\">admin user</a>. See the <a href=\"index#authorization\">authorization section</a> for how to create an admin user.</p> <blockquote> <p><strong>Note:</strong> If you enable authentication and have no users, InfluxDB will <strong>not</strong> enforce authentication and will only accept the <a href=\"index#create-a-new-admin-user\">query</a> that creates a new admin user. InfluxDB will enforce authentication once there is an admin user.</p> </blockquote>\n</li> <li>\n<p>By default, authentication is disabled in the configuration file. Enable authentication by setting the <code>auth-enabled</code> option to <code>true</code> in the <code>[http]</code> section of the configuration file:</p> <pre>[http]  \nenabled = true  \nbind-address = \":8086\"  \nauth-enabled = true # ✨\nlog-enabled = true  \nwrite-tracing = false  \npprof-enabled = false  \nhttps-enabled = false  \nhttps-certificate = \"/etc/ssl/influxdb.pem\"  \n</pre>\n</li> <li><p>Restart the process.</p></li> </ol> <p>Now InfluxDB will check user credentials on every request and will only process requests that have valid credentials for an existing user.</p> <h3 id=\"authenticating-requests\">Authenticating requests</h3>  <h4 id=\"authenticate-using-the-http-api\">Authenticate using the HTTP API</h4> <p>There are two options for authenticating with the HTTP API.</p> <ul> <li>Authenticate with Basic Authentication as described in <a href=\"http://tools.ietf.org/html/rfc2617\">RFC 2617, Section 2</a> - this is the preferred method for providing user credentials.</li> </ul> <p>Example:</p> <pre data-language=\"bash\">curl -G http://localhost:8086/query -u todd:influxdb4ever --data-urlencode \"q=SHOW DATABASES\"\n</pre> <ul> <li>Authenticate by providing query parameters in the URL. Set <code>u</code> as the username and <code>p</code> as the password.</li> </ul> <p>Example:</p> <pre data-language=\"bash\">curl -G http://localhost:8086/query --data-urlencode \"u=todd\" --data-urlencode \"p=influxdb4ever\" --data-urlencode \"q=SHOW DATABASES\"\n</pre> <p>The queries in both examples assume that the user is an <a href=\"index#admin-users\">admin user</a>. See the section on <a href=\"index#authorization\">authorization</a> for the different user types, their privileges, and more on user management.</p> <p>If you authenticate with both Basic Authentication <strong>and</strong> the URL query parameters, the user credentials specified in the query parameters take precedence.</p> <blockquote> <p><strong>Note:</strong> InfluxDB redacts passwords when you enable authentication.</p> </blockquote> <h4 id=\"authenticate-using-the-cli\">Authenticate using the CLI</h4> <p>There are two options for authenticating with the CLI.</p> <ul> <li>\n<p>Authenticate with <code>auth &lt;username&gt; &lt;password&gt;</code> after starting the CLI.</p> <p>Example:</p> <pre data-language=\"bash\">$ influx\nConnected to http://localhost:8086 version 0.13.x\nInfluxDB shell 0.13.x\n&gt; auth todd influxdb4ever\n&gt;\n</pre>\n</li> <li>\n<p>Authenticate by setting the <code>username</code> and <code>password</code> flags when you start the CLI.</p> <p>Example:</p> <pre data-language=\"bash\">influx -username todd -password influxdb4ever\n</pre>\n</li> </ul> <h2 id=\"authorization\">Authorization</h2> <p>Authorization is only enforced once you’ve <a href=\"index#set-up-authentication\">enabled authentication</a>. By default, authentication is disabled, all credentials are silently ignored, and all users have all privileges.</p> <h3 id=\"user-types-and-their-privileges\">User types and their privileges</h3>  <h4 id=\"admin-users\">Admin users</h4> <p>Admin users have <code>READ</code> and <code>WRITE</code> access to all databases and full access to the following administrative queries:</p> <p>Database management:<br> ◦ <code>CREATE DATABASE</code>, and <code>DROP DATABASE</code><br> ◦ <code>DROP SERIES</code> and <code>DROP MEASUREMENT</code><br> ◦ <code>CREATE RETENTION POLICY</code>, <code>ALTER RETENTION POLICY</code>, and <code>DROP RETENTION POLICY</code><br> ◦ <code>CREATE CONTINUOUS QUERY</code> and <code>DROP CONTINUOUS QUERY</code></p> <p>See the <a href=\"../../query_language/database_management/index\">database management</a> and <a href=\"../../query_language/continuous_queries/index\">continuous queries</a> pages for a complete discussion of the commands listed above.</p> <p>User management:<br> ◦ Admin user management:<br> <a href=\"index#create-a-new-admin-user\"><code>CREATE USER</code></a>, <a href=\"index#grant-administrative-privileges-to-an-existing-user\"><code>GRANT ALL PRIVILEGES</code></a>, <a href=\"index#revoke-administrative-privileges-from-an-admin-user\"><code>REVOKE ALL PRIVILEGES</code></a>, and <a href=\"index#show-all-existing-users-and-their-admin-status\"><code>SHOW USERS</code></a><br> ◦ Non-admin user management:<br> <a href=\"index#create-a-new-non-admin-user\"><code>CREATE USER</code></a>, <a href=\"index#grant-read-write-or-all-database-privileges-to-an-existing-user\"><code>GRANT [READ,WRITE,ALL]</code></a>, <a href=\"index#revoke-read-write-or-all-database-privileges-from-an-existing-user\"><code>REVOKE [READ,WRITE,ALL]</code></a>, and <a href=\"index#show-a-user-s-database-privileges\"><code>SHOW GRANTS</code></a><br> ◦ General user management:<br> <a href=\"index#re-set-a-user-s-password\"><code>SET PASSWORD</code></a> and <a href=\"index#drop-a-user\"><code>DROP USER</code></a></p> <p>See <a href=\"index#user-management-commands\">below</a> for a complete discussion of the user management commands.</p> <h4 id=\"non-admin-users\">Non-admin users</h4> <p>Non-admin users can have one of the following three privileges per database:<br> ◦ <code>READ</code><br> ◦ <code>WRITE</code><br> ◦ <code>ALL</code> (both <code>READ</code> and <code>WRITE</code> access)</p> <p><code>READ</code>, <code>WRITE</code>, and <code>ALL</code> privileges are controlled per user per database. A new non-admin user has no access to any database until they are specifically <a href=\"index#grant-read-write-or-all-database-privileges-to-an-existing-user\">granted privileges to a database</a> by an admin user.</p> <h3 id=\"user-management-commands\">User management commands</h3>  <h4 id=\"admin-user-management\">Admin user management</h4> <ul> <li>\n<h5 id=\"create-a-new-admin-user\">\n<code>CREATE</code> a new admin user:</h5> <pre data-language=\"sql\">CREATE USER &lt;username&gt; WITH PASSWORD '&lt;password&gt;' WITH ALL PRIVILEGES\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; CREATE USER paul WITH PASSWORD 'timeseries4days' WITH ALL PRIVILEGES\n&gt;\n</pre>\n</li> <li>\n<h5 id=\"grant-administrative-privileges-to-an-existing-user\">\n<code>GRANT</code> administrative privileges to an existing user:</h5> <pre data-language=\"sql\">GRANT ALL PRIVILEGES TO &lt;username&gt;\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; GRANT ALL PRIVILEGES TO todd\n&gt;\n</pre>\n</li> <li>\n<h5 id=\"revoke-administrative-privileges-from-an-admin-user\">\n<code>REVOKE</code> administrative privileges from an admin user:</h5> <pre data-language=\"sql\">REVOKE ALL PRIVILEGES FROM &lt;username&gt;\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; REVOKE ALL PRIVILEGES FROM todd\n&gt;\n</pre>\n</li> <li>\n<h5 id=\"show-all-existing-users-and-their-admin-status\">\n<code>SHOW</code> all existing users and their admin status:</h5> <pre data-language=\"sql\">SHOW USERS\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; SHOW USERS\nuser     admin\ntodd     false\npaul     true\nhermione false\ndobby    false\n</pre>\n</li> </ul> <h4 id=\"non-admin-user-management\">Non-admin user management</h4> <ul> <li>\n<h5 id=\"create-a-new-non-admin-user\">\n<code>CREATE</code> a new non-admin user:</h5> <pre data-language=\"sql\">CREATE USER &lt;username&gt; WITH PASSWORD '&lt;password&gt;'\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; CREATE USER todd WITH PASSWORD 'influxdb41yf3'\n&gt;\n</pre> <blockquote> <p><strong>Note:</strong> The password <a href=\"../../query_language/spec/index#strings\">string</a> must be wrapped in single quotes. Do not include the single quotes when authenticating requests. For passwords that include a single quote or a newline character, escape the single quote or newline character with a backslash both when creating the password and when submitting authentication requests.</p> </blockquote>\n</li> <li>\n<h5 id=\"grant-read-write-or-all-database-privileges-to-an-existing-user\">\n<code>GRANT</code> <code>READ</code>, <code>WRITE</code> or <code>ALL</code> database privileges to an existing user:</h5> <pre data-language=\"sql\">GRANT [READ,WRITE,ALL] ON &lt;database_name&gt; TO &lt;username&gt;\n</pre> <p>CLI examples:</p> <p><code>GRANT</code> <code>READ</code> access to <code>todd</code> on the <code>NOAA_water_database</code> database:</p> <pre data-language=\"bash\">&gt; GRANT READ ON NOAA_water_database TO todd\n&gt;\n</pre> <p><code>GRANT</code> <code>ALL</code> access to <code>todd</code> on the <code>NOAA_water_database</code> database:</p> <pre data-language=\"bash\">&gt; GRANT ALL ON NOAA_water_database TO todd\n&gt;\n</pre>\n</li> <li>\n<h5 id=\"revoke-read-write-or-all-database-privileges-from-an-existing-user\">\n<code>REVOKE</code> <code>READ</code>, <code>WRITE</code>, or <code>ALL</code> database privileges from an existing user:</h5> <pre data-language=\"sql\">REVOKE [READ,WRITE,ALL] ON &lt;database_name&gt; FROM &lt;username&gt;\n</pre> <p>CLI examples:</p> <p><code>REVOKE</code> <code>ALL</code> privileges from <code>todd</code> on the <code>NOAA_water_database</code> database:</p> <pre data-language=\"bash\">&gt; REVOKE ALL ON NOAA_water_database FROM todd\n&gt;\n</pre> <p><code>REVOKE</code> <code>WRITE</code> privileges from <code>todd</code> on the <code>NOAA_water_database</code> database:</p> <pre data-language=\"bash\">&gt; REVOKE WRITE ON NOAA_water_database FROM todd\n&gt;\n</pre> <blockquote> <p><strong>Note:</strong> If a user with <code>ALL</code> privileges has <code>WRITE</code> privileges revoked, they are left with <code>READ</code> privileges, and vice versa.</p> </blockquote>\n</li> <li>\n<h5 id=\"show-a-user-s-database-privileges\">\n<code>SHOW</code> a user’s database privileges:</h5> <pre data-language=\"sql\">SHOW GRANTS FOR &lt;user_name&gt;\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; SHOW GRANTS FOR todd\ndatabase                    privilege\nNOAA_water_database         WRITE\nanother_database_name       READ\nyet_another_database_name   ALL PRIVILEGES\n</pre>\n</li> </ul> <h4 id=\"general-admin-and-non-admin-user-management\">General admin and non-admin user management</h4> <ul> <li>\n<h5 id=\"re-set-a-user-s-password\">Re<code>SET</code> a user’s password:</h5> <pre data-language=\"sql\">SET PASSWORD FOR &lt;username&gt; = '&lt;password&gt;'\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; SET PASSWORD FOR todd = 'influxdb4ever'\n&gt;\n</pre> <blockquote> <p><strong>Note:</strong> The password <a href=\"../../query_language/spec/index#strings\">string</a> must be wrapped in single quotes. Do not include the single quotes when authenticating requests. For passwords that include a single quote or a newline character, escape the single quote or newline character with a backslash both when creating the password and when submitting authentication requests.</p> </blockquote>\n</li> <li>\n<h5 id=\"drop-a-user\">\n<code>DROP</code> a user:</h5> <pre data-language=\"sql\">DROP USER &lt;username&gt;\n</pre> <p>CLI example:</p> <pre data-language=\"bash\">&gt; DROP USER todd\n&gt;\n</pre>\n</li> </ul> <h2 id=\"authentication-and-authorization-http-errors\">Authentication and authorization HTTP errors</h2> <p>Requests with invalid credentials and requests by unauthorized users yield the <code>HTTP 401 Unauthorized</code> response.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/administration/authentication_and_authorization/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/administration/authentication_and_authorization/</a>\n  </p>\n</div>\n","influxdb/v0.13/tools/shell/index":"<h1>InfluxDB CLI/Shell</h1>     <p>The InfluxDB’s command line interface (<code>influx</code>) is an interactive shell for the HTTP API. Use <code>influx</code> to write data (manually or from a file), query data interactively, and view query output in different formats.</p> <ul> <li><a href=\"index#launch-influx\">Launch <code>influx</code></a></li> <li><a href=\"index#influx-arguments\"><code>influx</code> Arguments</a></li> <li><a href=\"index#influx-commands\"><code>influx</code> Commands</a></li> </ul> <h2 id=\"launch-influx\">Launch <code>influx</code>\n</h2> <p>If you <a href=\"https://influxdata.com/downloads/\">install</a> InfluxDB via a package manager, the CLI is installed at <code>/usr/bin/influx</code> (<code>/usr/local/bin/influx</code> on OS X).</p> <p>To access the CLI, first launch the <code>influxd</code> database process and then launch <code>influx</code> in your terminal. Once you’ve entered the shell and successfully connected to an InfluxDB node, you’ll see the following output: <br> <br></p> <pre data-language=\"bash\">$ influx\nConnected to http://localhost:8086 version 0.13.x\nInfluxDB shell 0.13.x\n</pre> <h2 id=\"influx-arguments\">\n<code>influx</code> Arguments</h2> <p>There are several arguments you can pass into <code>influx</code> when starting. List them with <code>$ influx --help</code>. The list below offers a brief discussion of each option. We provide detailed information on <code>-execute</code>, <code>-format</code>, and <code>-import</code> at the end of this section.</p> <p><code>-compressed</code><br> Set to true if the import file is compressed. Use with <code>-import</code>.</p> <p><code>-consistency 'any|one|quorum|all'</code><br> Set the write consistency level.</p> <p><code>-database 'database name'</code><br> The database to which <code>influx</code> connects.</p> <p><code>-execute 'command'</code><br> Execute an <a href=\"../../query_language/data_exploration/index\">InfluxQL</a> command and quit. See <a href=\"index#execute-an-influxql-command-and-quit-with-execute\">-execute</a>.</p> <p><code>-format 'json|csv|column'</code><br> Specifies the format of the server responses. See <a href=\"index#specify-the-format-of-the-server-responses-with-format\">-format</a>.</p> <p><code>-host 'host name'</code><br> The host to which <code>influx</code> connects. By default, InfluxDB runs on localhost.</p> <p><code>-import</code><br> Import new data from a file or import a previously <a href=\"https://github.com/influxdb/influxdb/blob/master/importer/README.md\">exported</a> database from a file. See <a href=\"index#import-data-from-a-file-with-import\">-import</a>.</p> <p><code>-password 'password'</code><br> The password <code>influx</code> uses to connect to the server. <code>influx</code> will prompt for a password if you leave it blank (<code>-password ''</code>).</p> <p><code>-path</code><br> The path to the file to import. Use with <code>-import</code>.</p> <p><code>-port 'port #'</code><br> The port to which <code>influx</code> connects. By default, InfluxDB runs on port <code>8086</code>.</p> <p><code>-pps</code><br> How many points per second the import will allow. By default, pps is zero and influx will not throttle importing. Use with <code>-import</code>.</p> <p><code>-precision 'rfc3339|h|m|s|ms|u|ns'</code><br> Specifies the format/precision of the timestamp: <code>rfc3339</code> (<code>YYYY-MM-DDTHH:MM:SS.nnnnnnnnnZ</code>), <code>h</code> (hours), <code>m</code> (minutes), <code>s</code> (seconds), <code>ms</code> (milliseconds), <code>u</code> (microseconds), <code>ns</code> (nanoseconds). Precision defaults to nanoseconds.</p> <blockquote> <p><strong>Note:</strong> Setting the precision to <code>rfc3339</code> (<code>-precision rfc3339</code>) works with the <code>-execute</code> option, but it does not work with the <code>-import option</code>. All other precision formats (e.g. <code>h</code>,<code>m</code>,<code>s</code>,<code>ms</code>,<code>u</code>, and <code>ns</code>) work with the <code>-execute</code> and <code>-import</code> options.</p> </blockquote> <p><code>-pretty</code><br> Turns on pretty print for the <code>json</code> format.</p> <p><code>-ssl</code><br> Use https for requests.</p> <p><code>-username 'username'</code><br> The username <code>influx</code> uses to connect to the server.</p> <p><code>-version</code><br> Display the InfluxDB version and exit.</p> <h3 id=\"execute-an-influxql-command-and-quit-with-execute\">Execute an InfluxQL command and quit with <code>-execute</code>\n</h3> <p>Execute queries that don’t require a database specification:</p> <pre data-language=\"bash\">$ influx -execute 'SHOW DATABASES'\nname: databases\n---------------\nname\nNOAA_water_database\n_internal\ntelegraf\npirates\n</pre> <p>Execute queries that do require a database specification, and change the timestamp precision:</p> <pre data-language=\"bash\">$ influx -execute 'SELECT * FROM h2o_feet LIMIT 3' -database=NOAA_water_database -precision=rfc3339\nname: h2o_feet\n--------------\ntime\t\t\t               level description\t    location\t     water_level\n2015-08-18T00:00:00Z\t below 3 feet\t\t        santa_monica\t 2.064\n2015-08-18T00:00:00Z\t between 6 and 9 feet  coyote_creek  8.12\n2015-08-18T00:06:00Z\t between 6 and 9 feet  coyote_creek  8.005\n</pre> <h3 id=\"specify-the-format-of-the-server-responses-with-format\">Specify the format of the server responses with <code>-format</code>\n</h3> <p>The default format is <code>column</code>:</p> <pre data-language=\"bash\">$ influx -format=column\n[...]\n&gt; SHOW DATABASES\nname: databases\n---------------\nname\nNOAA_water_database\n_internal\ntelegraf\npirates\n</pre> <p>Change the format to <code>csv</code>:</p> <pre data-language=\"bash\">$ influx -format=csv\n[...]\n&gt; SHOW DATABASES\nname,name\ndatabases,NOAA_water_database\ndatabases,_internal\ndatabases,telegraf\ndatabases,pirates\n</pre> <p>Change the format to <code>json</code>:</p> <pre data-language=\"bash\">$ influx -format=json\n[...]\n&gt; SHOW DATABASES\n{\"results\":[{\"series\":[{\"name\":\"databases\",\"columns\":[\"name\"],\"values\":[[\"NOAA_water_database\"],[\"_internal\"],[\"telegraf\"],[\"pirates\"]]}]}]}\n</pre> <p>Change the format to <code>json</code> and turn on pretty print:</p> <pre data-language=\"bash\">$ influx -format=json -pretty\n[...]\n&gt; SHOW DATABASES\n{\n    \"results\": [\n        {\n            \"series\": [\n                {\n                    \"name\": \"databases\",\n                    \"columns\": [\n                        \"name\"\n                    ],\n                    \"values\": [\n                        [\n                            \"NOAA_water_database\"\n                        ],\n                        [\n                            \"_internal\"\n                        ],\n                        [\n                            \"telegraf\"\n                        ],\n                        [\n                            \"pirates\"\n                        ]\n                    ]\n                }\n            ]\n        }\n    ]\n}\n</pre> <h3 id=\"import-data-from-a-file-with-import\">Import data from a file with <code>-import</code>\n</h3> <p>The import file has two sections:</p> <ul> <li>\n<strong>DDL (Data Definition Language)</strong>: Contains the <a href=\"../../query_language/database_management/index\">InfluxQL commands</a> for creating the relevant <a href=\"../../concepts/glossary/index\">database</a> and managing the <a href=\"../../concepts/glossary/index#retention-policy-rp\">retention policy</a>. If your database and retention policy already exist, your file can skip this section.</li> <li>\n<strong>DML (Data Manipulation Language)</strong>: Lists the relevant database and (if desired) retention policy and contains the data in <a href=\"../../write_protocols/line/index\">line protocol</a>.</li> </ul> <p>Example:</p> <p>File (<code>datarrr.txt</code>):</p> <pre># DDL\nCREATE DATABASE pirates\nCREATE RETENTION POLICY oneday ON pirates DURATION 1d REPLICATION 1\n\n# DML\n# CONTEXT-DATABASE: pirates\n# CONTEXT-RETENTION-POLICY: oneday\n\ntreasures,captain_id=dread_pirate_roberts value=801 1439856000\ntreasures,captain_id=flint value=29 1439856000\ntreasures,captain_id=sparrow value=38 1439856000\ntreasures,captain_id=tetra value=47 1439856000\ntreasures,captain_id=crunch value=109 1439858880\n</pre> <p>Command:</p> <pre>$influx -import -path=datarrr.txt -precision=s\n</pre> <p>Results:</p> <pre>2015/12/22 12:25:06 Processed 2 commands\n2015/12/22 12:25:06 Processed 5 inserts\n2015/12/22 12:25:06 Failed 0 inserts\n</pre> <blockquote> <p><strong>Note:</strong> For large datasets, <code>influx</code> writes out a status message every 100,000 points. For example: <br> <br></p> <pre>2015/08/21 14:48:01 Processed 3100000 lines.\nTime elapsed: 56.740578415s.\nPoints per second (PPS): 54634\n</pre> </blockquote> <p>Things to note about <code>-import</code>:</p> <ul> <li>Allow the database to ingest points by using <code>-pps</code> to set the number of points per second allowed by the import. By default, pps is zero and <code>influx</code> does not throttle importing.</li> <li>Imports work with <code>.gz</code> files, just include <code>-compressed</code> in the command.</li> <li>If your data file has more than 5,000 points, it may be necessary to split that file into several files in order to write your data in batches to InfluxDB. By default, the HTTP request times out after five seconds. InfluxDB will still attempt to write the points after that time out but there will be no confirmation that they were successfully written.</li> </ul> <blockquote> <p><strong>Note:</strong> For how to export data from InfluxDB version 0.8.9, see <a href=\"https://github.com/influxdb/influxdb/blob/master/importer/README.md\">Exporting from 0.8.9</a>.</p> </blockquote> <h2 id=\"influx-commands\">\n<code>influx</code> Commands</h2> <p>Enter <code>help</code> in the CLI for a partial list of the available commands.</p> <h3 id=\"commands\">Commands</h3> <p>The list below offers a brief discussion of each command. We provide detailed information on <code>insert</code> at the end of this section.</p> <p><code>auth</code><br> Prompts you for your username and password. <code>influx</code> uses those credentials when querying a database.</p> <p><code>connect &lt;host:port&gt;</code><br> Connect to a different server without exiting the shell. By default, <code>influx</code> connects to <code>localhost:8086</code>. If you do not specify either the host or the port, <code>influx</code> assumes the default setting for the missing attribute.</p> <p><code>consistency &lt;level&gt;</code><br> Sets the write consistency level: <code>any</code>, <code>one</code>, <code>quorum</code>, or <code>all</code>.</p> <p><code>exit</code><br> Quits the <code>influx</code> shell.</p> <p><code>format &lt;format&gt;</code><br> Specifies the format of the server responses: <code>json</code>, <code>csv</code>, or <code>column</code>. See the description of <a href=\"index#specify-the-format-of-the-server-responses-with-format\">-format</a> for examples of each format.</p> <p><code>history</code><br> Displays your command history. To use the history while in the shell, simply use the “up” arrow. <code>influx</code> stores your last 1,000 commands in your home directory in <code>.influx_history</code>.</p> <p><code>insert</code><br> Write data using line protocol. See <a href=\"index#write-data-to-influxdb-with-insert\">insert</a>.</p> <p><code>precision &lt;format&gt;</code><br> Specifies the format/precision of the timestamp: <code>rfc3339</code> (<code>YYYY-MM-DDTHH:MM:SS.nnnnnnnnnZ</code>), <code>h</code> (hours), <code>m</code> (minutes), <code>s</code> (seconds), <code>ms</code> (milliseconds), <code>u</code> (microseconds), <code>ns</code> (nanoseconds). Precision defaults to nanoseconds.</p> <p><code>pretty</code><br> Turns on pretty print for the <code>json</code> format.</p> <p><code>settings</code><br> Outputs the current settings for the shell including the <code>Host</code>, <code>Username</code>, <code>Database</code>, <code>Pretty</code> status, <code>Format</code>, and <code>Write Consistency</code>.</p> <p><code>use &lt;db_name&gt;</code><br> Sets the current database. Once <code>influx</code> sets the current database, there is no need to specify that database in queries. <code>influx</code> automatically queries the current database and its <code>DEFAULT</code> retention policy.</p> <h4 id=\"write-data-to-influxdb-with-insert\">Write data to InfluxDB with <code>insert</code>\n</h4> <p>Enter <code>insert</code> followed by the data in <a href=\"../../write_protocols/line/index\">line protocol</a> to write data to InfluxDB. Use <code>insert into &lt;retention policy&gt; &lt;line protocol&gt;</code> to write data to a specific <a href=\"../../concepts/glossary/index#retention-policy-rp\">retention policy</a>.</p> <p>Write data to a single field in the measurement <code>treasures</code> with the tag <code>captain_id = pirate_king</code>. <code>influx</code> automatically writes the point to the database’s <code>DEFAULT</code> retention policy.</p> <pre>&gt; INSERT treasures,captain_id=pirate_king value=2\n&gt;\n</pre> <p>Write the same point to the already-existing retention policy <code>oneday</code>:</p> <pre>&gt; INSERT INTO oneday treasures,captain_id=pirate_king value=2\nUsing retention policy oneday\n&gt;\n</pre> <p>Note that once you specify a retention policy with <code>INSERT INTO</code>, <code>influx</code> automatically writes data to that retention policy. This occurs even for later <code>INSERT</code> entries that do not include an <code>INTO</code> clause. Restarting the CLI will revert to using the <code>DEFAULT</code> retention policy.</p> <h3 id=\"queries\">Queries</h3> <p>Execute all InfluxQL queries in <code>influx</code>. See <a href=\"../../query_language/data_exploration/index\">Data Exploration</a>, <a href=\"../../query_language/schema_exploration/index\">Schema Exploration</a>, <a href=\"../../query_language/database_management/index\">Database Management</a>, <a href=\"../../administration/authentication_and_authorization/index\">Authentication and Authorization</a> for InfluxQL documentation.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/tools/shell/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/tools/shell/</a>\n  </p>\n</div>\n","kapacitor/v0.13/tick/index":"<h1>TICKscript Language Reference</h1>     <p>Kapacitor uses a DSL named <code>TICKscript</code>. The DSL is used to define the pipelines for processing data in Kapacitor.</p> <p>The TICKscript language is an invocation chaining language. Each script has a flat scope and each variable in the scope defines methods that can be called on it. These methods come in two flavors.</p> <ul> <li>Property methods – Modifies the node they are called on and returns a reference to the same node.</li> <li>Chaining methods – Creates a new node as a child of the node they are called on and returns a reference to the new node.</li> </ul> <p>The following reference documentation list each node’s <code>Property</code> methods and <code>Chaining</code> methods along with examples and descriptions of the function of the node.</p> <p>Every TICKscript will have either a <code>stream</code> or <code>batch</code> variable defined depending on the type of task you want to run. The <code>stream</code> and <code>batch</code> variables are an instance of a <a href=\"../nodes/stream_node/index\">StreamNode</a> or <a href=\"https://docs.influxdata.com/kapacitor/v0.11/nodes/source_batch_node/\">SourceBatchNode</a> respectively.</p> <h2 id=\"pipelines\">Pipelines</h2> <p>Kapacitor uses TICKscripts to define data processing pipelines. A pipeline is set of nodes that process data and edges that connect the nodes. Pipelines in Kapacitor are directed acyclic graphs (<a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\">DAGs</a>) meaning each edge has a direction that data flows and there cannot be any cycles in the pipeline.</p> <p>Each edge has a type, one of the following:</p> <ul> <li>StreamEdge – an edge that transfers data a single data point at a time.</li> <li>BatchEdge – an edge that transfers data in chunks instead of one point at a time.</li> </ul> <p>When connecting nodes the TICKscript language will not prevent you from connecting edges of the wrong type but rather the check will be performed at runtime. So just be aware that a syntactically correct script may define a pipeline that is invalid.</p> <h2 id=\"example\">Example</h2> <pre data-language=\"javascript\">stream\n    |from()\n        .measurement('app')\n    |eval(lambda: \"errors\" / \"total\")\n        .as('error_percent')\n    // Write the transformed data to InfluxDB\n    |influxDBOut()\n        .database('mydb')\n        .retentionPolicy('myrp')\n        .measurement('errors')\n        .tag('kapacitor', 'true')\n        .tag('version', '0.2')\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/tick/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/tick/</a>\n  </p>\n</div>\n","kapacitor/v0.13/tick/syntax/index":"<h1>Syntax</h1>     <h2 id=\"literals\">Literals</h2> <h3 id=\"booleans\">Booleans</h3> <p>Boolean literals are the keywords <code>TRUE</code> and <code>FALSE</code>. They are case sensitive.</p> <h3 id=\"numbers\">Numbers</h3> <p>Numbers are typed and are either a <code>float64</code> or an <code>int64</code>. If the number contains a decimal it is considered to be a <code>float64</code> otherwise it is an <code>int64</code>. All numbers are considered to be base 10 numbers.</p> <p>Valid number literals:</p> <ul> <li>1 – int64</li> <li>1.2 – float64</li> <li>5 – int64</li> <li>5.0 – float64</li> <li>0.42 – float64</li> </ul> <p>Invalid number literals:</p> <ul> <li>.1 – decimals must have a leading zero</li> </ul> <h3 id=\"strings\">Strings</h3> <p>There are two ways to write string literals:</p> <p>1. Single quoted strings with backslash escaped single quotes.</p> <pre>This string `'single \\' quoted'` becomes the literal `single ' quoted`.\n</pre> <p>2. Triple single quoted strings with no escaping.</p> <pre>This string `'''triple \\' quoted'''` becomes the literal `triple \\' quoted`.\n</pre> <h3 id=\"durations\">Durations</h3> <p>TICKscript supports durations literals. They are of the form of an InfluxQL duration literals. See <a href=\"https://influxdb.com/docs/v0.9/query_language/spec.html#literals\">https://influxdb.com/docs/v0.9/query_language/spec.html#literals</a></p> <p>Duration literals specify a length of time. An integer literal followed immediately (with no spaces) by a duration unit listed below is interpreted as a duration literal.</p> <h4 id=\"duration-unit-definitions\">Duration unit definitions</h4> <table> <thead> <tr> <th>Units</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>u or µ</td> <td>microseconds (1 millionth of a second)</td> </tr> <tr> <td>ms</td> <td>milliseconds (1 thousandth of a second)</td> </tr> <tr> <td>s</td> <td>second</td> </tr> <tr> <td>m</td> <td>minute</td> </tr> <tr> <td>h</td> <td>hour</td> </tr> <tr> <td>d</td> <td>day</td> </tr> <tr> <td>w</td> <td>week</td> </tr> </tbody> </table> <h2 id=\"statements\">Statements</h2> <p>A statement begins with an identifier and any number of chaining function calls. The result of a statement can be assigned to a variable using the <code>var</code> keyword and assignment operator <code>=</code>.</p> <p>Example:</p> <pre data-language=\"javascript\">var errors = stream\n    |from()\n        .measurement('errors')\nvar requests = stream\n    |from()\n    .measurement('requests')\n// Join the errors and requests stream\nerrors\n    |join(requests)\n        .as('errors', 'requests')\n    |eval(lambda: \"errors.value\" / \"requests.value\")\n</pre> <h2 id=\"format\">Format</h2> <h3 id=\"whitespace\">Whitespace</h3> <p>Whitespace is ignored and can be used to format the code as you like.</p> <p>Typically property methods are indented in from their calling node. This way methods along the left edge are chaining methods.</p> <p>For example:</p> <pre data-language=\"javascript\">stream\n    |eval(lambda: \"views\" + \"errors\")\n        .as('total_views') // Increase indent for property method.\n    |httpOut('example') // Decrease indent for chaining method.\n</pre> <h3 id=\"comments\">Comments</h3> <p>Basic <code>//</code> style single line comments are supported.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/tick/syntax/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/tick/syntax/</a>\n  </p>\n</div>\n","influxdb/v0.13/administration/config/index":"<h1>Database Configuration</h1>     <p>The InfluxDB configuration file contains configuration settings specific to a local node.</p> <h2 id=\"using-configuration-files\">Using Configuration Files</h2> <p>The <code>influxd config</code> command will print out a new TOML-formatted configuration with all the available configuration options set to their default values. On POSIX systems, a new configuration file can be generated by redirecting the output of the command to a file.</p> <pre data-language=\"bash\">influxd config &gt; /etc/influxdb/influxdb.generated.conf\n</pre> <p>Custom settings from an older configuration file can be preserved when generating a new config file using the <code>-config</code> option.</p> <pre data-language=\"bash\">influxd config -config /etc/influxdb/influxdb.conf.old &gt; /etc/influxdb/influxdb.conf.new\n</pre> <p>There are two ways to launch InfluxDB with your configuration file:</p> <ul> <li>\n<p>Point the process to the correct configuration file by using the <code>-config</code> option:</p> <pre data-language=\"bash\">influxd -config influxdb.generated.conf\n</pre>\n</li> <li>\n<p>Set the environment variable <code>INFLUXDB_CONFIG_PATH</code> to the path of your configuration file and start the process. For example:</p> <pre>echo $INFLUXDB_CONFIG_PATH\n/root/influxdb.generated.conf\n\n\ninfluxd\n</pre>\n</li> </ul> <p>InfluxDB first checks for the <code>-config</code> option and then for the environment variable. If you do not supply a configuration file, InfluxDB uses an internal default configuration (equivalent to the output of <code>influxd config</code>). A new configuration file should be generated every time InfluxDB is upgraded.</p> <p>See the <a href=\"../../introduction/installation/index#generate-a-configuration-file\">installation documentation</a> for more detail on generating and using configuration files.</p> <h2 id=\"environment-variables\">Environment variables</h2> <p>Set any configuration option with an environment variable. When the process launches, InfluxDB checks for environment variables of the form below. The environment variable overrides the equivalent option in the configuration file.</p> <pre>INFLUXDB_&lt;CONFIG_SECTION_NAME&gt;_&lt;OPTION_NAME&gt;\n</pre> <p>Example:</p> <p>InfluxDB will use <code>~/mydataz</code> as the <a href=\"#dir-var-lib-influxdb-data\">data directory</a>:</p> <pre>$ echo $INFLUXDB_DATA_DIR\n~/mydataz\n</pre> <blockquote> <p><strong>Note:</strong> Replace any dashes in option names with underscores. For example, this is the valid environment variable for the <code>store-database</code> option in the <code>[monitor]</code> section:</p> <pre>INFLUXDB_MONITOR_STORE_DATABASE\n</pre> </blockquote> <h2 id=\"configuration-sections\">Configuration Sections</h2> <ul> <li><a href=\"#global-options\">Global Options</a></li> <li><a href=\"#meta\">[meta]</a></li> <li><a href=\"#data\">[data]</a></li> <li><a href=\"#cluster\">[cluster]</a></li> <li><a href=\"#retention\">[retention]</a></li> <li><a href=\"#shard-precreation\">[shard-precreation]</a></li> <li><a href=\"#admin\">[admin]</a></li> <li><a href=\"#monitor\">[monitor]</a></li> <li><a href=\"#subscriber\">[subscriber]</a></li> <li><a href=\"#http\">[http]</a></li> <li><a href=\"#graphite\">[[graphite]]</a></li> <li><a href=\"#collectd\">[[collectd]]</a></li> <li><a href=\"#opentsdb\">[[opentsdb]]</a></li> <li><a href=\"#udp\">[[udp]]</a></li> <li><a href=\"#continuous-queries\">[continuous_queries]</a></li> </ul> <h2 id=\"configuration-options\">Configuration Options</h2> <p>Every configuration section has configuration options. Every configuration option is optional. If a configuration option is not provided, its default value will be used. All configuration options listed below are set to their default value.</p> <blockquote> <p><strong>Note:</strong> This page documents configuration options for the latest official release - the <a href=\"https://github.com/influxdb/influxdb/blob/master/etc/config.sample.toml\">sample configuration file on GitHub</a> will always be slightly ahead of what is documented here.</p> </blockquote> <h2 id=\"global-options\">Global Options</h2> <h3 id=\"reporting-disabled-false\">reporting-disabled = false</h3> <p>InfluxData, the company, relies on reported data from running nodes primarily to track the adoption rates of different InfluxDB versions. This data helps InfluxData support the continuing development of InfluxDB.</p> <p>The <code>reporting-disabled</code> option toggles the reporting of data every 24 hours to <code>usage.influxdata.com</code>. Each report includes a randomly-generated identifier, OS, architecture, InfluxDB version, and the number of <a href=\"../../concepts/glossary/index#database\">databases</a>, <a href=\"../../concepts/glossary/index#measurement\">measurements</a>, and unique <a href=\"../../concepts/glossary/index#series\">series</a>. Setting this option to <code>true</code> will disable reporting.</p> <blockquote> <p><strong>Note:</strong> No data from user databases is ever transmitted.</p> </blockquote> <h2 id=\"meta\">[meta]</h2> <p>This section controls parameters for InfluxDB’s metastore, which stores information on users, databases, retention policies, shards, and continuous queries.</p> <h3 id=\"dir-var-lib-influxdb-meta\">dir = “/var/lib/influxdb/meta”</h3> <p>The <code>meta</code> directory. Files in the <code>meta</code> directory include <code>meta.db</code>.</p> <blockquote> <p><strong>Note:</strong> The default directory for OS X installations is <code>/Users/&lt;username&gt;/.influxdb/meta</code></p> </blockquote> <h3 id=\"retention-autocreate-true\">retention-autocreate = true</h3> <p>Retention policy auto-creation automatically creates a <a href=\"../../concepts/glossary/index#retention-policy-rp\"><code>default</code> retention policy</a> when a database is created. The retention policy is named <code>default</code>, has an infinite duration, and is also set as the database’s <code>DEFAULT</code> retention policy, which is used when a write or query does not specify a retention policy. Disable this setting to prevent the creation of a <code>default</code> retention policy when creating databases.</p> <h3 id=\"logging-enabled-true\">logging-enabled = true</h3> <p>Meta logging toggles the logging of messages from the meta service.</p> <h3 id=\"pprof-enabled-false\">pprof-enabled = false</h3> <h3 id=\"lease-duration-1m0s\">lease-duration = “1m0s”</h3> <p>The default duration for leases.</p> <h2 id=\"data\">[data]</h2> <p>This section controls where the actual shard data for InfluxDB lives and how it is flushed from the WAL. <code>dir</code> may need to be changed to a suitable place for you system, but the WAL settings are an advanced configuration. The defaults should work for most systems.</p> <h3 id=\"dir-var-lib-influxdb-data\">dir = “/var/lib/influxdb/data”</h3> <p>The directory where InfluxDB stores the data. This directory may be changed.</p> <blockquote> <p><strong>Note:</strong> The default directory for OS X installations is <code>/Users/&lt;username&gt;/.influxdb/data</code></p> </blockquote> <h3 id=\"engine-tsm1\">engine = “tsm1”</h3> <p>The storage engine. This is the only option for version 0.13.</p> <h3 id=\"wal-dir-var-lib-influxdb-wal\">wal-dir = “/var/lib/influxdb/wal”</h3> <p>The WAL directory is the location of the <a href=\"../../concepts/glossary/index#wal-write-ahead-log\">write ahead log</a>.</p> <h3 id=\"wal-logging-enabled-true\">wal-logging-enabled = true</h3> <p>The WAL logging enabled toggles the logging of WAL operations such as WAL flushes to disk.</p> <h3 id=\"query-log-enabled-true\">query-log-enabled = true</h3> <p>The query log enabled setting toggles the logging of parsed queries before execution. Very useful for troubleshooting, but will log any sensitive data contained within a query.</p> <h3 id=\"cache-max-memory-size-524288000\">cache-max-memory-size = 524288000</h3> <p>The cache maximum memory size is the maximum size (in bytes) a shard’s cache can reach before it starts rejecting writes.</p> <h3 id=\"cache-snapshot-memory-size-26214400\">cache-snapshot-memory-size = 26214400</h3> <p>The cache snapshot memory size is the size at which the engine will snapshot the cache and write it to a TSM file, freeing up memory.</p> <h3 id=\"cache-snapshot-write-cold-duration-1h0m0s\">cache-snapshot-write-cold-duration = “1h0m0s”</h3> <p>The cache snapshot write cold duration is the length of time at which the engine will snapshot the cache and write it to a new TSM file if the shard hasn’t received writes or deletes.</p> <h3 id=\"compact-full-write-cold-duration-24h0m0s\">compact-full-write-cold-duration = “24h0m0s”</h3> <p>The compact full write cold duration is the duration at which the engine will compact all TSM files in a shard if it hasn’t received a write or delete.</p> <h3 id=\"max-points-per-block-0\">max-points-per-block = 0</h3> <p>The maximum points per block is the maximum number of points in an encoded block in a TSM file. Larger numbers may yield better compression but could incur a performance penalty when querying.</p> <h3 id=\"data-logging-enabled-true\">data-logging-enabled = true</h3> <h2 id=\"cluster\">[cluster]</h2> <p>This section contains configuration options for query management. For more on managing queries, see <a href=\"../../troubleshooting/query_management/index\">Query Management</a>.</p> <h3 id=\"force-remote-mapping-false\">force-remote-mapping = false</h3> <h3 id=\"write-timeout-10s\">write-timeout = “10s”</h3> <p>The time within which a write request must complete on the cluster.</p> <h3 id=\"shard-writer-timeout-5s\">shard-writer-timeout = “5s”</h3> <p>The time within which a remote shard must respond to a write request.</p> <h3 id=\"max-remote-write-connections-3\">max-remote-write-connections = 3</h3> <h3 id=\"shard-mapper-timeout-5s\">shard-mapper-timeout = “5s”</h3> <h3 id=\"max-concurrent-queries-0\">max-concurrent-queries = 0</h3> <p>The maximum number of running queries allowed on your instance. The default setting (<code>0</code>) allows for an unlimited number of queries.</p> <h3 id=\"query-timeout-0\">query-timeout = “0”</h3> <p>The maximum time for which a query can run on your instance before InfluxDB kills the query. The default setting (<code>0</code>) allows queries to run with no time restrictions. This setting is a <a href=\"../../query_language/spec/index#durations\">duration literal</a>.</p> <h3 id=\"log-queries-after-0\">log-queries-after = “0”</h3> <p>The maximum time a query can run after which InfluxDB logs the query with a <code>Detected slow query</code> message. The default setting (<code>\"0\"</code>) will never tell InfluxDB to log the query. This setting is a <a href=\"../../query_language/spec/index#durations\">duration literal</a>.</p> <h3 id=\"max-select-point-0\">max-select-point = 0</h3> <p>The maximum number of <a href=\"../../concepts/glossary/index#point\">points</a> that a <code>SELECT</code> statement can process. The default setting (<code>0</code>) allows the <code>SELECT</code> statement to process an unlimited number of points.</p> <h3 id=\"max-select-series-0\">max-select-series = 0</h3> <p>The maximum number of <a href=\"../../concepts/glossary/index#series\">series</a> that a <code>SELECT</code> statement can process. The default setting (<code>0</code>) allows the <code>SELECT</code> statement to process an unlimited number of series.</p> <h3 id=\"max-select-buckets-0\">max-select-buckets = 0</h3> <p>The maximum number of <code>GROUP BY time()</code> buckets that a query can process. The default setting (<code>0</code>) allows a query to process an unlimited number of buckets.</p> <h2 id=\"retention\">[retention]</h2> <p>This section controls the enforcement of retention policies for evicting old data.</p> <h3 id=\"enabled-true\">enabled = true</h3> <p>Set to <code>false</code> to prevent InfluxDB from enforcing retention policies.</p> <h3 id=\"check-interval-30m0s\">check-interval = “30m0s”</h3> <p>The rate at which InfluxDB checks to enforce a retention policy.</p> <h2 id=\"shard-precreation\">[shard-precreation]</h2> <p>Controls the precreation of shards so that shards are available before data arrive. Only shards that, after creation, will have both a start- and end-time in the future are ever created. Shards that would be wholly or partially in the past are never precreated.</p> <h3 id=\"enabled-true-1\">enabled = true</h3> <h3 id=\"check-interval-10m0s\">check-interval = “10m0s”</h3> <h3 id=\"advance-period-30m0s\">advance-period = “30m0s”</h3> <p>The maximum period in the future for which InfluxDB precreates shards. The <code>30m</code> default should work for most systems. Increasing this setting too far in the future can cause inefficiencies.</p> <h2 id=\"admin\">[admin]</h2> <p>Controls the availability of the built-in, web-based admin interface.</p> <h3 id=\"enabled-true-2\">enabled = true</h3> <p>Set to <code>false</code> to disable the admin interface.</p> <h3 id=\"bind-address-8083\">bind-address = “:8083”</h3> <p>The port used by the admin interface.</p> <h3 id=\"https-enabled-false\">https-enabled = false</h3> <p>Set to <code>true</code> to enable HTTPS for the admin interface.</p> <blockquote> <p><strong>Note:</strong> HTTPS must be enable for the <a href=\"index#http\">[http]</a> service for the admin UI to function properly using HTTPS.</p> </blockquote> <h3 id=\"https-certificate-etc-ssl-influxdb-pem\">https-certificate = “/etc/ssl/influxdb.pem”</h3> <p>The path of the certificate file.</p> <h3 id=\"version\">Version = “”</h3> <h2 id=\"monitor\">[monitor]</h2> <p>This section controls InfluxDB’s <a href=\"https://github.com/influxdb/influxdb/blob/master/monitor/README.md\">system self-monitoring</a>.</p> <p>By default, InfluxDB writes the data to the <code>_internal</code> database. If that database does not exist, InfluxDB creates it automatically. The <code>DEFAULT</code> retention policy on the <code>_internal</code> database is seven days. If you want to use a retention policy other than the seven-day retention policy, you must <a href=\"../../query_language/database_management/index#retention-policy-management\">create</a> it.</p> <h3 id=\"store-enabled-true\">store-enabled = true</h3> <p>Set to <code>false</code> to disable recording statistics internally. If set to <code>false</code> it will make it substantially more difficult to diagnose issues with your installation.</p> <h3 id=\"store-database-internal\">store-database = “_internal”</h3> <p>The destination database for recorded statistics.</p> <h3 id=\"store-interval-10s\">store-interval = “10s”</h3> <p>The interval at which InfluxDB records statistics.</p> <h2 id=\"subscriber\">[subscriber]</h2> <h3 id=\"enabled-true-3\">enabled = true</h3> <h2 id=\"http\">[http]</h2> <p>This section controls how InfluxDB configures the HTTP endpoints. These are the primary mechanisms for getting data into and out of InfluxDB. Edit the options in this section to enable HTTPS and authentication. See <a href=\"../authentication_and_authorization/index\">Authentication and Authorization</a>.</p> <h3 id=\"enabled-true-4\">enabled = true</h3> <p>Set to <code>false</code> to disable HTTP. Note that the InfluxDB <a href=\"../../tools/shell/index\">command line interface (CLI)</a> connects to the database using the HTTP API.</p> <h3 id=\"bind-address-8086\">bind-address = “:8086”</h3> <p>The port used by the HTTP API.</p> <h3 id=\"auth-enabled-false\">auth-enabled = false</h3> <p>Set to <code>true</code> to require authentication.</p> <h3 id=\"log-enabled-true\">log-enabled = true</h3> <p>Set to <code>false</code> to disable logging.</p> <h3 id=\"write-tracing-false\">write-tracing = false</h3> <p>Set to <code>true</code> to enable logging for the write payload. If set to <code>true</code>, this will duplicate every write statement in the logs and is thus not recommended for general use.</p> <h3 id=\"pprof-enabled-false-1\">pprof-enabled = false</h3> <p>Set to <code>true</code> to enable <a href=\"http://blog.golang.org/profiling-go-programs\">pprof</a> on InfluxDB so that it gathers detailed performance information.</p> <h3 id=\"https-enabled-false-1\">https-enabled = false</h3> <p>Set to <code>true</code> to enable HTTPS.</p> <h3 id=\"https-certificate-etc-ssl-influxdb-pem-1\">https-certificate = “/etc/ssl/influxdb.pem”</h3> <p>The path of the certificate file.</p> <h3 id=\"max-row-limit-10000\">max-row-limit = 10000</h3> <h2 id=\"graphite\">[[graphite]]</h2> <p>This section controls one or many listeners for Graphite data. See the <a href=\"https://github.com/influxdb/influxdb/blob/master/services/graphite/README.md\">README</a> on GitHub for more information.</p> <h3 id=\"enabled-false\">enabled = false</h3> <p>Set to <code>true</code> to enable Graphite input.</p> <h3 id=\"bind-address-2003\">bind-address = “:2003”</h3> <p>The default port.</p> <h3 id=\"database-graphite\">database = “graphite”</h3> <p>The name of the database that you want to write to.</p> <h3 id=\"protocol-tcp\">protocol = “tcp”</h3> <p>Set to <code>tcp</code> or <code>udp</code>.</p> <p><em>The next three options control how batching works. You should have this enabled otherwise you could get dropped metrics or poor performance. Batching will buffer points in memory if you have many coming in.</em></p> <h3 id=\"batch-size-5000\">batch-size = 5000</h3> <p>The input will flush if this many points get buffered.</p> <h3 id=\"batch-pending-10\">batch-pending = 10</h3> <p>The number of batches that may be pending in memory.</p> <h3 id=\"batch-timeout-1s\">batch-timeout = “1s”</h3> <p>The input will flush at least this often even if it hasn’t reached the configured batch-size.</p> <h3 id=\"consistency-level-one\">consistency-level = “one”</h3> <p>The number of nodes that must confirm the write. If the requirement is not met the return value will be either <code>partial write</code> if some points in the batch fail or <code>write failure</code> if all points in the batch fail. For more information, see the Query String Parameters for Writes section in the <a href=\"../../write_protocols/write_syntax/index\">Line Protocol Syntax Reference </a>.</p> <h3 id=\"separator\">separator = “.”</h3> <p>This string joins multiple matching ‘measurement’ values providing more control over the final measurement name.</p> <h3 id=\"udp-read-buffer-0\">udp-read-buffer = 0</h3> <p>UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.</p> <h2 id=\"collectd\">[[collectd]]</h2> <p>This section controls the listener for collectd data. See the <a href=\"https://github.com/influxdata/influxdb/tree/master/services/collectd\">README</a> on Github for more information.</p> <h3 id=\"enabled-false-1\">enabled = false</h3> <p>Set to <code>true</code> to enable collectd writes.</p> <h3 id=\"bind-address-25826\">bind-address = “:25826”</h3> <p>The port.</p> <h3 id=\"database-collectd\">database = “collectd”</h3> <p>The name of the database that you want to write to. This defaults to <code>collectd</code>.</p> <p><em>The next three options control how batching works. You should have this enabled otherwise you could get dropped metrics or poor performance. Batching will buffer points in memory if you have many coming in.</em></p> <h3 id=\"batch-size-5000-1\">batch-size = 5000</h3> <p>The input will flush if this many points get buffered.</p> <h3 id=\"batch-pending-10-1\">batch-pending = 10</h3> <p>The number of batches that may be pending in memory.</p> <h3 id=\"batch-timeout-10s\">batch-timeout = “10s”</h3> <p>The input will flush at least this often even if it hasn’t reached the configured batch-size.</p> <h3 id=\"read-buffer-0\">read-buffer = 0</h3> <p>UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.</p> <h3 id=\"typesdb-usr-share-collectd-types-db\">typesdb = “/usr/share/collectd/types.db”</h3> <p>Defaults to <code>/usr/share/collectd/types.db</code>. A sample <code>types.db</code> file can be found <a href=\"https://github.com/collectd/collectd/blob/master/src/types.db\">here</a>.</p> <h2 id=\"opentsdb\">[[opentsdb]]</h2> <p>Controls the listener for OpenTSDB data. See the <a href=\"https://github.com/influxdb/influxdb/blob/master/services/opentsdb/README.md\">README</a> on GitHub for more information.</p> <h3 id=\"enabled-false-2\">enabled = false</h3> <p>Set to <code>true</code> to enable openTSDB writes.</p> <h3 id=\"bind-address-4242\">bind-address = “:4242”</h3> <p>The default port.</p> <h3 id=\"database-opentsdb\">database = “opentsdb”</h3> <p>The name of the database that you want to write to. If the database does not exist, it will be created automatically when the input is initialized.</p> <h3 id=\"retention-policy\">retention-policy = “”</h3> <p>The relevant retention policy. An empty string is equivalent to the database’s <code>DEFAULT</code> retention policy.</p> <h3 id=\"consistency-level-one-1\">consistency-level = “one”</h3> <h3 id=\"tls-enabled-false\">tls-enabled = false</h3> <h3 id=\"certificate-etc-ssl-influxdb-pem\">certificate = “/etc/ssl/influxdb.pem”</h3> <p><em>The next three options control how batching works. You should have this enabled otherwise you could get dropped metrics or poor performance. Only points metrics received over the telnet protocol undergo batching.</em></p> <h3 id=\"batch-size-1000\">batch-size = 1000</h3> <p>The input will flush if this many points get buffered.</p> <h3 id=\"batch-pending-5\">batch-pending = 5</h3> <p>The number of batches that may be pending in memory.</p> <h3 id=\"batch-timeout-1s-1\">batch-timeout = “1s”</h3> <p>The input will flush at least this often even if it hasn’t reached the configured batch-size.</p> <h3 id=\"log-point-errors-true\">log-point-errors = true</h3> <p>Log an error for every malformed point.</p> <h2 id=\"udp\">[[udp]]</h2> <p>This section controls the listeners for InfluxDB line protocol data via UDP. See the <a href=\"../../write_protocols/udp/index\">UDP page</a> for more information.</p> <h3 id=\"enabled-false-3\">enabled = false</h3> <p>Set to <code>true</code> to enable writes over UDP.</p> <h3 id=\"bind-address-8089\">bind-address = “:8089”</h3> <p>An empty string is equivalent to <code>0.0.0.0</code>.</p> <h3 id=\"database-udp\">database = “udp”</h3> <p>The name of the database that you want to write to.</p> <h3 id=\"retention-policy-1\">retention-policy = “”</h3> <p>The relevant retention policy for your data. An empty string is equivalent to the database’s <code>DEFAULT</code> retention policy.</p> <p><em>The next three options control how batching works. You should have this enabled otherwise you could get dropped metrics or poor performance. Batching will buffer points in memory if you have many coming in.</em></p> <h3 id=\"batch-size-5000-2\">batch-size = 5000</h3> <p>The input will flush if this many points get buffered.</p> <h3 id=\"batch-pending-10-2\">batch-pending = 10</h3> <p>The number of batches that may be pending in memory.</p> <h3 id=\"batch-timeout-1s-2\">batch-timeout = “1s”</h3> <p>The input will flush at least this often even if it hasn’t reached the configured batch-size.</p> <h3 id=\"read-buffer-0-1\">read-buffer = 0</h3> <p>UDP read buffer size, 0 means OS default. UDP listener will fail if set above OS max.</p> <h3 id=\"precision\">precision = “”</h3> <h2 id=\"continuous-queries\">[continuous_queries]</h2> <p>This section controls how <a href=\"../../concepts/glossary/index#continuous-query-cq\">continuous queries (CQs)</a> run within InfluxDB. CQs are automated batches of queries that execute over recent time intervals. InfluxDB executes one auto-generated query per <code>GROUP BY time()</code> interval.</p> <h3 id=\"log-enabled-true-1\">log-enabled = true</h3> <p>Set to <code>false</code> to disable logging for CQ events.</p> <h3 id=\"enabled-true-5\">enabled = true</h3> <p>Set to <code>false</code> to disable CQs.</p> <h3 id=\"run-interval-1s\">run-interval = “1s”</h3> <p>The interval at which InfluxDB checks to see if a CQ needs to run. Set this option to the lowest interval at which your CQs run. For example, if your most frequent CQ runs every minute, set <code>run-interval</code> to <code>1m</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/influxdb/v0.13/administration/config/\" class=\"_attribution-link\">https://docs.influxdata.com/influxdb/v0.13/administration/config/</a>\n  </p>\n</div>\n","kapacitor/v0.13/introduction/installation/index":"<h1>Installation</h1>     <p>This page provides directions for installing, starting, and configuring Kapacitor.</p> <h2 id=\"requirements\">Requirements</h2> <p>For packaged installations, root or sudo access may be required to complete the installation.</p> <h3 id=\"networking\">Networking</h3> <p>Kapacitor listens on TCP port <code>9092</code> for all API and write calls.</p> <p>Kapacitor may also bind to randomized UDP ports for handling of InfluxDB data via subscriptions.</p> <h2 id=\"installation\">Installation</h2> <p>Kapacitor has two binaries:</p> <ul> <li>kapacitor – a CLI program for calling the Kapacitor API.</li> <li>kapacitord – the Kapacitor server daemon.</li> </ul> <p>You can either download the binaries directly from the <a href=\"https://influxdata.com/downloads/#kapacitor\">downloads</a> page or by using the Go command <code>go get</code>:</p> <pre data-language=\"bash\">go get github.com/influxdb/kapacitor/cmd/kapacitor\ngo get github.com/influxdb/kapacitor/cmd/kapacitord\n</pre> <h3 id=\"start-the-kapacitor-service\">Start the Kapacitor Service</h3> <p>For packaged installations, please see the respective sections below for your operating system. For non-packaged installations (tarballs or from source), you will need to start the Kapacitor application manually by running:</p> <pre>./kapacitord -config &lt;PATH TO CONFIGURATION&gt;\n</pre> <h4 id=\"os-x-via-homebrew\">OS X (via Homebrew)</h4> <p>To have <code>launchd</code> start Kapacitor at login:</p> <pre>ln -sfv /usr/local/opt/kapacitor/*.plist ~/Library/LaunchAgents\n</pre> <p>Then to load Kapacitor now:</p> <pre>launchctl load ~/Library/LaunchAgents/homebrew.mxcl.kapacitor.plist\n</pre> <p>Or, if you don’t want/need <code>lanchctl</code>, you can just run:</p> <pre>kapacitord -config /usr/local/etc/kapacitor.conf\n</pre> <h4 id=\"linux-sysv-or-upstart-systems\">Linux - SysV or Upstart Systems</h4> <p>To start the Kapacitor service, run:</p> <pre>sudo service kapacitor start\n</pre> <h4 id=\"linux-systemd-systems\">Linux - systemd Systems</h4> <p>To start the Kapacitor service, run:</p> <pre>sudo systemctl start kapacitor\n</pre> <h2 id=\"configuration\">Configuration</h2> <p>An example configuration file can be found <a href=\"https://github.com/influxdb/kapacitor/blob/master/etc/kapacitor/kapacitor.conf\">here</a></p> <p>Kapacitor can also provide an example config for you using this command:</p> <pre data-language=\"bash\">kapacitord config\n</pre> <p>To generate a new configuration file, run:</p> <pre>kapacitord config &gt; kapacitor.generated.conf\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/introduction/installation/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/introduction/installation/</a>\n  </p>\n</div>\n","kapacitor/v0.13/tick/spec/index":"<h1>TICKscript Specification</h1>     <h2 id=\"introduction\">Introduction</h2> <p>The TICKscript language is an invocation chaining language used to define data processing pipelines.</p> <h2 id=\"notation\">Notation</h2> <p>The syntax is specified using Extended Backus-Naur Form (“EBNF”). EBNF is the same notation used in the <a href=\"http://golang.org/\">Go</a> programming language specification, which can be found <a href=\"https://golang.org/ref/spec\">here</a>.</p> <pre>Production  = production_name \"=\" [ Expression ] \".\" .\nExpression  = Alternative { \"|\" Alternative } .\nAlternative = Term { Term } .\nTerm        = production_name | token [ \"…\" token ] | Group | Option | Repetition .\nGroup       = \"(\" Expression \")\" .\nOption      = \"[\" Expression \"]\" .\nRepetition  = \"{\" Expression \"}\" .\n</pre> <p>Notation operators in order of increasing precedence:</p> <pre>|   alternation\n()  grouping\n[]  option (0 or 1 times)\n{}  repetition (0 to n times)\n</pre> <h2 id=\"grammar\">Grammar</h2> <p>The following is the EBNF grammar definition of TICKscript.</p> <pre>\nunicode_char        = (* an arbitrary Unicode code point except newline *) .\ndigit               = \"0\" … \"9\" .\nascii_letter        = \"A\" … \"Z\" | \"a\" … \"z\" .\nletter              = ascii_letter | \"_\" .\nidentifier          = ( letter ) { letter | digit } .\nboolean_lit         = \"TRUE\" | \"FALSE\" .\nint_lit             = \"1\" … \"9\" { digit }\nletter              = ascii_letter | \"_\" .\nnumber_lit          = digit { digit } { \".\" {digit} } .\nduration_lit        = int_lit duration_unit .\nduration_unit       = \"u\" | \"µ\" | \"ms\" | \"s\" | \"m\" | \"h\" | \"d\" | \"w\" .\nstring_lit          = `'` { unicode_char } `'` .\nstar_lit            = \"*\"\nregex_lit           = `/` { unicode_char } `/` .\n\noperator_lit       = \"+\" | \"-\" | \"*\" | \"/\" | \"==\" | \"!=\" |\n                     \"&lt;\" | \"&lt;=\" | \"&gt;\" | \"&gt;=\" | \"=~\" | \"!~\" |\n                     \"AND\" | \"OR\" .\n\nProgram      = Statement { Statement } .\nStatement    = Declaration | Expression .\nDeclaration  = \"var\" identifier \"=\" Expression .\nExpression   = identifier { Chain } | Function { Chain } | Primary .\nChain        = \"@\" Function | \"|\" Function { Chain } | \".\" Function { Chain} | \".\" identifier { Chain } .\nFunction     = identifier \"(\" Parameters \")\" .\nParameters   = { Parameter \",\" } [ Parameter ] .\nParameter    = Expression | \"lambda:\" LambdaExpr | Primary .\nPrimary      = \"(\" LambdaExpr \")\" | number_lit | string_lit |\n                boolean_lit | duration_lit | regex_lit | star_lit |\n                LFunc | identifier | Reference | \"-\" Primary | \"!\" Primary .\nReference    = `\"` { unicode_char } `\"` .\nLambdaExpr   =  Primary operator_lit Primary .\nLFunc        = identifier \"(\" LParameters \")\"\nLParameters  = { LParameter \",\" } [ LParameter ] .\nLParameter   = LambdaExpr |  Primary .\n\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/tick/spec/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/tick/spec/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/index":"<h1>TICKscript Node Overview</h1>     <blockquote> <p>Note: Before continuing, please make sure you have read the <a href=\"../tick/index\">TICKscript Language Specification</a>.</p> </blockquote> <p>Property methods modify the node they are called on and return a reference to the same node. The two most important property methods are:</p> <ul> <li><a href=\"batch_node/index\">Batch Node</a></li> <li><a href=\"stream_node/index\">Stream Node</a></li> </ul> <p>Which define the type of task that you are running (either <a href=\"../introduction/getting_started/index#trigger-alert-from-stream-data\">stream</a> or <a href=\"../introduction/getting_started/index#trigger-alert-from-batch-data\">batch</a>). The other available property methods are:</p> <ul> <li><a href=\"alert_node/index\">Alert Node</a></li> <li><a href=\"derivative_node/index\">Derivative Node</a></li> <li><a href=\"eval_node/index\">Eval Node</a></li> <li><a href=\"group_by_node/index\">Group By Node</a></li> <li><a href=\"http_out_node/index\">HTTP Output Node</a></li> <li><a href=\"influx_d_b_out_node/index\">InfluxDB Output Node</a></li> <li><a href=\"influx_q_l_node/index\">InfluxQL Node</a></li> <li><a href=\"join_node/index\">Join Node</a></li> <li><a href=\"log_node/index\">Log Node</a></li> <li><a href=\"no_op_node/index\">NoOp Node</a></li> <li><a href=\"sample_node/index\">Sample Node</a></li> <li><a href=\"shift_node/index\">Shift Node</a></li> <li><a href=\"source_batch_node/index\">Source Batch Node</a></li> <li><a href=\"source_stream_node/index\">Source Stream Node</a></li> <li><a href=\"stats_node/index\">Stats Node</a></li> <li><a href=\"u_d_f_node/index\">UDF (User Defined Function) Node</a></li> <li><a href=\"union_node/index\">Union Node</a></li> <li><a href=\"where_node/index\">Where Node</a></li> <li><a href=\"window_node/index\">Window Node</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/stream_node/index":"<h1>StreamNode</h1>     <p>A <a href=\"index\">StreamNode</a> represents the source of data being streamed to Kapacitor via any of its inputs. The <code>stream</code> variable in stream tasks is an instance of a <a href=\"index\">StreamNode.</a> <a href=\"index#from\">StreamNode.From</a> is the method/property of this node.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#from\">From</a></li> <li><a href=\"index#stats\">Stats</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"from\">From</h3> <p>Creates a new <a href=\"../from_node/index\">FromNode</a> that can be further filtered using the Database, RetentionPolicy, Measurement and Where properties. From can be called multiple times to create multiple independent forks of the data stream.</p> <p>Example:</p> <pre data-language=\"javascript\">    // Select the 'cpu' measurement from just the database 'mydb'\n    // and retention policy 'myrp'.\n    var cpu = stream\n        |from()\n            .database('mydb')\n            .retentionPolicy('myrp')\n            .measurement('cpu')\n    // Select the 'load' measurement from any database and retention policy.\n    var load = stream\n        |from()\n            .measurement('load')\n    // Join cpu and load streams and do further processing.\n    cpu\n        |join(load)\n            .as('cpu', 'load')\n        ...\n</pre> <pre data-language=\"javascript\">node|from()\n</pre> <p>Returns: <a href=\"../from_node/index\">FromNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/stream_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/stream_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/batch_node/index":"<h1>BatchNode</h1>     <p>A node that handles creating several child QueryNodes. Each call to <code>query</code> creates a child batch node that can further be configured. See <a href=\"../query_node/index\">QueryNode</a> The <code>batch</code> variable in batch tasks is an instance of a <a href=\"index\">BatchNode.</a></p> <p>Example:</p> <pre data-language=\"javascript\">     var errors = batch\n                      |query('SELECT value from errors')\n                      ...\n     var views = batch\n                      |query('SELECT value from views')\n                      ...\n</pre> <p>Available Statistics:</p> <ul> <li>query_errors – number of errors when querying</li> <li>connect_errors – number of errors connecting to InfluxDB</li> <li>batches_queried – number of batches returned from queries</li> <li>points_queried – total number of points in batches</li> </ul> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#query\">Query</a></li> <li><a href=\"index#stats\">Stats</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"query\">Query</h3> <p>The query to execute. Must not contain a time condition in the <code>WHERE</code> clause or contain a <code>GROUP BY</code> clause. The time conditions are added dynamically according to the period, offset and schedule. The <code>GROUP BY</code> clause is added dynamically according to the dimensions passed to the <code>groupBy</code> method.</p> <pre data-language=\"javascript\">node|query(q string)\n</pre> <p>Returns: <a href=\"../query_node/index\">QueryNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/batch_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/batch_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/eval_node/index":"<h1>EvalNode</h1>     <p>Evaluates expressions on each data point it receives. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions. See the property <a href=\"index#as\">EvalNode.As</a> for details on how to reference the results.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |eval(lambda: \"error_count\" / \"total_count\")\n          .as('error_percent')\n</pre> <p>The above example will add a new field <code>error_percent</code> to each data point with the result of <code>error_count / total_count</code> where <code>error_count</code> and <code>total_count</code> are existing fields on the data point.</p> <p>Available Statistics:</p> <ul> <li>eval_errors – number of errors evaluating any expressions.</li> </ul> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#as\">As</a></li> <li><a href=\"index#keep\">Keep</a></li> <li><a href=\"index#quiet\">Quiet</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"as\">As</h3> <p>List of names for each expression. The expressions are evaluated in order and the result of a previous expression will be available in later expressions via the name provided.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |eval(lambda: \"value\" * \"value\", lambda: 1.0 / \"value2\")\n            .as('value2', 'inv_value2')\n</pre> <p>The above example calculates two fields from the value and names them <code>value2</code> and <code>inv_value2</code> respectively.</p> <pre data-language=\"javascript\">node.as(names ...string)\n</pre> <h3 id=\"keep\">Keep</h3> <p>If called the existing fields will be preserved in addition to the new fields being set. If not called then only new fields are preserved.</p> <p>Optionally intermediate values can be discarded by passing a list of field names. Only fields in the list will be kept. If no list is given then all fields, new and old, are kept.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |eval(lambda: \"value\" * \"value\", lambda: 1.0 / \"value2\")\n            .as('value2', 'inv_value2')\n            .keep('value', 'inv_value2')\n</pre> <p>In the above example the original field <code>value</code> is preserved. In addition the new field <code>value2</code> is calculated and used in evaluating <code>inv_value2</code> but is discarded before the point is sent on to children nodes. The resulting point has only two fields <code>value</code> and <code>inv_value2</code>.</p> <pre data-language=\"javascript\">node.keep(fields ...string)\n</pre> <h3 id=\"quiet\">Quiet</h3> <p>Suppress errors during evaluation.</p> <pre data-language=\"javascript\">node.quiet()\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/eval_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/eval_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/group_by_node/index":"<h1>GroupByNode</h1>     <p>A <a href=\"index\">GroupByNode</a> will group the incoming data. Each group is then processed independently for the rest of the pipeline. Only tags that are dimensions in the grouping will be preserved; all other tags are dropped.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |groupBy('service', 'datacenter')\n        ...\n</pre> <p>The above example groups the data along two dimensions <code>service</code> and <code>datacenter</code>. Groups are dynamically created as new data arrives and each group is processed independently.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/group_by_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/group_by_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/influx_q_l_node/index":"<h1>InfluxQLNode</h1>     <p>An <a href=\"index\">InfluxQLNode</a> performs the available function from the InfluxQL language. These function can be performed on a stream or batch edge. The resulting edge is dependent on the function. For a stream edge all points with the same time are accumulated into the function. For a batch edge all points in the batch are accumulated into the function.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |window()\n            .period(10s)\n            .every(10s)\n        // Sum the values for each 10s window of data.\n        |sum('value')\n</pre> <p>Note: Derivative has its own implementation as a <a href=\"../derivative_node/index\">DerivativeNode</a> instead of as part of the InfluxQL functions.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#as\">As</a></li> <li><a href=\"index#usepointtimes\">UsePointTimes</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"as\">As</h3> <p>The name of the field, defaults to the name of function used (i.e. .mean -&gt; 'mean')</p> <pre data-language=\"javascript\">node.as(value string)\n</pre> <h3 id=\"usepointtimes\">UsePointTimes</h3> <p>Use the time of the selected point instead of the time of the batch.</p> <p>Only applies to selector functions like first, last, top, bottom, etc. Aggregation functions always use the batch time.</p> <pre data-language=\"javascript\">node.usePointTimes()\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/influx_q_l_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/influx_q_l_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/tick/expr/index":"<h1>Lambda Expressions</h1>     <p>TICKscript uses lambda expressions to define transformations on data points as well as define boolean conditions that act as filters. TICKscript tries to be similar to InfluxQL in that most expressions that you would use in an InfluxQL <code>WHERE</code> clause will work as expressions in TICKscript. There are few exceptions:</p> <ul> <li>All field or tag identifiers must be double quoted.</li> <li>The comparison operator for equality is <code>==</code> not <code>=</code>.</li> </ul> <p>All expressions in TICKscript begin with the <code>lambda:</code> keyword.</p> <pre data-language=\"javascript\">.where(lambda: \"host\" == 'server001.example.com')\n</pre> <h2 id=\"stateful\">Stateful</h2> <p>These lambda expressions are stateful, meaning that each time they are evaluated internal state can change and will persist until the next evaluation. This may seem odd as part of an expression language but it has a powerful use case. You can define a function within the language that is essentially a online/streaming algorithm and with each call the function state is updated. For example the built-in function <code>sigma</code> that calculates a running mean and standard deviation and returns the number of standard deviations the current data point is away from the mean.</p> <p>Example:</p> <pre data-language=\"javascript\">sigma(\"value\") &gt; 3\n</pre> <p>Each time that the expression is evaluated the new value it updates the running statistics and then returns the deviation. This simple expression evaluates to <code>false</code> while the stream of data points it has received remains within <code>3</code> standard deviations of the running mean. As soon as a value is processed that is more than 3 standard deviation it evaluates to <code>true</code>. Now you can use that expression inside of a TICKscript to define powerful alerts.</p> <p>TICKscript with lambda expression:</p> <pre data-language=\"javascript\">stream\n    |alert()\n        // use an expression to define when an alert should go critical.\n        .crit(lambda: sigma(\"value\") &gt; 3)\n</pre> <h2 id=\"builtin-functions\">Builtin Functions</h2> <h3 id=\"bool\">Bool</h3> <p>Converts a string into a boolean via Go’s <a href=\"https://golang.org/pkg/strconv/#ParseBool\">strconv.ParseBool</a></p> <pre data-language=\"javascript\">bool(value string) bool\n</pre> <h3 id=\"int\">Int</h3> <p>Converts a string or float64 into an int64 via Go’s <a href=\"https://golang.org/pkg/strconv/#ParseInt\">strconv.ParseInt</a> or simple <code>float64()</code> coercion. Strings are assumed to be decimal numbers.</p> <pre data-language=\"javascript\">int(value float64 or string) int64\n</pre> <h3 id=\"float\">Float</h3> <p>Converts a string or int64 into an float64 via Go’s <a href=\"https://golang.org/pkg/strconv/#ParseInt\">strconv.ParseFloat</a> or simple <code>int64()</code> coercion.</p> <pre data-language=\"javascript\">float(value int64 or string) float64\n</pre> <h3 id=\"sigma\">Sigma</h3> <p>Computes the number of standard deviations a given value is away from the running mean. Each time the expression is evaluated the running mean and standard deviation are updated.</p> <pre data-language=\"javascript\">sigma(value float64) float64\n</pre> <h3 id=\"count\">Count</h3> <p>Count takes no arguments but returns the number of times the expression has been evaluated.</p> <pre data-language=\"javascript\">count() int64\n</pre> <h3 id=\"time-functions\">Time functions</h3> <p>Within each expression the <code>time</code> field contains the time of the current data point. The following functions can be used on the <code>time</code> field. Each function returns an int64.</p> <table> <thead> <tr> <th>Function</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>minute</td> <td>the minute within the hour: range [0,59]</td> </tr> <tr> <td>hour</td> <td>the hour within the day: range [0,23]</td> </tr> <tr> <td>weekday</td> <td>the weekday within the week: range [0,6] 0 is Sunday</td> </tr> <tr> <td>day</td> <td>the day within the month: range [1,31]</td> </tr> <tr> <td>month</td> <td>the month within the year: range [1,12]</td> </tr> <tr> <td>year</td> <td>the year</td> </tr> </tbody> </table> <p>Example usage:</p> <pre data-language=\"javascript\">lambda: hour(\"time\") == 9\n</pre> <p>The above expression evaluates to <code>true</code> if the hour of the day for the data point is 9 AM, using local time.</p> <h3 id=\"math-functions\">Math functions</h3> <p>The following mathematical functions are available. Each function is implemented via the equivalent Go function. Short descriptions are provided here but see the Go <a href=\"https://golang.org/pkg/math/\">docs</a> for more details.</p> <table> <thead> <tr> <th>Function</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><a href=\"https://golang.org/pkg/math/#Abs\">abs</a></td> <td>Abs returns the absolute value of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Acos\">acos</a></td> <td>Acos returns the arccosine, in radians, of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Acosh\">acosh</a></td> <td>Acosh returns the inverse hyperbolic cosine of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Asin\">asin</a></td> <td>Asin returns the arcsine, in radians, of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Asinh\">asinh</a></td> <td>Asinh returns the inverse hyperbolic sine of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Atan\">atan</a></td> <td>Atan returns the arctangent, in radians, of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Atan2\">atan2</a></td> <td>Atan2 returns the arc tangent of y/x, using the signs of the two to determine the quadrant of the return value.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Atanh\">atanh</a></td> <td>Atanh returns the inverse hyperbolic tangent of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Cbrt\">cbrt</a></td> <td>Cbrt returns the cube root of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Ceil\">ceil</a></td> <td>Ceil returns the least integer value greater than or equal to x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Cos\">cos</a></td> <td>Cos returns the cosine of the radian argument x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Cosh\">cosh</a></td> <td>Cosh returns the hyperbolic cosine of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Erf\">erf</a></td> <td>Erf returns the error function of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Erfc\">erfc</a></td> <td>Erfc returns the complementary error function of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Exp\">exp</a></td> <td>Exp returns e**x, the base-e exponential of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Exp2\">exp2</a></td> <td>Exp2 returns 2**x, the base-2 exponential of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Expm1\">expm1</a></td> <td>Expm1 returns e**x - 1, the base-e exponential of x minus 1. It is more accurate than Exp(x) - 1 when x is near zero.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Floor\">floor</a></td> <td>Floor returns the greatest integer value less than or equal to x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Gamma\">gamma</a></td> <td>Gamma returns the Gamma function of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Hypot\">hypot</a></td> <td>Hypot returns Sqrt(p*p + q*q), taking care to avoid unnecessary overflow and underflow.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#J0\">j0</a></td> <td>J0 returns the order-zero Bessel function of the first kind.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#J1\">j1</a></td> <td>J1 returns the order-one Bessel function of the first kind.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Jn\">jn</a></td> <td>Jn returns the order-n Bessel function of the first kind.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Log\">log</a></td> <td>Log returns the natural logarithm of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Log10\">log10</a></td> <td>Log10 returns the decimal logarithm of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Log1p\">log1p</a></td> <td>Log1p returns the natural logarithm of 1 plus its argument x. It is more accurate than Log(1 + x) when x is near zero.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Log2\">log2</a></td> <td>Log2 returns the binary logarithm of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Logb\">logb</a></td> <td>Logb returns the binary exponent of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Max\">max</a></td> <td>Max returns the larger of x or y.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Min\">min</a></td> <td>Min returns the smaller of x or y.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Mod\">mod</a></td> <td>Mod returns the floating-point remainder of x/y. The magnitude of the result is less than y and its sign agrees with that of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Pow\">pow</a></td> <td>Pow returns x**y, the base-x exponential of y.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Pow10\">pow10</a></td> <td>Pow10 returns 10**e, the base-10 exponential of e.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Sin\">sin</a></td> <td>Sin returns the sine of the radian argument x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Sinh\">sinh</a></td> <td>Sinh returns the hyperbolic sine of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Sqrt\">sqrt</a></td> <td>Sqrt returns the square root of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Tan\">tan</a></td> <td>Tan returns the tangent of the radian argument x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Tanh\">tanh</a></td> <td>Tanh returns the hyperbolic tangent of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Trunc\">trunc</a></td> <td>Trunc returns the integer value of x.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Y0\">y0</a></td> <td>Y0 returns the order-zero Bessel function of the second kind.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Y1\">y1</a></td> <td>Y1 returns the order-one Bessel function of the second kind.</td> </tr> <tr> <td><a href=\"https://golang.org/pkg/math/#Yn\">yn</a></td> <td>Yn returns the order-n Bessel function of the second kind.</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/tick/expr/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/tick/expr/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/default_node/index":"<h1>DefaultNode</h1>     <p>Defaults fields and tags on data points.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |default()\n            .field('value', 0.0)\n            .tag('host', '')\n</pre> <p>The above example will set the field <code>value</code> to float64(0) if it does not already exist It will also set the tag <code>host</code> to string(\"\") if it does not already exist.</p> <p>Available Statistics:</p> <ul> <li>fields_defaulted – number of fields that were missing</li> <li>tags_defaulted – number of tags that were missing</li> </ul> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#field\">Field</a></li> <li><a href=\"index#tag\">Tag</a></li> <li><a href=\"index#tags\">Tags</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"field\">Field</h3> <p>Define a field default.</p> <pre data-language=\"javascript\">node.field(name string, value interface{})\n</pre> <h3 id=\"tag\">Tag</h3> <p>Define a tag default.</p> <pre data-language=\"javascript\">node.tag(name string, value string)\n</pre> <h3 id=\"tags\">Tags</h3> <p>Set of tags to default</p> <pre data-language=\"javascript\">node.tags(value map[string]string)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/default_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/default_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/derivative_node/index":"<h1>DerivativeNode</h1>     <p>Compute the derivative of a stream or batch. The derivative is computed on a single field and behaves similarly to the InfluxQL derivative function. Deriviative is not a MapReduce function and as a result is not part of the normal influxql functions.</p> <p>Example:</p> <pre data-language=\"javascript\">     stream\n         |from()\n             .measurement('net_rx_packets')\n         |derivative('value')\n            .unit(1s) // default\n            .nonNegative()\n         ...\n</pre> <p>Computes the derivative via: (current - previous ) / ( time_difference / unit)</p> <p>For batch edges the derivative is computed for each point in the batch and because of boundary conditions the number of points is reduced by one.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#as\">As</a></li> <li><a href=\"index#nonnegative\">NonNegative</a></li> <li><a href=\"index#unit\">Unit</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"as\">As</h3> <p>The new name of the derivative field. Default is the name of the field used when calculating the derivative.</p> <pre data-language=\"javascript\">node.as(value string)\n</pre> <h3 id=\"nonnegative\">NonNegative</h3> <p>If called the derivative will skip negative results.</p> <pre data-language=\"javascript\">node.nonNegative()\n</pre> <h3 id=\"unit\">Unit</h3> <p>The time unit of the resulting derivative value. Default: 1s</p> <pre data-language=\"javascript\">node.unit(value time.Duration)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/derivative_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/derivative_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/http_out_node/index":"<h1>HTTPOutNode</h1>     <p>An <a href=\"index\">HTTPOutNode</a> caches the most recent data for each group it has received.</p> <p>The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/kapacitor/v1/tasks/&lt;task_id&gt;\" and endpoint is \"top10\", then the data can be requested from \"/kapacitor/v1/tasks/&lt;task_id&gt;/top10\".</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |window()\n            .period(10s)\n            .every(5s)\n        |top('value', 10)\n        //Publish the top 10 results over the last 10s updated every 5s.\n        |httpOut('top10')\n</pre> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/http_out_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/http_out_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/influx_d_b_out_node/index":"<h1>InfluxDBOutNode</h1>     <p>Writes the data to InfluxDB as it is received.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |eval(lambda: \"errors\" / \"total\")\n            .as('error_percent')\n        // Write the transformed data to InfluxDB\n        |influxDBOut()\n            .database('mydb')\n            .retentionPolicy('myrp')\n            .measurement('errors')\n            .tag('kapacitor', 'true')\n            .tag('version', '0.2')\n</pre> <p>Available Statistics:</p> <ul> <li>points_written – number of points written to InfluxDB</li> <li>write_errors – number of errors attempting to write to InfluxDB</li> </ul> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#buffer\">Buffer</a></li> <li><a href=\"index#cluster\">Cluster</a></li> <li><a href=\"index#database\">Database</a></li> <li><a href=\"index#flushinterval\">FlushInterval</a></li> <li><a href=\"index#measurement\">Measurement</a></li> <li><a href=\"index#precision\">Precision</a></li> <li><a href=\"index#retentionpolicy\">RetentionPolicy</a></li> <li><a href=\"index#tag\">Tag</a></li> <li><a href=\"index#writeconsistency\">WriteConsistency</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#stats\">Stats</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"buffer\">Buffer</h3> <p>Number of points to buffer when writing to InfluxDB. Default: 1000</p> <pre data-language=\"javascript\">node.buffer(value int64)\n</pre> <h3 id=\"cluster\">Cluster</h3> <p>The name of the InfluxDB instance to connect to. If empty the configured default will be used.</p> <pre data-language=\"javascript\">node.cluster(value string)\n</pre> <h3 id=\"database\">Database</h3> <p>The name of the database.</p> <pre data-language=\"javascript\">node.database(value string)\n</pre> <h3 id=\"flushinterval\">FlushInterval</h3> <p>Write points to InfluxDB after interval even if buffer is not full. Default: 10s</p> <pre data-language=\"javascript\">node.flushInterval(value time.Duration)\n</pre> <h3 id=\"measurement\">Measurement</h3> <p>The name of the measurement.</p> <pre data-language=\"javascript\">node.measurement(value string)\n</pre> <h3 id=\"precision\">Precision</h3> <p>The precision to use when writing the data.</p> <pre data-language=\"javascript\">node.precision(value string)\n</pre> <h3 id=\"retentionpolicy\">RetentionPolicy</h3> <p>The name of the retention policy.</p> <pre data-language=\"javascript\">node.retentionPolicy(value string)\n</pre> <h3 id=\"tag\">Tag</h3> <p>Add a static tag to all data points. Tag can be called more than once.</p> <pre data-language=\"javascript\">node.tag(key string, value string)\n</pre> <h3 id=\"writeconsistency\">WriteConsistency</h3> <p>The write consistency to use when writing the data.</p> <pre data-language=\"javascript\">node.writeConsistency(value string)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/influx_d_b_out_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/influx_d_b_out_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/no_op_node/index":"<h1>NoOpNode</h1>     <p>A node that does not perform any operation.</p> <p><em>Do not use this node in a TICKscript there should be no need for it.</em></p> <p>If a node does not have any children, then its emitted count remains zero. Using a <a href=\"index\">NoOpNode</a> is a work around so that statistics are accurately reported for nodes with no real children. A <a href=\"index\">NoOpNode</a> is automatically appended to any node that is a source for a <a href=\"../stats_node/index\">StatsNode</a> and does not have any children.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/no_op_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/no_op_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/alert_node/index":"<h1>AlertNode</h1>     <p>An <a href=\"index\">AlertNode</a> can trigger an event of varying severity levels, and pass the event to alert handlers. The criteria for triggering an alert is specified via a <a href=\"https://docs.influxdata.com/kapacitor/latest/tick/expr/\">lambda expression</a>. See <a href=\"index#info\">AlertNode.Info,</a> <a href=\"index#warn\">AlertNode.Warn,</a> and <a href=\"index#crit\">AlertNode.Crit</a> below.</p> <p>Different event handlers can be configured for each <a href=\"index\">AlertNode.</a> Some handlers like Email, HipChat, Sensu, Slack, OpsGenie, VictorOps, PagerDuty and Talk have a configuration option 'global' that indicates that all alerts implicitly use the handler.</p> <p>Available event handlers:</p> <ul> <li>log – log alert data to file.</li> <li>post – HTTP POST data to a specified URL.</li> <li>email – Send and email with alert data.</li> <li>exec – Execute a command passing alert data over STDIN.</li> <li>HipChat – Post alert message to HipChat room.</li> <li>Alerta – Post alert message to Alerta.</li> <li>Sensu – Post alert message to Sensu client.</li> <li>Slack – Post alert message to Slack channel.</li> <li>OpsGenie – Send alert to OpsGenie.</li> <li>VictorOps – Send alert to VictorOps.</li> <li>PagerDuty – Send alert to PagerDuty.</li> <li>Talk – Post alert message to Talk client.</li> </ul> <p>See below for more details on configuring each handler.</p> <p>Each event that gets sent to a handler contains the following alert data:</p> <ul> <li>ID – the ID of the alert, user defined.</li> <li>Message – the alert message, user defined.</li> <li>Details – the alert details, user defined HTML content.</li> <li>Time – the time the alert occurred.</li> <li>Duration – the duration of the alert in nanoseconds.</li> <li>Level – one of OK, INFO, WARNING or CRITICAL.</li> <li>Data – influxql.Result containing the data that triggered the alert.</li> </ul> <p>Events are sent to handlers if the alert is in a state other than 'OK' or the alert just changed to the 'OK' state from a non 'OK' state (a.k.a. the alert recovered). Using the <a href=\"index#statechangesonly\">AlertNode.StateChangesOnly</a> property events will only be sent to handlers if the alert changed state.</p> <p>It is valid to configure multiple alert handlers, even with the same type.</p> <p>Example:</p> <pre data-language=\"javascript\">   stream\n           .groupBy('service')\n       |alert()\n           .id('kapacitor/{{ index .Tags \"service\" }}')\n           .message('{{ .ID }} is {{ .Level }} value:{{ index .Fields \"value\" }}')\n           .info(lambda: \"value\" &gt; 10)\n           .warn(lambda: \"value\" &gt; 20)\n           .crit(lambda: \"value\" &gt; 30)\n           .post(\"http://example.com/api/alert\")\n           .post(\"http://another.example.com/api/alert\")\n           .email().to('oncall@example.com')\n</pre> <p>It is assumed that each successive level filters a subset of the previous level. As a result, the filter will only be applied if a data point passed the previous level. In the above example, if value = 15 then the INFO and WARNING expressions would be evaluated, but not the CRITICAL expression. Each expression maintains its own state.</p> <p>Available Statistics:</p> <ul> <li>alerts_triggered – Total number of alerts triggered</li> <li>oks_triggered – Number of OK alerts triggered</li> <li>infos_triggered – Number of Info alerts triggered</li> <li>warns_triggered – Number of Warn alerts triggered</li> <li>crits_triggered – Number of Crit alerts triggered</li> </ul> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#alerta\">Alerta</a></li> <li><a href=\"index#all\">All</a></li> <li><a href=\"index#crit\">Crit</a></li> <li><a href=\"index#details\">Details</a></li> <li><a href=\"index#durationfield\">DurationField</a></li> <li><a href=\"index#email\">Email</a></li> <li><a href=\"index#exec\">Exec</a></li> <li><a href=\"index#flapping\">Flapping</a></li> <li><a href=\"index#hipchat\">HipChat</a></li> <li><a href=\"index#history\">History</a></li> <li><a href=\"index#id\">Id</a></li> <li><a href=\"index#idfield\">IdField</a></li> <li><a href=\"index#idtag\">IdTag</a></li> <li><a href=\"index#info\">Info</a></li> <li><a href=\"index#levelfield\">LevelField</a></li> <li><a href=\"index#leveltag\">LevelTag</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#message\">Message</a></li> <li><a href=\"index#opsgenie\">OpsGenie</a></li> <li><a href=\"index#pagerduty\">PagerDuty</a></li> <li><a href=\"index#post\">Post</a></li> <li><a href=\"index#sensu\">Sensu</a></li> <li><a href=\"index#slack\">Slack</a></li> <li><a href=\"index#statechangesonly\">StateChangesOnly</a></li> <li><a href=\"index#talk\">Talk</a></li> <li><a href=\"index#victorops\">VictorOps</a></li> <li><a href=\"index#warn\">Warn</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"alerta\">Alerta</h3> <p>Send the alert to Alerta.</p> <p>Example:</p> <pre data-language=\"javascript\">    [alerta]\n      enabled = true\n      url = \"https://alerta.yourdomain\"\n      token = \"9hiWoDOZ9IbmHsOTeST123ABciWTIqXQVFDo63h9\"\n      environment = \"Production\"\n      origin = \"Kapacitor\"\n</pre> <p>In order to not post a message every alert interval use <a href=\"index#statechangesonly\">AlertNode.StateChangesOnly</a> so that only events where the alert changed state are sent to Alerta.</p> <p>Send alerts to Alerta. The resource and event properties are required.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .alerta()\n                 .resource('Hostname or service')\n                 .event('Something went wrong')\n</pre> <p>Alerta also accepts optional alert information.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .alerta()\n                 .resource('Hostname or service')\n                 .event('Something went wrong')\n                 .environment('Development')\n                 .group('Dev. Servers')\n</pre> <p>NOTE: Alerta cannot be configured globally because of its required properties.</p> <pre data-language=\"javascript\">node.alerta()\n</pre> <h4 id=\"alerta-environment\">Alerta Environment</h4> <p>Alerta environment. Can be a template and has access to the same data as the <a href=\"index#details\">AlertNode.Details</a> property. Defaut is set from the configuration.</p> <pre data-language=\"javascript\">node.alerta()\n      .environment(value string)\n</pre> <h4 id=\"alerta-event\">Alerta Event</h4> <p>Alerta event. Can be a template and has access to the same data as the idInfo property. Default: {{ .ID }}</p> <pre data-language=\"javascript\">node.alerta()\n      .event(value string)\n</pre> <h4 id=\"alerta-group\">Alerta Group</h4> <p>Alerta group. Can be a template and has access to the same data as the <a href=\"index#details\">AlertNode.Details</a> property. Default: {{ .Group }}</p> <pre data-language=\"javascript\">node.alerta()\n      .group(value string)\n</pre> <h4 id=\"alerta-origin\">Alerta Origin</h4> <p>Alerta origin. If empty uses the origin from the configuration.</p> <pre data-language=\"javascript\">node.alerta()\n      .origin(value string)\n</pre> <h4 id=\"alerta-resource\">Alerta Resource</h4> <p>Alerta resource. Can be a template and has access to the same data as the <a href=\"index#details\">AlertNode.Details</a> property. Default: {{ .Name }}</p> <pre data-language=\"javascript\">node.alerta()\n      .resource(value string)\n</pre> <h4 id=\"alerta-token\">Alerta Token</h4> <p>Alerta authentication token. If empty uses the token from the configuration.</p> <pre data-language=\"javascript\">node.alerta()\n      .token(value string)\n</pre> <h4 id=\"alerta-value\">Alerta Value</h4> <p>Alerta value. Can be a template and has access to the same data as the <a href=\"index#details\">AlertNode.Details</a> property. Default is an empty string.</p> <pre data-language=\"javascript\">node.alerta()\n      .value(value string)\n</pre> <h3 id=\"all\">All</h3> <p>Indicates an alert should trigger only if all points in a batch match the criteria Does not apply to stream alerts.</p> <pre data-language=\"javascript\">node.all()\n</pre> <h3 id=\"crit\">Crit</h3> <p>Filter expression for the CRITICAL alert level. An empty value indicates the level is invalid and is skipped.</p> <pre data-language=\"javascript\">node.crit(value tick.Node)\n</pre> <h3 id=\"details\">Details</h3> <p>Template for constructing a detailed HTML message for the alert. The same template data is available as the <a href=\"index#message\">AlertNode.Message</a> property, in addition to a Message field that contains the rendered Message value.</p> <p>The intent is that the Message property be a single line summary while the Details property is a more detailed message possibly spanning multiple lines, and containing HTML formatting.</p> <p>This template is rendered using the html/template package in Go so that safe and valid HTML can be generated.</p> <p>The <code>json</code> method is available within the template to convert any variable to a valid JSON string.</p> <p>Example:</p> <pre data-language=\"javascript\">    |alert()\n       .id('{{ .Name }}')\n       .details('''\n&lt;h1&gt;{{ .ID }}&lt;/h1&gt;\n&lt;b&gt;{{ .Message }}&lt;/b&gt;\nValue: {{ index .Fields \"value\" }}\n''')\n       .email()\n</pre> <p>Default: {{ json . }}</p> <pre data-language=\"javascript\">node.details(value string)\n</pre> <h3 id=\"durationfield\">DurationField</h3> <p>Optional field key to add the alert duration to the data. The duration is always in units of nanoseconds.</p> <pre data-language=\"javascript\">node.durationField(value string)\n</pre> <h3 id=\"email\">Email</h3> <p>Email the alert data.</p> <p>If the To list is empty, the To addresses from the configuration are used. The email subject is the <a href=\"index#message\">AlertNode.Message</a> property. The email body is the <a href=\"index#details\">AlertNode.Details</a> property. The emails are sent as HTML emails and so the body can contain html markup.</p> <p>If the 'smtp' section in the configuration has the option: global = true then all alerts are sent via email without the need to explicitly state it in the TICKscript.</p> <p>Example:</p> <pre data-language=\"javascript\">    |alert()\n       .id('{{ .Name }}')\n       // Email subject\n       .meassage('{{ .ID }}:{{ .Level }}')\n       //Email body as HTML\n       .details('''\n&lt;h1&gt;{{ .ID }}&lt;/h1&gt;\n&lt;b&gt;{{ .Message }}&lt;/b&gt;\nValue: {{ index .Fields \"value\" }}\n''')\n       .email()\n</pre> <p>Send an email with custom subject and body.</p> <p>Example:</p> <pre data-language=\"javascript\">     [smtp]\n       enabled = true\n       host = \"localhost\"\n       port = 25\n       username = \"\"\n       password = \"\"\n       from = \"kapacitor@example.com\"\n       to = [\"oncall@example.com\"]\n       # Set global to true so all alert trigger emails.\n       global = true\n       state-changes-only =  true\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n</pre> <p>Send email to 'oncall@example.com' from 'kapacitor@example.com'</p> <pre data-language=\"javascript\">node.email(to ...string)\n</pre> <h3 id=\"exec\">Exec</h3> <p>Execute a command whenever an alert is triggered and pass the alert data over STDIN in JSON format.</p> <pre data-language=\"javascript\">node.exec(executable string, args ...string)\n</pre> <h3 id=\"flapping\">Flapping</h3> <p>Perform flap detection on the alerts. The method used is similar method to Nagios: <a href=\"https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/3/en/flapping.html\">https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/3/en/flapping.html</a></p> <p>Each different alerting level is considered a different state. The low and high thresholds are inverted thresholds of a percentage of state changes. Meaning that if the percentage of state changes goes above the <code>high</code> threshold, the alert enters a flapping state. The alert remains in the flapping state until the percentage of state changes goes below the <code>low</code> threshold. Typical values are low: 0.25 and high: 0.5. The percentage values represent the number state changes over the total possible number of state changes. A percentage change of 0.5 means that the alert changed state in half of the recorded history, and remained the same in the other half of the history.</p> <pre data-language=\"javascript\">node.flapping(low float64, high float64)\n</pre> <h3 id=\"hipchat\">HipChat</h3> <p>If the 'hipchat' section in the configuration has the option: global = true then all alerts are sent to HipChat without the need to explicitly state it in the TICKscript.</p> <p>Example:</p> <pre data-language=\"javascript\">    [hipchat]\n      enabled = true\n      url = \"https://orgname.hipchat.com/v2/room\"\n      room = \"Test Room\"\n      token = \"9hiWoDOZ9IbmHsOTeST123ABciWTIqXQVFDo63h9\"\n      global = true\n      state-changes-only = true\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n</pre> <p>Send alert to HipChat using default room 'Test Room'.</p> <pre data-language=\"javascript\">node.hipChat()\n</pre> <h4 id=\"hipchat-room\">HipChat Room</h4> <p>HipChat room in which to post messages. If empty uses the channel from the configuration.</p> <pre data-language=\"javascript\">node.hipChat()\n      .room(value string)\n</pre> <h4 id=\"hipchat-token\">HipChat Token</h4> <p>HipChat authentication token. If empty uses the token from the configuration.</p> <pre data-language=\"javascript\">node.hipChat()\n      .token(value string)\n</pre> <h3 id=\"history\">History</h3> <p>Number of previous states to remember when computing flapping levels and checking for state changes. Minimum value is 2 in order to keep track of current and previous states.</p> <p>Default: 21</p> <pre data-language=\"javascript\">node.history(value int64)\n</pre> <h3 id=\"id\">Id</h3> <p>Template for constructing a unique ID for a given alert.</p> <p>Available template data:</p> <ul> <li>Name – Measurement name.</li> <li>TaskName – The name of the task</li> <li>Group – Concatenation of all group-by tags of the form [key=value,]+. If no groupBy is performed equal to literal 'nil'.</li> <li>Tags – Map of tags. Use '{{ index .Tags \"key\" }}' to get a specific tag value.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">   stream\n       |from()\n           .measurement('cpu')\n           .groupBy('cpu')\n       |alert()\n           .id('kapacitor/{{ .Name }}/{{ .Group }}')\n</pre> <p>ID: kapacitor/cpu/cpu=cpu0,</p> <p>Example:</p> <pre data-language=\"javascript\">   stream\n       |from()\n           .measurement('cpu')\n           .groupBy('service')\n       |alert()\n           .id('kapacitor/{{ index .Tags \"service\" }}')\n</pre> <p>ID: kapacitor/authentication</p> <p>Example:</p> <pre data-language=\"javascript\">   stream\n       |from()\n           .measurement('cpu')\n           .groupBy('service', 'host')\n       |alert()\n           .id('kapacitor/{{ index .Tags \"service\" }}/{{ index .Tags \"host\" }}')\n</pre> <p>ID: kapacitor/authentication/auth001.example.com</p> <p>Default: {{ .Name }}:{{ .Group }}</p> <pre data-language=\"javascript\">node.id(value string)\n</pre> <h3 id=\"idfield\">IdField</h3> <p>Optional field key to add to the data, containing the alert ID as a string.</p> <pre data-language=\"javascript\">node.idField(value string)\n</pre> <h3 id=\"idtag\">IdTag</h3> <p>Optional tag key to use when tagging the data with the alert ID.</p> <pre data-language=\"javascript\">node.idTag(value string)\n</pre> <h3 id=\"info\">Info</h3> <p>Filter expression for the INFO alert level. An empty value indicates the level is invalid and is skipped.</p> <pre data-language=\"javascript\">node.info(value tick.Node)\n</pre> <h3 id=\"levelfield\">LevelField</h3> <p>Optional field key to add to the data, containing the alert level as a string.</p> <pre data-language=\"javascript\">node.levelField(value string)\n</pre> <h3 id=\"leveltag\">LevelTag</h3> <p>Optional tag key to use when tagging the data with the alert level.</p> <pre data-language=\"javascript\">node.levelTag(value string)\n</pre> <h3 id=\"log\">Log</h3> <p>Log JSON alert data to file. One event per line. Must specify the absolute path to the log file. It will be created if it does not exist. Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .log('/tmp/alert')\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .log('/tmp/alert')\n             .mode(0644)\n</pre> <pre data-language=\"javascript\">node.log(filepath string)\n</pre> <h4 id=\"log-mode\">Log Mode</h4> <p>File's mode and permissions, default is 0600</p> <pre data-language=\"javascript\">node.log(filepath string)\n      .mode(value int64)\n</pre> <h3 id=\"message\">Message</h3> <p>Template for constructing a meaningful message for the alert.</p> <p>Available template data:</p> <ul> <li>ID – The ID of the alert.</li> <li>Name – Measurement name.</li> <li>TaskName – The name of the task</li> <li>Group – Concatenation of all group-by tags of the form [key=value,]+. If no groupBy is performed equal to literal 'nil'.</li> <li>Tags – Map of tags. Use '{{ index .Tags \"key\" }}' to get a specific tag value.</li> <li>Level – Alert Level, one of: INFO, WARNING, CRITICAL.</li> <li>Fields – Map of fields. Use '{{ index .Fields \"key\" }}' to get a specific field value.</li> <li>Time – The time of the point that triggered the event.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">   stream\n       |from()\n           .measurement('cpu')\n           .groupBy('service', 'host')\n       |alert()\n           .id('{{ index .Tags \"service\" }}/{{ index .Tags \"host\" }}')\n           .message('{{ .ID }} is {{ .Level}} value: {{ index .Fields \"value\" }}')\n</pre> <p>Message: authentication/auth001.example.com is CRITICAL value:42</p> <p>Default: {{ .ID }} is {{ .Level }}</p> <pre data-language=\"javascript\">node.message(value string)\n</pre> <h3 id=\"opsgenie\">OpsGenie</h3> <p>Send alert to OpsGenie. To use OpsGenie alerting you must first enable the 'Alert Ingestion API' in the 'Integrations' section of OpsGenie. Then place the API key from the URL into the 'opsgenie' section of the Kapacitor configuration.</p> <p>Example:</p> <pre data-language=\"javascript\">    [opsgenie]\n      enabled = true\n      api-key = \"xxxxx\"\n      teams = [\"everyone\"]\n      recipients = [\"jim\", \"bob\"]\n</pre> <p>With the correct configuration you can now use OpsGenie in TICKscripts.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .opsGenie()\n</pre> <p>Send alerts to OpsGenie using the teams and recipients in the configuration file.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .opsGenie()\n             .teams('team_rocket','team_test')\n</pre> <p>Send alerts to OpsGenie with team set to 'team_rocket' and 'team_test'</p> <p>If the 'opsgenie' section in the configuration has the option: global = true then all alerts are sent to OpsGenie without the need to explicitly state it in the TICKscript.</p> <p>Example:</p> <pre data-language=\"javascript\">    [opsgenie]\n      enabled = true\n      api-key = \"xxxxx\"\n      recipients = [\"johndoe\"]\n      global = true\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n</pre> <p>Send alert to OpsGenie using the default recipients, found in the configuration.</p> <pre data-language=\"javascript\">node.opsGenie()\n</pre> <h4 id=\"opsgenie-recipients\">OpsGenie Recipients</h4> <p>The list of recipients to be alerted. If empty defaults to the recipients from the configuration.</p> <pre data-language=\"javascript\">node.opsGenie()\n      .recipients(recipients ...string)\n</pre> <h4 id=\"opsgenie-teams\">OpsGenie Teams</h4> <p>The list of teams to be alerted. If empty defaults to the teams from the configuration.</p> <pre data-language=\"javascript\">node.opsGenie()\n      .teams(teams ...string)\n</pre> <h3 id=\"pagerduty\">PagerDuty</h3> <p>Send the alert to PagerDuty. To use PagerDuty alerting you must first follow the steps to enable a new 'Generic API' service.</p> <p>From <a href=\"https://developer.pagerduty.com/documentation/integration/events\">https://developer.pagerduty.com/documentation/integration/events</a></p> <ol> <li>In your account, under the Services tab, click \"Add New Service\".</li> <li>Enter a name for the service and select an escalation policy. Then, select \"Generic API\" for the Service Type.</li> <li>Click the \"Add Service\" button.</li> <li>Once the service is created, you'll be taken to the service page. On this page, you'll see the \"Service key\", which is needed to access the API</li> </ol> <p>Place the 'service key' into the 'pagerduty' section of the Kapacitor configuration as the option 'service-key'.</p> <p>Example:</p> <pre data-language=\"javascript\">    [pagerduty]\n      enabled = true\n      service-key = \"xxxxxxxxx\"\n</pre> <p>With the correct configuration you can now use PagerDuty in TICKscripts.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .pagerDuty()\n</pre> <p>If the 'pagerduty' section in the configuration has the option: global = true then all alerts are sent to PagerDuty without the need to explicitly state it in the TICKscript.</p> <p>Example:</p> <pre data-language=\"javascript\">    [pagerduty]\n      enabled = true\n      service-key = \"xxxxxxxxx\"\n      global = true\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n</pre> <p>Send alert to PagerDuty.</p> <pre data-language=\"javascript\">node.pagerDuty()\n</pre> <h3 id=\"post\">Post</h3> <p>HTTP POST JSON alert data to a specified URL.</p> <pre data-language=\"javascript\">node.post(url string)\n</pre> <h3 id=\"sensu\">Sensu</h3> <p>Send the alert to Sensu.</p> <p>Example:</p> <pre data-language=\"javascript\">    [sensu]\n      enabled = true\n      url = \"http://sensu:3030\"\n      source = \"Kapacitor\"\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .sensu()\n</pre> <p>Send alerts to Sensu client.</p> <pre data-language=\"javascript\">node.sensu()\n</pre> <h3 id=\"slack\">Slack</h3> <p>Send the alert to Slack. To allow Kapacitor to post to Slack, go to the URL <a href=\"https://slack.com/services/new/incoming-webhook\">https://slack.com/services/new/incoming-webhook</a> and create a new incoming webhook and place the generated URL in the 'slack' configuration section.</p> <p>Example:</p> <pre data-language=\"javascript\">    [slack]\n      enabled = true\n      url = \"https://hooks.slack.com/services/xxxxxxxxx/xxxxxxxxx/xxxxxxxxxxxxxxxxxxxxxxxx\"\n      channel = \"#general\"\n</pre> <p>In order to not post a message every alert interval use <a href=\"index#statechangesonly\">AlertNode.StateChangesOnly</a> so that only events where the alert changed state are posted to the channel.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .slack()\n</pre> <p>Send alerts to Slack channel in the configuration file.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .slack()\n             .channel('#alerts')\n</pre> <p>Send alerts to Slack channel '#alerts'</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .slack()\n             .channel('@jsmith')\n</pre> <p>Send alert to user '@jsmith'</p> <p>If the 'slack' section in the configuration has the option: global = true then all alerts are sent to Slack without the need to explicitly state it in the TICKscript.</p> <p>Example:</p> <pre data-language=\"javascript\">    [slack]\n      enabled = true\n      url = \"https://hooks.slack.com/services/xxxxxxxxx/xxxxxxxxx/xxxxxxxxxxxxxxxxxxxxxxxx\"\n      channel = \"#general\"\n      global = true\n      state-changes-only = true\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n</pre> <p>Send alert to Slack using default channel '#general'.</p> <pre data-language=\"javascript\">node.slack()\n</pre> <h4 id=\"slack-channel\">Slack Channel</h4> <p>Slack channel in which to post messages. If empty uses the channel from the configuration.</p> <pre data-language=\"javascript\">node.slack()\n      .channel(value string)\n</pre> <h3 id=\"statechangesonly\">StateChangesOnly</h3> <p>Only sends events where the state changed. Each different alert level OK, INFO, WARNING, and CRITICAL are considered different states.</p> <p>Example:</p> <pre data-language=\"javascript\">   stream\n       |from()\n           .measurement('cpu')\n       |window()\n            .period(10s)\n            .every(10s)\n       |alert()\n           .crit(lambda: \"value\" &gt; 10)\n           .stateChangesOnly()\n           .slack()\n</pre> <p>If the \"value\" is greater than 10 for a total of 60s, then only two events will be sent. First, when the value crosses the threshold, and second, when it falls back into an OK state. Without stateChangesOnly, the alert would have triggered 7 times: 6 times for each 10s period where the condition was met and once more for the recovery.</p> <p>An optional maximum interval duration can be provided. An event will not be ignore (aka trigger an alert) if more than the maximum interval has elapsed since the last alert.</p> <p>Example:</p> <pre data-language=\"javascript\">   stream\n       |from()\n           .measurement('cpu')\n       |window()\n            .period(10s)\n            .every(10s)\n       |alert()\n           .crit(lambda: \"value\" &gt; 10)\n           .stateChangesOnly(10m)\n           .slack()\n</pre> <p>The abvove usage will only trigger alerts to slack on state changes or at least every 10 minutes.</p> <pre data-language=\"javascript\">node.stateChangesOnly(maxInterval ...time.Duration)\n</pre> <h3 id=\"talk\">Talk</h3> <p>Send the alert to Talk. To use Talk alerting you must first follow the steps to create a new incoming webhook.</p> <ol> <li>Go to the URL https:/account.jianliao.com/signin.</li> <li>Sign in with you account. under the Team tab, click \"Integrations\".</li> <li>Select \"Customize service\", click incoming Webhook \"Add\" button.</li> <li>After choose the topic to connect with \"xxx\", click \"Confirm Add\" button.</li> <li>Once the service is created, you'll see the \"Generate Webhook url\".</li> </ol> <p>Place the 'Generate Webhook url' into the 'Talk' section of the Kapacitor configuration as the option 'url'.</p> <p>Example:</p> <pre data-language=\"javascript\">    [talk]\n      enabled = true\n      url = \"https://jianliao.com/v2/services/webhook/uuid\"\n      author_name = \"Kapacitor\"\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .talk()\n</pre> <p>Send alerts to Talk client.</p> <pre data-language=\"javascript\">node.talk()\n</pre> <h3 id=\"victorops\">VictorOps</h3> <p>Send alert to VictorOps. To use VictorOps alerting you must first enable the 'Alert Ingestion API' in the 'Integrations' section of VictorOps. Then place the API key from the URL into the 'victorops' section of the Kapacitor configuration.</p> <p>Example:</p> <pre data-language=\"javascript\">    [victorops]\n      enabled = true\n      api-key = \"xxxxx\"\n      routing-key = \"everyone\"\n</pre> <p>With the correct configuration you can now use VictorOps in TICKscripts.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .victorOps()\n</pre> <p>Send alerts to VictorOps using the routing key in the configuration file.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n             .victorOps()\n             .routingKey('team_rocket')\n</pre> <p>Send alerts to VictorOps with routing key 'team_rocket'</p> <p>If the 'victorops' section in the configuration has the option: global = true then all alerts are sent to VictorOps without the need to explicitly state it in the TICKscript.</p> <p>Example:</p> <pre data-language=\"javascript\">    [victorops]\n      enabled = true\n      api-key = \"xxxxx\"\n      routing-key = \"everyone\"\n      global = true\n</pre> <p>Example:</p> <pre data-language=\"javascript\">    stream\n         |alert()\n</pre> <p>Send alert to VictorOps using the default routing key, found in the configuration.</p> <pre data-language=\"javascript\">node.victorOps()\n</pre> <h4 id=\"victorops-routingkey\">VictorOps RoutingKey</h4> <p>The routing key to use for the alert. Defaults to the value in the configuration if empty.</p> <pre data-language=\"javascript\">node.victorOps()\n      .routingKey(value string)\n</pre> <h3 id=\"warn\">Warn</h3> <p>Filter expression for the WARNING alert level. An empty value indicates the level is invalid and is skipped.</p> <pre data-language=\"javascript\">node.warn(value tick.Node)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/alert_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/alert_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/from_node/index":"<h1>FromNode</h1>     <p>A <a href=\"index\">FromNode</a> selects a subset of the data flowing through a <a href=\"../stream_node/index\">StreamNode.</a> The stream node allows you to select which portion of the stream you want to process.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |from()\n           .database('mydb')\n           .retentionPolicy('myrp')\n           .measurement('mymeasurement')\n           .where(lambda: \"host\" =~ /logger\\d+/)\n        |window()\n        ...\n</pre> <p>The above example selects only data points from the database <code>mydb</code> and retention policy <code>myrp</code> and measurement <code>mymeasurement</code> where the tag <code>host</code> matches the regex <code>logger\\d+</code></p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#database\">Database</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#measurement\">Measurement</a></li> <li><a href=\"index#retentionpolicy\">RetentionPolicy</a></li> <li><a href=\"index#truncate\">Truncate</a></li> <li><a href=\"index#where\">Where</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#from\">From</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"database\">Database</h3> <p>The database name. If empty any database will be used.</p> <pre data-language=\"javascript\">node.database(value string)\n</pre> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">  stream\n      |from()\n          .groupBy(*)\n</pre> <pre data-language=\"javascript\">node.groupBy(tag ...interface{})\n</pre> <h3 id=\"measurement\">Measurement</h3> <p>The measurement name If empty any measurement will be used.</p> <pre data-language=\"javascript\">node.measurement(value string)\n</pre> <h3 id=\"retentionpolicy\">RetentionPolicy</h3> <p>The retention policy name If empty any retention policy will be used.</p> <pre data-language=\"javascript\">node.retentionPolicy(value string)\n</pre> <h3 id=\"truncate\">Truncate</h3> <p>Optional duration for truncating timestamps. Helpful to ensure data points land on specific boundaries Example:</p> <pre data-language=\"javascript\">    stream\n       |from()\n           .measurement('mydata')\n           .truncate(1s)\n</pre> <p>All incoming data will be truncated to 1 second resolution.</p> <pre data-language=\"javascript\">node.truncate(value time.Duration)\n</pre> <h3 id=\"where\">Where</h3> <p>Filter the current stream using the given expression. This expression is a Kapacitor expression. Kapacitor expressions are a superset of InfluxQL WHERE expressions. See the <a href=\"https://docs.influxdata.com/kapacitor/latest/tick/expr/\">expression</a> docs for more information.</p> <p>Multiple calls to the Where method will <code>AND</code> together each expression.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n       |from()\n          .where(lambda: condition1)\n          .where(lambda: condition2)\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    stream\n       |from()\n          .where(lambda: condition1 AND condition2)\n</pre> <p>NOTE: Becareful to always use <code>|from</code> if you want multiple different streams.</p> <p>Example:</p> <pre data-language=\"javascript\">  var data = stream\n      |from()\n          .measurement('cpu')\n  var total = data\n      .where(lambda: \"cpu\" == 'cpu-total')\n  var others = data\n      .where(lambda: \"cpu\" != 'cpu-total')\n</pre> <p>The example above is equivalent to the example below, which is obviously not what was intended.</p> <p>Example:</p> <pre data-language=\"javascript\">  var data = stream\n      |from()\n          .measurement('cpu')\n          .where(lambda: \"cpu\" == 'cpu-total' AND \"cpu\" != 'cpu-total')\n  var total = data\n  var others = total\n</pre> <p>The example below will create two different streams each selecting a different subset of the original stream.</p> <p>Example:</p> <pre data-language=\"javascript\">  var data = stream\n      |from()\n          .measurement('cpu')\n  var total = stream\n      |from()\n          .measurement('cpu')\n          .where(lambda: \"cpu\" == 'cpu-total')\n  var others = stream\n      |from()\n          .measurement('cpu')\n          .where(lambda: \"cpu\" != 'cpu-total')\n</pre> <p>If empty then all data points are considered to match.</p> <pre data-language=\"javascript\">node.where(expression tick.Node)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"from\">From</h3> <p>Creates a new stream node that can be further filtered using the Database, RetentionPolicy, Measurement and Where properties. From can be called multiple times to create multiple independent forks of the data stream.</p> <p>Example:</p> <pre data-language=\"javascript\">    // Select the 'cpu' measurement from just the database 'mydb'\n    // and retention policy 'myrp'.\n    var cpu = stream\n        |from()\n            .database('mydb')\n            .retentionPolicy('myrp')\n            .measurement('cpu')\n    // Select the 'load' measurement from any database and retention policy.\n    var load = stream\n        |from()\n            .measurement('load')\n    // Join cpu and load streams and do further processing.\n    cpu\n        |join(load)\n            .as('cpu', 'load')\n        ...\n</pre> <pre data-language=\"javascript\">node|from()\n</pre> <p>Returns: <a href=\"index\">FromNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/from_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/from_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/join_node/index":"<h1>JoinNode</h1>     <p>Joins the data from any number of nodes. As each data point is received from a parent node it is paired with the next data points from the other parent nodes with a matching timestamp. Each parent node contributes at most one point to each joined point. A tolerance can be supplied to join points that do not have perfectly aligned timestamps. Any points that fall within the tolerance are joined on the timestamp. If multiple points fall within the same tolerance window than they are joined in the order they arrive.</p> <p>Aliases are used to prefix all fields from the respective nodes.</p> <p>The join can be an inner or outer join, see the <a href=\"index#fill\">JoinNode.Fill</a> property.</p> <p>Example:</p> <pre data-language=\"javascript\">    var errors = stream\n        |from()\n            .measurement('errors')\n    var requests = stream\n        |from()\n            .measurement('requests')\n    // Join the errors and requests streams\n    errors\n        |join(requests)\n            // Provide prefix names for the fields of the data points.\n            .as('errors', 'requests')\n            // points that are within 1 second are considered the same time.\n            .tolerance(1s)\n            // fill missing values with 0, implies outer join.\n            .fill(0.0)\n            // name the resulting stream\n            .streamName('error_rate')\n        // Both the \"value\" fields from each parent have been prefixed\n        // with the respective names 'errors' and 'requests'.\n        |eval(lambda: \"errors.value\" / \"requests.value\")\n           .as('rate')\n        ...\n</pre> <p>In the above example the <code>errors</code> and <code>requests</code> streams are joined and then transformed to calculate a combined field.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#as\">As</a></li> <li><a href=\"index#fill\">Fill</a></li> <li><a href=\"index#on\">On</a></li> <li><a href=\"index#streamname\">StreamName</a></li> <li><a href=\"index#tolerance\">Tolerance</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"as\">As</h3> <p>Prefix names for all fields from the respective nodes. Each field from the parent nodes will be prefixed with the provided name and a '.'. See the example above.</p> <p>The names cannot have a dot '.' character.</p> <pre data-language=\"javascript\">node.as(names ...string)\n</pre> <h3 id=\"fill\">Fill</h3> <p>Fill the data. The fill option implies the type of join: inner or full outer Options are:</p> <ul> <li>none - (default) skip rows where a point is missing, inner join.</li> <li>null - fill missing points with null, full outer join.</li> <li>Any numerical value - fill fields with given value, full outer join.</li> </ul> <pre data-language=\"javascript\">node.fill(value interface{})\n</pre> <h3 id=\"on\">On</h3> <p>Join on specific dimensions. For example given two measurements:</p> <ol> <li>building_power – tagged by building, value is the total power consumed by the building.</li> <li>floor_power – tagged by building and floor, values is the total power consumed by the floor.</li> </ol> <p>You want to calculate the percentage of the total building power consumed by each floor.</p> <p>Example:</p> <pre data-language=\"javascript\">    var buidling = stream\n        |from()\n            .measurement('building_power')\n            .groupBy('building')\n    var floor = stream\n        |from()\n            .measurement('floor_power')\n            .groupBy('building', 'floor')\n    building\n        |join(floor)\n            .as('building', 'floor')\n            .on('building')\n        |eval(lambda: \"floor.value\" / \"building.value\")\n            ... // Values here are grouped by 'building' and 'floor'\n</pre> <pre data-language=\"javascript\">node.on(dims ...string)\n</pre> <h3 id=\"streamname\">StreamName</h3> <p>The name of this new joined data stream. If empty the name of the left parent is used.</p> <pre data-language=\"javascript\">node.streamName(value string)\n</pre> <h3 id=\"tolerance\">Tolerance</h3> <p>The maximum duration of time that two incoming points can be apart and still be considered to be equal in time. The joined data point's time will be rounded to the nearest multiple of the tolerance duration.</p> <pre data-language=\"javascript\">node.tolerance(value time.Duration)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/join_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/join_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/log_node/index":"<h1>LogNode</h1>     <p>A node that logs all data that passes through the node.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream.from()...\n      |window()\n          .period(10s)\n          .every(10s)\n      |log()\n      |count('value')\n</pre> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#level\">Level</a></li> <li><a href=\"index#prefix\">Prefix</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"level\">Level</h3> <p>The level at which to log the data. One of: DEBUG, INFO, WARN, ERROR Default: INFO</p> <pre data-language=\"javascript\">node.level(value string)\n</pre> <h3 id=\"prefix\">Prefix</h3> <p>Optional prefix to add to all log messages</p> <pre data-language=\"javascript\">node.prefix(value string)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/log_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/log_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/source_batch_node/index":"<h1>SourceBatchNode</h1>     <p>A node that handles creating several child BatchNodes. Each call to <code>query</code> creates a child batch node that can further be configured. See <a href=\"../batch_node/index\">BatchNode</a> The <code>batch</code> variable in batch tasks is an instance of a <a href=\"index\">SourceBatchNode.</a></p> <p>Example:</p> <pre data-language=\"javascript\">     var errors = batch\n                      |query('SELECT value from errors')\n                      ...\n     var views = batch\n                      |query('SELECT value from views')\n                      ...\n</pre> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#query\">Query</a></li> <li><a href=\"index#stats\">Stats</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"query\">Query</h3> <p>The query to execute. Must not contain a time condition in the <code>WHERE</code> clause or contain a <code>GROUP BY</code> clause. The time conditions are added dynamically according to the period, offset and schedule. The <code>GROUP BY</code> clause is added dynamically according to the dimensions passed to the <code>groupBy</code> method.</p> <pre data-language=\"javascript\">node|query(q string)\n</pre> <p>Returns: <a href=\"../batch_node/index\">BatchNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/source_batch_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/source_batch_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/source_stream_node/index":"<h1>SourceStreamNode</h1>     <p>A <a href=\"index\">SourceStreamNode</a> represents the source of data being streamed to Kapacitor via any of its inputs. The <code>stream</code> variable in stream tasks is an instance of a <a href=\"index\">SourceStreamNode.</a> <a href=\"index#from\">SourceStreamNode.From</a> is the method/property of this node.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#from\">From</a></li> <li><a href=\"index#stats\">Stats</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"from\">From</h3> <p>Creates a new <a href=\"../stream_node/index\">StreamNode</a> that can be further filtered using the Database, RetentionPolicy, Measurement and Where properties. From can be called multiple times to create multiple independent forks of the data stream.</p> <p>Example:</p> <pre data-language=\"javascript\">    // Select the 'cpu' measurement from just the database 'mydb'\n    // and retention policy 'myrp'.\n    var cpu = stream\n        |from()\n            .database('mydb')\n            .retentionPolicy('myrp')\n            .measurement('cpu')\n    // Select the 'load' measurement from any database and retention policy.\n    var load = stream\n        |from()\n            .measurement('load')\n    // Join cpu and load streams and do further processing.\n    cpu\n        |join(load)\n            .as('cpu', 'load')\n        ...\n</pre> <pre data-language=\"javascript\">node|from()\n</pre> <p>Returns: <a href=\"../stream_node/index\">StreamNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/source_stream_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/source_stream_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/stats_node/index":"<h1>StatsNode</h1>     <p>A <a href=\"index\">StatsNode</a> emits internal statistics about the another node at a given interval.</p> <p>The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the other node is receiving. As a result the <a href=\"index\">StatsNode</a> is a root node in the task pipeline.</p> <p>The currently available internal statistics:</p> <ul> <li>emitted – the number of points or batches this node has sent to its children.</li> </ul> <p>Each stat is available as a field in the data stream.</p> <p>The stats are in groups according to the original data. Meaning that if the source node is grouped by the tag 'host' as an example, then the counts are output per host with the appropriate 'host' tag. Since its possible for groups to change when crossing a node only the emitted groups are considered.</p> <p>Example:</p> <pre data-language=\"javascript\">     var data = stream\n         |from()...\n     // Emit statistics every 1 minute and cache them via the HTTP API.\n     data\n         |stats(1m)\n         |httpOut('stats')\n     // Continue normal processing of the data stream\n     data...\n</pre> <p>WARNING: It is not recommended to join the stats stream with the original data stream. Since they operate on different clocks you could potentially create a deadlock. This is a limitation of the current implementation and may be removed in the future.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/stats_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/stats_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/query_node/index":"<h1>QueryNode</h1>     <p>A <a href=\"index\">QueryNode</a> defines a source and a schedule for processing batch data. The data is queried from an InfluxDB database and then passed into the data pipeline.</p> <p>Example:</p> <pre data-language=\"javascript\"> batch\n     |query('''\n         SELECT mean(\"value\")\n         FROM \"telegraf\".\"default\".cpu_usage_idle\n         WHERE \"host\" = 'serverA'\n     ''')\n         .period(1m)\n         .every(20s)\n         .groupBy(time(10s), 'cpu')\n     ...\n</pre> <p>In the above example InfluxDB is queried every 20 seconds; the window of time returned spans 1 minute and is grouped into 10 second buckets.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#align\">Align</a></li> <li><a href=\"index#cluster\">Cluster</a></li> <li><a href=\"index#cron\">Cron</a></li> <li><a href=\"index#every\">Every</a></li> <li><a href=\"index#fill\">Fill</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#offset\">Offset</a></li> <li><a href=\"index#period\">Period</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"align\">Align</h3> <p>Align start and stop times for quiries with even boundaries of the <a href=\"index#every\">QueryNode.Every</a> property. Does not apply if using the <a href=\"index#cron\">QueryNode.Cron</a> property.</p> <pre data-language=\"javascript\">node.align()\n</pre> <h3 id=\"cluster\">Cluster</h3> <p>The name of a configured InfluxDB cluster. If empty the default cluster will be used.</p> <pre data-language=\"javascript\">node.cluster(value string)\n</pre> <h3 id=\"cron\">Cron</h3> <p>Define a schedule using a cron syntax.</p> <p>The specific cron implementation is documented here: <a href=\"https://github.com/gorhill/cronexpr#implementation\">https://github.com/gorhill/cronexpr#implementation</a></p> <p>The Cron property is mutually exclusive with the Every property.</p> <pre data-language=\"javascript\">node.cron(value string)\n</pre> <h3 id=\"every\">Every</h3> <p>How often to query InfluxDB.</p> <p>The Every property is mutually exclusive with the Cron property.</p> <pre data-language=\"javascript\">node.every(value time.Duration)\n</pre> <h3 id=\"fill\">Fill</h3> <p>Fill the data. Options are:</p> <ul> <li>Any numerical value</li> <li>null - exhibits the same behavior as the default</li> <li>previous - reports the value of the previous window</li> <li>none - suppresses timestamps and values where the value is null</li> </ul> <pre data-language=\"javascript\">node.fill(value interface{})\n</pre> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of dimensions. Can specify one time dimension.</p> <p>This property adds a <code>GROUP BY</code> clause to the query so all the normal behaviors when quering InfluxDB with a <code>GROUP BY</code> apply. More details: <a href=\"https://influxdb.com/docs/v0.9/query_language/data_exploration.html#the-group-by-clause\">https://influxdb.com/docs/v0.9/query_language/data_exploration.html#the-group-by-clause</a></p> <p>Example:</p> <pre data-language=\"javascript\">    batch\n        |query(...)\n            .groupBy(time(10s), 'tag1', 'tag2'))\n</pre> <pre data-language=\"javascript\">node.groupBy(d ...interface{})\n</pre> <h3 id=\"offset\">Offset</h3> <p>How far back in time to query from the current time</p> <p>For example an Offest of 2 hours and an Every of 5m, Kapacitor will query InfluxDB every 5 minutes for the window of data 2 hours ago.</p> <p>This applies to Cron schedules as well. If the cron specifies to run every Sunday at 1 AM and the Offset is 1 hour. Then at 1 AM on Sunday the data from 12 AM will be queried.</p> <pre data-language=\"javascript\">node.offset(value time.Duration)\n</pre> <h3 id=\"period\">Period</h3> <p>The period or length of time that will be queried from InfluxDB</p> <pre data-language=\"javascript\">node.period(value time.Duration)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/query_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/query_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/where_node/index":"<h1>WhereNode</h1>     <p>The <a href=\"index\">WhereNode</a> filters the data stream by a given expression.</p> <p>Example:</p> <pre data-language=\"javascript\"> var sums = stream\n     |from()\n         .groupBy('service', 'host')\n     |sum('value')\n //Watch particular host for issues.\n sums\n    |where(lambda: \"host\" == 'h001.example.com')\n    |alert()\n        .crit(lambda: TRUE)\n        .email().to('user@example.com')\n</pre> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>And another expression onto the existing expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/where_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/where_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/window_node/index":"<h1>WindowNode</h1>     <p>Windows data over time. A window has a length defined by <code>period</code> and a frequency at which it emits the window to the pipeline.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |window()\n            .period(10m)\n            .every(5m)\n        |httpOut('recent')\n</pre> <p>The above windowing example emits a window to the pipeline every <code>5 minutes</code> and the window contains the last <code>10 minutes</code> worth of data. As a result each time the window is emitted it contains half new data and half old data.</p> <p>NOTE: Time for a window (or any node) is implemented by inspecting the times on the incoming data points. As a result if the incoming data stream stops then no more windows will be emitted because time is no longer increasing for the window node.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#align\">Align</a></li> <li><a href=\"index#every\">Every</a></li> <li><a href=\"index#period\">Period</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"align\">Align</h3> <p>Wether to align the window edges with the zero time. If not aligned the window starts and ends relative to the first data point it receives.</p> <pre data-language=\"javascript\">node.align()\n</pre> <h3 id=\"every\">Every</h3> <p>How often the current window is emitted into the pipeline.</p> <pre data-language=\"javascript\">node.every(value time.Duration)\n</pre> <h3 id=\"period\">Period</h3> <p>The period, or length in time, of the window.</p> <pre data-language=\"javascript\">node.period(value time.Duration)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/window_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/window_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/api/index":"<h1>APIs</h1><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/api/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/api/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/sample_node/index":"<h1>SampleNode</h1>     <p>Sample points or batches. One point will be emitted every count or duration specified.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |sample(3)\n</pre> <p>Keep every third data point or batch.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |sample(10s)\n</pre> <p>Keep only samples that land on the 10s boundary. See <a href=\"../from_node/index#truncate\">FromNode.Truncate,</a> <a href=\"../query_node/index#groupby\">QueryNode.GroupBy</a> time or <a href=\"../window_node/index#align\">WindowNode.Align</a> for ensuring data is aligned with a boundary.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/sample_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/sample_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/shift_node/index":"<h1>ShiftNode</h1>     <p>Shift points and batches in time, this is useful for comparing batches or points from different times.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |shift(5m)\n</pre> <p>Shift all data points 5m forward in time.</p> <p>Example:</p> <pre data-language=\"javascript\">    stream\n        |shift(-10s)\n</pre> <p>Shift all data points 10s backward in time.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/shift_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/shift_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/u_d_f_node/index":"<h1>UDFNode</h1>     <p>A <a href=\"index\">UDFNode</a> is a node that can run a User Defined Function (UDF) in a separate process.</p> <p>A UDF is a custom script or binary that can communicate via Kapacitor's UDF RPC protocol. The path and arguments to the UDF program are specified in Kapacitor's configuration. Using TICKscripts you can invoke and configure your UDF for each task.</p> <p>See the <a href=\"https://github.com/influxdata/kapacitor/tree/master/udf/agent/\">README.md</a> for details on how to write your own UDF.</p> <p>UDFs are configured via Kapacitor's main configuration file.</p> <p>Example:</p> <pre data-language=\"javascript\">    [udf]\n    [udf.functions]\n        # Example moving average UDF.\n        [udf.functions.movingAverage]\n            prog = \"/path/to/executable/moving_avg\"\n            args = []\n            timeout = \"10s\"\n</pre> <p>UDFs are first class objects in TICKscripts and are referenced via their configuration name.</p> <p>Example:</p> <pre data-language=\"javascript\">     // Given you have a UDF that computes a moving average\n     // The UDF can define what its options are and then can be\n     // invoked via a TICKscript like so:\n     stream\n         |from()...\n         @movingAverage()\n             .field('value')\n             .size(100)\n             .as('mavg')\n         |httpOut('movingaverage')\n</pre> <p>NOTE: The UDF process runs as the same user as the Kapacitor daemon. As a result make the user is properly secured as well as the configuration file.</p> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#udfname\">UDFName</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"udfname\">UDFName</h3> <pre data-language=\"javascript\">node.uDFName(value string)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"../union_node/index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/u_d_f_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/u_d_f_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/examples/index":"<h1>Examples</h1>     <p>The following is a list of examples in no particular order that demonstrate some of the features of Kapacitor. These guides assume your are familiar with the basics of defining, recording, replaying and enabling tasks within Kapacitor. See the <a href=\"../introduction/getting_started/index\">getting started</a> guide if you need a refresher.</p> <h3 id=\"calculate-rates-across-joined-series-backfill-kapacitor-v0-13-examples-join-backfill\"><a href=\"join_backfill/index\">Calculate Rates across joined series + Backfill</a></h3> <p>Learn how to join two series and calculate a combined results, plus how to perform that operation on historical data.</p> <h3 id=\"live-leaderboard-of-game-scores-kapacitor-v0-13-examples-live-leaderboard\"><a href=\"live_leaderboard/index\">Live Leaderboard of game scores</a></h3> <p>See how you can use Kapacitor to create a live updating leaderboard for a game.</p> <h3 id=\"custom-anomaly-detection-kapacitor-v0-13-examples-anomaly-detection\"><a href=\"anomaly_detection/index\">Custom Anomaly Detection</a></h3> <p>Integrate your custom anomaly detection algorithm with Kapacitor.</p> <h3 id=\"continuous-queries-kapacitor-v0-13-examples-continuous-queries\"><a href=\"continuous_queries/index\">Continuous Queries</a></h3> <p>See how to use Kapacitor as a continuous query engine.</p> <h3 id=\"socket-based-udf-kapacitor-v0-13-examples-socket-udf\"><a href=\"socket_udf/index\">Socket based UDF</a></h3> <p>Learn how to write a simple socket based UDF.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/examples/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/examples/</a>\n  </p>\n</div>\n","kapacitor/v0.13/examples/live_leaderboard/index":"<h1>Live Leaderboard of game scores</h1>     <p><strong>If you do not have a running Kapacitor instance check out the <a href=\"../../introduction/getting_started/index\">getting started guide</a> to get Kapacitor up and running on localhost.</strong></p> <p>Today we are game developers. We host a several game servers each running an instance of the game code with about a hundred players per game.</p> <p>We need to build a leaderboard so spectators can see the player’s scores in real time. We would also like to have historical data on leaders in order to do post game analysis on who was leading for how long etc.</p> <p>We will use Kapacitor’s stream processing to do the heavy lifting for us. The game servers can send a UDP packet anytime a player’s score changes or at least every 10 seconds if the score hasn’t changed.</p> <h3 id=\"setup\">Setup</h3> <p><strong>All snippets below can be found <a href=\"https://github.com/influxdb/kapacitor/tree/master/examples/scores\">here</a></strong></p> <p>Our first order of business is to configure Kapacitor to receive the stream of scores. In this case the scores update too often to store all of them in InfluxDB so we will send them directly to Kapacitor. Like InfluxDB you can configure a UDP listener. Add this configuration section to the end of your Kapacitor configuration.</p> <pre>[[udp]]\n    enabled = true\n    bind-address = \":9100\"\n    database = \"game\"\n    retention-policy = \"default\"\n</pre> <p>This configuration tells Kapacitor to listen on port <code>9100</code> for UDP packets in the line protocol format. It will scope in incoming data to be in the <code>game.default</code> database and retention policy. Start Kapacitor running with that added to the configuration.</p> <p>Here is a simple bash script to generate random score data so we can test it without messing with the real game servers.</p> <pre data-language=\"bash\">#!/bin/bash\n\n# default options: can be overriden with corresponding arguments.\nhost=${1-localhost}\nport=${2-9100}\ngames=${3-10}\nplayers=${4-100}\n\ngames=$(seq $games)\nplayers=$(seq $players)\n# Spam score updates over UDP\nwhile true\ndo\n    for game in $games\n    do\n        game=\"g$game\"\n        for player in $players\n        do\n            player=\"p$player\"\n            score=$(($RANDOM % 1000))\n            echo \"scores,player=$player,game=$game value=$score\" &gt; /dev/udp/$host/$port\n        done\n    done\n    sleep 0.1\ndone\n</pre> <p>Place the above script into a file <code>scores.sh</code> and run it:</p> <pre data-language=\"bash\">chmod +x ./scores.sh\n./scores.sh\n</pre> <p>Now we are spaming Kapacitor with our fake score data. We can just leave that running since Kapacitor will drop the incoming data until it has a task that wants it.</p> <h3 id=\"defining-the-kapacitor-task\">Defining the Kapacitor task</h3> <p>What does a leaderboard need to do?</p> <p>1. Get the most recent score per player per game. 2. Calculate the top X player scores per game. 3. Publish the results. 4. Store the results.</p> <p>To complete step one we need to buffer the incoming stream and return the most recent score update per player per game. Our <a href=\"../../tick/index\">TICKscript</a> will look like this:</p> <pre data-language=\"javascript\">var topPlayerScores = stream\n    |from()\n        .measurement('scores')\n        // Get the most recent score for each player per game.\n        // Not likely that a player is playing two games but just in case.\n        .groupBy('game', 'player')\n    |window()\n        // keep a buffer of the last 11s of scores\n        // just in case a player score hasn't updated in a while\n        .period(11s)\n        // Emit the current score per player every second.\n        .every(1s)\n        // Align the window boundaries to be on the second.\n        .align()\n    |last('value')\n</pre> <p>Place this script in a file called <code>top_scores.tick</code>.</p> <p>Now our <code>topPlayerScores</code> variable contains each player’s most recent score. Next to calculate the top scores per game we just need to group by game and run another map reduce job. Let’s keep the top 15 scores per game. Add these lines to the <code>top_scores.tick</code> file.</p> <pre data-language=\"javascript\">// Calculate the top 15 scores per game\nvar topScores = topPlayerScores\n    |groupBy('game')\n    |top(15, 'last', 'player')\n</pre> <p>The <code>topScores</code> variable now contains the top 15 player’s score per game. All we need to be able to build our leaderboard. Kapacitor can expose the scores over HTTP via the <a href=\"../../nodes/http_out_node/index\">HTTPOutNode</a>. We will call our task <code>top_scores</code>; with the following addition the most recent scores will be available at <code>http://localhost:9092/kapacitor/v1/tasks/top_scores/top_scores</code>.</p> <pre data-language=\"javascript\">// Expose top scores over the HTTP API at the 'top_scores' endpoint.\n// Now your app can just request the top scores from Kapacitor\n// and always get the most recent result.\n//\n// http://localhost:9092/kapacitor/v1/tasks/top_scores/top_scores\ntopScores\n   |httpOut('top_scores')\n</pre> <p>Finally we want to store the top scores over time so we can do in depth analysis to ensure the best game play. But we do not want to store the scores every second as that is still too much data. First we will sample the data and store scores only every 10 seconds. Also let’s do some basic analysis ahead of time since we already have a stream of all the data. For now we will just do basic gap analysis where we will store the gap between the top player and the 15th player. Add these lines to <code>top_scores.tick</code> to complete our task.</p> <pre data-language=\"javascript\">// Sample the top scores and keep a score once every 10s\nvar topScoresSampled = topScores\n    |sample(10s)\n\n// Store top fifteen player scores in InfluxDB.\ntopScoresSampled\n    |influxDBOut()\n        .database('game')\n        .measurement('top_scores')\n\n// Calculate the max and min of the top scores.\nvar max = topScoresSampled\n    |max('top')\n\nvar min = topScoresSampled\n    |min('top')\n\n// Join the max and min streams back together and calculate the gap.\nmax\n    |join(min)\n        .as('max', 'min')\n    // Calculate the difference between the max and min scores.\n    // Rename the max and min fields to more friendly names 'topFirst', 'topLast'.\n    |eval(lambda: \"max.max\" - \"min.min\", lambda: \"max.max\", lambda: \"min.min\")\n        .as('gap', 'topFirst', 'topLast')\n    // Store the fields: gap, topFirst and topLast in InfluxDB.\n    |influxDBOut()\n        .database('game')\n        .measurement('top_scores_gap')\n</pre> <p>Since we are writing data back to InfluxDB create a database <code>game</code> for our results.</p> <pre>curl -G 'http://localhost:8086/query?' --data-urlencode 'q=CREATE DATABASE game'\n</pre> <p>Here is the complete task TICKscript if you don’t want to copy paste as much :)</p> <pre data-language=\"javascript\">// Define a result that contains the most recent score per player.\nvar topPlayerScores = stream\n    |from()\n        .measurement('scores')\n        // Get the most recent score for each player per game.\n        // Not likely that a player is playing two games but just in case.\n        .groupBy('game', 'player')\n    |window()\n        // keep a buffer of the last 11s of scores\n        // just in case a player score hasn't updated in a while\n        .period(11s)\n        // Emit the current score per player every second.\n        .every(1s)\n        // Align the window boundaries to be on the second.\n        .align()\n    |last('value')\n\n// Calculate the top 15 scores per game\nvar topScores = topPlayerScores\n    |groupBy('game')\n    |top(15, 'last', 'player')\n\n// Expose top scores over the HTTP API at the 'top_scores' endpoint.\n// Now your app can just request the top scores from Kapacitor\n// and always get the most recent result.\n//\n// http://localhost:9092/kapacitor/v1/tasks/top_scores/top_scores\ntopScores\n   |httpOut('top_scores')\n\n// Sample the top scores and keep a score once every 10s\nvar topScoresSampled = topScores\n    |sample(10s)\n\n// Store top fifteen player scores in InfluxDB.\ntopScoresSampled\n    |influxDBOut()\n        .database('game')\n        .measurement('top_scores')\n\n// Calculate the max and min of the top scores.\nvar max = topScoresSampled\n    |max('top')\n\nvar min = topScoresSampled\n    |min('top')\n\n// Join the max and min streams back together and calculate the gap.\nmax\n    |join(min)\n        .as('max', 'min')\n    // calculate the difference between the max and min scores.\n    |eval(lambda: \"max.max\" - \"min.min\", lambda: \"max.max\", lambda: \"min.min\")\n        .as('gap', 'topFirst', 'topLast')\n    // store the fields: gap, topFirst, and topLast in InfluxDB.\n    |influxDBOut()\n        .database('game')\n        .measurement('top_scores_gap')\n</pre> <p>Define and enable our task to see it in action:</p> <pre data-language=\"bash\">kapacitor define top_scores -tick top_scores.tick -type stream -dbrp game.default\nkapacitor enable top_scores\n</pre> <p>First let’s check that the HTTP output is working.</p> <pre data-language=\"bash\">curl 'http://localhost:9092/kapacitor/v1/tasks/top_scores/top_scores'\n</pre> <p>You should have a JSON result of the top 15 players and their scores per game. Hit the endpoint several times to see that the scores are updating once a second.</p> <p>Now, let’s check InfluxDB to see our historical data.</p> <pre data-language=\"bash\">curl \\\n    -G 'http://localhost:8086/query?db=game' \\\n    --data-urlencode 'q=SELECT * FROM top_scores  WHERE time &gt; now() - 5m GROUP BY game'\n\ncurl \\\n    -G 'http://localhost:8086/query?db=game' \\\n    --data-urlencode 'q=SELECT * FROM top_scores_gap WHERE time &gt; now() - 5m GROUP BY game'\n</pre> <p>Great! the hard work is done. All that is left is to configure the game server to send score updates to Kapacitor and update the spectator dashboard to pull scores from Kapacitor.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/examples/live_leaderboard/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/examples/live_leaderboard/</a>\n  </p>\n</div>\n","kapacitor/v0.13/administration/index":"<h1>Administration</h1>     <h2 id=\"upgrading-to-kapacitor-0-13-kapacitor-v0-13-administration-upgrading\"><a href=\"upgrading/index\">Upgrading to Kapacitor 0.13</a></h2><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/administration/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/administration/</a>\n  </p>\n</div>\n","kapacitor/v0.13/nodes/union_node/index":"<h1>UnionNode</h1>     <p>Takes the union of all of its parents. The union is just a simple pass through. Each data points received from each parent is passed onto children nodes without modification.</p> <p>Example:</p> <pre data-language=\"javascript\">    var logins = stream\n        |from()\n            .measurement('logins')\n    var logouts = stream\n        |from()\n            .measurement('logouts')\n    var frontpage = stream\n        |from()\n            .measurement('frontpage')\n    // Union all user actions into a single stream\n    logins\n        |union(logouts, frontpage)\n            .rename('user_actions')\n        ...\n</pre> <h2 id=\"index\">Index</h2> <h3 id=\"properties\">Properties</h3> <ul> <li><a href=\"index#rename\">Rename</a></li> </ul> <h3 id=\"chaining-methods\">Chaining Methods</h3> <ul> <li><a href=\"index#alert\">Alert</a></li> <li><a href=\"index#bottom\">Bottom</a></li> <li><a href=\"index#count\">Count</a></li> <li><a href=\"index#deadman\">Deadman</a></li> <li><a href=\"index#default\">Default</a></li> <li><a href=\"index#derivative\">Derivative</a></li> <li><a href=\"index#distinct\">Distinct</a></li> <li><a href=\"index#elapsed\">Elapsed</a></li> <li><a href=\"index#eval\">Eval</a></li> <li><a href=\"index#first\">First</a></li> <li><a href=\"index#groupby\">GroupBy</a></li> <li><a href=\"index#httpout\">HttpOut</a></li> <li><a href=\"index#influxdbout\">InfluxDBOut</a></li> <li><a href=\"index#join\">Join</a></li> <li><a href=\"index#last\">Last</a></li> <li><a href=\"index#log\">Log</a></li> <li><a href=\"index#max\">Max</a></li> <li><a href=\"index#mean\">Mean</a></li> <li><a href=\"index#median\">Median</a></li> <li><a href=\"index#min\">Min</a></li> <li><a href=\"index#percentile\">Percentile</a></li> <li><a href=\"index#sample\">Sample</a></li> <li><a href=\"index#shift\">Shift</a></li> <li><a href=\"index#spread\">Spread</a></li> <li><a href=\"index#stats\">Stats</a></li> <li><a href=\"index#stddev\">Stddev</a></li> <li><a href=\"index#sum\">Sum</a></li> <li><a href=\"index#top\">Top</a></li> <li><a href=\"index#union\">Union</a></li> <li><a href=\"index#where\">Where</a></li> <li><a href=\"index#window\">Window</a></li> </ul> <h2 id=\"properties-1\">Properties</h2> <p>Property methods modify state on the calling node. They do not add another node to the pipeline, and always return a reference to the calling node. Property methods are marked using the <code>.</code> operator.</p> <h3 id=\"rename\">Rename</h3> <p>The new name of the stream. If empty the name of the left node (i.e. <code>leftNode.union(otherNode1, otherNode2)</code>) is used.</p> <pre data-language=\"javascript\">node.rename(value string)\n</pre> <h2 id=\"chaining-methods-1\">Chaining Methods</h2> <p>Chaining methods create a new node in the pipeline as a child of the calling node. They do not modify the calling node. Chaining methods are marked using the <code>|</code> operator.</p> <h3 id=\"alert\">Alert</h3> <p>Create an alert node, which can trigger alerts.</p> <pre data-language=\"javascript\">node|alert()\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"bottom\">Bottom</h3> <p>Select the bottom <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|bottom(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"count\">Count</h3> <p>Count the number of points.</p> <pre data-language=\"javascript\">node|count(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"deadman\">Deadman</h3> <p>Helper function for creating an alert on low throughput, aka deadman's switch.</p> <ul> <li>Threshold – trigger alert if throughput drops below threshold in points/interval.</li> <li>Interval – how often to check the throughput.</li> <li>Expressions – optional list of expressions to also evaluate. Useful for time of day alerting.</li> </ul> <p>Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n    //Do normal processing of data\n    data...\n</pre> <p>The above is equivalent to this Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    data\n        |stats(10s)\n        |derivative('emitted')\n            .unit(10s)\n            .nonNegative()\n        |alert()\n            .id('node \\'stream0\\' in task \\'{{ .TaskName }}\\'')\n            .message('{{ .ID }} is {{ if eq .Level \"OK\" }}alive{{ else }}dead{{ end }}: {{ index .Fields \"emitted\" | printf \"%0.3f\" }} points/10s.')\n            .crit(lamdba: \"emitted\" &lt;= 100.0)\n    //Do normal processing of data\n    data...\n</pre> <p>The <code>id</code> and <code>message</code> alert properties can be configured globally via the 'deadman' configuration section.</p> <p>Since the <a href=\"../alert_node/index\">AlertNode</a> is the last piece it can be further modified as normal. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 1s and checked every 10s.\n    data\n        |deadman(100.0, 10s)\n            .slack()\n            .channel('#dead_tasks')\n    //Do normal processing of data\n    data...\n</pre> <p>You can specify additional lambda expressions to further constrain when the deadman's switch is triggered. Example:</p> <pre data-language=\"javascript\">    var data = stream\n        |from()...\n    // Trigger critical alert if the throughput drops below 100 points per 10s and checked every 10s.\n    // Only trigger the alert if the time of day is between 8am-5pm.\n    data\n        |deadman(100.0, 10s, lambda: hour(\"time\") &gt;= 8 AND hour(\"time\") &lt;= 17)\n    //Do normal processing of data\n    data...\n</pre> <pre data-language=\"javascript\">node|deadman(threshold float64, interval time.Duration, expr ...tick.Node)\n</pre> <p>Returns: <a href=\"../alert_node/index\">AlertNode</a></p> <h3 id=\"default\">Default</h3> <p>Create a node that can set defaults for missing tags or fields.</p> <pre data-language=\"javascript\">node|default()\n</pre> <p>Returns: <a href=\"../default_node/index\">DefaultNode</a></p> <h3 id=\"derivative\">Derivative</h3> <p>Create a new node that computes the derivative of adjacent points.</p> <pre data-language=\"javascript\">node|derivative(field string)\n</pre> <p>Returns: <a href=\"../derivative_node/index\">DerivativeNode</a></p> <h3 id=\"distinct\">Distinct</h3> <p>Produce batch of only the distinct points.</p> <pre data-language=\"javascript\">node|distinct(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"elapsed\">Elapsed</h3> <p>Compute the elapsed time between points</p> <pre data-language=\"javascript\">node|elapsed(field string, unit time.Duration)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"eval\">Eval</h3> <p>Create an eval node that will evaluate the given transformation function to each data point. A list of expressions may be provided and will be evaluated in the order they are given and results of previous expressions are made available to later expressions.</p> <pre data-language=\"javascript\">node|eval(expressions ...tick.Node)\n</pre> <p>Returns: <a href=\"../eval_node/index\">EvalNode</a></p> <h3 id=\"first\">First</h3> <p>Select the first point.</p> <pre data-language=\"javascript\">node|first(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"groupby\">GroupBy</h3> <p>Group the data by a set of tags.</p> <p>Can pass literal * to group by all dimensions. Example:</p> <pre data-language=\"javascript\">    |groupBy(*)\n</pre> <pre data-language=\"javascript\">node|groupBy(tag ...interface{})\n</pre> <p>Returns: <a href=\"../group_by_node/index\">GroupByNode</a></p> <h3 id=\"httpout\">HttpOut</h3> <p>Create an http output node that caches the most recent data it has received. The cached data is available at the given endpoint. The endpoint is the relative path from the API endpoint of the running task. For example if the task endpoint is at \"/api/v1/task/&lt;task_name&gt;\" and endpoint is \"top10\", then the data can be requested from \"/api/v1/task/&lt;task_name&gt;/top10\".</p> <pre data-language=\"javascript\">node|httpOut(endpoint string)\n</pre> <p>Returns: <a href=\"../http_out_node/index\">HTTPOutNode</a></p> <h3 id=\"influxdbout\">InfluxDBOut</h3> <p>Create an influxdb output node that will store the incoming data into InfluxDB.</p> <pre data-language=\"javascript\">node|influxDBOut()\n</pre> <p>Returns: <a href=\"../influx_d_b_out_node/index\">InfluxDBOutNode</a></p> <h3 id=\"join\">Join</h3> <p>Join this node with other nodes. The data is joined on timestamp.</p> <pre data-language=\"javascript\">node|join(others ...Node)\n</pre> <p>Returns: <a href=\"../join_node/index\">JoinNode</a></p> <h3 id=\"last\">Last</h3> <p>Select the last point.</p> <pre data-language=\"javascript\">node|last(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"log\">Log</h3> <p>Create a node that logs all data it receives.</p> <pre data-language=\"javascript\">node|log()\n</pre> <p>Returns: <a href=\"../log_node/index\">LogNode</a></p> <h3 id=\"max\">Max</h3> <p>Select the maximum point.</p> <pre data-language=\"javascript\">node|max(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"mean\">Mean</h3> <p>Compute the mean of the data.</p> <pre data-language=\"javascript\">node|mean(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"median\">Median</h3> <p>Compute the median of the data. Note, this method is not a selector, if you want the median point use .percentile(field, 50.0).</p> <pre data-language=\"javascript\">node|median(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"min\">Min</h3> <p>Select the minimum point.</p> <pre data-language=\"javascript\">node|min(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"percentile\">Percentile</h3> <p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed.</p> <pre data-language=\"javascript\">node|percentile(field string, percentile float64)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sample\">Sample</h3> <p>Create a new node that samples the incoming points or batches.</p> <p>One point will be emitted every count or duration specified.</p> <pre data-language=\"javascript\">node|sample(rate interface{})\n</pre> <p>Returns: <a href=\"../sample_node/index\">SampleNode</a></p> <h3 id=\"shift\">Shift</h3> <p>Create a new node that shifts the incoming points or batches in time.</p> <pre data-language=\"javascript\">node|shift(shift time.Duration)\n</pre> <p>Returns: <a href=\"../shift_node/index\">ShiftNode</a></p> <h3 id=\"spread\">Spread</h3> <p>Compute the difference between min and max points.</p> <pre data-language=\"javascript\">node|spread(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"stats\">Stats</h3> <p>Create a new stream of data that contains the internal statistics of the node. The interval represents how often to emit the statistics based on real time. This means the interval time is independent of the times of the data points the source node is receiving.</p> <pre data-language=\"javascript\">node|stats(interval time.Duration)\n</pre> <p>Returns: <a href=\"../stats_node/index\">StatsNode</a></p> <h3 id=\"stddev\">Stddev</h3> <p>Compute the standard deviation.</p> <pre data-language=\"javascript\">node|stddev(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"sum\">Sum</h3> <p>Compute the sum of all values.</p> <pre data-language=\"javascript\">node|sum(field string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"top\">Top</h3> <p>Select the top <code>num</code> points for <code>field</code> and sort by any extra tags or fields.</p> <pre data-language=\"javascript\">node|top(num int64, field string, fieldsAndTags ...string)\n</pre> <p>Returns: <a href=\"../influx_q_l_node/index\">InfluxQLNode</a></p> <h3 id=\"union\">Union</h3> <p>Perform the union of this node and all other given nodes.</p> <pre data-language=\"javascript\">node|union(node ...Node)\n</pre> <p>Returns: <a href=\"index\">UnionNode</a></p> <h3 id=\"where\">Where</h3> <p>Create a new node that filters the data stream by a given expression.</p> <pre data-language=\"javascript\">node|where(expression tick.Node)\n</pre> <p>Returns: <a href=\"../where_node/index\">WhereNode</a></p> <h3 id=\"window\">Window</h3> <p>Create a new node that windows the stream by time.</p> <p>NOTE: Window can only be applied to stream edges.</p> <pre data-language=\"javascript\">node|window()\n</pre> <p>Returns: <a href=\"../window_node/index\">WindowNode</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/nodes/union_node/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/nodes/union_node/</a>\n  </p>\n</div>\n","kapacitor/v0.13/examples/join_backfill/index":"<h1>Calculate Rates across joined series + Backfill</h1>     <p>Often times we have set of series where each series is counting a particular event. Using Kapacitor we can join those series and calculate a combined value.</p> <p>Let’s say we have two measurements:</p> <ul> <li>\n<code>errors</code> – the number of page views that had an error.</li> <li>\n<code>views</code> – the number of page views that had no errror.</li> </ul> <p>Both measurements exist in a database called <code>pages</code> and in the retention policy <code>default</code>.</p> <p>We want to know the percent of page views that resulted in an error. The process is to select both existing measurements join them and calculate the percentage. Then to store the data back into InfluxDB as a new measurement.</p> <h3 id=\"joining-with-batch-data\">Joining with batch data</h3> <p>We need to query the two measurements, <code>errors</code> and <code>views</code>.</p> <pre data-language=\"javascript\">// Get errors batch data\nvar errors = batch\n    |query('SELECT sum(value) FROM \"pages\".\"default\".errors')\n        .period(1h)\n        .every(1h)\n        .groupBy(time(1m), *)\n        .fill(0)\n\n// Get views batch data\nvar views = batch\n    |query('SELECT sum(value) FROM \"pages\".\"default\".views')\n        .period(1h)\n        .every(1h)\n        .groupBy(time(1m), *)\n        .fill(0)\n</pre> <p>The join process skips points that do not have a matching point in time from the other source. As a result it is important to both <code>groupBy</code> and <code>fill</code> the data while joining batch data. Grouping the data by time ensures that each source has data points at consistent time values. Filling the data ensures every point will have a match with a sane default.</p> <p>Now that we have two batch sources for each measurement we need to join them like so.</p> <pre data-language=\"javascript\">// Join errors and views\nerrors\n    |join(views)\n        .as('errors', 'views')\n</pre> <p>The data is joined by time, meaning that as pairs of batches arrive from each source they will be combined into a single batch. As a result the fields from each source need to be renamed to properly namespace the fields. This is done via the <code>.as('errors', 'views')</code> line. In this example each measurement has only one field named <code>sum</code>, the joined fields will be called <code>errors.sum</code> and <code>views.sum</code> respectively.</p> <p>Now that the data is joined we can calculate the percentage. Using the new names for the fields we can write this expression to calculate our desired percentage.</p> <pre data-language=\"javascript\">    //Calculate percentage\n    |eval(lambda: \"errors.sum\" / (\"views.sum\" + \"errors.sum\"))\n        // Give the resulting field a name\n        .as('value')\n\n</pre> <p>Finally we want to store this data back into InfluxDB.</p> <pre data-language=\"javascript\">    |influxDBOut()\n        .database('pages')\n        .measurement('error_percent')\n\n</pre> <p>Here is the complete TICKscript for the batch task:</p> <pre data-language=\"javascript\">// Get errors batch data\nvar errors = batch\n    |query('SELECT sum(value) FROM \"pages\".\"default\".errors')\n        .period(1h)\n        .every(1h)\n        .groupBy(time(1m), *)\n        .fill(0)\n\n// Get views batch data\nvar views = batch\n    |query('SELECT sum(value) FROM \"pages\".\"default\".views')\n        .period(1h)\n        .every(1h)\n        .groupBy(time(1m), *)\n        .fill(0)\n\n// Join errors and views\nerrors\n    |join(views)\n        .as('errors', 'views')\n    //Calculate percentage\n    |eval(lambda: \"errors.sum\" / (\"views.sum\" + \"errors.sum\"))\n        // Give the resulting field a name\n        .as('value')\n    |influxDBOut()\n        .database('pages')\n        .measurement('error_percent')\n\n</pre> <h3 id=\"backfill\">Backfill</h3> <p>Now for a fun little trick. Using Kapacitor’s record/replay actions we can actually run this TICKscript on historical data. First save the above script as <code>error_percent.tick</code> and define it. Then create a recording for the past time frame we want.</p> <pre data-language=\"bash\">kapacitor define error_percent \\\n    -type batch \\\n    -tick error_percent.tick \\\n    -dbrp pages.default\nkapacitor record batch -task error_percent -past 1d\n</pre> <p>Grab the recording ID and replay the historical data against the task. Here we specify the <code>-rec-time</code> flag to instruct Kapacitor to use the actual time stored in the recording when processing the data instead of adjusting to the present time.</p> <pre data-language=\"bash\">kapacitor replay -task error_percent -recording RECORDING_ID -rec-time\n</pre> <p>If the data set is too large to keep in one recording you can define a specific range of time to record and then replay each range individually.</p> <pre data-language=\"bash\">rid=$(kapacitor record batch -task error_percent -start 2015-10-01 -stop 2015-10-02)\necho $rid\nkapacitor replay -task error_percent -recording $rid -rec-time\nkapacitor delete recordings $rid\n</pre> <p>Just loop through the above script for each time window and reconstruct all the historical data you need. With that we now have the error_percent every minute backfilled for the historical data we had.</p> <p>If you would like to try out this example case there are scripts <a href=\"https://github.com/influxdb/kapacitor/blob/master/examples/error_percent/\">here</a> that create test data and backfill the data using Kapacitor.</p> <h3 id=\"stream-method\">Stream method</h3> <p>To do the same for the streaming case the TICKscript is very similar:</p> <pre data-language=\"javascript\">// Get errors stream data\nvar errors = stream\n    |from()\n        .measurement('errors')\n        .groupBy(*)\n\n// Get views stream data\nvar views = stream\n    |from()\n        .measurement('views')\n        .groupBy(*)\n\n// Join errors and views\nerrors\n    |join(views)\n        .as('errors', 'views')\n    //Calculate percentage\n    |eval(lambda: \"errors.value\" / \"views.value\")\n        // Give the resulting field a name\n        .as('error_percent')\n    |influxDBOut()\n        .database('pages')\n        .measurement('error_percent')\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/examples/join_backfill/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/examples/join_backfill/</a>\n  </p>\n</div>\n","kapacitor/v0.13/about_the_project/index":"<h1>About the project</h1>     <p>Kapacitor is open source and we welcome contributions from the community. For information on what you need to build, test etc. the code see <a href=\"https://github.com/influxdb/kapacitor/blob/master/CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p> <p>We will also ask you to sign our CLA which can be found <a href=\"http://influxdb.com/community/cla.html\">here</a></p> <p>If you want Kapacitor to be able to output to you own endpoint see this <a href=\"custom_output/index\">How To</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/about_the_project/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/about_the_project/</a>\n  </p>\n</div>\n","kapacitor/v0.13/examples/anomaly_detection/index":"<h1>Custom Anomaly Detection</h1>     <p>Everyone has their own anomaly detection algorithm, so we have built Kapacitor to integrate easily with which ever algorithm fits your domain. Kapacitor calls these custom algorithms UDFs for User Defined Functions. This guide will walk through the necessary steps for writing and using your own UDFs within Kapacitor.</p> <p>If you haven’t already, we recommend following the <a href=\"../../introduction/getting_started/index\">getting started guide</a> for Kapacitor prior to continuing.</p> <h2 id=\"3d-printing\">3D Printing</h2> <p>If you own or have recently purchased a 3D printer, you may know that 3D printing requires the environment to be at certain temperatures in order to ensure quality prints. Prints can also take a long time (some can take more than 24 hours), so you can’t just watch the temperature graphs the whole time to make sure the print is going well. Also, if a print goes bad early, you want to make sure and stop it so that you can restart it, and not waste materials on continuing a bad print.</p> <p>Due to the physical limitations of 3D printing, the printer software is typically designed to keep the temperatures within certain tolerances. For the sake of argument, let’s say that you don’t trust the software to do it’s job (or want to create your own), and want to be alerted when the temperature reaches an abnormal level.</p> <p>There are three temperatures when it comes to 3D printing:</p> <ol> <li>The temperature of the hot end (where the plastic is melted before being printed).</li> <li>The temperature of the bed (where the part is being printed).</li> <li>The temperature of the ambient air (the air around the printer).</li> </ol> <p>All three of these temperatures affect the quality of the print (some being more important than others), but we want to make sure and track all of them.</p> <p>To keep our anomaly detection algorithm simple, let’s compute a <code>p-value</code> for each window of data we receive, and then emit a single data point with that <code>p-value</code>. To compute the <code>p-value</code>, we will use <a href=\"https://en.wikipedia.org/wiki/Welch%27s_t_test\">Welch’s t-test</a>. For a null hypothesis, we will state that a new window is from the same population as the historical windows. If the <code>p-value</code> drops low enough, we can reject the null hypothesis and conclude that the window must be from something different than the historical data population, or <em>an anomaly</em>. This is an oversimplified approach, but we are learning how to write UDFs, not statistics.</p> <h2 id=\"writing-a-udf\">Writing a UDF</h2> <p>Now that we have an idea of what we want to do, let’s understand how Kapacitor wants to communicate with our process. From the <a href=\"https://github.com/influxdata/kapacitor/tree/master/udf/agent\">UDF README</a> we learn that Kapacitor will spawn a process called an <code>agent</code>. The <code>agent</code> is responsible for describing what options it has, and then initializing itself with a set of options. As data is received by the UDF, the <code>agent</code> performs its computation and then returns the resulting data to Kapacitor. All of this communication occurs over STDIN and STDOUT using protocol buffers. As of this writing, Kapacitor has agents implemented in Go and Python that take care of the communication details and expose an interface for doing the actual work. For this guide, we will be using the Python agent.</p> <h3 id=\"the-handler-interface\">The Handler Interface</h3> <p>Here is the Python handler interface for the agent:</p> <pre data-language=\"python\"># The Agent calls the appropriate methods on the Handler as requests are read off STDIN.\n#\n# Throwing an exception will cause the Agent to stop and an ErrorResponse to be sent.\n# Some *Response objects (like SnapshotResponse) allow for returning their own error within the object itself.\n# These types of errors will not stop the Agent and Kapacitor will deal with them appropriately.\n#\n# The Handler is called from a single thread, meaning methods will not be called concurrently.\n#\n# To write Points/Batches back to the Agent/Kapacitor use the Agent.write_response method, which is thread safe.\nclass Handler(object):\n    def info(self):\n        pass\n    def init(self, init_req):\n        pass\n    def snapshot(self):\n        pass\n    def restore(self, restore_req):\n        pass\n    def begin_batch(self):\n        pass\n    def point(self):\n        pass\n    def end_batch(self, end_req):\n        pass\n</pre> <h3 id=\"the-info-method\">The Info Method</h3> <p>Let’s start with the <code>info</code> method. When Kapacitor starts up it will call <code>info</code> and expect in return some information about how this UDF behaves. Specifically, Kapacitor expects the kind of edge the UDF wants and provides.</p> <blockquote> <p><strong>Remember</strong>: within Kapacitor, data is transported in streams or batches, so the UDF must declare what it expects.</p> </blockquote> <p>In addition, UDFs can accept certain options so that they are individually configurable. The <code>info</code> response can contain a list of options, their names, and expected arguments.</p> <p>For our example UDF, we need to know three things:</p> <ol> <li>The field to operate on.</li> <li>The size of the historical window to keep.</li> <li>The significance level or <code>alpha</code> being used.</li> </ol> <p>Below we have the implementation of the <code>info</code> method for our handler that defines the edge types and options available:</p> <pre data-language=\"python\">...\n    def info(self):\n        \"\"\"\n        Respond with which type of edges we want/provide and any options we have.\n        \"\"\"\n        response = udf_pb2.Response()\n\n        # We will consume batch edges aka windows of data.\n        response.info.wants = udf_pb2.BATCH\n        # We will produce single points of data aka stream.\n        response.info.provides = udf_pb2.STREAM\n\n        # Here we can define options for the UDF.\n        # Define which field we should process.\n        response.info.options['field'].valueTypes.append(udf_pb2.STRING)\n\n        # Since we will be computing a moving average let's make the size configurable.\n        # Define an option 'size' that takes one integer argument.\n        response.info.options['size'].valueTypes.append(udf_pb2.INT)\n\n        # We need to know the alpha level so that we can ignore bad windows.\n        # Define an option 'alpha' that takes one double valued argument.\n        response.info.options['alpha'].valueTypes.append(udf_pb2.DOUBLE)\n\n        return response\n...\n</pre> <p>When Kapacitor starts, it will spawn our UDF process and request the <code>info</code> data and then shutdown the process. Kapacitor will remember this information for each UDF. This way, Kapacitor can understand the available options for a given UDF before its executed inside of a task.</p> <h3 id=\"the-init-method\">The Init Method</h3> <p>Next let’s implement the <code>init</code> method, which is called once the task starts executing. The <code>init</code> method receives a list of chosen options, which are then used to configure the handler appropriately. In response, we indicate whether the <code>init</code> request was successful, and, if not, any error messages if the options were invalid.</p> <pre data-language=\"python\">...\n    def init(self, init_req):\n        \"\"\"\n        Given a list of options initialize this instance of the handler\n        \"\"\"\n        success = True\n        msg = ''\n        size = 0\n        for opt in init_req.options:\n            if opt.name == 'field':\n                self._field = opt.values[0].stringValue\n            elif opt.name == 'size':\n                size = opt.values[0].intValue\n            elif opt.name == 'alpha':\n                self._alpha = opt.values[0].doubleValue\n\n        if size &lt;= 1:\n            success = False\n            msg += ' must supply window size &gt; 1'\n        if self._field == '':\n            success = False\n            msg += ' must supply a field name'\n        if self._alpha == 0:\n            success = False\n            msg += ' must supply an alpha value'\n\n        # Initialize our historical window\n        # We will define MovingStats in the next step\n        self._history = MovingStats(size)\n\n        response = udf_pb2.Response()\n        response.init.success = success\n        response.init.error = msg[1:]\n\n        return response\n...\n</pre> <p>When a task starts, Kapacitor spawns a new process for the UDF and calls <code>init</code>, passing any specified options from the TICKscript. Once initialized, the process will remain running and Kapacitor will begin sending data as it arrives.</p> <h3 id=\"the-batch-and-point-methods\">The Batch and Point Methods</h3> <p>Our task wants a <code>batch</code> edge, meaning it expects to get data in batches or windows. To send a batch of data to the UDF process, Kapacitor first calls the <code>begin_batch</code> method, which indicates that all subsequent points belong to a batch. Once the batch is complete, the <code>end_batch</code> method is called with some metadata about the batch.</p> <p>At a high level, this is what our UDF code will do for each of the <code>begin_batch</code>, <code>point</code>, and <code>end_batch</code> calls:</p> <ul> <li>\n<code>begin_batch</code> – mark the start of a new batch and initialize a structure for it</li> <li>\n<code>point</code> – store the point</li> <li>\n<code>end_batch</code> – perform the <code>t-test</code> and then update the historical data</li> </ul> <h3 id=\"the-complete-udf-script\">The Complete UDF Script</h3> <p>What follows is the complete UDF implementation with our <code>info</code>, <code>init</code>, and batching methods (as well as everything else we need).</p> <pre data-language=\"python\">\nfrom agent import Agent, Handler\nfrom scipy import stats\nimport math\nimport udf_pb2\nimport sys\n\nclass TTestHandler(Handler):\n    \"\"\"\n    Keep a rolling window of historically normal data\n    When a new window arrives use a two-sided t-test to determine\n    if the new window is statistically significantly different.\n    \"\"\"\n    def __init__(self, agent):\n        self._agent = agent\n\n        self._field = ''\n        self._history = None\n\n        self._batch = None\n\n        self._alpha = 0.0\n\n    def info(self):\n        \"\"\"\n        Respond with which type of edges we want/provide and any options we have.\n        \"\"\"\n        response = udf_pb2.Response()\n        # We will consume batch edges aka windows of data.\n        response.info.wants = udf_pb2.BATCH\n        # We will produce single points of data aka stream.\n        response.info.provides = udf_pb2.STREAM\n\n        # Here we can define options for the UDF.\n        # Define which field we should process\n        response.info.options['field'].valueTypes.append(udf_pb2.STRING)\n\n        # Since we will be computing a moving average let's make the size configurable.\n        # Define an option 'size' that takes one integer argument.\n        response.info.options['size'].valueTypes.append(udf_pb2.INT)\n\n        # We need to know the alpha level so that we can ignore bad windows\n        # Define an option 'alpha' that takes one double argument.\n        response.info.options['alpha'].valueTypes.append(udf_pb2.DOUBLE)\n\n        return response\n\n    def init(self, init_req):\n        \"\"\"\n        Given a list of options initialize this instance of the handler\n        \"\"\"\n        success = True\n        msg = ''\n        size = 0\n        for opt in init_req.options:\n            if opt.name == 'field':\n                self._field = opt.values[0].stringValue\n            elif opt.name == 'size':\n                size = opt.values[0].intValue\n            elif opt.name == 'alpha':\n                self._alpha = opt.values[0].doubleValue\n\n        if size &lt;= 1:\n            success = False\n            msg += ' must supply window size &gt; 1'\n        if self._field == '':\n            success = False\n            msg += ' must supply a field name'\n        if self._alpha == 0:\n            success = False\n            msg += ' must supply an alpha value'\n\n        # Initialize our historical window\n        self._history = MovingStats(size)\n\n        response = udf_pb2.Response()\n        response.init.success = success\n        response.init.error = msg[1:]\n\n        return response\n\n    def begin_batch(self):\n        # create new window for batch\n        self._batch = MovingStats(-1)\n\n    def point(self, point):\n        self._batch.update(point.fieldsDouble[self._field])\n\n    def end_batch(self, batch_meta):\n        pvalue = 1.0\n        if self._history.n != 0:\n            # Perform Welch's t test\n            t, pvalue = stats.ttest_ind_from_stats(\n                    self._history.mean, self._history.stddev(), self._history.n,\n                    self._batch.mean, self._batch.stddev(), self._batch.n,\n                    equal_var=False)\n\n\n            # Send pvalue point back to Kapacitor\n            response = udf_pb2.Response()\n            response.point.time = batch_meta.tmax\n            response.point.name = batch_meta.name\n            response.point.group = batch_meta.group\n            response.point.tags.update(batch_meta.tags)\n            response.point.fieldsDouble[\"t\"] = t\n            response.point.fieldsDouble[\"pvalue\"] = pvalue\n            self._agent.write_response(response)\n\n        # Update historical stats with batch, but only if it was normal.\n        if pvalue &gt; self._alpha:\n            for value in self._batch._window:\n                self._history.update(value)\n\n\nclass MovingStats(object):\n    \"\"\"\n    Calculate the moving mean and variance of a window.\n    Uses Welford's Algorithm.\n    \"\"\"\n    def __init__(self, size):\n        \"\"\"\n        Create new MovingStats object.\n        Size can be -1, infinite size or &gt; 1 meaning static size\n        \"\"\"\n        self.size = size\n        if not (self.size == -1 or self.size &gt; 1):\n            raise Exception(\"size must be -1 or &gt; 1\")\n\n\n        self._window = []\n        self.n = 0.0\n        self.mean = 0.0\n        self._s = 0.0\n\n    def stddev(self):\n        \"\"\"\n        Return the standard deviation\n        \"\"\"\n        if self.n == 1:\n            return 0.0\n        return math.sqrt(self._s / (self.n - 1))\n\n    def update(self, value):\n\n        # update stats for new value\n        self.n += 1.0\n        diff = (value - self.mean)\n        self.mean += diff / self.n\n        self._s += diff * (value - self.mean)\n\n        if self.n == self.size + 1:\n            # update stats for removing old value\n            old = self._window.pop(0)\n            oldM = (self.n * self.mean - old)/(self.n - 1)\n            self._s -= (old - self.mean) * (old - oldM)\n            self.mean = oldM\n            self.n -= 1\n\n        self._window.append(value)\n\nif __name__ == '__main__':\n    # Create an agent\n    agent = Agent()\n\n    # Create a handler and pass it an agent so it can write points\n    h = TTestHandler(agent)\n\n    # Set the handler on the agent\n    agent.handler = h\n\n    # Anything printed to STDERR from a UDF process gets captured into the Kapacitor logs.\n    print &gt;&gt; sys.stderr, \"Starting agent for TTestHandler\"\n    agent.start()\n    agent.wait()\n    print &gt;&gt; sys.stderr, \"Agent finished\"\n\n</pre> <p>That was a lot, but now we are ready to configure Kapacitor to run our code. Create a scratch dir for working through the rest of this guide:</p> <pre data-language=\"bash\">mkdir /tmp/kapacitor_udf\ncd /tmp/kapacitor_udf\n</pre> <p>Save the above UDF python script into <code>/tmp/kapacitor_udf/ttest.py</code>.</p> <h3 id=\"configuring-kapacitor-for-our-udf\">Configuring Kapacitor for our UDF</h3> <p>Add this snippet to your Kapacitor configuration file (typically located at <code>/etc/kapacitor/kapacitor.conf</code>):</p> <pre>[udf]\n[udf.functions]\n    [udf.functions.tTest]\n        # Run python\n        prog = \"/usr/bin/python2\"\n        # Pass args to python\n        # -u for unbuffered STDIN and STDOUT\n        # and the path to the script\n        args = [\"-u\", \"/tmp/kapacitor_udf/ttest.py\"]\n        # If the python process is unresponsive for 10s kill it\n        timeout = \"10s\"\n        # Define env vars for the process, in this case the PYTHONPATH\n        [udf.functions.tTest.env]\n            PYTHONPATH = \"/tmp/kapacitor_udf/kapacitor/udf/agent/py\"\n</pre> <p>In the configuration we called the function <code>tTest</code>. That is also how we will reference it in the TICKscript.</p> <p>Notice that our Python script imported the <code>Agent</code> object, and we set the <code>PYTHONPATH</code> in the configuration. Clone the Kapacitor source into the tmp dir so we can point the <code>PYTHONPATH</code> at the necessary python code. This is typically overkill since it’s just two Python files, but it makes it easy to follow:</p> <pre>git clone https://github.com/influxdata/kapacitor.git /tmp/kapacitor_udf/kapacitor\n</pre> <h3 id=\"running-kapacitor-with-the-udf\">Running Kapacitor with the UDF</h3> <p>Restart the Kapacitor daemon to make sure everything is configured correctly:</p> <pre data-language=\"bash\">service kapacitor restart\n</pre> <p>Check the logs (<code>/var/log/kapacitor/</code>) to make sure you see a <em>Listening for signals</em> line and that no errors occurred. If you don’t see the line, it’s because the UDF process is hung and not responding. It should be killed after a timeout, so give it a moment to stop properly. Once stopped, you can fix any errors and try again.</p> <h3 id=\"the-tickscript\">The TICKscript</h3> <p>If everything was started correctly, then it’s time to write our TICKscript to use the <code>tTest</code> UDF method:</p> <pre data-language=\"javascript\">// This TICKscript monitors the three temperatures for a 3d printing job,\n// and triggers alerts if the temperatures start to experience abnormal behavior.\n\n// Define our desired significance level.\nvar alpha = 0.001\n\n// Select the temperatures measurements\nvar data = stream\n    |from()\n        .measurement('temperatures')\n    |window()\n        .period(5m)\n        .every(5m)\n\ndata\n    //Run our tTest UDF on the hotend temperature\n    @tTest()\n        // specify the hotend field\n        .field('hotend')\n        // Keep a 1h rolling window\n        .size(3600)\n        // pass in the alpha value\n        .alpha(alpha)\n    |alert()\n        .id('hotend')\n        .crit(lambda: \"pvalue\" &lt; alpha)\n        .log('/tmp/kapacitor_udf/hotend_failure.log')\n\n// Do the same for the bed and air temperature.\ndata\n    @tTest()\n        .field('bed')\n        .size(3600)\n        .alpha(alpha)\n    |alert()\n        .id('bed')\n        .crit(lambda: \"pvalue\" &lt; alpha)\n        .log('/tmp/kapacitor_udf/bed_failure.log')\n\ndata\n    @tTest()\n        .field('air')\n        .size(3600)\n        .alpha(alpha)\n    |alert()\n        .id('air')\n        .crit(lambda: \"pvalue\" &lt; alpha)\n        .log('/tmp/kapacitor_udf/air_failure.log')\n\n</pre> <p>Notice that we have called <code>tTest</code> three times. This means that Kapacitor will spawn three different Python processes and pass the respective init option to each one.</p> <p>Save this script as <code>/tmp/kapacitor_udf/print_temps.tick</code> and define the Kapacitor task:</p> <pre data-language=\"bash\">kapacitor define print_temps -type stream -dbrp printer.default -tick print_temps.tick\n</pre> <h3 id=\"generating-test-data\">Generating Test Data</h3> <p>To simulate our printer for testing, we will write a simple Python script to generate temperatures. This script generates random temperatures that are normally distributed around a target temperature. At specified times, the variation and offset of the temperatures changes, creating an anomaly.</p> <blockquote> <p>Don’t worry too much about the details here. It would be much better to use real data for testing our TICKscript and UDF, but this is faster (and much cheaper than a 3D printer).</p> </blockquote> <pre data-language=\"python\">#!/usr/bin/python2\n\nfrom numpy import random\nfrom datetime import timedelta, datetime\nimport sys\nimport time\nimport requests\n\n\n# Target temperatures in C\nhotend_t = 220\nbed_t = 90\nair_t = 70\n\n# Connection info\nwrite_url = 'http://localhost:9092/write?db=printer&amp;rp=default&amp;precision=s'\nmeasurement = 'temperatures'\n\ndef temp(target, sigma):\n    \"\"\"\n    Pick a random temperature from a normal distribution\n    centered on target temperature.\n    \"\"\"\n    return random.normal(target, sigma)\n\ndef main():\n    hotend_sigma = 0\n    bed_sigma = 0\n    air_sigma = 0\n    hotend_offset = 0\n    bed_offset = 0\n    air_offset = 0\n\n    # Define some anomalies by changing sigma at certain times\n    # list of sigma values to start at a specified iteration\n    hotend_anomalies =[\n        (0, 0.5, 0), # normal sigma\n        (3600, 3.0, -1.5), # at one hour the hotend goes bad\n        (3900, 0.5, 0), # 5 minutes later recovers\n    ]\n    bed_anomalies =[\n        (0, 1.0, 0), # normal sigma\n        (28800, 5.0, 2.0), # at 8 hours the bed goes bad\n        (29700, 1.0, 0), # 15 minutes later recovers\n    ]\n    air_anomalies = [\n        (0, 3.0, 0), # normal sigma\n        (10800, 5.0, 0), # at 3 hours air starts to fluctuate more\n        (43200, 15.0, -5.0), # at 12 hours air goes really bad\n        (45000, 5.0, 0), # 30 minutes later recovers\n        (72000, 3.0, 0), # at 20 hours goes back to normal\n    ]\n\n    # Start from 2016-01-01 00:00:00 UTC\n    # This makes it easy to reason about the data later\n    now = datetime(2016, 1, 1)\n    second = timedelta(seconds=1)\n    epoch = datetime(1970,1,1)\n\n    # 24 hours of temperatures once per second\n    points = []\n    for i in range(60*60*24+2):\n        # update sigma values\n        if len(hotend_anomalies) &gt; 0 and i == hotend_anomalies[0][0]:\n            hotend_sigma = hotend_anomalies[0][1]\n            hotend_offset = hotend_anomalies[0][2]\n            hotend_anomalies = hotend_anomalies[1:]\n\n        if len(bed_anomalies) &gt; 0 and i == bed_anomalies[0][0]:\n            bed_sigma = bed_anomalies[0][1]\n            bed_offset = bed_anomalies[0][2]\n            bed_anomalies = bed_anomalies[1:]\n\n        if len(air_anomalies) &gt; 0 and i == air_anomalies[0][0]:\n            air_sigma = air_anomalies[0][1]\n            air_offset = air_anomalies[0][2]\n            air_anomalies = air_anomalies[1:]\n\n        # generate temps\n        hotend = temp(hotend_t+hotend_offset, hotend_sigma)\n        bed = temp(bed_t+bed_offset, bed_sigma)\n        air = temp(air_t+air_offset, air_sigma)\n        points.append(\"%s hotend=%f,bed=%f,air=%f %d\" % (\n            measurement,\n            hotend,\n            bed,\n            air,\n            (now - epoch).total_seconds(),\n        ))\n        now += second\n\n    # Write data to Kapacitor\n    r = requests.post(write_url, data='\\n'.join(points))\n    if r.status_code != 204:\n        print &gt;&gt; sys.stderr, r.text\n        return 1\n    return 0\n\nif __name__ == '__main__':\n    exit(main())\n\n</pre> <p>Save the above script as <code>/tmp/kapacitor_udf/printer_data.py</code>.</p> <blockquote> <p>This Python script has two Python dependencies: <code>requests</code> and <code>numpy</code>. They can easily be installed via <code>pip</code> or your package manager.</p> </blockquote> <p>At this point we have a task ready to go, and a script to generate some fake data with anomalies. Now we can create a recording of our fake data so that we can easily iterate on the task:</p> <pre data-language=\"sh\"># Start the recording in the background\nkapacitor record stream -task print_temps -duration 24h -no-wait\n# Grab the ID from the output and store it in a var\nrid=7bd3ced5-5e95-4a67-a0e1-f00860b1af47\n# Run our python script to generate data\nchmod +x ./printer_data.py\n./printer_data.py\n</pre> <p>We can verify it worked by listing information about the recording. Our recording came out to <code>1.6MB</code>, so yours should come out somewhere close to that:</p> <pre>$ kapacitor list recordings $rid\nID                                      Type    Status    Size      Date\n7bd3ced5-5e95-4a67-a0e1-f00860b1af47    stream  finished  1.6 MB    04 May 16 11:44 MDT\n</pre> <h3 id=\"detecting-anomalies\">Detecting Anomalies</h3> <p>Finally, let’s run the play against our task and see how it works:</p> <pre>kapacitor replay -task print_temps -recording $rid -rec-time\n</pre> <p>Check the various log files to see if the algorithm caught the anomalies:</p> <pre>cat /tmp/kapacitor_udf/{hotend,bed,air}_failure.log\n</pre> <p>Based on the <code>printer_data.py</code> script above, there should be anomalies at:</p> <ul> <li>1hr – hotend</li> <li>8hr – bed</li> <li>12hr – air</li> </ul> <p>There may be some false positives as well, but, since we want this to work with real data (not our nice clean fake data), it doesn’t help much to tweak it at this point.</p> <p>Well, there we have it. We can now get alerts when the temperatures for our prints deviates from the norm. Hopefully you now have a better understanding of how Kapacitor UDFs work, and have a good working example as a launching point into further work with UDFS.</p> <p>The framework is in place, now go plug in a real anomaly detection algorithm that works for your domain!</p> <h2 id=\"extending-this-example\">Extending This Example</h2> <p>There are a few things that we have left as exercises to the reader:</p> <ol> <li><p>Snapshot/Restore – Kapacitor will regularly snapshot the state of your UDF process so that it can be restored if the process is restarted. The examples <a href=\"https://github.com/influxdata/kapacitor/tree/master/udf/agent/examples/\">here</a> have implementations for the <code>snapshot</code> and <code>restore</code> methods. Implement them for the <code>TTestHandler</code> handler as an exercise.</p></li> <li><p>Change the algorithm from a t-test to something more fitting for your domain. Both <code>numpy</code> and <code>scipy</code> have a wealth of algorithms.</p></li> <li><p>The options returned by the <code>info</code> request can contain multiple arguments. Modify the <code>field</code> option to accept three field names and change the <code>TTestHandler</code> to maintain historical data and batches for each field instead of just the one. That way only one ttest.py process needs to be running.</p></li> </ol><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/examples/anomaly_detection/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/examples/anomaly_detection/</a>\n  </p>\n</div>\n","kapacitor/v0.13/examples/continuous_queries/index":"<h1>Kapacitor as a Continous Query engine</h1>     <p>Kapacitor can be used to do the same work as Continuous Queries in InfluxDB. Today we are going to explore reasons to use one over the other and the basics of using Kapacitor for CQ type workloads.</p> <h2 id=\"an-example\">An Example</h2> <p>First, lets take a simple CQ and rewrite it as a Kapacitor TICKscript.</p> <p>Here is a CQ that computes the mean of the <code>cpu.usage_idle</code> every 5m and stores it in the new measurement <code>mean_cpu_idle</code>.</p> <pre>CREATE CONTINUOUS QUERY cpu_idle_mean ON telegraf BEGIN SELECT mean(\"usage_idle\") as usage_idle INTO mean_cpu_idle FROM cpu GROUP BY time(5m),* END\n</pre> <p>To do the same with Kapacitor here is a streaming TICKscript.</p> <pre data-language=\"javascript\">stream\n    |from()\n        .database('telegraf')\n        .measurement('cpu')\n        .groupBy(*)\n    |window()\n        .period(5m)\n        .every(5m)\n        .align()\n    |mean('usage_idle')\n        .as('usage_idle')\n    |influxDBOut()\n        .database('telegratelegraff')\n        .retentionPolicy('default')\n        .measurement('mean_cpu_idle')\n        .precision('s')\n</pre> <p>The same thing can also be done as a batch task in Kapacitor.</p> <pre data-language=\"javascript\">batch\n    |query('SELECT mean(usage_idle) as usage_idle FROM \"telegraf\".\"default\".cpu')\n        .period(5m)\n        .every(5m)\n        .groupBy(*)\n    |influxDBOut()\n        .database('telegraf')\n        .retentionPolicy('default')\n        .measurement('mean_cpu_idle')\n        .precision('s')\n</pre> <p>All three of these methods will produce the same results.</p> <h2 id=\"questions\">Questions</h2> <p>At this point there are a few questions we should answer:</p> <ol> <li>When should we use Kapacitor instead of CQs?</li> <li>When should we use stream tasks vs batch tasks in Kapacitor?</li> </ol> <h3 id=\"when-should-we-use-kapacitor-instead-of-cqs\">When should we use Kapacitor instead of CQs?</h3> <p>There are a few reasons to use Kapacitor instead of CQs.</p> <ul> <li>You are performing a significant number of CQs and want to isolate the work load. By using Kapacitor to perform the aggregations InfluxDB’s performance profile can remain more stable and isolated from Kapacitor’s.</li> <li>You need to do more than just perform a query, for example maybe you only want to store only outliers from an aggregation instead of all of them. Kapacitor can do significantly more with the data than CQs so you have more flexibility in transforming your data.</li> </ul> <p>There are a few use cases where using CQs almost always makes sense.</p> <ul> <li>Performing downsampling for retention policies. This is what CQs are designed for and do well. No need to add another moving piece (i.e. Kapacitor) to your infrastructure if you do not need it. Keep it simple.</li> <li>You only have a handful of CQs, again keep it simple, do not add more moving parts to your setup unless you need it.</li> </ul> <h3 id=\"when-should-we-use-stream-tasks-vs-batch-tasks-in-kapacitor\">When should we use stream tasks vs batch tasks in Kapacitor?</h3> <p>Basically the answer boils down to two things, the available RAM and time period being used.</p> <p>A stream task will have to keep all data in RAM for the specified period. If this period is too long for the available RAM then you will first need to store the data in InfluxDB and then query using a batch task.</p> <p>A stream task does have one slight advantage in that since its watching the stream of data it understands time by the timestamps on the data. As such there are no race conditions for whether a given point will make it into a window or not. If you are using a batch task it is still possible for a point to arrive late and be missed in a window.</p> <h2 id=\"another-example\">Another Example</h2> <p>Create a continuous query to down sample across retention policies.</p> <pre>CREATE CONTINUOUS QUERY cpu_idle_median ON telegraf BEGIN SELECT median(\"usage_idle\") as usage_idle INTO \"telegraf\".\"sampled_5m\".\"median_cpu_idle\" FROM \"telegraf\".\"default\".\"cpu\" GROUP BY time(5m),* END\n</pre> <p>The stream TICKscript:</p> <pre data-language=\"javascript\">stream\n    |from()\n        .database('telegraf')\n        .retentionPolicy('default')\n        .measurement('cpu')\n        .groupBy(*)\n    |window()\n        .period(5m)\n        .every(5m)\n        .align()\n    |median('usage_idle')\n        .as('usage_idle')\n    |influxDBOut()\n        .database('telegraf')\n        .retentionPolicy('sampled_5m')\n        .measurement('median_cpu_idle')\n        .precision('s')\n</pre> <p>And the batch TICKscript:</p> <pre data-language=\"javascript\">batch\n    |query('SELECT median(usage_idle) as usage_idle FROM \"telegraf\".\"default\".\"cpu\"')\n        .period(5m)\n        .every(5m)\n        .groupBy(*)\n    |influxDBOut()\n        .database('telegraf')\n        .retentionPolicy('sampled_5m')\n        .measurement('median_cpu_idle')\n        .precision('s')\n</pre> <h2 id=\"summary\">Summary</h2> <p>Kapacitor is a powerful tool, if you need more power use it. If not keep using CQs until you do. For more information and help writing TICKscripts from InfluxQL queries take a looks at these <a href=\"https://docs.influxdata.com/kapacitor/latest/nodes/influx_q_l_node/\">docs</a> on the InfluxQL node in Kapacitor. Every function available in the InfluxDB query language is available in Kapacitor, so you can convert any query into a Kapacitor TICKscript.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/examples/continuous_queries/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/examples/continuous_queries/</a>\n  </p>\n</div>\n","kapacitor/v0.13/administration/upgrading/index":"<h1>Upgrading to Kapacitor v0.13</h1>     <p>There are a few breaking changes from v0.12 that may require some work to upgrade from a v0.12 instance. These changes are:</p> <ul> <li><a href=\"#changes-to-storage\">Changes to Storage</a></li> <li><a href=\"#changes-to-http-api\">Changes to HTTP API</a></li> <li><a href=\"#changes-to-cli\">Changes to CLI</a></li> </ul> <blockquote> <p>NOTE: You must now use the correct <code>.</code>, <code>|</code> and <code>@</code> operators for property, chain and UDF methods respectively. See <a href=\"https://docs.influxdata.com/kapacitor/v0.12/introduction/upgrading/#tickscript-chain-operator\">this</a> for more details.</p> </blockquote> <h2 id=\"changes-to-storage\">Changes to Storage</h2> <p>Changes to how and where task data is store have been made. In order to safely upgrade to version 0.13 you need to follow these steps:</p> <ol> <li>Upgrade InfluxDB to version 0.13 first.</li> <li>Update all TICKscripts to use the new <code>|</code> and <code>@</code> operators. Once Kapacitor no longer issues any <code>DEPRECATION</code> warnings you are ready to begin the upgrade. The upgrade will work without this step but tasks using the old syntax cannot be enabled, until modified to use the new syntax.</li> <li>Upgrade the Kapacitor binary/package.</li> <li>\n<p>Configure new database location. By default the location <code>/var/lib/kapacitor/kapacitor.db</code> is chosen for package installs or <code>./kapacitor.db</code> for manual installs. Do <strong>not</strong> remove the configuration for the location of the old task.db database file since it is still needed to do the migration.</p> <pre>[storage]\nboltdb = \"/var/lib/kapacitor/kapacitor.db\"\n</pre>\n</li> <li><p>Restart Kapacitor. At this point Kapacitor will migrate all existing data to the new database file. If any errors occur Kapacitor will log them and fail to startup. This way if Kapacitor starts up you can be sure the migration was a success and can continue normal operation. The old database is opened in read only mode so that existing data cannot be corrupted. Its recommended to start Kapacitor in debug logging mode for the migration so you can follow the details of the migration process.</p></li> </ol> <p>At this point you may remove the configuration for the old <code>task</code> <code>dir</code> and restart Kapacitor to ensure everything is working. Kapacitor will attempt the migration on every startup while the old configuration and db file exist, but will skip any data that was already migrated.</p> <h2 id=\"changes-to-http-api\">Changes to HTTP API</h2> <p>With this release the API has been updated to what we believe will be the stable version for a 1.0 release. Small changes may still be made but the significant work to create a RESTful HTTP API is complete. Many breaking changes introduced, see the <a href=\"http://github.com/influxdata/kapacitor/blob/master/client/API.md\">client/API.md</a> doc for details on how the API works now.</p> <h2 id=\"changes-to-cli\">Changes to CLI</h2> <p>Along with the API changes, breaking changes where also made to the <code>kapacitor</code> CLI command. Here is a break down of the CLI changes:</p> <ul> <li>Every thing has an ID now: tasks, recordings, even replays. The <code>name</code> used before to define a task is now its <code>ID</code>. As such instead of using <code>-name</code> and <code>-id</code> to refer to tasks and recordings, the flags have been changed to <code>-task</code> and <code>-recording</code> accordingly.</li> <li>Replays can be listed and deleted like tasks and recordings.</li> <li>Replays default to <code>fast</code> clock mode.</li> <li>The record and replay commands now have a <code>-no-wait</code> option to start but not wait for the recording/replay to complete.</li> <li>Listing recordings and replays displays the status of the respective action.</li> <li>Record and Replay command now have an optional flag <code>-replay-id</code>/<code>-recording-id</code> to specify the ID of the replay or recording. If not set then a random ID will be chosen like the previous behavior.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/administration/upgrading/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/administration/upgrading/</a>\n  </p>\n</div>\n","kapacitor/v0.13/about_the_project/custom_output/index":"<h1>How to contribute a new output to Kapacitor</h1>     <p>If you haven’t already check out <a href=\"https://github.com/influxdb/kapacitor/blob/master/CONTRIBUTING.md\">this</a> information to get started contributing.</p> <h2 id=\"the-goal\">The Goal</h2> <p>Add a new node to Kapacitor that can output data to a custom endpoint. For this guide lets say we want to output data to a fictitous in-house database called HouseDB.</p> <h2 id=\"overview\">Overview</h2> <p>Kapacitor processes data via a pipeline. A pipeline is formally a directed acyclic graph (DAG). The basic idea is that each node in the graph represents some form of processing on the data and each edge passes the data between nodes. In order to add a new type of node there are two components that need to be written:</p> <p>1. The API (TICKscript) for creating and configuring the node. 2. The implementation of the data processing step. In this case, the implementation of outputting the data to HouseDB.</p> <p>The code mirrors these requirements with two Go packages.</p> <p>1. <code>pipeline</code> – this package defines what types of nodes are available and how they are configured. 2. <code>kapacitor</code> – this package provides implementations of each of the nodes defined in the <code>pipeline</code> package.</p> <p>The reason for splitting out defining the node from the implementation of the node is to make the API (i.e. a TICKscript) clean and easy to follow.</p> <h3 id=\"updating-tickscript\">Updating TICKscript</h3> <p>First things first, we need to update TICKscript so that users can define a our new node. What should the TICKscript look like in order to send data to a HouseDB instance? To connect to a HouseDB instance we need both a URL and a database name, so we need a way to provide that information. How about this?</p> <pre data-language=\"javascript\">    node\n        |houseDBOut()\n            .url('house://housedb.example.com')\n            .database('metrics')\n</pre> <p>In order to update TICKscript to support those new methods we need to write a Go type that implements the <code>pipeline.Node</code> interface. The interface can be found <a href=\"https://github.com/influxdb/kapacitor/blob/master/pipeline/node.go\">here</a> as well as a complete implementation via the <code>pipeline.node</code> type. Since the implementation of the <code>Node</code> is done for us we just need to use it. First we need a name, <code>HouseDBOutNode</code> follows the convention. Let’s define a Go <code>struct</code> that will implement the interface via composition. Create a file called <code>housedb_out.go</code> in the <code>pipeline</code> directory with the contents below.</p> <pre data-language=\"go\">package pipeline\n\n// A HouseDBOutNode will take the incoming data stream and store it in a\n// HouseDB instance.\ntype HouseDBOutNode struct {\n    // Include the generic node implementation.\n    node\n}\n</pre> <p>Just like that we have a type in Go that implements the needed interface. In order to allow for the <code>.url</code> and <code>.database</code> methods we want, just define fields on the type with the same name. The first letter needs to capitalized so that it is exported the rest of the name should have the same case as the method name. TICKscript will take care of matching the case at runtime. Update the <code>housedb_out.go</code> file.</p> <pre data-language=\"go\">package pipeline\n\n// A HouseDBOutNode will take the incoming data stream and store it in a\n// HouseDB instance.\ntype HouseDBOutNode struct {\n    // Include the generic node implementation.\n    node\n\n    // URL for connecting to HouseDB\n    Url string\n\n    // Database name\n    Database string\n}\n</pre> <p>It’s important that the fields be exported since they will be consumed by the node in the <code>kapacitor</code> package.</p> <p>Next we need a consistent way to create a new instance of our node. But to do so we need to think about how this node connects to other nodes. Since we are an output node as far as Kapacitor is concerned we are the end of the pipeline and we do not provide any outbound edges. HouseDB is a flexible datastore and can store data in batches or as single data points, as a result we do not care what type of data the HouseDBOutNode node receives. With that in mind we can define a function to create a new HouseDBOutNode. Add this function to the end of the <code>housedb_out.go</code> file.</p> <pre data-language=\"go\">// Create a new HouseDBOutNode that accepts any edge type.\nfunc newHouseDBOutNode(wants EdgeType) *HouseDBOutNode {\n    return &amp;HouseDBOutNode{\n        node: node{\n            desc: \"housedb\",\n            wants: wants,\n            provides: NoEdge,\n        }\n    }\n}\n</pre> <p>By explicitly stating what types of edges the node <code>wants</code> and <code>provides</code> Kapacitor will do the necessary type checking so that users cannot define invalid pipelines.</p> <p>Finally we need to add a new <code>chaining method</code> so that users can connect HouseDBOutNodes to their existing pipelines. A <code>chaining method</code> is one that creates a new node and adds it as a child of the calling node, in effect the method chains nodes together. The <code>pipeline.chainnode</code> type contains the set of all methods that can be used for chaining nodes. By adding our method to that type automatically any other node can now chain with a HouseDBOutNode. Add this function to the end of the <code>pipeline/node.go</code> file.</p> <pre data-language=\"go\">// Create a new HouseDBOutNode as a child of the calling node.\nfunc (c *chainnode) HouseDBOut() *HouseDBOutNode {\n    h := newHouseDBOutNode(c.Provides())\n    c.linkChild(h)\n    return h\n}\n</pre> <p>That should do it. In review we now have defined all the necessary pieces so that TICKscripts can define HouseDBOutNodes.</p> <pre data-language=\"javascript\">    node\n        |houseDBOut() // added as a method to the 'chainnode' type\n            .url('house://housedb.example.com') // added as a field to the HouseDBOutNode\n            .database('metrics') // added as a field to the HouseDBOutNode\n</pre> <h3 id=\"implementing-the-housedb-output\">Implementing the HouseDB output</h3> <p>Now that a TICKscript can define our new output node we need to actually provide an implementation so that Kapacitor knows what to do with the node. Each node in the <code>pipeline</code> package has a node of the same name in the <code>kapacitor</code> package. Create a file called <code>housedb_out.go</code> and put it in the root of the repo. Put the contents below in the file.</p> <pre>package kapacitor\n\nimport (\n    \"github.com/influxdb/kapacitor/pipeline\"\n)\n\ntype HouseDBOutNode struct {\n    // Include the generic node implementation\n    node\n    // Keep a reference to the pipeline node\n    h    *pipeline.HouseDBOutNode\n}\n</pre> <p>The <code>kapacitor</code> package also defines an interface named <code>Node</code> and provides a default implementation via the <code>kapacitor.node</code> type. Again we use composition to implement the interface. Notice we also have a field that will contain an instance of the <code>pipeline.HouseDBOutNode</code> we just finished defining. This <code>pipeline.HouseDBOutNode</code> acts like a configuration struct telling the <code>kapacitor.HouseDBOutNode</code> what it needs to do its job.</p> <p>Now that we have a struct let’s define a function for creating an instance of our new struct. The <code>new*Node</code> methods in the <code>kapacitor</code> package follow a convention of:</p> <pre data-language=\"go\">func newNodeName(et *ExecutingTask, n *pipeline.NodeName) (*NodeName, error) {}\n</pre> <p>In our case we want to define a function called <code>newHouseDBOutNode</code> like so:</p> <pre data-language=\"go\">func newHouseDBOutNode(et *ExecutingTask, n *pipeline.HouseDBOutNode) (*HouseDBOutNode, error) {\n    h := &amp;HouseDBOutNode{\n        // pass in necessary fields to the 'node' struct\n        node: node{Node: n, et: et},\n        // Keep a reference to the pipeline.HouseDBOutNode\n        h: n,\n    }\n    // Set the function to be called when running the node\n    // more on this in a bit.\n    h.node.runF = h.runOut\n    return h\n}\n</pre> <p>Add the above method to the <code>housedb_out.go</code> file. In order for an instance of our node to be created we need to associate it with the node from the <code>pipeline</code> package. This can be done via the switch statement in the method <code>createNode</code> in the file <code>task.go</code>. Add a new case like so:</p> <pre data-language=\"go\">// Create a node from a given pipeline node.\nfunc (et *ExecutingTask) createNode(p pipeline.Node, l *log.Logger) (n Node, err error) {\n    switch t := p.(type) {\n    ...\n\tcase *pipeline.HouseDBOutNode:\n\t\tn, err = newHouseDBOutNode(et, t, l)\n    ...\n}\n</pre> <p>Now that we have associated our two types let’s get back to implementing the output code. Notice the line <code>h.node.runF = h.runOut</code> in the <code>newHouseDBOutNode</code> function. This line sets the method of the <code>kapacitor.HouseDBOutNode</code> that will be called when the node should start executing. We need to define the <code>runOut</code> method now. In the file <code>housedb_out.go</code> add this method:</p> <pre data-language=\"go\">func (h *HouseDBOutNode) runOut() error {\n    return nil\n}\n</pre> <p>With that the HouseDBOutNode is complete but obviously won’t do anything yet. As we learned earlier node communicate via edges. There is a Go type <code>kapacitor.Edge</code> that handles this communication. All that we need to do is read data off the edge and send it to HouseDB. Remember that we said that a HouseDBOutNode wants whatever edge type we give it? Because its flexible we will need to define how to read the data whether its stream or batch data. Lets update the <code>runOut</code> method with an appropriate switch statement.</p> <pre data-language=\"go\">func (h *HouseDBOutNode) runOut() error {\n    switch h.Wants() {\n    case pipeline.StreamEdge:\n        // Read stream data and send to HouseDB\n    case pipeline.BatchEdge:\n        // Read batch data and send to HouseDB\n    }\n    return nil\n}\n</pre> <p>The <code>node</code> type we included via composition in the HouseDBOutNode provides us with a list of edges in the field named <code>ins</code>. Since we can only have one parent the edge we are concerned about is only the 0th edge. The <code>Edge</code> type provides two methods:</p> <ul> <li>\n<code>NextPoint</code> for reading stream data.</li> <li>\n<code>NextBatch</code> for reading batch data.</li> </ul> <p>Update the cases in the switch statements to loop through all data.</p> <pre data-language=\"go\">func (h *HouseDBOutNode) runOut() error {\n    switch h.Wants() {\n    case pipeline.StreamEdge:\n        // Read stream data and send to HouseDB\n        for p, ok := h.ins[0].NextPoint(); ok; p, ok = h.ins[0].NextPoint() {\n            // Process a single point\n        }\n    case pipeline.BatchEdge:\n        // Read batch data and send to HouseDB\n        for b, ok := h.ins[0].NextBatch(); ok; b, ok = h.ins[0].NextBatch() {\n            // Process a batch of points\n        }\n    }\n    return nil\n}\n</pre> <p>To make it easy on ourselves we can convert the single point into a batch of just that point. Then all we need to do is write a function that takes a batch and writes it to HouseDB.</p> <pre data-language=\"go\">func (h *HouseDBOutNode) runOut() error {\n    switch h.Wants() {\n    case pipeline.StreamEdge:\n        // Read stream data and send to HouseDB\n        for p, ok := h.ins[0].NextPoint(); ok; p, ok = h.ins[0].NextPoint() {\n            // Turn the point into a batch with just one point.\n            batch := models.Batch{\n                Name:   p.Name,\n                Group:  p.Group,\n                Tags:   p.Tags,\n                Points: []models.TimeFields{{Time: p.Time, Fields: p.Fields}},\n            }\n            // Write the batch\n            err := h.write(batch)\n            if err != nil {\n                return err\n            }\n        }\n    case pipeline.BatchEdge:\n        // Read batch data and send to HouseDB\n        for b, ok := h.ins[0].NextBatch(); ok; b, ok = h.ins[0].NextBatch() {\n            // Write the batch\n            err := h.write(b)\n            if err != nil {\n                return err\n            }\n        }\n    }\n    return nil\n}\n\n// Write a batch of data to HouseDB\nfunc (h *HouseDBOutNode) write(batch models.Batch) error {\n    // Implement writing to HouseDB here...\n    return nil\n}\n</pre> <p>Once you have implemented the <code>write</code> method you are done, now as the data arrives it will be written to the specified HouseDB instance.</p> <h3 id=\"summary\">Summary</h3> <p>In summary we first wrote a node in the <code>pipeline</code> package (filepath: <code>pipeline/housedb_out.go</code>) that defines how the TICKscript API will work for sending data to a HouseDB instance. Then we wrote the implementation of that node in the <code>kapacitor</code> package (filepath: <code>housedb_out.go</code>). We also had to update two existing files <code>pipeline/node.go</code> to add a new chaining method, and <code>task.go</code> to associate the two types.</p> <p>Here are the complete file contents:</p> <p>pipeline/housedb_out.go:</p> <pre data-language=\"go\">package pipeline\n\n// A HouseDBOutNode will take the incoming data stream and store it in a\n// HouseDB instance.\ntype HouseDBOutNode struct {\n    // Include the generic node implementation.\n    node\n\n    // URL for connecting to HouseDB\n    Url string\n\n    // Database name\n    Database string\n}\n\n// Create a new HouseDBOutNode that accepts any edge type.\nfunc newHouseDBOutNode(wants EdgeType) *HouseDBOutNode {\n    return &amp;HouseDBOutNode{\n        node: node{\n            desc: \"housedb\",\n            wants: wants,\n            provides: NoEdge,\n        }\n    }\n}\n</pre> <p>housedb_out.go</p> <pre data-language=\"go\">package kapacitor\n\nimport (\n    \"github.com/influxdb/kapacitor/pipeline\"\n)\n\ntype HouseDBOutNode struct {\n    // Include the generic node implementation\n    node\n    // Keep a reference to the pipeline node\n    h    *pipeline.HouseDBOutNode\n}\n\nfunc newHouseDBOutNode(et *ExecutingTask, n *pipeline.HouseDBOutNode) (*HouseDBOutNode, error) {\n    h := &amp;HouseDBOutNode{\n        // pass in necessary fields to the 'node' struct\n        node: node{Node: n, et: et},\n        // Keep a reference to the pipeline.HouseDBOutNode\n        h: n,\n    }\n    // Set the function to be called when running the node\n    h.node.runF = h.runOut\n    return h\n}\n\nfunc (h *HouseDBOutNode) runOut() error {\n    switch h.Wants() {\n    case pipeline.StreamEdge:\n        // Read stream data and send to HouseDB\n        for p, ok := h.ins[0].NextPoint(); ok; p, ok = h.ins[0].NextPoint() {\n            // Turn the point into a batch with just one point.\n        batch := models.Batch{\n                Name:   p.Name,\n                Group:  p.Group,\n                Tags:   p.Tags,\n                Points: []models.TimeFields{{Time: p.Time, Fields: p.Fields}},\n            }\n            // Write the batch\n            err := h.write(batch)\n            if err != nil {\n                return err\n            }\n        }\n    case pipeline.BatchEdge:\n        // Read batch data and send to HouseDB\n        for b, ok := h.ins[0].NextBatch(); ok; b, ok = h.ins[0].NextBatch() {\n            // Write the batch\n            err := h.write(b)\n            if err != nil {\n                return err\n            }\n        }\n    }\n    return nil\n}\n\n// Write a batch of data to HouseDB\nfunc (h *HouseDBOutNode) write(batch models.Batch) error {\n    // Implement writing to HouseDB here...\n    return nil\n}\n</pre> <p>pipeline/node.go (just the new chaining method is shown):</p> <pre data-language=\"go\">// Create a new HouseDBOutNode as a child of the calling node.\nfunc (c *chainnode) HouseDBOut() *HouseDBOutNode {\n    h := newHouseDBOutNode(c.Provides())\n    c.linkChild(h)\n    return h\n}\n</pre> <p>task.go (just the new case is shown):</p> <pre data-language=\"go\">// Create a node from a given pipeline node.\nfunc (et *ExecutingTask) createNode(p pipeline.Node, l *log.Logger) (n Node, err error) {\n    switch t := p.(type) {\n    ...\n\tcase *pipeline.HouseDBOutNode:\n\t\tn, err = newHouseDBOutNode(et, t, l)\n    ...\n}\n</pre> <h3 id=\"documenting-your-new-node\">Documenting your new node</h3> <p>Since TICKscript is its own language we have built a small utility similiar to <a href=\"https://godoc.org/golang.org/x/tools/cmd/godoc\">godoc</a> named <a href=\"https://github.com/influxdb/kapacitor/tree/master/tick/cmd/tickdoc\">tickdoc</a> that generates documentation from the comments in the code. The <code>tickdoc</code> utility understands two special comments to help it generate clean documentation.</p> <p>1. <code>tick:ignore</code> – can be added to any field, method, function or struct and tickdoc will simply skip it and not generate any documentation for it. Useful for ignore fields that are set via property methods. 2. <code>tick:property</code> – is only added to methods and informs tickdoc that the method is a <code>property method</code> not a <code>chaining method</code>.</p> <p>Just place one of these comments on a line all by itself and tickdoc will find it and behave accordingly.</p> <p>Otherwise just document your code normaly and tickdoc will do the rest.</p> <h3 id=\"contributing-non-output-node\">Contributing non output node.</h3> <p>Writing any node not just an output node is a very similar process and is left as an exercise to the reader. There are few things that are different.</p> <p>First is that your new node in the <code>pipeline</code> package will want to use the <code>pipeline.chainnode</code> implementation of the <code>pipeline.Node</code> interface if it wishes to send data on to children. For example:</p> <pre data-language=\"go\">package pipeline\n\ntype MyCustomNode struct {\n    // Include pipeline.chainnode so we have all the chaining methods available\n    // to our new node\n    chainnode\n\n}\n\nfunc newMyCustomNode(e EdgeType, n Node) *MyCustomNode {\n    m := &amp;MyCustomNode{\n        chainnode: newBasicChainNode(\"mycustom\", e, e),\n    }\n    n.linkChild(m)\n    return m\n}\n</pre> <p>Second it is possible to define a method that sets fields on a pipeline Node and just return the same instance in order to create a <code>property method</code>. For example:</p> <pre data-language=\"go\">package pipeline\n\ntype MyCustomNode struct {\n    // Include pipeline.chainnode so we have all the chaining methods available\n    // to our new node\n    chainnode\n\n    // Mark this field as ignored for docs\n    // Since it is set via the Names method below\n    // tick:ignore\n    NameList []string `tick:\"Names\"`\n\n}\n\nfunc newMyCustomNode(e EdgeType, n Node) *MyCustomNode {\n    m := &amp;MyCustomNode{\n        chainnode: newBasicChainNode(\"mycustom\", e, e),\n    }\n    n.linkChild(m)\n    return m\n}\n\n// Set the NameList field on the node via this method.\n//\n// Example:\n//    node.names('name0', 'name1')\n//\n// Use the tickdoc comment 'tick:property' to mark this method\n// as a 'property method'\n// tick:property\nfunc (m *MyCustomNode) Names(name ...string) *MyCustomNode {\n    m.NameList = name\n    return m\n}\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/about_the_project/custom_output/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/about_the_project/custom_output/</a>\n  </p>\n</div>\n","kapacitor/v0.13/api/api/index":"<h1>HTTP API Reference</h1>     <h1 id=\"kapacitor-api-reference-documentation\">Kapacitor API Reference Documentation</h1> <ul> <li><a href=\"#general-information\">General Information</a></li> <li><a href=\"#writing-data\">Writing Data</a></li> <li><a href=\"#tasks\">Tasks</a></li> <li><a href=\"#recordings\">Recordings</a></li> <li><a href=\"#replays\">Replays</a></li> <li><a href=\"#miscellaneous\">Miscellaneous</a></li> </ul> <h2 id=\"general-information\">General Information</h2> <p>Kapacitor provides an HTTP API on port 9092 by default. With the API you can control which tasks are executing, query status of tasks and manage recordings etc.</p> <p>Each section below defines the available API endpoints and there inputs and outputs.</p> <p>All requests are versioned and namespaced using the base path <code>/kapacitor/v1/</code>.</p> <h3 id=\"response-codes\">Response Codes</h3> <p>All requests can return these response codes:</p> <table> <thead> <tr> <th>HTTP Response Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>2xx</td> <td>The request was a success, content is dependent on the request.</td> </tr> <tr> <td>4xx</td> <td>Invalid request, refer to error for what it wrong with the request. Repeating the request will continue to return the same error.</td> </tr> <tr> <td>5xx</td> <td>The server was unable to process the request, refer to the error for a reason. Repeating the request may result in a success if the server issue has been resolved.</td> </tr> </tbody> </table> <h3 id=\"errors\">Errors</h3> <p>All requests can return JSON in the following format to provide more information about a failed request.</p> <pre>{\n    \"error\" : \"error message\"\n}\n</pre> <h3 id=\"query-parameters-vs-json-body\">Query Parameters vs JSON body</h3> <p>To make using this API a consistent and easy experience we follow one simple rule for when extra information about a request is found in the query parameters of the URL or when they are part of the submitted JSON body.</p> <p>Query parameters are used only for GET requests and all other requests expect parameters to be specified in the JSON body.</p> <blockquote> <p>NOTE: The /kapacitor/v1/write endpoint is the one exception to this rule since Kapacitor is compatible with the InfluxDB /write endpoint.</p> </blockquote> <h3 id=\"links\">Links</h3> <p>When creating resources in Kapacitor the API server will return a <code>link</code> object with an <code>href</code> of the resource. Clients should not need to perform path manipulation in most cases and can use the links provided from previous calls.</p> <h2 id=\"writing-data\">Writing Data</h2> <p>Kapacitor can accept writes over HTTP using the line protocol. This endpoint is identical in nature to the InfluxDB write endpoint.</p> <table> <thead> <tr> <th>Query Parameter</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>db</td> <td>Database name for the writes.</td> </tr> <tr> <td>rp</td> <td>Retention policy name for the writes.</td> </tr> </tbody> </table> <blockquote> <p>NOTE: Kapacitor scopes all points by their database and retention policy. This means you MUST specify the <code>rp</code> for writes or Kapacitor will not know which retention policy to use.</p> </blockquote> <h4 id=\"example\">Example</h4> <p>Write data to Kapacitor.</p> <pre>POST /kapacitor/v1/write?db=DB_NAME&amp;rp=RP_NAME\ncpu,host=example.com value=87.6\n</pre> <p>For compatibility with the equivalent InfluxDB write endpoint the <code>/write</code> endpoint is maintained as an alias to the <code>/kapacitor/v1/write</code> endpoint.</p> <pre>POST /write?db=DB_NAME&amp;rp=RP_NAME\ncpu,host=example.com value=87.6\n</pre> <h2 id=\"tasks\">Tasks</h2> <p>A task represents work for Kapacitor to perform. A task is defined by its id, type, TICKscript, and list of database retention policy pairs it is allowed to access.</p> <h3 id=\"define-task\">Define Task</h3> <p>To define a task POST to the <code>/kapacitor/v1/tasks/</code> endpoint. If a task already exists then use the <code>PATCH</code> method to modify any property of the task.</p> <p>Define a task using a JSON object with the following options:</p> <table> <thead> <tr> <th>Property</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>id</td> <td>Unique identifier for the task. If empty a random ID will be chosen.</td> </tr> <tr> <td>type</td> <td>The task type: <code>stream</code> or <code>batch</code>.</td> </tr> <tr> <td>dbrps</td> <td>List of database retention policy pairs the task is allowed to access.</td> </tr> <tr> <td>script</td> <td>The content of the script.</td> </tr> <tr> <td>status</td> <td>One of <code>enabled</code> or <code>disabled</code>.</td> </tr> </tbody> </table> <p>When using PATCH, if any option is missing it will be left unmodified.</p> <h4 id=\"example-1\">Example</h4> <p>Create a new task with ID TASK_ID.</p> <pre>POST /kapacitor/v1/tasks/\n{\n    \"id\" : \"TASK_ID\",\n    \"type\" : \"stream\",\n    \"dbrps\": [{\"db\": \"DATABASE_NAME\", \"rp\" : \"RP_NAME\"}],\n    \"script\": \"stream\\n    |from()\\n        .measurement('cpu')\\n\"\n}\n</pre> <p>Response with task id and link.</p> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/tasks/TASK_ID\"},\n    \"id\" : \"TASK_ID\",\n    \"type\" : \"stream\",\n    \"dbrps\" : [{\"db\": \"DATABASE_NAME\", \"rp\" : \"RP_NAME\"}],\n    \"script\" : \"stream\\n    |from()\\n        .measurement('cpu')\\n\",\n    \"dot\" : \"digraph TASK_ID { ... }\",\n    \"status\" : \"enabled\",\n    \"executing\" : true,\n    \"error\" : \"\",\n    \"created\": \"2006-01-02T15:04:05Z07:00\",\n    \"modified\": \"2006-01-02T15:04:05Z07:00\",\n    \"stats\" : {}\n}\n</pre> <p>Modify only the dbrps of the task.</p> <pre>PATCH /kapacitor/v1/tasks/TASK_ID\n{\n    \"dbrps\": [{\"db\": \"NEW_DATABASE_NAME\", \"rp\" : \"NEW_RP_NAME\"}]\n}\n</pre> <blockquote> <p>NOTE: Setting any DBRP will overwrite all stored DBRPs.</p> </blockquote> <p>Enable an existing task.</p> <pre>PATCH /kapacitor/v1/tasks/TASK_ID\n{\n    \"status\" : \"enabled\",\n}\n</pre> <p>Disable an existing task.</p> <pre>PATCH /kapacitor/v1/tasks/TASK_ID\n{\n    \"status\" : \"disabled\",\n}\n</pre> <p>Define a new task that is enabled on creation.</p> <pre>POST /kapacitor/v1/tasks/TASK_ID\n{\n    \"type\" : \"stream\",\n    \"dbrps\" : [{\"db\": \"DATABASE_NAME\", \"rp\" : \"RP_NAME\"}],\n    \"script\" : \"stream\\n    |from()\\n        .measurement('cpu')\\n\",\n    \"status\" : \"enabled\"\n}\n</pre> <p>Response with task id and link.</p> <pre>{\n    \"id\" : \"TASK_ID\",\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/tasks/TASK_ID\"}\n}\n</pre> <h4 id=\"response\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>200</td> <td>Task created, contains task information.</td> </tr> <tr> <td>204</td> <td>Task updated, no content</td> </tr> <tr> <td>404</td> <td>Task does not exist</td> </tr> </tbody> </table> <h3 id=\"get-task\">Get Task</h3> <p>To get information about a task make a GET request to the <code>/kapacitor/v1/tasks/TASK_ID</code> endpoint.</p> <table> <thead> <tr> <th>Query Parameter</th> <th>Default</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>dot-view</td> <td>attributes</td> <td>One of <code>labels</code> or <code>attributes</code>. Labels is less readable but will correctly render with all the information contained in labels.</td> </tr> <tr> <td>script-format</td> <td>formatted</td> <td>One of <code>formatted</code> or <code>raw</code>. Raw will return the script identical to how it was defined. Formatted will first format the script.</td> </tr> </tbody> </table> <p>A task has these read only properties in addition to the properties listed <a href=\"#define-task\">above</a>.</p> <table> <thead> <tr> <th>Property</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>dot</td> <td>\n<a href=\"https://en.wikipedia.org/wiki/DOT_(graph_description_language\">GraphViz DOT</a>) syntax formatted representation of the task DAG.</td> </tr> <tr> <td>executing</td> <td>Whether the task is currently executing.</td> </tr> <tr> <td>error</td> <td>Any error encountered when executing the task.</td> </tr> <tr> <td>stats</td> <td>Map of statistics about a task.</td> </tr> <tr> <td>created</td> <td>Date the task was first created</td> </tr> <tr> <td>modified</td> <td>Date the task was last modified</td> </tr> <tr> <td>last-enabled</td> <td>Date the task was last set to status <code>enabled</code>\n</td> </tr> </tbody> </table> <h4 id=\"example-2\">Example</h4> <p>Get information about a task using defaults.</p> <pre>GET /kapacitor/v1/tasks/TASK_ID\n</pre> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/tasks/TASK_ID\"},\n    \"id\" : \"TASK_ID\",\n    \"type\" : \"stream\",\n    \"dbrps\" : [{\"db\": \"DATABASE_NAME\", \"rp\" : \"RP_NAME\"}],\n    \"script\" : \"stream\\n    |from()\\n        .measurement('cpu')\\n\",\n    \"dot\" : \"digraph TASK_ID { ... }\",\n    \"status\" : \"enabled\",\n    \"executing\" : true,\n    \"error\" : \"\",\n    \"created\": \"2006-01-02T15:04:05Z07:00\",\n    \"modified\": \"2006-01-02T15:04:05Z07:00\",\n    \"last-enabled\": \"2006-01-03T15:04:05Z07:00\",\n    \"stats\" : {}\n}\n</pre> <p>Get information about a task using only labels in the DOT content and skip the format step.</p> <pre>GET /kapacitor/v1/tasks/TASK_ID?dot-view=labels&amp;script-format=raw\n</pre> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/tasks/TASK_ID\"},\n    \"id\" : \"TASK_ID\",\n    \"type\" : \"stream\",\n    \"dbrps\" : [{\"db\": \"DATABASE_NAME\", \"rp\" : \"RP_NAME\"}],\n    \"script\" : \"stream|from().measurement('cpu')\",\n    \"dot\" : \"digraph TASK_ID { ... }\",\n    \"status\" : \"enabled\",\n    \"executing\" : true,\n    \"error\" : \"\",\n    \"created\": \"2006-01-02T15:04:05Z07:00\",\n    \"modified\": \"2006-01-02T15:04:05Z07:00\",\n    \"last-enabled\": \"2006-01-03T15:04:05Z07:00\",\n    \"stats\" : {}\n}\n</pre> <h4 id=\"response-1\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>200</td> <td>Success</td> </tr> <tr> <td>404</td> <td>Task does not exist</td> </tr> </tbody> </table> <h3 id=\"delete-task\">Delete Task</h3> <p>To delete a task make a DELETE request to the <code>/kapacitor/v1/tasks/TASK_ID</code> endpoint.</p> <pre>DELETE /kapacitor/v1/tasks/TASK_ID\n</pre> <h4 id=\"response-2\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>204</td> <td>Success</td> </tr> </tbody> </table> <blockquote> <p>NOTE: Deleting a non-existent task is not an error and will return a 204 success.</p> </blockquote> <h3 id=\"list-tasks\">List Tasks</h3> <p>To get information about several tasks make a GET request to the <code>/kapacitor/v1/tasks</code> endpoint.</p> <table> <thead> <tr> <th>Query Parameter</th> <th>Default</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>pattern</td> <td></td> <td>Filter results based on the pattern. Uses standard shell glob matching, see <a href=\"https://golang.org/pkg/path/filepath/#Match\">this</a> for more details.</td> </tr> <tr> <td>fields</td> <td></td> <td>List of fields to return. If empty returns all fields. Fields <code>id</code> and <code>link</code> are always returned.</td> </tr> <tr> <td>dot-view</td> <td>attributes</td> <td>One of <code>labels</code> or <code>attributes</code>. Labels is less readable but will correctly render with all the information contained in labels.</td> </tr> <tr> <td>script-format</td> <td>formatted</td> <td>One of <code>formatted</code> or <code>raw</code>. Raw will return the script identical to how it was defined. Formatted will first format the script.</td> </tr> <tr> <td>offset</td> <td>0</td> <td>Offset count for paginating through tasks.</td> </tr> <tr> <td>limit</td> <td>100</td> <td>Maximum number of tasks to return.</td> </tr> </tbody> </table> <h4 id=\"example-3\">Example</h4> <p>Get all tasks.</p> <pre>GET /kapacitor/v1/tasks\n</pre> <pre>{\n    \"tasks\" : [\n        {\n            \"link\" : {\"rel\":\"self\", \"href\":\"/kapacitor/v1/tasks/TASK_ID\"},\n            \"id\" : \"TASK_ID\",\n            \"type\" : \"stream\",\n            \"dbrps\" : [{\"db\": \"DATABASE_NAME\", \"rp\" : \"RP_NAME\"}],\n            \"script\" : \"stream|from().measurement('cpu')\",\n            \"dot\" : \"digraph TASK_ID { ... }\",\n            \"status\" : \"enabled\",\n            \"executing\" : true,\n            \"error\" : \"\",\n            \"stats\" : {}\n        },\n        {\n            \"link\" : {\"rel\":\"self\", \"href\":\"/kapacitor/v1/tasks/ANOTHER_TASK_ID\"},\n            \"id\" : \"ANOTHER_TASK_ID\",\n            \"type\" : \"stream\",\n            \"dbrps\" : [{\"db\": \"DATABASE_NAME\", \"rp\" : \"RP_NAME\"}],\n            \"script\" : \"stream|from().measurement('cpu')\",\n            \"dot\" : \"digraph ANOTHER_TASK_ID{ ... }\",\n            \"status\" : \"disabled\",\n            \"executing\" : true,\n            \"error\" : \"\",\n            \"stats\" : {}\n        }\n    ]\n}\n</pre> <p>Optionally specify a glob <code>pattern</code> to list only matching tasks.</p> <pre>GET /kapacitor/v1/task?pattern=TASK*\n</pre> <pre>{\n    \"tasks\" : [\n        {\n            \"link\" : {\"rel\":\"self\", \"href\":\"/kapacitor/v1/tasks/TASK_ID\"},\n            \"id\" : \"TASK_ID\",\n            \"type\" : \"stream\",\n            \"dbrps\" : [{\"db\": \"DATABASE_NAME\", \"rp\" : \"RP_NAME\"}],\n            \"script\" : \"stream|from().measurement('cpu')\",\n            \"dot\" : \"digraph TASK_ID { ... }\",\n            \"status\" : \"enabled:,\n            \"executing\" : true,\n            \"error\" : \"\",\n            \"stats\" : {}\n        }\n    ]\n}\n</pre> <p>Get all tasks, but only the status, executing and error fields.</p> <pre>GET /kapacitor/v1/tasks?fields=status&amp;fields=executing&amp;fields=error\n</pre> <pre>{\n    \"tasks\" : [\n        {\n            \"link\" : {\"rel\":\"self\", \"href\":\"/kapacitor/v1/tasks/TASK_ID\"},\n            \"id\" : \"TASK_ID\",\n            \"status\" : \"enabled\",\n            \"executing\" : true,\n            \"error\" : \"\",\n        },\n        {\n            \"link\" : {\"rel\":\"self\", \"href\":\"/kapacitor/v1/tasks/ANOTHER_TASK_ID\"},\n            \"id\" : \"ANOTHER_TASK_ID\",\n            \"status\" : \"disabled\",\n            \"executing\" : true,\n            \"error\" : \"\",\n        }\n    ]\n}\n</pre> <h4 id=\"response-3\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>200</td> <td>Success</td> </tr> </tbody> </table> <blockquote> <p>NOTE: If the pattern does not match any tasks an empty list will be returned, with a 200 success.</p> </blockquote> <h3 id=\"custom-task-http-endpoints\">Custom Task HTTP Endpoints</h3> <p>In TICKscript it is possible to expose a cache of recent data via the <a href=\"https://docs.influxdata.com/kapacitor/latest/nodes/http_out_node/\">HTTPOut</a> node. The data is available at the path <code>/kapacitor/v1/tasks/TASK_ID/ENDPOINT_NAME</code>.</p> <h3 id=\"example-4\">Example</h3> <p>For the TICKscript:</p> <pre data-language=\"go\">stream\n    |from()\n        .measurement('cpu')\n    |window()\n        .period(60s)\n        .every(60s)\n    |httpOut('mycustom_endpoint')\n</pre> <pre>GET /kapacitor/v1/tasks/TASK_ID/mycustom_endpoint\n</pre> <pre>{\n    \"series\": [\n        {\n            \"name\": \"cpu\",\n            \"columns\": [\n                \"time\",\n                \"value\"\n            ],\n            \"values\": [\n                [\n                    \"2015-01-29T21:55:43.702900257Z\",\n                    55\n                ],\n                [\n                    \"2015-01-29T21:56:43.702900257Z\",\n                    42\n                ],\n            ]\n        }\n    ]\n}\n</pre> <p>The output is the same as a query for data to <a href=\"https://docs.influxdata.com/influxdb/latest/guides/querying_data/\">InfluxDB</a>.</p> <h2 id=\"recordings\">Recordings</h2> <p>Kapacitor can save recordings of data and replay them against a specified task.</p> <h3 id=\"start-recording\">Start Recording</h3> <p>There are three methods for recording data with Kapacitor: To create a recording make a POST request to the <code>/kapacitor/v1/recordings/METHOD</code> endpoint.</p> <table> <thead> <tr> <th>Method</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>stream</td> <td>Record the incoming stream of data.</td> </tr> <tr> <td>batch</td> <td>Record the results of the queries in a batch task.</td> </tr> <tr> <td>query</td> <td>Record the result of an explicit query.</td> </tr> </tbody> </table> <p>The request returns once the recording is started and does not wait for it to finish. A recording ID is returned to later identify the recording.</p> <h5 id=\"stream\">Stream</h5> <table> <thead> <tr> <th>Parameter</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>id</td> <td>Unique identifier for the recording. If empty a random one will be chosen.</td> </tr> <tr> <td>task</td> <td>ID of a task, used to only record data for the DBRPs of the task.</td> </tr> <tr> <td>stop</td> <td>Record stream data until stop date.</td> </tr> </tbody> </table> <h5 id=\"batch\">Batch</h5> <table> <thead> <tr> <th>Parameter</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>id</td> <td>Unique identifier for the recording. If empty a random one will be chosen.</td> </tr> <tr> <td>task</td> <td>ID of a task, records the results of the queries defined in the task.</td> </tr> <tr> <td>start</td> <td>Earliest date for which data will be recorded. RFC3339Nano formatted.</td> </tr> <tr> <td>stop</td> <td>Latest date for which data will be recorded. If not specified uses the current time. RFC3339Nano formatted data.</td> </tr> <tr> <td>cluster</td> <td>Name of a configured InfluxDB cluster. If empty uses the default cluster.</td> </tr> </tbody> </table> <h5 id=\"query\">Query</h5> <table> <thead> <tr> <th>Parameter</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>id</td> <td>Unique identifier for the recording. If empty a random one will be chosen.</td> </tr> <tr> <td>type</td> <td>Type of recording, <code>stream</code> or <code>batch</code>.</td> </tr> <tr> <td>query</td> <td>Query to execute.</td> </tr> <tr> <td>cluster</td> <td>Name of a configured InfluxDB cluster. If empty uses the default cluster.</td> </tr> </tbody> </table> <blockquote> <p>NOTE: A recording itself is typed as either a stream or batch recording and can only be replayed to a task of a corresponding type. Therefore when you record the result of a raw query you must specify the type recording you wish to create.</p> </blockquote> <h4 id=\"example-5\">Example</h4> <p>Create a recording using the <code>stream</code> method</p> <pre>POST /kapacitor/v1/recordings/stream\n{\n    \"task\" : \"TASK_ID\",\n    \"stop\" : \"2006-01-02T15:04:05Z07:00\"\n}\n</pre> <p>Create a recording using the <code>batch</code> method specifying a start time.</p> <pre>POST /kapacitor/v1/recordings/batch\n{\n    \"task\" : \"TASK_ID\",\n    \"start\" : \"2006-01-02T15:04:05Z07:00\"\n}\n</pre> <p>Create a recording using the <code>query</code> method specifying a <code>stream</code> type.</p> <pre>POST /kapacitor/v1/recordings/query\n{\n    \"query\" : \"SELECT mean(usage_idle) FROM cpu WHERE time &gt; now() - 1h GROUP BY time(10m)\",\n    \"type\" : \"stream\"\n}\n</pre> <p>Create a recording using the <code>query</code> method specifying a <code>batch</code> type.</p> <pre>POST /kapacitor/v1/recording/query\n{\n    \"query\" : \"SELECT mean(usage_idle) FROM cpu WHERE time &gt; now() - 1h GROUP BY time(10m)\",\n    \"type\" : \"batch\"\n}\n</pre> <p>Create a recording with a custom ID.</p> <pre>POST /kapacitor/v1/recording/query\n{\n    \"id\" : \"MY_RECORDING_ID\",\n    \"query\" : \"SELECT mean(usage_idle) FROM cpu WHERE time &gt; now() - 1h GROUP BY time(10m)\",\n    \"type\" : \"batch\"\n}\n</pre> <h4 id=\"response-4\">Response</h4> <p>All recordings are assigned an ID which is returned in this format with a link.</p> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/recordings/e24db07d-1646-4bb3-a445-828f5049bea0\"},\n    \"id\" : \"e24db07d-1646-4bb3-a445-828f5049bea0\",\n    \"type\" : \"stream\",\n    \"size\" : 0,\n    \"date\" : \"2006-01-02T15:04:05Z07:00\",\n    \"error\" : \"\",\n    \"status\" : \"running\",\n    \"progress\" : 0\n}\n</pre> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>201</td> <td>Success, the recording has started.</td> </tr> </tbody> </table> <h3 id=\"wait-for-recording\">Wait for Recording</h3> <p>In order to determine when a recording has finished you must make a GET request to the returned link typically something like <code>/kapacitor/v1/recordings/RECORDING_ID</code>.</p> <p>A recording has these read only properties.</p> <table> <thead> <tr> <th>Property</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>size</td> <td>Size of the recording on disk in bytes.</td> </tr> <tr> <td>date</td> <td>Date the recording finished.</td> </tr> <tr> <td>error</td> <td>Any error encountered when creating the recording.</td> </tr> <tr> <td>status</td> <td>One of <code>recording</code> or <code>finished</code>.</td> </tr> <tr> <td>progress</td> <td>Number between 0 and 1 indicating the approximate progress of the recording.</td> </tr> </tbody> </table> <h4 id=\"example-6\">Example</h4> <pre>GET /kapacitor/v1/recordings/e24db07d-1646-4bb3-a445-828f5049bea0\n</pre> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/recordings/e24db07d-1646-4bb3-a445-828f5049bea0\"},\n    \"id\" : \"e24db07d-1646-4bb3-a445-828f5049bea0\",\n    \"type\" : \"stream\",\n    \"size\" : 1980353,\n    \"date\" : \"2006-01-02T15:04:05Z07:00\",\n    \"error\" : \"\",\n    \"status\" : \"running\",\n    \"progress\" : 0.75\n}\n</pre> <p>Once the recording is complete.</p> <pre>GET /kapacitor/v1/recordings/e24db07d-1646-4bb3-a445-828f5049bea0\n</pre> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/recordings/e24db07d-1646-4bb3-a445-828f5049bea0\"},\n    \"id\" : \"e24db07d-1646-4bb3-a445-828f5049bea0\",\n    \"type\" : \"stream\",\n    \"size\" : 1980353,\n    \"date\" : \"2006-01-02T15:04:05Z07:00\",\n    \"error\" : \"\",\n    \"status\" : \"finished\",\n    \"progress\" : 1\n}\n</pre> <p>Or if the recording fails.</p> <pre>GET /kapacitor/v1/recordings/e24db07d-1646-4bb3-a445-828f5049bea0\n</pre> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/recordings/e24db07d-1646-4bb3-a445-828f5049bea0\"},\n    \"id\" : \"e24db07d-1646-4bb3-a445-828f5049bea0\",\n    \"type\" : \"stream\",\n    \"size\" : 1980353,\n    \"date\" : \"2006-01-02T15:04:05Z07:00\",\n    \"error\" : \"error message explaining failure\",\n    \"status\" : \"failed\",\n    \"progress\" : 1\n}\n</pre> <h4 id=\"response-5\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>200</td> <td>Success, the recording is no longer running.</td> </tr> <tr> <td>202</td> <td>Success, the recording exists but is not finished.</td> </tr> <tr> <td>404</td> <td>No such recording exists.</td> </tr> </tbody> </table> <h3 id=\"delete-recording\">Delete Recording</h3> <p>To delete a recording make a DELETE request to the <code>/kapacitor/v1/recordings/RECORDING_ID</code> endpoint.</p> <pre>DELETE /kapacitor/v1/recordings/RECORDING_ID\n</pre> <h4 id=\"response-6\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>204</td> <td>Success</td> </tr> </tbody> </table> <blockquote> <p>NOTE: Deleting a non-existent recording is not an error and will return a 204 success.</p> </blockquote> <h3 id=\"list-recordings\">List Recordings</h3> <p>To list all recordings make a GET request to the <code>/kapacitor/v1/recordings</code> endpoint. Recordings are sorted by date.</p> <table> <thead> <tr> <th>Query Parameter</th> <th>Default</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>pattern</td> <td></td> <td>Filter results based on the pattern. Uses standard shell glob matching, see <a href=\"https://golang.org/pkg/path/filepath/#Match\">this</a> for more details.</td> </tr> <tr> <td>fields</td> <td></td> <td>List of fields to return. If empty returns all fields. Fields <code>id</code> and <code>link</code> are always returned.</td> </tr> <tr> <td>offset</td> <td>0</td> <td>Offset count for paginating through tasks.</td> </tr> <tr> <td>limit</td> <td>100</td> <td>Maximum number of tasks to return.</td> </tr> </tbody> </table> <h4 id=\"example-7\">Example</h4> <pre>GET /kapacitor/v1/recordings\n</pre> <pre>{\n    \"recordings\" : [\n        {\n            \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/recordings/e24db07d-1646-4bb3-a445-828f5049bea0\"},\n            \"id\" : \"e24db07d-1646-4bb3-a445-828f5049bea0\",\n            \"type\" : \"stream\",\n            \"size\" : 1980353,\n            \"date\" : \"2006-01-02T15:04:05Z07:00\",\n            \"error\" : \"\",\n            \"status\" : \"finished\",\n            \"progress\" : 1\n        },\n        {\n            \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/recordings/8a4c06c6-30fb-42f4-ac4a-808aa31278f6\"},\n            \"id\" : \"8a4c06c6-30fb-42f4-ac4a-808aa31278f6\",\n            \"type\" : \"batch\",\n            \"size\" : 216819562,\n            \"date\" : \"2006-01-02T15:04:05Z07:00\",\n            \"error\" : \"\",\n            \"status\" : \"finished\",\n            \"progress\" : 1\n        }\n    ]\n}\n</pre> <h4 id=\"response-7\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>200</td> <td>Success</td> </tr> </tbody> </table> <h2 id=\"replays\">Replays</h2> <h3 id=\"replaying-a-recording\">Replaying a recording</h3> <p>To replay a recording make a POST request to <code>/kapacitor/v1/replays/</code></p> <table> <thead> <tr> <th>Parameter</th> <th>Default</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>id</td> <td>random</td> <td>Unique identifier for the replay. If empty a random ID is chosen.</td> </tr> <tr> <td>task</td> <td></td> <td>ID of task.</td> </tr> <tr> <td>recording</td> <td></td> <td>ID of recording.</td> </tr> <tr> <td>recording-time</td> <td>false</td> <td>If true, use the times in the recording, otherwise adjust times relative to the current time.</td> </tr> <tr> <td>clock</td> <td>fast</td> <td>One of <code>fast</code> or <code>real</code>. If <code>real</code> wait for real time to pass corresponding with the time in the recordings. If <code>fast</code> replay data without delay. For example, if clock is <code>real</code> then a stream recording of duration 5m will take 5m to replay.</td> </tr> </tbody> </table> <h4 id=\"example-8\">Example</h4> <p>Replay a recording using default parameters.</p> <pre>POST /kapacitor/v1/replays/\n{\n    \"task\" : \"TASK_ID\",\n    \"recording\" : \"RECORDING_ID\"\n}\n</pre> <p>Replay a recording in real-time mode and preserve recording times.</p> <pre>POST /kapacitor/v1/replays/\n{\n    \"task\" : \"TASK_ID\",\n    \"recording\" : \"RECORDING_ID\",\n    \"clock\" : \"real\",\n    \"recording-time\" : true,\n}\n</pre> <p>Replay a recording using a custom ID.</p> <pre>POST /kapacitor/v1/replays/\n{\n    \"id\" : \"MY_REPLAY_ID\",\n    \"task\" : \"TASK_ID\",\n    \"recording\" : \"RECORDING_ID\"\n}\n</pre> <h4 id=\"response-8\">Response</h4> <p>The request returns once the replay is started and provides a replay ID and link.</p> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/replays/ad95677b-096b-40c8-82a8-912706f41d4c\"},\n    \"id\" : \"ad95677b-096b-40c8-82a8-912706f41d4c\",\n    \"task\" : \"TASK_ID\",\n    \"recording\" : \"RECORDING_ID\",\n    \"clock\" : \"fast\",\n    \"recording-time\" : false,\n    \"status\" : \"running\",\n    \"progress\" : 0,\n    \"error\" : \"\"\n}\n</pre> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>201</td> <td>Success, replay has started.</td> </tr> </tbody> </table> <h3 id=\"waiting-for-a-replay\">Waiting for a Replay</h3> <p>Like recordings you make a GET request to the <code>/kapacitor/v1/replays/REPLAY_ID</code> endpoint to get the status of the replay.</p> <p>A replay has these read only properties in addition to the properties listed <a href=\"#replay-recording\">above</a>.</p> <table> <thead> <tr> <th>Property</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>status</td> <td>One of <code>replaying</code> or <code>finished</code>.</td> </tr> <tr> <td>progress</td> <td>Number between 0 and 1 indicating the approximate progress of the replay.</td> </tr> <tr> <td>error</td> <td>Any error that occured while perfoming the replay</td> </tr> </tbody> </table> <h4 id=\"example-9\">Example</h4> <p>Get the status of a replay.</p> <pre>GET /kapacitor/v1/replays/ad95677b-096b-40c8-82a8-912706f41d4c\n</pre> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/replays/ad95677b-096b-40c8-82a8-912706f41d4c\"},\n    \"id\" : \"ad95677b-096b-40c8-82a8-912706f41d4c\",\n    \"task\" : \"TASK_ID\",\n    \"recording\" : \"RECORDING_ID\",\n    \"clock\" : \"fast\",\n    \"recording-time\" : false,\n    \"status\" : \"running\",\n    \"progress\" : 0.57,\n    \"error\" : \"\"\n}\n</pre> <p>Once the replay is complete.</p> <pre>GET /kapacitor/v1/replays/ad95677b-096b-40c8-82a8-912706f41d4c\n</pre> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/replays/ad95677b-096b-40c8-82a8-912706f41d4c\"},\n    \"id\" : \"ad95677b-096b-40c8-82a8-912706f41d4c\",\n    \"task\" : \"TASK_ID\",\n    \"recording\" : \"RECORDING_ID\",\n    \"clock\" : \"fast\",\n    \"recording-time\" : false,\n    \"status\" : \"finished\",\n    \"progress\" : 1,\n    \"error\" : \"\"\n}\n</pre> <p>Or if the replay fails.</p> <pre>GET /kapacitor/v1/replays/ad95677b-096b-40c8-82a8-912706f41d4c\n</pre> <pre>{\n    \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/replays/ad95677b-096b-40c8-82a8-912706f41d4c\"},\n    \"id\" : \"ad95677b-096b-40c8-82a8-912706f41d4c\",\n    \"task\" : \"TASK_ID\",\n    \"recording\" : \"RECORDING_ID\",\n    \"clock\" : \"fast\",\n    \"recording-time\" : false,\n    \"status\" : \"failed\",\n    \"progress\" : 1,\n    \"error\" : \"error message explaining failure\"\n}\n</pre> <h4 id=\"response-9\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>200</td> <td>Success, replay is no longer running.</td> </tr> <tr> <td>202</td> <td>Success, the replay exists but is not finished.</td> </tr> <tr> <td>404</td> <td>No such replay exists.</td> </tr> </tbody> </table> <h3 id=\"delete-replay\">Delete Replay</h3> <p>To delete a replay make a DELETE request to the <code>/kapacitor/v1/replays/REPLAY_ID</code> endpoint.</p> <pre>DELETE /kapacitor/v1/replays/REPLAY_ID\n</pre> <h4 id=\"response-10\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>204</td> <td>Success</td> </tr> </tbody> </table> <blockquote> <p>NOTE: Deleting a non-existent replay is not an error and will return a 204 success.</p> </blockquote> <h3 id=\"list-replays\">List Replays</h3> <p>You can list replays for a given recording by making a GET request to <code>/kapacitor/v1/replays</code>.</p> <table> <thead> <tr> <th>Query Parameter</th> <th>Default</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>pattern</td> <td></td> <td>Filter results based on the pattern. Uses standard shell glob matching, see <a href=\"https://golang.org/pkg/path/filepath/#Match\">this</a> for more details.</td> </tr> <tr> <td>fields</td> <td></td> <td>List of fields to return. If empty returns all fields. Fields <code>id</code> and <code>link</code> are always returned.</td> </tr> <tr> <td>offset</td> <td>0</td> <td>Offset count for paginating through tasks.</td> </tr> <tr> <td>limit</td> <td>100</td> <td>Maximum number of tasks to return.</td> </tr> </tbody> </table> <h4 id=\"example-10\">Example</h4> <pre>GET /kapacitor/v1/replays\n</pre> <pre>{\n    \"replays\" [\n        {\n            \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/replays/ad95677b-096b-40c8-82a8-912706f41d4c\"},\n            \"id\" : \"ad95677b-096b-40c8-82a8-912706f41d4c\",\n            \"task\" : \"TASK_ID\",\n            \"recording\" : \"RECORDING_ID\",\n            \"clock\" : \"fast\",\n            \"recording-time\" : false,\n            \"status\" : \"finished\",\n            \"progress\" : 1,\n            \"error\" : \"\"\n        },\n        {\n            \"link\" : {\"rel\": \"self\", \"href\": \"/kapacitor/v1/replays/be33f0a1-0272-4019-8662-c730706dac7d\"},\n            \"id\" : \"be33f0a1-0272-4019-8662-c730706dac7d\",\n            \"task\" : \"TASK_ID\",\n            \"recording\" : \"RECORDING_ID\",\n            \"clock\" : \"fast\",\n            \"recording-time\" : false,\n            \"status\" : \"finished\",\n            \"progress\" : 1,\n            \"error\" : \"\"\n        }\n    ]\n}\n</pre> <h2 id=\"miscellaneous\">Miscellaneous</h2> <h3 id=\"ping\">Ping</h3> <p>You can ‘ping’ the Kapacitor server to validate you have a successful connection. A ping request does nothing but respond with a 204.</p> <blockquote> <p>NOTE: The Kapacitor server version is returned in the <code>X-Kapacitor-Version</code> HTTP header on all requests. Ping is a useful request if you simply need the verify the version of server you are talking to.</p> </blockquote> <h4 id=\"example-11\">Example</h4> <pre>GET /kapacitor/v1/ping\n</pre> <h4 id=\"response-11\">Response</h4> <table> <thead> <tr> <th>Code</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>204</td> <td>Success</td> </tr> </tbody> </table> <h3 id=\"debug-vars\">Debug Vars</h3> <p>Kapacitor also exposes several statistics and information about its runtime. These can be accessed at the <code>/kapacitor/v1/debug/vars</code> endpoint.</p> <h4 id=\"example-12\">Example</h4> <pre>GET /kapacitor/v1/debug/vars\n</pre> <h3 id=\"debug-pprof\">Debug Pprof</h3> <p>Kapacitor also the standard Go <a href=\"https://golang.org/pkg/net/http/pprof/\">net/http/pprof</a> endpoints.</p> <pre>GET /kapacitor/v1/debug/pprof/...\n</pre> <blockquote> <p>NOTE: Not all of these endpoints return JSON content.</p> </blockquote> <h3 id=\"routes\">Routes</h3> <p>Displays available routes for the API</p> <pre>GET /kapacitor/v1/:routes\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/api/api/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/api/api/</a>\n  </p>\n</div>\n","kapacitor/v0.13/examples/socket_udf/index":"<h1>Writing a socket based UDF</h1>     <p>In <a href=\"../anomaly_detection/index\">another example</a> we saw how to write a process based UDF for custom anomaly detection workloads. In this example we are going to learn how to write a simple socket based UDF.</p> <h2 id=\"what-is-a-udf\">What is a UDF?</h2> <p>A UDF is a user defined function that can communicate with Kapacitor to process data. Kapacitor will send it data and the UDF can respond with new or modified data. A UDF can be written in any language that has <a href=\"https://developers.google.com/protocol-buffers/\">protocol buffer</a> support.</p> <h2 id=\"what-is-the-difference-between-a-socket-udf-and-a-process-udf\">What is the difference between a socket UDF and a process UDF?</h2> <ul> <li>A process UDF, is a child process of Kapacitor that communicates over STDIN/STDOUT with Kapacitor and is completely managed by Kapacitor.</li> <li>A socket UDF is process external to Kapacitor that communicates over a configured unix domain socket. The process itself is not managed by Kapacitor.</li> </ul> <p>Using a process UDF can be simpler than a socket UDF because Kapacitor will spawn the process and manage everything for you. On the other hand you may want more control over the UDF process itself and rather expose only a socket to Kapacitor. One use case that is common is running Kapacitor in a Docker container and the UDF in another container that exposes the socket via a Docker volume.</p> <p>In both cases the protocol is the same the only difference is the transport mechanism. Also note that since multiple Kapacitor tasks can use the same UDF, for a process based UDF a new child process will be spawned for each use of the UDF. In contrast for a socket based UDF, a new connection will be made to the socket for each use of the UDF. If you have many uses of the same UDF it may be better to use a socket UDF to keep the number of running processes low.</p> <h2 id=\"writing-a-udf\">Writing a UDF</h2> <p>A UDF communicates with Kapacitor via a protocol buffer request/response system. We provide implementations of that communication layer in both Go and Python. Since the other example used Python we will use the Go version here.</p> <p>Our example is going to implement a <code>mirror</code> UDF which simply reflects all data it receives back to the Kapacitor server. This example is actually part of the test suite and a Python and Go implementation can be found <a href=\"https://github.com/influxdata/kapacitor/tree/master/udf/agent/examples/mirror\">here</a>.</p> <h3 id=\"lifecycle\">Lifecycle</h3> <p>Before we write any code lets look at the lifecycle of a socket UDF:</p> <ol> <li>The UDF process is started, independently from Kapacitor.</li> <li>The process listens on a unix domain socket.</li> <li>Kapacitor connects to the socket and queries basic information about the UDFs options.</li> <li>A Kapacitor task is enabled that uses the UDF and Kapacitor makes a new connection to the socket.</li> <li>The task reads and writes data over the socket connection.</li> <li>If the task is stopped for any reason the socket connection is closed.</li> </ol> <h3 id=\"the-main-method\">The Main method</h3> <p>We need to write a program that starts up and listens on a socket. The following code is a main function that listens on a socket at a default path, or on a custom path specified as the <code>-socket</code> flag.</p> <pre data-language=\"go\">package main\n\nimport (\n    \"flag\"\n    \"log\"\n    \"net\"\n)\n\n\nvar socketPath = flag.String(\"socket\", \"/tmp/mirror.sock\", \"Where to create the unix socket\")\n\nfunc main() {\n    flag.Parse()\n\n    // Create unix socket\n    addr, err := net.ResolveUnixAddr(\"unix\", *socketPath)\n    if err != nil {\n        log.Fatal(err)\n    }\n    l, err := net.ListenUnix(\"unix\", addr)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // More to come here...\n}\n</pre> <p>Place the above code in a scratch directory called <code>main.go</code>. This above code can be run via <code>go run main.go</code>, but at this point it will exit immediately after listening on the socket.</p> <h3 id=\"the-agent\">The Agent</h3> <p>As mentioned earlier, Kapacitor provides an implementation of the communication layer for UDFs called the <code>agent</code>. Our code need only implement an interface in order to take advantage of the <code>agent</code> logic.</p> <p>The interface we need to implement is as follows:</p> <pre data-language=\"go\">// The Agent calls the appropriate methods on the Handler as it receives requests over a socket.\n//\n// Returning an error from any method will cause the Agent to stop and an ErrorResponse to be sent.\n// Some *Response objects (like SnapshotResponse) allow for returning their own error within the object itself.\n// These types of errors will not stop the Agent and Kapacitor will deal with them appropriately.\n//\n// The Handler is called from a single goroutine, meaning methods will not be called concurrently.\n//\n// To write Points/Batches back to the Agent/Kapacitor use the Agent.Responses channel.\ntype Handler interface {\n    // Return the InfoResponse. Describing the properties of this Handler\n    Info() (*udf.InfoResponse, error)\n    // Initialize the Handler with the provided options.\n    Init(*udf.InitRequest) (*udf.InitResponse, error)\n    // Create a snapshot of the running state of the handler.\n    Snaphost() (*udf.SnapshotResponse, error)\n    // Restore a previous snapshot.\n    Restore(*udf.RestoreRequest) (*udf.RestoreResponse, error)\n\n    // A batch has begun.\n    BeginBatch(*udf.BeginBatch) error\n    // A point has arrived.\n    Point(*udf.Point) error\n    // The batch is complete.\n    EndBatch(*udf.EndBatch) error\n\n    // Gracefully stop the Handler.\n    // No other methods will be called.\n    Stop()\n}\n</pre> <h3 id=\"the-handler\">The Handler</h3> <p>Let’s define our own type so we can start implementing the <code>Handler</code> interface. Update the <code>main.go</code> file as follows:</p> <pre data-language=\"go\">package main\n\nimport (\n    \"flag\"\n    \"log\"\n    \"net\"\n\n    \"github.com/influxdata/kapacitor/udf/agent\"\n)\n\n\n\n// Mirrors all points it receives back to Kapacitor\ntype mirrorHandler struct {\n    // We need a reference to the agent so we can write data\n    // back to Kapacitor.\n    agent *agent.Agent\n}\n\nfunc newMirrorHandler(agent *agent.Agent) *mirrorHandler {\n    return &amp;mirrorHandler{agent: agent}\n}\n\nvar socketPath = flag.String(\"socket\", \"/tmp/mirror.sock\", \"Where to create the unix socket\")\n\nfunc main() {\n    flag.Parse()\n\n    // Create unix socket\n    addr, err := net.ResolveUnixAddr(\"unix\", *socketPath)\n    if err != nil {\n        log.Fatal(err)\n    }\n    l, err := net.ListenUnix(\"unix\", addr)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // More to come here...\n}\n</pre> <p>Now let’s add in each of the methods needed to initialize the UDF. These next methods implement the behavior described in Step 3 of the UDF Lifecycle above, where Kapacitor connects to the socket in order to query basic information about the UDF.</p> <p>Add these methods to the <code>main.go</code> file:</p> <pre data-language=\"go\">\n// Return the InfoResponse. Describing the properties of this UDF agent.\nfunc (*mirrorHandler) Info() (*udf.InfoResponse, error) {\n    info := &amp;udf.InfoResponse{\n        // We want a stream edge\n        Wants:    udf.EdgeType_STREAM,\n        // We provide a stream edge\n        Provides: udf.EdgeType_STREAM,\n        // We expect no options.\n        Options:  map[string]*udf.OptionInfo{},\n    }\n    return info, nil\n}\n\n// Initialze the handler based of the provided options.\nfunc (*mirrorHandler) Init(r *udf.InitRequest) (*udf.InitResponse, error) {\n    // Since we expected no options this method is trivial\n    // and we return success.\n    init := &amp;udf.InitResponse{\n        Success: true,\n        Error:   \"\",\n    }\n    return init, nil\n}\n</pre> <p>For now, our simple mirroring UDF doesn’t need any options, so these methods are trivial. At the end of this example we will modify the code to accept a custom option.</p> <p>Now that Kapacitor knows which edge types and options our UDF uses, we need to implement the methods for handling data.</p> <p>Add this method to the <code>main.go</code> file which sends back every point it receives to Kapacitor via the agent:</p> <pre data-language=\"go\">func (h *mirrorHandler) Point(p *udf.Point) error {\n    // Send back the point we just received\n    h.agent.Responses &lt;- &amp;udf.Response{\n        Message: &amp;udf.Response_Point{\n            Point: p,\n        },\n    }\n    return nil\n}\n</pre> <p>Notice that the <code>agent</code> has a channel for responses, this is because your UDF can send data to Kapacitor at any time, so it does not need to be in a response to receive a point.</p> <p>As a result, we need to close the channel to let the <code>agent</code> know that we will not be sending any more data, which can be done via the <code>Stop</code> method. Once the <code>agent</code> calls <code>Stop</code> on the <code>handler</code>, no other methods will be called and the <code>agent</code> won’t stop until the channel is closed. This gives the UDF the chance to flush out any remaining data before it’s shutdown:</p> <pre data-language=\"go\">// Stop the handler gracefully.\nfunc (h *mirrorHandler) Stop() {\n    // Close the channel since we won't be sending any more data to Kapacitor\n    close(h.agent.Responses)\n}\n</pre> <p>Even though we have implemented the majority of the handler implementation, there are still a few missing methods. Specifically, the methods around batching and snapshot/restores are missing, but, since we don’t need them, we will just give them trivial implementations:</p> <pre data-language=\"go\">// Create a snapshot of the running state of the process.\nfunc (*mirrorHandler) Snaphost() (*udf.SnapshotResponse, error) {\n    return &amp;udf.SnapshotResponse{}, nil\n}\n// Restore a previous snapshot.\nfunc (*mirrorHandler) Restore(req *udf.RestoreRequest) (*udf.RestoreResponse, error) {\n    return &amp;udf.RestoreResponse{\n        Success: true,\n    }, nil\n}\n\n// Start working with the next batch\nfunc (*mirrorHandler) BeginBatch(begin *udf.BeginBatch) error {\n    return errors.New(\"batching not supported\")\n}\nfunc (*mirrorHandler) EndBatch(end *udf.EndBatch) error {\n    return nil\n}\n</pre> <h3 id=\"the-server\">The Server</h3> <p>At this point we have a complete implementation of the <code>Handler</code> interface. In step #4 of the Lifecycle above, Kapacitor makes a new connection to the UDF for each use in a task. Since it’s possible that our UDF process can handle multiple connections simultaneously, we need a mechanism for creating a new <code>agent</code> and <code>handler</code> per connection.</p> <p>A <code>server</code> is provided for this purpose, which expects an implementation of the <code>Accepter</code> interface:</p> <pre data-language=\"go\">type Accepter interface {\n    // Accept new connections from the listener and handle them accordingly.\n    // The typical action is to create a new Agent with the connection as both its in and out objects.\n    Accept(net.Conn)\n}\n</pre> <p>Here is a simple <code>accepter</code> that creates a new <code>agent</code> and <code>mirrorHandler</code> for each new connection. Add this to the <code>main.go</code> file:</p> <pre data-language=\"go\">type accepter struct {\n    count int64\n}\n\n// Create a new agent/handler for each new connection.\n// Count and log each new connection and termination.\nfunc (acc *accepter) Accept(conn net.Conn) {\n    count := acc.count\n    acc.count++\n    a := agent.New(conn, conn)\n    h := newMirrorHandler(a)\n    a.Handler = h\n\n    log.Println(\"Starting agent for connection\", count)\n    a.Start()\n    go func() {\n        err := a.Wait()\n        if err != nil {\n            log.Fatal(err)\n        }\n        log.Printf(\"Agent for connection %d finished\", count)\n    }()\n}\n</pre> <p>Now with all the pieces in place, we can update our <code>main</code> function to start up the <code>server</code>. Replace the previously provided <code>main</code> function with:</p> <pre data-language=\"go\">func main() {\n    flag.Parse()\n\n    // Create unix socket\n    addr, err := net.ResolveUnixAddr(\"unix\", *socketPath)\n    if err != nil {\n        log.Fatal(err)\n    }\n    l, err := net.ListenUnix(\"unix\", addr)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Create server that listens on the socket\n    s := agent.NewServer(l, &amp;accepter{})\n\n    // Setup signal handler to stop Server on various signals\n    s.StopOnSignals(os.Interrupt, syscall.SIGTERM)\n\n    log.Println(\"Server listening on\", addr.String())\n    err = s.Serve()\n    if err != nil {\n        log.Fatal(err)\n    }\n    log.Println(\"Server stopped\")\n}\n</pre> <h2 id=\"start-the-udf\">Start the UDF</h2> <p>At this point we are ready to start the UDF. Here is the complete <code>main.go</code> file for reference:</p> <pre data-language=\"go\">package main\n\nimport (\n    \"errors\"\n    \"flag\"\n    \"log\"\n    \"net\"\n    \"os\"\n    \"syscall\"\n\n    \"github.com/influxdata/kapacitor/udf\"\n    \"github.com/influxdata/kapacitor/udf/agent\"\n)\n\n// Mirrors all points it receives back to Kapacitor\ntype mirrorHandler struct {\n    agent *agent.Agent\n}\n\nfunc newMirrorHandler(agent *agent.Agent) *mirrorHandler {\n    return &amp;mirrorHandler{agent: agent}\n}\n\n// Return the InfoResponse. Describing the properties of this UDF agent.\nfunc (*mirrorHandler) Info() (*udf.InfoResponse, error) {\n    info := &amp;udf.InfoResponse{\n        Wants:    udf.EdgeType_STREAM,\n        Provides: udf.EdgeType_STREAM,\n        Options:  map[string]*udf.OptionInfo{},\n    }\n    return info, nil\n}\n\n// Initialze the handler based of the provided options.\nfunc (*mirrorHandler) Init(r *udf.InitRequest) (*udf.InitResponse, error) {\n    init := &amp;udf.InitResponse{\n        Success: true,\n        Error:   \"\",\n    }\n    return init, nil\n}\n\n// Create a snapshot of the running state of the process.\nfunc (*mirrorHandler) Snaphost() (*udf.SnapshotResponse, error) {\n    return &amp;udf.SnapshotResponse{}, nil\n}\n\n// Restore a previous snapshot.\nfunc (*mirrorHandler) Restore(req *udf.RestoreRequest) (*udf.RestoreResponse, error) {\n    return &amp;udf.RestoreResponse{\n        Success: true,\n    }, nil\n}\n\n// Start working with the next batch\nfunc (*mirrorHandler) BeginBatch(begin *udf.BeginBatch) error {\n    return errors.New(\"batching not supported\")\n}\n\nfunc (h *mirrorHandler) Point(p *udf.Point) error {\n    // Send back the point we just received\n    h.agent.Responses &lt;- &amp;udf.Response{\n        Message: &amp;udf.Response_Point{\n            Point: p,\n        },\n    }\n    return nil\n}\n\nfunc (*mirrorHandler) EndBatch(end *udf.EndBatch) error {\n    return nil\n}\n\n// Stop the handler gracefully.\nfunc (h *mirrorHandler) Stop() {\n    close(h.agent.Responses)\n}\n\ntype accepter struct {\n    count int64\n}\n\n// Create a new agent/handler for each new connection.\n// Count and log each new connection and termination.\nfunc (acc *accepter) Accept(conn net.Conn) {\n    count := acc.count\n    acc.count++\n    a := agent.New(conn, conn)\n    h := newMirrorHandler(a)\n    a.Handler = h\n\n    log.Println(\"Starting agent for connection\", count)\n    a.Start()\n    go func() {\n        err := a.Wait()\n        if err != nil {\n            log.Fatal(err)\n        }\n        log.Printf(\"Agent for connection %d finished\", count)\n    }()\n}\n\nvar socketPath = flag.String(\"socket\", \"/tmp/mirror.sock\", \"Where to create the unix socket\")\n\nfunc main() {\n    flag.Parse()\n\n    // Create unix socket\n    addr, err := net.ResolveUnixAddr(\"unix\", *socketPath)\n    if err != nil {\n        log.Fatal(err)\n    }\n    l, err := net.ListenUnix(\"unix\", addr)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Create server that listens on the socket\n    s := agent.NewServer(l, &amp;accepter{})\n\n    // Setup signal handler to stop Server on various signals\n    s.StopOnSignals(os.Interrupt, syscall.SIGTERM)\n\n    log.Println(\"Server listening on\", addr.String())\n    err = s.Serve()\n    if err != nil {\n        log.Fatal(err)\n    }\n    log.Println(\"Server stopped\")\n}\n</pre> <p>Run <code>go run main.go</code> to start the UDF. If you get an error about the socket being in use, just delete the socket file and try running the UDF again.</p> <h2 id=\"configure-kapacitor-to-talk-to-the-udf\">Configure Kapacitor to Talk to the UDF</h2> <p>Now that our UDF is ready, we need to tell Kapacitor where our UDF socket is, and give it a name so that we can use it. Add this to your Kapacitor configuration file:</p> <pre>[udf]\n[udf.functions]\n    [udf.functions.mirror]\n        socket = \"/tmp/mirror.sock\"\n        timeout = \"10s\"\n</pre> <h2 id=\"start-kapacitor\">Start Kapacitor</h2> <p>Start up Kapacitor and you should see it connect to your UDF in both the Kapacitor logs and the UDF process logs.</p> <h2 id=\"try-it-out\">Try it out</h2> <p>Take an existing task and add <code>@mirror()</code> at any point in the TICKscript pipeline to see it in action.</p> <p>Here is an example TICKscript, which will need to be saved to a file:</p> <pre data-language=\"go\">stream\n    |from()\n        .measurement('cpu')\n    @mirror()\n    |alert()\n        .crit(lambda: \"usage_idle\" &lt; 30)\n</pre> <p>Define the above alert from your terminal like so:</p> <pre data-language=\"sh\">kapacitor define mirror_udf_example -type stream -dbrp telegraf.default -tick path/to/above/script.tick\n</pre> <p>Start the task:</p> <pre data-language=\"sh\">kapacitor enable mirror_udf_example\n</pre> <p>Check the status of the task:</p> <pre data-language=\"sh\">kapacitor show mirror_udf_example\n</pre> <h2 id=\"adding-a-custom-field\">Adding a Custom Field</h2> <p>Now let’s change the UDF to add a field to the data. We can use the <code>Info/Init</code> methods to define and consume an option on the UDF, so let’s specify the name of the field to add.</p> <p>Update the <code>mirrorHandler</code> type and the methods <code>Info</code> and <code>Init</code> as follows:</p> <pre data-language=\"go\">// Mirrors all points it receives back to Kapacitor\ntype mirrorHandler struct {\n    agent *agent.Agent\n    name  string\n    value float64\n}\n\n// Return the InfoResponse. Describing the properties of this UDF agent.\nfunc (*mirrorHandler) Info() (*udf.InfoResponse, error) {\n    info := &amp;udf.InfoResponse{\n        Wants:    udf.EdgeType_STREAM,\n        Provides: udf.EdgeType_STREAM,\n        Options: map[string]*udf.OptionInfo{\n            \"field\": {ValueTypes: []udf.ValueType{\n                udf.ValueType_STRING,\n                udf.ValueType_DOUBLE,\n            }},\n        },\n    }\n    return info, nil\n}\n\n// Initialze the handler based of the provided options.\nfunc (h *mirrorHandler) Init(r *udf.InitRequest) (*udf.InitResponse, error) {\n    init := &amp;udf.InitResponse{\n        Success: true,\n        Error:   \"\",\n    }\n    for _, opt := range r.Options {\n        switch opt.Name {\n        case \"field\":\n            h.name = opt.Values[0].Value.(*udf.OptionValue_StringValue).StringValue\n            h.value = opt.Values[1].Value.(*udf.OptionValue_DoubleValue).DoubleValue\n        }\n    }\n\n    if h.name == \"\" {\n        init.Success = false\n        init.Error = \"must supply field\"\n    }\n    return init, nil\n}\n</pre> <p>Now we can set the field with its name and value on the points. Update the <code>Point</code> method:</p> <pre data-language=\"go\">func (h *mirrorHandler) Point(p *udf.Point) error {\n    // Send back the point we just received\n    if p.FieldsDouble == nil {\n        p.FieldsDouble = make(map[string]float64)\n    }\n    p.FieldsDouble[h.name] = h.value\n\n    h.agent.Responses &lt;- &amp;udf.Response{\n        Message: &amp;udf.Response_Point{\n            Point: p,\n        },\n    }\n    return nil\n}\n</pre> <p>Restart the UDF process and try it out again. Specify which field name and value to use with the <code>.field(name, value)</code> method. You can add a <code>|log()</code> after the <code>mirror</code> UDF to see that the new field has indeed been created.</p> <pre data-language=\"go\">stream\n    |from()\n        .measurement('cpu')\n    @mirror()\n        .field('mycustom_field', 42.0)\n    |log()\n    |alert()\n        .cirt(lambda: \"usage_idle\" &lt; 30)\n</pre> <h2 id=\"summary\">Summary</h2> <p>At this point, you should be able to write custom UDFs using either the socket or process-based methods. UDFs have a wide range of uses, from custom downsampling logic as part of a continuous query, custom anomaly detection algorithms, or simply a system to “massage” your data a bit.</p> <h3 id=\"next-steps\">Next Steps</h3> <p>If you want to learn more, here are a few places to start:</p> <ul> <li>Modify the mirror UDF, to function like the <a href=\"../../nodes/default_node/index\">DefaultNode</a>. Instead of always overwriting a field, only set it if the field is not absent. Also add support for setting tags as well as fields.</li> <li>Change the mirror UDF to work on batches instead of streams. This requires changing the edge type in the <code>Info</code> method as well as implementing the <code>BeginBatch</code> and <code>EndBatch</code> methods.</li> <li>Take a look at the other <a href=\"https://github.com/influxdata/kapacitor/tree/master/udf/agent/examples\">examples</a> and modify one to do something similar to one of your existing requirements.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 InfluxData, Inc.<br>Licensed under the MIT license.<br>\n    <a href=\"https://docs.influxdata.com/kapacitor/v0.13/examples/socket_udf/\" class=\"_attribution-link\">https://docs.influxdata.com/kapacitor/v0.13/examples/socket_udf/</a>\n  </p>\n</div>\n"}