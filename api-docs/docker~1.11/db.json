{"index":"<h1>Docker Documentation</h1><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/</a>\n  </p>\n</div>\n","engine/quickstart/index":"<h1 id=\"docker-engine-quickstart\">Docker Engine Quickstart</h1> <p>This quickstart assumes you have a working installation of Docker Engine. To verify Engine is installed and configured, use the following command:</p> <pre># Check that you have a working install\n$ docker info\n</pre> <p>If you have a successful install, the system information appears. If you get <code>docker: command not found</code> or something like <code>/var/lib/docker/repositories: permission denied</code> you may have an incomplete Docker installation or insufficient privileges to access Engine on your machine. With the default installation of Engine <code>docker</code> commands need to be run by a user that is in the <code>docker</code> group or by the <code>root</code> user.</p> <p>Depending on your Engine system configuration, you may be required to preface each <code>docker</code> command with <code>sudo</code>. If you want to run without using <code>sudo</code> with the <code>docker</code> commands, then create a Unix group called <code>docker</code> and add the user to the ‘docker’ group.</p> <p>For more information about installing Docker Engine or <code>sudo</code> configuration, refer to the <a href=\"../installation/index\">installation</a> instructions for your operating system.</p> <h2 id=\"download-a-pre-built-image\">Download a pre-built image</h2> <p>To pull an <code>ubuntu</code> image, run:</p> <pre># Download an ubuntu image\n$ docker pull ubuntu\n</pre> <p>This downloads the <code>ubuntu</code> image by name from <a href=\"https://hub.docker.com\">Docker Hub</a> to a local image cache. To search for an image, run <code>docker search</code>. For more information, go to: <a href=\"../userguide/containers/dockerrepos/index#searching-for-images\">Searching images</a></p> <blockquote> <p><strong>Note</strong>: When the image is successfully downloaded, you see a 12 character hash <code>539c0211cd76: Download complete</code> which is the short form of the Image ID. These short Image IDs are the first 12 characters of the full Image ID. To view this information, run <code>docker inspect</code> or <code>docker images --no-trunc=true</code>.</p> </blockquote> <p>To display a list of downloaded images, run <code>docker images</code>.</p> <h2 id=\"running-an-interactive-shell\">Running an interactive shell</h2> <p>To run an interactive shell in the Ubuntu image:</p> <pre>$ docker run -i -t ubuntu /bin/bash       \n</pre> <p>The <code>-i</code> flag starts an interactive container. The <code>-t</code> flag creates a pseudo-TTY that attaches <code>stdin</code> and <code>stdout</code>.<br> The image is <code>ubuntu</code>. The command <code>/bin/bash</code> starts a shell you can log in.</p> <p>To detach the <code>tty</code> without exiting the shell, use the escape sequence <code>Ctrl-p</code> + <code>Ctrl-q</code>. The container continues to exist in a stopped state once exited. To list all running containers, run <code>docker ps</code>. To view stopped and running containers, run <code>docker ps -a</code>.</p> <h2 id=\"bind-docker-to-another-host-port-or-a-unix-socket\">Bind Docker to another host/port or a Unix socket</h2> <blockquote> <p><strong>Warning</strong>: Changing the default <code>docker</code> daemon binding to a TCP port or Unix <em>docker</em> user group will increase your security risks by allowing non-root users to gain <em>root</em> access on the host. Make sure you control access to <code>docker</code>. If you are binding to a TCP port, anyone with access to that port has full Docker access; so it is not advisable on an open network.</p> </blockquote> <p>With <code>-H</code> it is possible to make the Docker daemon to listen on a specific IP and port. By default, it will listen on <code>unix:///var/run/docker.sock</code> to allow only local connections by the <em>root</em> user. You <em>could</em> set it to <code>0.0.0.0:2375</code> or a specific host IP to give access to everybody, but that is <strong>not recommended</strong> because then it is trivial for someone to gain root access to the host where the daemon is running.</p> <p>Similarly, the Docker client can use <code>-H</code> to connect to a custom port. The Docker client will default to connecting to <code>unix:///var/run/docker.sock</code> on Linux, and <code>tcp://127.0.0.1:2376</code> on Windows.</p> <p><code>-H</code> accepts host and port assignment in the following format:</p> <pre>tcp://[host]:[port][path] or unix://path\n</pre> <p>For example:</p> <ul> <li>\n<code>tcp://</code> -&gt; TCP connection to <code>127.0.0.1</code> on either port <code>2376</code> when TLS encryption is on, or port <code>2375</code> when communication is in plain text.</li> <li>\n<code>tcp://host:2375</code> -&gt; TCP connection on host:2375</li> <li>\n<code>tcp://host:2375/path</code> -&gt; TCP connection on host:2375 and prepend path to all requests</li> <li>\n<code>unix://path/to/socket</code> -&gt; Unix socket located at <code>path/to/socket</code>\n</li> </ul> <p><code>-H</code>, when empty, will default to the same value as when no <code>-H</code> was passed in.</p> <p><code>-H</code> also accepts short form for TCP bindings:</p> <pre>`host:` or `host:port` or `:port`\n</pre> <p>Run Docker in daemon mode:</p> <pre>$ sudo &lt;path to&gt;/docker daemon -H 0.0.0.0:5555 &amp;\n</pre> <p>Download an <code>ubuntu</code> image:</p> <pre>$ docker -H :5555 pull ubuntu\n</pre> <p>You can use multiple <code>-H</code>, for example, if you want to listen on both TCP and a Unix socket</p> <pre># Run docker in daemon mode\n$ sudo &lt;path to&gt;/docker daemon -H tcp://127.0.0.1:2375 -H unix:///var/run/docker.sock &amp;\n# Download an ubuntu image, use default Unix socket\n$ docker pull ubuntu\n# OR use the TCP port\n$ docker -H tcp://127.0.0.1:2375 pull ubuntu\n</pre> <h2 id=\"starting-a-long-running-worker-process\">Starting a long-running worker process</h2> <pre># Start a very useful long-running process\n$ JOB=$(docker run -d ubuntu /bin/sh -c \"while true; do echo Hello world; sleep 1; done\")\n\n# Collect the output of the job so far\n$ docker logs $JOB\n\n# Kill the job\n$ docker kill $JOB\n</pre> <h2 id=\"listing-containers\">Listing containers</h2> <pre>$ docker ps # Lists only running containers\n$ docker ps -a # Lists all containers\n</pre> <h2 id=\"controlling-containers\">Controlling containers</h2> <pre># Start a new container\n$ JOB=$(docker run -d ubuntu /bin/sh -c \"while true; do echo Hello world; sleep 1; done\")\n\n# Stop the container\n$ docker stop $JOB\n\n# Start the container\n$ docker start $JOB\n\n# Restart the container\n$ docker restart $JOB\n\n# SIGKILL a container\n$ docker kill $JOB\n\n# Remove a container\n$ docker stop $JOB # Container must be stopped to remove it\n$ docker rm $JOB\n</pre> <h2 id=\"bind-a-service-on-a-tcp-port\">Bind a service on a TCP port</h2> <pre># Bind port 4444 of this container, and tell netcat to listen on it\n$ JOB=$(docker run -d -p 4444 ubuntu:12.10 /bin/nc -l 4444)\n\n# Which public port is NATed to my container?\n$ PORT=$(docker port $JOB 4444 | awk -F: '{ print $2 }')\n\n# Connect to the public port\n$ echo hello world | nc 127.0.0.1 $PORT\n\n# Verify that the network connection worked\n$ echo \"Daemon received: $(docker logs $JOB)\"\n</pre> <h2 id=\"committing-saving-a-container-state\">Committing (saving) a container state</h2> <p>To save the current state of a container as an image:</p> <pre>$ docker commit &lt;container&gt; &lt;some_name&gt;\n</pre> <p>When you commit your container, Docker Engine only stores the diff (difference) between the source image and the current state of the container’s image. To list images you already have, run:</p> <pre># List your images\n$ docker images\n</pre> <p>You now have an image state from which you can create new instances.</p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li>Work your way through the <a href=\"../userguide/index\">Docker Engine User Guide</a>\n</li> <li>Read more about <a href=\"../userguide/containers/dockerrepos/index\">Store Images on Docker Hub</a>\n</li> <li>Review <a href=\"../reference/commandline/cli/index\">Command Line</a>\n</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/quickstart/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/quickstart/</a>\n  </p>\n</div>\n","engine/installation/linux/fedora/index":"<h1 id=\"fedora\">Fedora</h1> <p>Docker is supported on Fedora version 22 and 23. This page instructs you to install using Docker-managed release packages and installation mechanisms. Using these packages ensures you get the latest release of Docker. If you wish to install using Fedora-managed packages, consult your Fedora release documentation for information on Fedora’s Docker support.</p> <h2 id=\"prerequisites\">Prerequisites</h2> <p>Docker requires a 64-bit installation regardless of your Fedora version. Also, your kernel must be 3.10 at minimum. To check your current kernel version, open a terminal and use <code>uname -r</code> to display your kernel version:</p> <pre>$ uname -r\n3.19.5-100.fc21.x86_64\n</pre> <p>If your kernel is at an older version, you must update it.</p> <p>Finally, is it recommended that you fully update your system. Please keep in mind that your system should be fully patched to fix any potential kernel bugs. Any reported kernel bugs may have already been fixed on the latest kernel packages</p> <h2 id=\"install\">Install</h2> <p>There are two ways to install Docker Engine. You can install with the <code>dnf</code> package manager. Or you can use <code>curl</code> with the <code>get.docker.com</code> site. This second method runs an installation script which also installs via the <code>dnf</code> package manager.</p> <h3 id=\"install-with-dnf\">Install with DNF</h3> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Make sure your existing dnf packages are up-to-date.</p> <pre>$ sudo dnf update\n</pre>\n</li> <li>\n<p>Add the yum repo yourself.</p> <pre>$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/fedora/$releasever/\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n</pre>\n</li> <li>\n<p>Install the Docker package.</p> <pre>$ sudo dnf install docker-engine\n</pre>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo systemctl start docker\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly by running a test image in a container.</p> <pre>$ sudo docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from hello-world\na8219747be10: Pull complete\n91c95931e552: Already exists\nhello-world:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.\nDigest: sha256:aa03e5d0d5553b4c3473e89c8619cf79df368babd1.7.1cf5daeb82aab55838d\nStatus: Downloaded newer image for hello-world:latest\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (Assuming it was not already locally available.)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nFor more examples and ideas, visit:\n http://docs.docker.com/userguide/\n</pre>\n</li> </ol> <h3 id=\"install-with-the-script\">Install with the script</h3> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Make sure your existing dnf packages are up-to-date.</p> <pre>$ sudo dnf update\n</pre>\n</li> <li>\n<p>Run the Docker installation script.</p> <pre>$ curl -fsSL https://get.docker.com/ | sh\n</pre> <p>This script adds the <code>docker.repo</code> repository and installs Docker.</p>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo systemctl start docker\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly by running a test image in a container.</p> <pre>$ sudo docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"create-a-docker-group\">Create a docker group</h2> <p>The <code>docker</code> daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user <code>root</code> and other users can access it with <code>sudo</code>. For this reason, <code>docker</code> daemon always runs as the <code>root</code> user.</p> <p>To avoid having to use <code>sudo</code> when you use the <code>docker</code> command, create a Unix group called <code>docker</code> and add users to it. When the <code>docker</code> daemon starts, it makes the ownership of the Unix socket read/writable by the <code>docker</code> group.</p> <blockquote> <p><strong>Warning</strong>: The <code>docker</code> group is equivalent to the <code>root</code> user; For details on how this impacts security in your system, see <a href=\"../../../security/security/index#docker-daemon-attack-surface\"><em>Docker Daemon Attack Surface</em></a> for details.</p> </blockquote> <p>To create the <code>docker</code> group and add your user:</p> <ol> <li><p>Log into your system as a user with <code>sudo</code> privileges.</p></li> <li>\n<p>Create the <code>docker</code> group.</p> <p><code>sudo groupadd docker</code></p>\n</li> <li>\n<p>Add your user to <code>docker</code> group.</p> <p><code>sudo usermod -aG docker your_username</code></p>\n</li> <li>\n<p>Log out and log back in.</p> <p>This ensures your user is running with the correct permissions.</p>\n</li> <li>\n<p>Verify your work by running <code>docker</code> without <code>sudo</code>.</p> <pre>$ docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"start-the-docker-daemon-at-boot\">Start the docker daemon at boot</h2> <p>To ensure Docker starts when you boot your system, do the following:</p> <pre>$ sudo systemctl enable docker\n</pre> <p>If you need to add an HTTP Proxy, set a different directory or partition for the Docker runtime files, or make other customizations, read our Systemd article to learn how to <a href=\"../../../admin/systemd/index\">customize your Systemd Docker daemon options</a>.</p> <h2 id=\"running-docker-with-a-manually-defined-network\">Running Docker with a manually-defined network</h2> <p>If you manually configure your network using <code>systemd-network</code> with <code>systemd</code> version 219 or higher, containers you start with Docker may be unable to access your network. Beginning with version 220, the forwarding setting for a given network (<code>net.ipv4.conf.&lt;interface&gt;.forwarding</code>) defaults to <em>off</em>. This setting prevents IP forwarding. It also conflicts with Docker which enables the <code>net.ipv4.conf.all.forwarding</code> setting within a container.</p> <p>To work around this, edit the <code>&lt;interface&gt;.network</code> file in <code>/usr/lib/systemd/network/</code> on your Docker host (ex: <code>/usr/lib/systemd/network/80-container-host0.network</code>) add the following block:</p> <pre>[Network]\n...\nIPForward=kernel\n# OR\nIPForward=true\n...\n</pre> <p>This configuration allows IP forwarding from the container as expected.</p> <h2 id=\"uninstall\">Uninstall</h2> <p>You can uninstall the Docker software with <code>dnf</code>.</p> <ol> <li>\n<p>List the package you have installed.</p> <pre>$ dnf list installed | grep docker\ndocker-engine.x86_64     1.7.1-0.1.fc21 @/docker-engine-1.7.1-0.1.fc21.el7.x86_64\n</pre>\n</li> <li>\n<p>Remove the package.</p> <pre>$ sudo dnf -y remove docker-engine.x86_64\n</pre> <p>This command does not remove images, containers, volumes, or user-created configuration files on your host.</p>\n</li> <li>\n<p>To delete all images, containers, and volumes, run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre>\n</li> <li><p>Locate and delete any user-created configuration files.</p></li> </ol><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/fedora/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/fedora/</a>\n  </p>\n</div>\n","engine/installation/windows/index":"<h1 id=\"windows\">Windows</h1> <blockquote> <p><strong>Note</strong>: This release of Docker deprecates the Boot2Docker command line in favor of Docker Machine. Use the Docker Toolbox to install Docker Machine as well as the other Docker tools.</p> </blockquote> <p>You install Docker using Docker Toolbox. Docker Toolbox includes the following Docker tools:</p> <ul> <li>Docker Machine for running the <code>docker-machine</code> binary</li> <li>Docker Engine for running the <code>docker</code> binary</li> <li>Kitematic, the Docker GUI</li> <li>a shell preconfigured for a Docker command-line environment</li> <li>Oracle VM VirtualBox</li> </ul> <p>Because the Docker daemon uses Linux-specific kernel features, you can’t run Docker natively in Windows. Instead, you must use <code>docker-machine</code> to create and attach to a Docker VM on your machine. This VM hosts Docker for you on your Windows system.</p> <p>The virtual machine runs a lightweight Linux distribution made specifically to run the Docker daemon. The VirtualBox VM runs completely from RAM, is a small ~24MB download, and boots in approximately 5s.</p> <h2 id=\"requirements\">Requirements</h2> <p>To run Docker, your machine must have a 64-bit operating system running Windows 7 or higher. Additionally, you must make sure that virtualization is enabled on your machine. To verify your machine meets these requirements, do the following:</p> <ol> <li>\n<p>Right click the Windows Start Menu and choose <strong>System</strong>.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/win_ver.png\" alt=\"Which version\"></p> <p>If you are using an unsupported version of Windows, you should consider upgrading your operating system in order to try out Docker.</p>\n</li> <li>\n<p>Make sure your CPU supports <a href=\"https://en.wikipedia.org/wiki/X86_virtualization\">virtualization technology</a> and virtualization support is enabled in BIOS and recognized by Windows.</p> <h4 id=\"for-windows-8-8-1-or-10\">For Windows 8, 8.1 or 10</h4> <p>Choose <strong>Start &gt; Task Manager</strong>. On Windows 10, click more details. Navigate to the <strong>Performance</strong> tab. Under <strong>CPU</strong> you should see the following:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/virtualization.png\" alt=\"Release page\"></p> <p>If virtualization is not enabled on your system, follow the manufacturer’s instructions for enabling it.</p> <h4 id=\"for-windows-7\">For Windows 7</h4> <p>Run the <a href=\"http://www.microsoft.com/en-us/download/details.aspx?id=592\" target=\"_blank\"> Microsoft® Hardware-Assisted Virtualization Detection Tool</a> and follow the on-screen instructions.</p>\n</li> <li><p>Verify your Windows OS is 64-bit (x64)</p></li> </ol> <p>How you do this verification depends on your Windows version. For details, see the Windows article <a href=\"https://support.microsoft.com/en-us/kb/827218\">How to determine whether a computer is running a 32-bit version or 64-bit version of the Windows operating system</a>.</p> <blockquote> <p><strong>Note</strong>: If you have Docker hosts running and you don’t wish to do a Docker Toolbox installation, you can install the <code>docker.exe</code> using the <em>unofficial</em> Windows package manager Chocolatey. For information on how to do this, see <a href=\"http://chocolatey.org/packages/docker\">Docker package on Chocolatey</a>.</p> </blockquote> <h3 id=\"learn-the-key-concepts-before-installing\">Learn the key concepts before installing</h3> <p>In a Docker installation on Linux, your machine is both the localhost and the Docker host. In networking, localhost means your computer. The Docker host is the machine on which the containers run.</p> <p>On a typical Linux installation, the Docker client, the Docker daemon, and any containers run directly on your localhost. This means you can address ports on a Docker container using standard localhost addressing such as <code>localhost:8000</code> or <code>0.0.0.0:8376</code>.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/linux_docker_host.svg\" alt=\"Linux Architecture Diagram\"></p> <p>In a Windows installation, the <code>docker</code> daemon is running inside a Linux virtual machine. You use the Windows Docker client to talk to the Docker host VM. Your Docker containers run inside this host.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/win_docker_host.svg\" alt=\"Windows Architecture Diagram\"></p> <p>In Windows, the Docker host address is the address of the Linux VM. When you start the VM with <code>docker-machine</code> it is assigned an IP address. When you start a container, the ports on a container map to ports on the VM. To see this in practice, work through the exercises on this page.</p> <h3 id=\"installation\">Installation</h3> <p>If you have VirtualBox running, you must shut it down before running the installer.</p> <ol> <li><p>Go to the <a href=\"https://www.docker.com/toolbox\">Docker Toolbox</a> page.</p></li> <li><p>Click the installer link to download.</p></li> <li>\n<p>Install Docker Toolbox by double-clicking the installer.</p> <p>The installer launches the “Setup - Docker Toolbox” dialog.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/win-welcome.png\" alt=\"Install Docker Toolbox\"></p>\n</li> <li>\n<p>Press “Next” to install the toolbox.</p> <p>The installer presents you with options to customize the standard installation. By default, the standard Docker Toolbox installation:</p> <ul> <li>installs executables for the Docker tools in <code>C:\\Program Files\\Docker Toolbox</code>\n</li> <li>install VirtualBox; or updates any existing installation</li> <li>adds a Docker Inc. folder to your program shortcuts</li> <li>updates your <code>PATH</code> environment variable</li> <li>adds desktop icons for the Docker Quickstart Terminal and Kitematic</li> </ul> <p>This installation assumes the defaults are acceptable.</p>\n</li> <li>\n<p>Press “Next” until you reach the “Ready to Install” page.</p> <p>The system prompts you for your password.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/win-page-6.png\" alt=\"Install\"></p>\n</li> <li>\n<p>Press “Install” to continue with the installation.</p> <p>When it completes, the installer provides you with some information you can use to complete some common tasks.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/windows-finish.png\" alt=\"All finished\"></p>\n</li> <li><p>Press “Finish” to exit.</p></li> </ol> <h2 id=\"running-a-docker-container\">Running a Docker Container</h2> <p>To run a Docker container, you:</p> <ul> <li>Create a new (or start an existing) Docker virtual machine</li> <li>Switch your environment to your new VM</li> <li>Use the <code>docker</code> client to create, load, and manage containers</li> </ul> <p>Once you create a machine, you can reuse it as often as you like. Like any VirtualBox VM, it maintains its configuration between uses.</p> <p>There are several ways to use the installed tools, from the Docker Quickstart Terminal or <a href=\"#from-your-shell\">from your shell</a>.</p> <h3 id=\"using-the-docker-quickstart-terminal\">Using the Docker Quickstart Terminal</h3> <ol> <li>\n<p>Find the Docker Quickstart Terminal icon on your Desktop and double-click to launch it.</p> <p>The application:</p> <ul> <li>Opens a terminal window</li> <li>Creates a <code>default</code> VM if it doesn’t exist, and starts the VM after</li> <li>Points the terminal environment to this VM</li> </ul> <p>Once the launch completes, you can run <code>docker</code> commands.</p>\n</li> <li>\n<p>Verify your setup succeeded by running the <code>hello-world</code> container.</p> <pre>$ docker run hello-world\nUnable to find image 'hello-world:latest' locally\n511136ea3c5a: Pull complete\n31cbccb51277: Pull complete\ne45a5af57b00: Pull complete\nhello-world:latest: The image you are pulling has been verified.\nImportant: image verification is a tech preview feature and should not be\nrelied on to provide security.\nStatus: Downloaded newer image for hello-world:latest\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n   (Assuming it was not already locally available.)\n3. The Docker daemon created a new container from that image which runs the\n   executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\n   to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nFor more examples and ideas, visit:\nhttp://docs.docker.com/userguide/\n</pre>\n</li> </ol> <h3 id=\"using-docker-from-windows-command-prompt-cmd-exe\">Using Docker from Windows Command Prompt (cmd.exe)</h3> <ol> <li>\n<p>Launch a Windows Command Prompt (cmd.exe).</p> <p>The <code>docker-machine</code> command requires <code>ssh.exe</code> in your <code>PATH</code> environment variable. This <code>.exe</code> is in the MsysGit <code>bin</code> folder.</p>\n</li> <li>\n<p>Add this to the <code>%PATH%</code> environment variable by running:</p> <pre>set PATH=%PATH%;\"c:\\Program Files (x86)\\Git\\bin\"\n</pre>\n</li> <li>\n<p>Create a new Docker VM.</p> <pre>docker-machine create --driver virtualbox my-default\nCreating VirtualBox VM...\nCreating SSH key...\nStarting VirtualBox VM...\nStarting VM...\nTo see how to connect Docker to this machine, run: docker-machine env my-default\n</pre> <p>The command also creates a machine configuration in the <code>C:\\USERS\\USERNAME\\.docker\\machine\\machines</code> directory. You only need to run the <code>create</code> command once. Then, you can use <code>docker-machine</code> to start, stop, query, and otherwise manage the VM from the command line.</p>\n</li> <li>\n<p>List your available machines.</p> <pre>C:\\Users\\mary&gt; docker-machine ls\nNAME                ACTIVE   DRIVER       STATE     URL                         SWARM\nmy-default        *        virtualbox   Running   tcp://192.168.99.101:2376\n</pre> <p>If you have previously installed the deprecated Boot2Docker application or run the Docker Quickstart Terminal, you may have a <code>dev</code> VM as well.</p>\n</li> <li>\n<p>Get the environment commands for your new VM.</p> <pre>C:\\Users\\mary&gt; docker-machine env --shell cmd my-default\n</pre>\n</li> <li>\n<p>Connect your shell to the <code>my-default</code> machine.</p> <pre>C:\\Users\\mary&gt; eval \"$(docker-machine env my-default)\"\n</pre>\n</li> <li>\n<p>Run the <code>hello-world</code> container to verify your setup.</p> <pre>C:\\Users\\mary&gt; docker run hello-world\n</pre>\n</li> </ol> <h3 id=\"using-docker-from-powershell\">Using Docker from PowerShell</h3> <ol> <li><p>Launch a Windows PowerShell window.</p></li> <li>\n<p>Add <code>ssh.exe</code> to your PATH:</p> <pre>PS C:\\Users\\mary&gt; $Env:Path = \"${Env:Path};c:\\Program Files (x86)\\Git\\bin\"\n</pre>\n</li> <li>\n<p>Create a new Docker VM.</p> <pre>PS C:\\Users\\mary&gt; docker-machine create --driver virtualbox my-default\n</pre>\n</li> <li>\n<p>List your available machines.</p> <pre>C:\\Users\\mary&gt; docker-machine ls\nNAME                ACTIVE   DRIVER       STATE     URL                         SWARM\nmy-default        *        virtualbox   Running   tcp://192.168.99.101:2376\n</pre>\n</li> <li>\n<p>Get the environment commands for your new VM.</p> <pre>C:\\Users\\mary&gt; docker-machine env --shell powershell my-default\n</pre>\n</li> <li>\n<p>Connect your shell to the <code>my-default</code> machine.</p> <pre>C:\\Users\\mary&gt; eval \"$(docker-machine env my-default)\"\n</pre>\n</li> <li>\n<p>Run the <code>hello-world</code> container to verify your setup.</p> <pre>C:\\Users\\mary&gt; docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"learn-about-your-toolbox-installation\">Learn about your Toolbox installation</h2> <p>Toolbox installs the Docker Engine binary in the <code>C:\\Program Files\\Docker\nToolbox</code> directory. When you use the Docker Quickstart Terminal or create a <code>default</code> VM manually, Docker Machine updates the <code>C:\\USERS\\USERNAME\\.docker\\machine\\machines\\default</code> folder to your system. This folder contains the configuration for the VM.</p> <p>You can create multiple VMs on your system with Docker Machine. Therefore, you may end up with multiple VM folders if you have created more than one VM. To remove a VM, use the <code>docker-machine rm &lt;machine-name&gt;</code> command.</p> <h2 id=\"migrate-from-boot2docker\">Migrate from Boot2Docker</h2> <p>If you were using Boot2Docker previously, you have a pre-existing Docker <code>boot2docker-vm</code> VM on your local system. To allow Docker Machine to manage this older VM, you can migrate it.</p> <ol> <li><p>Open a terminal or the Docker CLI on your system.</p></li> <li>\n<p>Type the following command.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-import-boot2docker-vm boot2docker-vm docker-vm\n</pre>\n</li> <li><p>Use the <code>docker-machine</code> command to interact with the migrated VM.</p></li> </ol> <p>The <code>docker-machine</code> subcommands are slightly different than the <code>boot2docker</code> subcommands. The table below lists the equivalent <code>docker-machine</code> subcommand and what it does:</p> <table> <thead> <tr> <th><code>boot2docker</code></th> <th><code>docker-machine</code></th> <th>\n<code>docker-machine</code> description</th> </tr> </thead> <tbody> <tr> <td>init</td> <td>create</td> <td>Creates a new docker host.</td> </tr> <tr> <td>up</td> <td>start</td> <td>Starts a stopped machine.</td> </tr> <tr> <td>ssh</td> <td>ssh</td> <td>Runs a command or interactive ssh session on the machine.</td> </tr> <tr> <td>save</td> <td>-</td> <td>Not applicable.</td> </tr> <tr> <td>down</td> <td>stop</td> <td>Stops a running machine.</td> </tr> <tr> <td>poweroff</td> <td>stop</td> <td>Stops a running machine.</td> </tr> <tr> <td>reset</td> <td>restart</td> <td>Restarts a running machine.</td> </tr> <tr> <td>config</td> <td>inspect</td> <td>Prints machine configuration details.</td> </tr> <tr> <td>status</td> <td>ls</td> <td>Lists all machines and their status.</td> </tr> <tr> <td>info</td> <td>inspect</td> <td>Displays a machine’s details.</td> </tr> <tr> <td>ip</td> <td>ip</td> <td>Displays the machine’s ip address.</td> </tr> <tr> <td>shellinit</td> <td>env</td> <td>Displays shell commands needed to configure your shell to interact with a machine</td> </tr> <tr> <td>delete</td> <td>rm</td> <td>Removes a machine.</td> </tr> <tr> <td>download</td> <td>-</td> <td>Not applicable.</td> </tr> <tr> <td>upgrade</td> <td>upgrade</td> <td>Upgrades a machine’s Docker client to the latest stable release.</td> </tr> </tbody> </table> <h2 id=\"upgrade-docker-toolbox\">Upgrade Docker Toolbox</h2> <p>To upgrade Docker Toolbox, download and re-run <a href=\"https://www.docker.com/toolbox\">the Docker Toolbox installer</a>.</p> <h2 id=\"container-port-redirection\">Container port redirection</h2> <p>If you are curious, the username for the Docker default VM is <code>docker</code> and the password is <code>tcuser</code>. The latest version of <code>docker-machine</code> sets up a host only network adaptor which provides access to the container’s ports.</p> <p>If you run a container with a published port:</p> <pre>$ docker run --rm -i -t -p 80:80 nginx\n</pre> <p>Then you should be able to access that nginx server using the IP address reported to you using:</p> <pre>$ docker-machine ip\n</pre> <p>Typically, the IP is 192.168.59.103, but it could get changed by VirtualBox’s DHCP implementation.</p> <blockquote> <p><strong>Note</strong>: There is a <a href=\"https://docs.docker.com/machine/drivers/virtualbox/#known-issues\">known issue</a> that may cause files shared with your nginx container to not update correctly as you modify them on your host.</p> </blockquote> <h2 id=\"login-with-putty-instead-of-using-the-cmd\">Login with PUTTY instead of using the CMD</h2> <p>Docker Machine generates and uses the public/private key pair in your <code>%USERPROFILE%\\.docker\\machine\\machines\\&lt;name_of_your_machine&gt;</code> directory. To log in you need to use the private key from this same directory. The private key needs to be converted into the format PuTTY uses. You can do this with <a href=\"http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html\">puttygen</a>:</p> <ol> <li>\n<p>Open <code>puttygen.exe</code> and load (“File”-&gt;“Load” menu) the private key from (you may need to change to the <code>All Files (*.*)</code> filter)</p> <pre>%USERPROFILE%\\.docker\\machine\\machines\\&lt;name_of_your_machine&gt;\\id_rsa\n</pre>\n</li> <li><p>Click “Save Private Key”.</p></li> <li><p>Use the saved file to login with PuTTY using <code>docker@127.0.0.1:2022</code>.</p></li> </ol> <h2 id=\"uninstallation\">Uninstallation</h2> <p>You can uninstall Docker Toolbox using Window’s standard process for removing programs. This process does not remove the <code>docker-install.exe</code> file. You must delete that file yourself.</p> <h2 id=\"learn-more\">Learn more</h2> <p>You can continue with the <a href=\"../../userguide/index\">Docker Engine User Guide</a>. If you are interested in using the Kitematic GUI, see the <a href=\"https://docs.docker.com/kitematic/userguide/\">Kitematic user guide</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/windows/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/windows/</a>\n  </p>\n</div>\n","engine/installation/linux/rhel/index":"<h1 id=\"red-hat-enterprise-linux\">Red Hat Enterprise Linux</h1> <p>Docker is supported on Red Hat Enterprise Linux 7. This page instructs you to install using Docker-managed release packages and installation mechanisms. Using these packages ensures you get the latest release of Docker. If you wish to install using Red Hat-managed packages, consult your Red Hat release documentation for information on Red Hat’s Docker support.</p> <h2 id=\"prerequisites\">Prerequisites</h2> <p>Docker requires a 64-bit installation regardless of your Red Hat version. Docker requires that your kernel must be 3.10 at minimum, which Red Hat 7 runs.</p> <p>To check your current kernel version, open a terminal and use <code>uname -r</code> to display your kernel version:</p> <pre>$ uname -r\n3.10.0-229.el7.x86_64\n</pre> <p>Finally, is it recommended that you fully update your system. Please keep in mind that your system should be fully patched to fix any potential kernel bugs. Any reported kernel bugs may have already been fixed on the latest kernel packages.</p> <h2 id=\"install-docker-engine\">Install Docker Engine</h2> <p>There are two ways to install Docker Engine. You can install with the <code>yum</code> package manager directly yourself. Or you can use <code>curl</code> with the <code>get.docker.com</code> site. This second method runs an installation script which installs via the <code>yum</code> package manager.</p> <h3 id=\"install-with-yum\">Install with yum</h3> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Make sure your existing yum packages are up-to-date.</p> <pre>$ sudo yum update\n</pre>\n</li> <li>\n<p>Add the yum repo yourself.</p> <pre>$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-EOF\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n</pre>\n</li> <li>\n<p>Install the Docker package.</p> <pre>$ sudo yum install docker-engine\n</pre>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo service docker start\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly by running a test image in a container.</p> <pre>$ sudo docker run hello-world\nUnable to find image 'hello-world:latest' locally\n    latest: Pulling from hello-world\n    a8219747be10: Pull complete\n    91c95931e552: Already exists\n    hello-world:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.\n    Digest: sha256:aa03e5d0d5553b4c3473e89c8619cf79df368babd1.7.1cf5daeb82aab55838d\n    Status: Downloaded newer image for hello-world:latest\n    Hello from Docker.\n    This message shows that your installation appears to be working correctly.\n\n    To generate this message, Docker took the following steps:\n     1. The Docker client contacted the Docker daemon.\n     2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n            (Assuming it was not already locally available.)\n     3. The Docker daemon created a new container from that image which runs the\n            executable that produces the output you are currently reading.\n     4. The Docker daemon streamed that output to the Docker client, which sent it\n            to your terminal.\n\n    To try something more ambitious, you can run an Ubuntu container with:\n     $ docker run -it ubuntu bash\n\n    For more examples and ideas, visit:\n     http://docs.docker.com/userguide/\n</pre>\n</li> </ol> <h3 id=\"install-with-the-script\">Install with the script</h3> <p>You use the same installation procedure for all versions of CentOS.</p> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Make sure your existing yum packages are up-to-date.</p> <pre>$ sudo yum update\n</pre>\n</li> <li>\n<p>Run the Docker installation script.</p> <pre>$ curl -fsSL https://get.docker.com/ | sh\n</pre>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo service docker start\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly by running a test image in a container.</p> <pre>$ sudo docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"create-a-docker-group\">Create a docker group</h2> <p>The <code>docker</code> daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user <code>root</code> and other users can access it with <code>sudo</code>. For this reason, <code>docker</code> daemon always runs as the <code>root</code> user.</p> <p>To avoid having to use <code>sudo</code> when you use the <code>docker</code> command, create a Unix group called <code>docker</code> and add users to it. When the <code>docker</code> daemon starts, it makes the ownership of the Unix socket read/writable by the <code>docker</code> group.</p> <blockquote> <p><strong>Warning</strong>: The <code>docker</code> group is equivalent to the <code>root</code> user; For details on how this impacts security in your system, see <a href=\"../../../security/security/index#docker-daemon-attack-surface\"><em>Docker Daemon Attack Surface</em></a> for details.</p> </blockquote> <p>To create the <code>docker</code> group and add your user:</p> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Create the <code>docker</code> group.</p> <p><code>sudo groupadd docker</code></p>\n</li> <li>\n<p>Add your user to <code>docker</code> group.</p> <p><code>sudo usermod -aG docker your_username</code></p>\n</li> <li>\n<p>Log out and log back in.</p> <p>This ensures your user is running with the correct permissions.</p>\n</li> <li>\n<p>Verify your work by running <code>docker</code> without <code>sudo</code>.</p> <pre>    $ docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"start-the-docker-daemon-at-boot\">Start the docker daemon at boot</h2> <p>To ensure Docker starts when you boot your system, do the following:</p> <pre>$ sudo chkconfig docker on\n</pre> <p>If you need to add an HTTP Proxy, set a different directory or partition for the Docker runtime files, or make other customizations, read our Systemd article to learn how to <a href=\"../../../admin/systemd/index\">customize your Systemd Docker daemon options</a>.</p> <h2 id=\"uninstall\">Uninstall</h2> <p>You can uninstall the Docker software with <code>yum</code>.</p> <ol> <li>\n<p>List the package you have installed.</p> <pre>$ yum list installed | grep docker\nyum list installed | grep docker\ndocker-engine.x86_64                1.7.1-0.1.el7@/docker-engine-1.7.1-0.1.el7.x86_64\n</pre>\n</li> <li>\n<p>Remove the package.</p> <pre>$ sudo yum -y remove docker-engine.x86_64\n</pre> <p>This command does not remove images, containers, volumes, or user created configuration files on your host.</p>\n</li> <li>\n<p>To delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre>\n</li> <li><p>Locate and delete any user-created configuration files.</p></li> </ol><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/rhel/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/rhel/</a>\n  </p>\n</div>\n","engine/installation/linux/centos/index":"<h1 id=\"centos\">CentOS</h1> <p>Docker runs on CentOS 7.X. An installation on other binary compatible EL7 distributions such as Scientific Linux might succeed, but Docker does not test or support Docker on these distributions.</p> <p>This page instructs you to install using Docker-managed release packages and installation mechanisms. Using these packages ensures you get the latest release of Docker. If you wish to install using CentOS-managed packages, consult your CentOS documentation.</p> <h2 id=\"prerequisites\">Prerequisites</h2> <p>Docker requires a 64-bit installation regardless of your CentOS version. Also, your kernel must be 3.10 at minimum, which CentOS 7 runs.</p> <p>To check your current kernel version, open a terminal and use <code>uname -r</code> to display your kernel version:</p> <pre>$ uname -r\n3.10.0-229.el7.x86_64\n</pre> <p>Finally, it is recommended that you fully update your system. Please keep in mind that your system should be fully patched to fix any potential kernel bugs. Any reported kernel bugs may have already been fixed on the latest kernel packages.</p> <h2 id=\"install\">Install</h2> <p>There are two ways to install Docker Engine. You can install using the <code>yum</code> package manager. Or you can use <code>curl</code> with the <code>get.docker.com</code> site. This second method runs an installation script which also installs via the <code>yum</code> package manager.</p> <h3 id=\"install-with-yum\">Install with yum</h3> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Make sure your existing yum packages are up-to-date.</p> <pre>$ sudo yum update\n</pre>\n</li> <li>\n<p>Add the yum repo.</p> <pre>$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n</pre>\n</li> <li>\n<p>Install the Docker package.</p> <pre>$ sudo yum install docker-engine\n</pre>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo service docker start\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly by running a test image in a container.</p> <pre>$ sudo docker run hello-world\nUnable to find image 'hello-world:latest' locally\n    latest: Pulling from hello-world\n    a8219747be10: Pull complete\n    91c95931e552: Already exists\n    hello-world:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.\n    Digest: sha256:aa03e5d0d5553b4c3473e89c8619cf79df368babd1.7.1cf5daeb82aab55838d\n    Status: Downloaded newer image for hello-world:latest\n    Hello from Docker.\n    This message shows that your installation appears to be working correctly.\n\n    To generate this message, Docker took the following steps:\n     1. The Docker client contacted the Docker daemon.\n     2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n            (Assuming it was not already locally available.)\n     3. The Docker daemon created a new container from that image which runs the\n            executable that produces the output you are currently reading.\n     4. The Docker daemon streamed that output to the Docker client, which sent it\n            to your terminal.\n\n    To try something more ambitious, you can run an Ubuntu container with:\n     $ docker run -it ubuntu bash\n\n    For more examples and ideas, visit:\n     http://docs.docker.com/userguide/\n</pre>\n</li> </ol> <h3 id=\"install-with-the-script\">Install with the script</h3> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Make sure your existing yum packages are up-to-date.</p> <pre>$ sudo yum update\n</pre>\n</li> <li>\n<p>Run the Docker installation script.</p> <pre>$ curl -fsSL https://get.docker.com/ | sh\n</pre> <p>This script adds the <code>docker.repo</code> repository and installs Docker.</p>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo service docker start\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly by running a test image in a container.</p> <pre>$ sudo docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"create-a-docker-group\">Create a docker group</h2> <p>The <code>docker</code> daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user <code>root</code> and other users can access it with <code>sudo</code>. For this reason, <code>docker</code> daemon always runs as the <code>root</code> user.</p> <p>To avoid having to use <code>sudo</code> when you use the <code>docker</code> command, create a Unix group called <code>docker</code> and add users to it. When the <code>docker</code> daemon starts, it makes the ownership of the Unix socket read/writable by the <code>docker</code> group.</p> <blockquote> <p><strong>Warning</strong>: The <code>docker</code> group is equivalent to the <code>root</code> user; For details on how this impacts security in your system, see <a href=\"../../../security/security/index#docker-daemon-attack-surface\"><em>Docker Daemon Attack Surface</em></a> for details.</p> </blockquote> <p>To create the <code>docker</code> group and add your user:</p> <ol> <li><p>Log into Centos as a user with <code>sudo</code> privileges.</p></li> <li>\n<p>Create the <code>docker</code> group.</p> <p><code>sudo groupadd docker</code></p>\n</li> <li>\n<p>Add your user to <code>docker</code> group.</p> <p><code>sudo usermod -aG docker your_username</code></p>\n</li> <li>\n<p>Log out and log back in.</p> <p>This ensures your user is running with the correct permissions.</p>\n</li> <li>\n<p>Verify your work by running <code>docker</code> without <code>sudo</code>.</p> <pre>$ docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"start-the-docker-daemon-at-boot\">Start the docker daemon at boot</h2> <p>To ensure Docker starts when you boot your system, do the following:</p> <pre>  $ sudo chkconfig docker on\n</pre> <p>If you need to add an HTTP Proxy, set a different directory or partition for the Docker runtime files, or make other customizations, read our Systemd article to learn how to <a href=\"../../../admin/systemd/index\">customize your Systemd Docker daemon options</a>.</p> <h2 id=\"uninstall\">Uninstall</h2> <p>You can uninstall the Docker software with <code>yum</code>.</p> <ol> <li>\n<p>List the package you have installed.</p> <pre>$ yum list installed | grep docker\nyum list installed | grep docker\ndocker-engine.x86_64   1.7.1-1.el7 @/docker-engine-1.7.1-1.el7.x86_64.rpm\n</pre>\n</li> <li>\n<p>Remove the package.</p> <pre>$ sudo yum -y remove docker-engine.x86_64\n</pre> <p>This command does not remove images, containers, volumes, or user-created configuration files on your host.</p>\n</li> <li>\n<p>To delete all images, containers, and volumes, run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre>\n</li> <li><p>Locate and delete any user-created configuration files.</p></li> </ol><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/centos/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/centos/</a>\n  </p>\n</div>\n","engine/installation/mac/index":"<h1 id=\"mac-os-x\">Mac OS X</h1> <p>You install Docker using Docker Toolbox. Docker Toolbox includes the following Docker tools:</p> <ul> <li>Docker Machine for running the <code>docker-machine</code> binary</li> <li>Docker Engine for running the <code>docker</code> binary</li> <li>Docker Compose for running the <code>docker-compose</code> binary</li> <li>Kitematic, the Docker GUI</li> <li>a shell preconfigured for a Docker command-line environment</li> <li>Oracle VM VirtualBox</li> </ul> <p>Because the Docker daemon uses Linux-specific kernel features, you can’t run Docker natively in OS X. Instead, you must use <code>docker-machine</code> to create and attach to a virtual machine (VM). This machine is a Linux VM that hosts Docker for you on your Mac.</p> <p><strong>Requirements</strong></p> <p>Your Mac must be running OS X 10.8 “Mountain Lion” or newer to install the Docker Toolbox.</p> <h3 id=\"learn-the-key-concepts-before-installing\">Learn the key concepts before installing</h3> <p>In a Docker installation on Linux, your physical machine is both the localhost and the Docker host. In networking, localhost means your computer. The Docker host is the computer on which the containers run.</p> <p>On a typical Linux installation, the Docker client, the Docker daemon, and any containers run directly on your localhost. This means you can address ports on a Docker container using standard localhost addressing such as <code>localhost:8000</code> or <code>0.0.0.0:8376</code>.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/linux_docker_host.svg\" alt=\"Linux Architecture Diagram\"></p> <p>In an OS X installation, the <code>docker</code> daemon is running inside a Linux VM called <code>default</code>. The <code>default</code> is a lightweight Linux VM made specifically to run the Docker daemon on Mac OS X. The VM runs completely from RAM, is a small ~24MB download, and boots in approximately 5s.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/mac_docker_host.svg\" alt=\"OSX Architecture Diagram\"></p> <p>In OS X, the Docker host address is the address of the Linux VM. When you start the VM with <code>docker-machine</code> it is assigned an IP address. When you start a container, the ports on a container map to ports on the VM. To see this in practice, work through the exercises on this page.</p> <h3 id=\"installation\">Installation</h3> <p>If you have VirtualBox running, you must shut it down before running the installer.</p> <ol> <li><p>Go to the <a href=\"https://www.docker.com/toolbox\">Docker Toolbox</a> page.</p></li> <li><p>Click the Download link.</p></li> <li>\n<p>Install Docker Toolbox by double-clicking the package or by right-clicking and choosing “Open” from the pop-up menu.</p> <p>The installer launches the “Install Docker Toolbox” dialog.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/mac-welcome-page.png\" alt=\"Install Docker Toolbox\"></p>\n</li> <li>\n<p>Press “Continue” to install the toolbox.</p> <p>The installer presents you with options to customize the standard installation.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/mac-page-two.png\" alt=\"Standard install\"></p> <p>By default, the standard Docker Toolbox installation:</p> <ul> <li>installs binaries for the Docker tools in <code>/usr/local/bin</code>\n</li> <li>makes these binaries available to all users</li> <li>installs VirtualBox; or updates any existing installation</li> </ul> <p>To change these defaults, press “Customize” or “Change Install Location.”</p>\n</li> <li>\n<p>Press “Install” to perform the standard installation.</p> <p>The system prompts you for your password.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/mac-password-prompt.png\" alt=\"Password prompt\"></p>\n</li> <li>\n<p>Provide your password to continue with the installation.</p> <p>When it completes, the installer provides you with some information you can use to complete some common tasks.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/mac-page-finished.png\" alt=\"All finished\"></p>\n</li> <li><p>Press “Close” to exit.</p></li> </ol> <h2 id=\"running-a-docker-container\">Running a Docker Container</h2> <p>To run a Docker container, you:</p> <ul> <li>Create a new (or start an existing) virtual machine</li> <li>Switch your environment to your new VM</li> <li>Use the <code>docker</code> client to create, load, and manage containers</li> </ul> <p>You can reuse this virtual machine as often as you like. Like any VirtualBox VM, it maintains its configuration between uses.</p> <p>There are two ways to use the installed tools, from the Docker Quickstart Terminal or <a href=\"#from-your-shell\">from your shell</a>.</p> <h3 id=\"from-the-docker-quickstart-terminal\">From the Docker Quickstart Terminal</h3> <ol> <li><p>Open the “Applications” folder or the “Launchpad”.</p></li> <li>\n<p>Find the Docker Quickstart Terminal and double-click to launch it.</p> <p>The application:</p> <ul> <li>Opens a terminal window</li> <li>Creates a <code>default</code> VM if it doesn’t exists, and starts the VM after</li> <li>Points the terminal environment to this VM</li> </ul> <p>Once the launch completes, the Docker Quickstart Terminal reports:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/mac-success.png\" alt=\"All finished\"></p> <p>Now, you can run <code>docker</code> commands.</p>\n</li> <li>\n<p>Verify your setup succeeded by running the <code>hello-world</code> container.</p> <pre>$ docker run hello-world\nUnable to find image 'hello-world:latest' locally\n511136ea3c5a: Pull complete\n31cbccb51277: Pull complete\ne45a5af57b00: Pull complete\nhello-world:latest: The image you are pulling has been verified.\nImportant: image verification is a tech preview feature and should not be\nrelied on to provide security.\nStatus: Downloaded newer image for hello-world:latest\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n   (Assuming it was not already locally available.)\n3. The Docker daemon created a new container from that image which runs the\n   executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\n   to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nFor more examples and ideas, visit:\nhttp://docs.docker.com/userguide/\n</pre>\n</li> </ol> <p>A more typical way to interact with the Docker tools is from your regular shell command line.</p> <h3 id=\"from-your-shell\">From your shell</h3> <p>This section assumes you are running a Bash shell. You may be running a different shell such as C Shell but the commands are the same.</p> <ol> <li>\n<p>Create a new Docker VM.</p> <pre>$ docker-machine create --driver virtualbox default\nCreating VirtualBox VM...\nCreating SSH key...\nStarting VirtualBox VM...\nStarting VM...\nTo see how to connect Docker to this machine, run: docker-machine env default\n</pre> <p>This creates a new <code>default</code> VM in VirtualBox.</p> <p>The command also creates a machine configuration in the <code>~/.docker/machine/machines/default</code> directory. You only need to run the <code>create</code> command once. Then, you can use <code>docker-machine</code> to start, stop, query, and otherwise manage the VM from the command line.</p>\n</li> <li>\n<p>List your available machines.</p> <pre>$ docker-machine ls\nNAME                ACTIVE   DRIVER       STATE     URL                         SWARM\ndefault             *        virtualbox   Running   tcp://192.168.99.101:2376\n</pre> <p>If you have previously installed the deprecated Boot2Docker application or run the Docker Quickstart Terminal, you may have a <code>dev</code> VM as well. When you created <code>default</code> VM, the <code>docker-machine</code> command provided instructions for learning how to connect the VM.</p>\n</li> <li>\n<p>Get the environment commands for your new VM.</p> <pre>$ docker-machine env default\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://192.168.99.101:2376\"\nexport DOCKER_CERT_PATH=\"/Users/mary/.docker/machine/machines/default\"\nexport DOCKER_MACHINE_NAME=\"default\"\n# Run this command to configure your shell:\n# eval \"$(docker-machine env default)\"\n</pre>\n</li> <li>\n<p>Connect your shell to the <code>default</code> machine.</p> <pre>$ eval \"$(docker-machine env default)\"\n</pre>\n</li> <li>\n<p>Run the <code>hello-world</code> container to verify your setup.</p> <pre>$ docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"learn-about-your-toolbox-installation\">Learn about your Toolbox installation</h2> <p>Toolbox installs the Docker Engine binary, the Docker binary on your system. When you use the Docker Quickstart Terminal or create a <code>default</code> VM manually, Docker Machine updates the <code>~/.docker/machine/machines/default</code> folder to your system. This folder contains the configuration for the VM.</p> <p>You can create multiple VMs on your system with Docker Machine. Therefore, you may end up with multiple VM folders if you have more than one VM. To remove a VM, use the <code>docker-machine rm &lt;machine-name&gt;</code> command.</p> <h2 id=\"migrate-from-boot2docker\">Migrate from Boot2Docker</h2> <p>If you were using Boot2Docker previously, you have a pre-existing Docker <code>boot2docker-vm</code> VM on your local system. To allow Docker Machine to manage this older VM, you can migrate it.</p> <ol> <li><p>Open a terminal or the Docker CLI on your system.</p></li> <li>\n<p>Type the following command.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-import-boot2docker-vm boot2docker-vm docker-vm\n</pre>\n</li> <li><p>Use the <code>docker-machine</code> command to interact with the migrated VM.</p></li> </ol> <p>The <code>docker-machine</code> subcommands are slightly different than the <code>boot2docker</code> subcommands. The table below lists the equivalent <code>docker-machine</code> subcommand and what it does:</p> <table> <thead> <tr> <th><code>boot2docker</code></th> <th><code>docker-machine</code></th> <th>\n<code>docker-machine</code> description</th> </tr> </thead> <tbody> <tr> <td>init</td> <td>create</td> <td>Creates a new docker host.</td> </tr> <tr> <td>up</td> <td>start</td> <td>Starts a stopped machine.</td> </tr> <tr> <td>ssh</td> <td>ssh</td> <td>Runs a command or interactive ssh session on the machine.</td> </tr> <tr> <td>save</td> <td>-</td> <td>Not applicable.</td> </tr> <tr> <td>down</td> <td>stop</td> <td>Stops a running machine.</td> </tr> <tr> <td>poweroff</td> <td>stop</td> <td>Stops a running machine.</td> </tr> <tr> <td>reset</td> <td>restart</td> <td>Restarts a running machine.</td> </tr> <tr> <td>config</td> <td>inspect</td> <td>Prints machine configuration details.</td> </tr> <tr> <td>status</td> <td>ls</td> <td>Lists all machines and their status.</td> </tr> <tr> <td>info</td> <td>inspect</td> <td>Displays a machine’s details.</td> </tr> <tr> <td>ip</td> <td>ip</td> <td>Displays the machine’s ip address.</td> </tr> <tr> <td>shellinit</td> <td>env</td> <td>Displays shell commands needed to configure your shell to interact with a machine</td> </tr> <tr> <td>delete</td> <td>rm</td> <td>Removes a machine.</td> </tr> <tr> <td>download</td> <td>-</td> <td>Not applicable.</td> </tr> <tr> <td>upgrade</td> <td>upgrade</td> <td>Upgrades a machine’s Docker client to the latest stable release.</td> </tr> </tbody> </table> <h2 id=\"examples-on-mac-os-x\">Examples on Mac OS X</h2> <p>Work through this section to try some practical container tasks on a VM. At this point, you should have a VM running and be connected to it through your shell. To verify this, run the following commands:</p> <pre>$ docker-machine ls\nNAME                ACTIVE   DRIVER       STATE     URL                         SWARM\ndefault             *        virtualbox   Running   tcp://192.168.99.100:2376\n</pre> <p>The <code>ACTIVE</code> machine, in this case <code>default</code>, is the one your environment is pointing to.</p> <h3 id=\"access-container-ports\">Access container ports</h3> <ol> <li>\n<p>Start an NGINX container on the DOCKER_HOST.</p> <pre>$ docker run -d -P --name web nginx\n</pre> <p>Normally, the <code>docker run</code> commands starts a container, runs it, and then exits. The <code>-d</code> flag keeps the container running in the background after the <code>docker run</code> command completes. The <code>-P</code> flag publishes exposed ports from the container to your local host; this lets you access them from your Mac.</p>\n</li> <li>\n<p>Display your running container with <code>docker ps</code> command</p> <pre>CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                                           NAMES\n5fb65ff765e9        nginx:latest        \"nginx -g 'daemon of   3 minutes ago       Up 3 minutes        0.0.0.0:49156-&gt;443/tcp, 0.0.0.0:49157-&gt;80/tcp   web\n</pre> <p>At this point, you can see <code>nginx</code> is running as a daemon.</p>\n</li> <li>\n<p>View just the container’s ports.</p> <pre>$ docker port web\n443/tcp -&gt; 0.0.0.0:49156\n80/tcp -&gt; 0.0.0.0:49157\n</pre> <p>This tells you that the <code>web</code> container’s port <code>80</code> is mapped to port <code>49157</code> on your Docker host.</p>\n</li> <li>\n<p>Enter the <code>http://localhost:49157</code> address (<code>localhost</code> is <code>0.0.0.0</code>) in your browser:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/bad_host.png\" alt=\"Bad Address\"></p> <p>This didn’t work. The reason it doesn’t work is your <code>DOCKER_HOST</code> address is not the localhost address (0.0.0.0) but is instead the address of your Docker VM.</p>\n</li> <li>\n<p>Get the address of the <code>default</code> VM.</p> <pre>$ docker-machine ip default\n192.168.59.103\n</pre>\n</li> <li>\n<p>Enter the <code>http://192.168.59.103:49157</code> address in your browser:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/good_host.png\" alt=\"Correct Addressing\"></p> <p>Success!</p>\n</li> <li>\n<p>To stop and then remove your running <code>nginx</code> container, do the following:</p> <pre>$ docker stop web\n$ docker rm web\n</pre>\n</li> </ol> <h3 id=\"mount-a-volume-on-the-container\">Mount a volume on the container</h3> <p>When you start a container it automatically shares your <code>/Users/username</code> directory with the VM. You can use this share point to mount directories onto your container. The next exercise demonstrates how to do this.</p> <ol> <li>\n<p>Change to your user <code>$HOME</code> directory.</p> <pre>$ cd $HOME\n</pre>\n</li> <li>\n<p>Make a new <code>site</code> directory.</p> <pre>$ mkdir site\n</pre>\n</li> <li>\n<p>Change into the <code>site</code> directory.</p> <pre>$ cd site\n</pre>\n</li> <li>\n<p>Create a new <code>index.html</code> file.</p> <pre>$ echo \"my new site\" &gt; index.html\n</pre>\n</li> <li>\n<p>Start a new <code>nginx</code> container and replace the <code>html</code> folder with your <code>site</code> directory.</p> <pre>$ docker run -d -P -v $HOME/site:/usr/share/nginx/html \\\n  --name mysite nginx\n</pre>\n</li> <li>\n<p>View the <code>mysite</code> container’s port.</p> <pre>$ docker port mysite\n80/tcp -&gt; 0.0.0.0:49166\n443/tcp -&gt; 0.0.0.0:49165\n</pre>\n</li> <li>\n<p>Open the site in a browser:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/newsite_view.png\" alt=\"My site page\"></p>\n</li> <li>\n<p>Add a page to your <code>$HOME/site</code> in real time.</p> <pre>$ echo \"This is cool\" &gt; cool.html\n</pre>\n</li> <li>\n<p>Open the new page in the browser.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/cool_view.png\" alt=\"Cool page\"></p>\n</li> <li>\n<p>Stop and then remove your running <code>mysite</code> container.</p> <pre>$ docker stop mysite\n$ docker rm mysite\n</pre>\n</li> </ol> <blockquote> <p><strong>Note</strong>: There is a <a href=\"https://docs.docker.com/machine/drivers/virtualbox/#known-issues\">known issue</a> that may cause files shared with your nginx container to not update correctly as you modify them on your host.</p> </blockquote> <h2 id=\"upgrade-docker-toolbox\">Upgrade Docker Toolbox</h2> <p>To upgrade Docker Toolbox, download and re-run the <a href=\"https://docker.com/toolbox/\">Docker Toolbox installer</a>.</p> <h2 id=\"uninstall-docker-toolbox\">Uninstall Docker Toolbox</h2> <p>To uninstall, do the following:</p> <ol> <li>\n<p>List your machines.</p> <pre>$ docker-machine ls\nNAME                ACTIVE   DRIVER       STATE     URL                         SWARM\ndev                 *        virtualbox   Running   tcp://192.168.99.100:2376\nmy-docker-machine            virtualbox   Stopped\ndefault                      virtualbox   Stopped\n</pre>\n</li> <li>\n<p>Remove each machine.</p> <pre>$ docker-machine rm dev\nSuccessfully removed dev\n</pre> <p>Removing a machine deletes its VM from VirtualBox and from the <code>~/.docker/machine/machines</code> directory.</p>\n</li> <li><p>Remove the Docker Quickstart Terminal and Kitematic from your “Applications” folder.</p></li> <li>\n<p>Remove the <code>docker</code>, <code>docker-compose</code>, and <code>docker-machine</code> commands from the <code>/usr/local/bin</code> folder.</p> <pre>$ rm /usr/local/bin/docker\n</pre>\n</li> <li><p>Delete the <code>~/.docker</code> folder from your system.</p></li> </ol> <h2 id=\"learning-more\">Learning more</h2> <p>Use <code>docker-machine help</code> to list the full command line reference for Docker Machine. For more information about using SSH or SCP to access a VM, see the <a href=\"https://docs.docker.com/machine/\">Docker Machine documentation</a>.</p> <p>You can continue with the <a href=\"../../userguide/index\">Docker Engine User Guide</a>. If you are interested in using the Kitematic GUI, see the <a href=\"https://docs.docker.com/kitematic/userguide/\">Kitematic user guide</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/mac/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/mac/</a>\n  </p>\n</div>\n","engine/installation/linux/suse/index":"<h1 id=\"opensuse-and-suse-linux-enterprise\">openSUSE and SUSE Linux Enterprise</h1> <p>This page provides instructions for installing and configuring the latest Docker Engine software on openSUSE and SUSE systems.</p> <blockquote> <p><strong>Note:</strong> You can also find bleeding edge Docker versions inside of the repositories maintained by the <a href=\"https://build.opensuse.org/project/show/Virtualization:containers\">Virtualization:containers project</a> on the <a href=\"https://build.opensuse.org/\">Open Build Service</a>. This project delivers also other packages that are related with the Docker ecosystem (for example, Docker Compose).</p> </blockquote> <h2 id=\"prerequisites\">Prerequisites</h2> <p>You must be running a 64 bit architecture.</p> <h2 id=\"opensuse\">openSUSE</h2> <p>Docker is part of the official openSUSE repositories starting from 13.2. No additional repository is required on your system.</p> <h2 id=\"suse-linux-enterprise\">SUSE Linux Enterprise</h2> <p>Docker is officially supported on SUSE Linux Enterprise 12 and later. You can find the latest supported Docker packages inside the <code>Container</code> module. To enable this module, do the following:</p> <ol> <li>Start YaST, and select <em>Software &gt; Software Repositories</em>.</li> <li>Click <em>Add</em> to open the add-on dialog.</li> <li>Select <em>Extensions and Module from Registration Server</em> and click <em>Next</em>.</li> <li>From the list of available extensions and modules, select <em>Container Module</em> and click <em>Next</em>. The containers module and its repositories are added to your system.</li> <li>If you use Subscription Management Tool, update the list of repositories at the SMT server.</li> </ol> <p>Otherwise execute the following command:</p> <pre>$ sudo SUSEConnect -p sle-module-containers/12/x86_64 -r ''\n\n&gt;**Note:** currently the `-r ''` flag is required to avoid a known limitation of `SUSEConnect`.\n</pre> <p>The <a href=\"https://build.opensuse.org/project/show/Virtualization:containers\">Virtualization:containers project</a> on the <a href=\"https://build.opensuse.org/\">Open Build Service</a> contains also bleeding edge Docker packages for SUSE Linux Enterprise. However these packages are <strong>not supported</strong> by SUSE.</p> <h3 id=\"install-docker\">Install Docker</h3> <ol> <li>\n<p>Install the Docker package:</p> <pre>$ sudo zypper in docker\n</pre>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo systemctl start docker\n</pre>\n</li> <li>\n<p>Test the Docker installation.</p> <pre>$ sudo docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"configure-docker-boot-options\">Configure Docker boot options</h2> <p>You can use these steps on openSUSE or SUSE Linux Enterprise. To start the <code>docker daemon</code> at boot, set the following:</p> <pre>$ sudo systemctl enable docker\n</pre> <p>The <code>docker</code> package creates a new group named <code>docker</code>. Users, other than <code>root</code> user, must be part of this group to interact with the Docker daemon. You can add users with this command syntax:</p> <pre>sudo /usr/sbin/usermod -a -G docker &lt;username&gt;\n</pre> <p>Once you add a user, make sure they relog to pick up these new permissions.</p> <h2 id=\"enable-external-network-access\">Enable external network access</h2> <p>If you want your containers to be able to access the external network, you must enable the <code>net.ipv4.ip_forward</code> rule. To do this, use YaST.</p> <p>For openSUSE Tumbleweed and later, browse to the <strong>System -&gt; Network Settings -&gt; Routing</strong> menu. For SUSE Linux Enterprise 12 and previous openSUSE versions, browse to <strong>Network Devices -&gt; Network Settings -&gt; Routing</strong> menu (f) and check the <em>Enable IPv4 Forwarding</em> box.</p> <p>When networking is handled by the Network Manager, instead of YaST you must edit the <code>/etc/sysconfig/SuSEfirewall2</code> file needs by hand to ensure the <code>FW_ROUTE</code> flag is set to <code>yes</code> like so:</p> <pre>FW_ROUTE=\"yes\"\n</pre> <h2 id=\"custom-daemon-options\">Custom daemon options</h2> <p>If you need to add an HTTP Proxy, set a different directory or partition for the Docker runtime files, or make other customizations, read the systemd article to learn how to <a href=\"../../../admin/systemd/index\">customize your systemd Docker daemon options</a>.</p> <h2 id=\"uninstallation\">Uninstallation</h2> <p>To uninstall the Docker package:</p> <pre>$ sudo zypper rm docker\n</pre> <p>The above command does not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre> <p>You must delete the user created configuration files manually.</p> <h2 id=\"where-to-go-from-here\">Where to go from here</h2> <p>You can find more details about Docker on openSUSE or SUSE Linux Enterprise in the <a href=\"https://www.suse.com/documentation/sles-12/dockerquick/data/dockerquick.html\">Docker quick start guide</a> on the SUSE website. The document targets SUSE Linux Enterprise, but its contents apply also to openSUSE.</p> <p>Continue to the <a href=\"../../../userguide/index\">User Guide</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/SUSE/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/SUSE/</a>\n  </p>\n</div>\n","engine/installation/cloud/cloud-ex-aws/index":"<h1 id=\"example-manual-install-on-cloud-provider\">Example: Manual install on cloud provider</h1> <p>You can install Docker Engine directly to servers you have on cloud providers. This example shows how to create an <a href=\"https://aws.amazon.com/\" target=\"_blank\"> Amazon Web Services (AWS)</a> EC2 instance, and install Docker Engine on it.</p> <p>You can use this same general approach to create Dockerized hosts on other cloud providers.</p> <h3 id=\"step-1-sign-up-for-aws\">Step 1. Sign up for AWS</h3> <ol> <li><p>If you are not already an AWS user, sign up for <a href=\"https://aws.amazon.com/\" target=\"_blank\"> AWS</a> to create an account and get root access to EC2 cloud computers. If you have an Amazon account, you can use it as your root user account.</p></li> <li>\n<p>Create an IAM (Identity and Access Management) administrator user, an admin group, and a key pair associated with a region.</p> <p>From the AWS menus, select <strong>Services</strong> &gt; <strong>IAM</strong> to get started.</p> <p>See the AWS documentation on <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html\" target=\"_blank\">Setting Up with Amazon EC2</a>. Follow the steps for “Create an IAM User” and “Create a Key Pair”.</p> <p>If you are just getting started with AWS and EC2, you do not need to create a virtual private cloud (VPC) or specify a subnet. The newer EC2-VPC platform (accounts created after 2013-12-04) comes with a default VPC and subnet in each availability zone. When you launch an instance, it automatically uses the default VPC.</p>\n</li> </ol> <h3 id=\"step-2-configure-and-start-an-ec2-instance\">Step 2. Configure and start an EC2 instance</h3> <p>Launch an instance to create a virtual machine (VM) with a specified operating system (OS) as follows.</p> <ol> <li>\n<p>Log into AWS with your IAM credentials.</p> <p>On the AWS home page, click <strong>EC2</strong> to go to the dashboard, then click <strong>Launch Instance</strong>.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ec2_launch_instance.png\" alt=\"EC2 dashboard\"></p> <p>AWS EC2 virtual servers are called <em>instances</em> in Amazon parlance. Once you set up an account, IAM user and key pair, you are ready to launch an instance. It is at this point that you select the OS for the VM.</p>\n</li> <li>\n<p>Choose an Amazon Machine Image (AMI) with the OS and applications you want. For this example, we select an Ubuntu server.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ec2-ubuntu.png\" alt=\"Launch Ubuntu\"></p>\n</li> <li>\n<p>Choose an instance type.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ec2_instance_type.png\" alt=\"Choose a general purpose instance type\"></p>\n</li> <li>\n<p>Configure the instance.</p> <p>You can select the default network and subnet, which are inherently linked to a region and availability zone.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ec2_instance_details.png\" alt=\"Configure the instance\"></p>\n</li> <li><p>Click <strong>Review and Launch</strong>.</p></li> <li>\n<p>Select a key pair to use for this instance.</p> <p>When you choose to launch, you need to select a key pair to use. Save the <code>.pem</code> file to use in the next steps.</p>\n</li> </ol> <p>The instance is now up-and-running. The menu path to get back to your EC2 instance on AWS is: <strong>EC2 (Virtual Servers in Cloud)</strong> &gt; <strong>EC2 Dashboard</strong> &gt; <strong>Resources</strong> &gt; <strong>Running instances</strong>.</p> <p>To get help with your private key file, instance IP address, and how to log into the instance via SSH, click the <strong>Connect</strong> button at the top of the AWS instance dashboard.</p> <h3 id=\"step-3-log-in-from-a-terminal-configure-apt-and-get-packages\">Step 3. Log in from a terminal, configure apt, and get packages</h3> <ol> <li>\n<p>Log in to the EC2 instance from a command line terminal.</p> <p>Change directories into the directory containing the SSH key and run this command (or give the path to it as part of the command):</p> <pre>$ ssh -i \"YourKey\" ubuntu@xx.xxx.xxx.xxx\n</pre> <p>For our example:</p> <pre>$ cd ~/Desktop/keys/amazon_ec2\n$ ssh -i \"my-key-pair.pem\" ubuntu@xx.xxx.xxx.xxx\n</pre> <p>We’ll follow the instructions for installing Docker on Ubuntu at <a href=\"https://docs.docker.com/engine/installation/ubuntulinux/\">https://docs.docker.com/engine/installation/ubuntulinux/</a>. The next few steps reflect those instructions.</p>\n</li> <li>\n<p>Check the kernel version to make sure it’s 3.10 or higher.</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ uname -r\n3.13.0-48-generic\n</pre>\n</li> <li>\n<p>Add the new <code>gpg</code> key.</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\nExecuting: gpg --ignore-time-conflict --no-options --no-default-keyring --homedir /tmp/tmp.jNZLKNnKte --no-auto-check-trustdb --trust-model always --keyring /etc/apt/trusted.gpg --primary-keyring /etc/apt/trusted.gpg --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\ngpg: requesting key 2C52609D from hkp server p80.pool.sks-keyservers.net\ngpg: key 2C52609D: public key \"Docker Release Tool (releasedocker) &lt;docker@docker.com&gt;\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1  (RSA: 1)\n</pre>\n</li> <li>\n<p>Create a <code>docker.list</code> file, and add an entry for our OS, Ubuntu Trusty 14.04 (LTS).</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ sudo vi /etc/apt/sources.list.d/docker.list\n</pre> <p>If we were updating an existing file, we’d delete any existing entries.</p>\n</li> <li>\n<p>Update the <code>apt</code> package index.</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ sudo apt-get update\n</pre>\n</li> <li>\n<p>Purge the old repo if it exists.</p> <p>In our case the repo doesn’t because this is a new VM, but let’s run it anyway just to be sure.</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ sudo apt-get purge lxc-docker\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nPackage 'lxc-docker' is not installed, so not removed\n0 upgraded, 0 newly installed, 0 to remove and 139 not upgraded.\n</pre>\n</li> <li>\n<p>Verify that <code>apt</code> is pulling from the correct repository.</p> <pre>ubuntu@ip-172-31-0-151:~$ sudo apt-cache policy docker-engine\ndocker-engine:\nInstalled: (none)\nCandidate: 1.9.1-0~trusty\nVersion table:\n1.9.1-0~trusty 0\n500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 Packages\n1.9.0-0~trusty 0\n500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 Packages\n    . . .\n</pre> <p>From now on when you run <code>apt-get upgrade</code>, <code>apt</code> pulls from the new repository.</p>\n</li> </ol> <h3 id=\"step-4-install-recommended-prerequisites-for-the-os\">Step 4. Install recommended prerequisites for the OS</h3> <p>For Ubuntu Trusty (and some other versions), it’s recommended to install the <code>linux-image-extra</code> kernel package, which allows you use the <code>aufs</code> storage driver, so we’ll do that now.</p> <pre>    ubuntu@ip-xxx-xx-x-xxx:~$ sudo apt-get update\n    ubuntu@ip-172-31-0-151:~$ sudo apt-get install linux-image-extra-$(uname -r)\n</pre> <h3 id=\"step-5-install-docker-engine-on-the-remote-instance\">Step 5. Install Docker Engine on the remote instance</h3> <ol> <li>\n<p>Update the apt package index.</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ sudo apt-get update\n</pre>\n</li> <li>\n<p>Install Docker Engine.</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ sudo apt-get install docker-engine\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following extra packages will be installed:\naufs-tools cgroup-lite git git-man liberror-perl\nSuggested packages:\ngit-daemon-run git-daemon-sysvinit git-doc git-el git-email git-gui gitk\ngitweb git-arch git-bzr git-cvs git-mediawiki git-svn\nThe following NEW packages will be installed:\naufs-tools cgroup-lite docker-engine git git-man liberror-perl\n0 upgraded, 6 newly installed, 0 to remove and 139 not upgraded.\nNeed to get 11.0 MB of archives.\nAfter this operation, 60.3 MB of additional disk space will be used.\nDo you want to continue? [Y/n] y\nGet:1 http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ trusty/universe aufs-tools amd64 1:3.2+20130722-1.1 [92.3 kB]\nGet:2 http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ trusty/main liberror-perl all 0.17-1.1 [21.1 kB]\n. . .\n</pre>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ sudo service docker start\n</pre>\n</li> <li>\n<p>Verify Docker Engine is installed correctly by running <code>docker run hello-world</code>.</p> <pre>ubuntu@ip-xxx-xx-x-xxx:~$ sudo docker run hello-world\nubuntu@ip-172-31-0-151:~$ sudo docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nb901d36b6f2f: Pull complete\n0a6ba66e537a: Pull complete\nDigest: sha256:8be990ef2aeb16dbcb9271ddfe2610fa6658d13f6dfb8bc72074cc1ca36966a7\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker Hub account:\nhttps://hub.docker.com\n\nFor more examples and ideas, visit:\nhttps://docs.docker.com/userguide/\n</pre>\n</li> </ol> <h2 id=\"where-to-go-next\">Where to go next</h2> <p><em>Looking for a quicker way to do Docker cloud installs and provision multiple hosts?</em> You can use <a href=\"https://docs.docker.com/machine/overview/\">Docker Machine</a> to provision hosts.</p> <ul> <li><p><a href=\"https://docs.docker.com/machine/get-started-cloud/\">Use Docker Machine to provision hosts on cloud providers</a></p></li> <li><p><a href=\"https://docs.docker.com/machine/drivers/\">Docker Machine driver reference</a></p></li> <li><p><a href=\"../../index\">Install Docker Engine</a></p></li> <li><p><a href=\"../../../userguide/intro/index\">Docker User Guide</a></p></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/cloud/cloud-ex-aws/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/cloud/cloud-ex-aws/</a>\n  </p>\n</div>\n","engine/installation/binaries/index":"<h1 id=\"installation-from-binaries\">Installation from binaries</h1> <p><strong>This instruction set is meant for hackers who want to try out Docker on a variety of environments.</strong></p> <p>Before following these directions, you should really check if a packaged version of Docker is already available for your distribution. We have packages for many distributions, and more keep showing up all the time!</p> <h2 id=\"check-runtime-dependencies\">Check runtime dependencies</h2> <p>To run properly, docker needs the following software to be installed at runtime:</p> <ul> <li>iptables version 1.4 or later</li> <li>Git version 1.7 or later</li> <li>procps (or similar provider of a “ps” executable)</li> <li>XZ Utils 4.9 or later</li> <li>a <a href=\"https://github.com/tianon/cgroupfs-mount/blob/master/cgroupfs-mount\">properly mounted</a> cgroupfs hierarchy (having a single, all-encompassing “cgroup” mount point <a href=\"https://github.com/docker/docker/issues/2683\">is</a> <a href=\"https://github.com/docker/docker/issues/3485\">not</a> <a href=\"https://github.com/docker/docker/issues/4568\">sufficient</a>)</li> </ul> <h2 id=\"check-kernel-dependencies\">Check kernel dependencies</h2> <p>Docker in daemon mode has specific kernel requirements. For details, check your distribution in <a href=\"../index#on-linux\"><em>Installation</em></a>.</p> <p>A 3.10 Linux kernel is the minimum requirement for Docker. Kernels older than 3.10 lack some of the features required to run Docker containers. These older versions are known to have bugs which cause data loss and frequently panic under certain conditions.</p> <p>The latest minor version (3.x.y) of the 3.10 (or a newer maintained version) Linux kernel is recommended. Keeping the kernel up to date with the latest minor version will ensure critical kernel bugs get fixed.</p> <blockquote> <p><strong>Warning</strong>: Installing custom kernels and kernel packages is probably not supported by your Linux distribution’s vendor. Please make sure to ask your vendor about Docker support first before attempting to install custom kernels on your distribution.</p> <p><strong>Warning</strong>: Installing a newer kernel might not be enough for some distributions which provide packages which are too old or incompatible with newer kernels.</p> </blockquote> <p>Note that Docker also has a client mode, which can run on virtually any Linux kernel (it even builds on OS X!).</p> <h2 id=\"enable-apparmor-and-selinux-when-possible\">Enable AppArmor and SELinux when possible</h2> <p>Please use AppArmor or SELinux if your Linux distribution supports either of the two. This helps improve security and blocks certain types of exploits. Your distribution’s documentation should provide detailed steps on how to enable the recommended security mechanism.</p> <p>Some Linux distributions enable AppArmor or SELinux by default and they run a kernel which doesn’t meet the minimum requirements (3.10 or newer). Updating the kernel to 3.10 or newer on such a system might not be enough to start Docker and run containers. Incompatibilities between the version of AppArmor/SELinux user space utilities provided by the system and the kernel could prevent Docker from running, from starting containers or, cause containers to exhibit unexpected behaviour.</p> <blockquote> <p><strong>Warning</strong>: If either of the security mechanisms is enabled, it should not be disabled to make Docker or its containers run. This will reduce security in that environment, lose support from the distribution’s vendor for the system, and might break regulations and security policies in heavily regulated environments.</p> </blockquote> <h2 id=\"get-the-docker-engine-binaries\">Get the Docker Engine binaries</h2> <p>You can download either the latest release binaries or a specific version. To get the list of stable release version numbers from GitHub, view the <code>docker/docker</code> <a href=\"https://github.com/docker/docker/releases\">releases page</a>. You can get the MD5 and SHA256 hashes by appending .md5 and .sha256 to the URLs respectively</p> <h3 id=\"get-the-linux-binaries\">Get the Linux binaries</h3> <p>To download the latest version for Linux, use the following URLs:</p> <pre>https://get.docker.com/builds/Linux/i386/docker-latest.tgz\n\nhttps://get.docker.com/builds/Linux/x86_64/docker-latest.tgz\n</pre> <p>To download a specific version for Linux, use the following URL patterns:</p> <pre>https://get.docker.com/builds/Linux/i386/docker-&lt;version&gt;.tgz\n\nhttps://get.docker.com/builds/Linux/x86_64/docker-&lt;version&gt;.tgz\n</pre> <p>For example:</p> <pre>https://get.docker.com/builds/Linux/i386/docker-1.11.0.tgz\n\nhttps://get.docker.com/builds/Linux/x86_64/docker-1.11.0.tgz\n</pre> <blockquote> <p><strong>Note</strong> These instructions are for Docker Engine 1.11 and up. Engine 1.10 and under consists of a single binary, and instructions for those versions are different. To install version 1.10 or below, follow the instructions in the <a href=\"https://docs.docker.com/v1.11/v1.10/engine/installation/binaries/\" target=\"_blank\">1.10 documentation</a>.</p> </blockquote> <h4 id=\"install-the-linux-binaries\">Install the Linux binaries</h4> <p>After downloading, you extract the archive, which puts the binaries in a directory named <code>docker</code> in your current location.</p> <pre>$ tar -xvzf docker-latest.tgz\n\ndocker/\ndocker/docker-containerd-ctr\ndocker/docker\ndocker/docker-containerd\ndocker/docker-runc\ndocker/docker-containerd-shim\n</pre> <p>Engine requires these binaries to be installed in your host’s <code>$PATH</code>. For example, to install the binaries in <code>/usr/bin</code>:</p> <pre>$ mv docker/* /usr/bin/\n</pre> <blockquote> <p><strong>Note</strong>: If you already have Engine installed on your host, make sure you stop Engine before installing (<code>killall docker</code>), and install the binaries in the same location. You can find the location of the current installation with <code>dirname $(which docker)</code>.</p> </blockquote> <h4 id=\"run-the-engine-daemon-on-linux\">Run the Engine daemon on Linux</h4> <p>You can manually start the Engine in daemon mode using:</p> <pre>$ sudo docker daemon &amp;\n</pre> <p>The GitHub repository provides samples of init-scripts you can use to control the daemon through a process manager, such as upstart or systemd. You can find these scripts in the <a href=\"https://github.com/docker/docker/tree/master/contrib/init\"> contrib directory</a>.</p> <p>For additional information about running the Engine in daemon mode, refer to the <a href=\"../../reference/commandline/daemon/index\">daemon command</a> in the Engine command line reference.</p> <h3 id=\"get-the-mac-os-x-binary\">Get the Mac OS X binary</h3> <p>The Mac OS X binary is only a client. You cannot use it to run the <code>docker</code> daemon. To download the latest version for Mac OS X, use the following URLs:</p> <pre>https://get.docker.com/builds/Darwin/x86_64/docker-latest.tgz\n</pre> <p>To download a specific version for Mac OS X, use the following URL pattern:</p> <pre>https://get.docker.com/builds/Darwin/x86_64/docker-&lt;version&gt;.tgz\n</pre> <p>For example:</p> <pre>https://get.docker.com/builds/Darwin/x86_64/docker-1.11.0.tgz\n</pre> <p>You can extract the downloaded archive either by double-clicking the downloaded <code>.tgz</code> or on the command line, using <code>tar -xvzf docker-1.11.0.tgz</code>. The client binary can be executed from any location on your filesystem.</p> <h3 id=\"get-the-windows-binary\">Get the Windows binary</h3> <p>You can only download the Windows binary for version <code>1.9.1</code> onwards. Moreover, the 32-bit (<code>i386</code>) binary is only a client, you cannot use it to run the <code>docker</code> daemon. The 64-bit binary (<code>x86_64</code>) is both a client and daemon.</p> <p>To download the latest version for Windows, use the following URLs:</p> <pre>https://get.docker.com/builds/Windows/i386/docker-latest.zip\n\nhttps://get.docker.com/builds/Windows/x86_64/docker-latest.zip\n</pre> <p>To download a specific version for Windows, use the following URL pattern:</p> <pre>https://get.docker.com/builds/Windows/i386/docker-&lt;version&gt;.zip\n\nhttps://get.docker.com/builds/Windows/x86_64/docker-&lt;version&gt;.zip\n</pre> <p>For example:</p> <pre>https://get.docker.com/builds/Windows/i386/docker-1.11.0.zip\n\nhttps://get.docker.com/builds/Windows/x86_64/docker-1.11.0.zip\n</pre> <blockquote> <p><strong>Note</strong> These instructions are for Engine 1.11 and up. Instructions for older versions are slightly different. To install version 1.10 or below, follow the instructions in the <a href=\"https://docs.docker.com/v1.11/v1.10/engine/installation/binaries/\" target=\"_blank\">1.10 documentation</a>.</p> </blockquote> <h2 id=\"giving-non-root-access\">Giving non-root access</h2> <p>The <code>docker</code> daemon always runs as the root user, and the <code>docker</code> daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user <em>root</em>, and so, by default, you can access it with <code>sudo</code>.</p> <p>If you (or your Docker installer) create a Unix group called <em>docker</em> and add users to it, then the <code>docker</code> daemon will make the ownership of the Unix socket read/writable by the <em>docker</em> group when the daemon starts. The <code>docker</code> daemon must always run as the root user, but if you run the <code>docker</code> client as a user in the <em>docker</em> group then you don’t need to add <code>sudo</code> to all the client commands.</p> <blockquote> <p><strong>Warning</strong>: The <em>docker</em> group (or the group specified with <code>-G</code>) is root-equivalent; see <a href=\"../../security/security/index#docker-daemon-attack-surface\"><em>Docker Daemon Attack Surface</em></a> details.</p> </blockquote> <h2 id=\"upgrade-docker-engine\">Upgrade Docker Engine</h2> <p>To upgrade your manual installation of Docker Engine on Linux, first kill the docker daemon:</p> <pre>$ killall docker\n</pre> <p>Then follow the <a href=\"#get-the-linux-binaries\">regular installation steps</a>.</p> <h2 id=\"next-steps\">Next steps</h2> <p>Continue with the <a href=\"../../userguide/index\">User Guide</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/binaries/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/binaries/</a>\n  </p>\n</div>\n","engine/userguide/containers/dockerizing/index":"<h1 id=\"hello-world-in-a-container\">Hello world in a container</h1> <p><em>So what’s this Docker thing all about?</em></p> <p>Docker allows you to run applications, worlds you create, inside containers. Running an application inside a container takes a single command: <code>docker run</code>.</p> <blockquote> <p><strong>Note</strong>: Depending on your Docker system configuration, you may be required to preface each <code>docker</code> command on this page with <code>sudo</code>. To avoid this behavior, your system administrator can create a Unix group called <code>docker</code> and add users to it.</p> </blockquote> <h2 id=\"run-a-hello-world\">Run a Hello world</h2> <p>Let’s run a hello world container.</p> <pre>$ docker run ubuntu /bin/echo 'Hello world'\nHello world\n</pre> <p>You just launched your first container!</p> <p>In this example:</p> <ul> <li><p><code>docker run</code> runs a container.</p></li> <li><p><code>ubuntu</code> is the image you run, for example the Ubuntu operating system image. When you specify an image, Docker looks first for the image on your Docker host. If the image does not exist locally, then the image is pulled from the public image registry <a href=\"https://hub.docker.com\">Docker Hub</a>.</p></li> <li><p><code>/bin/echo</code> is the command to run inside the new container.</p></li> </ul> <p>The container launches. Docker creates a new Ubuntu environment and executes the <code>/bin/echo</code> command inside it and then prints out:</p> <pre>Hello world\n</pre> <p>So what happened to the container after that? Well, Docker containers only run as long as the command you specify is active. Therefore, in the above example, the container stops once the command is executed.</p> <h2 id=\"run-an-interactive-container\">Run an interactive container</h2> <p>Let’s specify a new command to run in the container.</p> <pre>$ docker run -t -i ubuntu /bin/bash\nroot@af8bae53bdd3:/#\n</pre> <p>In this example:</p> <ul> <li>\n<code>docker run</code> runs a container.</li> <li>\n<code>ubuntu</code> is the image you would like to run.</li> <li>\n<code>-t</code> flag assigns a pseudo-tty or terminal inside the new container.</li> <li>\n<code>-i</code> flag allows you to make an interactive connection by grabbing the standard in (<code>STDIN</code>) of the container.</li> <li>\n<code>/bin/bash</code> launches a Bash shell inside our container.</li> </ul> <p>The container launches. We can see there is a command prompt inside it:</p> <pre>root@af8bae53bdd3:/#\n</pre> <p>Let’s try running some commands inside the container:</p> <pre>root@af8bae53bdd3:/# pwd\n/\nroot@af8bae53bdd3:/# ls\nbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var\n</pre> <p>In this example:</p> <ul> <li>\n<code>pwd</code> displays the current directory, the <code>/</code> root directory.<br>\n</li> <li>\n<code>ls</code> displays the directory listing of the root directory of a typical Linux file system.</li> </ul> <p>Now, you can play around inside this container. When completed, run the <code>exit</code> command or enter Ctrl-D to exit the interactive shell.</p> <pre>root@af8bae53bdd3:/# exit\n</pre> <blockquote> <p><strong>Note:</strong> As with our previous container, once the Bash shell process has finished, the container stops.</p> </blockquote> <h2 id=\"start-a-daemonized-hello-world\">Start a daemonized Hello world</h2> <p>Let’s create a container that runs as a daemon.</p> <pre>$ docker run -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n1e5535038e285177d5214659a068137486f96ee5c2e85a4ac52dc83f2ebe4147\n</pre> <p>In this example:</p> <ul> <li>\n<code>docker run</code> runs the container.</li> <li>\n<code>-d</code> flag runs the container in the background (to daemonize it).</li> <li>\n<code>ubuntu</code> is the image you would like to run.</li> </ul> <p>Finally, we specify a command to run:</p> <pre>/bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n</pre> <p>In the output, we do not see <code>hello world</code> but a long string:</p> <pre>1e5535038e285177d5214659a068137486f96ee5c2e85a4ac52dc83f2ebe4147\n</pre> <p>This long string is called a <em>container ID</em>. It uniquely identifies a container so we can work with it.</p> <blockquote> <p><strong>Note:</strong> The container ID is a bit long and unwieldy. Later, we will cover the short ID and ways to name our containers to make working with them easier.</p> </blockquote> <p>We can use this container ID to see what’s happening with our <code>hello world</code> daemon.</p> <p>First, let’s make sure our container is running. Run the <code>docker ps</code> command. The <code>docker ps</code> command queries the Docker daemon for information about all the containers it knows about.</p> <pre>$ docker ps\nCONTAINER ID  IMAGE         COMMAND               CREATED        STATUS       PORTS NAMES\n1e5535038e28  ubuntu  /bin/sh -c 'while tr  2 minutes ago  Up 1 minute        insane_babbage\n</pre> <p>In this example, we can see our daemonized container. The <code>docker ps</code> returns some useful information:</p> <ul> <li>\n<code>1e5535038e28</code> is the shorter variant of the container ID.</li> <li>\n<code>ubuntu</code> is the used image.</li> <li>the command, status, and assigned name <code>insane_babbage</code>.</li> </ul> <blockquote> <p><strong>Note:</strong> Docker automatically generates names for any containers started. We’ll see how to specify your own names a bit later.</p> </blockquote> <p>Now, we know the container is running. But is it doing what we asked it to do? To see this we’re going to look inside the container using the <code>docker logs</code> command.</p> <p>Let’s use the container name <code>insane_babbage</code>.</p> <pre>$ docker logs insane_babbage\nhello world\nhello world\nhello world\n. . .\n</pre> <p>In this example:</p> <ul> <li>\n<code>docker logs</code> looks inside the container and returns <code>hello world</code>.</li> </ul> <p>Awesome! The daemon is working and you have just created your first Dockerized application!</p> <p>Next, run the <code>docker stop</code> command to stop our detached container.</p> <pre>$ docker stop insane_babbage\ninsane_babbage\n</pre> <p>The <code>docker stop</code> command tells Docker to politely stop the running container and returns the name of the container it stopped.</p> <p>Let’s check it worked with the <code>docker ps</code> command.</p> <pre>$ docker ps\nCONTAINER ID  IMAGE         COMMAND               CREATED        STATUS       PORTS NAMES\n</pre> <p>Excellent. Our container is stopped.</p> <h1 id=\"next-steps\">Next steps</h1> <p>So far, you launched your first containers using the <code>docker run</code> command. You ran an <em>interactive container</em> that ran in the foreground. You also ran a <em>detached container</em> that ran in the background. In the process you learned about several Docker commands:</p> <ul> <li>\n<code>docker ps</code> - Lists containers.</li> <li>\n<code>docker logs</code> - Shows us the standard output of a container.</li> <li>\n<code>docker stop</code> - Stops running containers.</li> </ul> <p>Now, you have the basis learn more about Docker and how to do some more advanced tasks. Go to <a href=\"../usingdocker/index\">“<em>Run a simple application</em>“</a> to actually build a web application with the Docker client.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/containers/dockerizing/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/containers/dockerizing/</a>\n  </p>\n</div>\n","engine/understanding-docker/index":"<h1 id=\"understand-the-architecture\">Understand the architecture</h1> <p>Docker is an open platform for developing, shipping, and running applications. Docker is designed to deliver your applications faster. With Docker you can separate your applications from your infrastructure and treat your infrastructure like a managed application. Docker helps you ship code faster, test faster, deploy faster, and shorten the cycle between writing code and running code.</p> <p>Docker does this by combining kernel containerization features with workflows and tooling that help you manage and deploy your applications.</p> <p>At its core, Docker provides a way to run almost any application securely isolated in a container. The isolation and security allow you to run many containers simultaneously on your host. The lightweight nature of containers, which run without the extra load of a hypervisor, means you can get more out of your hardware.</p> <p>Surrounding the container is tooling and a platform which can help you in several ways:</p> <ul> <li>Get your applications (and supporting components) into Docker containers</li> <li>Distribute and ship those containers to your teams for further development and testing</li> <li>Deploy those applications to your production environment, whether it is in a local data center or the Cloud</li> </ul> <h2 id=\"what-can-i-use-docker-for\">What can I use Docker for?</h2> <p><em>Faster delivery of your applications</em></p> <p>Docker is perfect for helping you with the development lifecycle. Docker allows your developers to develop on local containers that contain your applications and services. It can then integrate into a continuous integration and deployment workflow.</p> <p>For example, your developers write code locally and share their development stack via Docker with their colleagues. When they are ready, they push their code and the stack they are developing onto a test environment and execute any required tests. From the testing environment, you can then push the Docker images into production and deploy your code.</p> <p><em>Deploying and scaling more easily</em></p> <p>Docker’s container-based platform allows for highly portable workloads. Docker containers can run on a developer’s local host, on physical or virtual machines in a data center, or in the Cloud.</p> <p>Docker’s portability and lightweight nature also make dynamically managing workloads easy. You can use Docker to quickly scale up or tear down applications and services. Docker’s speed means that scaling can be near real time.</p> <p><em>Achieving higher density and running more workloads</em></p> <p>Docker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines. This is especially useful in high density environments: for example, building your own Cloud or Platform-as-a-Service. But it is also useful for small and medium deployments where you want to get more out of the resources you have.</p> <h2 id=\"what-are-the-major-docker-components\">What are the major Docker components?</h2> <p>Docker has two major components:</p> <ul> <li>Docker Engine: the open source containerization platform.</li> <li>\n<a href=\"https://hub.docker.com\">Docker Hub</a>: our Software-as-a-Service platform for sharing and managing Docker containers.</li> </ul> <blockquote> <p><strong>Note:</strong> Docker is licensed under the open source Apache 2.0 license.</p> </blockquote> <h2 id=\"what-is-docker-s-architecture\">What is Docker’s architecture?</h2> <p>Docker uses a client-server architecture. The Docker <em>client</em> talks to the Docker <em>daemon</em>, which does the heavy lifting of building, running, and distributing your Docker containers. Both the Docker client and the daemon <em>can</em> run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate via sockets or through a RESTful API.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/article-img/architecture.svg\" alt=\"Docker Architecture Diagram\"></p> <h3 id=\"the-docker-daemon\">The Docker daemon</h3> <p>As shown in the diagram above, the Docker daemon runs on a host machine. The user does not directly interact with the daemon, but instead through the Docker client.</p> <h3 id=\"the-docker-client\">The Docker client</h3> <p>The Docker client, in the form of the <code>docker</code> binary, is the primary user interface to Docker. It accepts commands from the user and communicates back and forth with a Docker daemon.</p> <h3 id=\"inside-docker\">Inside Docker</h3> <p>To understand Docker’s internals, you need to know about three resources:</p> <ul> <li>Docker images</li> <li>Docker registries</li> <li>Docker containers</li> </ul> <h4 id=\"docker-images\">Docker images</h4> <p>A Docker image is a read-only template. For example, an image could contain an Ubuntu operating system with Apache and your web application installed. Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images, or you can download Docker images that other people have already created. Docker images are the <strong>build</strong> component of Docker.</p> <h4 id=\"docker-registries\">Docker registries</h4> <p>Docker registries hold images. These are public or private stores from which you upload or download images. The public Docker registry is provided with the <a href=\"http://hub.docker.com\">Docker Hub</a>. It serves a huge collection of existing images for your use. These can be images you create yourself or you can use images that others have previously created. Docker registries are the <strong>distribution</strong> component of Docker. For more information, go to <a href=\"https://docs.docker.com/registry/overview/\">Docker Registry</a> and <a href=\"https://docs.docker.com/docker-trusted-registry/overview/\">Docker Trusted Registry</a>.</p> <h4 id=\"docker-containers\">Docker containers</h4> <p>Docker containers are similar to a directory. A Docker container holds everything that is needed for an application to run. Each container is created from a Docker image. Docker containers can be run, started, stopped, moved, and deleted. Each container is an isolated and secure application platform. Docker containers are the <strong>run</strong> component of Docker.</p> <h3 id=\"how-does-a-docker-image-work\">How does a Docker image work?</h3> <p>We’ve already seen that Docker images are read-only templates from which Docker containers are launched. Each image consists of a series of layers. Docker makes use of <a href=\"http://en.wikipedia.org/wiki/UnionFS\">union file systems</a> to combine these layers into a single image. Union file systems allow files and directories of separate file systems, known as branches, to be transparently overlaid, forming a single coherent file system.</p> <p>One of the reasons Docker is so lightweight is because of these layers. When you change a Docker image—for example, update an application to a new version— a new layer gets built. Thus, rather than replacing the whole image or entirely rebuilding, as you may do with a virtual machine, only that layer is added or updated. Now you don’t need to distribute a whole new image, just the update, making distributing Docker images faster and simpler.</p> <p>Every image starts from a base image, for example <code>ubuntu</code>, a base Ubuntu image, or <code>fedora</code>, a base Fedora image. You can also use images of your own as the basis for a new image, for example if you have a base Apache image you could use this as the base of all your web application images.</p> <blockquote> <p><strong>Note:</strong> <a href=\"https://hub.docker.com\">Docker Hub</a> is a public registry and stores images.</p> </blockquote> <p>Docker images are then built from these base images using a simple, descriptive set of steps we call <em>instructions</em>. Each instruction creates a new layer in our image. Instructions include actions like:</p> <ul> <li>Run a command</li> <li>Add a file or directory</li> <li>Create an environment variable</li> <li>What process to run when launching a container from this image</li> </ul> <p>These instructions are stored in a file called a <code>Dockerfile</code>. A <code>Dockerfile</code> is a text based script that contains instructions and commands for building the image from the base image. Docker reads this <code>Dockerfile</code> when you request a build of an image, executes the instructions, and returns a final image.</p> <h3 id=\"how-does-a-docker-registry-work\">How does a Docker registry work?</h3> <p>The Docker registry is the store for your Docker images. Once you build a Docker image you can <em>push</em> it to a public registry such as <a href=\"https://hub.docker.com\">Docker Hub</a> or to your own registry running behind your firewall.</p> <p>Using the Docker client, you can search for already published images and then pull them down to your Docker host to build containers from them.</p> <p><a href=\"https://hub.docker.com\">Docker Hub</a> provides both public and private storage for images. Public storage is searchable and can be downloaded by anyone. Private storage is excluded from search results and only you and your users can pull images down and use them to build containers. You can <a href=\"https://hub.docker.com/plans\">sign up for a storage plan here</a>.</p> <h3 id=\"how-does-a-container-work\">How does a container work?</h3> <p>A container consists of an operating system, user-added files, and meta-data. As we’ve seen, each container is built from an image. That image tells Docker what the container holds, what process to run when the container is launched, and a variety of other configuration data. The Docker image is read-only. When Docker runs a container from an image, it adds a read-write layer on top of the image (using a union file system as we saw earlier) in which your application can then run.</p> <h3 id=\"what-happens-when-you-run-a-container\">What happens when you run a container?</h3> <p>Either by using the <code>docker</code> binary or via the API, the Docker client tells the Docker daemon to run a container.</p> <pre>$ docker run -i -t ubuntu /bin/bash\n</pre> <p>The Docker Engine client is launched using the <code>docker</code> binary with the <code>run</code> option running a new container. The bare minimum the Docker client needs to tell the Docker daemon to run the container is:</p> <ul> <li>What Docker image to build the container from, for example, <code>ubuntu</code>\n</li> <li>The command you want to run inside the container when it is launched, for example,<code>/bin/bash</code>\n</li> </ul> <p>So what happens under the hood when we run this command?</p> <p>In order, Docker Engine does the following:</p> <ul> <li>\n<strong>Pulls the <code>ubuntu</code> image:</strong> Docker Engine checks for the presence of the <code>ubuntu</code> image. If the image already exists, then Docker Engine uses it for the new container. If it doesn’t exist locally on the host, then Docker Engine pulls it from <a href=\"https://hub.docker.com\">Docker Hub</a>.</li> <li>\n<strong>Creates a new container:</strong> Once Docker Engine has the image, it uses it to create a container.</li> <li>\n<strong>Allocates a filesystem and mounts a read-write <em>layer</em>:</strong> The container is created in the file system and a read-write layer is added to the image.</li> <li>\n<strong>Allocates a network / bridge interface:</strong> Creates a network interface that allows the Docker container to talk to the local host.</li> <li>\n<strong>Sets up an IP address:</strong> Finds and attaches an available IP address from a pool.</li> <li>\n<strong>Executes a process that you specify:</strong> Runs your application, and;</li> <li>\n<strong>Captures and provides application output:</strong> Connects and logs standard input, outputs and errors for you to see how your application is running.</li> </ul> <p>You now have a running container! Now you can manage your container, interact with your application and then, when finished, stop and remove your container.</p> <h2 id=\"the-underlying-technology\">The underlying technology</h2> <p>Docker is written in Go and makes use of several kernel features to deliver the functionality we’ve seen.</p> <h3 id=\"namespaces\">Namespaces</h3> <p>Docker takes advantage of a technology called <code>namespaces</code> to provide the isolated workspace we call the <em>container</em>. When you run a container, Docker creates a set of <em>namespaces</em> for that container.</p> <p>This provides a layer of isolation: each aspect of a container runs in its own namespace and does not have access outside it.</p> <p>Some of the namespaces that Docker Engine uses on Linux are:</p> <ul> <li>\n<strong>The <code>pid</code> namespace:</strong> Process isolation (PID: Process ID).</li> <li>\n<strong>The <code>net</code> namespace:</strong> Managing network interfaces (NET: Networking).</li> <li>\n<strong>The <code>ipc</code> namespace:</strong> Managing access to IPC resources (IPC: InterProcess Communication).</li> <li>\n<strong>The <code>mnt</code> namespace:</strong> Managing mount-points (MNT: Mount).</li> <li>\n<strong>The <code>uts</code> namespace:</strong> Isolating kernel and version identifiers. (UTS: Unix Timesharing System).</li> </ul> <h3 id=\"control-groups\">Control groups</h3> <p>Docker Engine on Linux also makes use of another technology called <code>cgroups</code> or control groups. A key to running applications in isolation is to have them only use the resources you want. This ensures containers are good multi-tenant citizens on a host. Control groups allow Docker Engine to share available hardware resources to containers and, if required, set up limits and constraints. For example, limiting the memory available to a specific container.</p> <h3 id=\"union-file-systems\">Union file systems</h3> <p>Union file systems, or UnionFS, are file systems that operate by creating layers, making them very lightweight and fast. Docker Engine uses union file systems to provide the building blocks for containers. Docker Engine can make use of several union file system variants including: AUFS, btrfs, vfs, and DeviceMapper.</p> <h3 id=\"container-format\">Container format</h3> <p>Docker Engine combines these components into a wrapper we call a container format. The default container format is called <code>libcontainer</code>. In the future, Docker may support other container formats, for example, by integrating with BSD Jails or Solaris Zones.</p> <h2 id=\"next-steps\">Next steps</h2> <p>Read about <a href=\"../installation/index#installation\">Installing Docker Engine</a>. Learn about the <a href=\"../userguide/index\">Docker Engine User Guide</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/understanding-docker/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/understanding-docker/</a>\n  </p>\n</div>\n","engine/installation/linux/debian/index":"<h1 id=\"debian\">Debian</h1> <p>Docker is supported on the following versions of Debian:</p> <ul> <li><a href=\"#debian-wheezy-stable-7-x-64-bit\"><em>Debian testing stretch (64-bit)</em></a></li> <li><a href=\"#debian-jessie-80-64-bit\"><em>Debian 8.0 Jessie (64-bit)</em></a></li> <li>\n<a href=\"#debian-wheezy-stable-7-x-64-bit\"><em>Debian 7.7 Wheezy (64-bit)</em></a> (backports required)</li> </ul> <blockquote> <p><strong>Note</strong>: If you previously installed Docker using <code>APT</code>, make sure you update your <code>APT</code> sources to the new <code>APT</code> repository.</p> </blockquote> <h2 id=\"prerequisites\">Prerequisites</h2> <p>Docker requires a 64-bit installation regardless of your Debian version. Additionally, your kernel must be 3.10 at minimum. The latest 3.10 minor version or a newer maintained version are also acceptable.</p> <p>Kernels older than 3.10 lack some of the features required to run Docker containers. These older versions are known to have bugs which cause data loss and frequently panic under certain conditions.</p> <p>To check your current kernel version, open a terminal and use <code>uname -r</code> to display your kernel version:</p> <pre> $ uname -r\n</pre> <p>Additionally, for users of Debian Wheezy, backports must be available. To enable backports in Wheezy:</p> <ol> <li><p>Log into your machine and open a terminal with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Open the <code>/etc/apt/sources.list.d/backports.list</code> file in your favorite editor.</p> <p>If the file doesn’t exist, create it.</p>\n</li> <li><p>Remove any existing entries.</p></li> <li>\n<p>Add an entry for backports on Debian Wheezy.</p> <p>An example entry:</p> <pre> deb http://http.debian.net/debian wheezy-backports main\n</pre>\n</li> <li>\n<p>Update package information:</p> <pre> $ apt-get update\n</pre>\n</li> </ol> <h3 id=\"update-your-apt-repository\">Update your apt repository</h3> <p>Docker’s <code>APT</code> repository contains Docker 1.7.1 and higher. To set <code>APT</code> to use from the new repository:</p> <ol> <li><p>If you haven’t already done so, log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li><p>Open a terminal window.</p></li> <li>\n<p>Purge any older repositories.</p> <pre> $ apt-get purge lxc-docker*\n $ apt-get purge docker.io*\n</pre>\n</li> <li>\n<p>Update package information, ensure that APT works with the <code>https</code> method, and that CA certificates are installed.</p> <pre> $ apt-get update\n $ apt-get install apt-transport-https ca-certificates\n</pre>\n</li> <li>\n<p>Add the new <code>GPG</code> key.</p> <pre> $ apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n</pre>\n</li> <li>\n<p>Open the <code>/etc/apt/sources.list.d/docker.list</code> file in your favorite editor.</p> <p>If the file doesn’t exist, create it.</p>\n</li> <li><p>Remove any existing entries.</p></li> <li>\n<p>Add an entry for your Debian operating system.</p> <p>The possible entries are:</p> <ul> <li>\n<p>On Debian Wheezy</p> <pre>deb https://apt.dockerproject.org/repo debian-wheezy main\n</pre>\n</li> <li>\n<p>On Debian Jessie</p> <pre>deb https://apt.dockerproject.org/repo debian-jessie main\n</pre>\n</li> <li>\n<p>On Debian Stretch/Sid</p> <pre>deb https://apt.dockerproject.org/repo debian-stretch main\n</pre>\n</li> </ul> <blockquote> <p><strong>Note</strong>: Docker does not provide packages for all architectures. To install docker on a multi-architecture system, add an <code>[arch=...]</code> clause to the entry. Refer to the <a href=\"https://wiki.debian.org/Multiarch/HOWTO#Setting_up_apt_sources\">Debian Multiarch wiki</a> for details.</p> </blockquote>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>Update the <code>APT</code> package index.</p> <pre> $ apt-get update\n</pre>\n</li> <li>\n<p>Verify that <code>APT</code> is pulling from the right repository.</p> <pre> $ apt-cache policy docker-engine\n</pre> <p>From now on when you run <code>apt-get upgrade</code>, <code>APT</code> pulls from the new apt repository.</p>\n</li> </ol> <h2 id=\"install-docker\">Install Docker</h2> <p>Before installing Docker, make sure you have set your <code>APT</code> repository correctly as described in the prerequisites.</p> <ol> <li>\n<p>Update the <code>APT</code> package index.</p> <pre>$ sudo apt-get update\n</pre>\n</li> <li>\n<p>Install Docker.</p> <pre>$ sudo apt-get install docker-engine\n</pre>\n</li> <li>\n<p>Start the <code>docker</code> daemon.</p> <pre>$ sudo service docker start\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly.</p> <pre>$ sudo docker run hello-world\n</pre> <p>This command downloads a test image and runs it in a container. When the container runs, it prints an informational message. Then, it exits.</p>\n</li> </ol> <h2 id=\"giving-non-root-access\">Giving non-root access</h2> <p>The <code>docker</code> daemon always runs as the <code>root</code> user and the <code>docker</code> daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user <code>root</code>, and so, by default, you can access it with <code>sudo</code>.</p> <p>If you (or your Docker installer) create a Unix group called <code>docker</code> and add users to it, then the <code>docker</code> daemon will make the ownership of the Unix socket read/writable by the <code>docker</code> group when the daemon starts. The <code>docker</code> daemon must always run as the root user, but if you run the <code>docker</code> client as a user in the <code>docker</code> group then you don’t need to add <code>sudo</code> to all the client commands. From Docker 0.9.0 you can use the <code>-G</code> flag to specify an alternative group.</p> <blockquote> <p><strong>Warning</strong>: The <code>docker</code> group (or the group specified with the <code>-G</code> flag) is <code>root</code>-equivalent; see <a href=\"../../../security/security/index#docker-daemon-attack-surface\"><em>Docker Daemon Attack Surface</em></a> details.</p> </blockquote> <p><strong>Example:</strong></p> <pre># Add the docker group if it doesn't already exist.\n$ sudo groupadd docker\n\n# Add the connected user \"${USER}\" to the docker group.\n# Change the user name to match your preferred user.\n# You may have to logout and log back in again for\n# this to take effect.\n$ sudo gpasswd -a ${USER} docker\n\n# Restart the Docker daemon.\n$ sudo service docker restart\n</pre> <h2 id=\"upgrade-docker\">Upgrade Docker</h2> <p>To install the latest version of Docker with <code>apt-get</code>:</p> <pre>$ apt-get upgrade docker-engine\n</pre> <h2 id=\"uninstall\">Uninstall</h2> <p>To uninstall the Docker package:</p> <pre>$ sudo apt-get purge docker-engine\n</pre> <p>To uninstall the Docker package and dependencies that are no longer needed:</p> <pre>$ sudo apt-get autoremove --purge docker-engine\n</pre> <p>The above commands will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre> <p>You must delete the user created configuration files manually.</p> <h2 id=\"what-next\">What next?</h2> <p>Continue with the <a href=\"../../../userguide/index\">User Guide</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/debian/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/debian/</a>\n  </p>\n</div>\n","engine/installation/linux/archlinux/index":"<h1 id=\"arch-linux\">Arch Linux</h1> <p>Installing on Arch Linux can be handled via the package in community:</p> <ul> <li><a href=\"https://www.archlinux.org/packages/community/x86_64/docker/\">docker</a></li> </ul> <p>or the following AUR package:</p> <ul> <li><a href=\"https://aur.archlinux.org/packages/docker-git/\">docker-git</a></li> </ul> <p>The docker package will install the latest tagged version of docker. The docker-git package will build from the current master branch.</p> <h2 id=\"dependencies\">Dependencies</h2> <p>Docker depends on several packages which are specified as dependencies in the packages. The core dependencies are:</p> <ul> <li>bridge-utils</li> <li>device-mapper</li> <li>iproute2</li> <li>sqlite</li> </ul> <h2 id=\"installation\">Installation</h2> <p>For the normal package a simple</p> <pre>$ sudo pacman -S docker\n</pre> <p>is all that is needed.</p> <p>For the AUR package execute:</p> <pre>$ yaourt -S docker-git\n</pre> <p>The instructions here assume <strong>yaourt</strong> is installed. See <a href=\"https://wiki.archlinux.org/index.php/Arch_User_Repository#Installing_packages\">Arch User Repository</a> for information on building and installing packages from the AUR if you have not done so before.</p> <h2 id=\"starting-docker\">Starting Docker</h2> <p>There is a systemd service unit created for docker. To start the docker service:</p> <pre>$ sudo systemctl start docker\n</pre> <p>To start on system boot:</p> <pre>$ sudo systemctl enable docker\n</pre> <h2 id=\"custom-daemon-options\">Custom daemon options</h2> <p>If you need to add an HTTP Proxy, set a different directory or partition for the Docker runtime files, or make other customizations, read our systemd article to learn how to <a href=\"../../../admin/systemd/index\">customize your systemd Docker daemon options</a>.</p> <h2 id=\"running-docker-with-a-manually-defined-network\">Running Docker with a manually-defined network</h2> <p>If you manually configure your network using <code>systemd-network</code> version 220 or higher, containers you start with Docker may be unable to access your network. Beginning with version 220, the forwarding setting for a given network (<code>net.ipv4.conf.&lt;interface&gt;.forwarding</code>) defaults to <em>off</em>. This setting prevents IP forwarding. It also conflicts with Docker which enables the <code>net.ipv4.conf.all.forwarding</code> setting within a container.</p> <p>To work around this, edit the <code>&lt;interface&gt;.network</code> file in <code>/etc/systemd/network/</code> on your Docker host add the following block:</p> <pre>[Network]\n...\nIPForward=kernel\n...\n</pre> <p>This configuration allows IP forwarding from the container as expected.</p> <h2 id=\"uninstallation\">Uninstallation</h2> <p>To uninstall the Docker package:</p> <pre>$ sudo pacman -R docker\n</pre> <p>To uninstall the Docker package and dependencies that are no longer needed:</p> <pre>$ sudo pacman -Rns docker\n</pre> <p>The above commands will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre> <p>You must delete the user created configuration files manually.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/archlinux/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/archlinux/</a>\n  </p>\n</div>\n","engine/installation/linux/cruxlinux/index":"<h1 id=\"crux-linux\">CRUX Linux</h1> <p>Installing on CRUX Linux can be handled via the contrib ports from <a href=\"http://prologic.shortcircuit.net.au/\">James Mills</a> and are included in the official <a href=\"http://crux.nu/portdb/?a=repo&amp;q=contrib\">contrib</a> ports:</p> <ul> <li>docker</li> </ul> <p>The <code>docker</code> port will build and install the latest tagged version of Docker.</p> <h2 id=\"installation\">Installation</h2> <p>Assuming you have contrib enabled, update your ports tree and install docker:</p> <pre>$ sudo prt-get depinst docker\n</pre> <h2 id=\"kernel-requirements\">Kernel requirements</h2> <p>To have a working <strong>CRUX+Docker</strong> Host you must ensure your Kernel has the necessary modules enabled for the Docker Daemon to function correctly.</p> <p>Please read the <code>README</code>:</p> <pre>$ sudo prt-get readme docker\n</pre> <p>The <code>docker</code> port installs the <code>contrib/check-config.sh</code> script provided by the Docker contributors for checking your kernel configuration as a suitable Docker host.</p> <p>To check your Kernel configuration run:</p> <pre>$ /usr/share/docker/check-config.sh\n</pre> <h2 id=\"starting-docker\">Starting Docker</h2> <p>There is a rc script created for Docker. To start the Docker service:</p> <pre>$ sudo /etc/rc.d/docker start\n</pre> <p>To start on system boot:</p> <ul> <li>Edit <code>/etc/rc.conf</code>\n</li> <li>Put <code>docker</code> into the <code>SERVICES=(...)</code> array after <code>net</code>.</li> </ul> <h2 id=\"images\">Images</h2> <p>There is a CRUX image maintained by <a href=\"http://prologic.shortcircuit.net.au/\">James Mills</a> as part of the Docker “Official Library” of images. To use this image simply pull it or use it as part of your <code>FROM</code> line in your <code>Dockerfile(s)</code>.</p> <pre>$ docker pull crux\n$ docker run -i -t crux\n</pre> <p>There are also user contributed <a href=\"https://hub.docker.com/_/crux/\">CRUX based image(s)</a> on the Docker Hub.</p> <h2 id=\"uninstallation\">Uninstallation</h2> <p>To uninstall the Docker package:</p> <pre>$ sudo prt-get remove docker\n</pre> <p>The above command will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre> <p>You must delete the user created configuration files manually.</p> <h2 id=\"issues\">Issues</h2> <p>If you have any issues please file a bug with the <a href=\"http://crux.nu/bugs/\">CRUX Bug Tracker</a>.</p> <h2 id=\"support\">Support</h2> <p>For support contact the <a href=\"http://crux.nu/Main/MailingLists\">CRUX Mailing List</a> or join CRUX’s <a href=\"http://crux.nu/Main/IrcChannels\">IRC Channels</a>. on the <a href=\"http://freenode.net/\">FreeNode</a> IRC Network.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/cruxlinux/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/cruxlinux/</a>\n  </p>\n</div>\n","engine/installation/linux/frugalware/index":"<h1 id=\"frugalware\">FrugalWare</h1> <p>Installing on FrugalWare is handled via the official packages:</p> <ul> <li><a href=\"http://www.frugalware.org/packages/200141\">lxc-docker i686</a></li> <li><a href=\"http://www.frugalware.org/packages/200130\">lxc-docker x86_64</a></li> </ul> <p>The lxc-docker package will install the latest tagged version of Docker.</p> <h2 id=\"dependencies\">Dependencies</h2> <p>Docker depends on several packages which are specified as dependencies in the packages. The core dependencies are:</p> <ul> <li>systemd</li> <li>lvm2</li> <li>sqlite3</li> <li>libguestfs</li> <li>lxc</li> <li>iproute2</li> <li>bridge-utils</li> </ul> <h2 id=\"installation\">Installation</h2> <p>A simple</p> <pre>$ sudo pacman -S lxc-docker\n</pre> <p>is all that is needed.</p> <h2 id=\"starting-docker\">Starting Docker</h2> <p>There is a systemd service unit created for Docker. To start Docker as service:</p> <pre>$ sudo systemctl start lxc-docker\n</pre> <p>To start on system boot:</p> <pre>$ sudo systemctl enable lxc-docker\n</pre> <h2 id=\"custom-daemon-options\">Custom daemon options</h2> <p>If you need to add an HTTP Proxy, set a different directory or partition for the Docker runtime files, or make other customizations, read our systemd article to learn how to <a href=\"../../../admin/systemd/index\">customize your systemd Docker daemon options</a>.</p> <h2 id=\"uninstallation\">Uninstallation</h2> <p>To uninstall the Docker package:</p> <pre>$ sudo pacman -R lxc-docker\n</pre> <p>To uninstall the Docker package and dependencies that are no longer needed:</p> <pre>$ sudo pacman -Rns lxc-docker\n</pre> <p>The above commands will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre> <p>You must delete the user created configuration files manually.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/frugalware/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/frugalware/</a>\n  </p>\n</div>\n","engine/installation/linux/gentoolinux/index":"<h1 id=\"gentoo\">Gentoo</h1> <p>Installing Docker on Gentoo Linux can be accomplished using one of two ways: the <strong>official</strong> way and the <code>docker-overlay</code> way.</p> <p>Official project page of <a href=\"https://wiki.gentoo.org/wiki/Project:Docker\">Gentoo Docker</a> team.</p> <h2 id=\"official-way\">Official way</h2> <p>The first and recommended way if you are looking for a stable<br> experience is to use the official <code>app-emulation/docker</code> package directly<br> from the tree.</p> <p>If any issues arise from this ebuild including, missing kernel configuration flags or dependencies, open a bug on the Gentoo <a href=\"https://bugs.gentoo.org\">Bugzilla</a> assigned to <code>docker AT gentoo DOT org</code> or join and ask in the official <a href=\"http://webchat.freenode.net?channels=%23gentoo-containers&amp;uio=d4\">IRC</a> channel on the Freenode network.</p> <h2 id=\"docker-overlay-way\">docker-overlay way</h2> <p>If you’re looking for a <code>-bin</code> ebuild, a live ebuild, or a bleeding edge ebuild, use the provided overlay, <a href=\"https://github.com/tianon/docker-overlay\">docker-overlay</a> which can be added using <code>app-portage/layman</code>. The most accurate and up-to-date documentation for properly installing and using the overlay can be found in the <a href=\"https://github.com/tianon/docker-overlay/blob/master/README.md#using-this-overlay\">overlay</a>.</p> <p>If any issues arise from this ebuild or the resulting binary, including and especially missing kernel configuration flags or dependencies, open an <a href=\"https://github.com/tianon/docker-overlay/issues\">issue</a> on the <code>docker-overlay</code> repository or ping <code>tianon</code> directly in the <code>#docker</code> IRC channel on the Freenode network.</p> <h2 id=\"installation\">Installation</h2> <h3 id=\"available-use-flags\">Available USE flags</h3> <table> <thead> <tr> <th>USE Flag</th> <th align=\"center\">Default</th> <th align=\"left\">Description</th> </tr> </thead> <tbody> <tr> <td>aufs</td> <td align=\"center\"></td> <td align=\"left\">Enables dependencies for the “aufs” graph driver, including necessary kernel flags.</td> </tr> <tr> <td>btrfs</td> <td align=\"center\"></td> <td align=\"left\">Enables dependencies for the “btrfs” graph driver, including necessary kernel flags.</td> </tr> <tr> <td>contrib</td> <td align=\"center\">Yes</td> <td align=\"left\">Install additional contributed scripts and components.</td> </tr> <tr> <td>device-mapper</td> <td align=\"center\">Yes</td> <td align=\"left\">Enables dependencies for the “devicemapper” graph driver, including necessary kernel flags.</td> </tr> <tr> <td>doc</td> <td align=\"center\"></td> <td align=\"left\">Add extra documentation (API, Javadoc, etc). It is recommended to enable per package instead of globally.</td> </tr> <tr> <td>vim-syntax</td> <td align=\"center\"></td> <td align=\"left\">Pulls in related vim syntax scripts.</td> </tr> <tr> <td>zsh-completion</td> <td align=\"center\"></td> <td align=\"left\">Enable zsh completion support.</td> </tr> </tbody> </table> <p>USE flags are described in detail on <a href=\"https://tianon.github.io/post/2014/05/17/docker-on-gentoo.html\">tianon’s blog</a>.</p> <p>The package should properly pull in all the necessary dependencies and prompt for all necessary kernel options.</p> <pre>$ sudo emerge -av app-emulation/docker\n</pre> <blockquote> <p>Note: Sometimes there is a disparity between the latest versions in the official <strong>Gentoo tree</strong> and the <strong>docker-overlay</strong>.<br> Please be patient, and the latest version should propagate shortly.</p> </blockquote> <h2 id=\"starting-docker\">Starting Docker</h2> <p>Ensure that you are running a kernel that includes all the necessary modules and configuration (and optionally for device-mapper and AUFS or Btrfs, depending on the storage driver you’ve decided to use).</p> <p>To use Docker, the <code>docker</code> daemon must be running as <strong>root</strong>.<br> To use Docker as a <strong>non-root</strong> user, add yourself to the <strong>docker</strong> group by running the following command:</p> <pre>$ sudo groupadd docker\n$ sudo usermod -a -G docker user\n</pre> <h3 id=\"openrc\">OpenRC</h3> <p>To start the <code>docker</code> daemon:</p> <pre>$ sudo /etc/init.d/docker start\n</pre> <p>To start on system boot:</p> <pre>$ sudo rc-update add docker default\n</pre> <h3 id=\"systemd\">systemd</h3> <p>To start the <code>docker</code> daemon:</p> <pre>$ sudo systemctl start docker\n</pre> <p>To start on system boot:</p> <pre>$ sudo systemctl enable docker\n</pre> <p>If you need to add an HTTP Proxy, set a different directory or partition for the Docker runtime files, or make other customizations, read our systemd article to learn how to <a href=\"../../../admin/systemd/index\">customize your systemd Docker daemon options</a>.</p> <h2 id=\"uninstallation\">Uninstallation</h2> <p>To uninstall the Docker package:</p> <pre>$ sudo emerge -cav app-emulation/docker\n</pre> <p>To uninstall the Docker package and dependencies that are no longer needed:</p> <pre>$ sudo emerge -C app-emulation/docker\n</pre> <p>The above commands will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre> <p>You must delete the user created configuration files manually.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/gentoolinux/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/gentoolinux/</a>\n  </p>\n</div>\n","engine/installation/cloud/overview/index":"<h1 id=\"choose-how-to-install\">Choose how to install</h1> <p>You can install Docker Engine on any cloud platform that runs an operating system (OS) that Docker supports. This includes many flavors and versions of Linux, along with Mac and Windows.</p> <p>You have two options for installing:</p> <ul> <li>Manually install on the cloud (create cloud hosts, then install Docker Engine on them)</li> <li>Use Docker Machine to provision cloud hosts</li> </ul> <h2 id=\"manually-install-docker-engine-on-a-cloud-host\">Manually install Docker Engine on a cloud host</h2> <p>To install on a cloud provider:</p> <ol> <li><p>Create an account with the cloud provider, and read cloud provider documentation to understand their process for creating hosts.</p></li> <li><p>Decide which OS you want to run on the cloud host.</p></li> <li><p>Understand the Docker prerequisites and install process for the chosen OS. See <a href=\"../../index\">Install Docker Engine</a> for a list of supported systems and links to the install guides.</p></li> <li><p>Create a host with a Docker supported OS, and install Docker per the instructions for that OS.</p></li> </ol> <p><a href=\"../cloud-ex-aws/index\">Example (AWS): Manual install on a cloud provider</a> shows how to create an <a href=\"https://aws.amazon.com/\" target=\"_blank\"> Amazon Web Services (AWS)</a> EC2 instance, and install Docker Engine on it.</p> <h2 id=\"use-docker-machine-to-provision-cloud-hosts\">Use Docker Machine to provision cloud hosts</h2> <p>Docker Machine driver plugins are available for several popular cloud platforms, so you can use Machine to provision one or more Dockerized hosts on those platforms.</p> <p>With Docker Machine, you can use the same interface to create cloud hosts with Docker Engine on them, each configured per the options you specify.</p> <p>To do this, you use the <code>docker-machine create</code> command with the driver for the cloud provider, and provider-specific flags for account verification, security credentials, and other configuration details.</p> <p><a href=\"../cloud-ex-machine-ocean/index\">Example: Use Docker Machine to provision cloud hosts</a> walks you through the steps to set up Docker Machine and provision a Dockerized host on <a href=\"https://www.digitalocean.com/\" target=\"_blank\">Digital Ocean</a>).</p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><p><a href=\"../cloud-ex-aws/index\">Example: Manual install on a cloud provider</a> (AWS EC2)</p></li> <li><p><a href=\"../cloud-ex-machine-ocean/index\">Example: Use Docker Machine to provision cloud hosts</a> (Digital Ocean)</p></li> <li><p>For supported platforms, see <a href=\"../../index\">Install Docker Engine</a>.</p></li> <li><p>To get started with Docker post-install, see <a href=\"../../../userguide/intro/index\">Docker User Guide</a>.</p></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/cloud/overview/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/cloud/overview/</a>\n  </p>\n</div>\n","engine/userguide/intro/index":"<h1 id=\"engine-user-guide\">Engine user guide</h1> <p>This guide takes you through the fundamentals of using Docker Engine and integrating it into your environment. You’ll learn how to use Engine to:</p> <ul> <li>Dockerize your applications.</li> <li>Run your own containers.</li> <li>Build Docker images.</li> <li>Share your Docker images with others.</li> <li>And a whole lot more!</li> </ul> <p>This guide is broken into major sections that take you through learning the basics of Docker Engine and the other Docker products that support it.</p> <h2 id=\"dockerizing-applications-a-hello-world\">Dockerizing applications: A “Hello world”</h2> <p><em>How do I run applications inside containers?</em></p> <p>Docker Engine offers a containerization platform to power your applications. To learn how to Dockerize applications and run them:</p> <p>Go to <a href=\"../containers/dockerizing/index\">Dockerizing Applications</a>.</p> <h2 id=\"working-with-containers\">Working with containers</h2> <p><em>How do I manage my containers?</em></p> <p>Once you get a grip on running your applications in Docker containers, you’ll learn how to manage those containers. To find out about how to inspect, monitor and manage containers:</p> <p>Go to <a href=\"../containers/usingdocker/index\">Working with Containers</a>.</p> <h2 id=\"working-with-docker-images\">Working with Docker images</h2> <p><em>How can I access, share and build my own images?</em></p> <p>Once you’ve learnt how to use Docker it’s time to take the next step and learn how to build your own application images with Docker.</p> <p>Go to <a href=\"../containers/dockerimages/index\">Working with Docker Images</a>.</p> <h2 id=\"networking-containers\">Networking containers</h2> <p>Until now we’ve seen how to build individual applications inside Docker containers. Now learn how to build whole application stacks with Docker networking.</p> <p>Go to <a href=\"../containers/networkingcontainers/index\">Networking Containers</a>.</p> <h2 id=\"managing-data-in-containers\">Managing data in containers</h2> <p>Now we know how to link Docker containers together the next step is learning how to manage data, volumes and mounts inside our containers.</p> <p>Go to <a href=\"../containers/dockervolumes/index\">Managing Data in Containers</a>.</p> <h2 id=\"docker-products-that-complement-engine\">Docker products that complement Engine</h2> <p>Often, one powerful technology spawns many other inventions that make that easier to get to, easier to use, and more powerful. These spawned things share one common characteristic: they augment the central technology. The following Docker products expand on the core Docker Engine functions.</p> <h3 id=\"docker-hub\">Docker Hub</h3> <p>Docker Hub is the central hub for Docker. It hosts public Docker images and provides services to help you build and manage your Docker environment. To learn more:</p> <p>Go to <a href=\"https://docs.docker.com/docker-hub\">Using Docker Hub</a>.</p> <h3 id=\"docker-machine\">Docker Machine</h3> <p>Docker Machine helps you get Docker Engines up and running quickly. Machine can set up hosts for Docker Engines on your computer, on cloud providers, and/or in your data center, and then configure your Docker client to securely talk to them.</p> <p>Go to <a href=\"https://docs.docker.com/machine/\">Docker Machine user guide</a>.</p> <h3 id=\"docker-compose\">Docker Compose</h3> <p>Docker Compose allows you to define an application’s components -- their containers, configuration, links and volumes -- in a single file. Then a single command will set everything up and start your application running.</p> <p>Go to <a href=\"https://docs.docker.com/compose/\">Docker Compose user guide</a>.</p> <h3 id=\"docker-swarm\">Docker Swarm</h3> <p>Docker Swarm pools several Docker Engines together and exposes them as a single virtual Docker Engine. It serves the standard Docker API, so any tool that already works with Docker can now transparently scale up to multiple hosts.</p> <p>Go to <a href=\"https://docs.docker.com/swarm/\">Docker Swarm user guide</a>.</p> <h2 id=\"getting-help\">Getting help</h2> <ul> <li><a href=\"https://www.docker.com/\">Docker homepage</a></li> <li><a href=\"https://hub.docker.com\">Docker Hub</a></li> <li><a href=\"https://blog.docker.com/\">Docker blog</a></li> <li><a href=\"https://docs.docker.com/\">Docker documentation</a></li> <li><a href=\"https://docs.docker.com/mac/started/\">Docker Getting Started Guide</a></li> <li><a href=\"https://github.com/docker/docker\">Docker code on GitHub</a></li> <li><a href=\"https://groups.google.com/forum/#!forum/docker-user\">Docker mailing list</a></li> <li>Docker on IRC: irc.freenode.net and channel #docker</li> <li><a href=\"https://twitter.com/docker\">Docker on Twitter</a></li> <li>Get <a href=\"https://stackoverflow.com/search?q=docker\">Docker help</a> on StackOverflow</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/intro/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/intro/</a>\n  </p>\n</div>\n","engine/installation/linux/ubuntulinux/index":"<h1 id=\"ubuntu\">Ubuntu</h1> <p>Docker is supported on these Ubuntu operating systems:</p> <ul> <li>Ubuntu Xenial 16.04 (LTS)</li> <li>Ubuntu Wily 15.10</li> <li>Ubuntu Trusty 14.04 (LTS)</li> <li>Ubuntu Precise 12.04 (LTS)</li> </ul> <p>This page instructs you to install using Docker-managed release packages and installation mechanisms. Using these packages ensures you get the latest release of Docker. If you wish to install using Ubuntu-managed packages, consult your Ubuntu documentation.</p> <blockquote> <p><strong>Note</strong>: Ubuntu Utopic 14.10 and 15.04 exist in Docker’s <code>APT</code> repository but are no longer officially supported.</p> </blockquote> <h2 id=\"prerequisites\">Prerequisites</h2> <p>Docker requires a 64-bit installation regardless of your Ubuntu version. Additionally, your kernel must be 3.10 at minimum. The latest 3.10 minor version or a newer maintained version are also acceptable.</p> <p>Kernels older than 3.10 lack some of the features required to run Docker containers. These older versions are known to have bugs which cause data loss and frequently panic under certain conditions.</p> <p>To check your current kernel version, open a terminal and use <code>uname -r</code> to display your kernel version:</p> <pre>$ uname -r\n3.11.0-15-generic\n</pre> <blockquote> <p><strong>Note</strong>: If you previously installed Docker using <code>APT</code>, make sure you update your <code>APT</code> sources to the new Docker repository.</p> </blockquote> <h3 id=\"update-your-apt-sources\">Update your apt sources</h3> <p>Docker’s <code>APT</code> repository contains Docker 1.7.1 and higher. To set <code>APT</code> to use packages from the new repository:</p> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li><p>Open a terminal window.</p></li> <li>\n<p>Update package information, ensure that APT works with the <code>https</code> method, and that CA certificates are installed.</p> <pre> $ sudo apt-get update\n $ sudo apt-get install apt-transport-https ca-certificates\n</pre>\n</li> <li>\n<p>Add the new <code>GPG</code> key.</p> <pre>$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n</pre>\n</li> <li>\n<p>Open the <code>/etc/apt/sources.list.d/docker.list</code> file in your favorite editor.</p> <p>If the file doesn’t exist, create it.</p>\n</li> <li><p>Remove any existing entries.</p></li> <li>\n<p>Add an entry for your Ubuntu operating system.</p> <p>The possible entries are:</p> <ul> <li>\n<p>On Ubuntu Precise 12.04 (LTS)</p> <pre>deb https://apt.dockerproject.org/repo ubuntu-precise main\n</pre>\n</li> <li>\n<p>On Ubuntu Trusty 14.04 (LTS)</p> <pre>deb https://apt.dockerproject.org/repo ubuntu-trusty main\n</pre>\n</li> <li>\n<p>Ubuntu Wily 15.10</p> <pre>deb https://apt.dockerproject.org/repo ubuntu-wily main\n</pre>\n</li> <li>\n<p>Ubuntu Xenial 16.04 (LTS)</p> <pre>deb https://apt.dockerproject.org/repo ubuntu-xenial main\n</pre>\n</li> </ul> <blockquote> <p><strong>Note</strong>: Docker does not provide packages for all architectures. You can find nightly built binaries in <a href=\"https://master.dockerproject.org\">https://master.dockerproject.org</a>. To install docker on a multi-architecture system, add an <code>[arch=...]</code> clause to the entry. Refer to the <a href=\"https://wiki.debian.org/Multiarch/HOWTO#Setting_up_apt_sources\">Debian Multiarch wiki</a> for details.</p> </blockquote>\n</li> <li><p>Save and close the <code>/etc/apt/sources.list.d/docker.list</code> file.</p></li> <li>\n<p>Update the <code>APT</code> package index.</p> <pre>$ sudo apt-get update\n</pre>\n</li> <li>\n<p>Purge the old repo if it exists.</p> <pre>$ sudo apt-get purge lxc-docker\n</pre>\n</li> <li>\n<p>Verify that <code>APT</code> is pulling from the right repository.</p> <pre>$ apt-cache policy docker-engine\n</pre> <p>From now on when you run <code>apt-get upgrade</code>, <code>APT</code> pulls from the new repository.</p>\n</li> </ol> <h3 id=\"prerequisites-by-ubuntu-version\">Prerequisites by Ubuntu Version</h3> <ul> <li>Ubuntu Xenial 16.04 (LTS)</li> <li>Ubuntu Wily 15.10</li> <li>Ubuntu Trusty 14.04 (LTS)</li> </ul> <p>For Ubuntu Trusty, Wily, and Xenial, it’s recommended to install the <code>linux-image-extra</code> kernel package. The <code>linux-image-extra</code> package allows you use the <code>aufs</code> storage driver.</p> <p>To install the <code>linux-image-extra</code> package for your kernel version:</p> <ol> <li><p>Open a terminal on your Ubuntu host.</p></li> <li>\n<p>Update your package manager.</p> <pre>$ sudo apt-get update\n</pre>\n</li> <li>\n<p>Install the recommended package.</p> <pre>$ sudo apt-get install linux-image-extra-$(uname -r)\n</pre>\n</li> <li><p>Go ahead and install Docker.</p></li> </ol> <p>If you are installing on Ubuntu 14.04 or 12.04, <code>apparmor</code> is required. You can install it using: <code>apt-get install apparmor</code></p> <h4 id=\"ubuntu-precise-12-04-lts\">Ubuntu Precise 12.04 (LTS)</h4> <p>For Ubuntu Precise, Docker requires the 3.13 kernel version. If your kernel version is older than 3.13, you must upgrade it. Refer to this table to see which packages are required for your environment:</p> \n<table class=\"tg\"> <tr> <td class=\"tg-031\">linux-image-generic-lts-trusty</td> <td class=\"tg-031e\">Generic Linux kernel image. This kernel has AUFS built in. This is required to run Docker.</td> </tr> <tr> <td class=\"tg-031\">linux-headers-generic-lts-trusty</td> <td class=\"tg-031e\">Allows packages such as ZFS and VirtualBox guest additions which depend on them. If you didn’t install the headers for your existing kernel, then you can skip these headers for the”trusty” kernel. If you’re unsure, you should include this package for safety.</td> </tr> <tr> <td class=\"tg-031\">xserver-xorg-lts-trusty</td> <td class=\"tg-031e\" rowspan=\"2\">Optional in non-graphical environments without Unity/Xorg. <b>Required</b> when running Docker on machine with a graphical environment. <br> <br>To learn more about the reasons for these packages, read the installation instructions for backported kernels, specifically the <a href=\"https://wiki.ubuntu.com/Kernel/LTSEnablementStack\" target=\"_blank\">LTS Enablement Stack</a> — refer to note 5 under each version. </td> </tr> <tr> <td class=\"tg-031\">libgl1-mesa-glx-lts-trusty</td> </tr> </table> <p>To upgrade your kernel and install the additional packages, do the following:</p> <ol> <li><p>Open a terminal on your Ubuntu host.</p></li> <li>\n<p>Update your package manager.</p> <pre>$ sudo apt-get update\n</pre>\n</li> <li>\n<p>Install both the required and optional packages.</p> <pre>$ sudo apt-get install linux-image-generic-lts-trusty\n</pre> <p>Depending on your environment, you may install more as described in the preceding table.</p>\n</li> <li>\n<p>Reboot your host.</p> <pre>$ sudo reboot\n</pre>\n</li> <li><p>After your system reboots, go ahead and install Docker.</p></li> </ol> <h2 id=\"install\">Install</h2> <p>Make sure you have installed the prerequisites for your Ubuntu version.</p> <p>Then, install Docker using the following:</p> <ol> <li><p>Log into your Ubuntu installation as a user with <code>sudo</code> privileges.</p></li> <li>\n<p>Update your <code>APT</code> package index.</p> <pre>$ sudo apt-get update\n</pre>\n</li> <li>\n<p>Install Docker.</p> <pre>$ sudo apt-get install docker-engine\n</pre>\n</li> <li>\n<p>Start the <code>docker</code> daemon.</p> <pre>$ sudo service docker start\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly.</p> <pre>$ sudo docker run hello-world\n</pre> <p>This command downloads a test image and runs it in a container. When the container runs, it prints an informational message. Then, it exits.</p>\n</li> </ol> <h2 id=\"optional-configurations\">Optional configurations</h2> <p>This section contains optional procedures for configuring your Ubuntu to work better with Docker.</p> <ul> <li><a href=\"#create-a-docker-group\">Create a docker group</a></li> <li><a href=\"#adjust-memory-and-swap-accounting\">Adjust memory and swap accounting</a></li> <li><a href=\"#enable-ufw-forwarding\">Enable UFW forwarding</a></li> <li><a href=\"#configure-a-dns-server-for-use-by-docker\">Configure a DNS server for use by Docker</a></li> <li><a href=\"#configure-docker-to-start-on-boot\">Configure Docker to start on boot</a></li> </ul> <h3 id=\"create-a-docker-group\">Create a Docker group</h3> <p>The <code>docker</code> daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user <code>root</code> and other users can access it with <code>sudo</code>. For this reason, <code>docker</code> daemon always runs as the <code>root</code> user.</p> <p>To avoid having to use <code>sudo</code> when you use the <code>docker</code> command, create a Unix group called <code>docker</code> and add users to it. When the <code>docker</code> daemon starts, it makes the ownership of the Unix socket read/writable by the <code>docker</code> group.</p> <blockquote> <p><strong>Warning</strong>: The <code>docker</code> group is equivalent to the <code>root</code> user; For details on how this impacts security in your system, see <a href=\"../../../security/security/index#docker-daemon-attack-surface\"><em>Docker Daemon Attack Surface</em></a> for details.</p> </blockquote> <p>To create the <code>docker</code> group and add your user:</p> <ol> <li>\n<p>Log into Ubuntu as a user with <code>sudo</code> privileges.</p> <p>This procedure assumes you log in as the <code>ubuntu</code> user.</p>\n</li> <li>\n<p>Create the <code>docker</code> group.</p> <pre>$ sudo groupadd docker\n</pre>\n</li> <li>\n<p>Add your user to <code>docker</code> group.</p> <pre>$ sudo usermod -aG docker ubuntu\n</pre>\n</li> <li>\n<p>Log out and log back in.</p> <p>This ensures your user is running with the correct permissions.</p>\n</li> <li>\n<p>Verify your work by running <code>docker</code> without <code>sudo</code>.</p> <pre>$ docker run hello-world\n</pre> <p>If this fails with a message similar to this:</p> <pre>Cannot connect to the Docker daemon. Is 'docker daemon' running on this host?\n</pre> <p>Check that the <code>DOCKER_HOST</code> environment variable is not set for your shell. If it is, unset it.</p>\n</li> </ol> <h3 id=\"adjust-memory-and-swap-accounting\">Adjust memory and swap accounting</h3> <p>When users run Docker, they may see these messages when working with an image:</p> <pre>WARNING: Your kernel does not support cgroup swap limit. WARNING: Your\nkernel does not support swap limit capabilities. Limitation discarded.\n</pre> <p>To prevent these messages, enable memory and swap accounting on your system. Enabling memory and swap accounting does induce both a memory overhead and a performance degradation even when Docker is not in use. The memory overhead is about 1% of the total available memory. The performance degradation is roughly 10%.</p> <p>To enable memory and swap on system using GNU GRUB (GNU GRand Unified Bootloader), do the following:</p> <ol> <li><p>Log into Ubuntu as a user with <code>sudo</code> privileges.</p></li> <li><p>Edit the <code>/etc/default/grub</code> file.</p></li> <li>\n<p>Set the <code>GRUB_CMDLINE_LINUX</code> value as follows:</p> <pre>GRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1\"\n</pre>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>Update GRUB.</p> <pre>$ sudo update-grub\n</pre>\n</li> <li><p>Reboot your system.</p></li> </ol> <h3 id=\"enable-ufw-forwarding\">Enable UFW forwarding</h3> <p>If you use <a href=\"https://help.ubuntu.com/community/UFW\">UFW (Uncomplicated Firewall)</a> on the same host as you run Docker, you’ll need to do additional configuration. Docker uses a bridge to manage container networking. By default, UFW drops all forwarding traffic. As a result, for Docker to run when UFW is enabled, you must set UFW’s forwarding policy appropriately.</p> <p>Also, UFW’s default set of rules denies all incoming traffic. If you want to reach your containers from another host allow incoming connections on the Docker port. The Docker port defaults to <code>2376</code> if TLS is enabled or <code>2375</code> when it is not. If TLS is not enabled, communication is unencrypted. By default, Docker runs without TLS enabled.</p> <p>To configure UFW and allow incoming connections on the Docker port:</p> <ol> <li><p>Log into Ubuntu as a user with <code>sudo</code> privileges.</p></li> <li>\n<p>Verify that UFW is installed and enabled.</p> <pre>$ sudo ufw status\n</pre>\n</li> <li>\n<p>Open the <code>/etc/default/ufw</code> file for editing.</p> <pre>$ sudo nano /etc/default/ufw\n</pre>\n</li> <li>\n<p>Set the <code>DEFAULT_FORWARD_POLICY</code> policy to:</p> <pre>DEFAULT_FORWARD_POLICY=\"ACCEPT\"\n</pre>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>Reload UFW to use the new setting.</p> <pre>$ sudo ufw reload\n</pre>\n</li> <li>\n<p>Allow incoming connections on the Docker port.</p> <pre>$ sudo ufw allow 2375/tcp\n</pre>\n</li> </ol> <h3 id=\"configure-a-dns-server-for-use-by-docker\">Configure a DNS server for use by Docker</h3> <p>Systems that run Ubuntu or an Ubuntu derivative on the desktop typically use <code>127.0.0.1</code> as the default <code>nameserver</code> in <code>/etc/resolv.conf</code> file. The NetworkManager also sets up <code>dnsmasq</code> to use the real DNS servers of the connection and sets up <code>nameserver 127.0.0.1</code> in /<code>etc/resolv.conf</code>.</p> <p>When starting containers on desktop machines with these configurations, Docker users see this warning:</p> <pre>WARNING: Local (127.0.0.1) DNS resolver found in resolv.conf and containers\ncan't use it. Using default external servers : [8.8.8.8 8.8.4.4]\n</pre> <p>The warning occurs because Docker containers can’t use the local DNS nameserver. Instead, Docker defaults to using an external nameserver.</p> <p>To avoid this warning, you can specify a DNS server for use by Docker containers. Or, you can disable <code>dnsmasq</code> in NetworkManager. Though, disabling <code>dnsmasq</code> might make DNS resolution slower on some networks.</p> <p>The instructions below describe how to configure the Docker daemon running on Ubuntu 14.10 or below. Ubuntu 15.04 and above use <code>systemd</code> as the boot and service manager. Refer to <a href=\"../../../admin/systemd/index#custom-docker-daemon-options\">control and configure Docker with systemd</a> to configure a daemon controlled by <code>systemd</code>.</p> <p>To specify a DNS server for use by Docker:</p> <ol> <li><p>Log into Ubuntu as a user with <code>sudo</code> privileges.</p></li> <li>\n<p>Open the <code>/etc/default/docker</code> file for editing.</p> <pre>$ sudo nano /etc/default/docker\n</pre>\n</li> <li>\n<p>Add a setting for Docker.</p> <pre>DOCKER_OPTS=\"--dns 8.8.8.8\"\n</pre> <p>Replace <code>8.8.8.8</code> with a local DNS server such as <code>192.168.1.1</code>. You can also specify multiple DNS servers. Separated them with spaces, for example:</p> <pre>--dns 8.8.8.8 --dns 192.168.1.1\n</pre> <blockquote> <p><strong>Warning</strong>: If you’re doing this on a laptop which connects to various networks, make sure to choose a public DNS server.</p> </blockquote>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>Restart the Docker daemon.</p> <pre>$ sudo service docker restart\n</pre>\n</li> </ol>  <p><strong>Or, as an alternative to the previous procedure,</strong> disable <code>dnsmasq</code> in NetworkManager (this might slow your network).</p> <ol> <li>\n<p>Open the <code>/etc/NetworkManager/NetworkManager.conf</code> file for editing.</p> <pre>$ sudo nano /etc/NetworkManager/NetworkManager.conf\n</pre>\n</li> <li>\n<p>Comment out the <code>dns=dnsmasq</code> line:</p> <pre>dns=dnsmasq\n</pre>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>Restart both the NetworkManager and Docker.</p> <pre>$ sudo restart network-manager\n$ sudo restart docker\n</pre>\n</li> </ol> <h3 id=\"configure-docker-to-start-on-boot\">Configure Docker to start on boot</h3> <p>Ubuntu uses <code>systemd</code> as its boot and service manager <code>15.04</code> onwards and <code>upstart</code> for versions <code>14.10</code> and below.</p> <p>For <code>15.04</code> and up, to configure the <code>docker</code> daemon to start on boot, run</p> <pre>$ sudo systemctl enable docker\n</pre> <p>For <code>14.10</code> and below the above installation method automatically configures <code>upstart</code> to start the docker daemon on boot</p> <h2 id=\"upgrade-docker\">Upgrade Docker</h2> <p>To install the latest version of Docker with <code>apt-get</code>:</p> <pre>$ sudo apt-get upgrade docker-engine\n</pre> <h2 id=\"uninstallation\">Uninstallation</h2> <p>To uninstall the Docker package:</p> <pre>$ sudo apt-get purge docker-engine\n</pre> <p>To uninstall the Docker package and dependencies that are no longer needed:</p> <pre>$ sudo apt-get autoremove --purge docker-engine\n</pre> <p>The above commands will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre> <p>You must delete the user created configuration files manually.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/ubuntulinux/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/ubuntulinux/</a>\n  </p>\n</div>\n","engine/installation/linux/oracle/index":"<h1 id=\"oracle-linux\">Oracle Linux</h1> <p>Docker is supported Oracle Linux 6 and 7. You do not require an Oracle Linux Support subscription to install Docker on Oracle Linux.</p> <h2 id=\"prerequisites\">Prerequisites</h2> <p>Due to current Docker limitations, Docker is only able to run only on the x86_64 architecture. Docker requires the use of the Unbreakable Enterprise Kernel Release 4 (4.1.12) or higher on Oracle Linux. This kernel supports the Docker btrfs storage engine on both Oracle Linux 6 and 7.</p> <h2 id=\"install\">Install</h2> <blockquote> <p><strong>Note</strong>: The procedure below installs binaries built by Docker. These binaries are not covered by Oracle Linux support. To ensure Oracle Linux support, please follow the installation instructions provided in the <a href=\"https://docs.oracle.com/en/operating-systems/?tab=2\">Oracle Linux documentation</a>.</p> <p>The installation instructions for Oracle Linux 6 and 7 can be found in <a href=\"https://docs.oracle.com/cd/E52668_01/E75728/html/docker_install_upgrade.html\">Chapter 2 of the Docker User's Guide</a></p> </blockquote> <ol> <li><p>Log into your machine as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Make sure your existing yum packages are up-to-date.</p> <pre>$ sudo yum update\n</pre>\n</li> <li>\n<p>Add the yum repo yourself.</p> <p>For version 6:</p> <pre>$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-EOF\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/oraclelinux/6\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n</pre> <p>For version 7:</p> <pre>$ cat &gt;/etc/yum.repos.d/docker.repo &lt;&lt;-EOF\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/oraclelinux/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n</pre>\n</li> <li>\n<p>Install the Docker package.</p> <pre>$ sudo yum install docker-engine\n</pre>\n</li> <li>\n<p>Start the Docker daemon.</p> <p>On Oracle Linux 6:</p> <pre>$ sudo service docker start\n</pre> <p>On Oracle Linux 7:</p> <pre>$ sudo systemctl start docker.service\n</pre>\n</li> <li>\n<p>Verify <code>docker</code> is installed correctly by running a test image in a container.</p> <pre>$ sudo docker run hello-world\n</pre>\n</li> </ol> <h2 id=\"optional-configurations\">Optional configurations</h2> <p>This section contains optional procedures for configuring your Oracle Linux to work better with Docker.</p> <ul> <li><a href=\"#create-a-docker-group\">Create a docker group</a></li> <li><a href=\"#configure-docker-to-start-on-boot\">Configure Docker to start on boot</a></li> <li><a href=\"#use-the-btrfs-storage-engine\">Use the btrfs storage engine</a></li> </ul> <h3 id=\"create-a-docker-group\">Create a Docker group</h3> <p>The <code>docker</code> daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user <code>root</code> and other users can access it with <code>sudo</code>. For this reason, <code>docker</code> daemon always runs as the <code>root</code> user.</p> <p>To avoid having to use <code>sudo</code> when you use the <code>docker</code> command, create a Unix group called <code>docker</code> and add users to it. When the <code>docker</code> daemon starts, it makes the ownership of the Unix socket read/writable by the <code>docker</code> group.</p> <blockquote> <p><strong>Warning</strong>: The <code>docker</code> group is equivalent to the <code>root</code> user; For details on how this impacts security in your system, see <a href=\"../../../security/security/index#docker-daemon-attack-surface\"><em>Docker Daemon Attack Surface</em></a> for details.</p> </blockquote> <p>To create the <code>docker</code> group and add your user:</p> <ol> <li><p>Log into Oracle Linux as a user with <code>sudo</code> privileges.</p></li> <li>\n<p>Create the <code>docker</code> group.</p> <pre>sudo groupadd docker\n</pre>\n</li> <li>\n<p>Add your user to <code>docker</code> group.</p> <pre>sudo usermod -aG docker username\n</pre>\n</li> <li>\n<p>Log out and log back in.</p> <p>This ensures your user is running with the correct permissions.</p>\n</li> <li>\n<p>Verify your work by running <code>docker</code> without <code>sudo</code>.</p> <pre>$ docker run hello-world\n</pre> <p>If this fails with a message similar to this:</p> <pre>Cannot connect to the Docker daemon. Is 'docker daemon' running on this host?\n</pre> <p>Check that the <code>DOCKER_HOST</code> environment variable is not set for your shell. If it is, unset it.</p>\n</li> </ol> <h3 id=\"configure-docker-to-start-on-boot\">Configure Docker to start on boot</h3> <p>You can configure the Docker daemon to start automatically at boot.</p> <p>On Oracle Linux 6:</p> <pre>$ sudo chkconfig docker on\n</pre> <p>On Oracle Linux 7:</p> <pre>$ sudo systemctl enable docker.service\n</pre> <p>If you need to add an HTTP Proxy, set a different directory or partition for the Docker runtime files, or make other customizations, read our systemd article to learn how to <a href=\"../../../admin/systemd/index\">customize your systemd Docker daemon options</a>.</p> <h3 id=\"use-the-btrfs-storage-engine\">Use the btrfs storage engine</h3> <p>Docker on Oracle Linux 6 and 7 supports the use of the btrfs storage engine. Before enabling btrfs support, ensure that <code>/var/lib/docker</code> is stored on a btrfs-based filesystem. Review <a href=\"http://docs.oracle.com/cd/E37670_01/E37355/html/ol_btrfs.html\">Chapter 5</a> of the <a href=\"http://docs.oracle.com/cd/E37670_01/E37355/html/index.html\">Oracle Linux Administrator’s Solution Guide</a> for details on how to create and mount btrfs filesystems.</p> <p>To enable btrfs support on Oracle Linux:</p> <ol> <li><p>Ensure that <code>/var/lib/docker</code> is on a btrfs filesystem.</p></li> <li><p>Edit <code>/etc/sysconfig/docker</code> and add <code>-s btrfs</code> to the <code>OTHER_ARGS</code> field.</p></li> <li><p>Restart the Docker daemon:</p></li> </ol> <h2 id=\"uninstallation\">Uninstallation</h2> <p>To uninstall the Docker package:</p> <pre>$ sudo yum -y remove docker-engine\n</pre> <p>The above command will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:</p> <pre>$ rm -rf /var/lib/docker\n</pre> <p>You must delete the user created configuration files manually.</p> <h2 id=\"known-issues\">Known issues</h2> <h3 id=\"docker-unmounts-btrfs-filesystem-on-shutdown\">Docker unmounts btrfs filesystem on shutdown</h3> <p>If you’re running Docker using the btrfs storage engine and you stop the Docker service, it will unmount the btrfs filesystem during the shutdown process. You should ensure the filesystem is mounted properly prior to restarting the Docker service.</p> <p>On Oracle Linux 7, you can use a <code>systemd.mount</code> definition and modify the Docker <code>systemd.service</code> to depend on the btrfs mount defined in systemd.</p> <h3 id=\"selinux-support-on-oracle-linux-7\">SElinux support on Oracle Linux 7</h3> <p>SElinux must be set to <code>Permissive</code> or <code>Disabled</code> in <code>/etc/sysconfig/selinux</code> to use the btrfs storage engine on Oracle Linux 7.</p> <h2 id=\"further-issues\">Further issues?</h2> <p>If you have a current Basic or Premier Support Subscription for Oracle Linux, you can report any issues you have with the installation of Docker via a Service Request at <a href=\"http://support.oracle.com\">My Oracle Support</a>.</p> <p>If you do not have an Oracle Linux Support Subscription, you can use the <a href=\"https://community.oracle.com/community/server_%26_storage_systems/linux/oracle_linux\">Oracle Linux Forum</a> for community-based support.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/linux/oracle/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/linux/oracle/</a>\n  </p>\n</div>\n","engine/installation/cloud/cloud-ex-machine-ocean/index":"<h1 id=\"example-use-docker-machine-to-provision-cloud-hosts\">Example: Use Docker Machine to provision cloud hosts</h1> <p>Docker Machine driver plugins are available for many cloud platforms, so you can use Machine to provision cloud hosts. When you use Docker Machine for provisioning, you create cloud hosts with Docker Engine installed on them.</p> <p>You’ll need to install and run Docker Machine, and create an account with the cloud provider.</p> <p>Then you provide account verification, security credentials, and configuration options for the providers as flags to <code>docker-machine create</code>. The flags are unique for each cloud-specific driver. For instance, to pass a Digital Ocean access token, you use the <code>--digitalocean-access-token</code> flag.</p> <p>As an example, let’s take a look at how to create a Dockerized <a href=\"https://digitalocean.com\" target=\"_blank\">Digital Ocean</a> <em>Droplet</em> (cloud server).</p> <h3 id=\"step-1-create-a-digital-ocean-account-and-log-in\">Step 1. Create a Digital Ocean account and log in</h3> <p>If you have not done so already, go to <a href=\"https://digitalocean.com\" target=\"_blank\">Digital Ocean</a>, create an account, and log in.</p> <h3 id=\"step-2-generate-a-personal-access-token\">Step 2. Generate a personal access token</h3> <p>To generate your access token:</p> <ol> <li>\n<p>Go to the Digital Ocean administrator console and click <strong>API</strong> in the header.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ocean_click_api.png\" alt=\"Click API in Digital Ocean console\"></p>\n</li> <li>\n<p>Click <strong>Generate New Token</strong> to get to the token generator.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ocean_gen_token.png\" alt=\"Generate token\"></p>\n</li> <li>\n<p>Give the token a clever name (e.g. “machine”), make sure the <strong>Write (Optional)</strong> checkbox is checked, and click <strong>Generate Token</strong>.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ocean_token_create.png\" alt=\"Name and generate token\"></p>\n</li> <li>\n<p>Grab (copy to clipboard) the generated big long hex string and store it somewhere safe.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ocean_save_token.png\" alt=\"Copy and save personal access token\"></p> <p>This is the personal access token you’ll use in the next step to create your cloud server.</p>\n</li> </ol> <h3 id=\"step-3-install-docker-machine\">Step 3. Install Docker Machine</h3> <ol> <li>\n<p>If you have not done so already, install Docker Machine on your local host.</p> <ul> <li><p><a href=\"https://docs.docker.com/engine/installation/mac/\" target=\"_blank\"> How to install Docker Machine on Mac OS X</a></p></li> <li><p><a href=\"https://docs.docker.com/engine/installation/windows/\" target=\"_blank\">How to install Docker Machine on Windows</a></p></li> <li><p><a href=\"https://docs.docker.com/machine/install-machine/\" target=\"_blank\">Install Docker Machine directly</a> (e.g., on Linux)</p></li> </ul>\n</li> <li>\n<p>At a command terminal, use <code>docker-machine ls</code> to get a list of Docker Machines and their status.</p> <pre>$ docker-machine ls\nNAME      ACTIVE   DRIVER       STATE     URL                         SWARM\ndefault   *        virtualbox   Running   tcp:////xxx.xxx.xx.xxx:xxxx  \n</pre>\n</li> <li>\n<p>Run some Docker commands to make sure that Docker Engine is also up-and-running.</p> <p>We’ll run <code>docker run hello-world</code> again, but you could try <code>docker ps</code>, <code>docker run docker/whalesay cowsay boo</code>, or another command to verify that Docker is running.</p> <pre>$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n</pre>\n</li> </ol> <h3 id=\"step-4-use-machine-to-create-the-droplet\">Step 4. Use Machine to Create the Droplet</h3> <ol> <li>\n<p>Run <code>docker-machine create</code> with the <code>digitalocean</code> driver and pass your key to the <code>--digitalocean-access-token</code> flag, along with a name for the new cloud server.</p> <p>For this example, we’ll call our new Droplet “docker-sandbox”.</p> <pre>$ docker-machine create --driver digitalocean --digitalocean-access-token xxxxx docker-sandbox\nRunning pre-create checks...\nCreating machine...\n(docker-sandbox) OUT | Creating SSH key...\n(docker-sandbox) OUT | Creating Digital Ocean droplet...\n(docker-sandbox) OUT | Waiting for IP address to be assigned to the Droplet...\nWaiting for machine to be running, this may take a few minutes...\nMachine is running, waiting for SSH to be available...\nDetecting operating system of created instance...\nDetecting the provisioner...\nProvisioning created instance...\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\nTo see how to connect Docker to this machine, run: docker-machine env docker-sandbox\n</pre> <p>When the Droplet is created, Docker generates a unique SSH key and stores it on your local system in <code>~/.docker/machines</code>. Initially, this is used to provision the host. Later, it’s used under the hood to access the Droplet directly with the <code>docker-machine ssh</code> command. Docker Engine is installed on the cloud server and the daemon is configured to accept remote connections over TCP using TLS for authentication.</p>\n</li> <li>\n<p>Go to the Digital Ocean console to view the new Droplet.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/ocean_droplet.png\" alt=\"Droplet in Digital Ocean created with Machine\"></p>\n</li> <li>\n<p>At the command terminal, run <code>docker-machine ls</code>.</p> <pre>$ docker-machine ls\nNAME             ACTIVE   DRIVER         STATE     URL                         SWARM\ndefault          *        virtualbox     Running   tcp://192.168.99.100:2376   \ndocker-sandbox   -        digitalocean   Running   tcp://45.55.139.48:2376     \n</pre> <p>Notice that the new cloud server is running but is not the active host. Our command shell is still connected to the default machine, which is currently the active host as indicated by the asterisk (*).</p>\n</li> <li>\n<p>Run <code>docker-machine env docker-sandbox</code> to get the environment commands for the new remote host, then run <code>eval</code> as directed to re-configure the shell to connect to <code>docker-sandbox</code>.</p> <pre>$ docker-machine env docker-sandbox\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://45.55.222.72:2376\"\nexport DOCKER_CERT_PATH=\"/Users/victoriabialas/.docker/machine/machines/docker-sandbox\"\nexport DOCKER_MACHINE_NAME=\"docker-sandbox\"\n# Run this command to configure your shell:\n# eval \"$(docker-machine env docker-sandbox)\"\n\n$ eval \"$(docker-machine env docker-sandbox)\"\n</pre>\n</li> <li>\n<p>Re-run <code>docker-machine ls</code> to verify that our new server is the active machine, as indicated by the asterisk (*) in the ACTIVE column.</p> <pre>$ docker-machine ls\nNAME             ACTIVE   DRIVER         STATE     URL                         SWARM\ndefault          -        virtualbox     Running   tcp://192.168.99.100:2376   \ndocker-sandbox   *        digitalocean   Running   tcp://45.55.222.72:2376     \n</pre>\n</li> <li>\n<p>Run some <code>docker-machine</code> commands to inspect the remote host. For example, <code>docker-machine ip &lt;machine&gt;</code> gets the host IP adddress and <code>docker-machine inspect &lt;machine&gt;</code> lists all the details.</p> <pre>$ docker-machine ip docker-sandbox\n104.131.43.236\n\n$ docker-machine inspect docker-sandbox\n{\n    \"ConfigVersion\": 3,\n    \"Driver\": {\n    \"IPAddress\": \"104.131.43.236\",\n    \"MachineName\": \"docker-sandbox\",\n    \"SSHUser\": \"root\",\n    \"SSHPort\": 22,\n    \"SSHKeyPath\": \"/Users/samanthastevens/.docker/machine/machines/docker-sandbox/id_rsa\",\n    \"StorePath\": \"/Users/samanthastevens/.docker/machine\",\n    \"SwarmMaster\": false,\n    \"SwarmHost\": \"tcp://0.0.0.0:3376\",\n    \"SwarmDiscovery\": \"\",\n    ...\n</pre>\n</li> <li>\n<p>Verify Docker Engine is installed correctly by running <code>docker</code> commands.</p> <p>Start with something basic like <code>docker run hello-world</code>, or for a more interesting test, run a Dockerized webserver on your new remote machine.</p> <p>In this example, the <code>-p</code> option is used to expose port 80 from the <code>nginx</code> container and make it accessible on port <code>8000</code> of the <code>docker-sandbox</code> host.</p> <pre>$ docker run -d -p 8000:80 --name webserver kitematic/hello-world-nginx\nUnable to find image 'kitematic/hello-world-nginx:latest' locally\nlatest: Pulling from kitematic/hello-world-nginx\na285d7f063ea: Pull complete\n2d7baf27389b: Pull complete\n...\nDigest: sha256:ec0ca6dcb034916784c988b4f2432716e2e92b995ac606e080c7a54b52b87066\nStatus: Downloaded newer image for kitematic/hello-world-nginx:latest\n942dfb4a0eaae75bf26c9785ade4ff47ceb2ec2a152be82b9d7960e8b5777e65\n</pre> <p>In a web browser, go to <code>http://&lt;host_ip&gt;:8000</code> to bring up the webserver home page. You got the <code>&lt;host_ip&gt;</code> from the output of the <code>docker-machine ip &lt;machine&gt;</code> command you ran in a previous step. Use the port you exposed in the <code>docker run</code> command.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/installation/images/nginx-webserver.png\" alt=\"nginx webserver\"></p>\n</li> </ol> <h4 id=\"understand-the-defaults-and-options-on-the-create-command\">Understand the defaults and options on the create command</h4> <p>For convenience, <code>docker-machine</code> will use sensible defaults for choosing settings such as the image that the server is based on, but you override the defaults using the respective flags (e.g. <code>--digitalocean-image</code>). This is useful if, for example, you want to create a cloud server with a lot of memory and CPUs (by default <code>docker-machine</code> creates a small server). For a full list of the flags/settings available and their defaults, see the output of <code>docker-machine create -h</code> at the command line. See also <a href=\"https://docs.docker.com/machine/drivers/os-base/\" target=\"_blank\">Driver options and operating system defaults</a> and information about the <a href=\"https://docs.docker.com/machine/reference/create/\" target=\"_blank\">create</a> command in the Docker Machine documentation.</p> <h3 id=\"step-5-use-machine-to-remove-the-droplet\">Step 5. Use Machine to remove the Droplet</h3> <p>To remove a host and all of its containers and images, first stop the machine, then use <code>docker-machine rm</code>:</p> <pre>$ docker-machine stop docker-sandbox\n$ docker-machine rm docker-sandbox\nDo you really want to remove \"docker-sandbox\"? (y/n): y\nSuccessfully removed docker-sandbox\n\n$ docker-machine ls\nNAME      ACTIVE   DRIVER       STATE     URL                         SWARM\ndefault   *        virtualbox   Running   tcp:////xxx.xxx.xx.xxx:xxxx\n</pre> <p>If you monitor the Digital Ocean console while you run these commands, you will see it update first to reflect that the Droplet was stopped, and then removed.</p> <p>If you create a host with Docker Machine, but remove it through the cloud provider console, Machine will lose track of the server status. So please use the <code>docker-machine rm</code> command for hosts you create with <code>docker-machine --create</code>.</p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><p><a href=\"https://docs.docker.com/machine/drivers/\">Docker Machine driver reference</a></p></li> <li><p><a href=\"https://docs.docker.com/machine/overview/\">Docker Machine Overview</a></p></li> <li><p><a href=\"https://docs.docker.com/machine/get-started-cloud/\">Use Docker Machine to provision hosts on cloud providers</a></p></li> <li><p><a href=\"../../index\">Install Docker Engine</a></p></li> <li><p><a href=\"../../../userguide/intro/index\">Docker User Guide</a></p></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/cloud/cloud-ex-machine-ocean/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/cloud/cloud-ex-machine-ocean/</a>\n  </p>\n</div>\n","engine/userguide/containers/networkingcontainers/index":"<h1 id=\"network-containers\">Network containers</h1> <p>If you are working your way through the user guide, you just built and ran a simple application. You’ve also built in your own images. This section teaches you how to network your containers.</p> <h2 id=\"name-a-container\">Name a container</h2> <p>You’ve already seen that each container you create has an automatically created name; indeed you’ve become familiar with our old friend <code>nostalgic_morse</code> during this guide. You can also name containers yourself. This naming provides two useful functions:</p> <ul> <li><p>You can name containers that do specific functions in a way that makes it easier for you to remember them, for example naming a container containing a web application <code>web</code>.</p></li> <li><p>Names provide Docker with a reference point that allows it to refer to other containers. There are several commands that support this and you’ll use one in an exercise later.</p></li> </ul> <p>You name your container by using the <code>--name</code> flag, for example launch a new container called web:</p> <pre>$ docker run -d -P --name web training/webapp python app.py\n</pre> <p>Use the <code>docker ps</code> command to check the name:</p> <pre>$ docker ps -l\nCONTAINER ID  IMAGE                  COMMAND        CREATED       STATUS       PORTS                    NAMES\naed84ee21bde  training/webapp:latest python app.py  12 hours ago  Up 2 seconds 0.0.0.0:49154-&gt;5000/tcp  web\n</pre> <p>You can also use <code>docker inspect</code> with the container’s name.</p> <pre>$ docker inspect web\n[\n{\n    \"Id\": \"3ce51710b34f5d6da95e0a340d32aa2e6cf64857fb8cdb2a6c38f7c56f448143\",\n    \"Created\": \"2015-10-25T22:44:17.854367116Z\",\n    \"Path\": \"python\",\n    \"Args\": [\n        \"app.py\"\n    ],\n    \"State\": {\n        \"Status\": \"running\",\n        \"Running\": true,\n        \"Paused\": false,\n        \"Restarting\": false,\n        \"OOMKilled\": false,\n  ...\n</pre> <p>Container names must be unique. That means you can only call one container <code>web</code>. If you want to re-use a container name you must delete the old container (with <code>docker rm</code>) before you can reuse the name with a new container. Go ahead and stop and remove your old <code>web</code> container.</p> <pre>$ docker stop web\nweb\n$ docker rm web\nweb\n</pre> <h2 id=\"launch-a-container-on-the-default-network\">Launch a container on the default network</h2> <p>Docker includes support for networking containers through the use of <strong>network drivers</strong>. By default, Docker provides two network drivers for you, the <code>bridge</code> and the <code>overlay</code> drivers. You can also write a network driver plugin so that you can create your own drivers but that is an advanced task.</p> <p>Every installation of the Docker Engine automatically includes three default networks. You can list them:</p> <pre>$ docker network ls\nNETWORK ID          NAME                DRIVER\n18a2866682b8        none                null                \nc288470c46f6        host                host                \n7b369448dccb        bridge              bridge  \n</pre> <p>The network named <code>bridge</code> is a special network. Unless you tell it otherwise, Docker always launches your containers in this network. Try this now:</p> <pre>$ docker run -itd --name=networktest ubuntu\n74695c9cea6d9810718fddadc01a727a5dd3ce6a69d09752239736c030599741\n</pre> <p>Inspecting the network is an easy way to find out the container’s IP address.</p> <pre>$ docker network inspect bridge\n[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"f7ab26d71dbd6f557852c7156ae0574bbf62c42f539b50c8ebde0f728a253b6f\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.1/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Containers\": {\n            \"3386a527aa08b37ea9232cbcace2d2458d49f44bb05a6b775fba7ddd40d8f92c\": {\n                \"EndpointID\": \"647c12443e91faf0fd508b6edfe59c30b642abb60dfab890b4bdccee38750bc1\",\n                \"MacAddress\": \"02:42:ac:11:00:02\",\n                \"IPv4Address\": \"172.17.0.2/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"94447ca479852d29aeddca75c28f7104df3c3196d7b6d83061879e339946805c\": {\n                \"EndpointID\": \"b047d090f446ac49747d3c37d63e4307be745876db7f0ceef7b311cbba615f48\",\n                \"MacAddress\": \"02:42:ac:11:00:03\",\n                \"IPv4Address\": \"172.17.0.3/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"9001\"\n        }\n    }\n]\n</pre> <p>You can remove a container from a network by disconnecting the container. To do this, you supply both the network name and the container name. You can also use the container id. In this example, though, the name is faster.</p> <pre>$ docker network disconnect bridge networktest\n</pre> <p>While you can disconnect a container from a network, you cannot remove the builtin <code>bridge</code> network named <code>bridge</code>. Networks are natural ways to isolate containers from other containers or other networks. So, as you get more experienced with Docker, you’ll want to create your own networks.</p> <h2 id=\"create-your-own-bridge-network\">Create your own bridge network</h2> <p>Docker Engine natively supports both bridge networks and overlay networks. A bridge network is limited to a single host running Docker Engine. An overlay network can include multiple hosts and is a more advanced topic. For this example, you’ll create a bridge network:</p> <pre>$ docker network create -d bridge my-bridge-network\n</pre> <p>The <code>-d</code> flag tells Docker to use the <code>bridge</code> driver for the new network. You could have left this flag off as <code>bridge</code> is the default value for this flag. Go ahead and list the networks on your machine:</p> <pre>$ docker network ls\nNETWORK ID          NAME                DRIVER\n7b369448dccb        bridge              bridge              \n615d565d498c        my-bridge-network   bridge              \n18a2866682b8        none                null                \nc288470c46f6        host                host\n</pre> <p>If you inspect the network, you’ll find that it has nothing in it.</p> <pre>$ docker network inspect my-bridge-network\n[\n    {\n        \"Name\": \"my-bridge-network\",\n        \"Id\": \"5a8afc6364bccb199540e133e63adb76a557906dd9ff82b94183fc48c40857ac\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.18.0.0/16\",\n                    \"Gateway\": \"172.18.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {},\n        \"Options\": {}\n    }\n]\n</pre> <h2 id=\"add-containers-to-a-network\">Add containers to a network</h2> <p>To build web applications that act in concert but do so securely, create a network. Networks, by definition, provide complete isolation for containers. You can add containers to a network when you first run a container.</p> <p>Launch a container running a PostgreSQL database and pass it the <code>--net=my-bridge-network</code> flag to connect it to your new network:</p> <pre>$ docker run -d --net=my-bridge-network --name db training/postgres\n</pre> <p>If you inspect your <code>my-bridge-network</code> you’ll see it has a container attached. You can also inspect your container to see where it is connected:</p> <pre>$ docker inspect --format='{{json .NetworkSettings.Networks}}'  db\n{\"my-bridge-network\":{\"NetworkID\":\"7d86d31b1478e7cca9ebed7e73aa0fdeec46c5ca29497431d3007d2d9e15ed99\",\n\"EndpointID\":\"508b170d56b2ac9e4ef86694b0a76a22dd3df1983404f7321da5649645bf7043\",\"Gateway\":\"172.18.0.1\",\"IPAddress\":\"172.18.0.2\",\"IPPrefixLen\":16,\"IPv6Gateway\":\"\",\"GlobalIPv6Address\":\"\",\"GlobalIPv6PrefixLen\":0,\"MacAddress\":\"02:42:ac:11:00:02\"}}\n</pre> <p>Now, go ahead and start your by now familiar web application. This time leave off the <code>-P</code> flag and also don’t specify a network.</p> <pre>$ docker run -d --name web training/webapp python app.py\n</pre> <p>Which network is your <code>web</code> application running under? Inspect the application and you’ll find it is running in the default <code>bridge</code> network.</p> <pre>$ docker inspect --format='{{json .NetworkSettings.Networks}}'  web\n{\"bridge\":{\"NetworkID\":\"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n\"EndpointID\":\"508b170d56b2ac9e4ef86694b0a76a22dd3df1983404f7321da5649645bf7043\",\"Gateway\":\"172.17.0.1\",\"IPAddress\":\"172.17.0.2\",\"IPPrefixLen\":16,\"IPv6Gateway\":\"\",\"GlobalIPv6Address\":\"\",\"GlobalIPv6PrefixLen\":0,\"MacAddress\":\"02:42:ac:11:00:02\"}}\n</pre> <p>Then, get the IP address of your <code>web</code></p> <pre>$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' web\n172.17.0.2\n</pre> <p>Now, open a shell to your running <code>db</code> container:</p> <pre>$ docker exec -it db bash\nroot@a205f0dd33b2:/# ping 172.17.0.2\nping 172.17.0.2\nPING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.\n^C\n--- 172.17.0.2 ping statistics ---\n44 packets transmitted, 0 received, 100% packet loss, time 43185ms\n</pre> <p>After a bit, use <code>CTRL-C</code> to end the <code>ping</code> and you’ll find the ping failed. That is because the two containers are running on different networks. You can fix that. Then, use the <code>exit</code> command to close the container.</p> <p>Docker networking allows you to attach a container to as many networks as you like. You can also attach an already running container. Go ahead and attach your running <code>web</code> app to the <code>my-bridge-network</code>.</p> <pre>$ docker network connect my-bridge-network web\n</pre> <p>Open a shell into the <code>db</code> application again and try the ping command. This time just use the container name <code>web</code> rather than the IP Address.</p> <pre>$ docker exec -it db bash\nroot@a205f0dd33b2:/# ping web\nPING web (172.18.0.3) 56(84) bytes of data.\n64 bytes from web (172.18.0.3): icmp_seq=1 ttl=64 time=0.095 ms\n64 bytes from web (172.18.0.3): icmp_seq=2 ttl=64 time=0.060 ms\n64 bytes from web (172.18.0.3): icmp_seq=3 ttl=64 time=0.066 ms\n^C\n--- web ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2000ms\nrtt min/avg/max/mdev = 0.060/0.073/0.095/0.018 ms\n</pre> <p>The <code>ping</code> shows it is contacting a different IP address, the address on the <code>my-bridge-network</code> which is different from its address on the <code>bridge</code> network.</p> <h2 id=\"next-steps\">Next steps</h2> <p>Now that you know how to network containers, see <a href=\"../dockervolumes/index\">how to manage data in containers</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/containers/networkingcontainers/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/containers/networkingcontainers/</a>\n  </p>\n</div>\n","engine/userguide/containers/usingdocker/index":"<h1 id=\"run-a-simple-application\">Run a simple application</h1> <p>In the <a href=\"../dockerizing/index\">“<em>Hello world in a container</em>“</a> you launched your first containers using the <code>docker run</code> command. You ran an <em>interactive container</em> that ran in the foreground. You also ran a <em>detached container</em> that ran in the background. In the process you learned about several Docker commands:</p> <ul> <li>\n<code>docker ps</code> - Lists containers.</li> <li>\n<code>docker logs</code> - Shows us the standard output of a container.</li> <li>\n<code>docker stop</code> - Stops running containers.</li> </ul> <h2 id=\"learn-about-the-docker-client\">Learn about the Docker client</h2> <p>If you didn’t realize it yet, you’ve been using the Docker client each time you typed <code>docker</code> in your Bash terminal. The client is a simple command line client also known as a command-line interface (CLI). Each action you can take with the client is a command and each command can take a series of flags and arguments.</p> <pre># Usage:  [sudo] docker [subcommand] [flags] [arguments] ..\n# Example:\n$ docker run -i -t ubuntu /bin/bash\n</pre> <p>You can see this in action by using the <code>docker version</code> command to return version information on the currently installed Docker client and daemon.</p> <pre>$ docker version\n</pre> <p>This command will not only provide you the version of Docker client and daemon you are using, but also the version of Go (the programming language powering Docker).</p> <pre>Client:\n  Version:      1.8.1\n  API version:  1.20\n  Go version:   go1.4.2\n  Git commit:   d12ea79\n  Built:        Thu Aug 13 02:35:49 UTC 2015\n  OS/Arch:      linux/amd64\n\nServer:\n  Version:      1.8.1\n  API version:  1.20\n  Go version:   go1.4.2\n  Git commit:   d12ea79\n  Built:        Thu Aug 13 02:35:49 UTC 2015\n  OS/Arch:      linux/amd64\n</pre> <h2 id=\"get-docker-command-help\">Get Docker command help</h2> <p>You can display the help for specific Docker commands. The help details the options and their usage. To see a list of all the possible commands, use the following:</p> <pre>$ docker --help\n</pre> <p>To see usage for a specific command, specify the command with the <code>--help</code> flag:</p> <pre>$ docker attach --help\n\nUsage: docker attach [OPTIONS] CONTAINER\n\nAttach to a running container\n\n  --help              Print usage\n  --no-stdin          Do not attach stdin\n  --sig-proxy=true    Proxy all received signals to the process\n</pre> <blockquote> <p><strong>Note:</strong> For further details and examples of each command, see the <a href=\"../../../reference/commandline/cli/index\">command reference</a> in this guide.</p> </blockquote> <h2 id=\"running-a-web-application-in-docker\">Running a web application in Docker</h2> <p>So now you’ve learned a bit more about the <code>docker</code> client you can move onto the important stuff: running more containers. So far none of the containers you’ve run did anything particularly useful, so you can change that by running an example web application in Docker.</p> <p>For our web application we’re going to run a Python Flask application. Start with a <code>docker run</code> command.</p> <pre>$ docker run -d -P training/webapp python app.py\n</pre> <p>Review what the command did. You’ve specified two flags: <code>-d</code> and <code>-P</code>. You’ve already seen the <code>-d</code> flag which tells Docker to run the container in the background. The <code>-P</code> flag is new and tells Docker to map any required network ports inside our container to our host. This lets us view our web application.</p> <p>You’ve specified an image: <code>training/webapp</code>. This image is a pre-built image you’ve created that contains a simple Python Flask web application.</p> <p>Lastly, you’ve specified a command for our container to run: <code>python app.py</code>. This launches our web application.</p> <blockquote> <p><strong>Note:</strong> You can see more detail on the <code>docker run</code> command in the <a href=\"../../../reference/commandline/run/index\">command reference</a> and the <a href=\"../../../reference/run/index\">Docker Run Reference</a>.</p> </blockquote> <h2 id=\"viewing-our-web-application-container\">Viewing our web application container</h2> <p>Now you can see your running container using the <code>docker ps</code> command.</p> <pre>$ docker ps -l\nCONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES\nbc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse\n</pre> <p>You can see you’ve specified a new flag, <code>-l</code>, for the <code>docker ps</code> command. This tells the <code>docker ps</code> command to return the details of the <em>last</em> container started.</p> <blockquote> <p><strong>Note:</strong> By default, the <code>docker ps</code> command only shows information about running containers. If you want to see stopped containers too use the <code>-a</code> flag.</p> </blockquote> <p>We can see the same details we saw <a href=\"../dockerizing/index\">when we first Dockerized a container</a> with one important addition in the <code>PORTS</code> column.</p> <pre>PORTS\n0.0.0.0:49155-&gt;5000/tcp\n</pre> <p>When we passed the <code>-P</code> flag to the <code>docker run</code> command Docker mapped any ports exposed in our image to our host.</p> <blockquote> <p><strong>Note:</strong> We’ll learn more about how to expose ports in Docker images when <a href=\"../dockerimages/index\">we learn how to build images</a>.</p> </blockquote> <p>In this case Docker has exposed port 5000 (the default Python Flask port) on port 49155.</p> <p>Network port bindings are very configurable in Docker. In our last example the <code>-P</code> flag is a shortcut for <code>-p 5000</code> that maps port 5000 inside the container to a high port (from <em>ephemeral port range</em> which typically ranges from 32768 to 61000) on the local Docker host. We can also bind Docker containers to specific ports using the <code>-p</code> flag, for example:</p> <pre>$ docker run -d -p 80:5000 training/webapp python app.py\n</pre> <p>This would map port 5000 inside our container to port 80 on our local host. You might be asking about now: why wouldn’t we just want to always use 1:1 port mappings in Docker containers rather than mapping to high ports? Well 1:1 mappings have the constraint of only being able to map one of each port on your local host.</p> <p>Suppose you want to test two Python applications: both bound to port 5000 inside their own containers. Without Docker’s port mapping you could only access one at a time on the Docker host.</p> <p>So you can now browse to port 49155 in a web browser to see the application.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/containers/webapp1.png\" alt=\"Viewing the web application\">.</p> <p>Our Python application is live!</p> <blockquote> <p><strong>Note:</strong> If you have been using a virtual machine on OS X, Windows or Linux, you’ll need to get the IP of the virtual host instead of using localhost. You can do this by running the <code>docker-machine ip your_vm_name</code> from your command line or terminal application, for example:</p> <pre>$ docker-machine ip my-docker-vm\n192.168.99.100\n</pre> <p>In this case you’d browse to <code>http://192.168.99.100:49155</code> for the above example.</p> </blockquote> <h2 id=\"a-network-port-shortcut\">A network port shortcut</h2> <p>Using the <code>docker ps</code> command to return the mapped port is a bit clumsy so Docker has a useful shortcut we can use: <code>docker port</code>. To use <code>docker port</code> we specify the ID or name of our container and then the port for which we need the corresponding public-facing port.</p> <pre>$ docker port nostalgic_morse 5000\n0.0.0.0:49155\n</pre> <p>In this case you’ve looked up what port is mapped externally to port 5000 inside the container.</p> <h2 id=\"viewing-the-web-application-s-logs\">Viewing the web application’s logs</h2> <p>You can also find out a bit more about what’s happening with our application and use another of the commands you’ve learned, <code>docker logs</code>.</p> <pre>$ docker logs -f nostalgic_morse\n* Running on http://0.0.0.0:5000/\n10.0.2.2 - - [23/May/2014 20:16:31] \"GET / HTTP/1.1\" 200 -\n10.0.2.2 - - [23/May/2014 20:16:31] \"GET /favicon.ico HTTP/1.1\" 404 -\n</pre> <p>This time though you’ve added a new flag, <code>-f</code>. This causes the <code>docker\nlogs</code> command to act like the <code>tail -f</code> command and watch the container’s standard out. We can see here the logs from Flask showing the application running on port 5000 and the access log entries for it.</p> <h2 id=\"looking-at-our-web-application-container-s-processes\">Looking at our web application container’s processes</h2> <p>In addition to the container’s logs we can also examine the processes running inside it using the <code>docker top</code> command.</p> <pre>$ docker top nostalgic_morse\nPID                 USER                COMMAND\n854                 root                python app.py\n</pre> <p>Here we can see our <code>python app.py</code> command is the only process running inside the container.</p> <h2 id=\"inspecting-our-web-application-container\">Inspecting our web application container</h2> <p>Lastly, we can take a low-level dive into our Docker container using the <code>docker inspect</code> command. It returns a JSON document containing useful configuration and status information for the specified container.</p> <pre>$ docker inspect nostalgic_morse\n</pre> <p>You can see a sample of that JSON output.</p> <pre>[{\n    \"ID\": \"bc533791f3f500b280a9626688bc79e342e3ea0d528efe3a86a51ecb28ea20\",\n    \"Created\": \"2014-05-26T05:52:40.808952951Z\",\n    \"Path\": \"python\",\n    \"Args\": [\n       \"app.py\"\n    ],\n    \"Config\": {\n       \"Hostname\": \"bc533791f3f5\",\n       \"Domainname\": \"\",\n       \"User\": \"\",\n. . .\n</pre> <p>We can also narrow down the information we want to return by requesting a specific element, for example to return the container’s IP address we would:</p> <pre>$ docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' nostalgic_morse\n172.17.0.5\n</pre> <h2 id=\"stopping-our-web-application-container\">Stopping our web application container</h2> <p>Okay you’ve seen web application working. Now you can stop it using the <code>docker stop</code> command and the name of our container: <code>nostalgic_morse</code>.</p> <pre>$ docker stop nostalgic_morse\nnostalgic_morse\n</pre> <p>We can now use the <code>docker ps</code> command to check if the container has been stopped.</p> <pre>$ docker ps -l\n</pre> <h2 id=\"restarting-our-web-application-container\">Restarting our web application container</h2> <p>Oops! Just after you stopped the container you get a call to say another developer needs the container back. From here you have two choices: you can create a new container or restart the old one. Look at starting your previous container back up.</p> <pre>$ docker start nostalgic_morse\nnostalgic_morse\n</pre> <p>Now quickly run <code>docker ps -l</code> again to see the running container is back up or browse to the container’s URL to see if the application responds.</p> <blockquote> <p><strong>Note:</strong> Also available is the <code>docker restart</code> command that runs a stop and then start on the container.</p> </blockquote> <h2 id=\"removing-our-web-application-container\">Removing our web application container</h2> <p>Your colleague has let you know that they’ve now finished with the container and won’t need it again. Now, you can remove it using the <code>docker rm</code> command.</p> <pre>$ docker rm nostalgic_morse\nError: Impossible to remove a running container, please stop it first or use -f\n2014/05/24 08:12:56 Error: failed to remove one or more containers\n</pre> <p>What happened? We can’t actually remove a running container. This protects you from accidentally removing a running container you might need. You can try this again by stopping the container first.</p> <pre>$ docker stop nostalgic_morse\nnostalgic_morse\n$ docker rm nostalgic_morse\nnostalgic_morse\n</pre> <p>And now our container is stopped and deleted.</p> <blockquote> <p><strong>Note:</strong> Always remember that removing a container is final!</p> </blockquote> <h1 id=\"next-steps\">Next steps</h1> <p>Until now you’ve only used images that you’ve downloaded from Docker Hub. Next, you can get introduced to building and sharing our own images.</p> <p>Go to <a href=\"../dockerimages/index\">Working with Docker Images</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/containers/usingdocker/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/containers/usingdocker/</a>\n  </p>\n</div>\n","engine/userguide/containers/dockerimages/index":"<h1 id=\"build-your-own-images\">Build your own images</h1> <p>Docker images are the basis of containers. Each time you’ve used <code>docker run</code> you told it which image you wanted. In the previous sections of the guide you used Docker images that already exist, for example the <code>ubuntu</code> image and the <code>training/webapp</code> image.</p> <p>You also discovered that Docker stores downloaded images on the Docker host. If an image isn’t already present on the host then it’ll be downloaded from a registry: by default the <a href=\"https://hub.docker.com\">Docker Hub Registry</a>.</p> <p>In this section you’re going to explore Docker images a bit more including:</p> <ul> <li>Managing and working with images locally on your Docker host.</li> <li>Creating basic images.</li> <li>Uploading images to <a href=\"https://hub.docker.com\">Docker Hub Registry</a>.</li> </ul> <h2 id=\"listing-images-on-the-host\">Listing images on the host</h2> <p>Let’s start with listing the images you have locally on our host. You can do this using the <code>docker images</code> command like so:</p> <pre>$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nubuntu              14.04               1d073211c498        3 days ago          187.9 MB\nbusybox             latest              2c5ac3f849df        5 days ago          1.113 MB\ntraining/webapp     latest              54bb4e8718e8        5 months ago        348.7 MB\n</pre> <p>You can see the images you’ve previously used in the user guide. Each has been downloaded from <a href=\"https://hub.docker.com\">Docker Hub</a> when you launched a container using that image. When you list images, you get three crucial pieces of information in the listing.</p> <ul> <li>What repository they came from, for example <code>ubuntu</code>.</li> <li>The tags for each image, for example <code>14.04</code>.</li> <li>The image ID of each image.</li> </ul> <blockquote> <p><strong>Tip:</strong> You can use <a href=\"https://github.com/justone/dockviz\">a third-party dockviz tool</a> or the <a href=\"https://imagelayers.io/\">Image layers site</a> to display<br> visualizations of image data.</p> </blockquote> <p>A repository potentially holds multiple variants of an image. In the case of our <code>ubuntu</code> image you can see multiple variants covering Ubuntu 10.04, 12.04, 12.10, 13.04, 13.10 and 14.04. Each variant is identified by a tag and you can refer to a tagged image like so:</p> <pre>ubuntu:14.04\n</pre> <p>So when you run a container you refer to a tagged image like so:</p> <pre>$ docker run -t -i ubuntu:14.04 /bin/bash\n</pre> <p>If instead you wanted to run an Ubuntu 12.04 image you’d use:</p> <pre>$ docker run -t -i ubuntu:12.04 /bin/bash\n</pre> <p>If you don’t specify a variant, for example you just use <code>ubuntu</code>, then Docker will default to using the <code>ubuntu:latest</code> image.</p> <blockquote> <p><strong>Tip:</strong> You should always specify an image tag, for example <code>ubuntu:14.04</code>. That way, you always know exactly what variant of an image you are using. This is useful for troubleshooting and debugging.</p> </blockquote> <h2 id=\"getting-a-new-image\">Getting a new image</h2> <p>So how do you get new images? Well Docker will automatically download any image you use that isn’t already present on the Docker host. But this can potentially add some time to the launch of a container. If you want to pre-load an image you can download it using the <code>docker pull</code> command. Suppose you’d like to download the <code>centos</code> image.</p> <pre>$ docker pull centos\nPulling repository centos\nb7de3133ff98: Pulling dependent layers\n5cc9e91966f7: Pulling fs layer\n511136ea3c5a: Download complete\nef52fb1fe610: Download complete\n. . .\n\nStatus: Downloaded newer image for centos\n</pre> <p>You can see that each layer of the image has been pulled down and now you can run a container from this image and you won’t have to wait to download the image.</p> <pre>$ docker run -t -i centos /bin/bash\nbash-4.1#\n</pre> <h2 id=\"finding-images\">Finding images</h2> <p>One of the features of Docker is that a lot of people have created Docker images for a variety of purposes. Many of these have been uploaded to <a href=\"https://hub.docker.com\">Docker Hub</a>. You can search these images on the <a href=\"https://hub.docker.com\">Docker Hub</a> website.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/containers/search.png\" alt=\"indexsearch\"></p> <p>You can also search for images on the command line using the <code>docker search</code> command. Suppose your team wants an image with Ruby and Sinatra installed on which to do our web application development. You can search for a suitable image by using the <code>docker search</code> command to find all the images that contain the term <code>sinatra</code>.</p> <pre>$ docker search sinatra\nNAME                                   DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\ntraining/sinatra                       Sinatra training image                          0                    [OK]\nmarceldegraaf/sinatra                  Sinatra test app                                0\nmattwarren/docker-sinatra-demo                                                         0                    [OK]\nluisbebop/docker-sinatra-hello-world                                                   0                    [OK]\nbmorearty/handson-sinatra              handson-ruby + Sinatra for Hands on with D...   0\nsubwiz/sinatra                                                                         0\nbmorearty/sinatra                                                                      0\n. . .\n</pre> <p>You can see the command returns a lot of images that use the term <code>sinatra</code>. You’ve received a list of image names, descriptions, Stars (which measure the social popularity of images - if a user likes an image then they can “star” it), and the Official and Automated build statuses. <a href=\"https://docs.docker.com/docker-hub/official_repos\">Official Repositories</a> are a carefully curated set of Docker repositories supported by Docker, Inc. Automated repositories are <a href=\"../dockerrepos/index#automated-builds\">Automated Builds</a> that allow you to validate the source and content of an image.</p> <p>You’ve reviewed the images available to use and you decided to use the <code>training/sinatra</code> image. So far you’ve seen two types of images repositories, images like <code>ubuntu</code>, which are called base or root images. These base images are provided by Docker Inc and are built, validated and supported. These can be identified by their single word names.</p> <p>You’ve also seen user images, for example the <code>training/sinatra</code> image you’ve chosen. A user image belongs to a member of the Docker community and is built and maintained by them. You can identify user images as they are always prefixed with the user name, here <code>training</code>, of the user that created them.</p> <h2 id=\"pulling-our-image\">Pulling our image</h2> <p>You’ve identified a suitable image, <code>training/sinatra</code>, and now you can download it using the <code>docker pull</code> command.</p> <pre>$ docker pull training/sinatra\n</pre> <p>The team can now use this image by running their own containers.</p> <pre>$ docker run -t -i training/sinatra /bin/bash\nroot@a8cb6ce02d85:/#\n</pre> <h2 id=\"creating-our-own-images\">Creating our own images</h2> <p>The team has found the <code>training/sinatra</code> image pretty useful but it’s not quite what they need and you need to make some changes to it. There are two ways you can update and create images.</p> <ol> <li>You can update a container created from an image and commit the results to an image.</li> <li>You can use a <code>Dockerfile</code> to specify instructions to create an image.</li> </ol> <h3 id=\"updating-and-committing-an-image\">Updating and committing an image</h3> <p>To update an image you first need to create a container from the image you’d like to update.</p> <pre>$ docker run -t -i training/sinatra /bin/bash\nroot@0b2616b0e5a8:/#\n</pre> <blockquote> <p><strong>Note:</strong> Take note of the container ID that has been created, <code>0b2616b0e5a8</code>, as you’ll need it in a moment.</p> </blockquote> <p>Inside our running container let’s add the <code>json</code> gem.</p> <pre>root@0b2616b0e5a8:/# gem install json\n</pre> <p>Once this has completed let’s exit our container using the <code>exit</code> command.</p> <p>Now you have a container with the change you want to make. You can then commit a copy of this container to an image using the <code>docker commit</code> command.</p> <pre>$ docker commit -m \"Added json gem\" -a \"Kate Smith\" \\\n0b2616b0e5a8 ouruser/sinatra:v2\n4f177bd27a9ff0f6dc2a830403925b5360bfe0b93d476f7fc3231110e7f71b1c\n</pre> <p>Here you’ve used the <code>docker commit</code> command. You’ve specified two flags: <code>-m</code> and <code>-a</code>. The <code>-m</code> flag allows us to specify a commit message, much like you would with a commit on a version control system. The <code>-a</code> flag allows us to specify an author for our update.</p> <p>You’ve also specified the container you want to create this new image from, <code>0b2616b0e5a8</code> (the ID you recorded earlier) and you’ve specified a target for the image:</p> <pre>ouruser/sinatra:v2\n</pre> <p>Break this target down. It consists of a new user, <code>ouruser</code>, that you’re writing this image to. You’ve also specified the name of the image, here you’re keeping the original image name <code>sinatra</code>. Finally you’re specifying a tag for the image: <code>v2</code>.</p> <p>You can then look at our new <code>ouruser/sinatra</code> image using the <code>docker images</code> command.</p> <pre>$ docker images\nREPOSITORY          TAG     IMAGE ID       CREATED       SIZE\ntraining/sinatra    latest  5bc342fa0b91   10 hours ago  446.7 MB\nouruser/sinatra     v2      3c59e02ddd1a   10 hours ago  446.7 MB\nouruser/sinatra     latest  5db5f8471261   10 hours ago  446.7 MB\n</pre> <p>To use our new image to create a container you can then:</p> <pre>$ docker run -t -i ouruser/sinatra:v2 /bin/bash\nroot@78e82f680994:/#\n</pre> <h3 id=\"building-an-image-from-a-dockerfile\">Building an image from a <code>Dockerfile</code>\n</h3> <p>Using the <code>docker commit</code> command is a pretty simple way of extending an image but it’s a bit cumbersome and it’s not easy to share a development process for images amongst a team. Instead you can use a new command, <code>docker build</code>, to build new images from scratch.</p> <p>To do this you create a <code>Dockerfile</code> that contains a set of instructions that tell Docker how to build our image.</p> <p>First, create a directory and a <code>Dockerfile</code>.</p> <pre>$ mkdir sinatra\n$ cd sinatra\n$ touch Dockerfile\n</pre> <p>If you are using Docker Machine on Windows, you may access your host directory by <code>cd</code> to <code>/c/Users/your_user_name</code>.</p> <p>Each instruction creates a new layer of the image. Try a simple example now for building your own Sinatra image for your fictitious development team.</p> <pre># This is a comment\nFROM ubuntu:14.04\nMAINTAINER Kate Smith &lt;ksmith@example.com&gt;\nRUN apt-get update &amp;&amp; apt-get install -y ruby ruby-dev\nRUN gem install sinatra\n</pre> <p>Examine what your <code>Dockerfile</code> does. Each instruction prefixes a statement and is capitalized.</p> <pre>INSTRUCTION statement\n</pre> <blockquote> <p><strong>Note:</strong> You use <code>#</code> to indicate a comment</p> </blockquote> <p>The first instruction <code>FROM</code> tells Docker what the source of our image is, in this case you’re basing our new image on an Ubuntu 14.04 image. The instruction uses the <code>MAINTAINER</code> instruction to specify who maintains the new image.</p> <p>Lastly, you’ve specified two <code>RUN</code> instructions. A <code>RUN</code> instruction executes a command inside the image, for example installing a package. Here you’re updating our APT cache, installing Ruby and RubyGems and then installing the Sinatra gem.</p> <p>Now let’s take our <code>Dockerfile</code> and use the <code>docker build</code> command to build an image.</p> <pre>$ docker build -t ouruser/sinatra:v2 .\nSending build context to Docker daemon 2.048 kB\nSending build context to Docker daemon\nStep 1 : FROM ubuntu:14.04\n ---&gt; e54ca5efa2e9\nStep 2 : MAINTAINER Kate Smith &lt;ksmith@example.com&gt;\n ---&gt; Using cache\n ---&gt; 851baf55332b\nStep 3 : RUN apt-get update &amp;&amp; apt-get install -y ruby ruby-dev\n ---&gt; Running in 3a2558904e9b\nSelecting previously unselected package libasan0:amd64.\n(Reading database ... 11518 files and directories currently installed.)\nPreparing to unpack .../libasan0_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking libasan0:amd64 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package libatomic1:amd64.\nPreparing to unpack .../libatomic1_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking libatomic1:amd64 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package libgmp10:amd64.\nPreparing to unpack .../libgmp10_2%3a5.1.3+dfsg-1ubuntu1_amd64.deb ...\nUnpacking libgmp10:amd64 (2:5.1.3+dfsg-1ubuntu1) ...\nSelecting previously unselected package libisl10:amd64.\nPreparing to unpack .../libisl10_0.12.2-1_amd64.deb ...\nUnpacking libisl10:amd64 (0.12.2-1) ...\nSelecting previously unselected package libcloog-isl4:amd64.\nPreparing to unpack .../libcloog-isl4_0.18.2-1_amd64.deb ...\nUnpacking libcloog-isl4:amd64 (0.18.2-1) ...\nSelecting previously unselected package libgomp1:amd64.\nPreparing to unpack .../libgomp1_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking libgomp1:amd64 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package libitm1:amd64.\nPreparing to unpack .../libitm1_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking libitm1:amd64 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package libmpfr4:amd64.\nPreparing to unpack .../libmpfr4_3.1.2-1_amd64.deb ...\nUnpacking libmpfr4:amd64 (3.1.2-1) ...\nSelecting previously unselected package libquadmath0:amd64.\nPreparing to unpack .../libquadmath0_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking libquadmath0:amd64 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package libtsan0:amd64.\nPreparing to unpack .../libtsan0_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking libtsan0:amd64 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package libyaml-0-2:amd64.\nPreparing to unpack .../libyaml-0-2_0.1.4-3ubuntu3_amd64.deb ...\nUnpacking libyaml-0-2:amd64 (0.1.4-3ubuntu3) ...\nSelecting previously unselected package libmpc3:amd64.\nPreparing to unpack .../libmpc3_1.0.1-1ubuntu1_amd64.deb ...\nUnpacking libmpc3:amd64 (1.0.1-1ubuntu1) ...\nSelecting previously unselected package openssl.\nPreparing to unpack .../openssl_1.0.1f-1ubuntu2.4_amd64.deb ...\nUnpacking openssl (1.0.1f-1ubuntu2.4) ...\nSelecting previously unselected package ca-certificates.\nPreparing to unpack .../ca-certificates_20130906ubuntu2_all.deb ...\nUnpacking ca-certificates (20130906ubuntu2) ...\nSelecting previously unselected package manpages.\nPreparing to unpack .../manpages_3.54-1ubuntu1_all.deb ...\nUnpacking manpages (3.54-1ubuntu1) ...\nSelecting previously unselected package binutils.\nPreparing to unpack .../binutils_2.24-5ubuntu3_amd64.deb ...\nUnpacking binutils (2.24-5ubuntu3) ...\nSelecting previously unselected package cpp-4.8.\nPreparing to unpack .../cpp-4.8_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking cpp-4.8 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package cpp.\nPreparing to unpack .../cpp_4%3a4.8.2-1ubuntu6_amd64.deb ...\nUnpacking cpp (4:4.8.2-1ubuntu6) ...\nSelecting previously unselected package libgcc-4.8-dev:amd64.\nPreparing to unpack .../libgcc-4.8-dev_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking libgcc-4.8-dev:amd64 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package gcc-4.8.\nPreparing to unpack .../gcc-4.8_4.8.2-19ubuntu1_amd64.deb ...\nUnpacking gcc-4.8 (4.8.2-19ubuntu1) ...\nSelecting previously unselected package gcc.\nPreparing to unpack .../gcc_4%3a4.8.2-1ubuntu6_amd64.deb ...\nUnpacking gcc (4:4.8.2-1ubuntu6) ...\nSelecting previously unselected package libc-dev-bin.\nPreparing to unpack .../libc-dev-bin_2.19-0ubuntu6_amd64.deb ...\nUnpacking libc-dev-bin (2.19-0ubuntu6) ...\nSelecting previously unselected package linux-libc-dev:amd64.\nPreparing to unpack .../linux-libc-dev_3.13.0-30.55_amd64.deb ...\nUnpacking linux-libc-dev:amd64 (3.13.0-30.55) ...\nSelecting previously unselected package libc6-dev:amd64.\nPreparing to unpack .../libc6-dev_2.19-0ubuntu6_amd64.deb ...\nUnpacking libc6-dev:amd64 (2.19-0ubuntu6) ...\nSelecting previously unselected package ruby.\nPreparing to unpack .../ruby_1%3a1.9.3.4_all.deb ...\nUnpacking ruby (1:1.9.3.4) ...\nSelecting previously unselected package ruby1.9.1.\nPreparing to unpack .../ruby1.9.1_1.9.3.484-2ubuntu1_amd64.deb ...\nUnpacking ruby1.9.1 (1.9.3.484-2ubuntu1) ...\nSelecting previously unselected package libruby1.9.1.\nPreparing to unpack .../libruby1.9.1_1.9.3.484-2ubuntu1_amd64.deb ...\nUnpacking libruby1.9.1 (1.9.3.484-2ubuntu1) ...\nSelecting previously unselected package manpages-dev.\nPreparing to unpack .../manpages-dev_3.54-1ubuntu1_all.deb ...\nUnpacking manpages-dev (3.54-1ubuntu1) ...\nSelecting previously unselected package ruby1.9.1-dev.\nPreparing to unpack .../ruby1.9.1-dev_1.9.3.484-2ubuntu1_amd64.deb ...\nUnpacking ruby1.9.1-dev (1.9.3.484-2ubuntu1) ...\nSelecting previously unselected package ruby-dev.\nPreparing to unpack .../ruby-dev_1%3a1.9.3.4_all.deb ...\nUnpacking ruby-dev (1:1.9.3.4) ...\nSetting up libasan0:amd64 (4.8.2-19ubuntu1) ...\nSetting up libatomic1:amd64 (4.8.2-19ubuntu1) ...\nSetting up libgmp10:amd64 (2:5.1.3+dfsg-1ubuntu1) ...\nSetting up libisl10:amd64 (0.12.2-1) ...\nSetting up libcloog-isl4:amd64 (0.18.2-1) ...\nSetting up libgomp1:amd64 (4.8.2-19ubuntu1) ...\nSetting up libitm1:amd64 (4.8.2-19ubuntu1) ...\nSetting up libmpfr4:amd64 (3.1.2-1) ...\nSetting up libquadmath0:amd64 (4.8.2-19ubuntu1) ...\nSetting up libtsan0:amd64 (4.8.2-19ubuntu1) ...\nSetting up libyaml-0-2:amd64 (0.1.4-3ubuntu3) ...\nSetting up libmpc3:amd64 (1.0.1-1ubuntu1) ...\nSetting up openssl (1.0.1f-1ubuntu2.4) ...\nSetting up ca-certificates (20130906ubuntu2) ...\ndebconf: unable to initialize frontend: Dialog\ndebconf: (TERM is not set, so the dialog frontend is not usable.)\ndebconf: falling back to frontend: Readline\ndebconf: unable to initialize frontend: Readline\ndebconf: (This frontend requires a controlling tty.)\ndebconf: falling back to frontend: Teletype\nSetting up manpages (3.54-1ubuntu1) ...\nSetting up binutils (2.24-5ubuntu3) ...\nSetting up cpp-4.8 (4.8.2-19ubuntu1) ...\nSetting up cpp (4:4.8.2-1ubuntu6) ...\nSetting up libgcc-4.8-dev:amd64 (4.8.2-19ubuntu1) ...\nSetting up gcc-4.8 (4.8.2-19ubuntu1) ...\nSetting up gcc (4:4.8.2-1ubuntu6) ...\nSetting up libc-dev-bin (2.19-0ubuntu6) ...\nSetting up linux-libc-dev:amd64 (3.13.0-30.55) ...\nSetting up libc6-dev:amd64 (2.19-0ubuntu6) ...\nSetting up manpages-dev (3.54-1ubuntu1) ...\nSetting up libruby1.9.1 (1.9.3.484-2ubuntu1) ...\nSetting up ruby1.9.1-dev (1.9.3.484-2ubuntu1) ...\nSetting up ruby-dev (1:1.9.3.4) ...\nSetting up ruby (1:1.9.3.4) ...\nSetting up ruby1.9.1 (1.9.3.484-2ubuntu1) ...\nProcessing triggers for libc-bin (2.19-0ubuntu6) ...\nProcessing triggers for ca-certificates (20130906ubuntu2) ...\nUpdating certificates in /etc/ssl/certs... 164 added, 0 removed; done.\nRunning hooks in /etc/ca-certificates/update.d....done.\n ---&gt; c55c31703134\nRemoving intermediate container 3a2558904e9b\nStep 4 : RUN gem install sinatra\n ---&gt; Running in 6b81cb6313e5\nunable to convert \"\\xC3\" to UTF-8 in conversion from ASCII-8BIT to UTF-8 to US-ASCII for README.rdoc, skipping\nunable to convert \"\\xC3\" to UTF-8 in conversion from ASCII-8BIT to UTF-8 to US-ASCII for README.rdoc, skipping\nSuccessfully installed rack-1.5.2\nSuccessfully installed tilt-1.4.1\nSuccessfully installed rack-protection-1.5.3\nSuccessfully installed sinatra-1.4.5\n4 gems installed\nInstalling ri documentation for rack-1.5.2...\nInstalling ri documentation for tilt-1.4.1...\nInstalling ri documentation for rack-protection-1.5.3...\nInstalling ri documentation for sinatra-1.4.5...\nInstalling RDoc documentation for rack-1.5.2...\nInstalling RDoc documentation for tilt-1.4.1...\nInstalling RDoc documentation for rack-protection-1.5.3...\nInstalling RDoc documentation for sinatra-1.4.5...\n ---&gt; 97feabe5d2ed\nRemoving intermediate container 6b81cb6313e5\nSuccessfully built 97feabe5d2ed\n</pre> <p>You’ve specified our <code>docker build</code> command and used the <code>-t</code> flag to identify our new image as belonging to the user <code>ouruser</code>, the repository name <code>sinatra</code> and given it the tag <code>v2</code>.</p> <p>You’ve also specified the location of our <code>Dockerfile</code> using the <code>.</code> to indicate a <code>Dockerfile</code> in the current directory.</p> <blockquote> <p><strong>Note:</strong> You can also specify a path to a <code>Dockerfile</code>.</p> </blockquote> <p>Now you can see the build process at work. The first thing Docker does is upload the build context: basically the contents of the directory you’re building in. This is done because the Docker daemon does the actual build of the image and it needs the local context to do it.</p> <p>Next you can see each instruction in the <code>Dockerfile</code> being executed step-by-step. You can see that each step creates a new container, runs the instruction inside that container and then commits that change - just like the <code>docker commit</code> work flow you saw earlier. When all the instructions have executed you’re left with the <code>97feabe5d2ed</code> image (also helpfuly tagged as <code>ouruser/sinatra:v2</code>) and all intermediate containers will get removed to clean things up.</p> <blockquote> <p><strong>Note:</strong> An image can’t have more than 127 layers regardless of the storage driver. This limitation is set globally to encourage optimization of the overall size of images.</p> </blockquote> <p>You can then create a container from our new image.</p> <pre>$ docker run -t -i ouruser/sinatra:v2 /bin/bash\nroot@8196968dac35:/#\n</pre> <blockquote> <p><strong>Note:</strong> This is just a brief introduction to creating images. We’ve skipped a whole bunch of other instructions that you can use. We’ll see more of those instructions in later sections of the Guide or you can refer to the <a href=\"../../../reference/builder/index\"><code>Dockerfile</code></a> reference for a detailed description and examples of every instruction. To help you write a clear, readable, maintainable <code>Dockerfile</code>, we’ve also written a <a href=\"../../eng-image/dockerfile_best-practices/index\"><code>Dockerfile</code> Best Practices guide</a>.</p> </blockquote> <h2 id=\"setting-tags-on-an-image\">Setting tags on an image</h2> <p>You can also add a tag to an existing image after you commit or build it. We can do this using the <code>docker tag</code> command. Now, add a new tag to your <code>ouruser/sinatra</code> image.</p> <pre>$ docker tag 5db5f8471261 ouruser/sinatra:devel\n</pre> <p>The <code>docker tag</code> command takes the ID of the image, here <code>5db5f8471261</code>, and our user name, the repository name and the new tag.</p> <p>Now, see your new tag using the <code>docker images</code> command.</p> <pre>$ docker images ouruser/sinatra\nREPOSITORY          TAG     IMAGE ID      CREATED        SIZE\nouruser/sinatra     latest  5db5f8471261  11 hours ago   446.7 MB\nouruser/sinatra     devel   5db5f8471261  11 hours ago   446.7 MB\nouruser/sinatra     v2      5db5f8471261  11 hours ago   446.7 MB\n</pre> <h2 id=\"image-digests\">Image Digests</h2> <p>Images that use the v2 or later format have a content-addressable identifier called a <code>digest</code>. As long as the input used to generate the image is unchanged, the digest value is predictable. To list image digest values, use the <code>--digests</code> flag:</p> <pre>$ docker images --digests | head\nREPOSITORY        TAG      DIGEST                                                                     IMAGE ID      CREATED       SIZE\nouruser/sinatra   latest   sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf    5db5f8471261  11 hours ago  446.7 MB\n</pre> <p>When pushing or pulling to a 2.0 registry, the <code>push</code> or <code>pull</code> command output includes the image digest. You can <code>pull</code> using a digest value.</p> <pre>$ docker pull ouruser/sinatra@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\n</pre> <p>You can also reference by digest in <code>create</code>, <code>run</code>, and <code>rmi</code> commands, as well as the <code>FROM</code> image reference in a Dockerfile.</p> <h2 id=\"push-an-image-to-docker-hub\">Push an image to Docker Hub</h2> <p>Once you’ve built or created a new image you can push it to <a href=\"https://hub.docker.com\">Docker Hub</a> using the <code>docker push</code> command. This allows you to share it with others, either publicly, or push it into <a href=\"https://hub.docker.com/account/billing-plans/\">a private repository</a>.</p> <pre>$ docker push ouruser/sinatra\nThe push refers to a repository [ouruser/sinatra] (len: 1)\nSending image list\nPushing repository ouruser/sinatra (3 tags)\n. . .\n</pre> <h2 id=\"remove-an-image-from-the-host\">Remove an image from the host</h2> <p>You can also remove images on your Docker host in a way <a href=\"../usingdocker/index\">similar to containers</a> using the <code>docker rmi</code> command.</p> <p>Delete the <code>training/sinatra</code> image as you don’t need it anymore.</p> <pre>$ docker rmi training/sinatra\nUntagged: training/sinatra:latest\nDeleted: 5bc342fa0b91cabf65246837015197eecfa24b2213ed6a51a8974ae250fedd8d\nDeleted: ed0fffdcdae5eb2c3a55549857a8be7fc8bc4241fb19ad714364cbfd7a56b22f\nDeleted: 5c58979d73ae448df5af1d8142436d81116187a7633082650549c52c3a2418f0\n</pre> <blockquote> <p><strong>Note:</strong> To remove an image from the host, please make sure that there are no containers actively based on it.</p> </blockquote> <h1 id=\"next-steps\">Next steps</h1> <p>Until now you’ve seen how to build individual applications inside Docker containers. Now learn how to build whole application stacks with Docker by networking together multiple Docker containers.</p> <p>Go to <a href=\"../networkingcontainers/index\">Network containers</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/containers/dockerimages/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/containers/dockerimages/</a>\n  </p>\n</div>\n","engine/userguide/containers/dockerrepos/index":"<h1 id=\"store-images-on-docker-hub\">Store images on Docker Hub</h1> <p>So far you’ve learned how to use the command line to run Docker on your local host. You’ve learned how to <a href=\"../usingdocker/index\">pull down images</a> to build containers from existing images and you’ve learned how to <a href=\"../dockerimages/index\">create your own images</a>.</p> <p>Next, you’re going to learn how to use the <a href=\"https://hub.docker.com\">Docker Hub</a> to simplify and enhance your Docker workflows.</p> <p>The <a href=\"https://hub.docker.com\">Docker Hub</a> is a public registry maintained by Docker, Inc. It contains images you can download and use to build containers. It also provides authentication, work group structure, workflow tools like webhooks and build triggers, and privacy tools like private repositories for storing images you don’t want to share publicly.</p> <h2 id=\"docker-commands-and-docker-hub\">Docker commands and Docker Hub</h2> <p>Docker itself provides access to Docker Hub services via the <code>docker search</code>, <code>pull</code>, <code>login</code>, and <code>push</code> commands. This page will show you how these commands work.</p> <h3 id=\"account-creation-and-login\">Account creation and login</h3> <p>Before you try an Engine CLI command, if you haven’t already, create a Docker ID. You can do this through <a href=\"https://hub.docker.com/\">Docker Hub</a>. Once you have a Docker ID, log into your account from the Engine CLI:</p> <pre>$ docker login\n</pre> <p>The <code>login</code> command stores your Docker ID authentication credentials in the <code>$HOME/.docker/config.json</code> (Bash notation). For Windows <code>cmd</code> users the notation for this file is <code>%HOME%\\.docker\\config.json</code> ; for PowerShell users the notation is <code>$env:Home\\.docker\\config.json</code>.</p> <p>Once you have logged in from the command line, you can <code>commit</code> and <code>push</code> Engine subcommands to interact with your repos on Docker Hub.</p> <h2 id=\"searching-for-images\">Searching for images</h2> <p>You can search the <a href=\"https://hub.docker.com\">Docker Hub</a> registry via its search interface or by using the command line interface. Searching can find images by image name, user name, or description:</p> <pre>$ docker search centos\nNAME           DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\ncentos         The official build of CentOS                    1223      [OK]\ntianon/centos  CentOS 5 and 6, created using rinse instea...   33\n...\n</pre> <p>There you can see two example results: <code>centos</code> and <code>tianon/centos</code>. The second result shows that it comes from the public repository of a user, named <code>tianon/</code>, while the first result, <code>centos</code>, doesn’t explicitly list a repository which means that it comes from the trusted top-level namespace for <a href=\"https://docs.docker.com/docker-hub/official_repos/\">Official Repositories</a>. The <code>/</code> character separates a user’s repository from the image name.</p> <p>Once you’ve found the image you want, you can download it with <code>docker pull &lt;imagename&gt;</code>:</p> <pre>$ docker pull centos\nUsing default tag: latest\nlatest: Pulling from library/centos\nf1b10cd84249: Pull complete\nc852f6d61e65: Pull complete\n7322fbe74aa5: Pull complete\nDigest: sha256:90305c9112250c7e3746425477f1c4ef112b03b4abe78c612e092037bfecc3b7\nStatus: Downloaded newer image for centos:latest\n</pre> <p>You now have an image from which you can run containers.</p> <h3 id=\"specific-versions-or-latest\">Specific Versions or Latest</h3> <p>Using <code>docker pull centos</code> is equivalent to using <code>docker pull centos:latest</code>. To pull an image that is not the default latest image you can be more precise with the image that you want.</p> <p>For example, to pull version 5 of <code>centos</code> use <code>docker pull centos:centos5</code>. In this example, <code>centos5</code> is the tag labeling an image in the <code>centos</code> repository for a version of <code>centos</code>.</p> <p>To find a list of tags pointing to currently available versions of a repository see the <a href=\"https://hub.docker.com\">Docker Hub</a> registry.</p> <h2 id=\"contributing-to-docker-hub\">Contributing to Docker Hub</h2> <p>Anyone can pull public images from the <a href=\"https://hub.docker.com\">Docker Hub</a> registry, but if you would like to share your own images, then you must <a href=\"https://docs.docker.com/docker-hub/accounts\">register first</a>.</p> <h2 id=\"pushing-a-repository-to-docker-hub\">Pushing a repository to Docker Hub</h2> <p>In order to push a repository to its registry, you need to have named an image or committed your container to a named image as we saw <a href=\"../dockerimages/index\">here</a>.</p> <p>Now you can push this repository to the registry designated by its name or tag.</p> <pre>$ docker push yourname/newimage\n</pre> <p>The image will then be uploaded and available for use by your team-mates and/or the community.</p> <h2 id=\"features-of-docker-hub\">Features of Docker Hub</h2> <p>Let’s take a closer look at some of the features of Docker Hub. You can find more information <a href=\"https://docs.docker.com/docker-hub/\">here</a>.</p> <ul> <li>Private repositories</li> <li>Organizations and teams</li> <li>Automated Builds</li> <li>Webhooks</li> </ul> <h3 id=\"private-repositories\">Private repositories</h3> <p>Sometimes you have images you don’t want to make public and share with everyone. So Docker Hub allows you to have private repositories. You can sign up for a plan <a href=\"https://hub.docker.com/account/billing-plans/\">here</a>.</p> <h3 id=\"organizations-and-teams\">Organizations and teams</h3> <p>One of the useful aspects of private repositories is that you can share them only with members of your organization or team. Docker Hub lets you create organizations where you can collaborate with your colleagues and manage private repositories. You can learn how to create and manage an organization <a href=\"https://hub.docker.com/organizations/\">here</a>.</p> <h3 id=\"automated-builds\">Automated Builds</h3> <p>Automated Builds automate the building and updating of images from <a href=\"https://www.github.com\">GitHub</a> or <a href=\"http://bitbucket.com\">Bitbucket</a>, directly on Docker Hub. It works by adding a commit hook to your selected GitHub or Bitbucket repository, triggering a build and update when you push a commit.</p> <h4 id=\"to-setup-an-automated-build\">To setup an Automated Build</h4> <ol> <li>Create a <a href=\"https://hub.docker.com/\">Docker Hub account</a> and login.</li> <li>Link your GitHub or Bitbucket account on the <a href=\"https://hub.docker.com/account/authorized-services/\">“Linked Accounts &amp; Services”</a> page.</li> <li>Select “Create Automated Build” from the “Create” dropdown menu</li> <li>Pick a GitHub or Bitbucket project that has a <code>Dockerfile</code> that you want to build.</li> <li>Pick the branch you want to build (the default is the <code>master</code> branch).</li> <li>Give the Automated Build a name.</li> <li>Assign an optional Docker tag to the Build.</li> <li>Specify where the <code>Dockerfile</code> is located. The default is <code>/</code>.</li> </ol> <p>Once the Automated Build is configured it will automatically trigger a build and, in a few minutes, you should see your new Automated Build on the <a href=\"https://hub.docker.com\">Docker Hub</a> Registry. It will stay in sync with your GitHub and Bitbucket repository until you deactivate the Automated Build.</p> <p>To check the output and status of your Automated Build repositories, click on a repository name within the <a href=\"https://registry.hub.docker.com/repos/\">“Your Repositories” page</a>. Automated Builds are indicated by a check-mark icon next to the repository name. Within the repository details page, you may click on the “Build Details” tab to view the status and output of all builds triggered by the Docker Hub.</p> <p>Once you’ve created an Automated Build you can deactivate or delete it. You cannot, however, push to an Automated Build with the <code>docker push</code> command. You can only manage it by committing code to your GitHub or Bitbucket repository.</p> <p>You can create multiple Automated Builds per repository and configure them to point to specific <code>Dockerfile</code>’s or Git branches.</p> <h4 id=\"build-triggers\">Build triggers</h4> <p>Automated Builds can also be triggered via a URL on Docker Hub. This allows you to rebuild an Automated build image on demand.</p> <h3 id=\"webhooks\">Webhooks</h3> <p>Webhooks are attached to your repositories and allow you to trigger an event when an image or updated image is pushed to the repository. With a webhook you can specify a target URL and a JSON payload that will be delivered when the image is pushed.</p> <p>See the Docker Hub documentation for <a href=\"https://docs.docker.com/docker-hub/repos/#webhooks\">more information on webhooks</a></p> <h2 id=\"next-steps\">Next steps</h2> <p>Go and use Docker!</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/containers/dockerrepos/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/containers/dockerrepos/</a>\n  </p>\n</div>\n","engine/userguide/eng-image/baseimages/index":"<h1 id=\"create-a-base-image\">Create a base image</h1> <p>So you want to create your own <a href=\"../../../reference/glossary/index#base-image\"><em>Base Image</em></a>? Great!</p> <p>The specific process will depend heavily on the Linux distribution you want to package. We have some examples below, and you are encouraged to submit pull requests to contribute new ones.</p> <h2 id=\"create-a-full-image-using-tar\">Create a full image using tar</h2> <p>In general, you’ll want to start with a working machine that is running the distribution you’d like to package as a base image, though that is not required for some tools like Debian’s <a href=\"https://wiki.debian.org/Debootstrap\">Debootstrap</a>, which you can also use to build Ubuntu images.</p> <p>It can be as simple as this to create an Ubuntu base image:</p> <pre>$ sudo debootstrap raring raring &gt; /dev/null\n$ sudo tar -C raring -c . | docker import - raring\na29c15f1bf7a\n$ docker run raring cat /etc/lsb-release\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=13.04\nDISTRIB_CODENAME=raring\nDISTRIB_DESCRIPTION=\"Ubuntu 13.04\"\n</pre> <p>There are more example scripts for creating base images in the Docker GitHub Repo:</p> <ul> <li><a href=\"https://github.com/docker/docker/blob/master/contrib/mkimage-busybox.sh\">BusyBox</a></li> <li>CentOS / Scientific Linux CERN (SLC) <a href=\"https://github.com/docker/docker/blob/master/contrib/mkimage-rinse.sh\">on Debian/Ubuntu</a> or <a href=\"https://github.com/docker/docker/blob/master/contrib/mkimage-yum.sh\">on CentOS/RHEL/SLC/etc.</a>\n</li> <li><a href=\"https://github.com/docker/docker/blob/master/contrib/mkimage-debootstrap.sh\">Debian / Ubuntu</a></li> </ul> <h2 id=\"creating-a-simple-base-image-using-scratch\">Creating a simple base image using scratch</h2> <p>You can use Docker’s reserved, minimal image, <code>scratch</code>, as a starting point for building containers. Using the <code>scratch</code> “image” signals to the build process that you want the next command in the <code>Dockerfile</code> to be the first filesystem layer in your image.</p> <p>While <code>scratch</code> appears in Docker’s repository on the hub, you can’t pull it, run it, or tag any image with the name <code>scratch</code>. Instead, you can refer to it in your <code>Dockerfile</code>. For example, to create a minimal container using <code>scratch</code>:</p> <pre>FROM scratch\nADD hello /\nCMD [\"/hello\"]\n</pre> <p>This example creates the hello-world image used in the tutorials. If you want to test it out, you can clone <a href=\"https://github.com/docker-library/hello-world\">the image repo</a></p> <h2 id=\"more-resources\">More resources</h2> <p>There are lots more resources available to help you write your ‘Dockerfile`.</p> <ul> <li>There’s a <a href=\"../../../reference/builder/index\">complete guide to all the instructions</a> available for use in a <code>Dockerfile</code> in the reference section.</li> <li>To help you write a clear, readable, maintainable <code>Dockerfile</code>, we’ve also written a <a href=\"../dockerfile_best-practices/index\"><code>Dockerfile</code> Best Practices guide</a>.</li> <li>If your goal is to create a new Official Repository, be sure to read up on Docker’s <a href=\"https://docs.docker.com/docker-hub/official_repos/\">Official Repositories</a>.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/eng-image/baseimages/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/eng-image/baseimages/</a>\n  </p>\n</div>\n","engine/userguide/containers/dockervolumes/index":"<h1 id=\"manage-data-in-containers\">Manage data in containers</h1> <p>So far you’ve been introduced to some <a href=\"../usingdocker/index\">basic Docker concepts</a>, seen how to work with <a href=\"../dockerimages/index\">Docker images</a> as well as learned about <a href=\"../../networking/default_network/dockerlinks/index\">networking and links between containers</a>. In this section you’re going to learn how you can manage data inside and between your Docker containers.</p> <p>You’re going to look at the two primary ways you can manage data with Docker Engine.</p> <ul> <li>Data volumes</li> <li>Data volume containers</li> </ul> <h2 id=\"data-volumes\">Data volumes</h2> <p>A <em>data volume</em> is a specially-designated directory within one or more containers that bypasses the <a href=\"../../../reference/glossary/index#union-file-system\"><em>Union File System</em></a>. Data volumes provide several useful features for persistent or shared data:</p> <ul> <li>Volumes are initialized when a container is created. If the container’s base image contains data at the specified mount point, that existing data is copied into the new volume upon volume initialization. (Note that this does not apply when <a href=\"#mount-a-host-directory-as-a-data-volume\">mounting a host directory</a>.)</li> <li>Data volumes can be shared and reused among containers.</li> <li>Changes to a data volume are made directly.</li> <li>Changes to a data volume will not be included when you update an image.</li> <li>Data volumes persist even if the container itself is deleted.</li> </ul> <p>Data volumes are designed to persist data, independent of the container’s life cycle. Docker therefore <em>never</em> automatically deletes volumes when you remove a container, nor will it “garbage collect” volumes that are no longer referenced by a container.</p> <h3 id=\"adding-a-data-volume\">Adding a data volume</h3> <p>You can add a data volume to a container using the <code>-v</code> flag with the <code>docker create</code> and <code>docker run</code> command. You can use the <code>-v</code> multiple times to mount multiple data volumes. Now, mount a single volume in your web application container.</p> <pre>$ docker run -d -P --name web -v /webapp training/webapp python app.py\n</pre> <p>This will create a new volume inside a container at <code>/webapp</code>.</p> <blockquote> <p><strong>Note:</strong> You can also use the <code>VOLUME</code> instruction in a <code>Dockerfile</code> to add one or more new volumes to any container created from that image.</p> </blockquote> <h3 id=\"locating-a-volume\">Locating a volume</h3> <p>You can locate the volume on the host by utilizing the <code>docker inspect</code> command.</p> <pre>$ docker inspect web\n</pre> <p>The output will provide details on the container configurations including the volumes. The output should look something similar to the following:</p> <pre>...\n\"Mounts\": [\n    {\n        \"Name\": \"fac362...80535\",\n        \"Source\": \"/var/lib/docker/volumes/fac362...80535/_data\",\n        \"Destination\": \"/webapp\",\n        \"Driver\": \"local\",\n        \"Mode\": \"\",\n        \"RW\": true,\n        \"Propagation\": \"\"\n    }\n]\n...\n</pre> <p>You will notice in the above <code>Source</code> is specifying the location on the host and <code>Destination</code> is specifying the volume location inside the container. <code>RW</code> shows if the volume is read/write.</p> <h3 id=\"mount-a-host-directory-as-a-data-volume\">Mount a host directory as a data volume</h3> <p>In addition to creating a volume using the <code>-v</code> flag you can also mount a directory from your Engine daemon’s host into a container.</p> <pre>$ docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n</pre> <p>This command mounts the host directory, <code>/src/webapp</code>, into the container at <code>/opt/webapp</code>. If the path <code>/opt/webapp</code> already exists inside the container’s image, the <code>/src/webapp</code> mount overlays but does not remove the pre-existing content. Once the mount is removed, the content is accessible again. This is consistent with the expected behavior of the <code>mount</code> command.</p> <p>The <code>container-dir</code> must always be an absolute path such as <code>/src/docs</code>. The <code>host-dir</code> can either be an absolute path or a <code>name</code> value. If you supply an absolute path for the <code>host-dir</code>, Docker bind-mounts to the path you specify. If you supply a <code>name</code>, Docker creates a named volume by that <code>name</code>.</p> <p>A <code>name</code> value must start with an alphanumeric character, followed by <code>a-z0-9</code>, <code>_</code> (underscore), <code>.</code> (period) or <code>-</code> (hyphen). An absolute path starts with a <code>/</code> (forward slash).</p> <p>For example, you can specify either <code>/foo</code> or <code>foo</code> for a <code>host-dir</code> value. If you supply the <code>/foo</code> value, Engine creates a bind-mount. If you supply the <code>foo</code> specification, Engine creates a named volume.</p> <p>If you are using Docker Machine on Mac or Windows, your Engine daemon has only limited access to your OS X or Windows filesystem. Docker Machine tries to auto-share your <code>/Users</code> (OS X) or <code>C:\\Users</code> (Windows) directory. So, you can mount files or directories on OS X using.</p> <pre>docker run -v /Users/&lt;path&gt;:/&lt;container path&gt; ...\n</pre> <p>On Windows, mount directories using:</p> <pre>docker run -v /c/Users/&lt;path&gt;:/&lt;container path&gt; ...`\n</pre> <p>All other paths come from your virtual machine’s filesystem, so if you want to make some other host folder available for sharing, you need to do additional work. In the case of VirtualBox you need to make the host folder available as a shared folder in VirtualBox. Then, you can mount it using the Docker <code>-v</code> flag.</p> <p>Mounting a host directory can be useful for testing. For example, you can mount source code inside a container. Then, change the source code and see its effect on the application in real time. The directory on the host must be specified as an absolute path and if the directory doesn’t exist the Engine daemon automatically creates it for you.</p> <p>Docker volumes default to mount in read-write mode, but you can also set it to be mounted read-only.</p> <pre>$ docker run -d -P --name web -v /src/webapp:/opt/webapp:ro training/webapp python app.py\n</pre> <p>Here you’ve mounted the same <code>/src/webapp</code> directory but you’ve added the <code>ro</code> option to specify that the mount should be read-only.</p> <p>Because of <a href=\"http://lists.linuxfoundation.org/pipermail/containers/2015-April/035788.html\">limitations in the <code>mount</code> function</a>, moving subdirectories within the host’s source directory can give access from the container to the host’s file system. This requires a malicious user with access to host and its mounted directory.</p> <blockquote> <p><strong>Note</strong>: The host directory is, by its nature, host-dependent. For this reason, you can’t mount a host directory from <code>Dockerfile</code> because built images should be portable. A host directory wouldn’t be available on all potential hosts.</p> </blockquote> <h3 id=\"mount-a-shared-storage-volume-as-a-data-volume\">Mount a shared-storage volume as a data volume</h3> <p>In addition to mounting a host directory in your container, some Docker <a href=\"../../../extend/plugins_volume/index\">volume plugins</a> allow you to provision and mount shared storage, such as iSCSI, NFS, or FC.</p> <p>A benefit of using shared volumes is that they are host-independent. This means that a volume can be made available on any host that a container is started on as long as it has access to the shared storage backend, and has the plugin installed.</p> <p>One way to use volume drivers is through the <code>docker run</code> command. Volume drivers create volumes by name, instead of by path like in the other examples.</p> <p>The following command creates a named volume, called <code>my-named-volume</code>, using the <code>flocker</code> volume driver, and makes it available within the container at <code>/opt/webapp</code>:</p> <pre>$ docker run -d -P \\\n  --volume-driver=flocker \\\n  -v my-named-volume:/opt/webapp \\\n  --name web training/webapp python app.py\n</pre> <p>You may also use the <code>docker volume create</code> command, to create a volume before using it in a container.</p> <p>The following example also creates the <code>my-named-volume</code> volume, this time using the <code>docker volume create</code> command.</p> <pre>$ docker volume create -d flocker --name my-named-volume -o size=20GB\n$ docker run -d -P \\\n  -v my-named-volume:/opt/webapp \\\n  --name web training/webapp python app.py\n</pre> <p>A list of available plugins, including volume plugins, is available <a href=\"../../../extend/plugins/index\">here</a>.</p> <h3 id=\"volume-labels\">Volume labels</h3> <p>Labeling systems like SELinux require that proper labels are placed on volume content mounted into a container. Without a label, the security system might prevent the processes running inside the container from using the content. By default, Docker does not change the labels set by the OS.</p> <p>To change a label in the container context, you can add either of two suffixes <code>:z</code> or <code>:Z</code> to the volume mount. These suffixes tell Docker to relabel file objects on the shared volumes. The <code>z</code> option tells Docker that two containers share the volume content. As a result, Docker labels the content with a shared content label. Shared volume labels allow all containers to read/write content. The <code>Z</code> option tells Docker to label the content with a private unshared label. Only the current container can use a private volume.</p> <h3 id=\"mount-a-host-file-as-a-data-volume\">Mount a host file as a data volume</h3> <p>The <code>-v</code> flag can also be used to mount a single file - instead of <em>just</em> directories - from the host machine.</p> <pre>$ docker run --rm -it -v ~/.bash_history:/root/.bash_history ubuntu /bin/bash\n</pre> <p>This will drop you into a bash shell in a new container, you will have your bash history from the host and when you exit the container, the host will have the history of the commands typed while in the container.</p> <blockquote> <p><strong>Note:</strong> Many tools used to edit files including <code>vi</code> and <code>sed --in-place</code> may result in an inode change. Since Docker v1.1.0, this will produce an error such as “<em>sed: cannot rename ./sedKdJ9Dy: Device or resource busy</em>”. In the case where you want to edit the mounted file, it is often easiest to instead mount the parent directory.</p> </blockquote> <h2 id=\"creating-and-mounting-a-data-volume-container\">Creating and mounting a data volume container</h2> <p>If you have some persistent data that you want to share between containers, or want to use from non-persistent containers, it’s best to create a named Data Volume Container, and then to mount the data from it.</p> <p>Let’s create a new named container with a volume to share. While this container doesn’t run an application, it reuses the <code>training/postgres</code> image so that all containers are using layers in common, saving disk space.</p> <pre>$ docker create -v /dbdata --name dbstore training/postgres /bin/true\n</pre> <p>You can then use the <code>--volumes-from</code> flag to mount the <code>/dbdata</code> volume in another container.</p> <pre>$ docker run -d --volumes-from dbstore --name db1 training/postgres\n</pre> <p>And another:</p> <pre>$ docker run -d --volumes-from dbstore --name db2 training/postgres\n</pre> <p>In this case, if the <code>postgres</code> image contained a directory called <code>/dbdata</code> then mounting the volumes from the <code>dbstore</code> container hides the <code>/dbdata</code> files from the <code>postgres</code> image. The result is only the files from the <code>dbstore</code> container are visible.</p> <p>You can use multiple <code>--volumes-from</code> parameters to combine data volumes from several containers. To find detailed information about <code>--volumes-from</code> see the <a href=\"../../../reference/commandline/run/index#mount-volumes-from-container-volumes-from\">Mount volumes from container</a> in the <code>run</code> command reference.</p> <p>You can also extend the chain by mounting the volume that came from the <code>dbstore</code> container in yet another container via the <code>db1</code> or <code>db2</code> containers.</p> <pre>$ docker run -d --name db3 --volumes-from db1 training/postgres\n</pre> <p>If you remove containers that mount volumes, including the initial <code>dbstore</code> container, or the subsequent containers <code>db1</code> and <code>db2</code>, the volumes will not be deleted. To delete the volume from disk, you must explicitly call <code>docker rm -v</code> against the last container with a reference to the volume. This allows you to upgrade, or effectively migrate data volumes between containers.</p> <blockquote> <p><strong>Note:</strong> Docker will not warn you when removing a container <em>without</em> providing the <code>-v</code> option to delete its volumes. If you remove containers without using the <code>-v</code> option, you may end up with “dangling” volumes; volumes that are no longer referenced by a container. You can use <code>docker volume ls -f dangling=true</code> to find dangling volumes, and use <code>docker volume rm &lt;volume name&gt;</code> to remove a volume that’s no longer needed.</p> </blockquote> <h2 id=\"backup-restore-or-migrate-data-volumes\">Backup, restore, or migrate data volumes</h2> <p>Another useful function we can perform with volumes is use them for backups, restores or migrations. You do this by using the <code>--volumes-from</code> flag to create a new container that mounts that volume, like so:</p> <pre>$ docker run --rm --volumes-from dbstore -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata\n</pre> <p>Here you’ve launched a new container and mounted the volume from the <code>dbstore</code> container. You’ve then mounted a local host directory as <code>/backup</code>. Finally, you’ve passed a command that uses <code>tar</code> to backup the contents of the <code>dbdata</code> volume to a <code>backup.tar</code> file inside our <code>/backup</code> directory. When the command completes and the container stops we’ll be left with a backup of our <code>dbdata</code> volume.</p> <p>You could then restore it to the same container, or another that you’ve made elsewhere. Create a new container.</p> <pre>$ docker run -v /dbdata --name dbstore2 ubuntu /bin/bash\n</pre> <p>Then un-tar the backup file in the new container`s data volume.</p> <pre>$ docker run --rm --volumes-from dbstore2 -v $(pwd):/backup ubuntu bash -c \"cd /dbdata &amp;&amp; tar xvf /backup/backup.tar --strip 1\"\n</pre> <p>You can use the techniques above to automate backup, migration and restore testing using your preferred tools.</p> <h2 id=\"removing-volumes\">Removing volumes</h2> <p>A Docker data volume persists after a container is deleted. You can create named or anonymous volumes. Named volumes have a specific source form outside the container, for example <code>awesome:/bar</code>. Anonymous volumes have no specific source. When the container is deleted, you should instruction the Engine daemon to clean up anonymous volumes. To do this, use the <code>--rm</code> option, for example:</p> <pre>$ docker run --rm -v /foo -v awesome:/bar busybox top\n</pre> <p>This command creates an anonymous <code>/foo</code> volume. When the container is removed, Engine removes the <code>/foo</code> volume but not the <code>awesome</code> volume.</p> <h2 id=\"important-tips-on-using-shared-volumes\">Important tips on using shared volumes</h2> <p>Multiple containers can also share one or more data volumes. However, multiple containers writing to a single shared volume can cause data corruption. Make sure your applications are designed to write to shared data stores.</p> <p>Data volumes are directly accessible from the Docker host. This means you can read and write to them with normal Linux tools. In most cases you should not do this as it can cause data corruption if your containers and applications are unaware of your direct access.</p> <h1 id=\"next-steps\">Next steps</h1> <p>Now you’ve learned a bit more about how to use Docker we’re going to see how to combine Docker with the services available on <a href=\"https://hub.docker.com\">Docker Hub</a> including Automated Builds and private repositories.</p> <p>Go to <a href=\"../dockerrepos/index\">Working with Docker Hub</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/containers/dockervolumes/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/containers/dockervolumes/</a>\n  </p>\n</div>\n","engine/userguide/eng-image/dockerfile_best-practices/index":"<h1 id=\"best-practices-for-writing-dockerfiles\">Best practices for writing Dockerfiles</h1> <p>Docker can build images automatically by reading the instructions from a <code>Dockerfile</code>, a text file that contains all the commands, in order, needed to build a given image. <code>Dockerfile</code>s adhere to a specific format and use a specific set of instructions. You can learn the basics on the <a href=\"../../../reference/builder/index\">Dockerfile Reference</a> page. If you’re new to writing <code>Dockerfile</code>s, you should start there.</p> <p>This document covers the best practices and methods recommended by Docker, Inc. and the Docker community for creating easy-to-use, effective <code>Dockerfile</code>s. We strongly suggest you follow these recommendations (in fact, if you’re creating an Official Image, you <em>must</em> adhere to these practices).</p> <p>You can see many of these practices and recommendations in action in the <a href=\"https://github.com/docker-library/buildpack-deps/blob/master/jessie/Dockerfile\">buildpack-deps <code>Dockerfile</code></a>.</p> <blockquote> <p>Note: for more detailed explanations of any of the Dockerfile commands mentioned here, visit the <a href=\"../../../reference/builder/index\">Dockerfile Reference</a> page.</p> </blockquote> <h2 id=\"general-guidelines-and-recommendations\">General guidelines and recommendations</h2> <h3 id=\"containers-should-be-ephemeral\">Containers should be ephemeral</h3> <p>The container produced by the image your <code>Dockerfile</code> defines should be as ephemeral as possible. By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.</p> <h3 id=\"use-a-dockerignore-file\">Use a .dockerignore file</h3> <p>In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a <code>.dockerignore</code> file to that directory as well. This file supports exclusion patterns similar to <code>.gitignore</code> files. For information on creating one, see the <a href=\"../../../reference/builder/index#dockerignore-file\">.dockerignore file</a>.</p> <h3 id=\"avoid-installing-unnecessary-packages\">Avoid installing unnecessary packages</h3> <p>In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.</p> <h3 id=\"run-only-one-process-per-container\">Run only one process per container</h3> <p>In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of <a href=\"../../networking/default_network/dockerlinks/index\">container linking</a>.</p> <h3 id=\"minimize-the-number-of-layers\">Minimize the number of layers</h3> <p>You need to find the balance between readability (and thus long-term maintainability) of the <code>Dockerfile</code> and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.</p> <h3 id=\"sort-multi-line-arguments\">Sort multi-line arguments</h3> <p>Whenever possible, ease later changes by sorting multi-line arguments alphanumerically. This will help you avoid duplication of packages and make the list much easier to update. This also makes PRs a lot easier to read and review. Adding a space before a backslash (<code>\\</code>) helps as well.</p> <p>Here’s an example from the <a href=\"https://github.com/docker-library/buildpack-deps\"><code>buildpack-deps</code> image</a>:</p> <pre>RUN apt-get update &amp;&amp; apt-get install -y \\\n  bzr \\\n  cvs \\\n  git \\\n  mercurial \\\n  subversion\n</pre> <h3 id=\"build-cache\">Build cache</h3> <p>During the process of building an image Docker will step through the instructions in your <code>Dockerfile</code> executing each in the order specified. As each instruction is examined Docker will look for an existing image in its cache that it can reuse, rather than creating a new (duplicate) image. If you do not want to use the cache at all you can use the <code>--no-cache=true</code> option on the <code>docker build</code> command.</p> <p>However, if you do let Docker use its cache then it is very important to understand when it will, and will not, find a matching image. The basic rules that Docker will follow are outlined below:</p> <ul> <li><p>Starting with a base image that is already in the cache, the next instruction is compared against all child images derived from that base image to see if one of them was built using the exact same instruction. If not, the cache is invalidated.</p></li> <li><p>In most cases simply comparing the instruction in the <code>Dockerfile</code> with one of the child images is sufficient. However, certain instructions require a little more examination and explanation.</p></li> <li><p>For the <code>ADD</code> and <code>COPY</code> instructions, the contents of the file(s) in the image are examined and a checksum is calculated for each file. The last-modified and last-accessed times of the file(s) are not considered in these checksums. During the cache lookup, the checksum is compared against the checksum in the existing images. If anything has changed in the file(s), such as the contents and metadata, then the cache is invalidated.</p></li> <li><p>Aside from the <code>ADD</code> and <code>COPY</code> commands, cache checking will not look at the files in the container to determine a cache match. For example, when processing a <code>RUN apt-get -y update</code> command the files updated in the container will not be examined to determine if a cache hit exists. In that case just the command string itself will be used to find a match.</p></li> </ul> <p>Once the cache is invalidated, all subsequent <code>Dockerfile</code> commands will generate new images and the cache will not be used.</p> <h2 id=\"the-dockerfile-instructions\">The Dockerfile instructions</h2> <p>Below you’ll find recommendations for the best way to write the various instructions available for use in a <code>Dockerfile</code>.</p> <h3 id=\"from\">FROM</h3> <p><a href=\"../../../reference/builder/index#from\">Dockerfile reference for the FROM instruction</a></p> <p>Whenever possible, use current Official Repositories as the basis for your image. We recommend the <a href=\"https://hub.docker.com/_/debian/\">Debian image</a> since it’s very tightly controlled and kept minimal (currently under 150 mb), while still being a full distribution.</p> <h3 id=\"run\">RUN</h3> <p><a href=\"../../../reference/builder/index#run\">Dockerfile reference for the RUN instruction</a></p> <p>As always, to make your <code>Dockerfile</code> more readable, understandable, and maintainable, split long or complex <code>RUN</code> statements on multiple lines separated with backslashes.</p> <h3 id=\"apt-get\">apt-get</h3> <p>Probably the most common use-case for <code>RUN</code> is an application of <code>apt-get</code>. The <code>RUN apt-get</code> command, because it installs packages, has several gotchas to look out for.</p> <p>You should avoid <code>RUN apt-get upgrade</code> or <code>dist-upgrade</code>, as many of the “essential” packages from the base images won’t upgrade inside an unprivileged container. If a package contained in the base image is out-of-date, you should contact its maintainers. If you know there’s a particular package, <code>foo</code>, that needs to be updated, use <code>apt-get install -y foo</code> to update automatically.</p> <p>Always combine <code>RUN apt-get update</code> with <code>apt-get install</code> in the same <code>RUN</code> statement, for example:</p> <pre>    RUN apt-get update &amp;&amp; apt-get install -y \\\n        package-bar \\\n        package-baz \\\n        package-foo\n</pre> <p>Using <code>apt-get update</code> alone in a <code>RUN</code> statement causes caching issues and subsequent <code>apt-get install</code> instructions fail. For example, say you have a Dockerfile:</p> <pre>    FROM ubuntu:14.04\n    RUN apt-get update\n    RUN apt-get install -y curl\n</pre> <p>After building the image, all layers are in the Docker cache. Suppose you later modify <code>apt-get install</code> by adding extra package:</p> <pre>    FROM ubuntu:14.04\n    RUN apt-get update\n    RUN apt-get install -y curl nginx\n</pre> <p>Docker sees the initial and modified instructions as identical and reuses the cache from previous steps. As a result the <code>apt-get update</code> is <em>NOT</em> executed because the build uses the cached version. Because the <code>apt-get update</code> is not run, your build can potentially get an outdated version of the <code>curl</code> and <code>nginx</code> packages.</p> <p>Using <code>RUN apt-get update &amp;&amp; apt-get install -y</code> ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as “cache busting”. You can also achieve cache-busting by specifying a package version. This is known as version pinning, for example:</p> <pre>    RUN apt-get update &amp;&amp; apt-get install -y \\\n        package-bar \\\n        package-baz \\\n        package-foo=1.3.*\n</pre> <p>Version pinning forces the build to retrieve a particular version regardless of what’s in the cache. This technique can also reduce failures due to unanticipated changes in required packages.</p> <p>Below is a well-formed <code>RUN</code> instruction that demonstrates all the <code>apt-get</code> recommendations.</p> <pre>RUN apt-get update &amp;&amp; apt-get install -y \\\n    aufs-tools \\\n    automake \\\n    build-essential \\\n    curl \\\n    dpkg-sig \\\n    libcap-dev \\\n    libsqlite3-dev \\\n    mercurial \\\n    reprepro \\\n    ruby1.9.1 \\\n    ruby1.9.1-dev \\\n    s3cmd=1.1.* \\\n &amp;&amp; rm -rf /var/lib/apt/lists/*\n</pre> <p>The <code>s3cmd</code> instructions specifies a version <code>1.1.0*</code>. If the image previously used an older version, specifying the new one causes a cache bust of <code>apt-get\nupdate</code> and ensure the installation of the new version. Listing packages on each line can also prevent mistakes in package duplication.</p> <p>In addition, cleaning up the apt cache and removing <code>/var/lib/apt/lists</code> helps keep the image size down. Since the <code>RUN</code> statement starts with <code>apt-get update</code>, the package cache will always be refreshed prior to <code>apt-get install</code>.</p> <blockquote> <p><strong>Note</strong>: The official Debian and Ubuntu images <a href=\"https://github.com/docker/docker/blob/03e2923e42446dbb830c654d0eec323a0b4ef02a/contrib/mkimage/debootstrap#L82-L105\">automatically run <code>apt-get clean</code></a>, so explicit invocation is not required.</p> </blockquote> <h3 id=\"cmd\">CMD</h3> <p><a href=\"../../../reference/builder/index#cmd\">Dockerfile reference for the CMD instruction</a></p> <p>The <code>CMD</code> instruction should be used to run the software contained by your image, along with any arguments. <code>CMD</code> should almost always be used in the form of <code>CMD [“executable”, “param1”, “param2”…]</code>. Thus, if the image is for a service (Apache, Rails, etc.), you would run something like <code>CMD [\"apache2\",\"-DFOREGROUND\"]</code>. Indeed, this form of the instruction is recommended for any service-based image.</p> <p>In most other cases, <code>CMD</code> should be given an interactive shell (bash, python, perl, etc), for example, <code>CMD [\"perl\", \"-de0\"]</code>, <code>CMD [\"python\"]</code>, or <code>CMD [“php”, “-a”]</code>. Using this form means that when you execute something like <code>docker run -it python</code>, you’ll get dropped into a usable shell, ready to go. <code>CMD</code> should rarely be used in the manner of <code>CMD [“param”, “param”]</code> in conjunction with <a href=\"../../../reference/builder/index#entrypoint\"><code>ENTRYPOINT</code></a>, unless you and your expected users are already quite familiar with how <code>ENTRYPOINT</code> works.</p> <h3 id=\"expose\">EXPOSE</h3> <p><a href=\"../../../reference/builder/index#expose\">Dockerfile reference for the EXPOSE instruction</a></p> <p>The <code>EXPOSE</code> instruction indicates the ports on which a container will listen for connections. Consequently, you should use the common, traditional port for your application. For example, an image containing the Apache web server would use <code>EXPOSE 80</code>, while an image containing MongoDB would use <code>EXPOSE 27017</code> and so on.</p> <p>For external access, your users can execute <code>docker run</code> with a flag indicating how to map the specified port to the port of their choice. For container linking, Docker provides environment variables for the path from the recipient container back to the source (ie, <code>MYSQL_PORT_3306_TCP</code>).</p> <h3 id=\"env\">ENV</h3> <p><a href=\"../../../reference/builder/index#env\">Dockerfile reference for the ENV instruction</a></p> <p>In order to make new software easier to run, you can use <code>ENV</code> to update the <code>PATH</code> environment variable for the software your container installs. For example, <code>ENV PATH /usr/local/nginx/bin:$PATH</code> will ensure that <code>CMD [“nginx”]</code> just works.</p> <p>The <code>ENV</code> instruction is also useful for providing required environment variables specific to services you wish to containerize, such as Postgres’s <code>PGDATA</code>.</p> <p>Lastly, <code>ENV</code> can also be used to set commonly used version numbers so that version bumps are easier to maintain, as seen in the following example:</p> <pre>ENV PG_MAJOR 9.3\nENV PG_VERSION 9.3.4\nRUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress &amp;&amp; …\nENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH\n</pre> <p>Similar to having constant variables in a program (as opposed to hard-coding values), this approach lets you change a single <code>ENV</code> instruction to auto-magically bump the version of the software in your container.</p> <h3 id=\"add-or-copy\">ADD or COPY</h3> <p><a href=\"../../../reference/builder/index#add\">Dockerfile reference for the ADD instruction</a><br> <a href=\"../../../reference/builder/index#copy\">Dockerfile reference for the COPY instruction</a></p> <p>Although <code>ADD</code> and <code>COPY</code> are functionally similar, generally speaking, <code>COPY</code> is preferred. That’s because it’s more transparent than <code>ADD</code>. <code>COPY</code> only supports the basic copying of local files into the container, while <code>ADD</code> has some features (like local-only tar extraction and remote URL support) that are not immediately obvious. Consequently, the best use for <code>ADD</code> is local tar file auto-extraction into the image, as in <code>ADD rootfs.tar.xz /</code>.</p> <p>If you have multiple <code>Dockerfile</code> steps that use different files from your context, <code>COPY</code> them individually, rather than all at once. This will ensure that each step’s build cache is only invalidated (forcing the step to be re-run) if the specifically required files change.</p> <p>For example:</p> <pre>COPY requirements.txt /tmp/\nRUN pip install --requirement /tmp/requirements.txt\nCOPY . /tmp/\n</pre> <p>Results in fewer cache invalidations for the <code>RUN</code> step, than if you put the <code>COPY . /tmp/</code> before it.</p> <p>Because image size matters, using <code>ADD</code> to fetch packages from remote URLs is strongly discouraged; you should use <code>curl</code> or <code>wget</code> instead. That way you can delete the files you no longer need after they’ve been extracted and you won’t have to add another layer in your image. For example, you should avoid doing things like:</p> <pre>ADD http://example.com/big.tar.xz /usr/src/things/\nRUN tar -xJf /usr/src/things/big.tar.xz -C /usr/src/things\nRUN make -C /usr/src/things all\n</pre> <p>And instead, do something like:</p> <pre>RUN mkdir -p /usr/src/things \\\n    &amp;&amp; curl -SL http://example.com/big.tar.xz \\\n    | tar -xJC /usr/src/things \\\n    &amp;&amp; make -C /usr/src/things all\n</pre> <p>For other items (files, directories) that do not require <code>ADD</code>’s tar auto-extraction capability, you should always use <code>COPY</code>.</p> <h3 id=\"entrypoint\">ENTRYPOINT</h3> <p><a href=\"../../../reference/builder/index#entrypoint\">Dockerfile reference for the ENTRYPOINT instruction</a></p> <p>The best use for <code>ENTRYPOINT</code> is to set the image’s main command, allowing that image to be run as though it was that command (and then use <code>CMD</code> as the default flags).</p> <p>Let’s start with an example of an image for the command line tool <code>s3cmd</code>:</p> <pre>ENTRYPOINT [\"s3cmd\"]\nCMD [\"--help\"]\n</pre> <p>Now the image can be run like this to show the command’s help:</p> <pre>$ docker run s3cmd\n</pre> <p>Or using the right parameters to execute a command:</p> <pre>$ docker run s3cmd ls s3://mybucket\n</pre> <p>This is useful because the image name can double as a reference to the binary as shown in the command above.</p> <p>The <code>ENTRYPOINT</code> instruction can also be used in combination with a helper script, allowing it to function in a similar way to the command above, even when starting the tool may require more than one step.</p> <p>For example, the <a href=\"https://hub.docker.com/_/postgres/\">Postgres Official Image</a> uses the following script as its <code>ENTRYPOINT</code>:</p> <pre>#!/bin/bash\nset -e\n\nif [ \"$1\" = 'postgres' ]; then\n    chown -R postgres \"$PGDATA\"\n\n    if [ -z \"$(ls -A \"$PGDATA\")\" ]; then\n        gosu postgres initdb\n    fi\n\n    exec gosu postgres \"$@\"\nfi\n\nexec \"$@\"\n</pre> <blockquote> <p><strong>Note</strong>: This script uses <a href=\"http://wiki.bash-hackers.org/commands/builtin/exec\">the <code>exec</code> Bash command</a> so that the final running application becomes the container’s PID 1. This allows the application to receive any Unix signals sent to the container. See the <a href=\"../../../reference/builder/index#entrypoint\"><code>ENTRYPOINT</code></a> help for more details.</p> </blockquote> <p>The helper script is copied into the container and run via <code>ENTRYPOINT</code> on container start:</p> <pre>COPY ./docker-entrypoint.sh /\nENTRYPOINT [\"/docker-entrypoint.sh\"]\n</pre> <p>This script allows the user to interact with Postgres in several ways.</p> <p>It can simply start Postgres:</p> <pre>$ docker run postgres\n</pre> <p>Or, it can be used to run Postgres and pass parameters to the server:</p> <pre>$ docker run postgres postgres --help\n</pre> <p>Lastly, it could also be used to start a totally different tool, such as Bash:</p> <pre>$ docker run --rm -it postgres bash\n</pre> <h3 id=\"volume\">VOLUME</h3> <p><a href=\"../../../reference/builder/index#volume\">Dockerfile reference for the VOLUME instruction</a></p> <p>The <code>VOLUME</code> instruction should be used to expose any database storage area, configuration storage, or files/folders created by your docker container. You are strongly encouraged to use <code>VOLUME</code> for any mutable and/or user-serviceable parts of your image.</p> <h3 id=\"user\">USER</h3> <p><a href=\"../../../reference/builder/index#user\">Dockerfile reference for the USER instruction</a></p> <p>If a service can run without privileges, use <code>USER</code> to change to a non-root user. Start by creating the user and group in the <code>Dockerfile</code> with something like <code>RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres</code>.</p> <blockquote> <p><strong>Note:</strong> Users and groups in an image get a non-deterministic UID/GID in that the “next” UID/GID gets assigned regardless of image rebuilds. So, if it’s critical, you should assign an explicit UID/GID.</p> </blockquote> <p>You should avoid installing or using <code>sudo</code> since it has unpredictable TTY and signal-forwarding behavior that can cause more problems than it solves. If you absolutely need functionality similar to <code>sudo</code> (e.g., initializing the daemon as root but running it as non-root), you may be able to use <a href=\"https://github.com/tianon/gosu\">“gosu”</a>.</p> <p>Lastly, to reduce layers and complexity, avoid switching <code>USER</code> back and forth frequently.</p> <h3 id=\"workdir\">WORKDIR</h3> <p><a href=\"../../../reference/builder/index#workdir\">Dockerfile reference for the WORKDIR instruction</a></p> <p>For clarity and reliability, you should always use absolute paths for your <code>WORKDIR</code>. Also, you should use <code>WORKDIR</code> instead of proliferating instructions like <code>RUN cd … &amp;&amp; do-something</code>, which are hard to read, troubleshoot, and maintain.</p> <h3 id=\"onbuild\">ONBUILD</h3> <p><a href=\"../../../reference/builder/index#onbuild\">Dockerfile reference for the ONBUILD instruction</a></p> <p>An <code>ONBUILD</code> command executes after the current <code>Dockerfile</code> build completes. <code>ONBUILD</code> executes in any child image derived <code>FROM</code> the current image. Think of the <code>ONBUILD</code> command as an instruction the parent <code>Dockerfile</code> gives to the child <code>Dockerfile</code>.</p> <p>A Docker build executes <code>ONBUILD</code> commands before any command in a child <code>Dockerfile</code>.</p> <p><code>ONBUILD</code> is useful for images that are going to be built <code>FROM</code> a given image. For example, you would use <code>ONBUILD</code> for a language stack image that builds arbitrary user software written in that language within the <code>Dockerfile</code>, as you can see in <a href=\"https://github.com/docker-library/ruby/blob/master/2.1/onbuild/Dockerfile\">Ruby’s <code>ONBUILD</code> variants</a>.</p> <p>Images built from <code>ONBUILD</code> should get a separate tag, for example: <code>ruby:1.9-onbuild</code> or <code>ruby:2.0-onbuild</code>.</p> <p>Be careful when putting <code>ADD</code> or <code>COPY</code> in <code>ONBUILD</code>. The “onbuild” image will fail catastrophically if the new build’s context is missing the resource being added. Adding a separate tag, as recommended above, will help mitigate this by allowing the <code>Dockerfile</code> author to make a choice.</p> <h2 id=\"examples-for-official-repositories\">Examples for Official Repositories</h2> <p>These Official Repositories have exemplary <code>Dockerfile</code>s:</p> <ul> <li><a href=\"https://hub.docker.com/_/golang/\">Go</a></li> <li><a href=\"https://hub.docker.com/_/perl/\">Perl</a></li> <li><a href=\"https://hub.docker.com/_/hylang/\">Hy</a></li> <li><a href=\"https://hub.docker.com/_/rails\">Rails</a></li> </ul> <h2 id=\"additional-resources\">Additional resources:</h2> <ul> <li><a href=\"../../../reference/builder/index\">Dockerfile Reference</a></li> <li><a href=\"../baseimages/index\">More about Base Images</a></li> <li><a href=\"https://docs.docker.com/docker-hub/builds/\">More about Automated Builds</a></li> <li><a href=\"https://docs.docker.com/docker-hub/official_repos/\">Guidelines for Creating Official Repositories</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/eng-image/dockerfile_best-practices/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/eng-image/dockerfile_best-practices/</a>\n  </p>\n</div>\n","engine/userguide/eng-image/image_management/index":"<h1 id=\"image-management\">Image management</h1> <p>The Docker Engine provides a client which you can use to create images on the command line or through a build process. You can run these images in a container or publish them for others to use. Storing the images you create, searching for images you might want, or publishing images others might use are all elements of image management.</p> <p>This section provides an overview of the major features and products Docker provides for image management.</p> <h2 id=\"docker-hub\">Docker Hub</h2> <p>The <a href=\"https://docs.docker.com/docker-hub/\">Docker Hub</a> is responsible for centralizing information about user accounts, images, and public name spaces. It has different components:</p> <ul> <li>Web UI</li> <li>Meta-data store (comments, stars, list public repositories)</li> <li>Authentication service</li> <li>Tokenization</li> </ul> <p>There is only one instance of the Docker Hub, run and managed by Docker Inc. This public Hub is useful for most individuals and smaller companies.</p> <h2 id=\"docker-registry-and-the-docker-trusted-registry\">Docker Registry and the Docker Trusted Registry</h2> <p>The Docker Registry is a component of Docker’s ecosystem. A registry is a storage and content delivery system, holding named Docker images, available in different tagged versions. For example, the image <code>distribution/registry</code>, with tags <code>2.0</code> and <code>latest</code>. Users interact with a registry by using docker push and pull commands such as, <code>docker pull myregistry.com/stevvooe/batman:voice</code>.</p> <p>The Docker Hub has its own registry which, like the Hub itself, is run and managed by Docker. However, there are other ways to obtain a registry. You can purchase the <a href=\"https://docs.docker.com/docker-trusted-registry\">Docker Trusted Registry</a> product to run on your company’s network. Alternatively, you can use the Docker Registry component to build a private registry. For information about using a registry, see overview for the <a href=\"https://docs.docker.com/registry\">Docker Registry</a>.</p> <h2 id=\"content-trust\">Content Trust</h2> <p>When transferring data among networked systems, <em>trust</em> is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and publisher of all of the data a system operates on. You use Docker to push and pull images (data) to a registry. Content trust gives you the ability to both verify the integrity and the publisher of all the data received from a registry over any channel.</p> <p><a href=\"../../../security/trust/index\">Content trust</a> is currently only available for users of the public Docker Hub. It is currently not available for the Docker Trusted Registry or for private registries.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/eng-image/image_management/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/eng-image/image_management/</a>\n  </p>\n</div>\n","engine/userguide/storagedriver/imagesandcontainers/index":"<h1 id=\"understand-images-containers-and-storage-drivers\">Understand images, containers, and storage drivers</h1> <p>To use storage drivers effectively, you must understand how Docker builds and stores images. Then, you need an understanding of how these images are used by containers. Finally, you’ll need a short introduction to the technologies that enable both images and container operations.</p> <h2 id=\"images-and-layers\">Images and layers</h2> <p>Each Docker image references a list of read-only layers that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s root filesystem. The diagram below shows the Ubuntu 15.04 image comprising 4 stacked image layers.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/image-layers.jpg\" alt=\"\"></p> <p>The Docker storage driver is responsible for stacking these layers and providing a single unified view.</p> <p>When you create a new container, you add a new, thin, writable layer on top of the underlying stack. This layer is often called the “container layer”. All changes made to the running container - such as writing new files, modifying existing files, and deleting files - are written to this thin writable container layer. The diagram below shows a container based on the Ubuntu 15.04 image.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/container-layers.jpg\" alt=\"\"></p> <h3 id=\"content-addressable-storage\">Content addressable storage</h3> <p>Docker 1.10 introduced a new content addressable storage model. This is a completely new way to address image and layer data on disk. Previously, image and layer data was referenced and stored using a randomly generated UUID. In the new model this is replaced by a secure <em>content hash</em>.</p> <p>The new model improves security, provides a built-in way to avoid ID collisions, and guarantees data integrity after pull, push, load, and save operations. It also enables better sharing of layers by allowing many images to freely share their layers even if they didn’t come from the same build.</p> <p>The diagram below shows an updated version of the previous diagram, highlighting the changes implemented by Docker 1.10.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/container-layers-cas.jpg\" alt=\"\"></p> <p>As can be seen, all image layer IDs are cryptographic hashes, whereas the container ID is still a randomly generated UUID.</p> <p>There are several things to note regarding the new model. These include:</p> <ol> <li>Migration of existing images</li> <li>Image and layer filesystem structures</li> </ol> <p>Existing images, those created and pulled by earlier versions of Docker, need to be migrated before they can be used with the new model. This migration involves calculating new secure checksums and is performed automatically the first time you start an updated Docker daemon. After the migration is complete, all images and tags will have brand new secure IDs.</p> <p>Although the migration is automatic and transparent, it is computationally intensive. This means it can take time if you have lots of image data. During this time your Docker daemon will not respond to other requests.</p> <p>A migration tool exists that allows you to migrate existing images to the new format before upgrading your Docker daemon. This means that upgraded Docker daemons do not need to perform the migration in-band, and therefore avoids any associated downtime. It also provides a way to manually migrate existing images so that they can be distributed to other Docker daemons in your environment that are already running the latest versions of Docker.</p> <p>The migration tool is provided by Docker, Inc., and runs as a container. You can download it from <a href=\"https://github.com/docker/v1.10-migrator/releases\">https://github.com/docker/v1.10-migrator/releases</a>.</p> <p>While running the “migrator” image you need to expose your Docker host’s data directory to the container. If you are using the default Docker data path, the command to run the container will look like this</p> <pre>$ sudo docker run --rm -v /var/lib/docker:/var/lib/docker docker/v1.10-migrator\n</pre> <p>If you use the <code>devicemapper</code> storage driver, you will need to include the <code>--privileged</code> option so that the container has access to your storage devices.</p> <h4 id=\"migration-example\">Migration example</h4> <p>The following example shows the migration tool in use on a Docker host running version 1.9.1 of the Docker daemon and the AUFS storage driver. The Docker host is running on a <strong>t2.micro</strong> AWS EC2 instance with 1 vCPU, 1GB RAM, and a single 8GB general purpose SSD EBS volume. The Docker data directory (<code>/var/lib/docker</code>) was consuming 2GB of space.</p> <pre>$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\njenkins             latest              285c9f0f9d3d        17 hours ago        708.5 MB\nmysql               latest              d39c3fa09ced        8 days ago          360.3 MB\nmongo               latest              a74137af4532        13 days ago         317.4 MB\npostgres            latest              9aae83d4127f        13 days ago         270.7 MB\nredis               latest              8bccd73928d9        2 weeks ago         151.3 MB\ncentos              latest              c8a648134623        4 weeks ago         196.6 MB\nubuntu              15.04               c8be1ac8145a        7 weeks ago         131.3 MB\n\n$ sudo du -hs /var/lib/docker\n2.0G    /var/lib/docker\n\n$ time docker run --rm -v /var/lib/docker:/var/lib/docker docker/v1.10-migrator\nUnable to find image 'docker/v1.10-migrator:latest' locally\nlatest: Pulling from docker/v1.10-migrator\ned1f33c5883d: Pull complete\nb3ca410aa2c1: Pull complete\n2b9c6ed9099e: Pull complete\ndce7e318b173: Pull complete\nDigest: sha256:bd2b245d5d22dd94ec4a8417a9b81bb5e90b171031c6e216484db3fe300c2097\nStatus: Downloaded newer image for docker/v1.10-migrator:latest\ntime=\"2016-01-27T12:31:06Z\" level=debug msg=\"Assembling tar data for 01e70da302a553ba13485ad020a0d77dbb47575a31c4f48221137bb08f45878d from /var/lib/docker/aufs/diff/01e70da302a553ba13485ad020a0d77dbb47575a31c4f48221137bb08f45878d\"\ntime=\"2016-01-27T12:31:06Z\" level=debug msg=\"Assembling tar data for 07ac220aeeef9febf1ac16a9d1a4eff7ef3c8cbf5ed0be6b6f4c35952ed7920d from /var/lib/docker/aufs/diff/07ac220aeeef9febf1ac16a9d1a4eff7ef3c8cbf5ed0be6b6f4c35952ed7920d\"\n&lt;snip&gt;\ntime=\"2016-01-27T12:32:00Z\" level=debug msg=\"layer dbacfa057b30b1feaf15937c28bd8ca0d6c634fc311ccc35bd8d56d017595d5b took 10.80 seconds\"\n\nreal    0m59.583s\nuser    0m0.046s\nsys     0m0.008s\n</pre> <p>The Unix <code>time</code> command prepends the <code>docker run</code> command to produce timings for the operation. As can be seen, the overall time taken to migrate 7 images comprising 2GB of disk space took approximately 1 minute. However, this included the time taken to pull the <code>docker/v1.10-migrator</code> image (approximately 3.5 seconds). The same operation on an m4.10xlarge EC2 instance with 40 vCPUs, 160GB RAM and an 8GB provisioned IOPS EBS volume resulted in the following improved timings:</p> <pre>real    0m9.871s\nuser    0m0.094s\nsys     0m0.021s\n</pre> <p>This shows that the migration operation is affected by the hardware spec of the machine performing the migration.</p> <h2 id=\"container-and-layers\">Container and layers</h2> <p>The major difference between a container and an image is the top writable layer. All writes to the container that add new or modify existing data are stored in this writable layer. When the container is deleted the writable layer is also deleted. The underlying image remains unchanged.</p> <p>Because each container has its own thin writable container layer, and all changes are stored this container layer, this means that multiple containers can share access to the same underlying image and yet have their own data state. The diagram below shows multiple containers sharing the same Ubuntu 15.04 image.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/sharing-layers.jpg\" alt=\"\"></p> <p>The Docker storage driver is responsible for enabling and managing both the image layers and the writable container layer. How a storage driver accomplishes these can vary between drivers. Two key technologies behind Docker image and container management are stackable image layers and copy-on-write (CoW).</p> <h2 id=\"the-copy-on-write-strategy\">The copy-on-write strategy</h2> <p>Sharing is a good way to optimize resources. People do this instinctively in daily life. For example, twins Jane and Joseph taking an Algebra class at different times from different teachers can share the same exercise book by passing it between each other. Now, suppose Jane gets an assignment to complete the homework on page 11 in the book. At that point, Jane copies page 11, completes the homework, and hands in her copy. The original exercise book is unchanged and only Jane has a copy of the changed page 11.</p> <p>Copy-on-write is a similar strategy of sharing and copying. In this strategy, system processes that need the same data share the same instance of that data rather than having their own copy. At some point, if one process needs to modify or write to the data, only then does the operating system make a copy of the data for that process to use. Only the process that needs to write has access to the data copy. All the other processes continue to use the original data.</p> <p>Docker uses a copy-on-write technology with both images and containers. This CoW strategy optimizes both image disk space usage and the performance of container start times. The next sections look at how copy-on-write is leveraged with images and containers through sharing and copying.</p> <h3 id=\"sharing-promotes-smaller-images\">Sharing promotes smaller images</h3> <p>This section looks at image layers and copy-on-write technology. All image and container layers exist inside the Docker host’s <em>local storage area</em> and are managed by the storage driver. On Linux-based Docker hosts this is usually located under <code>/var/lib/docker/</code>.</p> <p>The Docker client reports on image layers when instructed to pull and push images with <code>docker pull</code> and <code>docker push</code>. The command below pulls the <code>ubuntu:15.04</code> Docker image from Docker Hub.</p> <pre>$ docker pull ubuntu:15.04\n15.04: Pulling from library/ubuntu\n1ba8ac955b97: Pull complete\nf157c4e5ede7: Pull complete\n0b7e98f84c4c: Pull complete\na3ed95caeb02: Pull complete\nDigest: sha256:5e279a9df07990286cce22e1b0f5b0490629ca6d187698746ae5e28e604a640e\nStatus: Downloaded newer image for ubuntu:15.04\n</pre> <p>From the output, you’ll see that the command actually pulls 4 image layers. Each of the above lines lists an image layer and its UUID or cryptographic hash. The combination of these four layers makes up the <code>ubuntu:15.04</code> Docker image.</p> <p>Each of these layers is stored in its own directory inside the Docker host’s local storage are.</p> <p>Versions of Docker prior to 1.10 stored each layer in a directory with the same name as the image layer ID. However, this is not the case for images pulled with Docker version 1.10 and later. For example, the command below shows an image being pulled from Docker Hub, followed by a directory listing on a host running version 1.9.1 of the Docker Engine.</p> <pre>$  docker pull ubuntu:15.04\n15.04: Pulling from library/ubuntu\n47984b517ca9: Pull complete\ndf6e891a3ea9: Pull complete\ne65155041eed: Pull complete\nc8be1ac8145a: Pull complete\nDigest: sha256:5e279a9df07990286cce22e1b0f5b0490629ca6d187698746ae5e28e604a640e\nStatus: Downloaded newer image for ubuntu:15.04\n\n$ ls /var/lib/docker/aufs/layers\n47984b517ca9ca0312aced5c9698753ffa964c2015f2a5f18e5efa9848cf30e2\nc8be1ac8145a6e59a55667f573883749ad66eaeef92b4df17e5ea1260e2d7356\ndf6e891a3ea9cdce2a388a2cf1b1711629557454fd120abd5be6d32329a0e0ac\ne65155041eed7ec58dea78d90286048055ca75d41ea893c7246e794389ecf203\n</pre> <p>Notice how the four directories match up with the layer IDs of the downloaded image. Now compare this with the same operations performed on a host running version 1.10 of the Docker Engine.</p> <pre>$ docker pull ubuntu:15.04\n15.04: Pulling from library/ubuntu\n1ba8ac955b97: Pull complete\nf157c4e5ede7: Pull complete\n0b7e98f84c4c: Pull complete\na3ed95caeb02: Pull complete\nDigest: sha256:5e279a9df07990286cce22e1b0f5b0490629ca6d187698746ae5e28e604a640e\nStatus: Downloaded newer image for ubuntu:15.04\n\n$ ls /var/lib/docker/aufs/layers/\n1d6674ff835b10f76e354806e16b950f91a191d3b471236609ab13a930275e24\n5dbb0cbe0148cf447b9464a358c1587be586058d9a4c9ce079320265e2bb94e7\nbef7199f2ed8e86fa4ada1309cfad3089e0542fec8894690529e4c04a7ca2d73\nebf814eccfe98f2704660ca1d844e4348db3b5ccc637eb905d4818fbfb00a06a\n</pre> <p>See how the four directories do not match up with the image layer IDs pulled in the previous step.</p> <p>Despite the differences between image management before and after version 1.10, all versions of Docker still allow images to share layers. For example, If you <code>pull</code> an image that shares some of the same image layers as an image that has already been pulled, the Docker daemon recognizes this, and only pulls the layers it doesn’t already have stored locally. After the second pull, the two images will share any common image layers.</p> <p>You can illustrate this now for yourself. Starting with the <code>ubuntu:15.04</code> image that you just pulled, make a change to it, and build a new image based on the change. One way to do this is using a <code>Dockerfile</code> and the <code>docker build</code> command.</p> <ol> <li>\n<p>In an empty directory, create a simple <code>Dockerfile</code> that starts with the ubuntu:15.04 image.</p> <pre>FROM ubuntu:15.04\n</pre>\n</li> <li>\n<p>Add a new file called “newfile” in the image’s <code>/tmp</code> directory with the text “Hello world” in it.</p> <p>When you are done, the <code>Dockerfile</code> contains two lines:</p> <pre>FROM ubuntu:15.04\n\nRUN echo \"Hello world\" &gt; /tmp/newfile\n</pre>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>From a terminal in the same folder as your <code>Dockerfile</code>, run the following command:</p> <pre>$ docker build -t changed-ubuntu .\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM ubuntu:15.04\n ---&gt; 3f7bcee56709\nStep 2 : RUN echo \"Hello world\" &gt; /tmp/newfile\n ---&gt; Running in d14acd6fad4e\n ---&gt; 94e6b7d2c720\nRemoving intermediate container d14acd6fad4e\nSuccessfully built 94e6b7d2c720\n</pre> <blockquote> <p><strong>Note:</strong> The period (.) at the end of the above command is important. It tells the <code>docker build</code> command to use the current working directory as its build context.</p> </blockquote> <p>The output above shows a new image with image ID <code>94e6b7d2c720</code>.</p>\n</li> <li>\n<p>Run the <code>docker images</code> command to verify the new <code>changed-ubuntu</code> image is in the Docker host’s local storage area.</p> <pre>REPOSITORY       TAG      IMAGE ID       CREATED           SIZE\nchanged-ubuntu   latest   03b964f68d06   33 seconds ago    131.4 MB\nubuntu           15.04    013f3d01d247   6 weeks ago       131.3 MB\n</pre>\n</li> <li>\n<p>Run the <code>docker history</code> command to see which image layers were used to create the new <code>changed-ubuntu</code> image.</p> <pre>$ docker history changed-ubuntu\nIMAGE               CREATED              CREATED BY                                      SIZE        COMMENT\n94e6b7d2c720        2 minutes ago       /bin/sh -c echo \"Hello world\" &gt; /tmp/newfile    12 B \n3f7bcee56709        6 weeks ago         /bin/sh -c #(nop) CMD [\"/bin/bash\"]             0 B  \n&lt;missing&gt;           6 weeks ago         /bin/sh -c sed -i 's/^#\\s*\\(deb.*universe\\)$/   1.879 kB\n&lt;missing&gt;           6 weeks ago         /bin/sh -c echo '#!/bin/sh' &gt; /usr/sbin/polic   701 B\n&lt;missing&gt;           6 weeks ago         /bin/sh -c #(nop) ADD file:8e4943cd86e9b2ca13   131.3 MB\n</pre> <p>The <code>docker history</code> output shows the new <code>94e6b7d2c720</code> image layer at the top. You know that this is the new image layer added because it was created by the <code>echo \"Hello world\" &gt; /tmp/newfile</code> command in your <code>Dockerfile</code>. The 4 image layers below it are the exact same image layers that make up the <code>ubuntu:15.04</code> image.</p>\n</li> </ol> <blockquote> <p><strong>Note:</strong> Under the content addressable storage model introduced with Docker 1.10, image history data is no longer stored in a config file with each image layer. It is now stored as a string of text in a single config file that relates to the overall image. This can result in some image layers showing as “missing” in the output of the <code>docker history</code> command. This is normal behaviour and can be ignored.</p> <p>You may hear images like these referred to as <em>flat images</em>.</p> </blockquote> <p>Notice the new <code>changed-ubuntu</code> image does not have its own copies of every layer. As can be seen in the diagram below, the new image is sharing its four underlying layers with the <code>ubuntu:15.04</code> image.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/saving-space.jpg\" alt=\"\"></p> <p>The <code>docker history</code> command also shows the size of each image layer. As you can see, the <code>94e6b7d2c720</code> layer is only consuming 12 Bytes of disk space. This means that the <code>changed-ubuntu</code> image we just created is only consuming an additional 12 Bytes of disk space on the Docker host - all layers below the <code>94e6b7d2c720</code> layer already exist on the Docker host and are shared by other images.</p> <p>This sharing of image layers is what makes Docker images and containers so space efficient.</p> <h3 id=\"copying-makes-containers-efficient\">Copying makes containers efficient</h3> <p>You learned earlier that a container is a Docker image with a thin writable, container layer added. The diagram below shows the layers of a container based on the <code>ubuntu:15.04</code> image:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/container-layers-cas.jpg\" alt=\"\"></p> <p>All writes made to a container are stored in the thin writable container layer. The other layers are read-only (RO) image layers and can’t be changed. This means that multiple containers can safely share a single underlying image. The diagram below shows multiple containers sharing a single copy of the <code>ubuntu:15.04</code> image. Each container has its own thin RW layer, but they all share a single instance of the ubuntu:15.04 image:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/sharing-layers.jpg\" alt=\"\"></p> <p>When an existing file in a container is modified, Docker uses the storage driver to perform a copy-on-write operation. The specifics of operation depends on the storage driver. For the AUFS and OverlayFS storage drivers, the copy-on-write operation is pretty much as follows:</p> <ul> <li>Search through the image layers for the file to update. The process starts at the top, newest layer and works down to the base layer one layer at a time.</li> <li>Perform a “copy-up” operation on the first copy of the file that is found. A “copy up” copies the file up to the container’s own thin writable layer.</li> <li>Modify the <em>copy of the file</em> in container’s thin writable layer.</li> </ul> <p>Btrfs, ZFS, and other drivers handle the copy-on-write differently. You can read more about the methods of these drivers later in their detailed descriptions.</p> <p>Containers that write a lot of data will consume more space than containers that do not. This is because most write operations consume new space in the container’s thin writable top layer. If your container needs to write a lot of data, you should consider using a data volume.</p> <p>A copy-up operation can incur a noticeable performance overhead. This overhead is different depending on which storage driver is in use. However, large files, lots of layers, and deep directory trees can make the impact more noticeable. Fortunately, the operation only occurs the first time any particular file is modified. Subsequent modifications to the same file do not cause a copy-up operation and can operate directly on the file’s existing copy already present in the container layer.</p> <p>Let’s see what happens if we spin up 5 containers based on our <code>changed-ubuntu</code> image we built earlier:</p> <ol> <li>\n<p>From a terminal on your Docker host, run the following <code>docker run</code> command 5 times.</p> <pre>$ docker run -dit changed-ubuntu bash\n75bab0d54f3cf193cfdc3a86483466363f442fba30859f7dcd1b816b6ede82d4\n$ docker run -dit changed-ubuntu bash\n9280e777d109e2eb4b13ab211553516124a3d4d4280a0edfc7abf75c59024d47\n$ docker run -dit changed-ubuntu bash\na651680bd6c2ef64902e154eeb8a064b85c9abf08ac46f922ad8dfc11bb5cd8a\n$ docker run -dit changed-ubuntu bash\n8eb24b3b2d246f225b24f2fca39625aaad71689c392a7b552b78baf264647373\n$ docker run -dit changed-ubuntu bash\n0ad25d06bdf6fca0dedc38301b2aff7478b3e1ce3d1acd676573bba57cb1cfef\n</pre> <p>This launches 5 containers based on the <code>changed-ubuntu</code> image. As each container is created, Docker adds a writable layer and assigns it a random UUID. This is the value returned from the <code>docker run</code> command.</p>\n</li> <li>\n<p>Run the <code>docker ps</code> command to verify the 5 containers are running.</p> <pre>$ docker ps\nCONTAINER ID    IMAGE             COMMAND    CREATED              STATUS              PORTS    NAMES\n0ad25d06bdf6    changed-ubuntu    \"bash\"     About a minute ago   Up About a minute            stoic_ptolemy\n8eb24b3b2d24    changed-ubuntu    \"bash\"     About a minute ago   Up About a minute            pensive_bartik\na651680bd6c2    changed-ubuntu    \"bash\"     2 minutes ago        Up 2 minutes                 hopeful_turing\n9280e777d109    changed-ubuntu    \"bash\"     2 minutes ago        Up 2 minutes                 backstabbing_mahavira\n75bab0d54f3c    changed-ubuntu    \"bash\"     2 minutes ago        Up 2 minutes                 boring_pasteur\n</pre> <p>The output above shows 5 running containers, all sharing the <code>changed-ubuntu</code> image. Each <code>CONTAINER ID</code> is derived from the UUID when creating each container.</p>\n</li> <li>\n<p>List the contents of the local storage area.</p> <pre>$ sudo ls /var/lib/docker/containers\n0ad25d06bdf6fca0dedc38301b2aff7478b3e1ce3d1acd676573bba57cb1cfef\n9280e777d109e2eb4b13ab211553516124a3d4d4280a0edfc7abf75c59024d47\n75bab0d54f3cf193cfdc3a86483466363f442fba30859f7dcd1b816b6ede82d4\na651680bd6c2ef64902e154eeb8a064b85c9abf08ac46f922ad8dfc11bb5cd8a\n8eb24b3b2d246f225b24f2fca39625aaad71689c392a7b552b78baf264647373\n</pre>\n</li> </ol> <p>Docker’s copy-on-write strategy not only reduces the amount of space consumed by containers, it also reduces the time required to start a container. At start time, Docker only has to create the thin writable layer for each container. The diagram below shows these 5 containers sharing a single read-only (RO) copy of the <code>changed-ubuntu</code> image.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/shared-uuid.jpg\" alt=\"\"></p> <p>If Docker had to make an entire copy of the underlying image stack each time it started a new container, container start times and disk space used would be significantly increased.</p> <h2 id=\"data-volumes-and-the-storage-driver\">Data volumes and the storage driver</h2> <p>When a container is deleted, any data written to the container that is not stored in a <em>data volume</em> is deleted along with the container.</p> <p>A data volume is a directory or file in the Docker host’s filesystem that is mounted directly into a container. Data volumes are not controlled by the storage driver. Reads and writes to data volumes bypass the storage driver and operate at native host speeds. You can mount any number of data volumes into a container. Multiple containers can also share one or more data volumes.</p> <p>The diagram below shows a single Docker host running two containers. Each container exists inside of its own address space within the Docker host’s local storage area (<code>/var/lib/docker/...</code>). There is also a single shared data volume located at <code>/data</code> on the Docker host. This is mounted directly into both containers.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/shared-volume.jpg\" alt=\"\"></p> <p>Data volumes reside outside of the local storage area on the Docker host, further reinforcing their independence from the storage driver’s control. When a container is deleted, any data stored in data volumes persists on the Docker host.</p> <p>For detailed information about data volumes <a href=\"https://docs.docker.com/userguide/dockervolumes/\">Managing data in containers</a>.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../selectadriver/index\">Select a storage driver</a></li> <li><a href=\"../aufs-driver/index\">AUFS storage driver in practice</a></li> <li><a href=\"../btrfs-driver/index\">Btrfs storage driver in practice</a></li> <li><a href=\"../device-mapper-driver/index\">Device Mapper storage driver in practice</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/imagesandcontainers/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/storagedriver/imagesandcontainers/</a>\n  </p>\n</div>\n","engine/userguide/storagedriver/selectadriver/index":"<h1 id=\"select-a-storage-driver\">Select a storage driver</h1> <p>This page describes Docker’s storage driver feature. It lists the storage driver’s that Docker supports and the basic commands associated with managing them. Finally, this page provides guidance on choosing a storage driver.</p> <p>The material on this page is intended for readers who already have an <a href=\"../imagesandcontainers/index\">understanding of the storage driver technology</a>.</p> <h2 id=\"a-pluggable-storage-driver-architecture\">A pluggable storage driver architecture</h2> <p>Docker has a pluggable storage driver architecture. This gives you the flexibility to “plug in” the storage driver that is best for your environment and use-case. Each Docker storage driver is based on a Linux filesystem or volume manager. Further, each storage driver is free to implement the management of image layers and the container layer in its own unique way. This means some storage drivers perform better than others in different circumstances.</p> <p>Once you decide which driver is best, you set this driver on the Docker daemon at start time. As a result, the Docker daemon can only run one storage driver, and all containers created by that daemon instance use the same storage driver. The table below shows the supported storage driver technologies and their driver names:</p> <table> <thead> <tr> <th>Technology</th> <th>Storage driver name</th> </tr> </thead> <tbody> <tr> <td>OverlayFS</td> <td><code>overlay</code></td> </tr> <tr> <td>AUFS</td> <td><code>aufs</code></td> </tr> <tr> <td>Btrfs</td> <td><code>btrfs</code></td> </tr> <tr> <td>Device Mapper</td> <td><code>devicemapper</code></td> </tr> <tr> <td>VFS</td> <td><code>vfs</code></td> </tr> <tr> <td>ZFS</td> <td><code>zfs</code></td> </tr> </tbody> </table> <p>To find out which storage driver is set on the daemon, you use the <code>docker info</code> command:</p> <pre>$ docker info\nContainers: 0\nImages: 0\nStorage Driver: overlay\n Backing Filesystem: extfs\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 3.19.0-15-generic\nOperating System: Ubuntu 15.04\n... output truncated ...\n</pre> <p>The <code>info</code> subcommand reveals that the Docker daemon is using the <code>overlay</code> storage driver with a <code>Backing Filesystem</code> value of <code>extfs</code>. The <code>extfs</code> value means that the <code>overlay</code> storage driver is operating on top of an existing (ext) filesystem. The backing filesystem refers to the filesystem that was used to create the Docker host’s local storage area under <code>/var/lib/docker</code>.</p> <p>Which storage driver you use, in part, depends on the backing filesystem you plan to use for your Docker host’s local storage area. Some storage drivers can operate on top of different backing filesystems. However, other storage drivers require the backing filesystem to be the same as the storage driver. For example, the <code>btrfs</code> storage driver on a Btrfs backing filesystem. The following table lists each storage driver and whether it must match the host’s backing file system:</p> <table> <thead> <tr> <th>Storage driver</th> <th>Commonly used on</th> <th>Disabled on</th> </tr> </thead> <tbody> <tr> <td><code>overlay</code></td> <td>\n<code>ext4</code> <code>xfs</code>\n</td> <td>\n<code>btrfs</code> <code>aufs</code> <code>overlay</code> <code>zfs</code>\n</td> </tr> <tr> <td><code>aufs</code></td> <td>\n<code>ext4</code> <code>xfs</code>\n</td> <td>\n<code>btrfs</code> <code>aufs</code>\n</td> </tr> <tr> <td><code>btrfs</code></td> <td>\n<code>btrfs</code> <em>only</em>\n</td> <td>N/A</td> </tr> <tr> <td><code>devicemapper</code></td> <td><code>direct-lvm</code></td> <td>N/A</td> </tr> <tr> <td><code>vfs</code></td> <td>debugging only</td> <td>N/A</td> </tr> <tr> <td><code>zfs</code></td> <td>\n<code>zfs</code> <em>only</em>\n</td> <td>N/A</td> </tr> </tbody> </table> <blockquote> <p><strong>Note</strong> “Disabled on” means some storage drivers can not run over certain backing filesystem.</p> </blockquote> <p>You can set the storage driver by passing the <code>--storage-driver=&lt;name&gt;</code> option to the <code>docker daemon</code> command line, or by setting the option on the <code>DOCKER_OPTS</code> line in the <code>/etc/default/docker</code> file.</p> <p>The following command shows how to start the Docker daemon with the <code>devicemapper</code> storage driver using the <code>docker daemon</code> command:</p> <pre>$ docker daemon --storage-driver=devicemapper &amp;\n\n$ docker info\nContainers: 0\nImages: 0\nStorage Driver: devicemapper\n Pool Name: docker-252:0-147544-pool\n Pool Blocksize: 65.54 kB\n Backing Filesystem: extfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 1.821 GB\n Data Space Total: 107.4 GB\n Data Space Available: 3.174 GB\n Metadata Space Used: 1.479 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.146 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.90 (2014-09-01)\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 3.19.0-15-generic\nOperating System: Ubuntu 15.04\n&lt;output truncated&gt;\n</pre> <p>Your choice of storage driver can affect the performance of your containerized applications. So it’s important to understand the different storage driver options available and select the right one for your application. Later, in this page you’ll find some advice for choosing an appropriate driver.</p> <h2 id=\"shared-storage-systems-and-the-storage-driver\">Shared storage systems and the storage driver</h2> <p>Many enterprises consume storage from shared storage systems such as SAN and NAS arrays. These often provide increased performance and availability, as well as advanced features such as thin provisioning, deduplication and compression.</p> <p>The Docker storage driver and data volumes can both operate on top of storage provided by shared storage systems. This allows Docker to leverage the increased performance and availability these systems provide. However, Docker does not integrate with these underlying systems.</p> <p>Remember that each Docker storage driver is based on a Linux filesystem or volume manager. Be sure to follow existing best practices for operating your storage driver (filesystem or volume manager) on top of your shared storage system. For example, if using the ZFS storage driver on top of <em>XYZ</em> shared storage system, be sure to follow best practices for operating ZFS filesystems on top of XYZ shared storage system.</p> <h2 id=\"which-storage-driver-should-you-choose\">Which storage driver should you choose?</h2> <p>Several factors influence the selection of a storage driver. However, these two facts must be kept in mind:</p> <ol> <li>No single driver is well suited to every use-case</li> <li>Storage drivers are improving and evolving all of the time</li> </ol> <p>With these factors in mind, the following points, coupled with the table below, should provide some guidance.</p> <h3 id=\"stability\">Stability</h3> <p>For the most stable and hassle-free Docker experience, you should consider the following:</p> <ul> <li>\n<strong>Use the default storage driver for your distribution</strong>. When Docker installs, it chooses a default storage driver based on the configuration of your system. Stability is an important factor influencing which storage driver is used by default. Straying from this default may increase your chances of encountering bugs and nuances.</li> <li>\n<strong>Follow the configuration specified on the CS Engine <a href=\"https://www.docker.com/compatibility-maintenance\">compatibility matrix</a></strong>. The CS Engine is the commercially supported version of the Docker Engine. It’s code-base is identical to the open source Engine, but it has a limited set of supported configurations. These <em>supported configurations</em> use the most stable and mature storage drivers. Straying from these configurations may also increase your chances of encountering bugs and nuances.</li> </ul> <h3 id=\"experience-and-expertise\">Experience and expertise</h3> <p>Choose a storage driver that you and your team/organization have experience with. For example, if you use RHEL or one of its downstream forks, you may already have experience with LVM and Device Mapper. If so, you may wish to use the <code>devicemapper</code> driver.</p> <p>If you do not feel you have expertise with any of the storage drivers supported by Docker, and you want an easy-to-use stable Docker experience, you should consider using the default driver installed by your distribution’s Docker package.</p> <h3 id=\"future-proofing\">Future-proofing</h3> <p>Many people consider OverlayFS as the future of the Docker storage driver. However, it is less mature, and potentially less stable than some of the more mature drivers such as <code>aufs</code> and <code>devicemapper</code>. For this reason, you should use the OverlayFS driver with caution and expect to encounter more bugs and nuances than if you were using a more mature driver.</p> <p>The following diagram lists each storage driver and provides insight into some of their pros and cons. When selecting which storage driver to use, consider the guidance offered by the table below along with the points mentioned above.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/driver-pros-cons.png\" alt=\"\"></p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../imagesandcontainers/index\">Understand images, containers, and storage drivers</a></li> <li><a href=\"../aufs-driver/index\">AUFS storage driver in practice</a></li> <li><a href=\"../btrfs-driver/index\">Btrfs storage driver in practice</a></li> <li><a href=\"../device-mapper-driver/index\">Device Mapper storage driver in practice</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/selectadriver/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/storagedriver/selectadriver/</a>\n  </p>\n</div>\n","engine/userguide/storagedriver/zfs-driver/index":"<h1 id=\"docker-and-zfs-in-practice\">Docker and ZFS in practice</h1> <p>ZFS is a next generation filesystem that supports many advanced storage technologies such as volume management, snapshots, checksumming, compression and deduplication, replication and more.</p> <p>It was created by Sun Microsystems (now Oracle Corporation) and is open sourced under the CDDL license. Due to licensing incompatibilities between the CDDL and GPL, ZFS cannot be shipped as part of the mainline Linux kernel. However, the ZFS On Linux (ZoL) project provides an out-of-tree kernel module and userspace tools which can be installed separately.</p> <p>The ZFS on Linux (ZoL) port is healthy and maturing. However, at this point in time it is not recommended to use the <code>zfs</code> Docker storage driver for production use unless you have substantial experience with ZFS on Linux.</p> <blockquote> <p><strong>Note:</strong> There is also a FUSE implementation of ZFS on the Linux platform. This should work with Docker but is not recommended. The native ZFS driver (ZoL) is more tested, more performant, and is more widely used. The remainder of this document will relate to the native ZoL port.</p> </blockquote> <h2 id=\"image-layering-and-sharing-with-zfs\">Image layering and sharing with ZFS</h2> <p>The Docker <code>zfs</code> storage driver makes extensive use of three ZFS datasets:</p> <ul> <li>filesystems</li> <li>snapshots</li> <li>clones</li> </ul> <p>ZFS filesystems are thinly provisioned and have space allocated to them from a ZFS pool (zpool) via allocate on demand operations. Snapshots and clones are space-efficient point-in-time copies of ZFS filesystems. Snapshots are read-only. Clones are read-write. Clones can only be created from snapshots. This simple relationship is shown in the diagram below.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/zfs_clones.jpg\" alt=\"\"></p> <p>The solid line in the diagram shows the process flow for creating a clone. Step 1 creates a snapshot of the filesystem, and step two creates the clone from the snapshot. The dashed line shows the relationship between the clone and the filesystem, via the snapshot. All three ZFS datasets draw space form the same underlying zpool.</p> <p>On Docker hosts using the <code>zfs</code> storage driver, the base layer of an image is a ZFS filesystem. Each child layer is a ZFS clone based on a ZFS snapshot of the layer below it. A container is a ZFS clone based on a ZFS Snapshot of the top layer of the image it’s created from. All ZFS datasets draw their space from a common zpool. The diagram below shows how this is put together with a running container based on a two-layer image.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/zfs_zpool.jpg\" alt=\"\"></p> <p>The following process explains how images are layered and containers created. The process is based on the diagram above.</p> <ol> <li>\n<p>The base layer of the image exists on the Docker host as a ZFS filesystem.</p> <p>This filesystem consumes space from the zpool used to create the Docker host’s local storage area at <code>/var/lib/docker</code>.</p>\n</li> <li>\n<p>Additional image layers are clones of the dataset hosting the image layer directly below it.</p> <p>In the diagram, “Layer 1” is added by making a ZFS snapshot of the base layer and then creating a clone from that snapshot. The clone is writable and consumes space on-demand from the zpool. The snapshot is read-only, maintaining the base layer as an immutable object.</p>\n</li> <li>\n<p>When the container is launched, a read-write layer is added above the image.</p> <p>In the diagram above, the container’s read-write layer is created by making a snapshot of the top layer of the image (Layer 1) and creating a clone from that snapshot.</p> <p>As changes are made to the container, space is allocated to it from the zpool via allocate-on-demand operations. By default, ZFS will allocate space in blocks of 128K.</p>\n</li> </ol> <p>This process of creating child layers and containers from <em>read-only</em> snapshots allows images to be maintained as immutable objects.</p> <h2 id=\"container-reads-and-writes-with-zfs\">Container reads and writes with ZFS</h2> <p>Container reads with the <code>zfs</code> storage driver are very simple. A newly launched container is based on a ZFS clone. This clone initially shares all of its data with the dataset it was created from. This means that read operations with the <code>zfs</code> storage driver are fast – even if the data being read was note copied into the container yet. This sharing of data blocks is shown in the diagram below.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/zpool_blocks.jpg\" alt=\"\"></p> <p>Writing new data to a container is accomplished via an allocate-on-demand operation. Every time a new area of the container needs writing to, a new block is allocated from the zpool. This means that containers consume additional space as new data is written to them. New space is allocated to the container (ZFS Clone) from the underlying zpool.</p> <p>Updating <em>existing data</em> in a container is accomplished by allocating new blocks to the containers clone and storing the changed data in those new blocks. The original blocks are unchanged, allowing the underlying image dataset to remain immutable. This is the same as writing to a normal ZFS filesystem and is an implementation of copy-on-write semantics.</p> <h2 id=\"configure-docker-with-the-zfs-storage-driver\">Configure Docker with the ZFS storage driver</h2> <p>The <code>zfs</code> storage driver is only supported on a Docker host where <code>/var/lib/docker</code> is mounted as a ZFS filesystem. This section shows you how to install and configure native ZFS on Linux (ZoL) on an Ubuntu 14.04 system.</p> <h3 id=\"prerequisites\">Prerequisites</h3> <p>If you have already used the Docker daemon on your Docker host and have images you want to keep, <code>push</code> them Docker Hub or your private Docker Trusted Registry before attempting this procedure.</p> <p>Stop the Docker daemon. Then, ensure that you have a spare block device at <code>/dev/xvdb</code>. The device identifier may be be different in your environment and you should substitute your own values throughout the procedure.</p> <h3 id=\"install-zfs-on-ubuntu-14-04-lts\">Install Zfs on Ubuntu 14.04 LTS</h3> <ol> <li><p>If it is running, stop the Docker <code>daemon</code>.</p></li> <li>\n<p>Install the <code>software-properties-common</code> package.</p> <p>This is required for the <code>add-apt-repository</code> command.</p> <pre>$ sudo apt-get install -y software-properties-common\nReading package lists... Done\nBuilding dependency tree\n&lt;output truncated&gt;\n</pre>\n</li> <li>\n<p>Add the <code>zfs-native</code> package archive.</p> <pre>$ sudo add-apt-repository ppa:zfs-native/stable\n The native ZFS filesystem for Linux. Install the ubuntu-zfs package.\n&lt;output truncated&gt;\ngpg: key F6B0FC61: public key \"Launchpad PPA for Native ZFS for Linux\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1  (RSA: 1)\nOK\n</pre>\n</li> <li>\n<p>Get the latest package lists for all registered repositories and package archives.</p> <pre>$ sudo apt-get update\nIgn http://us-west-2.ec2.archive.ubuntu.com trusty InRelease\nGet:1 http://us-west-2.ec2.archive.ubuntu.com trusty-updates InRelease [64.4 kB]\n&lt;output truncated&gt;\nFetched 10.3 MB in 4s (2,370 kB/s)\nReading package lists... Done\n</pre>\n</li> <li>\n<p>Install the <code>ubuntu-zfs</code> package.</p> <pre>$ sudo apt-get install -y ubuntu-zfs\nReading package lists... Done\nBuilding dependency tree\n&lt;output truncated&gt;\n</pre>\n</li> <li>\n<p>Load the <code>zfs</code> module.</p> <pre>$ sudo modprobe zfs\n</pre>\n</li> <li>\n<p>Verify that it loaded correctly.</p> <pre>$ lsmod | grep zfs\nzfs                  2768247  0\nzunicode              331170  1 zfs\nzcommon                55411  1 zfs\nznvpair                89086  2 zfs,zcommon\nspl                    96378  3 zfs,zcommon,znvpair\nzavl                   15236  1 zfs\n</pre>\n</li> </ol> <h2 id=\"configure-zfs-for-docker\">Configure ZFS for Docker</h2> <p>Once ZFS is installed and loaded, you’re ready to configure ZFS for Docker.</p> <ol> <li>\n<p>Create a new <code>zpool</code>.</p> <pre>$ sudo zpool create -f zpool-docker /dev/xvdb\n</pre> <p>The command creates the <code>zpool</code> and gives it the name “zpool-docker”. The name is arbitrary.</p>\n</li> <li>\n<p>Check that the <code>zpool</code> exists.</p> <pre>$ sudo zfs list\nNAME            USED  AVAIL    REFER  MOUNTPOINT\nzpool-docker    55K   3.84G    19K    /zpool-docker\n</pre>\n</li> <li>\n<p>Create and mount a new ZFS filesystem to <code>/var/lib/docker</code>.</p> <pre>$ sudo zfs create -o mountpoint=/var/lib/docker zpool-docker/docker\n</pre>\n</li> <li>\n<p>Check that the previous step worked.</p> <pre>$ sudo zfs list -t all\nNAME                 USED  AVAIL  REFER  MOUNTPOINT\nzpool-docker         93.5K  3.84G    19K  /zpool-docker\nzpool-docker/docker  19K    3.84G    19K  /var/lib/docker\n</pre> <p>Now that you have a ZFS filesystem mounted to <code>/var/lib/docker</code>, the daemon should automatically load with the <code>zfs</code> storage driver.</p>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo service docker start\ndocker start/running, process 2315\n</pre> <p>The procedure for starting the Docker daemon may differ depending on the Linux distribution you are using. It is possible to force the Docker daemon to start with the <code>zfs</code> storage driver by passing the <code>--storage-driver=zfs</code>flag to the <code>docker daemon</code> command, or to the <code>DOCKER_OPTS</code> line in the Docker config file.</p>\n</li> <li>\n<p>Verify that the daemon is using the <code>zfs</code> storage driver.</p> <pre>$ sudo docker info\nContainers: 0\nImages: 0\nStorage Driver: zfs\n Zpool: zpool-docker\n Zpool Health: ONLINE\n Parent Dataset: zpool-docker/docker\n Space Used By Parent: 27648\n Space Available: 4128139776\n Parent Quota: no\n Compression: off\nExecution Driver: native-0.2\n[...]\n</pre> <p>The output of the command above shows that the Docker daemon is using the <code>zfs</code> storage driver and that the parent dataset is the <code>zpool-docker/docker</code> filesystem created earlier.</p>\n</li> </ol> <p>Your Docker host is now using ZFS to store to manage its images and containers.</p> <h2 id=\"zfs-and-docker-performance\">ZFS and Docker performance</h2> <p>There are several factors that influence the performance of Docker using the <code>zfs</code> storage driver.</p> <ul> <li><p><strong>Memory</strong>. Memory has a major impact on ZFS performance. This goes back to the fact that ZFS was originally designed for use on big Sun Solaris servers with large amounts of memory. Keep this in mind when sizing your Docker hosts.</p></li> <li><p><strong>ZFS Features</strong>. Using ZFS features, such as deduplication, can significantly increase the amount of memory ZFS uses. For memory consumption and performance reasons it is recommended to turn off ZFS deduplication. However, deduplication at other layers in the stack (such as SAN or NAS arrays) can still be used as these do not impact ZFS memory usage and performance. If using SAN, NAS or other hardware RAID technologies you should continue to follow existing best practices for using them with ZFS.</p></li> <li><p><strong>ZFS Caching</strong>. ZFS caches disk blocks in a memory structure called the adaptive replacement cache (ARC). The <em>Single Copy ARC</em> feature of ZFS allows a single cached copy of a block to be shared by multiple clones of a filesystem. This means that multiple running containers can share a single copy of cached block. This means that ZFS is a good option for PaaS and other high density use cases.</p></li> <li><p><strong>Fragmentation</strong>. Fragmentation is a natural byproduct of copy-on-write filesystems like ZFS. However, ZFS writes in 128K blocks and allocates <em>slabs</em> (multiple 128K blocks) to CoW operations in an attempt to reduce fragmentation. The ZFS intent log (ZIL) and the coalescing of writes (delayed writes) also help to reduce fragmentation.</p></li> <li><p><strong>Use the native ZFS driver for Linux</strong>. Although the Docker <code>zfs</code> storage driver supports the ZFS FUSE implementation, it is not recommended when high performance is required. The native ZFS on Linux driver tends to perform better than the FUSE implementation.</p></li> </ul> <p>The following generic performance best practices also apply to ZFS.</p> <ul> <li><p><strong>Use of SSD</strong>. For best performance it is always a good idea to use fast storage media such as solid state devices (SSD). However, if you only have a limited amount of SSD storage available it is recommended to place the ZIL on SSD.</p></li> <li><p><strong>Use Data Volumes</strong>. Data volumes provide the best and most predictable performance. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. For this reason, you should place heavy write workloads on data volumes.</p></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/zfs-driver/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/storagedriver/zfs-driver/</a>\n  </p>\n</div>\n","engine/userguide/storagedriver/aufs-driver/index":"<h1 id=\"docker-and-aufs-in-practice\">Docker and AUFS in practice</h1> <p>AUFS was the first storage driver in use with Docker. As a result, it has a long and close history with Docker, is very stable, has a lot of real-world deployments, and has strong community support. AUFS has several features that make it a good choice for Docker. These features enable:</p> <ul> <li>Fast container startup times.</li> <li>Efficient use of storage.</li> <li>Efficient use of memory.</li> </ul> <p>Despite its capabilities and long history with Docker, some Linux distributions do not support AUFS. This is usually because AUFS is not included in the mainline (upstream) Linux kernel.</p> <p>The following sections examine some AUFS features and how they relate to Docker.</p> <h2 id=\"image-layering-and-sharing-with-aufs\">Image layering and sharing with AUFS</h2> <p>AUFS is a <em>unification filesystem</em>. This means that it takes multiple directories on a single Linux host, stacks them on top of each other, and provides a single unified view. To achieve this, AUFS uses a <em>union mount</em>.</p> <p>AUFS stacks multiple directories and exposes them as a unified view through a single mount point. All of the directories in the stack, as well as the union mount point, must all exist on the same Linux host. AUFS refers to each directory that it stacks as a <em>branch</em>.</p> <p>Within Docker, AUFS union mounts enable image layering. The AUFS storage driver implements Docker image layers using this union mount system. AUFS branches correspond to Docker image layers. The diagram below shows a Docker container based on the <code>ubuntu:latest</code> image.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/aufs_layers.jpg\" alt=\"\"></p> <p>This diagram shows that each image layer, and the container layer, is represented in the Docker hosts filesystem as a directory under <code>/var/lib/docker/</code>. The union mount point provides the unified view of all layers. As of Docker 1.10, image layer IDs do not correspond to the names of the directories that contain their data.</p> <p>AUFS also supports the copy-on-write technology (CoW). Not all storage drivers do.</p> <h2 id=\"container-reads-and-writes-with-aufs\">Container reads and writes with AUFS</h2> <p>Docker leverages AUFS CoW technology to enable image sharing and minimize the use of disk space. AUFS works at the file level. This means that all AUFS CoW operations copy entire files - even if only a small part of the file is being modified. This behavior can have a noticeable impact on container performance, especially if the files being copied are large, below a lot of image layers, or the CoW operation must search a deep directory tree.</p> <p>Consider, for example, an application running in a container needs to add a single new value to a large key-value store (file). If this is the first time the file is modified, it does not yet exist in the container’s top writable layer. So, the CoW must <em>copy up</em> the file from the underlying image. The AUFS storage driver searches each image layer for the file. The search order is from top to bottom. When it is found, the entire file is <em>copied up</em> to the container’s top writable layer. From there, it can be opened and modified.</p> <p>Larger files obviously take longer to <em>copy up</em> than smaller files, and files that exist in lower image layers take longer to locate than those in higher layers. However, a <em>copy up</em> operation only occurs once per file on any given container. Subsequent reads and writes happen against the file’s copy already <em>copied-up</em> to the container’s top layer.</p> <h2 id=\"deleting-files-with-the-aufs-storage-driver\">Deleting files with the AUFS storage driver</h2> <p>The AUFS storage driver deletes a file from a container by placing a <em>whiteout file</em> in the container’s top layer. The whiteout file effectively obscures the existence of the file in the read-only image layers below. The simplified diagram below shows a container based on an image with three image layers.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/aufs_delete.jpg\" alt=\"\"></p> <p>The <code>file3</code> was deleted from the container. So, the AUFS storage driver placed a whiteout file in the container’s top layer. This whiteout file effectively “deletes” <code>file3</code> from the container by obscuring any of the original file’s existence in the image’s read-only layers. This works the same no matter which of the image’s read-only layers the file exists in.</p> <h2 id=\"configure-docker-with-aufs\">Configure Docker with AUFS</h2> <p>You can only use the AUFS storage driver on Linux systems with AUFS installed. Use the following command to determine if your system supports AUFS.</p> <pre>$ grep aufs /proc/filesystems\nnodev   aufs\n</pre> <p>This output indicates the system supports AUFS. Once you’ve verified your system supports AUFS, you can must instruct the Docker daemon to use it. You do this from the command line with the <code>docker daemon</code> command:</p> <pre>$ sudo docker daemon --storage-driver=aufs &amp;\n</pre> <p>Alternatively, you can edit the Docker config file and add the <code>--storage-driver=aufs</code> option to the <code>DOCKER_OPTS</code> line.</p> <pre># Use DOCKER_OPTS to modify the daemon startup options.\nDOCKER_OPTS=\"--storage-driver=aufs\"\n</pre> <p>Once your daemon is running, verify the storage driver with the <code>docker info</code> command.</p> <pre>$ sudo docker info\nContainers: 1\nImages: 4\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 6\n Dirperm1 Supported: false\nExecution Driver: native-0.2\n...output truncated...\n</pre> <p>The output above shows that the Docker daemon is running the AUFS storage driver on top of an existing <code>ext4</code> backing filesystem.</p> <h2 id=\"local-storage-and-aufs\">Local storage and AUFS</h2> <p>As the <code>docker daemon</code> runs with the AUFS driver, the driver stores images and containers within the Docker host’s local storage area under <code>/var/lib/docker/aufs/</code>.</p> <h3 id=\"images\">Images</h3> <p>Image layers and their contents are stored under <code>/var/lib/docker/aufs/diff/</code>. With Docker 1.10 and higher, image layer IDs do not correspond to directory names.</p> <p>The <code>/var/lib/docker/aufs/layers/</code> directory contains metadata about how image layers are stacked. This directory contains one file for every image or container layer on the Docker host (though file names no longer match image layer IDs). Inside each file are the names of the directories that exist below it in the stack</p> <p>The command below shows the contents of a metadata file in <code>/var/lib/docker/aufs/layers/</code> that lists the three directories that are stacked below it in the union mount. Remember, these directory names do no map to image layer IDs with Docker 1.10 and higher.</p> <pre>$ cat /var/lib/docker/aufs/layers/91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\nd74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\nc22013c8472965aa5b62559f2b540cd440716ef149756e7b958a1b2aba421e87\nd3a1f33e8a5a513092f01bb7eb1c2abf4d711e5105390a3fe1ae2248cfde1391\n</pre> <p>The base layer in an image has no image layers below it, so its file is empty.</p> <h3 id=\"containers\">Containers</h3> <p>Running containers are mounted below <code>/var/lib/docker/aufs/mnt/&lt;container-id&gt;</code>. This is where the AUFS union mount point that exposes the container and all underlying image layers as a single unified view exists. If a container is not running, it still has a directory here but it is empty. This is because AUFS only mounts a container when it is running. With Docker 1.10 and higher, container IDs no longer correspond to directory names under <code>/var/lib/docker/aufs/mnt/&lt;container-id&gt;</code>.</p> <p>Container metadata and various config files that are placed into the running container are stored in <code>/var/lib/docker/containers/&lt;container-id&gt;</code>. Files in this directory exist for all containers on the system, including ones that are stopped. However, when a container is running the container’s log files are also in this directory.</p> <p>A container’s thin writable layer is stored in a directory under <code>/var/lib/docker/aufs/diff/</code>. With Docker 1.10 and higher, container IDs no longer correspond to directory names. However, the containers thin writable layer still exists under here and is stacked by AUFS as the top writable layer and is where all changes to the container are stored. The directory exists even if the container is stopped. This means that restarting a container will not lose changes made to it. Once a container is deleted, it’s thin writable layer in this directory is deleted.</p> <h2 id=\"aufs-and-docker-performance\">AUFS and Docker performance</h2> <p>To summarize some of the performance related aspects already mentioned:</p> <ul> <li><p>The AUFS storage driver is a good choice for PaaS and other similar use-cases where container density is important. This is because AUFS efficiently shares images between multiple running containers, enabling fast container start times and minimal use of disk space.</p></li> <li><p>The underlying mechanics of how AUFS shares files between image layers and containers uses the systems page cache very efficiently.</p></li> <li><p>The AUFS storage driver can introduce significant latencies into container write performance. This is because the first time a container writes to any file, the file has be located and copied into the containers top writable layer. These latencies increase and are compounded when these files exist below many image layers and the files themselves are large.</p></li> </ul> <p>One final point. Data volumes provide the best and most predictable performance. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. For this reason, you may want to place heavy write workloads on data volumes.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../imagesandcontainers/index\">Understand images, containers, and storage drivers</a></li> <li><a href=\"../selectadriver/index\">Select a storage driver</a></li> <li><a href=\"../btrfs-driver/index\">Btrfs storage driver in practice</a></li> <li><a href=\"../device-mapper-driver/index\">Device Mapper storage driver in practice</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/aufs-driver/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/storagedriver/aufs-driver/</a>\n  </p>\n</div>\n","engine/userguide/storagedriver/btrfs-driver/index":"<h1 id=\"docker-and-btrfs-in-practice\">Docker and Btrfs in practice</h1> <p>Btrfs is a next generation copy-on-write filesystem that supports many advanced storage technologies that make it a good fit for Docker. Btrfs is included in the mainline Linux kernel and its on-disk-format is now considered stable. However, many of its features are still under heavy development and users should consider it a fast-moving target.</p> <p>Docker’s <code>btrfs</code> storage driver leverages many Btrfs features for image and container management. Among these features are thin provisioning, copy-on-write, and snapshotting.</p> <p>This article refers to Docker’s Btrfs storage driver as <code>btrfs</code> and the overall Btrfs Filesystem as Btrfs.</p> <blockquote> <p><strong>Note</strong>: The <a href=\"https://www.docker.com/compatibility-maintenance\">Commercially Supported Docker Engine (CS-Engine)</a> does not currently support the <code>btrfs</code> storage driver.</p> </blockquote> <h2 id=\"the-future-of-btrfs\">The future of Btrfs</h2> <p>Btrfs has been long hailed as the future of Linux filesystems. With full support in the mainline Linux kernel, a stable on-disk-format, and active development with a focus on stability, this is now becoming more of a reality.</p> <p>As far as Docker on the Linux platform goes, many people see the <code>btrfs</code> storage driver as a potential long-term replacement for the <code>devicemapper</code> storage driver. However, at the time of writing, the <code>devicemapper</code> storage driver should be considered safer, more stable, and more <em>production ready</em>. You should only consider the <code>btrfs</code> driver for production deployments if you understand it well and have existing experience with Btrfs.</p> <h2 id=\"image-layering-and-sharing-with-btrfs\">Image layering and sharing with Btrfs</h2> <p>Docker leverages Btrfs <em>subvolumes</em> and <em>snapshots</em> for managing the on-disk components of image and container layers. Btrfs subvolumes look and feel like a normal Unix filesystem. As such, they can have their own internal directory structure that hooks into the wider Unix filesystem.</p> <p>Subvolumes are natively copy-on-write and have space allocated to them on-demand from an underlying storage pool. They can also be nested and snapped. The diagram blow shows 4 subvolumes. ‘Subvolume 2’ and ‘Subvolume 3’ are nested, whereas ‘Subvolume 4’ shows its own internal directory tree.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/btfs_subvolume.jpg\" alt=\"\"></p> <p>Snapshots are a point-in-time read-write copy of an entire subvolume. They exist directly below the subvolume they were created from. You can create snapshots of snapshots as shown in the diagram below.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/btfs_snapshots.jpg\" alt=\"\"></p> <p>Btfs allocates space to subvolumes and snapshots on demand from an underlying pool of storage. The unit of allocation is referred to as a <em>chunk</em>, and <em>chunks</em> are normally ~1GB in size.</p> <p>Snapshots are first-class citizens in a Btrfs filesystem. This means that they look, feel, and operate just like regular subvolumes. The technology required to create them is built directly into the Btrfs filesystem thanks to its native copy-on-write design. This means that Btrfs snapshots are space efficient with little or no performance overhead. The diagram below shows a subvolume and its snapshot sharing the same data.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/btfs_pool.jpg\" alt=\"\"></p> <p>Docker’s <code>btrfs</code> storage driver stores every image layer and container in its own Btrfs subvolume or snapshot. The base layer of an image is stored as a subvolume whereas child image layers and containers are stored as snapshots. This is shown in the diagram below.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/btfs_container_layer.jpg\" alt=\"\"></p> <p>The high level process for creating images and containers on Docker hosts running the <code>btrfs</code> driver is as follows:</p> <ol> <li><p>The image’s base layer is stored in a Btrfs <em>subvolume</em> under <code>/var/lib/docker/btrfs/subvolumes</code>.</p></li> <li>\n<p>Subsequent image layers are stored as a Btrfs <em>snapshot</em> of the parent layer’s subvolume or snapshot.</p> <p>The diagram below shows a three-layer image. The base layer is a subvolume. Layer 1 is a snapshot of the base layer’s subvolume. Layer 2 is a snapshot of Layer 1’s snapshot.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/btfs_constructs.jpg\" alt=\"\"></p>\n</li> </ol> <p>As of Docker 1.10, image layer IDs no longer correspond to directory names under <code>/var/lib/docker/</code>.</p> <h2 id=\"image-and-container-on-disk-constructs\">Image and container on-disk constructs</h2> <p>Image layers and containers are visible in the Docker host’s filesystem at <code>/var/lib/docker/btrfs/subvolumes/</code>. However, as previously stated, directory names no longer correspond to image layer IDs. That said, directories for containers are present even for containers with a stopped status. This is because the <code>btrfs</code> storage driver mounts a default, top-level subvolume at <code>/var/lib/docker/subvolumes</code>. All other subvolumes and snapshots exist below that as Btrfs filesystem objects and not as individual mounts.</p> <p>Because Btrfs works at the filesystem level and not the block level, each image and container layer can be browsed in the filesystem using normal Unix commands. The example below shows a truncated output of an <code>ls -l</code> command an image layer:</p> <pre>$ ls -l /var/lib/docker/btrfs/subvolumes/0a17decee4139b0de68478f149cc16346f5e711c5ae3bb969895f22dd6723751/\ntotal 0\ndrwxr-xr-x 1 root root 1372 Oct  9 08:39 bin\ndrwxr-xr-x 1 root root    0 Apr 10  2014 boot\ndrwxr-xr-x 1 root root  882 Oct  9 08:38 dev\ndrwxr-xr-x 1 root root 2040 Oct 12 17:27 etc\ndrwxr-xr-x 1 root root    0 Apr 10  2014 home\n...output truncated...\n</pre> <h2 id=\"container-reads-and-writes-with-btrfs\">Container reads and writes with Btrfs</h2> <p>A container is a space-efficient snapshot of an image. Metadata in the snapshot points to the actual data blocks in the storage pool. This is the same as with a subvolume. Therefore, reads performed against a snapshot are essentially the same as reads performed against a subvolume. As a result, no performance overhead is incurred from the Btrfs driver.</p> <p>Writing a new file to a container invokes an allocate-on-demand operation to allocate new data block to the container’s snapshot. The file is then written to this new space. The allocate-on-demand operation is native to all writes with Btrfs and is the same as writing new data to a subvolume. As a result, writing new files to a container’s snapshot operate at native Btrfs speeds.</p> <p>Updating an existing file in a container causes a copy-on-write operation (technically <em>redirect-on-write</em>). The driver leaves the original data and allocates new space to the snapshot. The updated data is written to this new space. Then, the driver updates the filesystem metadata in the snapshot to point to this new data. The original data is preserved in-place for subvolumes and snapshots further up the tree. This behavior is native to copy-on-write filesystems like Btrfs and incurs very little overhead.</p> <p>With Btfs, writing and updating lots of small files can result in slow performance. More on this later.</p> <h2 id=\"configuring-docker-with-btrfs\">Configuring Docker with Btrfs</h2> <p>The <code>btrfs</code> storage driver only operates on a Docker host where <code>/var/lib/docker</code> is mounted as a Btrfs filesystem. The following procedure shows how to configure Btrfs on Ubuntu 14.04 LTS.</p> <h3 id=\"prerequisites\">Prerequisites</h3> <p>If you have already used the Docker daemon on your Docker host and have images you want to keep, <code>push</code> them to Docker Hub or your private Docker Trusted Registry before attempting this procedure.</p> <p>Stop the Docker daemon. Then, ensure that you have a spare block device at <code>/dev/xvdb</code>. The device identifier may be different in your environment and you should substitute your own values throughout the procedure.</p> <p>The procedure also assumes your kernel has the appropriate Btrfs modules loaded. To verify this, use the following command:</p> <pre>$ cat /proc/filesystems | grep btrfs\n</pre> <h3 id=\"configure-btrfs-on-ubuntu-14-04-lts\">Configure Btrfs on Ubuntu 14.04 LTS</h3> <p>Assuming your system meets the prerequisites, do the following:</p> <ol> <li>\n<p>Install the “btrfs-tools” package.</p> <pre>$ sudo apt-get install btrfs-tools\nReading package lists... Done\nBuilding dependency tree\n&lt;output truncated&gt;\n</pre>\n</li> <li>\n<p>Create the Btrfs storage pool.</p> <p>Btrfs storage pools are created with the <code>mkfs.btrfs</code> command. Passing multiple devices to the <code>mkfs.btrfs</code> command creates a pool across all of those devices. Here you create a pool with a single device at <code>/dev/xvdb</code>.</p> <pre>$ sudo mkfs.btrfs -f /dev/xvdb\nWARNING! - Btrfs v3.12 IS EXPERIMENTAL\nWARNING! - see http://btrfs.wiki.kernel.org before using\n\nTurning ON incompat feature 'extref': increased hardlink limit per file to 65536\nfs created label (null) on /dev/xvdb\n    nodesize 16384 leafsize 16384 sectorsize 4096 size 4.00GiB\nBtrfs v3.12\n</pre> <p>Be sure to substitute <code>/dev/xvdb</code> with the appropriate device(s) on your system.</p> <blockquote> <p><strong>Warning</strong>: Take note of the warning about Btrfs being experimental. As noted earlier, Btrfs is not currently recommended for production deployments unless you already have extensive experience.</p> </blockquote>\n</li> <li>\n<p>If it does not already exist, create a directory for the Docker host’s local storage area at <code>/var/lib/docker</code>.</p> <pre>$ sudo mkdir /var/lib/docker\n</pre>\n</li> <li>\n<p>Configure the system to automatically mount the Btrfs filesystem each time the system boots.</p> <p>a. Obtain the Btrfs filesystem’s UUID.</p> <pre>$ sudo blkid /dev/xvdb\n/dev/xvdb: UUID=\"a0ed851e-158b-4120-8416-c9b072c8cf47\" UUID_SUB=\"c3927a64-4454-4eef-95c2-a7d44ac0cf27\" TYPE=\"btrfs\"\n</pre> <p>b. Create an <code>/etc/fstab</code> entry to automatically mount <code>/var/lib/docker</code> each time the system boots. Either of the following lines will work, just remember to substitute the UUID value with the value obtained from the previous command.</p> <pre>/dev/xvdb /var/lib/docker btrfs defaults 0 0\nUUID=\"a0ed851e-158b-4120-8416-c9b072c8cf47\" /var/lib/docker btrfs defaults 0 0\n</pre>\n</li> <li>\n<p>Mount the new filesystem and verify the operation.</p> <pre>$ sudo mount -a\n$ mount\n/dev/xvda1 on / type ext4 (rw,discard)\n&lt;output truncated&gt;\n/dev/xvdb on /var/lib/docker type btrfs (rw)\n</pre> <p>The last line in the output above shows the <code>/dev/xvdb</code> mounted at <code>/var/lib/docker</code> as Btrfs.</p>\n</li> </ol> <p>Now that you have a Btrfs filesystem mounted at <code>/var/lib/docker</code>, the daemon should automatically load with the <code>btrfs</code> storage driver.</p> <ol> <li>\n<p>Start the Docker daemon.</p> <pre>$ sudo service docker start\ndocker start/running, process 2315\n</pre> <p>The procedure for starting the Docker daemon may differ depending on the Linux distribution you are using.</p> <p>You can force the Docker daemon to start with the <code>btrfs</code> storage driver by either passing the <code>--storage-driver=btrfs</code> flag to the <code>docker \ndaemon</code> at startup, or adding it to the <code>DOCKER_OPTS</code> line to the Docker config file.</p>\n</li> <li>\n<p>Verify the storage driver with the <code>docker info</code> command.</p> <pre>$ sudo docker info\nContainers: 0\nImages: 0\nStorage Driver: btrfs\n[...]\n</pre>\n</li> </ol> <p>Your Docker host is now configured to use the <code>btrfs</code> storage driver.</p> <h2 id=\"btrfs-and-docker-performance\">Btrfs and Docker performance</h2> <p>There are several factors that influence Docker’s performance under the <code>btrfs</code> storage driver.</p> <ul> <li><p><strong>Page caching</strong>. Btrfs does not support page cache sharing. This means that <em>n</em> containers accessing the same file require <em>n</em> copies to be cached. As a result, the <code>btrfs</code> driver may not be the best choice for PaaS and other high density container use cases.</p></li> <li>\n<p><strong>Small writes</strong>. Containers performing lots of small writes (including Docker hosts that start and stop many containers) can lead to poor use of Btrfs chunks. This can ultimately lead to out-of-space conditions on your Docker host and stop it working. This is currently a major drawback to using current versions of Btrfs.</p> <p>If you use the <code>btrfs</code> storage driver, closely monitor the free space on your Btrfs filesystem using the <code>btrfs filesys show</code> command. Do not trust the output of normal Unix commands such as <code>df</code>; always use the Btrfs native commands.</p>\n</li> <li><p><strong>Sequential writes</strong>. Btrfs writes data to disk via journaling technique. This can impact sequential writes, where performance can be up to half.</p></li> <li>\n<p><strong>Fragmentation</strong>. Fragmentation is a natural byproduct of copy-on-write filesystems like Btrfs. Many small random writes can compound this issue. It can manifest as CPU spikes on Docker hosts using SSD media and head thrashing on Docker hosts using spinning media. Both of these result in poor performance.</p> <p>Recent versions of Btrfs allow you to specify <code>autodefrag</code> as a mount option. This mode attempts to detect random writes and defragment them. You should perform your own tests before enabling this option on your Docker hosts. Some tests have shown this option has a negative performance impact on Docker hosts performing lots of small writes (including systems that start and stop many containers).</p>\n</li> <li>\n<p><strong>Solid State Devices (SSD)</strong>. Btrfs has native optimizations for SSD media. To enable these, mount with the <code>-o ssd</code> mount option. These optimizations include enhanced SSD write performance by avoiding things like <em>seek optimizations</em> that have no use on SSD media.</p> <p>Btfs also supports the TRIM/Discard primitives. However, mounting with the <code>-o discard</code> mount option can cause performance issues. Therefore, it is recommended you perform your own tests before using this option.</p>\n</li> <li><p><strong>Use Data Volumes</strong>. Data volumes provide the best and most predictable performance. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. For this reason, you should place heavy write workloads on data volumes.</p></li> </ul> <h2 id=\"related-information\">Related Information</h2> <ul> <li><a href=\"../imagesandcontainers/index\">Understand images, containers, and storage drivers</a></li> <li><a href=\"../selectadriver/index\">Select a storage driver</a></li> <li><a href=\"../aufs-driver/index\">AUFS storage driver in practice</a></li> <li><a href=\"../device-mapper-driver/index\">Device Mapper storage driver in practice</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/btrfs-driver/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/storagedriver/btrfs-driver/</a>\n  </p>\n</div>\n","engine/userguide/storagedriver/device-mapper-driver/index":"<h1 id=\"docker-and-the-device-mapper-storage-driver\">Docker and the Device Mapper storage driver</h1> <p>Device Mapper is a kernel-based framework that underpins many advanced volume management technologies on Linux. Docker’s <code>devicemapper</code> storage driver leverages the thin provisioning and snapshotting capabilities of this framework for image and container management. This article refers to the Device Mapper storage driver as <code>devicemapper</code>, and the kernel framework as <code>Device Mapper</code>.</p> <blockquote> <p><strong>Note</strong>: The <a href=\"https://www.docker.com/compatibility-maintenance\">Commercially Supported Docker Engine (CS-Engine) running on RHEL and CentOS Linux</a> requires that you use the <code>devicemapper</code> storage driver.</p> </blockquote> <h2 id=\"an-alternative-to-aufs\">An alternative to AUFS</h2> <p>Docker originally ran on Ubuntu and Debian Linux and used AUFS for its storage backend. As Docker became popular, many of the companies that wanted to use it were using Red Hat Enterprise Linux (RHEL). Unfortunately, because the upstream mainline Linux kernel did not include AUFS, RHEL did not use AUFS either.</p> <p>To correct this Red Hat developers investigated getting AUFS into the mainline kernel. Ultimately, though, they decided a better idea was to develop a new storage backend. Moreover, they would base this new storage backend on existing <code>Device Mapper</code> technology.</p> <p>Red Hat collaborated with Docker Inc. to contribute this new driver. As a result of this collaboration, Docker’s Engine was re-engineered to make the storage backend pluggable. So it was that the <code>devicemapper</code> became the second storage driver Docker supported.</p> <p>Device Mapper has been included in the mainline Linux kernel since version 2.6.9. It is a core part of RHEL family of Linux distributions. This means that the <code>devicemapper</code> storage driver is based on stable code that has a lot of real-world production deployments and strong community support.</p> <h2 id=\"image-layering-and-sharing\">Image layering and sharing</h2> <p>The <code>devicemapper</code> driver stores every image and container on its own virtual device. These devices are thin-provisioned copy-on-write snapshot devices. Device Mapper technology works at the block level rather than the file level. This means that <code>devicemapper</code> storage driver’s thin provisioning and copy-on-write operations work with blocks rather than entire files.</p> <blockquote> <p><strong>Note</strong>: Snapshots are also referred to as <em>thin devices</em> or <em>virtual devices</em>. They all mean the same thing in the context of the <code>devicemapper</code> storage driver.</p> </blockquote> <p>With <code>devicemapper</code> the high level process for creating images is as follows:</p> <ol> <li>\n<p>The <code>devicemapper</code> storage driver creates a thin pool.</p> <p>The pool is created from block devices or loop mounted sparse files (more on this later).</p>\n</li> <li>\n<p>Next it creates a <em>base device</em>.</p> <p>A base device is a thin device with a filesystem. You can see which filesystem is in use by running the <code>docker info</code> command and checking the <code>Backing filesystem</code> value.</p>\n</li> <li>\n<p>Each new image (and image layer) is a snapshot of this base device.</p> <p>These are thin provisioned copy-on-write snapshots. This means that they are initially empty and only consume space from the pool when data is written to them.</p>\n</li> </ol> <p>With <code>devicemapper</code>, container layers are snapshots of the image they are created from. Just as with images, container snapshots are thin provisioned copy-on-write snapshots. The container snapshot stores all updates to the container. The <code>devicemapper</code> allocates space to them on-demand from the pool as and when data is written to the container.</p> <p>The high level diagram below shows a thin pool with a base device and two images.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/base_device.jpg\" alt=\"\"></p> <p>If you look closely at the diagram you’ll see that it’s snapshots all the way down. Each image layer is a snapshot of the layer below it. The lowest layer of each image is a snapshot of the base device that exists in the pool. This base device is a <code>Device Mapper</code> artifact and not a Docker image layer.</p> <p>A container is a snapshot of the image it is created from. The diagram below shows two containers - one based on the Ubuntu image and the other based on the Busybox image.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/two_dm_container.jpg\" alt=\"\"></p> <h2 id=\"reads-with-the-devicemapper\">Reads with the devicemapper</h2> <p>Let’s look at how reads and writes occur using the <code>devicemapper</code> storage driver. The diagram below shows the high level process for reading a single block (<code>0x44f</code>) in an example container.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/dm_container.jpg\" alt=\"\"></p> <ol> <li>\n<p>An application makes a read request for block <code>0x44f</code> in the container.</p> <p>Because the container is a thin snapshot of an image it does not have the data. Instead, it has a pointer (PTR) to where the data is stored in the image snapshot lower down in the image stack.</p>\n</li> <li><p>The storage driver follows the pointer to block <code>0xf33</code> in the snapshot relating to image layer <code>a005...</code>.</p></li> <li><p>The <code>devicemapper</code> copies the contents of block <code>0xf33</code> from the image snapshot to memory in the container.</p></li> <li><p>The storage driver returns the data to the requesting application.</p></li> </ol> <h2 id=\"write-examples\">Write examples</h2> <p>With the <code>devicemapper</code> driver, writing new data to a container is accomplished by an <em>allocate-on-demand</em> operation. Updating existing data uses a copy-on-write operation. Because Device Mapper is a block-based technology these operations occur at the block level.</p> <p>For example, when making a small change to a large file in a container, the <code>devicemapper</code> storage driver does not copy the entire file. It only copies the blocks to be modified. Each block is 64KB.</p> <h3 id=\"writing-new-data\">Writing new data</h3> <p>To write 56KB of new data to a container:</p> <ol> <li><p>An application makes a request to write 56KB of new data to the container.</p></li> <li>\n<p>The allocate-on-demand operation allocates a single new 64KB block to the container’s snapshot.</p> <p>If the write operation is larger than 64KB, multiple new blocks are allocated to the container’s snapshot.</p>\n</li> <li><p>The data is written to the newly allocated block.</p></li> </ol> <h3 id=\"overwriting-existing-data\">Overwriting existing data</h3> <p>To modify existing data for the first time:</p> <ol> <li><p>An application makes a request to modify some data in the container.</p></li> <li><p>A copy-on-write operation locates the blocks that need updating.</p></li> <li><p>The operation allocates new empty blocks to the container snapshot and copies the data into those blocks.</p></li> <li><p>The modified data is written into the newly allocated blocks.</p></li> </ol> <p>The application in the container is unaware of any of these allocate-on-demand and copy-on-write operations. However, they may add latency to the application’s read and write operations.</p> <h2 id=\"configure-docker-with-devicemapper\">Configure Docker with devicemapper</h2> <p>The <code>devicemapper</code> is the default Docker storage driver on some Linux distributions. This includes RHEL and most of its forks. Currently, the following distributions support the driver:</p> <ul> <li>RHEL/CentOS/Fedora</li> <li>Ubuntu 12.04</li> <li>Ubuntu 14.04</li> <li>Debian</li> </ul> <p>Docker hosts running the <code>devicemapper</code> storage driver default to a configuration mode known as <code>loop-lvm</code>. This mode uses sparse files to build the thin pool used by image and container snapshots. The mode is designed to work out-of-the-box with no additional configuration. However, production deployments should not run under <code>loop-lvm</code> mode.</p> <p>You can detect the mode by viewing the <code>docker info</code> command:</p> <pre>$ sudo docker info\nContainers: 0\nImages: 0\nStorage Driver: devicemapper\n Pool Name: docker-202:2-25220302-pool\n Pool Blocksize: 65.54 kB\n Backing Filesystem: xfs\n [...]\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.93-RHEL7 (2015-01-28)\n [...]\n</pre> <p>The output above shows a Docker host running with the <code>devicemapper</code> storage driver operating in <code>loop-lvm</code> mode. This is indicated by the fact that the <code>Data loop file</code> and a <code>Metadata loop file</code> are on files under <code>/var/lib/docker/devicemapper/devicemapper</code>. These are loopback mounted sparse files.</p> <h3 id=\"configure-direct-lvm-mode-for-production\">Configure direct-lvm mode for production</h3> <p>The preferred configuration for production deployments is <code>direct-lvm</code>. This mode uses block devices to create the thin pool. The following procedure shows you how to configure a Docker host to use the <code>devicemapper</code> storage driver in a <code>direct-lvm</code> configuration.</p> <blockquote> <p><strong>Caution:</strong> If you have already run the Docker daemon on your Docker host and have images you want to keep, <code>push</code> them Docker Hub or your private Docker Trusted Registry before attempting this procedure.</p> </blockquote> <p>The procedure below will create a logical volume configured as a thin pool to use as backing for the storage pool. It assumes that you have a spare block device at <code>/dev/xvdf</code> with enough free space to complete the task. The device identifier and volume sizes may be be different in your environment and you should substitute your own values throughout the procedure. The procedure also assumes that the Docker daemon is in the <code>stopped</code> state.</p> <ol> <li><p>Log in to the Docker host you want to configure and stop the Docker daemon.</p></li> <li><p>Install the LVM2 package. The LVM2 package includes the userspace toolset that provides logical volume management facilities on linux.</p></li> <li>\n<p>Create a physical volume replacing <code>/dev/xvdf</code> with your block device.</p> <pre>$ pvcreate /dev/xvdf\n</pre>\n</li> <li>\n<p>Create a ‘docker’ volume group.</p> <pre>$ vgcreate docker /dev/xvdf\n</pre>\n</li> <li>\n<p>Create a thin pool named <code>thinpool</code>.</p> <p>In this example, the data logical is 95% of the ‘docker’ volume group size. Leaving this free space allows for auto expanding of either the data or metadata if space runs low as a temporary stopgap.</p> <pre>$ lvcreate --wipesignatures y -n thinpool docker -l 95%VG\n$ lvcreate --wipesignatures y -n thinpoolmeta docker -l 1%VG\n</pre>\n</li> <li>\n<p>Convert the pool to a thin pool.</p> <pre>$ lvconvert -y --zero n -c 512K --thinpool docker/thinpool --poolmetadata docker/thinpoolmeta\n</pre>\n</li> <li>\n<p>Configure autoextension of thin pools via an <code>lvm</code> profile.</p> <pre>$ vi /etc/lvm/profile/docker-thinpool.profile\n</pre>\n</li> <li>\n<p>Specify ‘thin_pool_autoextend_threshold’ value.</p> <p>The value should be the percentage of space used before <code>lvm</code> attempts to autoextend the available space (100 = disabled).</p> <pre>thin_pool_autoextend_threshold = 80\n</pre>\n</li> <li>\n<p>Modify the <code>thin_pool_autoextend_percent</code> for when thin pool autoextension occurs.</p> <p>The value’s setting is the perentage of space to increase the thin pool (100 = disabled)</p> <pre>thin_pool_autoextend_percent = 20\n</pre>\n</li> <li>\n<p>Check your work, your <code>docker-thinpool.profile</code> file should appear similar to the following:</p> <p>An example <code>/etc/lvm/profile/docker-thinpool.profile</code> file:</p> <pre>activation {\n    thin_pool_autoextend_threshold=80\n    thin_pool_autoextend_percent=20\n}\n</pre>\n</li> <li>\n<p>Apply your new lvm profile</p> <pre>$ lvchange --metadataprofile docker-thinpool docker/thinpool\n</pre>\n</li> <li>\n<p>Verify the <code>lv</code> is monitored.</p> <pre>$ lvs -o+seg_monitor\n</pre>\n</li> <li>\n<p>If the Docker daemon was previously started, clear your graph driver directory.</p> <p>Clearing your graph driver removes any images, containers, and volumes in your Docker installation.</p> <pre>$ rm -rf /var/lib/docker/*\n</pre>\n</li> <li>\n<p>Configure the Docker daemon with specific devicemapper options.</p> <p>There are two ways to do this. You can set options on the commmand line if you start the daemon there:</p> <pre>--storage-driver=devicemapper --storage-opt=dm.thinpooldev=/dev/mapper/docker-thinpool --storage-opt dm.use_deferred_removal=true\n</pre> <p>You can also set them for startup in the <code>daemon.json</code> configuration, for example:</p> <pre> {\n     \"storage-driver\": \"devicemapper\",\n     \"storage-opts\": [\n         \"dm.thinpooldev=/dev/mapper/docker-thinpool\",\n         \"dm.use_deferred_removal=true\"\n     ]\n }\n</pre>\n</li> <li>\n<p>If using systemd and modifying the daemon configuration via unit or drop-in file, reload systemd to scan for changes.</p> <pre>$ systemctl daemon-reload\n</pre>\n</li> <li>\n<p>Start the Docker daemon.</p> <pre>$ systemctl start docker\n</pre>\n</li> </ol> <p>After you start the Docker daemon, ensure you monitor your thin pool and volume group free space. While the volume group will auto-extend, it can still fill up. To monitor logical volumes, use <code>lvs</code> without options or <code>lvs -a</code> to see tha data and metadata sizes. To monitor volume group free space, use the <code>vgs</code> command.</p> <p>Logs can show the auto-extension of the thin pool when it hits the threshold, to view the logs use:</p> <pre>$ journalctl -fu dm-event.service\n</pre> <p>If you run into repeated problems with thin pool, you can use the <code>dm.min_free_space</code> option to tune the Engine behavior. This value ensures that operations fail with a warning when the free space is at or near the minimum. For information, see <a href=\"../../../reference/commandline/daemon/index#storage-driver-options\" target=\"_blank\">the storage driver options in the Engine daemon reference</a>.</p> <h3 id=\"examine-devicemapper-structures-on-the-host\">Examine devicemapper structures on the host</h3> <p>You can use the <code>lsblk</code> command to see the device files created above and the <code>pool</code> that the <code>devicemapper</code> storage driver creates on top of them.</p> <pre>$ sudo lsblk\nNAME\t\t\t   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nxvda\t\t\t   202:0\t0\t 8G  0 disk\n└─xvda1\t\t\t   202:1\t0\t 8G  0 part /\nxvdf\t\t\t   202:80\t0\t10G  0 disk\n├─vg--docker-data\t\t   253:0\t0\t90G  0 lvm\n│ └─docker-202:1-1032-pool 253:2\t0\t10G  0 dm\n└─vg--docker-metadata\t   253:1\t0\t 4G  0 lvm\n  └─docker-202:1-1032-pool 253:2\t0\t10G  0 dm\n</pre> <p>The diagram below shows the image from prior examples updated with the detail from the <code>lsblk</code> command above.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/lsblk-diagram.jpg\" alt=\"\"></p> <p>In the diagram, the pool is named <code>Docker-202:1-1032-pool</code> and spans the <code>data</code> and <code>metadata</code> devices created earlier. The <code>devicemapper</code> constructs the pool name as follows:</p> <pre>Docker-MAJ:MIN-INO-pool\n</pre> <p><code>MAJ</code>, <code>MIN</code> and <code>INO</code> refer to the major and minor device numbers and inode.</p> <p>Because Device Mapper operates at the block level it is more difficult to see diffs between image layers and containers. Docker 1.10 and later no longer matches image layer IDs with directory names in <code>/var/lib/docker</code>. However, there are two key directories. The <code>/var/lib/docker/devicemapper/mnt</code> directory contains the mount points for image and container layers. The <code>/var/lib/docker/devicemapper/metadata</code>directory contains one file for every image layer and container snapshot. The files contain metadata about each snapshot in JSON format.</p> <h2 id=\"increase-capacity-on-a-running-device\">Increase capacity on a running device</h2> <p>You can increase the capacity of the pool on a running thin-pool device. This is useful if the data’s logical volume is full and the volume group is at full capacity.</p> <h3 id=\"for-a-loop-lvm-configuration\">For a loop-lvm configuration</h3> <p>In this scenario, the thin pool is configured to use <code>loop-lvm</code> mode. To show the specifics of the existing configuration use <code>docker info</code>:</p> <pre>$ sudo docker info\nContainers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 2\nServer Version: 1.11.0-rc2\nStorage Driver: devicemapper\n Pool Name: docker-8:1-123141-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: ext4\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 1.202 GB\n Data Space Total: 107.4 GB\n Data Space Available: 4.506 GB\n Metadata Space Used: 1.729 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.146 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\n Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n WARNING: Usage of loopback devices is strongly discouraged for production use. Either use `--storage-opt dm.thinpooldev` or use `--storage-opt dm.no_warn_on_loop_devices=true` to suppress this warning.\n Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n Library Version: 1.02.90 (2014-09-01)\nLogging Driver: json-file\n[...]\n</pre> <p>The <code>Data Space</code> values show that the pool is 100GB total. This example extends the pool to 200GB.</p> <ol> <li>\n<p>List the sizes of the devices.</p> <pre>$ sudo ls -lh /var/lib/docker/devicemapper/devicemapper/\ntotal 1175492\n-rw------- 1 root root 100G Mar 30 05:22 data\n-rw------- 1 root root 2.0G Mar 31 11:17 metadata\n</pre>\n</li> <li>\n<p>Truncate <code>data</code> file to the size of the <code>metadata</code> file (approximage 200GB).</p> <pre>$ sudo truncate -s 214748364800 /var/lib/docker/devicemapper/devicemapper/data\n</pre>\n</li> <li>\n<p>Verify the file size changed.</p> <pre>$ sudo ls -lh /var/lib/docker/devicemapper/devicemapper/\ntotal 1.2G\n-rw------- 1 root root 200G Apr 14 08:47 data\n-rw------- 1 root root 2.0G Apr 19 13:27 metadata\n</pre>\n</li> <li>\n<p>Reload data loop device</p> <pre>$ sudo blockdev --getsize64 /dev/loop0\n107374182400\n$ sudo losetup -c /dev/loop0\n$ sudo blockdev --getsize64 /dev/loop0\n214748364800\n</pre>\n</li> <li>\n<p>Reload devicemapper thin pool.</p> <p>a. Get the pool name first.</p> <pre>$ sudo dmsetup status | grep pool\ndocker-8:1-123141-pool: 0 209715200 thin-pool 91\n422/524288 18338/1638400 - rw discard_passdown queue_if_no_space -\n</pre> <p>The name is the string before the colon.</p> <p>b. Dump the device mapper table first.</p> <pre>$ sudo dmsetup table docker-8:1-123141-pool\n0 209715200 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing\n</pre> <p>c. Calculate the real total sectors of the thin pool now.</p> <p>Change the second number of the table info (i.e. the disk end sector) to reflect the new number of 512 byte sectors in the disk. For example, as the new loop size is 200GB, change the second number to 419430400.</p> <p>d. Reload the thin pool with the new sector number</p> <pre>$ sudo dmsetup suspend docker-8:1-123141-pool \\\n    &amp;&amp; sudo dmsetup reload docker-8:1-123141-pool --table '0 419430400 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing' \\\n    &amp;&amp; sudo dmsetup resume docker-8:1-123141-pool\n</pre>\n</li> </ol> <h4 id=\"the-device-tool\">The device_tool</h4> <p>The Docker’s projects <code>contrib</code> directory contains not part of the core distribution. These tools that are often useful but can also be out-of-date. <a href=\"https://goo.gl/wNfDTi\">In this directory, is the <code>device_tool.go</code></a> which you can also resize the loop-lvm thin pool.</p> <p>To use the tool, compile it first. Then, do the following to resize the pool:</p> <pre>$ ./device_tool resize 200GB\n</pre> <h3 id=\"for-a-direct-lvm-mode-configuration\">For a direct-lvm mode configuration</h3> <p>In this example, you extend the capacity of a running device that uses the <code>direct-lvm</code> configuration. This example assumes you are using the <code>/dev/sdh1</code> disk partition.</p> <ol> <li>\n<p>Extend the volume group (VG) <code>vg-docker</code>.</p> <pre>$ sudo vgextend vg-docker /dev/sdh1\nVolume group \"vg-docker\" successfully extended\n</pre> <p>Your volume group may use a different name.</p>\n</li> <li>\n<p>Extend the <code>data</code> logical volume(LV) <code>vg-docker/data</code></p> <pre>$ sudo lvextend  -l+100%FREE -n vg-docker/data\nExtending logical volume data to 200 GiB\nLogical volume data successfully resized\n</pre>\n</li> <li>\n<p>Reload devicemapper thin pool.</p> <p>a. Get the pool name.</p> <pre>$ sudo dmsetup status | grep pool\ndocker-253:17-1835016-pool: 0 96460800 thin-pool 51593 6270/1048576 701943/753600 - rw no_discard_passdown queue_if_no_space\n</pre> <p>The name is the string before the colon.</p> <p>b. Dump the device mapper table.</p> <pre>$ sudo dmsetup table docker-253:17-1835016-pool\n0 96460800 thin-pool 252:0 252:1 128 32768 1 skip_block_zeroing\n</pre> <p>c. Calculate the real total sectors of the thin pool now. we can use <code>blockdev</code> to get the real size of data lv.</p> <p>Change the second number of the table info (i.e. the number of sectors) to reflect the new number of 512 byte sectors in the disk. For example, as the new data <code>lv</code> size is <code>264132100096</code> bytes, change the second number to <code>515883008</code>.</p> <pre>$ sudo blockdev --getsize64 /dev/vg-docker/data\n264132100096\n</pre> <p>d. Then reload the thin pool with the new sector number.</p> <pre>$ sudo dmsetup suspend docker-253:17-1835016-pool \\\n    &amp;&amp; sudo dmsetup reload docker-253:17-1835016-pool --table  '0 515883008 thin-pool 252:0 252:1 128 32768 1 skip_block_zeroing' \\\n    &amp;&amp; sudo dmsetup resume docker-253:17-1835016-pool\n</pre>\n</li> </ol> <h2 id=\"device-mapper-and-docker-performance\">Device Mapper and Docker performance</h2> <p>It is important to understand the impact that allocate-on-demand and copy-on-write operations can have on overall container performance.</p> <h3 id=\"allocate-on-demand-performance-impact\">Allocate-on-demand performance impact</h3> <p>The <code>devicemapper</code> storage driver allocates new blocks to a container via an allocate-on-demand operation. This means that each time an app writes to somewhere new inside a container, one or more empty blocks has to be located from the pool and mapped into the container.</p> <p>All blocks are 64KB. A write that uses less than 64KB still results in a single 64KB block being allocated. Writing more than 64KB of data uses multiple 64KB blocks. This can impact container performance, especially in containers that perform lots of small writes. However, once a block is allocated to a container subsequent reads and writes can operate directly on that block.</p> <h3 id=\"copy-on-write-performance-impact\">Copy-on-write performance impact</h3> <p>Each time a container updates existing data for the first time, the <code>devicemapper</code> storage driver has to perform a copy-on-write operation. This copies the data from the image snapshot to the container’s snapshot. This process can have a noticeable impact on container performance.</p> <p>All copy-on-write operations have a 64KB granularity. As a results, updating 32KB of a 1GB file causes the driver to copy a single 64KB block into the container’s snapshot. This has obvious performance advantages over file-level copy-on-write operations which would require copying the entire 1GB file into the container layer.</p> <p>In practice, however, containers that perform lots of small block writes (&lt;64KB) can perform worse with <code>devicemapper</code> than with AUFS.</p> <h3 id=\"other-device-mapper-performance-considerations\">Other device mapper performance considerations</h3> <p>There are several other things that impact the performance of the <code>devicemapper</code> storage driver.</p> <ul> <li><p><strong>The mode.</strong> The default mode for Docker running the <code>devicemapper</code> storage driver is <code>loop-lvm</code>. This mode uses sparse files and suffers from poor performance. It is <strong>not recommended for production</strong>. The recommended mode for production environments is <code>direct-lvm</code> where the storage driver writes directly to raw block devices.</p></li> <li><p><strong>High speed storage.</strong> For best performance you should place the <code>Data file</code> and <code>Metadata file</code> on high speed storage such as SSD. This can be direct attached storage or from a SAN or NAS array.</p></li> <li><p><strong>Memory usage.</strong> <code>devicemapper</code> is not the most memory efficient Docker storage driver. Launching <em>n</em> copies of the same container loads <em>n</em> copies of its files into memory. This can have a memory impact on your Docker host. As a result, the <code>devicemapper</code> storage driver may not be the best choice for PaaS and other high density use cases.</p></li> </ul> <p>One final point, data volumes provide the best and most predictable performance. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. For this reason, you should to place heavy write workloads on data volumes.</p> <h2 id=\"related-information\">Related Information</h2> <ul> <li><a href=\"../imagesandcontainers/index\">Understand images, containers, and storage drivers</a></li> <li><a href=\"../selectadriver/index\">Select a storage driver</a></li> <li><a href=\"../aufs-driver/index\">AUFS storage driver in practice</a></li> <li><a href=\"../btrfs-driver/index\">Btrfs storage driver in practice</a></li> <li><a href=\"../../../reference/commandline/daemon/index#storage-driver-options\">daemon reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/device-mapper-driver/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/storagedriver/device-mapper-driver/</a>\n  </p>\n</div>\n","engine/userguide/networking/dockernetworks/index":"<h1 id=\"understand-docker-container-networks\">Understand Docker container networks</h1> <p>To build web applications that act in concert but do so securely, use the Docker networks feature. Networks, by definition, provide complete isolation for containers. So, it is important to have control over the networks your applications run on. Docker container networks give you that control.</p> <p>This section provides an overview of the default networking behavior that Docker Engine delivers natively. It describes the type of networks created by default and how to create your own, user-defined networks. It also describes the resources required to create networks on a single host or across a cluster of hosts.</p> <h2 id=\"default-networks\">Default Networks</h2> <p>When you install Docker, it creates three networks automatically. You can list these networks using the <code>docker network ls</code> command:</p> <pre>$ docker network ls\nNETWORK ID          NAME                DRIVER\n7fca4eb8c647        bridge              bridge\n9f904ee27bf5        none                null\ncf03ee007fb4        host                host\n</pre> <p>Historically, these three networks are part of Docker’s implementation. When you run a container you can use the <code>--net</code> flag to specify which network you want to run a container on. These three networks are still available to you.</p> <p>The <code>bridge</code> network represents the <code>docker0</code> network present in all Docker installations. Unless you specify otherwise with the <code>docker run\n--net=&lt;NETWORK&gt;</code> option, the Docker daemon connects containers to this network by default. You can see this bridge as part of a host’s network stack by using the <code>ifconfig</code> command on the host.</p> <pre>$ ifconfig\ndocker0   Link encap:Ethernet  HWaddr 02:42:47:bc:3a:eb  \n          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:47ff:febc:3aeb/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:17 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:1100 (1.1 KB)  TX bytes:648 (648.0 B)\n</pre> <p>The <code>none</code> network adds a container to a container-specific network stack. That container lacks a network interface. Attaching to such a container and looking at its stack you see this:</p> <pre>$ docker attach nonenetcontainer\n\nroot@0cb243cd1293:/# cat /etc/hosts\n127.0.0.1\tlocalhost\n::1\tlocalhost ip6-localhost ip6-loopback\nfe00::0\tip6-localnet\nff00::0\tip6-mcastprefix\nff02::1\tip6-allnodes\nff02::2\tip6-allrouters\nroot@0cb243cd1293:/# ifconfig\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n\nroot@0cb243cd1293:/#\n</pre> <blockquote> <p><strong>Note</strong>: You can detach from the container and leave it running with <code>CTRL-p CTRL-q</code>.</p> </blockquote> <p>The <code>host</code> network adds a container on the hosts network stack. You’ll find the network configuration inside the container is identical to the host.</p> <p>With the exception of the <code>bridge</code> network, you really don’t need to interact with these default networks. While you can list and inspect them, you cannot remove them. They are required by your Docker installation. However, you can add your own user-defined networks and these you can remove when you no longer need them. Before you learn more about creating your own networks, it is worth looking at the default <code>bridge</code> network a bit.</p> <h3 id=\"the-default-bridge-network-in-detail\">The default bridge network in detail</h3> <p>The default <code>bridge</code> network is present on all Docker hosts. The <code>docker network inspect</code> command returns information about a network:</p> <pre>$ docker network inspect bridge\n[\n   {\n       \"Name\": \"bridge\",\n       \"Id\": \"f7ab26d71dbd6f557852c7156ae0574bbf62c42f539b50c8ebde0f728a253b6f\",\n       \"Scope\": \"local\",\n       \"Driver\": \"bridge\",\n       \"IPAM\": {\n           \"Driver\": \"default\",\n           \"Config\": [\n               {\n                   \"Subnet\": \"172.17.0.1/16\",\n                   \"Gateway\": \"172.17.0.1\"\n               }\n           ]\n       },\n       \"Containers\": {},\n       \"Options\": {\n           \"com.docker.network.bridge.default_bridge\": \"true\",\n           \"com.docker.network.bridge.enable_icc\": \"true\",\n           \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n           \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n           \"com.docker.network.bridge.name\": \"docker0\",\n           \"com.docker.network.driver.mtu\": \"9001\"\n       }\n   }\n]\n</pre> <p>The Engine automatically creates a <code>Subnet</code> and <code>Gateway</code> to the network. The <code>docker run</code> command automatically adds new containers to this network.</p> <pre>$ docker run -itd --name=container1 busybox\n3386a527aa08b37ea9232cbcace2d2458d49f44bb05a6b775fba7ddd40d8f92c\n\n$ docker run -itd --name=container2 busybox\n94447ca479852d29aeddca75c28f7104df3c3196d7b6d83061879e339946805c\n</pre> <p>Inspecting the <code>bridge</code> network again after starting two containers shows both newly launched containers in the network. Their ids show up in the “Containers” section of <code>docker network inspect</code>:</p> <pre>$ docker network inspect bridge\n{[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"f7ab26d71dbd6f557852c7156ae0574bbf62c42f539b50c8ebde0f728a253b6f\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.1/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Containers\": {\n            \"3386a527aa08b37ea9232cbcace2d2458d49f44bb05a6b775fba7ddd40d8f92c\": {\n                \"EndpointID\": \"647c12443e91faf0fd508b6edfe59c30b642abb60dfab890b4bdccee38750bc1\",\n                \"MacAddress\": \"02:42:ac:11:00:02\",\n                \"IPv4Address\": \"172.17.0.2/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"94447ca479852d29aeddca75c28f7104df3c3196d7b6d83061879e339946805c\": {\n                \"EndpointID\": \"b047d090f446ac49747d3c37d63e4307be745876db7f0ceef7b311cbba615f48\",\n                \"MacAddress\": \"02:42:ac:11:00:03\",\n                \"IPv4Address\": \"172.17.0.3/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"9001\"\n        }\n    }\n]\n</pre> <p>The <code>docker network inspect</code> command above shows all the connected containers and their network resources on a given network. Containers in this default network are able to communicate with each other using IP addresses. Docker does not support automatic service discovery on the default bridge network. If you want to communicate with container names in this default bridge network, you must connect the containers via the legacy <code>docker run --link</code> option.</p> <p>You can <code>attach</code> to a running <code>container</code> and investigate its configuration:</p> <pre>$ docker attach container1\n\nroot@0cb243cd1293:/# ifconfig\nifconfig\neth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02  \n          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:16 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:1296 (1.2 KiB)  TX bytes:648 (648.0 B)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n</pre> <p>Then use <code>ping</code> for about 3 seconds to test the connectivity of the containers on this <code>bridge</code> network.</p> <pre>root@0cb243cd1293:/# ping -w3 172.17.0.3\nPING 172.17.0.3 (172.17.0.3): 56 data bytes\n64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.096 ms\n64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.17.0.3: seq=2 ttl=64 time=0.074 ms\n\n--- 172.17.0.3 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 0.074/0.083/0.096 ms\n</pre> <p>Finally, use the <code>cat</code> command to check the <code>container1</code> network configuration:</p> <pre>root@0cb243cd1293:/# cat /etc/hosts\n172.17.0.2\t3386a527aa08\n127.0.0.1\tlocalhost\n::1\tlocalhost ip6-localhost ip6-loopback\nfe00::0\tip6-localnet\nff00::0\tip6-mcastprefix\nff02::1\tip6-allnodes\nff02::2\tip6-allrouters\n</pre> <p>To detach from a <code>container1</code> and leave it running use <code>CTRL-p CTRL-q</code>.Then, attach to <code>container2</code> and repeat these three commands.</p> <pre>$ docker attach container2\n\nroot@0cb243cd1293:/# ifconfig\neth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:03  \n          inet addr:172.17.0.3  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:acff:fe11:3/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:15 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:13 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:1166 (1.1 KiB)  TX bytes:1026 (1.0 KiB)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n\nroot@0cb243cd1293:/# ping -w3 172.17.0.2\nPING 172.17.0.2 (172.17.0.2): 56 data bytes\n64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.067 ms\n64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.075 ms\n64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.072 ms\n\n--- 172.17.0.2 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 0.067/0.071/0.075 ms\n/ # cat /etc/hosts\n172.17.0.3\t94447ca47985\n127.0.0.1\tlocalhost\n::1\tlocalhost ip6-localhost ip6-loopback\nfe00::0\tip6-localnet\nff00::0\tip6-mcastprefix\nff02::1\tip6-allnodes\nff02::2\tip6-allrouters\n</pre> <p>The default <code>docker0</code> bridge network supports the use of port mapping and <code>docker run --link</code> to allow communications between containers in the <code>docker0</code> network. These techniques are cumbersome to set up and prone to error. While they are still available to you as techniques, it is better to avoid them and define your own bridge networks instead.</p> <h2 id=\"user-defined-networks\">User-defined networks</h2> <p>You can create your own user-defined networks that better isolate containers. Docker provides some default <strong>network drivers</strong> for creating these networks. You can create a new <strong>bridge network</strong> or <strong>overlay network</strong>. You can also create a <strong>network plugin</strong> or <strong>remote network</strong> written to your own specifications.</p> <p>You can create multiple networks. You can add containers to more than one network. Containers can only communicate within networks but not across networks. A container attached to two networks can communicate with member containers in either network. When a container is connected to multiple networks, its external connectivity is provided via the first non-internal network, in lexical order.</p> <p>The next few sections describe each of Docker’s built-in network drivers in greater detail.</p> <h3 id=\"a-bridge-network\">A bridge network</h3> <p>The easiest user-defined network to create is a <code>bridge</code> network. This network is similar to the historical, default <code>docker0</code> network. There are some added features and some old features that aren’t available.</p> <pre>$ docker network create --driver bridge isolated_nw\n1196a4c5af43a21ae38ef34515b6af19236a3fc48122cf585e3f3054d509679b\n\n$ docker network inspect isolated_nw\n[\n    {\n        \"Name\": \"isolated_nw\",\n        \"Id\": \"1196a4c5af43a21ae38ef34515b6af19236a3fc48122cf585e3f3054d509679b\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.21.0.0/16\",\n                    \"Gateway\": \"172.21.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {},\n        \"Options\": {}\n    }\n]\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER\n9f904ee27bf5        none                null\ncf03ee007fb4        host                host\n7fca4eb8c647        bridge              bridge\nc5ee82f76de3        isolated_nw         bridge\n\n</pre> <p>After you create the network, you can launch containers on it using the <code>docker run --net=&lt;NETWORK&gt;</code> option.</p> <pre>$ docker run --net=isolated_nw -itd --name=container3 busybox\n8c1a0a5be480921d669a073393ade66a3fc49933f08bcc5515b37b8144f6d47c\n\n$ docker network inspect isolated_nw\n[\n    {\n        \"Name\": \"isolated_nw\",\n        \"Id\": \"1196a4c5af43a21ae38ef34515b6af19236a3fc48122cf585e3f3054d509679b\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {}\n            ]\n        },\n        \"Containers\": {\n            \"8c1a0a5be480921d669a073393ade66a3fc49933f08bcc5515b37b8144f6d47c\": {\n                \"EndpointID\": \"93b2db4a9b9a997beb912d28bcfc117f7b0eb924ff91d48cfa251d473e6a9b08\",\n                \"MacAddress\": \"02:42:ac:15:00:02\",\n                \"IPv4Address\": \"172.21.0.2/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {}\n    }\n]\n</pre> <p>The containers you launch into this network must reside on the same Docker host. Each container in the network can immediately communicate with other containers in the network. Though, the network itself isolates the containers from external networks.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/images/bridge_network.png\" alt=\"An isolated network\"></p> <p>Within a user-defined bridge network, linking is not supported. You can expose and publish container ports on containers in this network. This is useful if you want to make a portion of the <code>bridge</code> network available to an outside network.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/images/network_access.png\" alt=\"Bridge network\"></p> <p>A bridge network is useful in cases where you want to run a relatively small network on a single host. You can, however, create significantly larger networks by creating an <code>overlay</code> network.</p> <h3 id=\"an-overlay-network\">An overlay network</h3> <p>Docker’s <code>overlay</code> network driver supports multi-host networking natively out-of-the-box. This support is accomplished with the help of <code>libnetwork</code>, a built-in VXLAN-based overlay network driver, and Docker’s <code>libkv</code> library.</p> <p>The <code>overlay</code> network requires a valid key-value store service. Currently, Docker’s <code>libkv</code> supports Consul, Etcd, and ZooKeeper (Distributed store). Before creating a network you must install and configure your chosen key-value store service. The Docker hosts that you intend to network and the service must be able to communicate.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/images/key_value.png\" alt=\"Key-value store\"></p> <p>Each host in the network must run a Docker Engine instance. The easiest way to provision the hosts are with Docker Machine.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/images/engine_on_net.png\" alt=\"Engine on each host\"></p> <p>You should open the following ports between each of your hosts.</p> <table> <thead> <tr> <th>Protocol</th> <th>Port</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>udp</td> <td>4789</td> <td>Data plane (VXLAN)</td> </tr> <tr> <td>tcp/udp</td> <td>7946</td> <td>Control plane</td> </tr> </tbody> </table> <p>Your key-value store service may require additional ports. Check your vendor’s documentation and open any required ports.</p> <p>Once you have several machines provisioned, you can use Docker Swarm to quickly form them into a swarm which includes a discovery service as well.</p> <p>To create an overlay network, you configure options on the <code>daemon</code> on each Docker Engine for use with <code>overlay</code> network. There are three options to set:</p> <table> <thead> <tr> <th>Option</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><pre>--cluster-store=PROVIDER://URL</pre></td> <td>Describes the location of the KV service.</td> </tr> <tr> <td><pre>--cluster-advertise=HOST_IP|HOST_IFACE:PORT</pre></td> <td>The IP address or interface of the HOST used for clustering.</td> </tr> <tr> <td><pre>--cluster-store-opt=KEY-VALUE OPTIONS</pre></td> <td>Options such as TLS certificate or tuning discovery Timers</td> </tr> </tbody> </table> <p>Create an <code>overlay</code> network on one of the machines in the Swarm.</p> <pre>$ docker network create --driver overlay my-multi-host-network\n</pre> <p>This results in a single network spanning multiple hosts. An <code>overlay</code> network provides complete isolation for the containers.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/images/overlay_network.png\" alt=\"An overlay network\"></p> <p>Then, on each host, launch containers making sure to specify the network name.</p> <pre>$ docker run -itd --net=my-multi-host-network busybox\n</pre> <p>Once connected, each container has access to all the containers in the network regardless of which Docker host the container was launched on.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/images/overlay-network-final.png\" alt=\"Published port\"></p> <p>If you would like to try this for yourself, see the <a href=\"../get-started-overlay/index\">Getting started for overlay</a>.</p> <h3 id=\"custom-network-plugin\">Custom network plugin</h3> <p>If you like, you can write your own network driver plugin. A network driver plugin makes use of Docker’s plugin infrastructure. In this infrastructure, a plugin is a process running on the same Docker host as the Docker <code>daemon</code>.</p> <p>Network plugins follow the same restrictions and installation rules as other plugins. All plugins make use of the plugin API. They have a lifecycle that encompasses installation, starting, stopping and activation.</p> <p>Once you have created and installed a custom network driver, you use it like the built-in network drivers. For example:</p> <pre>$ docker network create --driver weave mynet\n</pre> <p>You can inspect it, add containers to and from it, and so forth. Of course, different plugins may make use of different technologies or frameworks. Custom networks can include features not present in Docker’s default networks. For more information on writing plugins, see <a href=\"../../../extend/index\">Extending Docker</a> and <a href=\"../../../extend/plugins_network/index\">Writing a network driver plugin</a>.</p> <h3 id=\"docker-embedded-dns-server\">Docker embedded DNS server</h3> <p>Docker daemon runs an embedded DNS server to provide automatic service discovery for containers connected to user defined networks. Name resolution requests from the containers are handled first by the embedded DNS server. If the embedded DNS server is unable to resolve the request it will be forwarded to any external DNS servers configured for the container. To facilitate this when the container is created, only the embedded DNS server reachable at <code>127.0.0.11</code> will be listed in the container’s <code>resolv.conf</code> file. More information on embedded DNS server on user-defined networks can be found in the <a href=\"../configure-dns/index\">embedded DNS server in user-defined networks</a></p> <h2 id=\"links\">Links</h2> <p>Before the Docker network feature, you could use the Docker link feature to allow containers to discover each other. With the introduction of Docker networks, containers can be discovered by its name automatically. But you can still create links but they behave differently when used in the default <code>docker0</code> bridge network compared to user-defined networks. For more information, please refer to <a href=\"../default_network/dockerlinks/index\">Legacy Links</a> for link feature in default <code>bridge</code> network and the <a href=\"../work-with-networks/index#linking-containers-in-user-defined-networks\">linking containers in user-defined networks</a> for links functionality in user-defined networks.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../work-with-networks/index\">Work with network commands</a></li> <li><a href=\"../get-started-overlay/index\">Get started with multi-host networking</a></li> <li><a href=\"../../containers/dockervolumes/index\">Managing Data in Containers</a></li> <li><a href=\"https://docs.docker.com/machine\">Docker Machine overview</a></li> <li><a href=\"https://docs.docker.com/swarm\">Docker Swarm overview</a></li> <li><a href=\"https://github.com/docker/libnetwork\">Investigate the LibNetwork project</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/dockernetworks/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/dockernetworks/</a>\n  </p>\n</div>\n","engine/userguide/labels-custom-metadata/index":"<h1 id=\"apply-custom-metadata\">Apply custom metadata</h1> <p>You can apply metadata to your images, containers, or daemons via labels. Labels serve a wide range of uses, such as adding notes or licensing information to an image, or to identify a host.</p> <p>A label is a <code>&lt;key&gt;</code> / <code>&lt;value&gt;</code> pair. Docker stores the label values as <em>strings</em>. You can specify multiple labels but each <code>&lt;key&gt;</code> must be unique or the value will be overwritten. If you specify the same <code>key</code> several times but with different values, newer labels overwrite previous labels. Docker uses the last <code>key=value</code> you supply.</p> <blockquote> <p><strong>Note:</strong> Support for daemon-labels was added in Docker 1.4.1. Labels on containers and images were added in Docker 1.6.0</p> </blockquote> <h2 id=\"label-keys-namespaces\">Label keys (namespaces)</h2> <p>Docker puts no hard restrictions on the <code>key</code> used for a label. However, using simple keys can easily lead to conflicts. For example, you have chosen to categorize your images by CPU architecture using “architecture” labels in your Dockerfiles:</p> <pre>LABEL architecture=\"amd64\"\n\nLABEL architecture=\"ARMv7\"\n</pre> <p>Another user may apply the same label based on a building’s “architecture”:</p> <pre>LABEL architecture=\"Art Nouveau\"\n</pre> <p>To prevent naming conflicts, Docker recommends using namespaces to label keys using reverse domain notation. Use the following guidelines to name your keys:</p> <ul> <li><p>All (third-party) tools should prefix their keys with the reverse DNS notation of a domain controlled by the author. For example, <code>com.example.some-label</code>.</p></li> <li><p>The <code>com.docker.*</code>, <code>io.docker.*</code> and <code>org.dockerproject.*</code> namespaces are reserved for Docker’s internal use.</p></li> <li><p>Keys should only consist of lower-cased alphanumeric characters, dots and dashes (for example, <code>[a-z0-9-.]</code>).</p></li> <li><p>Keys should start <em>and</em> end with an alpha numeric character.</p></li> <li><p>Keys may not contain consecutive dots or dashes.</p></li> <li><p>Keys <em>without</em> namespace (dots) are reserved for CLI use. This allows end- users to add metadata to their containers and images without having to type cumbersome namespaces on the command-line.</p></li> </ul> <p>These are simply guidelines and Docker does not <em>enforce</em> them. However, for the benefit of the community, you <em>should</em> use namespaces for your label keys.</p> <h2 id=\"store-structured-data-in-labels\">Store structured data in labels</h2> <p>Label values can contain any data type as long as it can be represented as a string. For example, consider this JSON document:</p> <pre>{\n    \"Description\": \"A containerized foobar\",\n    \"Usage\": \"docker run --rm example/foobar [args]\",\n    \"License\": \"GPL\",\n    \"Version\": \"0.0.1-beta\",\n    \"aBoolean\": true,\n    \"aNumber\" : 0.01234,\n    \"aNestedArray\": [\"a\", \"b\", \"c\"]\n}\n</pre> <p>You can store this struct in a label by serializing it to a string first:</p> <pre>LABEL com.example.image-specs=\"{\\\"Description\\\":\\\"A containerized foobar\\\",\\\"Usage\\\":\\\"docker run --rm example\\\\/foobar [args]\\\",\\\"License\\\":\\\"GPL\\\",\\\"Version\\\":\\\"0.0.1-beta\\\",\\\"aBoolean\\\":true,\\\"aNumber\\\":0.01234,\\\"aNestedArray\\\":[\\\"a\\\",\\\"b\\\",\\\"c\\\"]}\"\n</pre> <p>While it is <em>possible</em> to store structured data in label values, Docker treats this data as a ‘regular’ string. This means that Docker doesn’t offer ways to query (filter) based on nested properties. If your tool needs to filter on nested properties, the tool itself needs to implement this functionality.</p> <h2 id=\"add-labels-to-images\">Add labels to images</h2> <p>To add labels to an image, use the <code>LABEL</code> instruction in your Dockerfile:</p> <pre>LABEL [&lt;namespace&gt;.]&lt;key&gt;=&lt;value&gt; ...\n</pre> <p>The <code>LABEL</code> instruction adds a label to your image. A <code>LABEL</code> consists of a <code>&lt;key&gt;</code> and a <code>&lt;value&gt;</code>. Use an empty string for labels that don’t have a <code>&lt;value&gt;</code>, Use surrounding quotes or backslashes for labels that contain white space characters in the <code>&lt;value&gt;</code>:</p> <pre>LABEL vendor=ACME\\ Incorporated\nLABEL com.example.version.is-beta=\nLABEL com.example.version.is-production=\"\"\nLABEL com.example.version=\"0.0.1-beta\"\nLABEL com.example.release-date=\"2015-02-12\"\n</pre> <p>The <code>LABEL</code> instruction also supports setting multiple <code>&lt;key&gt;</code> / <code>&lt;value&gt;</code> pairs in a single instruction:</p> <pre>LABEL com.example.version=\"0.0.1-beta\" com.example.release-date=\"2015-02-12\"\n</pre> <p>Long lines can be split up by using a backslash (<code>\\</code>) as continuation marker:</p> <pre>LABEL vendor=ACME\\ Incorporated \\\n      com.example.is-beta= \\\n      com.example.is-production=\"\" \\\n      com.example.version=\"0.0.1-beta\" \\\n      com.example.release-date=\"2015-02-12\"\n</pre> <p>Docker recommends you add multiple labels in a single <code>LABEL</code> instruction. Using individual instructions for each label can result in an inefficient image. This is because each <code>LABEL</code> instruction in a Dockerfile produces a new IMAGE layer.</p> <p>You can view the labels via the <code>docker inspect</code> command:</p> <pre>$ docker inspect 4fa6e0f0c678\n\n...\n\"Labels\": {\n    \"vendor\": \"ACME Incorporated\",\n    \"com.example.is-beta\": \"\",\n    \"com.example.is-production\": \"\",\n    \"com.example.version\": \"0.0.1-beta\",\n    \"com.example.release-date\": \"2015-02-12\"\n}\n...\n\n# Inspect labels on container\n$ docker inspect -f \"{{json .Config.Labels }}\" 4fa6e0f0c678\n\n{\"Vendor\":\"ACME Incorporated\",\"com.example.is-beta\":\"\", \"com.example.is-production\":\"\", \"com.example.version\":\"0.0.1-beta\",\"com.example.release-date\":\"2015-02-12\"}\n\n# Inspect labels on images\n$ docker inspect -f \"{{json .ContainerConfig.Labels }}\" myimage\n</pre> <h2 id=\"query-labels\">Query labels</h2> <p>Besides storing metadata, you can filter images and containers by label. To list all running containers that have the <code>com.example.is-beta</code> label:</p> <pre># List all running containers that have a `com.example.is-beta` label\n$ docker ps --filter \"label=com.example.is-beta\"\n</pre> <p>List all running containers with the label <code>color</code> that have a value <code>blue</code>:</p> <pre>$ docker ps --filter \"label=color=blue\"\n</pre> <p>List all images with the label <code>vendor</code> that have the value <code>ACME</code>:</p> <pre>$ docker images --filter \"label=vendor=ACME\"\n</pre> <h2 id=\"container-labels\">Container labels</h2> <pre>docker run \\\n   -d \\\n   --label com.example.group=\"webservers\" \\\n   --label com.example.environment=\"production\" \\\n   busybox \\\n   top\n</pre> <p>Please refer to the <a href=\"#query-labels\">Query labels</a> section above for information on how to query labels set on a container.</p> <h2 id=\"daemon-labels\">Daemon labels</h2> <pre>docker daemon \\\n  --dns 8.8.8.8 \\\n  --dns 8.8.4.4 \\\n  -H unix:///var/run/docker.sock \\\n  --label com.example.environment=\"production\" \\\n  --label com.example.storage=\"ssd\"\n</pre> <p>These labels appear as part of the <code>docker info</code> output for the daemon:</p> <pre>$ docker -D info\nContainers: 12\n Running: 5\n Paused: 2\n Stopped: 5\nImages: 672\nServer Version: 1.9.0\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 697\n Dirperm1 Supported: true\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 3.19.0-22-generic\nOperating System: Ubuntu 15.04\nCPUs: 24\nTotal Memory: 62.86 GiB\nName: docker\nID: I54V:OLXT:HVMM:TPKO:JPHQ:CQCD:JNLC:O3BZ:4ZVJ:43XJ:PFHZ:6N2S\nDebug mode (server): true\n File Descriptors: 59\n Goroutines: 159\n System Time: 2015-09-23T14:04:20.699842089+08:00\n EventsListeners: 0\n Init SHA1:\n Init Path: /usr/bin/docker\n Docker Root Dir: /var/lib/docker\n Http Proxy: http://test:test@localhost:8080\n Https Proxy: https://test:test@localhost:8080\nWARNING: No swap limit support\nUsername: svendowideit\nRegistry: [https://index.docker.io/v1/]\nLabels:\n com.example.environment=production\n com.example.storage=ssd\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/labels-custom-metadata/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/labels-custom-metadata/</a>\n  </p>\n</div>\n","engine/userguide/networking/work-with-networks/index":"<h1 id=\"work-with-network-commands\">Work with network commands</h1> <p>This article provides examples of the network subcommands you can use to interact with Docker networks and the containers in them. The commands are available through the Docker Engine CLI. These commands are:</p> <ul> <li><code>docker network create</code></li> <li><code>docker network connect</code></li> <li><code>docker network ls</code></li> <li><code>docker network rm</code></li> <li><code>docker network disconnect</code></li> <li><code>docker network inspect</code></li> </ul> <p>While not required, it is a good idea to read <a href=\"../dockernetworks/index\">Understanding Docker network</a> before trying the examples in this section. The examples for the rely on a <code>bridge</code> network so that you can try them immediately. If you would prefer to experiment with an <code>overlay</code> network see the <a href=\"../get-started-overlay/index\">Getting started with multi-host networks</a> instead.</p> <h2 id=\"create-networks\">Create networks</h2> <p>Docker Engine creates a <code>bridge</code> network automatically when you install Engine. This network corresponds to the <code>docker0</code> bridge that Engine has traditionally relied on. In addition to this network, you can create your own <code>bridge</code> or <code>overlay</code> network.</p> <p>A <code>bridge</code> network resides on a single host running an instance of Docker Engine. An <code>overlay</code> network can span multiple hosts running their own engines. If you run <code>docker network create</code> and supply only a network name, it creates a bridge network for you.</p> <pre>$ docker network create simple-network\n69568e6336d8c96bbf57869030919f7c69524f71183b44d80948bd3927c87f6a\n$ docker network inspect simple-network\n[\n    {\n        \"Name\": \"simple-network\",\n        \"Id\": \"69568e6336d8c96bbf57869030919f7c69524f71183b44d80948bd3927c87f6a\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.22.0.0/16\",\n                    \"Gateway\": \"172.22.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {},\n        \"Options\": {}\n    }\n]\n</pre> <p>Unlike <code>bridge</code> networks, <code>overlay</code> networks require some pre-existing conditions before you can create one. These conditions are:</p> <ul> <li>Access to a key-value store. Engine supports Consul, Etcd, and ZooKeeper (Distributed store) key-value stores.</li> <li>A cluster of hosts with connectivity to the key-value store.</li> <li>A properly configured Engine <code>daemon</code> on each host in the swarm.</li> </ul> <p>The <code>docker daemon</code> options that support the <code>overlay</code> network are:</p> <ul> <li><code>--cluster-store</code></li> <li><code>--cluster-store-opt</code></li> <li><code>--cluster-advertise</code></li> </ul> <p>It is also a good idea, though not required, that you install Docker Swarm to manage the cluster. Swarm provides sophisticated discovery and server management that can assist your implementation.</p> <p>When you create a network, Engine creates a non-overlapping subnetwork for the network by default. You can override this default and specify a subnetwork directly using the <code>--subnet</code> option. On a <code>bridge</code> network you can only specify a single subnet. An <code>overlay</code> network supports multiple subnets.</p> <blockquote> <p><strong>Note</strong> : It is highly recommended to use the <code>--subnet</code> option while creating a network. If the <code>--subnet</code> is not specified, the docker daemon automatically chooses and assigns a subnet for the network and it could overlap with another subnet in your infrastructure that is not managed by docker. Such overlaps can cause connectivity issues or failures when containers are connected to that network.</p> </blockquote> <p>In addition to the <code>--subnet</code> option, you also specify the <code>--gateway</code> <code>--ip-range</code> and <code>--aux-address</code> options.</p> <pre>$ docker network create -d overlay\n  --subnet=192.168.0.0/16 --subnet=192.170.0.0/16\n  --gateway=192.168.0.100 --gateway=192.170.0.100\n  --ip-range=192.168.1.0/24\n  --aux-address a=192.168.1.5 --aux-address b=192.168.1.6\n  --aux-address a=192.170.1.5 --aux-address b=192.170.1.6\n  my-multihost-network\n</pre> <p>Be sure that your subnetworks do not overlap. If they do, the network create fails and Engine returns an error.</p> <p>When creating a custom network, the default network driver (i.e. <code>bridge</code>) has additional options that can be passed. The following are those options and the equivalent docker daemon flags used for docker0 bridge:</p> <table> <thead> <tr> <th>Option</th> <th>Equivalent</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>com.docker.network.bridge.name</code></td> <td>-</td> <td>bridge name to be used when creating the Linux bridge</td> </tr> <tr> <td><code>com.docker.network.bridge.enable_ip_masquerade</code></td> <td><code>--ip-masq</code></td> <td>Enable IP masquerading</td> </tr> <tr> <td><code>com.docker.network.bridge.enable_icc</code></td> <td><code>--icc</code></td> <td>Enable or Disable Inter Container Connectivity</td> </tr> <tr> <td><code>com.docker.network.bridge.host_binding_ipv4</code></td> <td><code>--ip</code></td> <td>Default IP when binding container ports</td> </tr> <tr> <td><code>com.docker.network.mtu</code></td> <td><code>--mtu</code></td> <td>Set the containers network MTU</td> </tr> </tbody> </table> <p>The following arguments can be passed to <code>docker network create</code> for any network driver.</p> <table> <thead> <tr> <th>Argument</th> <th>Equivalent</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>--internal</code></td> <td>-</td> <td>Restricts external access to the network</td> </tr> <tr> <td><code>--ipv6</code></td> <td><code>--ipv6</code></td> <td>Enable IPv6 networking</td> </tr> </tbody> </table> <p>For example, now let’s use <code>-o</code> or <code>--opt</code> options to specify an IP address binding when publishing ports:</p> <pre>$ docker network create -o \"com.docker.network.bridge.host_binding_ipv4\"=\"172.23.0.1\" my-network\nb1a086897963e6a2e7fc6868962e55e746bee8ad0c97b54a5831054b5f62672a\n$ docker network inspect my-network\n[\n    {\n        \"Name\": \"my-network\",\n        \"Id\": \"b1a086897963e6a2e7fc6868962e55e746bee8ad0c97b54a5831054b5f62672a\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.23.0.0/16\",\n                    \"Gateway\": \"172.23.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {},\n        \"Options\": {\n            \"com.docker.network.bridge.host_binding_ipv4\": \"172.23.0.1\"\n        }\n    }\n]\n$ docker run -d -P --name redis --net my-network redis\nbafb0c808c53104b2c90346f284bda33a69beadcab4fc83ab8f2c5a4410cd129\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                        NAMES\nbafb0c808c53        redis               \"/entrypoint.sh redis\"   4 seconds ago       Up 3 seconds        172.23.0.1:32770-&gt;6379/tcp   redis\n</pre> <h2 id=\"connect-containers\">Connect containers</h2> <p>You can connect containers dynamically to one or more networks. These networks can be backed the same or different network drivers. Once connected, the containers can communicate using another container’s IP address or name.</p> <p>For <code>overlay</code> networks or custom plugins that support multi-host connectivity, containers connected to the same multi-host network but launched from different hosts can also communicate in this way.</p> <p>Create two containers for this example:</p> <pre>$ docker run -itd --name=container1 busybox\n18c062ef45ac0c026ee48a83afa39d25635ee5f02b58de4abc8f467bcaa28731\n\n$ docker run -itd --name=container2 busybox\n498eaaaf328e1018042c04b2de04036fc04719a6e39a097a4f4866043a2c2152\n</pre> <p>Then create an isolated, <code>bridge</code> network to test with.</p> <pre>$ docker network create -d bridge --subnet 172.25.0.0/16 isolated_nw\n06a62f1c73c4e3107c0f555b7a5f163309827bfbbf999840166065a8f35455a8\n</pre> <p>Connect <code>container2</code> to the network and then <code>inspect</code> the network to verify the connection:</p> <pre>$ docker network connect isolated_nw container2\n$ docker network inspect isolated_nw\n[\n    {\n        \"Name\": \"isolated_nw\",\n        \"Id\": \"06a62f1c73c4e3107c0f555b7a5f163309827bfbbf999840166065a8f35455a8\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.21.0.0/16\",\n                    \"Gateway\": \"172.21.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {\n            \"90e1f3ec71caf82ae776a827e0712a68a110a3f175954e5bd4222fd142ac9428\": {\n                \"Name\": \"container2\",\n                \"EndpointID\": \"11cedac1810e864d6b1589d92da12af66203879ab89f4ccd8c8fdaa9b1c48b1d\",\n                \"MacAddress\": \"02:42:ac:19:00:02\",\n                \"IPv4Address\": \"172.25.0.2/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {}\n    }\n]\n</pre> <p>You can see that the Engine automatically assigns an IP address to <code>container2</code>. Given we specified a <code>--subnet</code> when creating the network, Engine picked an address from that same subnet. Now, start a third container and connect it to the network on launch using the <code>docker run</code> command’s <code>--net</code> option:</p> <pre>$ docker run --net=isolated_nw --ip=172.25.3.3 -itd --name=container3 busybox\n467a7863c3f0277ef8e661b38427737f28099b61fa55622d6c30fb288d88c551\n</pre> <p>As you can see you were able to specify the ip address for your container. As long as the network to which the container is connecting was created with a user specified subnet, you will be able to select the IPv4 and/or IPv6 address(es) for your container when executing <code>docker run</code> and <code>docker network connect</code> commands by respectively passing the <code>--ip</code> and <code>--ip6</code> flags for IPv4 and IPv6. The selected IP address is part of the container networking configuration and will be preserved across container reload. The feature is only available on user defined networks, because they guarantee their subnets configuration does not change across daemon reload.</p> <p>Now, inspect the network resources used by <code>container3</code>.</p> <pre>$ docker inspect --format='{{json .NetworkSettings.Networks}}'  container3\n{\"isolated_nw\":{\"IPAMConfig\":{\"IPv4Address\":\"172.25.3.3\"},\"NetworkID\":\"1196a4c5af43a21ae38ef34515b6af19236a3fc48122cf585e3f3054d509679b\",\n\"EndpointID\":\"dffc7ec2915af58cc827d995e6ebdc897342be0420123277103c40ae35579103\",\"Gateway\":\"172.25.0.1\",\"IPAddress\":\"172.25.3.3\",\"IPPrefixLen\":16,\"IPv6Gateway\":\"\",\"GlobalIPv6Address\":\"\",\"GlobalIPv6PrefixLen\":0,\"MacAddress\":\"02:42:ac:19:03:03\"}}\n</pre> <p>Repeat this command for <code>container2</code>. If you have Python installed, you can pretty print the output.</p> <pre>$ docker inspect --format='{{json .NetworkSettings.Networks}}'  container2 | python -m json.tool\n{\n    \"bridge\": {\n        \"NetworkID\":\"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n        \"EndpointID\": \"0099f9efb5a3727f6a554f176b1e96fca34cae773da68b3b6a26d046c12cb365\",\n        \"Gateway\": \"172.17.0.1\",\n        \"GlobalIPv6Address\": \"\",\n        \"GlobalIPv6PrefixLen\": 0,\n        \"IPAMConfig\": null,\n        \"IPAddress\": \"172.17.0.3\",\n        \"IPPrefixLen\": 16,\n        \"IPv6Gateway\": \"\",\n        \"MacAddress\": \"02:42:ac:11:00:03\"\n    },\n    \"isolated_nw\": {\n        \"NetworkID\":\"1196a4c5af43a21ae38ef34515b6af19236a3fc48122cf585e3f3054d509679b\",\n        \"EndpointID\": \"11cedac1810e864d6b1589d92da12af66203879ab89f4ccd8c8fdaa9b1c48b1d\",\n        \"Gateway\": \"172.25.0.1\",\n        \"GlobalIPv6Address\": \"\",\n        \"GlobalIPv6PrefixLen\": 0,\n        \"IPAMConfig\": null,\n        \"IPAddress\": \"172.25.0.2\",\n        \"IPPrefixLen\": 16,\n        \"IPv6Gateway\": \"\",\n        \"MacAddress\": \"02:42:ac:19:00:02\"\n    }\n}\n</pre> <p>You should find <code>container2</code> belongs to two networks. The <code>bridge</code> network which it joined by default when you launched it and the <code>isolated_nw</code> which you later connected it to.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/images/working.png\" alt=\"\"></p> <p>In the case of <code>container3</code>, you connected it through <code>docker run</code> to the <code>isolated_nw</code> so that container is not connected to <code>bridge</code>.</p> <p>Use the <code>docker attach</code> command to connect to the running <code>container2</code> and examine its networking stack:</p> <pre>$ docker attach container2\n</pre> <p>If you look at the container’s network stack you should see two Ethernet interfaces, one for the default bridge network and one for the <code>isolated_nw</code> network.</p> <pre>/ # ifconfig\neth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:03  \n          inet addr:172.17.0.3  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:acff:fe11:3/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:8 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)\n\neth1      Link encap:Ethernet  HWaddr 02:42:AC:15:00:02  \n          inet addr:172.25.0.2  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:acff:fe19:2/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:8 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n</pre> <p>On the <code>isolated_nw</code> which was user defined, the Docker embedded DNS server enables name resolution for other containers in the network. Inside of <code>container2</code> it is possible to ping <code>container3</code> by name.</p> <pre>/ # ping -w 4 container3\nPING container3 (172.25.3.3): 56 data bytes\n64 bytes from 172.25.3.3: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.25.3.3: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.25.3.3: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.25.3.3: seq=3 ttl=64 time=0.097 ms\n\n--- container3 ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n</pre> <p>This isn’t the case for the default <code>bridge</code> network. Both <code>container2</code> and <code>container1</code> are connected to the default bridge network. Docker does not support automatic service discovery on this network. For this reason, pinging <code>container1</code> by name fails as you would expect based on the <code>/etc/hosts</code> file:</p> <pre>/ # ping -w 4 container1\nping: bad address 'container1'\n</pre> <p>A ping using the <code>container1</code> IP address does succeed though:</p> <pre>/ # ping -w 4 172.17.0.2\nPING 172.17.0.2 (172.17.0.2): 56 data bytes\n64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.095 ms\n64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.075 ms\n64 bytes from 172.17.0.2: seq=2 ttl=64 time=0.072 ms\n64 bytes from 172.17.0.2: seq=3 ttl=64 time=0.101 ms\n\n--- 172.17.0.2 ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.072/0.085/0.101 ms\n</pre> <p>If you wanted you could connect <code>container1</code> to <code>container2</code> with the <code>docker\nrun --link</code> command and that would enable the two containers to interact by name as well as IP.</p> <p>Detach from a <code>container2</code> and leave it running using <code>CTRL-p CTRL-q</code>.</p> <p>In this example, <code>container2</code> is attached to both networks and so can talk to <code>container1</code> and <code>container3</code>. But <code>container3</code> and <code>container1</code> are not in the same network and cannot communicate. Test, this now by attaching to <code>container3</code> and attempting to ping <code>container1</code> by IP address.</p> <pre>$ docker attach container3\n/ # ping 172.17.0.2\nPING 172.17.0.2 (172.17.0.2): 56 data bytes\n^C\n--- 172.17.0.2 ping statistics ---\n10 packets transmitted, 0 packets received, 100% packet loss\n\n</pre> <p>You can connect both running and non-running containers to a network. However, <code>docker network inspect</code> only displays information on running containers.</p> <h3 id=\"linking-containers-in-user-defined-networks\">Linking containers in user-defined networks</h3> <p>In the above example, <code>container2</code> was able to resolve <code>container3</code>’s name automatically in the user defined network <code>isolated_nw</code>, but the name resolution did not succeed automatically in the default <code>bridge</code> network. This is expected in order to maintain backward compatibility with <a href=\"../default_network/dockerlinks/index\">legacy link</a>.</p> <p>The <code>legacy link</code> provided 4 major functionalities to the default <code>bridge</code> network.</p> <ul> <li>name resolution</li> <li>name alias for the linked container using <code>--link=CONTAINER-NAME:ALIAS</code>\n</li> <li>secured container connectivity (in isolation via <code>--icc=false</code>)</li> <li>environment variable injection</li> </ul> <p>Comparing the above 4 functionalities with the non-default user-defined networks such as <code>isolated_nw</code> in this example, without any additional config, <code>docker network</code> provides</p> <ul> <li>automatic name resolution using DNS</li> <li>automatic secured isolated environment for the containers in a network</li> <li>ability to dynamically attach and detach to multiple networks</li> <li>supports the <code>--link</code> option to provide name alias for the linked container</li> </ul> <p>Continuing with the above example, create another container <code>container4</code> in <code>isolated_nw</code> with <code>--link</code> to provide additional name resolution using alias for other containers in the same network.</p> <pre>$ docker run --net=isolated_nw -itd --name=container4 --link container5:c5 busybox\n01b5df970834b77a9eadbaff39051f237957bd35c4c56f11193e0594cfd5117c\n</pre> <p>With the help of <code>--link</code> <code>container4</code> will be able to reach <code>container5</code> using the aliased name <code>c5</code> as well.</p> <p>Please note that while creating <code>container4</code>, we linked to a container named <code>container5</code> which is not created yet. That is one of the differences in behavior between the <em>legacy link</em> in default <code>bridge</code> network and the new <em>link</em> functionality in user defined networks. The <em>legacy link</em> is static in nature and it hard-binds the container with the alias and it doesn’t tolerate linked container restarts. While the new <em>link</em> functionality in user defined networks are dynamic in nature and supports linked container restarts including tolerating ip-address changes on the linked container.</p> <p>Now let us launch another container named <code>container5</code> linking <code>container4</code> to c4.</p> <pre>$ docker run --net=isolated_nw -itd --name=container5 --link container4:c4 busybox\n72eccf2208336f31e9e33ba327734125af00d1e1d2657878e2ee8154fbb23c7a\n</pre> <p>As expected, <code>container4</code> will be able to reach <code>container5</code> by both its container name and its alias c5 and <code>container5</code> will be able to reach <code>container4</code> by its container name and its alias c4.</p> <pre>$ docker attach container4\n/ # ping -w 4 c5\nPING c5 (172.25.0.5): 56 data bytes\n64 bytes from 172.25.0.5: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.25.0.5: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.5: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.5: seq=3 ttl=64 time=0.097 ms\n\n--- c5 ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n\n/ # ping -w 4 container5\nPING container5 (172.25.0.5): 56 data bytes\n64 bytes from 172.25.0.5: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.25.0.5: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.5: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.5: seq=3 ttl=64 time=0.097 ms\n\n--- container5 ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n</pre> <pre>$ docker attach container5\n/ # ping -w 4 c4\nPING c4 (172.25.0.4): 56 data bytes\n64 bytes from 172.25.0.4: seq=0 ttl=64 time=0.065 ms\n64 bytes from 172.25.0.4: seq=1 ttl=64 time=0.070 ms\n64 bytes from 172.25.0.4: seq=2 ttl=64 time=0.067 ms\n64 bytes from 172.25.0.4: seq=3 ttl=64 time=0.082 ms\n\n--- c4 ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.065/0.070/0.082 ms\n\n/ # ping -w 4 container4\nPING container4 (172.25.0.4): 56 data bytes\n64 bytes from 172.25.0.4: seq=0 ttl=64 time=0.065 ms\n64 bytes from 172.25.0.4: seq=1 ttl=64 time=0.070 ms\n64 bytes from 172.25.0.4: seq=2 ttl=64 time=0.067 ms\n64 bytes from 172.25.0.4: seq=3 ttl=64 time=0.082 ms\n\n--- container4 ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.065/0.070/0.082 ms\n</pre> <p>Similar to the legacy link functionality the new link alias is localized to a container and the aliased name has no meaning outside of the container using the <code>--link</code>.</p> <p>Also, it is important to note that if a container belongs to multiple networks, the linked alias is scoped within a given network. Hence the containers can be linked to different aliases in different networks.</p> <p>Extending the example, let us create another network named <code>local_alias</code></p> <pre>$ docker network create -d bridge --subnet 172.26.0.0/24 local_alias\n76b7dc932e037589e6553f59f76008e5b76fa069638cd39776b890607f567aaa\n</pre> <p>let us connect <code>container4</code> and <code>container5</code> to the new network <code>local_alias</code></p> <pre>$ docker network connect --link container5:foo local_alias container4\n$ docker network connect --link container4:bar local_alias container5\n</pre> <pre>$ docker attach container4\n\n/ # ping -w 4 foo\nPING foo (172.26.0.3): 56 data bytes\n64 bytes from 172.26.0.3: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.26.0.3: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.26.0.3: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.26.0.3: seq=3 ttl=64 time=0.097 ms\n\n--- foo ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n\n/ # ping -w 4 c5\nPING c5 (172.25.0.5): 56 data bytes\n64 bytes from 172.25.0.5: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.25.0.5: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.5: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.5: seq=3 ttl=64 time=0.097 ms\n\n--- c5 ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n</pre> <p>Note that the ping succeeds for both the aliases but on different networks. Let us conclude this section by disconnecting <code>container5</code> from the <code>isolated_nw</code> and observe the results</p> <pre>$ docker network disconnect isolated_nw container5\n\n$ docker attach container4\n\n/ # ping -w 4 c5\nping: bad address 'c5'\n\n/ # ping -w 4 foo\nPING foo (172.26.0.3): 56 data bytes\n64 bytes from 172.26.0.3: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.26.0.3: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.26.0.3: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.26.0.3: seq=3 ttl=64 time=0.097 ms\n\n--- foo ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n\n</pre> <p>In conclusion, the new link functionality in user defined networks provides all the benefits of legacy links while avoiding most of the well-known issues with <em>legacy links</em>.</p> <p>One notable missing functionality compared to <em>legacy links</em> is the injection of environment variables. Though very useful, environment variable injection is static in nature and must be injected when the container is started. One cannot inject environment variables into a running container without significant effort and hence it is not compatible with <code>docker network</code> which provides a dynamic way to connect/ disconnect containers to/from a network.</p> <h3 id=\"network-scoped-alias\">Network-scoped alias</h3> <p>While *link*s provide private name resolution that is localized within a container, the network-scoped alias provides a way for a container to be discovered by an alternate name by any other container within the scope of a particular network. Unlike the <em>link</em> alias, which is defined by the consumer of a service, the network-scoped alias is defined by the container that is offering the service to the network.</p> <p>Continuing with the above example, create another container in <code>isolated_nw</code> with a network alias.</p> <pre>$ docker run --net=isolated_nw -itd --name=container6 --net-alias app busybox\n8ebe6767c1e0361f27433090060b33200aac054a68476c3be87ef4005eb1df17\n</pre> <pre>$ docker attach container4\n/ # ping -w 4 app\nPING app (172.25.0.6): 56 data bytes\n64 bytes from 172.25.0.6: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.25.0.6: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.6: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.6: seq=3 ttl=64 time=0.097 ms\n\n--- app ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n\n/ # ping -w 4 container6\nPING container5 (172.25.0.6): 56 data bytes\n64 bytes from 172.25.0.6: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.25.0.6: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.6: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.6: seq=3 ttl=64 time=0.097 ms\n\n--- container6 ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n</pre> <p>Now let us connect <code>container6</code> to the <code>local_alias</code> network with a different network-scoped alias.</p> <pre>$ docker network connect --alias scoped-app local_alias container6\n</pre> <p><code>container6</code> in this example now is aliased as <code>app</code> in network <code>isolated_nw</code> and as <code>scoped-app</code> in network <code>local_alias</code>.</p> <p>Let’s try to reach these aliases from <code>container4</code> (which is connected to both these networks) and <code>container5</code> (which is connected only to <code>isolated_nw</code>).</p> <pre>$ docker attach container4\n\n/ # ping -w 4 scoped-app\nPING foo (172.26.0.5): 56 data bytes\n64 bytes from 172.26.0.5: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.26.0.5: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.26.0.5: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.26.0.5: seq=3 ttl=64 time=0.097 ms\n\n--- foo ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n\n$ docker attach container5\n\n/ # ping -w 4 scoped-app\nping: bad address 'scoped-app'\n\n</pre> <p>As you can see, the alias is scoped to the network it is defined on and hence only those containers that are connected to that network can access the alias.</p> <p>In addition to the above features, multiple containers can share the same network-scoped alias within the same network. For example, let’s launch <code>container7</code> in <code>isolated_nw</code> with the same alias as <code>container6</code></p> <pre>$ docker run --net=isolated_nw -itd --name=container7 --net-alias app busybox\n3138c678c123b8799f4c7cc6a0cecc595acbdfa8bf81f621834103cd4f504554\n</pre> <p>When multiple containers share the same alias, name resolution to that alias will happen to one of the containers (typically the first container that is aliased). When the container that backs the alias goes down or disconnected from the network, the next container that backs the alias will be resolved.</p> <p>Let us ping the alias <code>app</code> from <code>container4</code> and bring down <code>container6</code> to verify that <code>container7</code> is resolving the <code>app</code> alias.</p> <pre>$ docker attach container4\n/ # ping -w 4 app\nPING app (172.25.0.6): 56 data bytes\n64 bytes from 172.25.0.6: seq=0 ttl=64 time=0.070 ms\n64 bytes from 172.25.0.6: seq=1 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.6: seq=2 ttl=64 time=0.080 ms\n64 bytes from 172.25.0.6: seq=3 ttl=64 time=0.097 ms\n\n--- app ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.070/0.081/0.097 ms\n\n$ docker stop container6\n\n$ docker attach container4\n/ # ping -w 4 app\nPING app (172.25.0.7): 56 data bytes\n64 bytes from 172.25.0.7: seq=0 ttl=64 time=0.095 ms\n64 bytes from 172.25.0.7: seq=1 ttl=64 time=0.075 ms\n64 bytes from 172.25.0.7: seq=2 ttl=64 time=0.072 ms\n64 bytes from 172.25.0.7: seq=3 ttl=64 time=0.101 ms\n\n--- app ping statistics ---\n4 packets transmitted, 4 packets received, 0% packet loss\nround-trip min/avg/max = 0.072/0.085/0.101 ms\n\n</pre> <h2 id=\"disconnecting-containers\">Disconnecting containers</h2> <p>You can disconnect a container from a network using the <code>docker network\ndisconnect</code> command.</p> <pre>$ docker network disconnect isolated_nw container2\n\ndocker inspect --format='{{json .NetworkSettings.Networks}}'  container2 | python -m json.tool\n{\n    \"bridge\": {\n        \"NetworkID\":\"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n        \"EndpointID\": \"9e4575f7f61c0f9d69317b7a4b92eefc133347836dd83ef65deffa16b9985dc0\",\n        \"Gateway\": \"172.17.0.1\",\n        \"GlobalIPv6Address\": \"\",\n        \"GlobalIPv6PrefixLen\": 0,\n        \"IPAddress\": \"172.17.0.3\",\n        \"IPPrefixLen\": 16,\n        \"IPv6Gateway\": \"\",\n        \"MacAddress\": \"02:42:ac:11:00:03\"\n    }\n}\n\n\n$ docker network inspect isolated_nw\n[\n    {\n        \"Name\": \"isolated_nw\",\n        \"Id\": \"06a62f1c73c4e3107c0f555b7a5f163309827bfbbf999840166065a8f35455a8\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.21.0.0/16\",\n                    \"Gateway\": \"172.21.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {\n            \"467a7863c3f0277ef8e661b38427737f28099b61fa55622d6c30fb288d88c551\": {\n                \"Name\": \"container3\",\n                \"EndpointID\": \"dffc7ec2915af58cc827d995e6ebdc897342be0420123277103c40ae35579103\",\n                \"MacAddress\": \"02:42:ac:19:03:03\",\n                \"IPv4Address\": \"172.25.3.3/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {}\n    }\n]\n</pre> <p>Once a container is disconnected from a network, it cannot communicate with other containers connected to that network. In this example, <code>container2</code> can no longer talk to <code>container3</code> on the <code>isolated_nw</code> network.</p> <pre>$ docker attach container2\n\n/ # ifconfig\neth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:03  \n          inet addr:172.17.0.3  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:acff:fe11:3/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:8 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n\n/ # ping container3\nPING container3 (172.25.3.3): 56 data bytes\n^C\n--- container3 ping statistics ---\n2 packets transmitted, 0 packets received, 100% packet loss\n</pre> <p>The <code>container2</code> still has full connectivity to the bridge network</p> <pre>/ # ping container1\nPING container1 (172.17.0.2): 56 data bytes\n64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.119 ms\n64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.174 ms\n^C\n--- container1 ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.119/0.146/0.174 ms\n/ #\n</pre> <p>There are certain scenarios such as ungraceful docker daemon restarts in multi-host network, where the daemon is unable to cleanup stale connectivity endpoints. Such stale endpoints may cause an error <code>container already connected to network</code> when a new container is connected to that network with the same name as the stale endpoint. In order to cleanup these stale endpoints, first remove the container and force disconnect (<code>docker network disconnect -f</code>) the endpoint from the network. Once the endpoint is cleaned up, the container can be connected to the network.</p> <pre>$ docker run -d --name redis_db --net multihost redis\nERROR: Cannot start container bc0b19c089978f7845633027aa3435624ca3d12dd4f4f764b61eac4c0610f32e: container already connected to network multihost\n\n$ docker rm -f redis_db\n$ docker network disconnect -f multihost redis_db\n\n$ docker run -d --name redis_db --net multihost redis\n7d986da974aeea5e9f7aca7e510bdb216d58682faa83a9040c2f2adc0544795a\n</pre> <h2 id=\"remove-a-network\">Remove a network</h2> <p>When all the containers in a network are stopped or disconnected, you can remove a network.</p> <pre>$ docker network disconnect isolated_nw container3\n</pre> <pre>docker network inspect isolated_nw\n[\n    {\n        \"Name\": \"isolated_nw\",\n        \"Id\": \"06a62f1c73c4e3107c0f555b7a5f163309827bfbbf999840166065a8f35455a8\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.21.0.0/16\",\n                    \"Gateway\": \"172.21.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {},\n        \"Options\": {}\n    }\n]\n\n$ docker network rm isolated_nw\n</pre> <p>List all your networks to verify the <code>isolated_nw</code> was removed:</p> <pre>$ docker network ls\nNETWORK ID          NAME                DRIVER\n72314fa53006        host                host                \nf7ab26d71dbd        bridge              bridge              \n0f32e83e61ac        none                null  \n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../../../reference/commandline/network_create/index\">network create</a></li> <li><a href=\"../../../reference/commandline/network_inspect/index\">network inspect</a></li> <li><a href=\"../../../reference/commandline/network_connect/index\">network connect</a></li> <li><a href=\"../../../reference/commandline/network_disconnect/index\">network disconnect</a></li> <li><a href=\"../../../reference/commandline/network_ls/index\">network ls</a></li> <li><a href=\"../../../reference/commandline/network_rm/index\">network rm</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/work-with-networks/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/work-with-networks/</a>\n  </p>\n</div>\n","engine/userguide/networking/get-started-overlay/index":"<h1 id=\"get-started-with-multi-host-networking\">Get started with multi-host networking</h1> <p>This article uses an example to explain the basics of creating a multi-host network. Docker Engine supports multi-host networking out-of-the-box through the <code>overlay</code> network driver. Unlike <code>bridge</code> networks, overlay networks require some pre-existing conditions before you can create one. These conditions are:</p> <ul> <li>Access to a key-value store. Docker supports Consul, Etcd, and ZooKeeper (Distributed store) key-value stores.</li> <li>A cluster of hosts with connectivity to the key-value store.</li> <li>A properly configured Engine <code>daemon</code> on each host in the cluster.</li> <li>Hosts within the cluster must have unique hostnames because the key-value store uses the hostnames to identify cluster members.</li> </ul> <p>Though Docker Machine and Docker Swarm are not mandatory to experience Docker multi-host networking, this example uses them to illustrate how they are integrated. You’ll use Machine to create both the key-value store server and the host cluster. This example creates a Swarm cluster.</p> <h2 id=\"prerequisites\">Prerequisites</h2> <p>Before you begin, make sure you have a system on your network with the latest version of Docker Engine and Docker Machine installed. The example also relies on VirtualBox. If you installed on a Mac or Windows with Docker Toolbox, you have all of these installed already.</p> <p>If you have not already done so, make sure you upgrade Docker Engine and Docker Machine to the latest versions.</p> <h2 id=\"step-1-set-up-a-key-value-store\">Step 1: Set up a key-value store</h2> <p>An overlay network requires a key-value store. The key-value store holds information about the network state which includes discovery, networks, endpoints, IP addresses, and more. Docker supports Consul, Etcd, and ZooKeeper key-value stores. This example uses Consul.</p> <ol> <li><p>Log into a system prepared with the prerequisite Docker Engine, Docker Machine, and VirtualBox software.</p></li> <li>\n<p>Provision a VirtualBox machine called <code>mh-keystore</code>.</p> <pre>$ docker-machine create -d virtualbox mh-keystore\n</pre> <p>When you provision a new machine, the process adds Docker Engine to the host. This means rather than installing Consul manually, you can create an instance using the <a href=\"https://hub.docker.com/r/progrium/consul/\">consul image from Docker Hub</a>. You’ll do this in the next step.</p>\n</li> <li>\n<p>Set your local environment to the <code>mh-keystore</code> machine.</p> <pre>$  eval \"$(docker-machine env mh-keystore)\"\n</pre>\n</li> <li>\n<p>Start a <code>progrium/consul</code> container running on the <code>mh-keystore</code> machine.</p> <pre>$  docker run -d \\\n    -p \"8500:8500\" \\\n    -h \"consul\" \\\n    progrium/consul -server -bootstrap\n</pre> <p>The client starts a <code>progrium/consul</code> image running in the <code>mh-keystore</code> machine. The server is called <code>consul</code> and is listening on port <code>8500</code>.</p>\n</li> <li>\n<p>Run the <code>docker ps</code> command to see the <code>consul</code> container.</p> <pre>$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                            NAMES\n4d51392253b3        progrium/consul     \"/bin/start -server -\"   25 minutes ago      Up 25 minutes       53/tcp, 53/udp, 8300-8302/tcp, 0.0.0.0:8500-&gt;8500/tcp, 8400/tcp, 8301-8302/udp   admiring_panini\n</pre>\n</li> </ol> <p>Keep your terminal open and move onto the next step.</p> <h2 id=\"step-2-create-a-swarm-cluster\">Step 2: Create a Swarm cluster</h2> <p>In this step, you use <code>docker-machine</code> to provision the hosts for your network. At this point, you won’t actually create the network. You’ll create several machines in VirtualBox. One of the machines will act as the Swarm master; you’ll create that first. As you create each host, you’ll pass the Engine on that machine options that are needed by the <code>overlay</code> network driver.</p> <ol> <li>\n<p>Create a Swarm master.</p> <pre>$ docker-machine create \\\n-d virtualbox \\\n--swarm --swarm-master \\\n--swarm-discovery=\"consul://$(docker-machine ip mh-keystore):8500\" \\\n--engine-opt=\"cluster-store=consul://$(docker-machine ip mh-keystore):8500\" \\\n--engine-opt=\"cluster-advertise=eth1:2376\" \\\nmhs-demo0\n</pre> <p>At creation time, you supply the Engine <code>daemon</code> with the <code>--cluster-store</code> option. This option tells the Engine the location of the key-value store for the <code>overlay</code> network. The bash expansion <code>$(docker-machine ip mh-keystore)</code> resolves to the IP address of the Consul server you created in “STEP 1”. The <code>--cluster-advertise</code> option advertises the machine on the network.</p>\n</li> <li>\n<p>Create another host and add it to the Swarm cluster.</p> <pre>$ docker-machine create -d virtualbox \\\n    --swarm \\\n    --swarm-discovery=\"consul://$(docker-machine ip mh-keystore):8500\" \\\n    --engine-opt=\"cluster-store=consul://$(docker-machine ip mh-keystore):8500\" \\\n    --engine-opt=\"cluster-advertise=eth1:2376\" \\\n  mhs-demo1\n</pre>\n</li> <li>\n<p>List your machines to confirm they are all up and running.</p> <pre>$ docker-machine ls\nNAME         ACTIVE   DRIVER       STATE     URL                         SWARM\ndefault      -        virtualbox   Running   tcp://192.168.99.100:2376\nmh-keystore  *        virtualbox   Running   tcp://192.168.99.103:2376\nmhs-demo0    -        virtualbox   Running   tcp://192.168.99.104:2376   mhs-demo0 (master)\nmhs-demo1    -        virtualbox   Running   tcp://192.168.99.105:2376   mhs-demo0\n</pre>\n</li> </ol> <p>At this point you have a set of hosts running on your network. You are ready to create a multi-host network for containers using these hosts.</p> <p>Leave your terminal open and go onto the next step.</p> <h2 id=\"step-3-create-the-overlay-network\">Step 3: Create the overlay Network</h2> <p>To create an overlay network</p> <ol> <li>\n<p>Set your docker environment to the Swarm master.</p> <pre>$ eval $(docker-machine env --swarm mhs-demo0)\n</pre> <p>Using the <code>--swarm</code> flag with <code>docker-machine</code> restricts the <code>docker</code> commands to Swarm information alone.</p>\n</li> <li>\n<p>Use the <code>docker info</code> command to view the Swarm.</p> <pre>$ docker info\nContainers: 3\nImages: 2\nRole: primary\nStrategy: spread\nFilters: affinity, health, constraint, port, dependency\nNodes: 2\nmhs-demo0: 192.168.99.104:2376\n└ Containers: 2\n└ Reserved CPUs: 0 / 1\n└ Reserved Memory: 0 B / 1.021 GiB\n└ Labels: executiondriver=native-0.2, kernelversion=4.1.10-boot2docker, operatingsystem=Boot2Docker 1.9.0-rc1 (TCL 6.4); master : 4187d2c - Wed Oct 14 14:00:28 UTC 2015, provider=virtualbox, storagedriver=aufs\nmhs-demo1: 192.168.99.105:2376\n└ Containers: 1\n└ Reserved CPUs: 0 / 1\n└ Reserved Memory: 0 B / 1.021 GiB\n└ Labels: executiondriver=native-0.2, kernelversion=4.1.10-boot2docker, operatingsystem=Boot2Docker 1.9.0-rc1 (TCL 6.4); master : 4187d2c - Wed Oct 14 14:00:28 UTC 2015, provider=virtualbox, storagedriver=aufs\nCPUs: 2\nTotal Memory: 2.043 GiB\nName: 30438ece0915\n</pre> <p>From this information, you can see that you are running three containers and two images on the Master.</p>\n</li> <li>\n<p>Create your <code>overlay</code> network.</p> <pre>$ docker network create --driver overlay --subnet=10.0.9.0/24 my-net\n</pre> <p>You only need to create the network on a single host in the cluster. In this case, you used the Swarm master but you could easily have run it on any host in the cluster.</p>\n</li> </ol> <blockquote> <p><strong>Note</strong> : It is highly recommended to use the <code>--subnet</code> option when creating a network. If the <code>--subnet</code> is not specified, the docker daemon automatically chooses and assigns a subnet for the network and it could overlap with another subnet in your infrastructure that is not managed by docker. Such overlaps can cause connectivity issues or failures when containers are connected to that network.</p> </blockquote> <ol> <li>\n<p>Check that the network is running:</p> <pre>$ docker network ls\nNETWORK ID          NAME                DRIVER\n412c2496d0eb        mhs-demo1/host      host\ndd51763e6dd2        mhs-demo0/bridge    bridge\n6b07d0be843f        my-net              overlay\nb4234109bd9b        mhs-demo0/none      null\n1aeead6dd890        mhs-demo0/host      host\nd0bb78cbe7bd        mhs-demo1/bridge    bridge\n1c0eb8f69ebb        mhs-demo1/none      null\n</pre> <p>As you are in the Swarm master environment, you see all the networks on all the Swarm agents: the default networks on each engine and the single overlay network. Notice that each <code>NETWORK ID</code> is unique.</p>\n</li> <li>\n<p>Switch to each Swarm agent in turn and list the networks.</p> <pre>$ eval $(docker-machine env mhs-demo0)\n$ docker network ls\nNETWORK ID          NAME                DRIVER\n6b07d0be843f        my-net              overlay\ndd51763e6dd2        bridge              bridge\nb4234109bd9b        none                null\n1aeead6dd890        host                host\n$ eval $(docker-machine env mhs-demo1)\n$ docker network ls\nNETWORK ID          NAME                DRIVER\nd0bb78cbe7bd        bridge              bridge\n1c0eb8f69ebb        none                null\n412c2496d0eb        host                host\n6b07d0be843f        my-net              overlay\n</pre>\n</li> </ol> <p>Both agents report they have the <code>my-net</code> network with the <code>6b07d0be843f</code> ID. You now have a multi-host container network running!</p> <h2 id=\"step-4-run-an-application-on-your-network\">Step 4: Run an application on your Network</h2> <p>Once your network is created, you can start a container on any of the hosts and it automatically is part of the network.</p> <ol> <li>\n<p>Point your environment to the Swarm master.</p> <pre>$ eval $(docker-machine env --swarm mhs-demo0)\n</pre>\n</li> <li>\n<p>Start an Nginx web server on the <code>mhs-demo0</code> instance.</p> <pre>$ docker run -itd --name=web --net=my-net --env=\"constraint:node==mhs-demo0\" nginx\n</pre>\n</li> <li>\n<p>Run a BusyBox instance on the <code>mhs-demo1</code> instance and get the contents of the Nginx server’s home page.</p> <pre>$ docker run -it --rm --net=my-net --env=\"constraint:node==mhs-demo1\" busybox wget -O- http://web\nUnable to find image 'busybox:latest' locally\nlatest: Pulling from library/busybox\nab2b8a86ca6c: Pull complete\n2c5ac3f849df: Pull complete\nDigest: sha256:5551dbdfc48d66734d0f01cafee0952cb6e8eeecd1e2492240bf2fd9640c2279\nStatus: Downloaded newer image for busybox:latest\nConnecting to web (10.0.0.2:80)\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nbody {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n}\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n-                    100% |*******************************|   612   0:00:00 ETA\n</pre>\n</li> </ol> <h2 id=\"step-5-check-external-connectivity\">Step 5: Check external connectivity</h2> <p>As you’ve seen, Docker’s built-in overlay network driver provides out-of-the-box connectivity between the containers on multiple hosts within the same network. Additionally, containers connected to the multi-host network are automatically connected to the <code>docker_gwbridge</code> network. This network allows the containers to have external connectivity outside of their cluster.</p> <ol> <li>\n<p>Change your environment to the Swarm agent.</p> <pre>$ eval $(docker-machine env mhs-demo1)\n</pre>\n</li> <li>\n<p>View the <code>docker_gwbridge</code> network, by listing the networks.</p> <pre>$ docker network ls\nNETWORK ID          NAME                DRIVER\n6b07d0be843f        my-net              overlay\ndd51763e6dd2        bridge              bridge\nb4234109bd9b        none                null\n1aeead6dd890        host                host\ne1dbd5dff8be        docker_gwbridge     bridge\n</pre>\n</li> <li>\n<p>Repeat steps 1 and 2 on the Swarm master.</p> <pre>$ eval $(docker-machine env mhs-demo0)\n$ docker network ls\nNETWORK ID          NAME                DRIVER\n6b07d0be843f        my-net              overlay\nd0bb78cbe7bd        bridge              bridge\n1c0eb8f69ebb        none                null\n412c2496d0eb        host                host\n97102a22e8d2        docker_gwbridge     bridge\n</pre>\n</li> <li>\n<p>Check the Nginx container’s network interfaces.</p> <pre>$ docker exec web ip addr\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default\nlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\ninet 127.0.0.1/8 scope host lo\n    valid_lft forever preferred_lft forever\ninet6 ::1/128 scope host\n    valid_lft forever preferred_lft forever\n22: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default\nlink/ether 02:42:0a:00:09:03 brd ff:ff:ff:ff:ff:ff\ninet 10.0.9.3/24 scope global eth0\n    valid_lft forever preferred_lft forever\ninet6 fe80::42:aff:fe00:903/64 scope link\n    valid_lft forever preferred_lft forever\n24: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default\nlink/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff\ninet 172.18.0.2/16 scope global eth1\n    valid_lft forever preferred_lft forever\ninet6 fe80::42:acff:fe12:2/64 scope link\n    valid_lft forever preferred_lft forever\n</pre> <p>The <code>eth0</code> interface represents the container interface that is connected to the <code>my-net</code> overlay network. While the <code>eth1</code> interface represents the container interface that is connected to the <code>docker_gwbridge</code> network.</p>\n</li> </ol> <h2 id=\"step-6-extra-credit-with-docker-compose\">Step 6: Extra Credit with Docker Compose</h2> <p>Please refer to the Networking feature introduced in <a href=\"https://docs.docker.com/compose/networking/\">Compose V2 format</a> and execute the multi-host networking scenario in the Swarm cluster used above.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../dockernetworks/index\">Understand Docker container networks</a></li> <li><a href=\"../work-with-networks/index\">Work with network commands</a></li> <li><a href=\"https://docs.docker.com/swarm\">Docker Swarm overview</a></li> <li><a href=\"https://docs.docker.com/machine\">Docker Machine overview</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/get-started-overlay/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/get-started-overlay/</a>\n  </p>\n</div>\n","engine/userguide/networking/configure-dns/index":"<h1 id=\"embedded-dns-server-in-user-defined-networks\">Embedded DNS server in user-defined networks</h1> <p>The information in this section covers the embedded DNS server operation for containers in user-defined networks. DNS lookup for containers connected to user-defined networks works differently compared to the containers connected to <code>default bridge</code> network.</p> <blockquote> <p><strong>Note</strong>: In order to maintain backward compatibility, the DNS configuration in <code>default bridge</code> network is retained with no behavioral change. Please refer to the <a href=\"../default_network/configure-dns/index\">DNS in default bridge network</a> for more information on DNS configuration in the <code>default bridge</code> network.</p> </blockquote> <p>As of Docker 1.10, the docker daemon implements an embedded DNS server which provides built-in service discovery for any container created with a valid <code>name</code> or <code>net-alias</code> or aliased by <code>link</code>. The exact details of how Docker manages the DNS configurations inside the container can change from one Docker version to the next. So you should not assume the way the files such as <code>/etc/hosts</code>, <code>/etc/resolv.conf</code> are managed inside the containers and leave the files alone and use the following Docker options instead.</p> <p>Various container options that affect container domain name services.</p> <table> <tr> <td> <p> <code>--name=CONTAINER-NAME</code> </p> </td> <td> <p> Container name configured using <code>--name</code> is used to discover a container within an user-defined docker network. The embedded DNS server maintains the mapping between the container name and its IP address (on the network the container is connected to). </p> </td> </tr> <tr> <td> <p> <code>--net-alias=ALIAS</code> </p> </td> <td> <p> In addition to <code>--name</code> as described above, a container is discovered by one or more of its configured <code>--net-alias</code> (or <code>--alias</code> in <code>docker network connect</code> command) within the user-defined network. The embedded DNS server maintains the mapping between all of the container aliases and its IP address on a specific user-defined network. A container can have different aliases in different networks by using the <code>--alias</code> option in <code>docker network connect</code> command. </p> </td> </tr> <tr> <td> <p> <code>--link=CONTAINER_NAME:ALIAS</code> </p> </td> <td> <p> Using this option as you <code>run</code> a container gives the embedded DNS an extra entry named <code>ALIAS</code> that points to the IP address of the container identified by <code>CONTAINER_NAME</code>. When using <code>--link</code> the embedded DNS will guarantee that localized lookup result only on that container where the <code>--link</code> is used. This lets processes inside the new container connect to container without having to know its name or IP. </p> </td> </tr> <tr> <td><p> <code>--dns=[IP_ADDRESS...]</code> </p></td> <td><p> The IP addresses passed via the <code>--dns</code> option is used by the embedded DNS server to forward the DNS query if embedded DNS server is unable to resolve a name resolution request from the containers. These <code>--dns</code> IP addresses are managed by the embedded DNS server and will not be updated in the container's <code>/etc/resolv.conf</code> file. </p></td>\n</tr> <tr> <td><p> <code>--dns-search=DOMAIN...</code> </p></td> <td>\n<p> Sets the domain names that are searched when a bare unqualified hostname is used inside of the container. These <code>--dns-search</code> options are managed by the embedded DNS server and will not be updated in the container's <code>/etc/resolv.conf</code> file. When a container process attempts to access <code>host</code> and the search domain <code>example.com</code> is set, for instance, the DNS logic will not only look up <code>host</code> but also <code>host.example.com</code>. </p> </td> </tr> <tr> <td><p> <code>--dns-opt=OPTION...</code> </p></td> <td>\n<p> Sets the options used by DNS resolvers. These options are managed by the embedded DNS server and will not be updated in the container's <code>/etc/resolv.conf</code> file. </p> <p> See documentation for <code>resolv.conf</code> for a list of valid options </p>\n</td> </tr> </table> <p>In the absence of the <code>--dns=IP_ADDRESS...</code>, <code>--dns-search=DOMAIN...</code>, or <code>--dns-opt=OPTION...</code> options, Docker uses the <code>/etc/resolv.conf</code> of the host machine (where the <code>docker</code> daemon runs). While doing so the daemon filters out all localhost IP address <code>nameserver</code> entries from the host’s original file.</p> <p>Filtering is necessary because all localhost addresses on the host are unreachable from the container’s network. After this filtering, if there are no more <code>nameserver</code> entries left in the container’s <code>/etc/resolv.conf</code> file, the daemon adds public Google DNS nameservers (8.8.8.8 and 8.8.4.4) to the container’s DNS configuration. If IPv6 is enabled on the daemon, the public IPv6 Google DNS nameservers will also be added (2001:4860:4860::8888 and 2001:4860:4860::8844).</p> <blockquote> <p><strong>Note</strong>: If you need access to a host’s localhost resolver, you must modify your DNS service on the host to listen on a non-localhost address that is reachable from within the container.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/configure-dns/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/configure-dns/</a>\n  </p>\n</div>\n","engine/userguide/networking/default_network/index":"<h1 id=\"docker-default-bridge-network\">Docker default bridge network</h1> <p>With the introduction of the Docker networks feature, you can create your own user-defined networks. The Docker default bridge is created when you install Docker Engine. It is a <code>bridge</code> network and is also named <code>bridge</code>. The topics in this section are related to interacting with that default bridge network.</p> <ul> <li><a href=\"container-communication/index\">Understand container communication</a></li> <li><a href=\"dockerlinks/index\">Legacy container links</a></li> <li><a href=\"binding/index\">Binding container ports to the host</a></li> <li><a href=\"build-bridges/index\">Build your own bridge</a></li> <li><a href=\"configure-dns/index\">Configure container DNS</a></li> <li>\n<a href=\"custom-docker0/index\">Customize the docker0 bridge</a><br>\n</li> <li>\n<a href=\"ipv6/index\">IPv6 with Docker</a><br>\n</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/default_network/</a>\n  </p>\n</div>\n","engine/userguide/storagedriver/overlayfs-driver/index":"<h1 id=\"docker-and-overlayfs-in-practice\">Docker and OverlayFS in practice</h1> <p>OverlayFS is a modern <em>union filesystem</em> that is similar to AUFS. In comparison to AUFS, OverlayFS:</p> <ul> <li>has a simpler design</li> <li>has been in the mainline Linux kernel since version 3.18</li> <li>is potentially faster</li> </ul> <p>As a result, OverlayFS is rapidly gaining popularity in the Docker community and is seen by many as a natural successor to AUFS. As promising as OverlayFS is, it is still relatively young. Therefore caution should be taken before using it in production Docker environments.</p> <p>Docker’s <code>overlay</code> storage driver leverages several OverlayFS features to build and manage the on-disk structures of images and containers.</p> <blockquote> <p><strong>Note</strong>: Since it was merged into the mainline kernel, the OverlayFS <em>kernel module</em> was renamed from “overlayfs” to “overlay”. As a result you may see the two terms used interchangeably in some documentation. However, this document uses “OverlayFS” to refer to the overall filesystem, and <code>overlay</code> to refer to Docker’s storage-driver.</p> </blockquote> <h2 id=\"image-layering-and-sharing-with-overlayfs\">Image layering and sharing with OverlayFS</h2> <p>OverlayFS takes two directories on a single Linux host, layers one on top of the other, and provides a single unified view. These directories are often referred to as <em>layers</em> and the technology used to layer them is known as a <em>union mount</em>. The OverlayFS terminology is “lowerdir” for the bottom layer and “upperdir” for the top layer. The unified view is exposed through its own directory called “merged”.</p> <p>The diagram below shows how a Docker image and a Docker container are layered. The image layer is the “lowerdir” and the container layer is the “upperdir”. The unified view is exposed through a directory called “merged” which is effectively the containers mount point. The diagram shows how Docker constructs map to OverlayFS constructs.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/images/overlay_constructs.jpg\" alt=\"\"></p> <p>Notice how the image layer and container layer can contain the same files. When this happens, the files in the container layer (“upperdir”) are dominant and obscure the existence of the same files in the image layer (“lowerdir”). The container mount (“merged”) presents the unified view.</p> <p>OverlayFS only works with two layers. This means that multi-layered images cannot be implemented as multiple OverlayFS layers. Instead, each image layer is implemented as its own directory under <code>/var/lib/docker/overlay</code>. Hard links are then used as a space-efficient way to reference data shared with lower layers. As of Docker 1.10, image layer IDs no longer correspond to directory names in <code>/var/lib/docker/</code></p> <p>To create a container, the <code>overlay</code> driver combines the directory representing the image’s top layer plus a new directory for the container. The image’s top layer is the “lowerdir” in the overlay and read-only. The new directory for the container is the “upperdir” and is writable.</p> <h2 id=\"example-image-and-container-on-disk-constructs\">Example: Image and container on-disk constructs</h2> <p>The following <code>docker pull</code> command shows a Docker host with downloading a Docker image comprising four layers.</p> <pre>$ sudo docker pull ubuntu\nUsing default tag: latest\nlatest: Pulling from library/ubuntu\n8387d9ff0016: Pull complete\n3b52deaaf0ed: Pull complete\n4bd501fad6de: Pull complete\na3ed95caeb02: Pull complete\nDigest: sha256:457b05828bdb5dcc044d93d042863fba3f2158ae249a6db5ae3934307c757c54\nStatus: Downloaded newer image for ubuntu:latest\n</pre> <p>Each image layer has it’s own directory under <code>/var/lib/docker/overlay/</code>. This is where the contents of each image layer are stored.</p> <p>The output of the command below shows the four directories that store the contents of each image layer just pulled. However, as can be seen, the image layer IDs do not match the directory names in <code>/var/lib/docker/overlay</code>. This is normal behavior in Docker 1.10 and later.</p> <pre>$ ls -l /var/lib/docker/overlay/\ntotal 24\ndrwx------ 3 root root 4096 Oct 28 11:02 1d073211c498fd5022699b46a936b4e4bdacb04f637ad64d3475f558783f5c3e\ndrwx------ 3 root root 4096 Oct 28 11:02 5a4526e952f0aa24f3fcc1b6971f7744eb5465d572a48d47c492cb6bbf9cbcda\ndrwx------ 5 root root 4096 Oct 28 11:06 99fcaefe76ef1aa4077b90a413af57fd17d19dce4e50d7964a273aae67055235\ndrwx------ 3 root root 4096 Oct 28 11:01 c63fb41c2213f511f12f294dd729b9903a64d88f098c20d2350905ac1fdbcbba\n</pre> <p>The image layer directories contain the files unique to that layer as well as hard links to the data that is shared with lower layers. This allows for efficient use of disk space.</p> <p>Containers also exist on-disk in the Docker host’s filesystem under <code>/var/lib/docker/overlay/</code>. If you inspect the directory relating to a running container using the <code>ls -l</code> command, you find the following file and directories.</p> <pre>$ ls -l /var/lib/docker/overlay/&lt;directory-of-running-container&gt;\ntotal 16\n-rw-r--r-- 1 root root   64 Oct 28 11:06 lower-id\ndrwxr-xr-x 1 root root 4096 Oct 28 11:06 merged\ndrwxr-xr-x 4 root root 4096 Oct 28 11:06 upper\ndrwx------ 3 root root 4096 Oct 28 11:06 work\n</pre> <p>These four filesystem objects are all artifacts of OverlayFS. The “lower-id” file contains the ID of the top layer of the image the container is based on. This is used by OverlayFS as the “lowerdir”.</p> <pre>$ cat /var/lib/docker/overlay/73de7176c223a6c82fd46c48c5f152f2c8a7e49ecb795a7197c3bb795c4d879e/lower-id\n1d073211c498fd5022699b46a936b4e4bdacb04f637ad64d3475f558783f5c3e\n</pre> <p>The “upper” directory is the containers read-write layer. Any changes made to the container are written to this directory.</p> <p>The “merged” directory is effectively the containers mount point. This is where the unified view of the image (“lowerdir”) and container (“upperdir”) is exposed. Any changes written to the container are immediately reflected in this directory.</p> <p>The “work” directory is required for OverlayFS to function. It is used for things such as <em>copy_up</em> operations.</p> <p>You can verify all of these constructs from the output of the <code>mount</code> command. (Ellipses and line breaks are used in the output below to enhance readability.)</p> <pre>$ mount | grep overlay\noverlay on /var/lib/docker/overlay/73de7176c223.../merged\ntype overlay (rw,relatime,lowerdir=/var/lib/docker/overlay/1d073211c498.../root,\nupperdir=/var/lib/docker/overlay/73de7176c223.../upper,\nworkdir=/var/lib/docker/overlay/73de7176c223.../work)\n</pre> <p>The output reflects that the overlay is mounted as read-write (“rw”).</p> <h2 id=\"container-reads-and-writes-with-overlay\">Container reads and writes with overlay</h2> <p>Consider three scenarios where a container opens a file for read access with overlay.</p> <ul> <li><p><strong>The file does not exist in the container layer</strong>. If a container opens a file for read access and the file does not already exist in the container (“upperdir”) it is read from the image (“lowerdir”). This should incur very little performance overhead.</p></li> <li><p><strong>The file only exists in the container layer</strong>. If a container opens a file for read access and the file exists in the container (“upperdir”) and not in the image (“lowerdir”), it is read directly from the container.</p></li> <li><p><strong>The file exists in the container layer and the image layer</strong>. If a container opens a file for read access and the file exists in the image layer and the container layer, the file’s version in the container layer is read. This is because files in the container layer (“upperdir”) obscure files with the same name in the image layer (“lowerdir”).</p></li> </ul> <p>Consider some scenarios where files in a container are modified.</p> <ul> <li>\n<p><strong>Writing to a file for the first time</strong>. The first time a container writes to an existing file, that file does not exist in the container (“upperdir”). The <code>overlay</code> driver performs a <em>copy_up</em> operation to copy the file from the image (“lowerdir”) to the container (“upperdir”). The container then writes the changes to the new copy of the file in the container layer.</p> <p>However, OverlayFS works at the file level not the block level. This means that all OverlayFS copy-up operations copy entire files, even if the file is very large and only a small part of it is being modified. This can have a noticeable impact on container write performance. However, two things are worth noting:</p> <ul> <li><p>The copy_up operation only occurs the first time any given file is written to. Subsequent writes to the same file will operate against the copy of the file already copied up to the container.</p></li> <li><p>OverlayFS only works with two layers. This means that performance should be better than AUFS which can suffer noticeable latencies when searching for files in images with many layers.</p></li> </ul>\n</li> <li>\n<p><strong>Deleting files and directories</strong>. When files are deleted within a container a <em>whiteout</em> file is created in the containers “upperdir”. The version of the file in the image layer (“lowerdir”) is not deleted. However, the whiteout file in the container obscures it.</p> <p>Deleting a directory in a container results in <em>opaque directory</em> being created in the “upperdir”. This has the same effect as a whiteout file and effectively masks the existence of the directory in the image’s “lowerdir”.</p>\n</li> </ul> <h2 id=\"configure-docker-with-the-overlay-storage-driver\">Configure Docker with the overlay storage driver</h2> <p>To configure Docker to use the overlay storage driver your Docker host must be running version 3.18 of the Linux kernel (preferably newer) with the overlay kernel module loaded. OverlayFS can operate on top of most supported Linux filesystems. However, ext4 is currently recommended for use in production environments.</p> <p>The following procedure shows you how to configure your Docker host to use OverlayFS. The procedure assumes that the Docker daemon is in a stopped state.</p> <blockquote> <p><strong>Caution:</strong> If you have already run the Docker daemon on your Docker host and have images you want to keep, <code>push</code> them Docker Hub or your private Docker Trusted Registry before attempting this procedure.</p> </blockquote> <ol> <li><p>If it is running, stop the Docker <code>daemon</code>.</p></li> <li>\n<p>Verify your kernel version and that the overlay kernel module is loaded.</p> <pre>$ uname -r\n3.19.0-21-generic\n\n$ lsmod | grep overlay\noverlay\n</pre>\n</li> <li>\n<p>Start the Docker daemon with the <code>overlay</code> storage driver.</p> <pre>$ docker daemon --storage-driver=overlay &amp;\n[1] 29403\nroot@ip-10-0-0-174:/home/ubuntu# INFO[0000] Listening for HTTP on unix (/var/run/docker.sock)\nINFO[0000] Option DefaultDriver: bridge\nINFO[0000] Option DefaultNetwork: bridge\n&lt;output truncated&gt;\n</pre> <p>Alternatively, you can force the Docker daemon to automatically start with the <code>overlay</code> driver by editing the Docker config file and adding the <code>--storage-driver=overlay</code> flag to the <code>DOCKER_OPTS</code> line. Once this option is set you can start the daemon using normal startup scripts without having to manually pass in the <code>--storage-driver</code> flag.</p>\n</li> <li>\n<p>Verify that the daemon is using the <code>overlay</code> storage driver</p> <pre>$ docker info\nContainers: 0\nImages: 0\nStorage Driver: overlay\n Backing Filesystem: extfs\n&lt;output truncated&gt;\n</pre> <p>Notice that the <em>Backing filesystem</em> in the output above is showing as <code>extfs</code>. Multiple backing filesystems are supported but <code>extfs</code> (ext4) is recommended for production use cases.</p>\n</li> </ol> <p>Your Docker host is now using the <code>overlay</code> storage driver. If you run the <code>mount</code> command, you’ll find Docker has automatically created the <code>overlay</code> mount with the required “lowerdir”, “upperdir”, “merged” and “workdir” constructs.</p> <h2 id=\"overlayfs-and-docker-performance\">OverlayFS and Docker Performance</h2> <p>As a general rule, the <code>overlay</code> driver should be fast. Almost certainly faster than <code>aufs</code> and <code>devicemapper</code>. In certain circumstances it may also be faster than <code>btrfs</code>. That said, there are a few things to be aware of relative to the performance of Docker using the <code>overlay</code> storage driver.</p> <ul> <li><p><strong>Page Caching</strong>. OverlayFS supports page cache sharing. This means multiple containers accessing the same file can share a single page cache entry (or entries). This makes the <code>overlay</code> driver efficient with memory and a good option for PaaS and other high density use cases.</p></li> <li>\n<p><strong>copy_up</strong>. As with AUFS, OverlayFS has to perform copy-up operations any time a container writes to a file for the first time. This can insert latency into the write operation — especially if the file being copied up is large. However, once the file has been copied up, all subsequent writes to that file occur without the need for further copy-up operations.</p> <p>The OverlayFS copy_up operation should be faster than the same operation with AUFS. This is because AUFS supports more layers than OverlayFS and it is possible to incur far larger latencies if searching through many AUFS layers.</p>\n</li> <li><p><strong>RPMs and Yum</strong>. OverlayFS only implements a subset of the POSIX standards. This can result in certain OverlayFS operations breaking POSIX standards. One such operation is the <em>copy-up</em> operation. Therefore, using <code>yum</code> inside of a container on a Docker host using the <code>overlay</code> storage driver is unlikely to work without implementing workarounds.</p></li> <li><p><strong>Inode limits</strong>. Use of the <code>overlay</code> storage driver can cause excessive inode consumption. This is especially so as the number of images and containers on the Docker host grows. A Docker host with a large number of images and lots of started and stopped containers can quickly run out of inodes.</p></li> </ul> <p>Unfortunately you can only specify the number of inodes in a filesystem at the time of creation. For this reason, you may wish to consider putting <code>/var/lib/docker</code> on a separate device with its own filesystem, or manually specifying the number of inodes when creating the filesystem.</p> <p>The following generic performance best practices also apply to OverlayFS.</p> <ul> <li><p><strong>Solid State Devices (SSD)</strong>. For best performance it is always a good idea to use fast storage media such as solid state devices (SSD).</p></li> <li><p><strong>Use Data Volumes</strong>. Data volumes provide the best and most predictable performance. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. For this reason, you should place heavy write workloads on data volumes.</p></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/storagedriver/overlayfs-driver/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/storagedriver/overlayfs-driver/</a>\n  </p>\n</div>\n","engine/admin/formatting/index":"<h1 id=\"formatting-reference\">Formatting reference</h1> <p>Docker uses <a href=\"https://golang.org/pkg/text/template/\">Go templates</a> to allow users manipulate the output format of certain commands and log drivers. Each command a driver provides a detailed list of elements they support in their templates:</p> <ul> <li><a href=\"https://docs.docker.com/engine/reference/commandline/images/#formatting\">Docker Images formatting</a></li> <li><a href=\"https://docs.docker.com/engine/reference/commandline/inspect/#examples\">Docker Inspect formatting</a></li> <li><a href=\"https://docs.docker.com/engine/admin/logging/log_tags/\">Docker Log Tag formatting</a></li> <li><a href=\"https://docs.docker.com/engine/reference/commandline/network_inspect/\">Docker Network Inspect formatting</a></li> <li><a href=\"https://docs.docker.com/engine/reference/commandline/ps/#formatting\">Docker PS formatting</a></li> <li><a href=\"https://docs.docker.com/engine/reference/commandline/volume_inspect/\">Docker Volume Inspect formatting</a></li> <li><a href=\"https://docs.docker.com/engine/reference/commandline/version/#examples\">Docker Version formatting</a></li> </ul> <h2 id=\"template-functions\">Template functions</h2> <p>Docker provides a set of basic functions to manipulate template elements. This is the complete list of the available functions with examples:</p> <h3 id=\"join\">Join</h3> <p>Join concatenates a list of strings to create a single string. It puts a separator between each element in the list.</p> <pre>$ docker ps --format '{{join .Names \" or \"}}'\n</pre> <h3 id=\"json\">Json</h3> <p>Json encodes an element as a json string.</p> <pre>$ docker inspect --format '{{json .Mounts}}' container\n</pre> <h3 id=\"lower\">Lower</h3> <p>Lower turns a string into its lower case representation.</p> <pre>$ docker inspect --format \"{{lower .Name}}\" container\n</pre> <h3 id=\"split\">Split</h3> <p>Split slices a string into a list of strings separated by a separator.</p> <pre># docker inspect --format '{{split (join .Names \"/\") \"/\"}}' container\n</pre> <h3 id=\"title\">Title</h3> <p>Title capitalizes a string.</p> <pre>$ docker inspect --format \"{{title .Name}}\" container\n</pre> <h3 id=\"upper\">Upper</h3> <p>Upper turms a string into its upper case representation.</p> <pre>$ docker inspect --format \"{{upper .Name}}\" container\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/formatting/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/formatting/</a>\n  </p>\n</div>\n","engine/admin/host_integration/index":"<h1 id=\"automatically-start-containers\">Automatically start containers</h1> <p>As of Docker 1.2, <a href=\"../../reference/run/index#restart-policies-restart\">restart policies</a> are the built-in Docker mechanism for restarting containers when they exit. If set, restart policies will be used when the Docker daemon starts up, as typically happens after a system boot. Restart policies will ensure that linked containers are started in the correct order.</p> <p>If restart policies don’t suit your needs (i.e., you have non-Docker processes that depend on Docker containers), you can use a process manager like <a href=\"http://upstart.ubuntu.com/\">upstart</a>, <a href=\"http://freedesktop.org/wiki/Software/systemd/\">systemd</a> or <a href=\"http://supervisord.org/\">supervisor</a> instead.</p> <h2 id=\"using-a-process-manager\">Using a process manager</h2> <p>Docker does not set any restart policies by default, but be aware that they will conflict with most process managers. So don’t set restart policies if you are using a process manager.</p> <p>When you have finished setting up your image and are happy with your running container, you can then attach a process manager to manage it. When you run <code>docker start -a</code>, Docker will automatically attach to the running container, or start it if needed and forward all signals so that the process manager can detect when a container stops and correctly restart it.</p> <p>Here are a few sample scripts for systemd and upstart to integrate with Docker.</p> <h2 id=\"examples\">Examples</h2> <p>The examples below show configuration files for two popular process managers, upstart and systemd. In these examples, we’ll assume that we have already created a container to run Redis with <code>--name=redis_server</code>. These files define a new service that will be started after the docker daemon service has started.</p> <h3 id=\"upstart\">upstart</h3> <pre>description \"Redis container\"\nauthor \"Me\"\nstart on filesystem and started docker\nstop on runlevel [!2345]\nrespawn\nscript\n  /usr/bin/docker start -a redis_server\nend script\n</pre> <h3 id=\"systemd\">systemd</h3> <pre>[Unit]\nDescription=Redis container\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nExecStart=/usr/bin/docker start -a redis_server\nExecStop=/usr/bin/docker stop -t 2 redis_server\n\n[Install]\nWantedBy=local.target\n</pre> <p>If you need to pass options to the redis container (such as <code>--env</code>), then you’ll need to use <code>docker run</code> rather than <code>docker start</code>. This will create a new container every time the service is started, which will be stopped and removed when the service is stopped.</p> <pre>[Service]\n...\nExecStart=/usr/bin/docker run --env foo=bar --name redis_server redis\nExecStop=/usr/bin/docker stop -t 2 redis_server ; /usr/bin/docker rm -f redis_server\n...\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/host_integration/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/host_integration/</a>\n  </p>\n</div>\n","engine/admin/systemd/index":"<h1 id=\"control-and-configure-docker-with-systemd\">Control and configure Docker with systemd</h1> <p>Many Linux distributions use systemd to start the Docker daemon. This document shows a few examples of how to customize Docker’s settings.</p> <h2 id=\"starting-the-docker-daemon\">Starting the Docker daemon</h2> <p>Once Docker is installed, you will need to start the Docker daemon.</p> <pre>$ sudo systemctl start docker\n# or on older distributions, you may need to use\n$ sudo service docker start\n</pre> <p>If you want Docker to start at boot, you should also:</p> <pre>$ sudo systemctl enable docker\n# or on older distributions, you may need to use\n$ sudo chkconfig docker on\n</pre> <h2 id=\"custom-docker-daemon-options\">Custom Docker daemon options</h2> <p>There are a number of ways to configure the daemon flags and environment variables for your Docker daemon.</p> <p>The recommended way is to use a systemd drop-in file (as described in the <a target=\"_blank\" href=\"https://www.freedesktop.org/software/systemd/man/systemd.unit.html\">systemd.unit</a> documentation). These are local files named <code>&lt;something&gt;.conf</code> in the <code>/etc/systemd/system/docker.service.d</code> directory. This could also be <code>/etc/systemd/system/docker.service</code>, which also works for overriding the defaults from <code>/lib/systemd/system/docker.service</code>.</p> <p>However, if you had previously used a package which had an <code>EnvironmentFile</code> (often pointing to <code>/etc/sysconfig/docker</code>) then for backwards compatibility, you drop a file with a <code>.conf</code> extension into the <code>/etc/systemd/system/docker.service.d</code> directory including the following:</p> <pre>[Service]\nEnvironmentFile=-/etc/sysconfig/docker\nEnvironmentFile=-/etc/sysconfig/docker-storage\nEnvironmentFile=-/etc/sysconfig/docker-network\nExecStart=\nExecStart=/usr/bin/docker daemon -H fd:// $OPTIONS \\\n          $DOCKER_STORAGE_OPTIONS \\\n          $DOCKER_NETWORK_OPTIONS \\\n          $BLOCK_REGISTRY \\\n          $INSECURE_REGISTRY\n</pre> <p>To check if the <code>docker.service</code> uses an <code>EnvironmentFile</code>:</p> <pre>$ systemctl show docker | grep EnvironmentFile\nEnvironmentFile=-/etc/sysconfig/docker (ignore_errors=yes)\n</pre> <p>Alternatively, find out where the service file is located:</p> <pre>$ systemctl show --property=FragmentPath docker\nFragmentPath=/usr/lib/systemd/system/docker.service\n$ grep EnvironmentFile /usr/lib/systemd/system/docker.service\nEnvironmentFile=-/etc/sysconfig/docker\n</pre> <p>You can customize the Docker daemon options using override files as explained in the <a href=\"#http-proxy\">HTTP Proxy example</a> below. The files located in <code>/usr/lib/systemd/system</code> or <code>/lib/systemd/system</code> contain the default options and should not be edited.</p> <h3 id=\"runtime-directory-and-storage-driver\">Runtime directory and storage driver</h3> <p>You may want to control the disk space used for Docker images, containers and volumes by moving it to a separate partition.</p> <p>In this example, we’ll assume that your <code>docker.service</code> file looks something like:</p> <pre>[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network.target docker.socket\nRequires=docker.socket\n\n[Service]\nType=notify\nExecStart=/usr/bin/docker daemon -H fd://\nLimitNOFILE=1048576\nLimitNPROC=1048576\nTasksMax=1048576\n\n[Install]\nAlso=docker.socket\n</pre> <p>This will allow us to add extra flags via a drop-in file (mentioned above) by placing a file containing the following in the <code>/etc/systemd/system/docker.service.d</code> directory:</p> <pre>[Service]\nExecStart=\nExecStart=/usr/bin/docker daemon -H fd:// --graph=\"/mnt/docker-data\" --storage-driver=overlay\n</pre> <p>You can also set other environment variables in this file, for example, the <code>HTTP_PROXY</code> environment variables described below.</p> <p>To modify the ExecStart configuration, specify an empty configuration followed by a new configuration as follows:</p> <pre>[Service]\nExecStart=\nExecStart=/usr/bin/docker daemon -H fd:// --bip=172.17.42.1/16\n</pre> <p>If you fail to specify an empty configuration, Docker reports an error such as:</p> <pre>docker.service has more than one ExecStart= setting, which is only allowed for Type=oneshot services. Refusing.\n</pre> <h3 id=\"http-proxy\">HTTP proxy</h3> <p>This example overrides the default <code>docker.service</code> file.</p> <p>If you are behind an HTTP proxy server, for example in corporate settings, you will need to add this configuration in the Docker systemd service file.</p> <p>First, create a systemd drop-in directory for the docker service:</p> <pre>mkdir /etc/systemd/system/docker.service.d\n</pre> <p>Now create a file called <code>/etc/systemd/system/docker.service.d/http-proxy.conf</code> that adds the <code>HTTP_PROXY</code> environment variable:</p> <pre>[Service]\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:80/\"\n</pre> <p>If you have internal Docker registries that you need to contact without proxying you can specify them via the <code>NO_PROXY</code> environment variable:</p> <pre>Environment=\"HTTP_PROXY=http://proxy.example.com:80/\" \"NO_PROXY=localhost,127.0.0.1,docker-registry.somecorporation.com\"\n</pre> <p>Flush changes:</p> <pre>$ sudo systemctl daemon-reload\n</pre> <p>Verify that the configuration has been loaded:</p> <pre>$ systemctl show --property=Environment docker\nEnvironment=HTTP_PROXY=http://proxy.example.com:80/\n</pre> <p>Restart Docker:</p> <pre>$ sudo systemctl restart docker\n</pre> <h2 id=\"manually-creating-the-systemd-unit-files\">Manually creating the systemd unit files</h2> <p>When installing the binary without a package, you may want to integrate Docker with systemd. For this, simply install the two unit files (service and socket) from <a href=\"https://github.com/docker/docker/tree/master/contrib/init/systemd\">the github repository</a> to <code>/etc/systemd/system</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/systemd/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/systemd/</a>\n  </p>\n</div>\n","engine/admin/dsc/index":"<h1 id=\"using-powershell-dsc\">Using PowerShell DSC</h1> <p>Windows PowerShell Desired State Configuration (DSC) is a configuration management tool that extends the existing functionality of Windows PowerShell. DSC uses a declarative syntax to define the state in which a target should be configured. More information about PowerShell DSC can be found at <a href=\"http://technet.microsoft.com/en-us/library/dn249912.aspx\">http://technet.microsoft.com/en-us/library/dn249912.aspx</a>.</p> <h2 id=\"requirements\">Requirements</h2> <p>To use this guide you’ll need a Windows host with PowerShell v4.0 or newer.</p> <p>The included DSC configuration script also uses the official PPA so only an Ubuntu target is supported. The Ubuntu target must already have the required OMI Server and PowerShell DSC for Linux providers installed. More information can be found at <a href=\"https://github.com/MSFTOSSMgmt/WPSDSCLinux\">https://github.com/MSFTOSSMgmt/WPSDSCLinux</a>. The source repository listed below also includes PowerShell DSC for Linux installation and init scripts along with more detailed installation information.</p> <h2 id=\"installation\">Installation</h2> <p>The DSC configuration example source is available in the following repository: <a href=\"https://github.com/anweiss/DockerClientDSC\">https://github.com/anweiss/DockerClientDSC</a>. It can be cloned with:</p> <pre>$ git clone https://github.com/anweiss/DockerClientDSC.git\n</pre> <h2 id=\"usage\">Usage</h2> <p>The DSC configuration utilizes a set of shell scripts to determine whether or not the specified Docker components are configured on the target node(s). The source repository also includes a script (<code>RunDockerClientConfig.ps1</code>) that can be used to establish the required CIM session(s) and execute the <code>Set-DscConfiguration</code> cmdlet.</p> <p>More detailed usage information can be found at <a href=\"https://github.com/anweiss/DockerClientDSC\">https://github.com/anweiss/DockerClientDSC</a>.</p> <h3 id=\"install-docker\">Install Docker</h3> <p>The Docker installation configuration is equivalent to running:</p> <pre>apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys\\\n36A1D7869245C8950F966E92D8576A8BA88D21E9\nsh -c \"echo deb https://apt.dockerproject.org/repo ubuntu-trusty main\\\n&gt; /etc/apt/sources.list.d/docker.list\"\napt-get update\napt-get install docker-engine\n</pre> <p>Ensure that your current working directory is set to the <code>DockerClientDSC</code> source and load the DockerClient configuration into the current PowerShell session</p> <pre>. .\\DockerClient.ps1\n</pre> <p>Generate the required DSC configuration .mof file for the targeted node</p> <pre>DockerClient -Hostname \"myhost\"\n</pre> <p>A sample DSC configuration data file has also been included and can be modified and used in conjunction with or in place of the <code>Hostname</code> parameter:</p> <pre>DockerClient -ConfigurationData .\\DockerConfigData.psd1\n</pre> <p>Start the configuration application process on the targeted node</p> <pre>.\\RunDockerClientConfig.ps1 -Hostname \"myhost\"\n</pre> <p>The <code>RunDockerClientConfig.ps1</code> script can also parse a DSC configuration data file and execute configurations against multiple nodes as such:</p> <pre>.\\RunDockerClientConfig.ps1 -ConfigurationData .\\DockerConfigData.psd1\n</pre> <h3 id=\"images\">Images</h3> <p>Image configuration is equivalent to running: <code>docker pull [image]</code> or <code>docker rmi -f [IMAGE]</code>.</p> <p>Using the same steps defined above, execute <code>DockerClient</code> with the <code>Image</code> parameter and apply the configuration:</p> <pre>DockerClient -Hostname \"myhost\" -Image \"node\"\n.\\RunDockerClientConfig.ps1 -Hostname \"myhost\"\n</pre> <p>You can also configure the host to pull multiple images:</p> <pre>DockerClient -Hostname \"myhost\" -Image \"node\",\"mongo\"\n.\\RunDockerClientConfig.ps1 -Hostname \"myhost\"\n</pre> <p>To remove images, use a hashtable as follows:</p> <pre>DockerClient -Hostname \"myhost\" -Image @{Name=\"node\"; Remove=$true}\n.\\RunDockerClientConfig.ps1 -Hostname $hostname\n</pre> <h3 id=\"containers\">Containers</h3> <p>Container configuration is equivalent to running:</p> <pre>docker run -d --name=\"[containername]\" -p '[port]' -e '[env]' --link '[link]'\\\n'[image]' '[command]'\n</pre> <p>or</p> <pre>docker rm -f [containername]\n</pre> <p>To create or remove containers, you can use the <code>Container</code> parameter with one or more hashtables. The hashtable(s) passed to this parameter can have the following properties:</p> <ul> <li>Name (required)</li> <li>Image (required unless Remove property is set to <code>$true</code>)</li> <li>Port</li> <li>Env</li> <li>Link</li> <li>Command</li> <li>Remove</li> </ul> <p>For example, create a hashtable with the settings for your container:</p> <pre>$webContainer = @{Name=\"web\"; Image=\"anweiss/docker-platynem\"; Port=\"80:80\"}\n</pre> <p>Then, using the same steps defined above, execute <code>DockerClient</code> with the <code>-Image</code> and <code>-Container</code> parameters:</p> <pre>DockerClient -Hostname \"myhost\" -Image node -Container $webContainer\n.\\RunDockerClientConfig.ps1 -Hostname \"myhost\"\n</pre> <p>Existing containers can also be removed as follows:</p> <pre>$containerToRemove = @{Name=\"web\"; Remove=$true}\nDockerClient -Hostname \"myhost\" -Container $containerToRemove\n.\\RunDockerClientConfig.ps1 -Hostname \"myhost\"\n</pre> <p>Here is a hashtable with all of the properties that can be used to create a container:</p> <pre>$containerProps = @{Name=\"web\"; Image=\"node:latest\"; Port=\"80:80\"; `\nEnv=\"PORT=80\"; Link=\"db:db\"; Command=\"grunt\"}\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/dsc/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/dsc/</a>\n  </p>\n</div>\n","engine/admin/puppet/index":"<h1 id=\"using-puppet\">Using Puppet</h1> <blockquote> <p><em>Note:</em> Please note this is a community contributed installation path. The only <code>official</code> installation is using the <a href=\"../../installation/linux/ubuntulinux/index\"><em>Ubuntu</em></a> installation path. This version may sometimes be out of date.</p> </blockquote> <h2 id=\"requirements\">Requirements</h2> <p>To use this guide you’ll need a working installation of Puppet from <a href=\"https://puppetlabs.com\">Puppet Labs</a> .</p> <p>The module also currently uses the official PPA so only works with Ubuntu.</p> <h2 id=\"installation\">Installation</h2> <p>The module is available on the <a href=\"https://forge.puppetlabs.com/garethr/docker/\">Puppet Forge</a> and can be installed using the built-in module tool.</p> <pre>$ puppet module install garethr/docker\n</pre> <p>It can also be found on <a href=\"https://github.com/garethr/garethr-docker\">GitHub</a> if you would rather download the source.</p> <h2 id=\"usage\">Usage</h2> <p>The module provides a puppet class for installing Docker and two defined types for managing images and containers.</p> <h3 id=\"installation-1\">Installation</h3> <pre>include 'docker'\n</pre> <h3 id=\"images\">Images</h3> <p>The next step is probably to install a Docker image. For this, we have a defined type which can be used like so:</p> <pre>docker::image { 'ubuntu': }\n</pre> <p>This is equivalent to running:</p> <pre>$ docker pull ubuntu\n</pre> <p>Note that it will only be downloaded if an image of that name does not already exist. This is downloading a large binary so on first run can take a while. For that reason this define turns off the default 5 minute timeout for the exec type. Note that you can also remove images you no longer need with:</p> <pre>docker::image { 'ubuntu':\n  ensure =&gt; 'absent',\n}\n</pre> <h3 id=\"containers\">Containers</h3> <p>Now you have an image where you can run commands within a container managed by Docker.</p> <pre>docker::run { 'helloworld':\n  image   =&gt; 'ubuntu',\n  command =&gt; '/bin/sh -c \"while true; do echo hello world; sleep 1; done\"',\n}\n</pre> <p>This is equivalent to running the following command, but under upstart:</p> <pre>$ docker run -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n</pre> <p>Run also contains a number of optional parameters:</p> <pre>docker::run { 'helloworld':\n  image        =&gt; 'ubuntu',\n  command      =&gt; '/bin/sh -c \"while true; do echo hello world; sleep 1; done\"',\n  ports        =&gt; ['4444', '4555'],\n  volumes      =&gt; ['/var/lib/couchdb', '/var/log'],\n  volumes_from =&gt; '6446ea52fbc9',\n  memory_limit =&gt; 10485760, # bytes\n  username     =&gt; 'example',\n  hostname     =&gt; 'example.com',\n  env          =&gt; ['FOO=BAR', 'FOO2=BAR2'],\n  dns          =&gt; ['8.8.8.8', '8.8.4.4'],\n}\n</pre> <blockquote> <p><em>Note:</em> The <code>ports</code>, <code>env</code>, <code>dns</code> and <code>volumes</code> attributes can be set with either a single string or as above with an array of values.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/puppet/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/puppet/</a>\n  </p>\n</div>\n","engine/admin/using_supervisord/index":"<h1 id=\"using-supervisor-with-docker\">Using Supervisor with Docker</h1> <blockquote> <p><strong>Note</strong>: - <strong>If you don’t like sudo</strong> then see <a href=\"../../installation/binaries/index#giving-non-root-access\"><em>Giving non-root access</em></a></p> </blockquote> <p>Traditionally a Docker container runs a single process when it is launched, for example an Apache daemon or a SSH server daemon. Often though you want to run more than one process in a container. There are a number of ways you can achieve this ranging from using a simple Bash script as the value of your container’s <code>CMD</code> instruction to installing a process management tool.</p> <p>In this example you’re going to make use of the process management tool, <a href=\"http://supervisord.org/\">Supervisor</a>, to manage multiple processes in a container. Using Supervisor allows you to better control, manage, and restart the processes inside the container. To demonstrate this we’re going to install and manage both an SSH daemon and an Apache daemon.</p> <h2 id=\"creating-a-dockerfile\">Creating a Dockerfile</h2> <p>Let’s start by creating a basic <code>Dockerfile</code> for our new image.</p> <pre>FROM ubuntu:16.04\nMAINTAINER examples@docker.com\n</pre> <h2 id=\"installing-supervisor\">Installing Supervisor</h2> <p>You can now install the SSH and Apache daemons as well as Supervisor in the container.</p> <pre>RUN apt-get update &amp;&amp; apt-get install -y openssh-server apache2 supervisor\nRUN mkdir -p /var/lock/apache2 /var/run/apache2 /var/run/sshd /var/log/supervisor\n</pre> <p>The first <code>RUN</code> instruction installs the <code>openssh-server</code>, <code>apache2</code> and <code>supervisor</code> (which provides the Supervisor daemon) packages. The next <code>RUN</code> instruction creates four new directories that are needed to run the SSH daemon and Supervisor.</p> <h2 id=\"adding-supervisor-s-configuration-file\">Adding Supervisor’s configuration file</h2> <p>Now let’s add a configuration file for Supervisor. The default file is called <code>supervisord.conf</code> and is located in <code>/etc/supervisor/conf.d/</code>.</p> <pre>COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n</pre> <p>Let’s see what is inside the <code>supervisord.conf</code> file.</p> <pre>[supervisord]\nnodaemon=true\n\n[program:sshd]\ncommand=/usr/sbin/sshd -D\n\n[program:apache2]\ncommand=/bin/bash -c \"source /etc/apache2/envvars &amp;&amp; exec /usr/sbin/apache2 -DFOREGROUND\"\n</pre> <p>The <code>supervisord.conf</code> configuration file contains directives that configure Supervisor and the processes it manages. The first block <code>[supervisord]</code> provides configuration for Supervisor itself. The <code>nodaemon</code> directive is used, which tells Supervisor to run interactively rather than daemonize.</p> <p>The next two blocks manage the services we wish to control. Each block controls a separate process. The blocks contain a single directive, <code>command</code>, which specifies what command to run to start each process.</p> <h2 id=\"exposing-ports-and-running-supervisor\">Exposing ports and running Supervisor</h2> <p>Now let’s finish the <code>Dockerfile</code> by exposing some required ports and specifying the <code>CMD</code> instruction to start Supervisor when our container launches.</p> <pre>EXPOSE 22 80\nCMD [\"/usr/bin/supervisord\"]\n</pre> <p>These instructions tell Docker that ports 22 and 80 are exposed by the container and that the <code>/usr/bin/supervisord</code> binary should be executed when the container launches.</p> <h2 id=\"building-our-image\">Building our image</h2> <p>Your completed Dockerfile now looks like this:</p> <pre>FROM ubuntu:16.04\nMAINTAINER examples@docker.com\n\nRUN apt-get update &amp;&amp; apt-get install -y openssh-server apache2 supervisor\nRUN mkdir -p /var/lock/apache2 /var/run/apache2 /var/run/sshd /var/log/supervisor\n\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\nEXPOSE 22 80\nCMD [\"/usr/bin/supervisord\"]\n</pre> <p>And your <code>supervisord.conf</code> file looks like this;</p> <pre>[supervisord]\nnodaemon=true\n\n[program:sshd]\ncommand=/usr/sbin/sshd -D\n\n[program:apache2]\ncommand=/bin/bash -c \"source /etc/apache2/envvars &amp;&amp; exec /usr/sbin/apache2 -DFOREGROUND\"\n</pre> <p>You can now build the image using this command;</p> <pre>$ docker build -t mysupervisord .\n</pre> <h2 id=\"running-your-supervisor-container\">Running your Supervisor container</h2> <p>Once you have built your image you can launch a container from it.</p> <pre>$ docker run -p 22 -p 80 -t -i mysupervisord\n2013-11-25 18:53:22,312 CRIT Supervisor running as root (no user in config file)\n2013-11-25 18:53:22,312 WARN Included extra file \"/etc/supervisor/conf.d/supervisord.conf\" during parsing\n2013-11-25 18:53:22,342 INFO supervisord started with pid 1\n2013-11-25 18:53:23,346 INFO spawned: 'sshd' with pid 6\n2013-11-25 18:53:23,349 INFO spawned: 'apache2' with pid 7\n...\n</pre> <p>You launched a new container interactively using the <code>docker run</code> command. That container has run Supervisor and launched the SSH and Apache daemons with it. We’ve specified the <code>-p</code> flag to expose ports 22 and 80. From here we can now identify the exposed ports and connect to one or both of the SSH and Apache daemons.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/using_supervisord/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/using_supervisord/</a>\n  </p>\n</div>\n","engine/admin/chef/index":"<h1 id=\"using-chef\">Using Chef</h1> <blockquote> <p><strong>Note</strong>: Please note this is a community contributed installation path.</p> </blockquote> <h2 id=\"requirements\">Requirements</h2> <p>To use this guide you’ll need a working installation of <a href=\"https://www.chef.io/\">Chef</a>. This cookbook supports a variety of operating systems.</p> <h2 id=\"installation\">Installation</h2> <p>The cookbook is available on the <a href=\"https://supermarket.chef.io/cookbooks/docker\">Chef Supermarket</a> and can be installed using your favorite cookbook dependency manager.</p> <p>The source can be found on <a href=\"https://github.com/someara/chef-docker\">GitHub</a>.</p> <h2 id=\"usage\">Usage</h2> <ul> <li>Add <code>depends 'docker', '~&gt; 2.0'</code> to your cookbook’s metadata.rb</li> <li>Use resources shipped in cookbook in a recipe, the same way you’d use core Chef resources (file, template, directory, package, etc).</li> </ul> <pre>docker_service 'default' do\n  action [:create, :start]\nend\n\ndocker_image 'busybox' do\n  action :pull\nend\n\ndocker_container 'an echo server' do\n  repo 'busybox'\n  port '1234:1234'\n  command \"nc -ll -p 1234 -e /bin/cat\"\nend\n</pre> <h2 id=\"getting-started\">Getting Started</h2> <p>Here’s a quick example of pulling the latest image and running a container with exposed ports.</p> <pre># Pull latest image\ndocker_image 'nginx' do\n  tag 'latest'\n  action :pull\nend\n\n# Run container exposing ports\ndocker_container 'my_nginx' do\n  repo 'nginx'\n  tag 'latest'\n  port '80:80'\n  binds [ '/some/local/files/:/etc/nginx/conf.d' ]\n  host_name 'www'\n  domain_name 'computers.biz'\n  env 'FOO=bar'\n  subscribes :redeploy, 'docker_image[nginx]'\nend\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/chef/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/chef/</a>\n  </p>\n</div>\n","engine/admin/configuring/index":"<h1 id=\"configuring-and-running-docker-on-various-distributions\">Configuring and running Docker on various distributions</h1> <p>After successfully installing Docker, the <code>docker</code> daemon runs with its default configuration.</p> <p>In a production environment, system administrators typically configure the <code>docker</code> daemon to start and stop according to an organization’s requirements. In most cases, the system administrator configures a process manager such as <code>SysVinit</code>, <code>Upstart</code>, or <code>systemd</code> to manage the <code>docker</code> daemon’s start and stop.</p> <h3 id=\"running-the-docker-daemon-directly\">Running the docker daemon directly</h3> <p>The <code>docker</code> daemon can be run directly using the <code>docker daemon</code> command. By default it listens on the Unix socket <code>unix:///var/run/docker.sock</code></p> <pre>$ docker daemon\n\nINFO[0000] +job init_networkdriver()\nINFO[0000] +job serveapi(unix:///var/run/docker.sock)\nINFO[0000] Listening for HTTP on unix (/var/run/docker.sock)\n...\n...\n</pre> <h3 id=\"configuring-the-docker-daemon-directly\">Configuring the docker daemon directly</h3> <p>If you’re running the <code>docker</code> daemon directly by running <code>docker daemon</code> instead of using a process manager, you can append the configuration options to the <code>docker</code> run command directly. Other options can be passed to the <code>docker</code> daemon to configure it.</p> <p>Some of the daemon’s options are:</p> <table> <thead> <tr> <th>Flag</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>\n<code>-D</code>, <code>--debug=false</code>\n</td> <td>Enable or disable debug mode. By default, this is false.</td> </tr> <tr> <td>\n<code>-H</code>,<code>--host=[]</code>\n</td> <td>Daemon socket(s) to connect to.</td> </tr> <tr> <td><code>--tls=false</code></td> <td>Enable or disable TLS. By default, this is false.</td> </tr> </tbody> </table> <p>Here is an example of running the <code>docker</code> daemon with configuration options:</p> <pre>$ docker daemon -D --tls=true --tlscert=/var/docker/server.pem --tlskey=/var/docker/serverkey.pem -H tcp://192.168.59.3:2376\n</pre> <p>These options :</p> <ul> <li>Enable <code>-D</code> (debug) mode</li> <li>Set <code>tls</code> to true with the server certificate and key specified using <code>--tlscert</code> and <code>--tlskey</code> respectively</li> <li>Listen for connections on <code>tcp://192.168.59.3:2376</code>\n</li> </ul> <p>The command line reference has the <a href=\"../../reference/commandline/daemon/index\">complete list of daemon flags</a> with explanations.</p> <h3 id=\"daemon-debugging\">Daemon debugging</h3> <p>As noted above, setting the log level of the daemon to “debug” or enabling debug mode with <code>-D</code> allows the administrator or operator to gain much more knowledge about the runtime activity of the daemon. If faced with a non-responsive daemon, the administrator can force a full stack trace of all threads to be added to the daemon log by sending the <code>SIGUSR1</code> signal to the Docker daemon. A common way to send this signal is using the <code>kill</code> command on Linux systems. For example, <code>kill -USR1 &lt;daemon-pid&gt;</code> sends the <code>SIGUSR1</code> signal to the daemon process, causing the stack dump to be added to the daemon log.</p> <blockquote> <p><strong>Note:</strong> The log level setting of the daemon must be at least “info” level and above for the stack trace to be saved to the logfile. By default the daemon’s log level is set to “info”.</p> </blockquote> <p>The daemon will continue operating after handling the <code>SIGUSR1</code> signal and dumping the stack traces to the log. The stack traces can be used to determine the state of all goroutines and threads within the daemon.</p> <h2 id=\"ubuntu\">Ubuntu</h2> <p>As of <code>14.04</code>, Ubuntu uses Upstart as a process manager. By default, Upstart jobs are located in <code>/etc/init</code> and the <code>docker</code> Upstart job can be found at <code>/etc/init/docker.conf</code>.</p> <p>After successfully <a href=\"../../installation/linux/ubuntulinux/index\">installing Docker for Ubuntu</a>, you can check the running status using Upstart in this way:</p> <pre>$ sudo status docker\n\ndocker start/running, process 989\n</pre> <h3 id=\"running-docker\">Running Docker</h3> <p>You can start/stop/restart the <code>docker</code> daemon using</p> <pre>$ sudo start docker\n\n$ sudo stop docker\n\n$ sudo restart docker\n</pre> <h3 id=\"configuring-docker\">Configuring Docker</h3> <p>The instructions below depict configuring Docker on a system that uses <code>upstart</code> as the process manager. As of Ubuntu 15.04, Ubuntu uses <code>systemd</code> as its process manager. For Ubuntu 15.04 and higher, refer to <a href=\"../systemd/index\">control and configure Docker with systemd</a>.</p> <p>You configure the <code>docker</code> daemon in the <code>/etc/default/docker</code> file on your system. You do this by specifying values in a <code>DOCKER_OPTS</code> variable.</p> <p>To configure Docker options:</p> <ol> <li><p>Log into your host as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li><p>If you don’t have one, create the <code>/etc/default/docker</code> file on your host. Depending on how you installed Docker, you may already have this file.</p></li> <li>\n<p>Open the file with your favorite editor.</p> <pre>$ sudo vi /etc/default/docker\n</pre>\n</li> <li><p>Add a <code>DOCKER_OPTS</code> variable with the following options. These options are appended to the <code>docker</code> daemon’s run command.</p></li> </ol> <pre>    DOCKER_OPTS=\"-D --tls=true --tlscert=/var/docker/server.pem --tlskey=/var/docker/serverkey.pem -H tcp://192.168.59.3:2376\"\n</pre> <p>These options :</p> <ul> <li>Enable <code>-D</code> (debug) mode</li> <li>Set <code>tls</code> to true with the server certificate and key specified using <code>--tlscert</code> and <code>--tlskey</code> respectively</li> <li>Listen for connections on <code>tcp://192.168.59.3:2376</code>\n</li> </ul> <p>The command line reference has the <a href=\"../../reference/commandline/daemon/index\">complete list of daemon flags</a> with explanations.</p> <ol> <li><p>Save and close the file.</p></li> <li>\n<p>Restart the <code>docker</code> daemon.</p> <pre>$ sudo restart docker\n</pre>\n</li> <li>\n<p>Verify that the <code>docker</code> daemon is running as specified with the <code>ps</code> command.</p> <pre>$ ps aux | grep docker | grep -v grep\n</pre>\n</li> </ol> <h3 id=\"logs\">Logs</h3> <p>By default logs for Upstart jobs are located in <code>/var/log/upstart</code> and the logs for <code>docker</code> daemon can be located at <code>/var/log/upstart/docker.log</code></p> <pre>$ tail -f /var/log/upstart/docker.log\nINFO[0000] Loading containers: done.\nINFO[0000] Docker daemon commit=1b09a95-unsupported graphdriver=aufs version=1.11.0-dev\nINFO[0000] +job acceptconnections()\nINFO[0000] -job acceptconnections() = OK (0)\nINFO[0000] Daemon has completed initialization\n</pre> <h2 id=\"centos-red-hat-enterprise-linux-fedora\">CentOS / Red Hat Enterprise Linux / Fedora</h2> <p>As of <code>7.x</code>, CentOS and RHEL use <code>systemd</code> as the process manager. As of <code>21</code>, Fedora uses <code>systemd</code> as its process manager.</p> <p>After successfully installing Docker for <a href=\"../../installation/linux/centos/index\">CentOS</a>/<a href=\"../../installation/linux/rhel/index\">Red Hat Enterprise Linux</a>/<a href=\"../../installation/linux/fedora/index\">Fedora</a>, you can check the running status in this way:</p> <pre>$ sudo systemctl status docker\n</pre> <h3 id=\"running-docker-1\">Running Docker</h3> <p>You can start/stop/restart the <code>docker</code> daemon using</p> <pre>$ sudo systemctl start docker\n\n$ sudo systemctl stop docker\n\n$ sudo systemctl restart docker\n</pre> <p>If you want Docker to start at boot, you should also:</p> <pre>$ sudo systemctl enable docker\n</pre> <h3 id=\"configuring-docker-1\">Configuring Docker</h3> <p>For CentOS 7.x and RHEL 7.x you can <a href=\"../systemd/index\">control and configure Docker with systemd</a>.</p> <p>Previously, for CentOS 6.x and RHEL 6.x you would configure the <code>docker</code> daemon in the <code>/etc/sysconfig/docker</code> file on your system. You would do this by specifying values in a <code>other_args</code> variable. For a short time in CentOS 7.x and RHEL 7.x you would specify values in a <code>OPTIONS</code> variable. This is no longer recommended in favor of using systemd directly.</p> <p>For this section, we will use CentOS 7.x as an example to configure the <code>docker</code> daemon.</p> <p>To configure Docker options:</p> <ol> <li><p>Log into your host as a user with <code>sudo</code> or <code>root</code> privileges.</p></li> <li>\n<p>Create the <code>/etc/systemd/system/docker.service.d</code> directory.</p> <pre>$ sudo mkdir /etc/systemd/system/docker.service.d\n</pre>\n</li> <li><p>Create a <code>/etc/systemd/system/docker.service.d/docker.conf</code> file.</p></li> <li>\n<p>Open the file with your favorite editor.</p> <pre>$ sudo vi /etc/systemd/system/docker.service.d/docker.conf\n</pre>\n</li> <li><p>Override the <code>ExecStart</code> configuration from your <code>docker.service</code> file to customize the <code>docker</code> daemon. To modify the <code>ExecStart</code> configuration you have to specify an empty configuration followed by a new one as follows:</p></li> </ol> <pre>[Service]\nExecStart=\nExecStart=/usr/bin/docker daemon -H fd:// -D --tls=true --tlscert=/var/docker/server.pem --tlskey=/var/docker/serverkey.pem -H tcp://192.168.59.3:2376\n</pre> <p>These options :</p> <ul> <li>Enable <code>-D</code> (debug) mode</li> <li>Set <code>tls</code> to true with the server certificate and key specified using <code>--tlscert</code> and <code>--tlskey</code> respectively</li> <li>Listen for connections on <code>tcp://192.168.59.3:2376</code>\n</li> </ul> <p>The command line reference has the <a href=\"../../reference/commandline/daemon/index\">complete list of daemon flags</a> with explanations.</p> <ol> <li><p>Save and close the file.</p></li> <li>\n<p>Flush changes.</p> <pre>$ sudo systemctl daemon-reload\n</pre>\n</li> <li>\n<p>Restart the <code>docker</code> daemon.</p> <pre>$ sudo systemctl restart docker\n</pre>\n</li> <li>\n<p>Verify that the <code>docker</code> daemon is running as specified with the <code>ps</code> command.</p> <pre>$ ps aux | grep docker | grep -v grep\n</pre>\n</li> </ol> <h3 id=\"logs-1\">Logs</h3> <p>systemd has its own logging system called the journal. The logs for the <code>docker</code> daemon can be viewed using <code>journalctl -u docker</code></p> <pre>$ sudo journalctl -u docker\nMay 06 00:22:05 localhost.localdomain systemd[1]: Starting Docker Application Container Engine...\nMay 06 00:22:05 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:05Z\" level=\"info\" msg=\"+job serveapi(unix:///var/run/docker.sock)\"\nMay 06 00:22:05 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:05Z\" level=\"info\" msg=\"Listening for HTTP on unix (/var/run/docker.sock)\"\nMay 06 00:22:06 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:06Z\" level=\"info\" msg=\"+job init_networkdriver()\"\nMay 06 00:22:06 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:06Z\" level=\"info\" msg=\"-job init_networkdriver() = OK (0)\"\nMay 06 00:22:06 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:06Z\" level=\"info\" msg=\"Loading containers: start.\"\nMay 06 00:22:06 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:06Z\" level=\"info\" msg=\"Loading containers: done.\"\nMay 06 00:22:06 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:06Z\" level=\"info\" msg=\"Docker daemon commit=1b09a95-unsupported graphdriver=aufs version=1.11.0-dev\"\nMay 06 00:22:06 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:06Z\" level=\"info\" msg=\"+job acceptconnections()\"\nMay 06 00:22:06 localhost.localdomain docker[2495]: time=\"2015-05-06T00:22:06Z\" level=\"info\" msg=\"-job acceptconnections() = OK (0)\"\n</pre> <p><em>Note: Using and configuring journal is an advanced topic and is beyond the scope of this article.</em></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/configuring/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/configuring/</a>\n  </p>\n</div>\n","engine/admin/ambassador_pattern_linking/index":"<h1 id=\"link-via-an-ambassador-container\">Link via an ambassador container</h1> <p>Rather than hardcoding network links between a service consumer and provider, Docker encourages service portability, for example instead of:</p> <pre>(consumer) --&gt; (redis)\n</pre> <p>Requiring you to restart the <code>consumer</code> to attach it to a different <code>redis</code> service, you can add ambassadors:</p> <pre>(consumer) --&gt; (redis-ambassador) --&gt; (redis)\n</pre> <p>Or</p> <pre>(consumer) --&gt; (redis-ambassador) ---network---&gt; (redis-ambassador) --&gt; (redis)\n</pre> <p>When you need to rewire your consumer to talk to a different Redis server, you can just restart the <code>redis-ambassador</code> container that the consumer is connected to.</p> <p>This pattern also allows you to transparently move the Redis server to a different docker host from the consumer.</p> <p>Using the <code>svendowideit/ambassador</code> container, the link wiring is controlled entirely from the <code>docker run</code> parameters.</p> <h2 id=\"two-host-example\">Two host example</h2> <p>Start actual Redis server on one Docker host</p> <pre>big-server $ docker run -d --name redis crosbymichael/redis\n</pre> <p>Then add an ambassador linked to the Redis server, mapping a port to the outside world</p> <pre>big-server $ docker run -d --link redis:redis --name redis_ambassador -p 6379:6379 svendowideit/ambassador\n</pre> <p>On the other host, you can set up another ambassador setting environment variables for each remote port we want to proxy to the <code>big-server</code></p> <pre>client-server $ docker run -d --name redis_ambassador --expose 6379 -e REDIS_PORT_6379_TCP=tcp://192.168.1.52:6379 svendowideit/ambassador\n</pre> <p>Then on the <code>client-server</code> host, you can use a Redis client container to talk to the remote Redis server, just by linking to the local Redis ambassador.</p> <pre>client-server $ docker run -i -t --rm --link redis_ambassador:redis relateiq/redis-cli\nredis 172.17.0.160:6379&gt; ping\nPONG\n</pre> <h2 id=\"how-it-works\">How it works</h2> <p>The following example shows what the <code>svendowideit/ambassador</code> container does automatically (with a tiny amount of <code>sed</code>)</p> <p>On the Docker host (192.168.1.52) that Redis will run on:</p> <pre># start actual redis server\n$ docker run -d --name redis crosbymichael/redis\n\n# get a redis-cli container for connection testing\n$ docker pull relateiq/redis-cli\n\n# test the redis server by talking to it directly\n$ docker run -t -i --rm --link redis:redis relateiq/redis-cli\nredis 172.17.0.136:6379&gt; ping\nPONG\n^D\n\n# add redis ambassador\n$ docker run -t -i --link redis:redis --name redis_ambassador -p 6379:6379 alpine:3.2 sh\n</pre> <p>In the <code>redis_ambassador</code> container, you can see the linked Redis containers <code>env</code>:</p> <pre>/ # env\nREDIS_PORT=tcp://172.17.0.136:6379\nREDIS_PORT_6379_TCP_ADDR=172.17.0.136\nREDIS_NAME=/redis_ambassador/redis\nHOSTNAME=19d7adf4705e\nSHLVL=1\nHOME=/root\nREDIS_PORT_6379_TCP_PORT=6379\nREDIS_PORT_6379_TCP_PROTO=tcp\nREDIS_PORT_6379_TCP=tcp://172.17.0.136:6379\nTERM=xterm\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/\n/ # exit\n</pre> <p>This environment is used by the ambassador <code>socat</code> script to expose Redis to the world (via the <code>-p 6379:6379</code> port mapping):</p> <pre>$ docker rm redis_ambassador\n$ CMD=\"apk update &amp;&amp; apk add socat &amp;&amp; sh\"\n$ docker run -t -i --link redis:redis --name redis_ambassador -p 6379:6379 alpine:3.2 sh -c \"$CMD\"\n[...]\n/ # socat -t 100000000 TCP4-LISTEN:6379,fork,reuseaddr TCP4:172.17.0.136:6379\n</pre> <p>Now ping the Redis server via the ambassador:</p> <p>Now go to a different server:</p> <pre>$ CMD=\"apk update &amp;&amp; apk add socat &amp;&amp; sh\"\n$ docker run -t -i --expose 6379 --name redis_ambassador alpine:3.2 sh -c \"$CMD\"\n[...]\n/ # socat -t 100000000 TCP4-LISTEN:6379,fork,reuseaddr TCP4:192.168.1.52:6379\n</pre> <p>And get the <code>redis-cli</code> image so we can talk over the ambassador bridge.</p> <pre>$ docker pull relateiq/redis-cli\n$ docker run -i -t --rm --link redis_ambassador:redis relateiq/redis-cli\nredis 172.17.0.160:6379&gt; ping\nPONG\n</pre> <h2 id=\"the-svendowideit-ambassador-dockerfile\">The svendowideit/ambassador Dockerfile</h2> <p>The <code>svendowideit/ambassador</code> image is based on the <code>alpine:3.2</code> image with <code>socat</code> installed. When you start the container, it uses a small <code>sed</code> script to parse out the (possibly multiple) link environment variables to set up the port forwarding. On the remote host, you need to set the variable using the <code>-e</code> command line option.</p> <pre>--expose 1234 -e REDIS_PORT_1234_TCP=tcp://192.168.1.52:6379\n</pre> <p>Will forward the local <code>1234</code> port to the remote IP and port, in this case <code>192.168.1.52:6379</code>.</p> <pre>#\n# do\n#   docker build -t svendowideit/ambassador .\n# then to run it (on the host that has the real backend on it)\n#   docker run -t -i -link redis:redis -name redis_ambassador -p 6379:6379 svendowideit/ambassador\n# on the remote host, you can set up another ambassador\n#    docker run -t -i -name redis_ambassador -expose 6379 -e REDIS_PORT_6379_TCP=tcp://192.168.1.52:6379 svendowideit/ambassador sh\n# you can read more about this process at https://docs.docker.com/articles/ambassador_pattern_linking/\n\n# use alpine because its a minimal image with a package manager.\n# prettymuch all that is needed is a container that has a functioning env and socat (or equivalent)\nFROM    alpine:3.2\nMAINTAINER  SvenDowideit@home.org.au\n\nRUN apk update &amp;&amp; \\\n    apk add socat &amp;&amp; \\\n    rm -r /var/cache/\n\nCMD env | grep _TCP= | (sed 's/.*_PORT_\\([0-9]*\\)_TCP=tcp:\\/\\/\\(.*\\):\\(.*\\)/socat -t 100000000 TCP4-LISTEN:\\1,fork,reuseaddr TCP4:\\2:\\3 \\&amp;/' &amp;&amp; echo wait) | sh\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/ambassador_pattern_linking/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/ambassador_pattern_linking/</a>\n  </p>\n</div>\n","engine/admin/runmetrics/index":"<h1 id=\"runtime-metrics\">Runtime metrics</h1> <h2 id=\"docker-stats\">Docker stats</h2> <p>You can use the <code>docker stats</code> command to live stream a container’s runtime metrics. The command supports CPU, memory usage, memory limit, and network IO metrics.</p> <p>The following is a sample output from the <code>docker stats</code> command</p> <pre>$ docker stats redis1 redis2\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O\nredis1              0.07%               796 KB / 64 MB        1.21%               788 B / 648 B       3.568 MB / 512 KB\nredis2              0.07%               2.746 MB / 64 MB      4.29%               1.266 KB / 648 B    12.4 MB / 0 B\n</pre> <p>The <a href=\"../../reference/commandline/stats/index\">docker stats</a> reference page has more details about the <code>docker stats</code> command.</p> <h2 id=\"control-groups\">Control groups</h2> <p>Linux Containers rely on <a href=\"https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt\">control groups</a> which not only track groups of processes, but also expose metrics about CPU, memory, and block I/O usage. You can access those metrics and obtain network usage metrics as well. This is relevant for “pure” LXC containers, as well as for Docker containers.</p> <p>Control groups are exposed through a pseudo-filesystem. In recent distros, you should find this filesystem under <code>/sys/fs/cgroup</code>. Under that directory, you will see multiple sub-directories, called devices, freezer, blkio, etc.; each sub-directory actually corresponds to a different cgroup hierarchy.</p> <p>On older systems, the control groups might be mounted on <code>/cgroup</code>, without distinct hierarchies. In that case, instead of seeing the sub-directories, you will see a bunch of files in that directory, and possibly some directories corresponding to existing containers.</p> <p>To figure out where your control groups are mounted, you can run:</p> <pre>$ grep cgroup /proc/mounts\n</pre> <h2 id=\"enumerating-cgroups\">Enumerating cgroups</h2> <p>You can look into <code>/proc/cgroups</code> to see the different control group subsystems known to the system, the hierarchy they belong to, and how many groups they contain.</p> <p>You can also look at <code>/proc/&lt;pid&gt;/cgroup</code> to see which control groups a process belongs to. The control group will be shown as a path relative to the root of the hierarchy mountpoint; e.g., <code>/</code> means “this process has not been assigned into a particular group”, while <code>/lxc/pumpkin</code> means that the process is likely to be a member of a container named <code>pumpkin</code>.</p> <h2 id=\"finding-the-cgroup-for-a-given-container\">Finding the cgroup for a given container</h2> <p>For each container, one cgroup will be created in each hierarchy. On older systems with older versions of the LXC userland tools, the name of the cgroup will be the name of the container. With more recent versions of the LXC tools, the cgroup will be <code>lxc/&lt;container_name&gt;.</code></p> <p>For Docker containers using cgroups, the container name will be the full ID or long ID of the container. If a container shows up as ae836c95b4c3 in <code>docker ps</code>, its long ID might be something like <code>ae836c95b4c3c9e9179e0e91015512da89fdec91612f63cebae57df9a5444c79</code>. You can look it up with <code>docker inspect</code> or <code>docker ps --no-trunc</code>.</p> <p>Putting everything together to look at the memory metrics for a Docker container, take a look at <code>/sys/fs/cgroup/memory/docker/&lt;longid&gt;/</code>.</p> <h2 id=\"metrics-from-cgroups-memory-cpu-block-i-o\">Metrics from cgroups: memory, CPU, block I/O</h2> <p>For each subsystem (memory, CPU, and block I/O), you will find one or more pseudo-files containing statistics.</p> <h3 id=\"memory-metrics-memory-stat\">Memory metrics: <code>memory.stat</code>\n</h3> <p>Memory metrics are found in the “memory” cgroup. Note that the memory control group adds a little overhead, because it does very fine-grained accounting of the memory usage on your host. Therefore, many distros chose to not enable it by default. Generally, to enable it, all you have to do is to add some kernel command-line parameters: <code>cgroup_enable=memory swapaccount=1</code>.</p> <p>The metrics are in the pseudo-file <code>memory.stat</code>. Here is what it will look like:</p> <pre>cache 11492564992\nrss 1930993664\nmapped_file 306728960\npgpgin 406632648\npgpgout 403355412\nswap 0\npgfault 728281223\npgmajfault 1724\ninactive_anon 46608384\nactive_anon 1884520448\ninactive_file 7003344896\nactive_file 4489052160\nunevictable 32768\nhierarchical_memory_limit 9223372036854775807\nhierarchical_memsw_limit 9223372036854775807\ntotal_cache 11492564992\ntotal_rss 1930993664\ntotal_mapped_file 306728960\ntotal_pgpgin 406632648\ntotal_pgpgout 403355412\ntotal_swap 0\ntotal_pgfault 728281223\ntotal_pgmajfault 1724\ntotal_inactive_anon 46608384\ntotal_active_anon 1884520448\ntotal_inactive_file 7003344896\ntotal_active_file 4489052160\ntotal_unevictable 32768\n</pre> <p>The first half (without the <code>total_</code> prefix) contains statistics relevant to the processes within the cgroup, excluding sub-cgroups. The second half (with the <code>total_</code> prefix) includes sub-cgroups as well.</p> <p>Some metrics are “gauges”, i.e., values that can increase or decrease (e.g., swap, the amount of swap space used by the members of the cgroup). Some others are “counters”, i.e., values that can only go up, because they represent occurrences of a specific event (e.g., pgfault, which indicates the number of page faults which happened since the creation of the cgroup; this number can never decrease).</p> <ul> <li><p><strong>cache:</strong><br> the amount of memory used by the processes of this control group that can be associated precisely with a block on a block device. When you read from and write to files on disk, this amount will increase. This will be the case if you use “conventional” I/O (<code>open</code>, <code>read</code>, <code>write</code> syscalls) as well as mapped files (with <code>mmap</code>). It also accounts for the memory used by <code>tmpfs</code> mounts, though the reasons are unclear.</p></li> <li><p><strong>rss:</strong><br> the amount of memory that <em>doesn’t</em> correspond to anything on disk: stacks, heaps, and anonymous memory maps.</p></li> <li><p><strong>mapped_file:</strong><br> indicates the amount of memory mapped by the processes in the control group. It doesn’t give you information about <em>how much</em> memory is used; it rather tells you <em>how</em> it is used.</p></li> <li><p><strong>pgfault and pgmajfault:</strong><br> indicate the number of times that a process of the cgroup triggered a “page fault” and a “major fault”, respectively. A page fault happens when a process accesses a part of its virtual memory space which is nonexistent or protected. The former can happen if the process is buggy and tries to access an invalid address (it will then be sent a <code>SIGSEGV</code> signal, typically killing it with the famous <code>Segmentation fault</code> message). The latter can happen when the process reads from a memory zone which has been swapped out, or which corresponds to a mapped file: in that case, the kernel will load the page from disk, and let the CPU complete the memory access. It can also happen when the process writes to a copy-on-write memory zone: likewise, the kernel will preempt the process, duplicate the memory page, and resume the write operation on the process` own copy of the page. “Major” faults happen when the kernel actually has to read the data from disk. When it just has to duplicate an existing page, or allocate an empty page, it’s a regular (or “minor”) fault.</p></li> <li><p><strong>swap:</strong><br> the amount of swap currently used by the processes in this cgroup.</p></li> <li><p><strong>active_anon and inactive_anon:</strong><br> the amount of <em>anonymous</em> memory that has been identified has respectively <em>active</em> and <em>inactive</em> by the kernel. “Anonymous” memory is the memory that is <em>not</em> linked to disk pages. In other words, that’s the equivalent of the rss counter described above. In fact, the very definition of the rss counter is <strong>active_anon</strong> + <strong>inactive_anon</strong> - <strong>tmpfs</strong> (where tmpfs is the amount of memory used up by <code>tmpfs</code> filesystems mounted by this control group). Now, what’s the difference between “active” and “inactive”? Pages are initially “active”; and at regular intervals, the kernel sweeps over the memory, and tags some pages as “inactive”. Whenever they are accessed again, they are immediately retagged “active”. When the kernel is almost out of memory, and time comes to swap out to disk, the kernel will swap “inactive” pages.</p></li> <li><p><strong>active_file and inactive_file:</strong><br> cache memory, with <em>active</em> and <em>inactive</em> similar to the <em>anon</em> memory above. The exact formula is cache = <strong>active_file</strong> + <strong>inactive_file</strong> + <strong>tmpfs</strong>. The exact rules used by the kernel to move memory pages between active and inactive sets are different from the ones used for anonymous memory, but the general principle is the same. Note that when the kernel needs to reclaim memory, it is cheaper to reclaim a clean (=non modified) page from this pool, since it can be reclaimed immediately (while anonymous pages and dirty/modified pages have to be written to disk first).</p></li> <li><p><strong>unevictable:</strong><br> the amount of memory that cannot be reclaimed; generally, it will account for memory that has been “locked” with <code>mlock</code>. It is often used by crypto frameworks to make sure that secret keys and other sensitive material never gets swapped out to disk.</p></li> <li><p><strong>memory and memsw limits:</strong><br> These are not really metrics, but a reminder of the limits applied to this cgroup. The first one indicates the maximum amount of physical memory that can be used by the processes of this control group; the second one indicates the maximum amount of RAM+swap.</p></li> </ul> <p>Accounting for memory in the page cache is very complex. If two processes in different control groups both read the same file (ultimately relying on the same blocks on disk), the corresponding memory charge will be split between the control groups. It’s nice, but it also means that when a cgroup is terminated, it could increase the memory usage of another cgroup, because they are not splitting the cost anymore for those memory pages.</p> <h3 id=\"cpu-metrics-cpuacct-stat\">CPU metrics: <code>cpuacct.stat</code>\n</h3> <p>Now that we’ve covered memory metrics, everything else will look very simple in comparison. CPU metrics will be found in the <code>cpuacct</code> controller.</p> <p>For each container, you will find a pseudo-file <code>cpuacct.stat</code>, containing the CPU usage accumulated by the processes of the container, broken down between <code>user</code> and <code>system</code> time. If you’re not familiar with the distinction, <code>user</code> is the time during which the processes were in direct control of the CPU (i.e., executing process code), and <code>system</code> is the time during which the CPU was executing system calls on behalf of those processes.</p> <p>Those times are expressed in ticks of 1/100th of a second. Actually, they are expressed in “user jiffies”. There are <code>USER_HZ</code> <em>“jiffies”</em> per second, and on x86 systems, <code>USER_HZ</code> is 100. This used to map exactly to the number of scheduler “ticks” per second; but with the advent of higher frequency scheduling, as well as <a href=\"http://lwn.net/Articles/549580/\">tickless kernels</a>, the number of kernel ticks wasn’t relevant anymore. It stuck around anyway, mainly for legacy and compatibility reasons.</p> <h3 id=\"block-i-o-metrics\">Block I/O metrics</h3> <p>Block I/O is accounted in the <code>blkio</code> controller. Different metrics are scattered across different files. While you can find in-depth details in the <a href=\"https://www.kernel.org/doc/Documentation/cgroup-v1/blkio-controller.txt\">blkio-controller</a> file in the kernel documentation, here is a short list of the most relevant ones:</p> <ul> <li><p><strong>blkio.sectors:</strong><br> contain the number of 512-bytes sectors read and written by the processes member of the cgroup, device by device. Reads and writes are merged in a single counter.</p></li> <li><p><strong>blkio.io_service_bytes:</strong><br> indicates the number of bytes read and written by the cgroup. It has 4 counters per device, because for each device, it differentiates between synchronous vs. asynchronous I/O, and reads vs. writes.</p></li> <li><p><strong>blkio.io_serviced:</strong><br> the number of I/O operations performed, regardless of their size. It also has 4 counters per device.</p></li> <li><p><strong>blkio.io_queued:</strong><br> indicates the number of I/O operations currently queued for this cgroup. In other words, if the cgroup isn’t doing any I/O, this will be zero. Note that the opposite is not true. In other words, if there is no I/O queued, it does not mean that the cgroup is idle (I/O-wise). It could be doing purely synchronous reads on an otherwise quiescent device, which is therefore able to handle them immediately, without queuing. Also, while it is helpful to figure out which cgroup is putting stress on the I/O subsystem, keep in mind that it is a relative quantity. Even if a process group does not perform more I/O, its queue size can increase just because the device load increases because of other devices.</p></li> </ul> <h2 id=\"network-metrics\">Network metrics</h2> <p>Network metrics are not exposed directly by control groups. There is a good explanation for that: network interfaces exist within the context of <em>network namespaces</em>. The kernel could probably accumulate metrics about packets and bytes sent and received by a group of processes, but those metrics wouldn’t be very useful. You want per-interface metrics (because traffic happening on the local <code>lo</code> interface doesn’t really count). But since processes in a single cgroup can belong to multiple network namespaces, those metrics would be harder to interpret: multiple network namespaces means multiple <code>lo</code> interfaces, potentially multiple <code>eth0</code> interfaces, etc.; so this is why there is no easy way to gather network metrics with control groups.</p> <p>Instead we can gather network metrics from other sources:</p> <h3 id=\"iptables\">IPtables</h3> <p>IPtables (or rather, the netfilter framework for which iptables is just an interface) can do some serious accounting.</p> <p>For instance, you can setup a rule to account for the outbound HTTP traffic on a web server:</p> <pre>$ iptables -I OUTPUT -p tcp --sport 80\n</pre> <p>There is no <code>-j</code> or <code>-g</code> flag, so the rule will just count matched packets and go to the following rule.</p> <p>Later, you can check the values of the counters, with:</p> <pre>$ iptables -nxvL OUTPUT\n</pre> <p>Technically, <code>-n</code> is not required, but it will prevent iptables from doing DNS reverse lookups, which are probably useless in this scenario.</p> <p>Counters include packets and bytes. If you want to setup metrics for container traffic like this, you could execute a <code>for</code> loop to add two <code>iptables</code> rules per container IP address (one in each direction), in the <code>FORWARD</code> chain. This will only meter traffic going through the NAT layer; you will also have to add traffic going through the userland proxy.</p> <p>Then, you will need to check those counters on a regular basis. If you happen to use <code>collectd</code>, there is a <a href=\"https://collectd.org/wiki/index.php/Table_of_Plugins\">nice plugin</a> to automate iptables counters collection.</p> <h3 id=\"interface-level-counters\">Interface-level counters</h3> <p>Since each container has a virtual Ethernet interface, you might want to check directly the TX and RX counters of this interface. You will notice that each container is associated to a virtual Ethernet interface in your host, with a name like <code>vethKk8Zqi</code>. Figuring out which interface corresponds to which container is, unfortunately, difficult.</p> <p>But for now, the best way is to check the metrics <em>from within the containers</em>. To accomplish this, you can run an executable from the host environment within the network namespace of a container using <strong>ip-netns magic</strong>.</p> <p>The <code>ip-netns exec</code> command will let you execute any program (present in the host system) within any network namespace visible to the current process. This means that your host will be able to enter the network namespace of your containers, but your containers won’t be able to access the host, nor their sibling containers. Containers will be able to “see” and affect their sub-containers, though.</p> <p>The exact format of the command is:</p> <pre>$ ip netns exec &lt;nsname&gt; &lt;command...&gt;\n</pre> <p>For example:</p> <pre>$ ip netns exec mycontainer netstat -i\n</pre> <p><code>ip netns</code> finds the “mycontainer” container by using namespaces pseudo-files. Each process belongs to one network namespace, one PID namespace, one <code>mnt</code> namespace, etc., and those namespaces are materialized under <code>/proc/&lt;pid&gt;/ns/</code>. For example, the network namespace of PID 42 is materialized by the pseudo-file <code>/proc/42/ns/net</code>.</p> <p>When you run <code>ip netns exec mycontainer ...</code>, it expects <code>/var/run/netns/mycontainer</code> to be one of those pseudo-files. (Symlinks are accepted.)</p> <p>In other words, to execute a command within the network namespace of a container, we need to:</p> <ul> <li>Find out the PID of any process within the container that we want to investigate;</li> <li>Create a symlink from <code>/var/run/netns/&lt;somename&gt;</code> to <code>/proc/&lt;thepid&gt;/ns/net</code>\n</li> <li>Execute <code>ip netns exec &lt;somename&gt; ....</code>\n</li> </ul> <p>Please review <a href=\"#enumerating-cgroups\"><em>Enumerating Cgroups</em></a> to learn how to find the cgroup of a process running in the container of which you want to measure network usage. From there, you can examine the pseudo-file named <code>tasks</code>, which contains the PIDs that are in the control group (i.e., in the container). Pick any one of them.</p> <p>Putting everything together, if the “short ID” of a container is held in the environment variable <code>$CID</code>, then you can do this:</p> <pre>$ TASKS=/sys/fs/cgroup/devices/docker/$CID*/tasks\n$ PID=$(head -n 1 $TASKS)\n$ mkdir -p /var/run/netns\n$ ln -sf /proc/$PID/ns/net /var/run/netns/$CID\n$ ip netns exec $CID netstat -i\n</pre> <h2 id=\"tips-for-high-performance-metric-collection\">Tips for high-performance metric collection</h2> <p>Note that running a new process each time you want to update metrics is (relatively) expensive. If you want to collect metrics at high resolutions, and/or over a large number of containers (think 1000 containers on a single host), you do not want to fork a new process each time.</p> <p>Here is how to collect metrics from a single process. You will have to write your metric collector in C (or any language that lets you do low-level system calls). You need to use a special system call, <code>setns()</code>, which lets the current process enter any arbitrary namespace. It requires, however, an open file descriptor to the namespace pseudo-file (remember: that’s the pseudo-file in <code>/proc/&lt;pid&gt;/ns/net</code>).</p> <p>However, there is a catch: you must not keep this file descriptor open. If you do, when the last process of the control group exits, the namespace will not be destroyed, and its network resources (like the virtual interface of the container) will stay around for ever (or until you close that file descriptor).</p> <p>The right approach would be to keep track of the first PID of each container, and re-open the namespace pseudo-file each time.</p> <h2 id=\"collecting-metrics-when-a-container-exits\">Collecting metrics when a container exits</h2> <p>Sometimes, you do not care about real time metric collection, but when a container exits, you want to know how much CPU, memory, etc. it has used.</p> <p>Docker makes this difficult because it relies on <code>lxc-start</code>, which carefully cleans up after itself, but it is still possible. It is usually easier to collect metrics at regular intervals (e.g., every minute, with the collectd LXC plugin) and rely on that instead.</p> <p>But, if you’d still like to gather the stats when a container stops, here is how:</p> <p>For each container, start a collection process, and move it to the control groups that you want to monitor by writing its PID to the tasks file of the cgroup. The collection process should periodically re-read the tasks file to check if it’s the last process of the control group. (If you also want to collect network statistics as explained in the previous section, you should also move the process to the appropriate network namespace.)</p> <p>When the container exits, <code>lxc-start</code> will try to delete the control groups. It will fail, since the control group is still in use; but that’s fine. You process should now detect that it is the only one remaining in the group. Now is the right time to collect all the metrics you need!</p> <p>Finally, your process should move itself back to the root control group, and remove the container control group. To remove a control group, just <code>rmdir</code> its directory. It’s counter-intuitive to <code>rmdir</code> a directory as it still contains files; but remember that this is a pseudo-filesystem, so usual rules don’t apply. After the cleanup is done, the collection process can exit safely.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/runmetrics/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/runmetrics/</a>\n  </p>\n</div>\n","engine/admin/logging/log_tags/index":"<h1 id=\"log-tags\">Log Tags</h1> <p>The <code>tag</code> log option specifies how to format a tag that identifies the container’s log messages. By default, the system uses the first 12 characters of the container id. To override this behavior, specify a <code>tag</code> option:</p> <pre>docker run --log-driver=fluentd --log-opt fluentd-address=myhost.local:24224 --log-opt tag=\"mailer\"\n</pre> <p>Docker supports some special template markup you can use when specifying a tag’s value:</p> <table> <thead> <tr> <th>Markup</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>{{.ID}}</code></td> <td>The first 12 characters of the container id.</td> </tr> <tr> <td><code>{{.FullID}}</code></td> <td>The full container id.</td> </tr> <tr> <td><code>{{.Name}}</code></td> <td>The container name.</td> </tr> <tr> <td><code>{{.ImageID}}</code></td> <td>The first 12 characters of the container’s image id.</td> </tr> <tr> <td><code>{{.ImageFullID}}</code></td> <td>The container’s full image identifier.</td> </tr> <tr> <td><code>{{.ImageName}}</code></td> <td>The name of the image used by the container.</td> </tr> </tbody> </table> <p>For example, specifying a <code>--log-opt tag=\"{{.ImageName}}/{{.Name}}/{{.ID}}\"</code> value yields <code>syslog</code> log lines like:</p> <pre>Aug  7 18:33:19 HOSTNAME docker/hello-world/foobar/5790672ab6a0[9103]: Hello from Docker.\n</pre> <p>At startup time, the system sets the <code>container_name</code> field and <code>{{.Name}}</code> in the tags. If you use <code>docker rename</code> to rename a container, the new name is not reflected in the log messages. Instead, these messages continue to use the original container name.</p> <p>For advanced usage, the generated tag’s use <a href=\"http://golang.org/pkg/text/template/\">go templates</a> and the container’s <a href=\"https://github.com/docker/docker/blob/master/daemon/logger/context.go\">logging context</a>.</p> <blockquote> <p><strong>Note</strong>:The driver specific log options <code>syslog-tag</code>, <code>fluentd-tag</code> and <code>gelf-tag</code> still work for backwards compatibility. However, going forward you should standardize on using the generic <code>tag</code> log option instead.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/logging/log_tags/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/logging/log_tags/</a>\n  </p>\n</div>\n","engine/admin/logging/awslogs/index":"<h1 id=\"amazon-cloudwatch-logs-logging-driver\">Amazon CloudWatch Logs logging driver</h1> <p>The <code>awslogs</code> logging driver sends container logs to <a href=\"https://aws.amazon.com/cloudwatch/details/#log-monitoring\">Amazon CloudWatch Logs</a>. Log entries can be retrieved through the <a href=\"https://console.aws.amazon.com/cloudwatch/home#logs:\">AWS Management Console</a> or the <a href=\"http://docs.aws.amazon.com/cli/latest/reference/logs/index.html\">AWS SDKs and Command Line Tools</a>.</p> <h2 id=\"usage\">Usage</h2> <p>You can configure the default logging driver by passing the <code>--log-driver</code> option to the Docker daemon:</p> <pre>docker daemon --log-driver=awslogs\n</pre> <p>You can set the logging driver for a specific container by using the <code>--log-driver</code> option to <code>docker run</code>:</p> <pre>docker run --log-driver=awslogs ...\n</pre> <h2 id=\"amazon-cloudwatch-logs-options\">Amazon CloudWatch Logs options</h2> <p>You can use the <code>--log-opt NAME=VALUE</code> flag to specify Amazon CloudWatch Logs logging driver options.</p> <h3 id=\"awslogs-region\">awslogs-region</h3> <p>The <code>awslogs</code> logging driver sends your Docker logs to a specific region. Use the <code>awslogs-region</code> log option or the <code>AWS_REGION</code> environment variable to set the region. By default, if your Docker daemon is running on an EC2 instance and no region is set, the driver uses the instance’s region.</p> <pre>docker run --log-driver=awslogs --log-opt awslogs-region=us-east-1 ...\n</pre> <h3 id=\"awslogs-group\">awslogs-group</h3> <p>You must specify a <a href=\"http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatchLogs.html\">log group</a> for the <code>awslogs</code> logging driver. You can specify the log group with the <code>awslogs-group</code> log option:</p> <pre>docker run --log-driver=awslogs --log-opt awslogs-region=us-east-1 --log-opt awslogs-group=myLogGroup ...\n</pre> <h3 id=\"awslogs-stream\">awslogs-stream</h3> <p>To configure which <a href=\"http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatchLogs.html\">log stream</a> should be used, you can specify the <code>awslogs-stream</code> log option. If not specified, the container ID is used as the log stream.</p> <blockquote> <p><strong>Note:</strong> Log streams within a given log group should only be used by one container at a time. Using the same log stream for multiple containers concurrently can cause reduced logging performance.</p> </blockquote> <h2 id=\"credentials\">Credentials</h2> <p>You must provide AWS credentials to the Docker daemon to use the <code>awslogs</code> logging driver. You can provide these credentials with the <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code> environment variables, the default AWS shared credentials file (<code>~/.aws/credentials</code> of the root user), or (if you are running the Docker daemon on an Amazon EC2 instance) the Amazon EC2 instance profile.</p> <p>Credentials must have a policy applied that allows the <code>logs:CreateLogStream</code> and <code>logs:PutLogEvents</code> actions, as shown in the following example.</p> <pre>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/logging/awslogs/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/logging/awslogs/</a>\n  </p>\n</div>\n","engine/admin/logging/fluentd/index":"<h1 id=\"fluentd-logging-driver\">Fluentd logging driver</h1> <p>The <code>fluentd</code> logging driver sends container logs to the <a href=\"http://www.fluentd.org/\">Fluentd</a> collector as structured log data. Then, users can use any of the <a href=\"http://www.fluentd.org/plugins\">various output plugins of Fluentd</a> to write these logs to various destinations.</p> <p>In addition to the log message itself, the <code>fluentd</code> log driver sends the following metadata in the structured log message:</p> <table> <thead> <tr> <th>Field</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>container_id</code></td> <td>The full 64-character container ID.</td> </tr> <tr> <td><code>container_name</code></td> <td>The container name at the time it was started. If you use <code>docker rename</code> to rename a container, the new name is not reflected in the journal entries.</td> </tr> <tr> <td><code>source</code></td> <td>\n<code>stdout</code> or <code>stderr</code>\n</td> </tr> </tbody> </table> <p>The <code>docker logs</code> command is not available for this logging driver.</p> <h2 id=\"usage\">Usage</h2> <p>Some options are supported by specifying <code>--log-opt</code> as many times as needed:</p> <ul> <li>\n<code>fluentd-address</code>: specify <code>host:port</code> to connect <code>localhost:24224</code>\n</li> <li>\n<code>tag</code>: specify tag for fluentd message, which interpret some markup, ex <code>{{.ID}}</code>, <code>{{.FullID}}</code> or <code>{{.Name}}</code> <code>docker.{{.ID}}</code>\n</li> </ul> <p>Configure the default logging driver by passing the <code>--log-driver</code> option to the Docker daemon:</p> <pre>docker daemon --log-driver=fluentd\n</pre> <p>To set the logging driver for a specific container, pass the <code>--log-driver</code> option to <code>docker run</code>:</p> <pre>docker run --log-driver=fluentd ...\n</pre> <p>Before using this logging driver, launch a Fluentd daemon. The logging driver connects to this daemon through <code>localhost:24224</code> by default. Use the <code>fluentd-address</code> option to connect to a different address.</p> <pre>docker run --log-driver=fluentd --log-opt fluentd-address=myhost.local:24224\n</pre> <p>If container cannot connect to the Fluentd daemon, the container stops immediately unless the <code>fluentd-async-connect</code> option is used.</p> <h2 id=\"options\">Options</h2> <p>Users can use the <code>--log-opt NAME=VALUE</code> flag to specify additional Fluentd logging driver options.</p> <h3 id=\"fluentd-address\">fluentd-address</h3> <p>By default, the logging driver connects to <code>localhost:24224</code>. Supply the <code>fluentd-address</code> option to connect to a different address.</p> <pre>docker run --log-driver=fluentd --log-opt fluentd-address=myhost.local:24224\n</pre> <h3 id=\"tag\">tag</h3> <p>By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the <a href=\"../log_tags/index\">log tag option documentation</a> for customizing the log tag format.</p> <h3 id=\"labels-and-env\">labels and env</h3> <p>The <code>labels</code> and <code>env</code> options each take a comma-separated list of keys. If there is collision between <code>label</code> and <code>env</code> keys, the value of the <code>env</code> takes precedence. Both options add additional fields to the extra attributes of a logging message.</p> <h3 id=\"fluentd-async-connect\">fluentd-async-connect</h3> <p>Docker connects to Fluentd in the background. Messages are buffered until the connection is established.</p> <h2 id=\"fluentd-daemon-management-with-docker\">Fluentd daemon management with Docker</h2> <p>About <code>Fluentd</code> itself, see <a href=\"http://www.fluentd.org\">the project webpage</a> and <a href=\"http://docs.fluentd.org/\">its documents</a>.</p> <p>To use this logging driver, start the <code>fluentd</code> daemon on a host. We recommend that you use <a href=\"https://hub.docker.com/r/fluent/fluentd/\">the Fluentd docker image</a>. This image is especially useful if you want to aggregate multiple container logs on each host then, later, transfer the logs to another Fluentd node to create an aggregate store.</p> <h3 id=\"testing-container-loggers\">Testing container loggers</h3> <ol> <li>\n<p>Write a configuration file (<code>test.conf</code>) to dump input logs:</p> <pre>&lt;source&gt;\n  @type forward\n&lt;/source&gt;\n\n&lt;match docker.**&gt;\n  @type stdout\n&lt;/match&gt;\n</pre>\n</li> <li>\n<p>Launch Fluentd container with this configuration file:</p> <pre>$ docker run -it -p 24224:24224 -v /path/to/conf/test.conf:/fluentd/etc -e FLUENTD_CONF=test.conf fluent/fluentd:latest\n</pre>\n</li> <li>\n<p>Start one or more containers with the <code>fluentd</code> logging driver:</p> <pre>$ docker run --log-driver=fluentd your/application\n</pre>\n</li> </ol><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/logging/fluentd/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/logging/fluentd/</a>\n  </p>\n</div>\n","engine/admin/registry_mirror/index":"<h1 id=\"run-a-local-registry-mirror\">Run a local registry mirror</h1> <p>The original content was deprecated. <a href=\"https://docs.docker.com/v1.6/articles/registry_mirror\">An archived version</a> is available in the 1.7 documentation. For information about configuring mirrors with the latest Docker Registry version, please file a support request with <a href=\"https://github.com/docker/distribution/issues\">the Distribution project</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/registry_mirror/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/registry_mirror/</a>\n  </p>\n</div>\n","engine/admin/logging/etwlogs/index":"<h1 id=\"etw-logging-driver\">ETW logging driver</h1> <p>The ETW logging driver forwards container logs as ETW events. ETW stands for Event Tracing in Windows, and is the common framework for tracing applications in Windows. Each ETW event contains a message with both the log and its context information. A client can then create an ETW listener to listen to these events.</p> <p>The ETW provider that this logging driver registers with Windows, has the GUID identifier of: <code>{a3693192-9ed6-46d2-a981-f8226c8363bd}</code>. A client creates an ETW listener and registers to listen to events from the logging driver’s provider. It does not matter the order in which the provider and listener are created. A client can create their ETW listener and start listening for events from the provider, before the provider has been registered with the system.</p> <h2 id=\"usage\">Usage</h2> <p>Here is an example of how to listen to these events using the logman utility program included in most installations of Windows:</p> <ol> <li><code>logman start -ets DockerContainerLogs -p {a3693192-9ed6-46d2-a981-f8226c8363bd} 0 0 -o trace.etl</code></li> <li>Run your container(s) with the etwlogs driver, by adding <code>--log-driver=etwlogs</code> to the Docker run command, and generate log messages.</li> <li><code>logman stop -ets DockerContainerLogs</code></li> <li>This will generate an etl file that contains the events. One way to convert this file into human-readable form is to run: <code>tracerpt -y trace.etl</code>. <br>\n</li> </ol> <p>Each ETW event will contain a structured message string in this format:</p> <pre>container_name: %s, image_name: %s, container_id: %s, image_id: %s, source: [stdout | stderr], log: %s\n</pre> <p>Details on each item in the message can be found below:</p> <table> <thead> <tr> <th>Field</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>container_name</code></td> <td>The container name at the time it was started.</td> </tr> <tr> <td><code>image_name</code></td> <td>The name of the container’s image.</td> </tr> <tr> <td><code>container_id</code></td> <td>The full 64-character container ID.</td> </tr> <tr> <td><code>image_id</code></td> <td>The full ID of the container’s image.</td> </tr> <tr> <td><code>source</code></td> <td>\n<code>stdout</code> or <code>stderr</code>.</td> </tr> <tr> <td><code>log</code></td> <td>The container log message.</td> </tr> </tbody> </table> <p>Here is an example event message:</p> <pre>container_name: backstabbing_spence, \nimage_name: windowsservercore, \ncontainer_id: f14bb55aa862d7596b03a33251c1be7dbbec8056bbdead1da8ec5ecebbe29731, \nimage_id: sha256:2f9e19bd998d3565b4f345ac9aaf6e3fc555406239a4fb1b1ba879673713824b, \nsource: stdout, \nlog: Hello world!\n</pre> <p>A client can parse this message string to get both the log message, as well as its context information. Note that the time stamp is also available within the ETW event.</p> <p><strong>Note</strong> This ETW provider emits only a message string, and not a specially structured ETW event. Therefore, it is not required to register a manifest file with the system to read and interpret its ETW events.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/logging/etwlogs/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/logging/etwlogs/</a>\n  </p>\n</div>\n","engine/admin/logging/gcplogs/index":"<h1 id=\"google-cloud-logging-driver\">Google Cloud Logging driver</h1> <p>The Google Cloud Logging driver sends container logs to <a href=\"https://cloud.google.com/logging/docs/\" target=\"_blank\">Google Cloud Logging</a>.</p> <h2 id=\"usage\">Usage</h2> <p>You can configure the default logging driver by passing the <code>--log-driver</code> option to the Docker daemon:</p> <pre>docker daemon --log-driver=gcplogs\n</pre> <p>You can set the logging driver for a specific container by using the <code>--log-driver</code> option to <code>docker run</code>:</p> <pre>docker run --log-driver=gcplogs ...\n</pre> <p>This log driver does not implement a reader so it is incompatible with <code>docker logs</code>.</p> <p>If Docker detects that it is running in a Google Cloud Project, it will discover configuration from the <a href=\"https://cloud.google.com/compute/docs/metadata\" target=\"_blank\">instance metadata service</a>. Otherwise, the user must specify which project to log to using the <code>--gcp-project</code> log option and Docker will attempt to obtain credentials from the <a href=\"https://developers.google.com/identity/protocols/application-default-credentials\" target=\"_blank\">Google Application Default Credential</a>. The <code>--gcp-project</code> takes precedence over information discovered from the metadata server so a Docker daemon running in a Google Cloud Project can be overriden to log to a different Google Cloud Project using <code>--gcp-project</code>.</p> <h2 id=\"gcplogs-options\">gcplogs options</h2> <p>You can use the <code>--log-opt NAME=VALUE</code> flag to specify these additional Google Cloud Logging driver options:</p> <table> <thead> <tr> <th>Option</th> <th>Required</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>gcp-project</code></td> <td>optional</td> <td>Which GCP project to log to. Defaults to discovering this value from the GCE metadata service.</td> </tr> <tr> <td><code>gcp-log-cmd</code></td> <td>optional</td> <td>Whether to log the command that the container was started with. Defaults to false.</td> </tr> <tr> <td><code>labels</code></td> <td>optional</td> <td>Comma-separated list of keys of labels, which should be included in message, if these labels are specified for container.</td> </tr> <tr> <td><code>env</code></td> <td>optional</td> <td>Comma-separated list of keys of environment variables, which should be included in message, if these variables are specified for container.</td> </tr> </tbody> </table> <p>If there is collision between <code>label</code> and <code>env</code> keys, the value of the <code>env</code> takes precedence. Both options add additional fields to the attributes of a logging message.</p> <p>Below is an example of the logging options required to log to the default logging destination which is discovered by querying the GCE metadata server.</p> <pre>docker run --log-driver=gcplogs \\\n    --log-opt labels=location\n    --log-opt env=TEST\n    --log-opt gcp-log-cmd=true\n    --env \"TEST=false\"\n    --label location=west\n    your/application\n</pre> <p>This configuration also directs the driver to include in the payload the label <code>location</code>, the environment variable <code>ENV</code>, and the command used to start the container.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/logging/gcplogs/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/logging/gcplogs/</a>\n  </p>\n</div>\n","engine/admin/logging/journald/index":"<h1 id=\"journald-logging-driver\">Journald logging driver</h1> <p>The <code>journald</code> logging driver sends container logs to the <a href=\"http://www.freedesktop.org/software/systemd/man/systemd-journald.service.html\">systemd journal</a>. Log entries can be retrieved using the <code>journalctl</code> command, through use of the journal API, or using the <code>docker logs</code> command.</p> <p>In addition to the text of the log message itself, the <code>journald</code> log driver stores the following metadata in the journal with each message:</p> <table> <thead> <tr> <th>Field</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>CONTAINER_ID</code></td> <td>The container ID truncated to 12 characters.</td> </tr> <tr> <td><code>CONTAINER_ID_FULL</code></td> <td>The full 64-character container ID.</td> </tr> <tr> <td><code>CONTAINER_NAME</code></td> <td>The container name at the time it was started. If you use <code>docker rename</code> to rename a container, the new name is not reflected in the journal entries.</td> </tr> <tr> <td><code>CONTAINER_TAG</code></td> <td>The container tag (<a href=\"../log_tags/index\">log tag option documentation</a>).</td> </tr> </tbody> </table> <h2 id=\"usage\">Usage</h2> <p>You can configure the default logging driver by passing the <code>--log-driver</code> option to the Docker daemon:</p> <pre>docker daemon --log-driver=journald\n</pre> <p>You can set the logging driver for a specific container by using the <code>--log-driver</code> option to <code>docker run</code>:</p> <pre>docker run --log-driver=journald ...\n</pre> <h2 id=\"options\">Options</h2> <p>Users can use the <code>--log-opt NAME=VALUE</code> flag to specify additional journald logging driver options.</p> <h3 id=\"tag\">tag</h3> <p>Specify template to set <code>CONTAINER_TAG</code> value in journald logs. Refer to <a href=\"../log_tags/index\">log tag option documentation</a> for customizing the log tag format.</p> <h3 id=\"labels-and-env\">labels and env</h3> <p>The <code>labels</code> and <code>env</code> options each take a comma-separated list of keys. If there is collision between <code>label</code> and <code>env</code> keys, the value of the <code>env</code> takes precedence. Both options add additional metadata in the journal with each message.</p> <h2 id=\"note-regarding-container-names\">Note regarding container names</h2> <p>The value logged in the <code>CONTAINER_NAME</code> field is the container name that was set at startup. If you use <code>docker rename</code> to rename a container, the new name will not be reflected in the journal entries. Journal entries will continue to use the original name.</p> <h2 id=\"retrieving-log-messages-with-journalctl\">Retrieving log messages with journalctl</h2> <p>You can use the <code>journalctl</code> command to retrieve log messages. You can apply filter expressions to limit the retrieved messages to a specific container. For example, to retrieve all log messages from a container referenced by name:</p> <pre># journalctl CONTAINER_NAME=webserver\n</pre> <p>You can make use of additional filters to further limit the messages retrieved. For example, to see just those messages generated since the system last booted:</p> <pre># journalctl -b CONTAINER_NAME=webserver\n</pre> <p>Or to retrieve log messages in JSON format with complete metadata:</p> <pre># journalctl -o json CONTAINER_NAME=webserver\n</pre> <h2 id=\"retrieving-log-messages-with-the-journal-api\">Retrieving log messages with the journal API</h2> <p>This example uses the <code>systemd</code> Python module to retrieve container logs:</p> <pre>import systemd.journal\n\nreader = systemd.journal.Reader()\nreader.add_match('CONTAINER_NAME=web')\n\nfor msg in reader:\n  print '{CONTAINER_ID_FULL}: {MESSAGE}'.format(**msg)\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/logging/journald/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/logging/journald/</a>\n  </p>\n</div>\n","engine/admin/logging/splunk/index":"<h1 id=\"splunk-logging-driver\">Splunk logging driver</h1> <p>The <code>splunk</code> logging driver sends container logs to <a href=\"http://dev.splunk.com/view/event-collector/SP-CAAAE6M\">HTTP Event Collector</a> in Splunk Enterprise and Splunk Cloud.</p> <h2 id=\"usage\">Usage</h2> <p>You can configure the default logging driver by passing the <code>--log-driver</code> option to the Docker daemon:</p> <pre>docker daemon --log-driver=splunk\n</pre> <p>You can set the logging driver for a specific container by using the <code>--log-driver</code> option to <code>docker run</code>:</p> <pre>docker run --log-driver=splunk ...\n</pre> <h2 id=\"splunk-options\">Splunk options</h2> <p>You can use the <code>--log-opt NAME=VALUE</code> flag to specify these additional Splunk logging driver options:</p> <table> <thead> <tr> <th>Option</th> <th>Required</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>splunk-token</code></td> <td>required</td> <td>Splunk HTTP Event Collector token.</td> </tr> <tr> <td><code>splunk-url</code></td> <td>required</td> <td>Path to your Splunk Enterprise or Splunk Cloud instance (including port and schema used by HTTP Event Collector) <code>https://your_splunk_instance:8088</code>.</td> </tr> <tr> <td><code>splunk-source</code></td> <td>optional</td> <td>Event source.</td> </tr> <tr> <td><code>splunk-sourcetype</code></td> <td>optional</td> <td>Event source type.</td> </tr> <tr> <td><code>splunk-index</code></td> <td>optional</td> <td>Event index.</td> </tr> <tr> <td><code>splunk-capath</code></td> <td>optional</td> <td>Path to root certificate.</td> </tr> <tr> <td><code>splunk-caname</code></td> <td>optional</td> <td>Name to use for validating server certificate; by default the hostname of the <code>splunk-url</code> will be used.</td> </tr> <tr> <td><code>splunk-insecureskipverify</code></td> <td>optional</td> <td>Ignore server certificate validation.</td> </tr> <tr> <td><code>tag</code></td> <td>optional</td> <td>Specify tag for message, which interpret some markup. Default value is <code>{{.ID}}</code> (12 characters of the container ID). Refer to the <a href=\"../log_tags/index\">log tag option documentation</a> for customizing the log tag format.</td> </tr> <tr> <td><code>labels</code></td> <td>optional</td> <td>Comma-separated list of keys of labels, which should be included in message, if these labels are specified for container.</td> </tr> <tr> <td><code>env</code></td> <td>optional</td> <td>Comma-separated list of keys of environment variables, which should be included in message, if these variables are specified for container.</td> </tr> </tbody> </table> <p>If there is collision between <code>label</code> and <code>env</code> keys, the value of the <code>env</code> takes precedence. Both options add additional fields to the attributes of a logging message.</p> <p>Below is an example of the logging option specified for the Splunk Enterprise instance. The instance is installed locally on the same machine on which the Docker daemon is running. The path to the root certificate and Common Name is specified using an HTTPS schema. This is used for verification. The <code>SplunkServerDefaultCert</code> is automatically generated by Splunk certificates.</p> <pre>docker run --log-driver=splunk \\\n    --log-opt splunk-token=176FCEBF-4CF5-4EDF-91BC-703796522D20 \\\n    --log-opt splunk-url=https://splunkhost:8088 \\\n    --log-opt splunk-capath=/path/to/cert/cacert.pem \\\n    --log-opt splunk-caname=SplunkServerDefaultCert\n    --log-opt tag=\"{{.Name}}/{{.FullID}}\"\n    --log-opt labels=location\n    --log-opt env=TEST\n    --env \"TEST=false\"\n    --label location=west\n    your/application\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/logging/splunk/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/logging/splunk/</a>\n  </p>\n</div>\n","engine/admin/logging/overview/index":"<h1 id=\"configure-logging-drivers\">Configure logging drivers</h1> <p>The container can have a different logging driver than the Docker daemon. Use the <code>--log-driver=VALUE</code> with the <code>docker run</code> command to configure the container’s logging driver. If the <code>--log-driver</code> option is not set, docker uses the default (<code>json-file</code>) logging driver. The following options are supported:</p> <table> <thead> <tr> <th>Driver</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>none</code></td> <td>Disables any logging for the container. <code>docker logs</code> won’t be available with this driver.</td> </tr> <tr> <td><code>json-file</code></td> <td>Default logging driver for Docker. Writes JSON messages to file.</td> </tr> <tr> <td><code>syslog</code></td> <td>Syslog logging driver for Docker. Writes log messages to syslog.</td> </tr> <tr> <td><code>journald</code></td> <td>Journald logging driver for Docker. Writes log messages to <code>journald</code>.</td> </tr> <tr> <td><code>gelf</code></td> <td>Graylog Extended Log Format (GELF) logging driver for Docker. Writes log messages to a GELF endpoint likeGraylog or Logstash.</td> </tr> <tr> <td><code>fluentd</code></td> <td>Fluentd logging driver for Docker. Writes log messages to <code>fluentd</code> (forward input).</td> </tr> <tr> <td><code>awslogs</code></td> <td>Amazon CloudWatch Logs logging driver for Docker. Writes log messages to Amazon CloudWatch Logs.</td> </tr> <tr> <td><code>splunk</code></td> <td>Splunk logging driver for Docker. Writes log messages to <code>splunk</code> using HTTP Event Collector.</td> </tr> <tr> <td><code>etwlogs</code></td> <td>ETW logging driver for Docker on Windows. Writes log messages as ETW events.</td> </tr> <tr> <td><code>gcplogs</code></td> <td>Google Cloud Logging driver for Docker. Writes log messages to Google Cloud Logging.</td> </tr> </tbody> </table> <p>The <code>docker logs</code>command is available only for the <code>json-file</code> and <code>journald</code> logging drivers.</p> <p>The <code>labels</code> and <code>env</code> options add additional attributes for use with logging drivers that accept them. Each option takes a comma-separated list of keys. If there is collision between <code>label</code> and <code>env</code> keys, the value of the <code>env</code> takes precedence.</p> <p>To use attributes, specify them when you start the Docker daemon. For example, to manually start the daemon with the <code>json-file</code> driver, and include additional attributes in the output, run the following command:</p> <pre>$ docker daemon \\\n    --log-driver=json-file \\\n    --log-opt labels=foo \\\n    --log-opt env=foo,fizz\n</pre> <p>Then, run a container and specify values for the <code>labels</code> or <code>env</code>. For example, you might use this:</p> <pre>$ docker run -dit --label foo=bar -e fizz=buzz alpine sh\n</pre> <p>This adds additional fields to the log depending on the driver, e.g. for <code>json-file</code> that looks like:</p> <pre>\"attrs\":{\"fizz\":\"buzz\",\"foo\":\"bar\"}\n</pre> <h2 id=\"json-file-options\">json-file options</h2> <p>The following logging options are supported for the <code>json-file</code> logging driver:</p> <pre>--log-opt max-size=[0-9+][k|m|g]\n--log-opt max-file=[0-9+]\n--log-opt labels=label1,label2\n--log-opt env=env1,env2\n</pre> <p>Logs that reach <code>max-size</code> are rolled over. You can set the size in kilobytes(k), megabytes(m), or gigabytes(g). eg <code>--log-opt max-size=50m</code>. If <code>max-size</code> is not set, then logs are not rolled over.</p> <p><code>max-file</code> specifies the maximum number of files that a log is rolled over before being discarded. eg <code>--log-opt max-file=100</code>. If <code>max-size</code> is not set, then <code>max-file</code> is not honored.</p> <p>If <code>max-size</code> and <code>max-file</code> are set, <code>docker logs</code> only returns the log lines from the newest log file.</p> <h2 id=\"syslog-options\">syslog options</h2> <p>The following logging options are supported for the <code>syslog</code> logging driver:</p> <pre>--log-opt syslog-address=[tcp|udp|tcp+tls]://host:port\n--log-opt syslog-address=unix://path\n--log-opt syslog-facility=daemon\n--log-opt syslog-tls-ca-cert=/etc/ca-certificates/custom/ca.pem\n--log-opt syslog-tls-cert=/etc/ca-certificates/custom/cert.pem\n--log-opt syslog-tls-key=/etc/ca-certificates/custom/key.pem\n--log-opt syslog-tls-skip-verify=true\n--log-opt tag=\"mailer\"\n--log-opt syslog-format=[rfc5424|rfc3164] \n</pre> <p><code>syslog-address</code> specifies the remote syslog server address where the driver connects to. If not specified it defaults to the local unix socket of the running system. If transport is either <code>tcp</code> or <code>udp</code> and <code>port</code> is not specified it defaults to <code>514</code> The following example shows how to have the <code>syslog</code> driver connect to a <code>syslog</code> remote server at <code>192.168.0.42</code> on port <code>123</code></p> <pre>$ docker run --log-driver=syslog --log-opt syslog-address=tcp://192.168.0.42:123\n</pre> <p>The <code>syslog-facility</code> option configures the syslog facility. By default, the system uses the <code>daemon</code> value. To override this behavior, you can provide an integer of 0 to 23 or any of the following named facilities:</p> <ul> <li><code>kern</code></li> <li><code>user</code></li> <li><code>mail</code></li> <li><code>daemon</code></li> <li><code>auth</code></li> <li><code>syslog</code></li> <li><code>lpr</code></li> <li><code>news</code></li> <li><code>uucp</code></li> <li><code>cron</code></li> <li><code>authpriv</code></li> <li><code>ftp</code></li> <li><code>local0</code></li> <li><code>local1</code></li> <li><code>local2</code></li> <li><code>local3</code></li> <li><code>local4</code></li> <li><code>local5</code></li> <li><code>local6</code></li> <li><code>local7</code></li> </ul> <p><code>syslog-tls-ca-cert</code> specifies the absolute path to the trust certificates signed by the CA. This option is ignored if the address protocol is not <code>tcp+tls</code>.</p> <p><code>syslog-tls-cert</code> specifies the absolute path to the TLS certificate file. This option is ignored if the address protocol is not <code>tcp+tls</code>.</p> <p><code>syslog-tls-key</code> specifies the absolute path to the TLS key file. This option is ignored if the address protocol is not <code>tcp+tls</code>.</p> <p><code>syslog-tls-skip-verify</code> configures the TLS verification. This verification is enabled by default, but it can be overriden by setting this option to <code>true</code>. This option is ignored if the address protocol is not <code>tcp+tls</code>.</p> <p>By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the <a href=\"../log_tags/index\">log tag option documentation</a> for customizing the log tag format.</p> <p><code>syslog-format</code> specifies syslog message format to use when logging. If not specified it defaults to the local unix syslog format without hostname specification. Specify rfc3164 to perform logging in RFC-3164 compatible format. Specify rfc5424 to perform logging in RFC-5424 compatible format</p> <h2 id=\"journald-options\">journald options</h2> <p>The <code>journald</code> logging driver stores the container id in the journal’s <code>CONTAINER_ID</code> field. For detailed information on working with this logging driver, see <a href=\"../journald/index\">the journald logging driver</a> reference documentation.</p> <h2 id=\"gelf-options\">GELF options</h2> <p>The GELF logging driver supports the following options:</p> <pre>--log-opt gelf-address=udp://host:port\n--log-opt tag=\"database\"\n--log-opt labels=label1,label2\n--log-opt env=env1,env2\n--log-opt gelf-compression-type=gzip\n--log-opt gelf-compression-level=1\n</pre> <p>The <code>gelf-address</code> option specifies the remote GELF server address that the driver connects to. Currently, only <code>udp</code> is supported as the transport and you must specify a <code>port</code> value. The following example shows how to connect the <code>gelf</code> driver to a GELF remote server at <code>192.168.0.42</code> on port <code>12201</code></p> <pre>$ docker run -dit \\\n    --log-driver=gelf \\\n    --log-opt gelf-address=udp://192.168.0.42:12201 \\\n    alpine sh\n</pre> <p>By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the <a href=\"../log_tags/index\">log tag option documentation</a> for customizing the log tag format.</p> <p>The <code>labels</code> and <code>env</code> options are supported by the gelf logging driver. It adds additional key on the <code>extra</code> fields, prefixed by an underscore (<code>_</code>).</p> <pre>// […]\n\"_foo\": \"bar\",\n\"_fizz\": \"buzz\",\n// […]\n</pre> <p>The <code>gelf-compression-type</code> option can be used to change how the GELF driver compresses each log message. The accepted values are <code>gzip</code>, <code>zlib</code> and <code>none</code>. <code>gzip</code> is chosen by default.</p> <p>The <code>gelf-compression-level</code> option can be used to change the level of compresssion when <code>gzip</code> or <code>zlib</code> is selected as <code>gelf-compression-type</code>. Accepted value must be from from -1 to 9 (BestCompression). Higher levels typically run slower but compress more. Default value is 1 (BestSpeed).</p> <h2 id=\"fluentd-options\">Fluentd options</h2> <p>You can use the <code>--log-opt NAME=VALUE</code> flag to specify these additional Fluentd logging driver options.</p> <ul> <li>\n<code>fluentd-address</code>: specify <code>host:port</code> to connect [localhost:24224]</li> <li>\n<code>tag</code>: specify tag for <code>fluentd</code> message</li> <li>\n<code>fluentd-buffer-limit</code>: specify the maximum size of the fluentd log buffer [8MB]</li> <li>\n<code>fluentd-retry-wait</code>: initial delay before a connection retry (after which it increases exponentially) [1000ms]</li> <li>\n<code>fluentd-max-retries</code>: maximum number of connection retries before abrupt failure of docker [1073741824]</li> <li>\n<code>fluentd-async-connect</code>: whether to block on initial connection or not [false]</li> </ul> <p>For example, to specify both additional options:</p> <pre>$ docker run -dit \\\n    --log-driver=fluentd \\\n    --log-opt fluentd-address=localhost:24224 \\\n    --log-opt tag=\"docker.{{.Name}}\" \\\n    alpine sh\n</pre> <p>If container cannot connect to the Fluentd daemon on the specified address and <code>fluentd-async-connect</code> is not enabled, the container stops immediately. For detailed information on working with this logging driver, see <a href=\"../fluentd/index\">the fluentd logging driver</a></p> <h2 id=\"amazon-cloudwatch-logs-options\">Amazon CloudWatch Logs options</h2> <p>The Amazon CloudWatch Logs logging driver supports the following options:</p> <pre>--log-opt awslogs-region=&lt;aws_region&gt;\n--log-opt awslogs-group=&lt;log_group_name&gt;\n--log-opt awslogs-stream=&lt;log_stream_name&gt;\n</pre> <p>For detailed information on working with this logging driver, see <a href=\"../awslogs/index\">the awslogs logging driver</a> reference documentation.</p> <h2 id=\"splunk-options\">Splunk options</h2> <p>The Splunk logging driver requires the following options:</p> <pre>--log-opt splunk-token=&lt;splunk_http_event_collector_token&gt;\n--log-opt splunk-url=https://your_splunk_instance:8088\n</pre> <p>For detailed information about working with this logging driver, see the <a href=\"../splunk/index\">Splunk logging driver</a> reference documentation.</p> <h2 id=\"etw-logging-driver-options\">ETW logging driver options</h2> <p>The etwlogs logging driver does not require any options to be specified. This logging driver forwards each log message as an ETW event. An ETW listener can then be created to listen for these events.</p> <p>The ETW logging driver is only available on Windows. For detailed information on working with this logging driver, see <a href=\"../etwlogs/index\">the ETW logging driver</a> reference documentation.</p> <h2 id=\"google-cloud-logging-options\">Google Cloud Logging options</h2> <p>The Google Cloud Logging driver supports the following options:</p> <pre>--log-opt gcp-project=&lt;gcp_projext&gt;\n--log-opt labels=&lt;label1&gt;,&lt;label2&gt;\n--log-opt env=&lt;envvar1&gt;,&lt;envvar2&gt;\n--log-opt log-cmd=true\n</pre> <p>For detailed information about working with this logging driver, see the <a href=\"../gcplogs/index\">Google Cloud Logging driver</a>. reference documentation.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/admin/logging/overview/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/admin/logging/overview/</a>\n  </p>\n</div>\n","engine/security/security/index":"<h1 id=\"docker-security\">Docker security</h1> <p>There are three major areas to consider when reviewing Docker security:</p> <ul> <li>the intrinsic security of the kernel and its support for namespaces and cgroups;</li> <li>the attack surface of the Docker daemon itself;</li> <li>loopholes in the container configuration profile, either by default, or when customized by users.</li> <li>the “hardening” security features of the kernel and how they interact with containers.</li> </ul> <h2 id=\"kernel-namespaces\">Kernel namespaces</h2> <p>Docker containers are very similar to LXC containers, and they have similar security features. When you start a container with <code>docker run</code>, behind the scenes Docker creates a set of namespaces and control groups for the container.</p> <p><strong>Namespaces provide the first and most straightforward form of isolation</strong>: processes running within a container cannot see, and even less affect, processes running in another container, or in the host system.</p> <p><strong>Each container also gets its own network stack</strong>, meaning that a container doesn’t get privileged access to the sockets or interfaces of another container. Of course, if the host system is setup accordingly, containers can interact with each other through their respective network interfaces — just like they can interact with external hosts. When you specify public ports for your containers or use <a href=\"../../userguide/networking/default_network/dockerlinks/index\"><em>links</em></a> then IP traffic is allowed between containers. They can ping each other, send/receive UDP packets, and establish TCP connections, but that can be restricted if necessary. From a network architecture point of view, all containers on a given Docker host are sitting on bridge interfaces. This means that they are just like physical machines connected through a common Ethernet switch; no more, no less.</p> <p>How mature is the code providing kernel namespaces and private networking? Kernel namespaces were introduced <a href=\"http://lxc.sourceforge.net/index.php/about/kernel-namespaces/\">between kernel version 2.6.15 and 2.6.26</a>. This means that since July 2008 (date of the 2.6.26 release ), namespace code has been exercised and scrutinized on a large number of production systems. And there is more: the design and inspiration for the namespaces code are even older. Namespaces are actually an effort to reimplement the features of <a href=\"http://en.wikipedia.org/wiki/OpenVZ\">OpenVZ</a> in such a way that they could be merged within the mainstream kernel. And OpenVZ was initially released in 2005, so both the design and the implementation are pretty mature.</p> <h2 id=\"control-groups\">Control groups</h2> <p>Control Groups are another key component of Linux Containers. They implement resource accounting and limiting. They provide many useful metrics, but they also help ensure that each container gets its fair share of memory, CPU, disk I/O; and, more importantly, that a single container cannot bring the system down by exhausting one of those resources.</p> <p>So while they do not play a role in preventing one container from accessing or affecting the data and processes of another container, they are essential to fend off some denial-of-service attacks. They are particularly important on multi-tenant platforms, like public and private PaaS, to guarantee a consistent uptime (and performance) even when some applications start to misbehave.</p> <p>Control Groups have been around for a while as well: the code was started in 2006, and initially merged in kernel 2.6.24.</p> <h2 id=\"docker-daemon-attack-surface\">Docker daemon attack surface</h2> <p>Running containers (and applications) with Docker implies running the Docker daemon. This daemon currently requires <code>root</code> privileges, and you should therefore be aware of some important details.</p> <p>First of all, <strong>only trusted users should be allowed to control your Docker daemon</strong>. This is a direct consequence of some powerful Docker features. Specifically, Docker allows you to share a directory between the Docker host and a guest container; and it allows you to do so without limiting the access rights of the container. This means that you can start a container where the <code>/host</code> directory will be the <code>/</code> directory on your host; and the container will be able to alter your host filesystem without any restriction. This is similar to how virtualization systems allow filesystem resource sharing. Nothing prevents you from sharing your root filesystem (or even your root block device) with a virtual machine.</p> <p>This has a strong security implication: for example, if you instrument Docker from a web server to provision containers through an API, you should be even more careful than usual with parameter checking, to make sure that a malicious user cannot pass crafted parameters causing Docker to create arbitrary containers.</p> <p>For this reason, the REST API endpoint (used by the Docker CLI to communicate with the Docker daemon) changed in Docker 0.5.2, and now uses a UNIX socket instead of a TCP socket bound on 127.0.0.1 (the latter being prone to cross-site request forgery attacks if you happen to run Docker directly on your local machine, outside of a VM). You can then use traditional UNIX permission checks to limit access to the control socket.</p> <p>You can also expose the REST API over HTTP if you explicitly decide to do so. However, if you do that, being aware of the above mentioned security implication, you should ensure that it will be reachable only from a trusted network or VPN; or protected with e.g., <code>stunnel</code> and client SSL certificates. You can also secure them with <a href=\"../https/index\">HTTPS and certificates</a>.</p> <p>The daemon is also potentially vulnerable to other inputs, such as image loading from either disk with ‘docker load’, or from the network with ‘docker pull’. This has been a focus of improvement in the community, especially for ‘pull’ security. While these overlap, it should be noted that ‘docker load’ is a mechanism for backup and restore and is not currently considered a secure mechanism for loading images. As of Docker 1.3.2, images are now extracted in a chrooted subprocess on Linux/Unix platforms, being the first-step in a wider effort toward privilege separation.</p> <p>Eventually, it is expected that the Docker daemon will run restricted privileges, delegating operations well-audited sub-processes, each with its own (very limited) scope of Linux capabilities, virtual network setup, filesystem management, etc. That is, most likely, pieces of the Docker engine itself will run inside of containers.</p> <p>Finally, if you run Docker on a server, it is recommended to run exclusively Docker in the server, and move all other services within containers controlled by Docker. Of course, it is fine to keep your favorite admin tools (probably at least an SSH server), as well as existing monitoring/supervision processes (e.g., NRPE, collectd, etc).</p> <h2 id=\"linux-kernel-capabilities\">Linux kernel capabilities</h2> <p>By default, Docker starts containers with a restricted set of capabilities. What does that mean?</p> <p>Capabilities turn the binary “root/non-root” dichotomy into a fine-grained access control system. Processes (like web servers) that just need to bind on a port below 1024 do not have to run as root: they can just be granted the <code>net_bind_service</code> capability instead. And there are many other capabilities, for almost all the specific areas where root privileges are usually needed.</p> <p>This means a lot for container security; let’s see why!</p> <p>Your average server (bare metal or virtual machine) needs to run a bunch of processes as root. Those typically include SSH, cron, syslogd; hardware management tools (e.g., load modules), network configuration tools (e.g., to handle DHCP, WPA, or VPNs), and much more. A container is very different, because almost all of those tasks are handled by the infrastructure around the container:</p> <ul> <li>SSH access will typically be managed by a single server running on the Docker host;</li> <li>\n<code>cron</code>, when necessary, should run as a user process, dedicated and tailored for the app that needs its scheduling service, rather than as a platform-wide facility;</li> <li>log management will also typically be handed to Docker, or by third-party services like Loggly or Splunk;</li> <li>hardware management is irrelevant, meaning that you never need to run <code>udevd</code> or equivalent daemons within containers;</li> <li>network management happens outside of the containers, enforcing separation of concerns as much as possible, meaning that a container should never need to perform <code>ifconfig</code>, <code>route</code>, or ip commands (except when a container is specifically engineered to behave like a router or firewall, of course).</li> </ul> <p>This means that in most cases, containers will not need “real” root privileges <em>at all</em>. And therefore, containers can run with a reduced capability set; meaning that “root” within a container has much less privileges than the real “root”. For instance, it is possible to:</p> <ul> <li>deny all “mount” operations;</li> <li>deny access to raw sockets (to prevent packet spoofing);</li> <li>deny access to some filesystem operations, like creating new device nodes, changing the owner of files, or altering attributes (including the immutable flag);</li> <li>deny module loading;</li> <li>and many others.</li> </ul> <p>This means that even if an intruder manages to escalate to root within a container, it will be much harder to do serious damage, or to escalate to the host.</p> <p>This won’t affect regular web apps; but malicious users will find that the arsenal at their disposal has shrunk considerably! By default Docker drops all capabilities except <a href=\"https://github.com/docker/docker/blob/master/oci/defaults_linux.go#L64-L79\">those needed</a>, a whitelist instead of a blacklist approach. You can see a full list of available capabilities in <a href=\"http://man7.org/linux/man-pages/man7/capabilities.7.html\">Linux manpages</a>.</p> <p>One primary risk with running Docker containers is that the default set of capabilities and mounts given to a container may provide incomplete isolation, either independently, or when used in combination with kernel vulnerabilities.</p> <p>Docker supports the addition and removal of capabilities, allowing use of a non-default profile. This may make Docker more secure through capability removal, or less secure through the addition of capabilities. The best practice for users would be to remove all capabilities except those explicitly required for their processes.</p> <h2 id=\"other-kernel-security-features\">Other kernel security features</h2> <p>Capabilities are just one of the many security features provided by modern Linux kernels. It is also possible to leverage existing, well-known systems like TOMOYO, AppArmor, SELinux, GRSEC, etc. with Docker.</p> <p>While Docker currently only enables capabilities, it doesn’t interfere with the other systems. This means that there are many different ways to harden a Docker host. Here are a few examples.</p> <ul> <li>You can run a kernel with GRSEC and PAX. This will add many safety checks, both at compile-time and run-time; it will also defeat many exploits, thanks to techniques like address randomization. It doesn’t require Docker-specific configuration, since those security features apply system-wide, independent of containers.</li> <li>If your distribution comes with security model templates for Docker containers, you can use them out of the box. For instance, we ship a template that works with AppArmor and Red Hat comes with SELinux policies for Docker. These templates provide an extra safety net (even though it overlaps greatly with capabilities).</li> <li>You can define your own policies using your favorite access control mechanism.</li> </ul> <p>Just like there are many third-party tools to augment Docker containers with e.g., special network topologies or shared filesystems, you can expect to see tools to harden existing Docker containers without affecting Docker’s core.</p> <p>As of Docker 1.10 User Namespaces are supported directly by the docker daemon. This feature allows for the root user in a container to be mapped to a non uid-0 user outside the container, which can help to mitigate the risks of container breakout. This facility is available but not enabled by default.</p> <p>Refer to the <a href=\"../../reference/commandline/daemon/index#daemon-user-namespace-options\">daemon command</a> in the command line reference for more information on this feature. Additional information on the implementation of User Namespaces in Docker can be found in <a href=\"https://integratedcode.us/2015/10/13/user-namespaces-have-arrived-in-docker/\" target=\"_blank\">this blog post</a>.</p> <h2 id=\"conclusions\">Conclusions</h2> <p>Docker containers are, by default, quite secure; especially if you take care of running your processes inside the containers as non-privileged users (i.e., non-<code>root</code>).</p> <p>You can add an extra layer of safety by enabling AppArmor, SELinux, GRSEC, or your favorite hardening solution.</p> <p>Last but not least, if you see interesting security features in other containerization systems, these are simply kernels features that may be implemented in Docker as well. We welcome users to submit issues, pull requests, and communicate via the mailing list.</p> <h2 id=\"related-information\">Related Information</h2> <ul> <li><a href=\"../trust/index\">Use trusted images</a></li> <li><a href=\"../seccomp/index\">Seccomp security profiles for Docker</a></li> <li><a href=\"../apparmor/index\">AppArmor security profiles for Docker</a></li> <li><a href=\"https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e\">On the Security of Containers (2014)</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/security/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/security/</a>\n  </p>\n</div>\n","engine/security/non-events/index":"<h1 id=\"docker-security-non-events\">Docker Security Non-events</h1> <p>This page lists security vulnerabilities which Docker mitigated, such that processes run in Docker containers were never vulnerable to the bug—even before it was fixed. This assumes containers are run without adding extra capabilities or not run as <code>--privileged</code>.</p> <p>The list below is not even remotely complete. Rather, it is a sample of the few bugs we’ve actually noticed to have attracted security review and publicly disclosed vulnerabilities. In all likelihood, the bugs that haven’t been reported far outnumber those that have. Luckily, since Docker’s approach to secure by default through apparmor, seccomp, and dropping capabilities, it likely mitigates unknown bugs just as well as it does known ones.</p> <p>Bugs mitigated:</p> <ul> <li>\n<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2013-1956\">CVE-2013-1956</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2013-1957\">1957</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2013-1958\">1958</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2013-1959\">1959</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2013-1979\">1979</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-4014\">CVE-2014-4014</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-5206\">5206</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-5207\">5207</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-7970\">7970</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-7975\">7975</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-2925\">CVE-2015-2925</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-8543\">8543</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-3134\">CVE-2016-3134</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-3135\">3135</a>, etc.: The introduction of unprivileged user namespaces lead to a huge increase in the attack surface available to unprivileged users by giving such users legitimate access to previously root-only system calls like <code>mount()</code>. All of these CVEs are examples of security vulnerabilities due to introduction of user namespaces. Docker can use user namespaces to set up containers, but then disallows the process inside the container from creating its own nested namespaces through the default seccomp profile, rendering these vulnerabilities unexploitable.</li> <li>\n<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0181\">CVE-2014-0181</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3339\">CVE-2015-3339</a>: These are bugs that require the presence of a setuid binary. Docker disables setuid binaries inside containers via the <code>NO_NEW_PRIVS</code> process flag and other mechanisms.</li> <li>\n<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-4699\">CVE-2014-4699</a>: A bug in <code>ptrace()</code> could allow privilege escalation. Docker disables <code>ptrace()</code> inside the container using apparmor, seccomp and by dropping <code>CAP_PTRACE</code>. Three times the layers of protection there!</li> <li>\n<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-9529\">CVE-2014-9529</a>: A series of crafted <code>keyctl()</code> calls could cause kernel DoS / memory corruption. Docker disables <code>keyctl()</code> inside containers using seccomp.</li> <li>\n<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3214\">CVE-2015-3214</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-4036\">4036</a>: These are bugs in common virtualization drivers which could allow a guest OS user to execute code on the host OS. Exploiting them requires access to virtualization devices in the guest. Docker hides direct access to these devices when run without <code>--privileged</code>. Interestingly, these seem to be cases where containers are “more secure” than a VM, going against common wisdom that VMs are “more secure” than containers.</li> <li>\n<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0728\">CVE-2016-0728</a>: Use-after-free caused by crafted <code>keyctl()</code> calls could lead to privilege escalation. Docker disables <code>keyctl()</code> inside containers using the default seccomp profile.</li> <li>\n<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-2383\">CVE-2016-2383</a>: A bug in eBPF -- the special in-kernel DSL used to express things like seccomp filters -- allowed arbitrary reads of kernel memory. The <code>bpf()</code> system call is blocked inside Docker containers using (ironically) seccomp.</li> </ul> <p>Bugs <em>not</em> mitigated:</p> <ul> <li>\n<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3290\">CVE-2015-3290</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-5157\">5157</a>: Bugs in the kernel’s non-maskable interrupt handling allowed privilege escalation. Can be exploited in Docker containers because the <code>modify_ldt()</code> system call is not currently blocked using seccomp.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/non-events/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/non-events/</a>\n  </p>\n</div>\n","engine/security/https/index":"<h1 id=\"protect-the-docker-daemon-socket\">Protect the Docker daemon socket</h1> <p>By default, Docker runs via a non-networked Unix socket. It can also optionally communicate using an HTTP socket.</p> <p>If you need Docker to be reachable via the network in a safe manner, you can enable TLS by specifying the <code>tlsverify</code> flag and pointing Docker’s <code>tlscacert</code> flag to a trusted CA certificate.</p> <p>In the daemon mode, it will only allow connections from clients authenticated by a certificate signed by that CA. In the client mode, it will only connect to servers with a certificate signed by that CA.</p> <blockquote> <p><strong>Warning</strong>: Using TLS and managing a CA is an advanced topic. Please familiarize yourself with OpenSSL, x509 and TLS before using it in production.</p> <p><strong>Warning</strong>: These TLS commands will only generate a working set of certificates on Linux. Mac OS X comes with a version of OpenSSL that is incompatible with the certificates that Docker requires.</p> </blockquote> <h2 id=\"create-a-ca-server-and-client-keys-with-openssl\">Create a CA, server and client keys with OpenSSL</h2> <blockquote> <p><strong>Note</strong>: replace all instances of <code>$HOST</code> in the following example with the DNS name of your Docker daemon’s host.</p> </blockquote> <p>First generate CA private and public keys:</p> <pre>$ openssl genrsa -aes256 -out ca-key.pem 4096\nGenerating RSA private key, 4096 bit long modulus\n............................................................................................................................................................................................++\n........++\ne is 65537 (0x10001)\nEnter pass phrase for ca-key.pem:\nVerifying - Enter pass phrase for ca-key.pem:\n$ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem\nEnter pass phrase for ca-key.pem:\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:\nState or Province Name (full name) [Some-State]:Queensland\nLocality Name (eg, city) []:Brisbane\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Docker Inc\nOrganizational Unit Name (eg, section) []:Sales\nCommon Name (e.g. server FQDN or YOUR name) []:$HOST\nEmail Address []:Sven@home.org.au\n</pre> <p>Now that we have a CA, you can create a server key and certificate signing request (CSR). Make sure that “Common Name” (i.e., server FQDN or YOUR name) matches the hostname you will use to connect to Docker:</p> <blockquote> <p><strong>Note</strong>: replace all instances of <code>$HOST</code> in the following example with the DNS name of your Docker daemon’s host.</p> </blockquote> <pre>$ openssl genrsa -out server-key.pem 4096\nGenerating RSA private key, 4096 bit long modulus\n.....................................................................++\n.................................................................................................++\ne is 65537 (0x10001)\n$ openssl req -subj \"/CN=$HOST\" -sha256 -new -key server-key.pem -out server.csr\n</pre> <p>Next, we’re going to sign the public key with our CA:</p> <p>Since TLS connections can be made via IP address as well as DNS name, they need to be specified when creating the certificate. For example, to allow connections using <code>10.10.10.20</code> and <code>127.0.0.1</code>:</p> <pre>$ echo subjectAltName = IP:10.10.10.20,IP:127.0.0.1 &gt; extfile.cnf\n\n$ openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \\\n  -CAcreateserial -out server-cert.pem -extfile extfile.cnf\nSignature ok\nsubject=/CN=your.host.com\nGetting CA Private Key\nEnter pass phrase for ca-key.pem:\n</pre> <p>For client authentication, create a client key and certificate signing request:</p> <pre>$ openssl genrsa -out key.pem 4096\nGenerating RSA private key, 4096 bit long modulus\n.........................................................++\n................++\ne is 65537 (0x10001)\n$ openssl req -subj '/CN=client' -new -key key.pem -out client.csr\n</pre> <p>To make the key suitable for client authentication, create an extensions config file:</p> <pre>$ echo extendedKeyUsage = clientAuth &gt; extfile.cnf\n</pre> <p>Now sign the public key:</p> <pre>$ openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \\\n  -CAcreateserial -out cert.pem -extfile extfile.cnf\nSignature ok\nsubject=/CN=client\nGetting CA Private Key\nEnter pass phrase for ca-key.pem:\n</pre> <p>After generating <code>cert.pem</code> and <code>server-cert.pem</code> you can safely remove the two certificate signing requests:</p> <pre>$ rm -v client.csr server.csr\n</pre> <p>With a default <code>umask</code> of 022, your secret keys will be <em>world-readable</em> and writable for you and your group.</p> <p>In order to protect your keys from accidental damage, you will want to remove their write permissions. To make them only readable by you, change file modes as follows:</p> <pre>$ chmod -v 0400 ca-key.pem key.pem server-key.pem\n</pre> <p>Certificates can be world-readable, but you might want to remove write access to prevent accidental damage:</p> <pre>$ chmod -v 0444 ca.pem server-cert.pem cert.pem\n</pre> <p>Now you can make the Docker daemon only accept connections from clients providing a certificate trusted by our CA:</p> <pre>$ docker daemon --tlsverify --tlscacert=ca.pem --tlscert=server-cert.pem --tlskey=server-key.pem \\\n  -H=0.0.0.0:2376\n</pre> <p>To be able to connect to Docker and validate its certificate, you now need to provide your client keys, certificates and trusted CA:</p> <blockquote> <p><strong>Note</strong>: replace all instances of <code>$HOST</code> in the following example with the DNS name of your Docker daemon’s host.</p> </blockquote> <pre>$ docker --tlsverify --tlscacert=ca.pem --tlscert=cert.pem --tlskey=key.pem \\\n  -H=$HOST:2376 version\n</pre> <blockquote> <p><strong>Note</strong>: Docker over TLS should run on TCP port 2376.</p> <p><strong>Warning</strong>: As shown in the example above, you don’t have to run the <code>docker</code> client with <code>sudo</code> or the <code>docker</code> group when you use certificate authentication. That means anyone with the keys can give any instructions to your Docker daemon, giving them root access to the machine hosting the daemon. Guard these keys as you would a root password!</p> </blockquote> <h2 id=\"secure-by-default\">Secure by default</h2> <p>If you want to secure your Docker client connections by default, you can move the files to the <code>.docker</code> directory in your home directory -- and set the <code>DOCKER_HOST</code> and <code>DOCKER_TLS_VERIFY</code> variables as well (instead of passing <code>-H=tcp://$HOST:2376</code> and <code>--tlsverify</code> on every call).</p> <pre>$ mkdir -pv ~/.docker\n$ cp -v {ca,cert,key}.pem ~/.docker\n$ export DOCKER_HOST=tcp://$HOST:2376 DOCKER_TLS_VERIFY=1\n</pre> <p>Docker will now connect securely by default:</p> <pre>$ docker ps\n</pre> <h2 id=\"other-modes\">Other modes</h2> <p>If you don’t want to have complete two-way authentication, you can run Docker in various other modes by mixing the flags.</p> <h3 id=\"daemon-modes\">Daemon modes</h3> <ul> <li>\n<code>tlsverify</code>, <code>tlscacert</code>, <code>tlscert</code>, <code>tlskey</code> set: Authenticate clients</li> <li>\n<code>tls</code>, <code>tlscert</code>, <code>tlskey</code>: Do not authenticate clients</li> </ul> <h3 id=\"client-modes\">Client modes</h3> <ul> <li>\n<code>tls</code>: Authenticate server based on public/default CA pool</li> <li>\n<code>tlsverify</code>, <code>tlscacert</code>: Authenticate server based on given CA</li> <li>\n<code>tls</code>, <code>tlscert</code>, <code>tlskey</code>: Authenticate with client certificate, do not authenticate server based on given CA</li> <li>\n<code>tlsverify</code>, <code>tlscacert</code>, <code>tlscert</code>, <code>tlskey</code>: Authenticate with client certificate and authenticate server based on given CA</li> </ul> <p>If found, the client will send its client certificate, so you just need to drop your keys into <code>~/.docker/{ca,cert,key}.pem</code>. Alternatively, if you want to store your keys in another location, you can specify that location using the environment variable <code>DOCKER_CERT_PATH</code>.</p> <pre>$ export DOCKER_CERT_PATH=~/.docker/zone1/\n$ docker --tlsverify ps\n</pre> <h3 id=\"connecting-to-the-secure-docker-port-using-curl\">Connecting to the secure Docker port using <code>curl</code>\n</h3> <p>To use <code>curl</code> to make test API requests, you need to use three extra command line flags:</p> <pre>$ curl https://$HOST:2376/images/json \\\n  --cert ~/.docker/cert.pem \\\n  --key ~/.docker/key.pem \\\n  --cacert ~/.docker/ca.pem\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../certificates/index\">Using certificates for repository client verification</a></li> <li><a href=\"../trust/index\">Use trusted images</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/https/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/https/</a>\n  </p>\n</div>\n","engine/security/trust/trust_automation/index":"<h1 id=\"automation-with-content-trust\">Automation with content trust</h1> <p>Your automation systems that pull or build images can also work with trust. Any automation environment must set <code>DOCKER_TRUST_ENABLED</code> either manually or in a scripted fashion before processing images.</p> <h2 id=\"bypass-requests-for-passphrases\">Bypass requests for passphrases</h2> <p>To allow tools to wrap docker and push trusted content, there are two environment variables that allow you to provide the passphrases without an expect script, or typing them in:</p> <ul> <li><code>DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE</code></li> <li><code>DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE</code></li> </ul> <p>Docker attempts to use the contents of these environment variables as passphrase for the keys. For example, an image publisher can export the repository <code>target</code> and <code>snapshot</code> passphrases:</p> <pre>$  export DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE=\"u7pEQcGoebUHm6LHe6\"\n$  export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=\"l7pEQcTKJjUHm6Lpe4\"\n</pre> <p>Then, when pushing a new tag the Docker client does not request these values but signs automatically:</p> <pre>$  docker push docker/trusttest:latest\nThe push refers to a repository [docker.io/docker/trusttest] (len: 1)\na9539b34a6ab: Image already exists\nb3dbab3810fc: Image already exists\nlatest: digest: sha256:d149ab53f871 size: 3355\nSigning and pushing trust metadata\n</pre> <h2 id=\"building-with-content-trust\">Building with content trust</h2> <p>You can also build with content trust. Before running the <code>docker build</code> command, you should set the environment variable <code>DOCKER_CONTENT_TRUST</code> either manually or in a scripted fashion. Consider the simple Dockerfile below.</p> <pre>FROM docker/trusttest:latest\nRUN echo\n</pre> <p>The <code>FROM</code> tag is pulling a signed image. You cannot build an image that has a <code>FROM</code> that is not either present locally or signed. Given that content trust data exists for the tag <code>latest</code>, the following build should succeed:</p> <pre>$  docker build -t docker/trusttest:testing .\nUsing default tag: latest\nlatest: Pulling from docker/trusttest\n\nb3dbab3810fc: Pull complete\na9539b34a6ab: Pull complete\nDigest: sha256:d149ab53f871\n</pre> <p>If content trust is enabled, building from a Dockerfile that relies on tag without trust data, causes the build command to fail:</p> <pre>$  docker build -t docker/trusttest:testing .\nunable to process Dockerfile: No trust data for notrust\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../content_trust/index\">Content trust in Docker</a></li> <li><a href=\"../trust_key_mng/index\">Manage keys for content trust</a></li> <li><a href=\"../trust_delegation/index\">Delegations for content trust</a></li> <li><a href=\"../trust_sandbox/index\">Play in a content trust sandbox</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/trust/trust_automation/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/trust/trust_automation/</a>\n  </p>\n</div>\n","engine/security/certificates/index":"<h1 id=\"using-certificates-for-repository-client-verification\">Using certificates for repository client verification</h1> <p>In <a href=\"../https/index\">Running Docker with HTTPS</a>, you learned that, by default, Docker runs via a non-networked Unix socket and TLS must be enabled in order to have the Docker client and the daemon communicate securely over HTTPS. TLS ensures authenticity of the registry endpoint and that traffic to/from registry is encrypted.</p> <p>This article demonstrates how to ensure the traffic between the Docker registry (i.e., <em>a server</em>) and the Docker daemon (i.e., <em>a client</em>) traffic is encrypted and a properly authenticated using <em>certificate-based client-server authentication</em>.</p> <p>We will show you how to install a Certificate Authority (CA) root certificate for the registry and how to set the client TLS certificate for verification.</p> <h2 id=\"understanding-the-configuration\">Understanding the configuration</h2> <p>A custom certificate is configured by creating a directory under <code>/etc/docker/certs.d</code> using the same name as the registry’s hostname (e.g., <code>localhost</code>). All <code>*.crt</code> files are added to this directory as CA roots.</p> <blockquote> <p><strong>Note:</strong> In the absence of any root certificate authorities, Docker will use the system default (i.e., host’s root CA set).</p> </blockquote> <p>The presence of one or more <code>&lt;filename&gt;.key/cert</code> pairs indicates to Docker that there are custom certificates required for access to the desired repository.</p> <blockquote> <p><strong>Note:</strong> If there are multiple certificates, each will be tried in alphabetical order. If there is an authentication error (e.g., 403, 404, 5xx, etc.), Docker will continue to try with the next certificate.</p> </blockquote> <p>The following illustrates a configuration with multiple certs:</p> <pre>    /etc/docker/certs.d/        &lt;-- Certificate directory\n    └── localhost               &lt;-- Hostname\n       ├── client.cert          &lt;-- Client certificate\n       ├── client.key           &lt;-- Client key\n       └── localhost.crt        &lt;-- Certificate authority that signed\n                                    the registry certificate\n</pre> <p>The preceding example is operating-system specific and is for illustrative purposes only. You should consult your operating system documentation for creating an os-provided bundled certificate chain.</p> <h2 id=\"creating-the-client-certificates\">Creating the client certificates</h2> <p>You will use OpenSSL’s <code>genrsa</code> and <code>req</code> commands to first generate an RSA key and then use the key to create the certificate.</p> <pre>$ openssl genrsa -out client.key 4096\n$ openssl req -new -x509 -text -key client.key -out client.cert\n</pre> <blockquote> <p><strong>Note:</strong> These TLS commands will only generate a working set of certificates on Linux. The version of OpenSSL in Mac OS X is incompatible with the type of certificate Docker requires.</p> </blockquote> <h2 id=\"troubleshooting-tips\">Troubleshooting tips</h2> <p>The Docker daemon interprets `<code>.crt</code> files as CA certificates and <code>.cert</code> files as client certificates. If a CA certificate is accidentally given the extension <code>.cert</code> instead of the correct <code>.crt</code> extension, the Docker daemon logs the following error message:</p> <pre>Missing key KEY_NAME for client certificate CERT_NAME. Note that CA certificates should use the extension .crt.\n</pre> <h2 id=\"related-information\">Related Information</h2> <ul> <li><a href=\"../index\">Use trusted images</a></li> <li><a href=\"../https/index\">Protect the Docker daemon socket</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/certificates/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/certificates/</a>\n  </p>\n</div>\n","engine/security/trust/trust_delegation/index":"<h1 id=\"delegations-for-content-trust\">Delegations for content trust</h1> <p>Docker Engine supports the usage of the <code>targets/releases</code> delegation as the canonical source of a trusted image tag.</p> <p>Using this delegation allows you to collaborate with other publishers without sharing your repository key (a combination of your targets and snapshot keys - please see “<a href=\"../trust_key_mng/index\">Manage keys for content trust</a>” for more information). A collaborator can keep their own delegation key private.</p> <p>The <code>targets/releases</code> delegation is currently an optional feature - in order to set up delegations, you must use the Notary CLI:</p> <ol> <li><p><a href=\"https://github.com/docker/notary/releases\">Download the client</a> and ensure that it is available on your path</p></li> <li>\n<p>Create a configuration file at <code>~/.notary/config.json</code> with the following content:</p> <pre>{\n  \"trust_dir\" : \"~/.docker/trust\",\n  \"remote_server\": {\n    \"url\": \"https://notary.docker.io\"\n  }\n}\n</pre> <p>This tells Notary where the Docker Content Trust data is stored, and to use the Notary server used for images in Docker Hub.</p>\n</li> </ol> <p>For more detailed information about how to use Notary outside of the default Docker Content Trust use cases, please refer to the <a href=\"https://docs.docker.com/v1.11/notary/getting_started/\">the Notary CLI documentation</a>.</p> <p>Note that when publishing and listing delegation changes using the Notary client, your Docker Hub credentials are required.</p> <h2 id=\"generating-delegation-keys\">Generating delegation keys</h2> <p>Your collaborator needs to generate a private key (either RSA or ECDSA) and give you the public key so that you can add it to the <code>targets/releases</code> delegation.</p> <p>The easiest way to for them to generate these keys is with OpenSSL. Here is an example of how to generate a 2048-bit RSA portion key (all RSA keys must be at least 2048 bits):</p> <pre>$ opensl genrsa -out delegation.key 2048\nGenerating RSA private key, 2048 bit long modulus\n....................................................+++\n............+++\ne is 65537 (0x10001)\n\n</pre> <p>They should keep <code>delegation.key</code> private - this is what they will use to sign tags.</p> <p>Then they need to generate an x509 certificate containing the public key, which is what they will give to you. Here is the command to generate a CSR (certificate signing request):</p> <pre>$ openssl req -new -sha256 -key delegation.key -out delegation.csr\n</pre> <p>Then they can send it to whichever CA you trust to sign certificates, or they can self-sign the certificate (in this example, creating a certificate that is valid for 1 year):</p> <pre>$ openssl x509 -req -days 365 -in delegation.csr -signkey delegation.key -out delegation.crt\n</pre> <p>Then they need to give you <code>delegation.crt</code>, whether it is self-signed or signed by a CA.</p> <h2 id=\"adding-a-delegation-key-to-an-existing-repository\">Adding a delegation key to an existing repository</h2> <p>If your repository was created using a version of Docker Engine prior to 1.11, then before adding any delegations, you should rotate the snapshot key to the server so that collaborators will not require your snapshot key to sign and publish tags:</p> <pre>$ notary key rotate docker.io/&lt;username&gt;/&lt;imagename&gt; snapshot -r\n</pre> <p>This tells Notary to rotate a key for your particular image repository - note that you must include the <code>docker.io/</code> prefix. <code>snapshot -r</code> specifies that you want to rotate the snapshot key specifically, and you want the server to manage it (<code>-r</code> stands for “remote”).</p> <p>When adding a delegation, your must acquire <a href=\"#generating-delegation-keys\">the PEM-encoded x509 certificate with the public key</a> of the collaborator you wish to delegate to.</p> <p>Assuming you have the certificate <code>delegation.crt</code>, you can add a delegation for this user and then publish the delegation change:</p> <pre>$ notary delegation add docker.io/&lt;username&gt;/&lt;imagename&gt; targets/releases delegation.crt --all-paths\n$ notary publish docker.io/&lt;username&gt;/&lt;imagename&gt;\n</pre> <p>The preceding example illustrates a request to add the delegation <code>targets/releases</code> to the image repository, if it doesn’t exist. Be sure to use <code>targets/releases</code> - Notary supports multiple delegation roles, so if you mistype the delegation name, the Notary CLI will not error. However, Docker Engine supports reading only from <code>targets/releases</code>.</p> <p>It also adds the collaborator’s public key to the delegation, enabling them to sign the <code>targets/releases</code> delegation so long as they have the private key corresponding to this public key. The <code>--all-paths</code> flags tells Notary not to restrict the tag names that can be signed into <code>targets/releases</code>, which we highly recommend for <code>targets/releases</code>.</p> <p>Publishing the changes tells the server about the changes to the <code>targets/releases</code> delegation.</p> <p>After publishing, view the delegation information to ensure that you correctly added the keys to <code>targets/releases</code>:</p> <pre>$ notary delegation list docker.io/&lt;username&gt;/&lt;imagename&gt;\n\n      ROLE               PATHS                                   KEY IDS                                THRESHOLD\n---------------------------------------------------------------------------------------------------------------\n  targets/releases   \"\" &lt;all paths&gt;  729c7094a8210fd1e780e7b17b7bb55c9a28a48b871b07f65d97baf93898523a   1\n</pre> <p>You can see the <code>targets/releases</code> with its paths and the key ID you just added.</p> <p>Notary currently does not map collaborators names to keys, so we recommend that you add and list delegation keys one at a time, and keep a mapping of the key IDs to collaborators yourself should you need to remove a collaborator.</p> <h2 id=\"removing-a-delegation-key-from-an-existing-repository\">Removing a delegation key from an existing repository</h2> <p>To revoke a collaborator’s permission to sign tags for your image repository, you must know the IDs of their keys, because you need to remove their keys from the <code>targets/releases</code> delegation.</p> <pre>$ notary delegation remove docker.io/&lt;username&gt;/&lt;imagename&gt; targets/releases 729c7094a8210fd1e780e7b17b7bb55c9a28a48b871b07f65d97baf93898523a\n\nRemoval of delegation role targets/releases with keys [729c7094a8210fd1e780e7b17b7bb55c9a28a48b871b07f65d97baf93898523a], to repository \"docker.io/&lt;username&gt;/&lt;imagename&gt;\" staged for next publish.\n</pre> <p>The revocation will take effect as soon as you publish:</p> <pre>$ notary publish docker.io/&lt;username&gt;/&lt;imagename&gt;\n</pre> <p>Note that by removing all the keys from the <code>targets/releases</code> delegation, the delegation (and any tags that are signed into it) is removed. That means that these tags will all be deleted, and you may end up with older, legacy tags that were signed directly by the targets key.</p> <h2 id=\"removing-the-targets-releases-delegation-entirely-from-a-repository\">Removing the <code>targets/releases</code> delegation entirely from a repository</h2> <p>If you’ve decided that delegations aren’t for you, you can delete the <code>targets/releases</code> delegation entirely. This also removes all the tags that are currently in <code>targets/releases</code>, however, and you may end up with older, legacy tags that were signed directly by the targets key.</p> <p>To delete the <code>targets/releases</code> delegation:</p> <pre>$ notary delegation remove docker.io/&lt;username&gt;/&lt;imagename&gt; targets/releases\n\nAre you sure you want to remove all data for this delegation? (yes/no)\nyes\n\nForced removal (including all keys and paths) of delegation role targets/releases to repository \"docker.io/&lt;username&gt;/&lt;imagename&gt;\" staged for next publish.\n\n$ notary publish docker.io/&lt;username&gt;/&lt;imagename&gt;\n</pre> <h2 id=\"pushing-trusted-data-as-a-collaborator\">Pushing trusted data as a collaborator</h2> <p>As a collaborator with a private key that has been added to a repository’s <code>targets/releases</code> delegation, you need to import the private key that you generated into Content Trust.</p> <p>To do so, you can run:</p> <pre>$ notary key import delegation.key --role user\n</pre> <p>where <code>delegation.key</code> is the file containing your PEM-encoded private key.</p> <p>After you have done so, running <code>docker push</code> on any repository that includes your key in the <code>targets/releases</code> delegation will automatically sign tags using this imported key.</p> <h2 id=\"docker-push-behavior\">\n<code>docker push</code> behavior</h2> <p>When running <code>docker push</code> with Docker Content Trust, Docker Engine will attempt to sign and push with the <code>targets/releases</code> delegation if it exists. If it does not, the targets key will be used to sign the tag, if the key is available.</p> <h2 id=\"docker-pull-and-docker-build-behavior\">\n<code>docker pull</code> and <code>docker build</code> behavior</h2> <p>When running <code>docker pull</code> or <code>docker build</code> with Docker Content Trust, Docker Engine will pull tags only signed by the <code>targets/releases</code> delegation role or the legacy tags that were signed directly with the <code>targets</code> key.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../content_trust/index\">Content trust in Docker</a></li> <li><a href=\"../trust_key_mng/index\">Manage keys for content trust</a></li> <li><a href=\"../trust_automation/index\">Automation with content trust</a></li> <li><a href=\"../trust_sandbox/index\">Play in a content trust sandbox</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/trust/trust_delegation/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/trust/trust_delegation/</a>\n  </p>\n</div>\n","engine/security/trust/content_trust/index":"<h1 id=\"content-trust-in-docker\">Content trust in Docker</h1> <p>When transferring data among networked systems, <em>trust</em> is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and the publisher of all the data a system operates on. You use Docker Engine to push and pull images (data) to a public or private registry. Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.</p> <h2 id=\"understand-trust-in-docker\">Understand trust in Docker</h2> <p>Content trust allows operations with a remote Docker registry to enforce client-side signing and verification of image tags. Content trust provides the ability to use digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side verification of the integrity and publisher of specific image tags.</p> <p>Currently, content trust is disabled by default. You must enable it by setting the <code>DOCKER_CONTENT_TRUST</code> environment variable. Refer to the <a href=\"../../../reference/commandline/cli/index#environment-variables\">environment variables</a> and <a href=\"../../../reference/commandline/cli/index#notary\">Notary</a> configuration for the docker client for more options.</p> <p>Once content trust is enabled, image publishers can sign their images. Image consumers can ensure that the images they use are signed. publishers and consumers can be individuals alone or in organizations. Docker’s content trust supports users and automated processes such as builds.</p> <h3 id=\"image-tags-and-content-trust\">Image tags and content trust</h3> <p>An individual image record has the following identifier:</p> <pre>[REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]\n</pre> <p>A particular image <code>REPOSITORY</code> can have multiple tags. For example, <code>latest</code> and <code>3.1.2</code> are both tags on the <code>mongo</code> image. An image publisher can build an image and tag combination many times changing the image with each build.</p> <p>Content trust is associated with the <code>TAG</code> portion of an image. Each image repository has a set of keys that image publishers use to sign an image tag. Image publishers have discretion on which tags they sign.</p> <p>An image repository can contain an image with one tag that is signed and another tag that is not. For example, consider <a href=\"https://hub.docker.com/r/library/mongo/tags/\">the Mongo image repository</a>. The <code>latest</code> tag could be unsigned while the <code>3.1.6</code> tag could be signed. It is the responsibility of the image publisher to decide if an image tag is signed or not. In this representation, some image tags are signed, others are not:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/security/trust/images/tag_signing.png\" alt=\"Signed tags\"></p> <p>Publishers can choose to sign a specific tag or not. As a result, the content of an unsigned tag and that of a signed tag with the same name may not match. For example, a publisher can push a tagged image <code>someimage:latest</code> and sign it. Later, the same publisher can push an unsigned <code>someimage:latest</code> image. This second push replaces the last unsigned tag <code>latest</code> but does not affect the signed <code>latest</code> version. The ability to choose which tags they can sign, allows publishers to iterate over the unsigned version of an image before officially signing it.</p> <p>Image consumers can enable content trust to ensure that images they use were signed. If a consumer enables content trust, they can only pull, run, or build with trusted images. Enabling content trust is like wearing a pair of rose-colored glasses. Consumers “see” only signed images tags and the less desirable, unsigned image tags are “invisible” to them.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/security/trust/images/trust_view.png\" alt=\"Trust view\"></p> <p>To the consumer who has not enabled content trust, nothing about how they work with Docker images changes. Every image is visible regardless of whether it is signed or not.</p> <h3 id=\"content-trust-operations-and-keys\">Content trust operations and keys</h3> <p>When content trust is enabled, <code>docker</code> CLI commands that operate on tagged images must either have content signatures or explicit content hashes. The commands that operate with content trust are:</p> <ul> <li><code>push</code></li> <li><code>build</code></li> <li><code>create</code></li> <li><code>pull</code></li> <li><code>run</code></li> </ul> <p>For example, with content trust enabled a <code>docker pull someimage:latest</code> only succeeds if <code>someimage:latest</code> is signed. However, an operation with an explicit content hash always succeeds as long as the hash exists:</p> <pre>$ docker pull someimage@sha256:d149ab53f8718e987c3a3024bb8aa0e2caadf6c0328f1d9d850b2a2a67f2819a\n</pre> <p>Trust for an image tag is managed through the use of signing keys. A key set is created when an operation using content trust is first invoked. A key set consists of the following classes of keys:</p> <ul> <li>an offline key that is the root of content trust for an image tag</li> <li>repository or tagging keys that sign tags</li> <li>server-managed keys such as the timestamp key, which provides freshness security guarantees for your repository</li> </ul> <p>The following image depicts the various signing keys and their relationships:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/security/trust/images/trust_components.png\" alt=\"Content trust components\"></p> <blockquote> <p><strong>WARNING</strong>: Loss of the root key is <strong>very difficult</strong> to recover from. Correcting this loss requires intervention from <a href=\"https://support.docker.com\">Docker Support</a> to reset the repository state. This loss also requires <strong>manual intervention</strong> from every consumer that used a signed tag from this repository prior to the loss.</p> </blockquote> <p>You should backup the root key somewhere safe. Given that it is only required to create new repositories, it is a good idea to store it offline in hardware. For details on securing, and backing up your keys, make sure you read how to <a href=\"../trust_key_mng/index\">manage keys for content trust</a>.</p> <h2 id=\"survey-of-typical-content-trust-operations\">Survey of typical content trust operations</h2> <p>This section surveys the typical trusted operations users perform with Docker images.</p> <h3 id=\"enable-and-disable-content-trust-per-shell-or-per-invocation\">Enable and disable content trust per-shell or per-invocation</h3> <p>In a shell, you can enable content trust by setting the <code>DOCKER_CONTENT_TRUST</code> environment variable. Enabling per-shell is useful because you can have one shell configured for trusted operations and another terminal shell for untrusted operations. You can also add this declaration to your shell profile to have it turned on always by default.</p> <p>To enable content trust in a <code>bash</code> shell enter the following command:</p> <pre>export DOCKER_CONTENT_TRUST=1\n</pre> <p>Once set, each of the “tag” operations requires a key for a trusted tag.</p> <p>In an environment where <code>DOCKER_CONTENT_TRUST</code> is set, you can use the <code>--disable-content-trust</code> flag to run individual operations on tagged images without content trust on an as-needed basis.</p> <pre>$  docker pull --disable-content-trust docker/trusttest:untrusted\n</pre> <p>To invoke a command with content trust enabled regardless of whether or how the <code>DOCKER_CONTENT_TRUST</code> variable is set:</p> <pre>$  docker build --disable-content-trust=false -t docker/trusttest:testing .\n</pre> <p>All of the trusted operations support the <code>--disable-content-trust</code> flag.</p> <h3 id=\"push-trusted-content\">Push trusted content</h3> <p>To create signed content for a specific image tag, simply enable content trust and push a tagged image. If this is the first time you have pushed an image using content trust on your system, the session looks like this:</p> <pre>$ docker push docker/trusttest:latest\nThe push refers to a repository [docker.io/docker/trusttest] (len: 1)\n9a61b6b1315e: Image already exists\n902b87aaaec9: Image already exists\nlatest: digest: sha256:d02adacee0ac7a5be140adb94fa1dae64f4e71a68696e7f8e7cbf9db8dd49418 size: 3220\nSigning and pushing trust metadata\nYou are about to create a new root signing key passphrase. This passphrase\nwill be used to protect the most sensitive key in your signing system. Please\nchoose a long, complex passphrase and be careful to keep the password and the\nkey file itself secure and backed up. It is highly recommended that you use a\npassword manager to generate the passphrase and keep it safe. There will be no\nway to recover this key. You can find the key in your config directory.\nEnter passphrase for new root key with id a1d96fb:\nRepeat passphrase for new root key with id a1d96fb:\nEnter passphrase for new repository key with id docker.io/docker/trusttest (3a932f1):\nRepeat passphrase for new repository key with id docker.io/docker/trusttest (3a932f1):\nFinished initializing \"docker.io/docker/trusttest\"\n</pre> <p>When you push your first tagged image with content trust enabled, the <code>docker</code> client recognizes this is your first push and:</p> <ul> <li>alerts you that it will create a new root key</li> <li>requests a passphrase for the root key</li> <li>generates a root key in the <code>~/.docker/trust</code> directory</li> <li>requests a passphrase for the repository key</li> <li>generates a repository key for in the <code>~/.docker/trust</code> directory</li> </ul> <p>The passphrase you chose for both the root key and your repository key-pair should be randomly generated and stored in a <em>password manager</em>.</p> <blockquote> <p><strong>NOTE</strong>: If you omit the <code>latest</code> tag, content trust is skipped. This is true even if content trust is enabled and even if this is your first push.</p> </blockquote> <pre>$ docker push docker/trusttest\nThe push refers to a repository [docker.io/docker/trusttest] (len: 1)\n9a61b6b1315e: Image successfully pushed\n902b87aaaec9: Image successfully pushed\nlatest: digest: sha256:a9a9c4402604b703bed1c847f6d85faac97686e48c579bd9c3b0fa6694a398fc size: 3220\nNo tag specified, skipping trust metadata push\n</pre> <p>It is skipped because as the message states, you did not supply an image <code>TAG</code> value. In Docker content trust, signatures are associated with tags.</p> <p>Once you have a root key on your system, subsequent images repositories you create can use that same root key:</p> <pre>$ docker push docker.io/docker/seaside:latest\nThe push refers to a repository [docker.io/docker/seaside] (len: 1)\na9539b34a6ab: Image successfully pushed\nb3dbab3810fc: Image successfully pushed\nlatest: digest: sha256:d2ba1e603661a59940bfad7072eba698b79a8b20ccbb4e3bfb6f9e367ea43939 size: 3346\nSigning and pushing trust metadata\nEnter key passphrase for root key with id a1d96fb:\nEnter passphrase for new repository key with id docker.io/docker/seaside (bb045e3):\nRepeat passphrase for new repository key with id docker.io/docker/seaside (bb045e3):\nFinished initializing \"docker.io/docker/seaside\"\n</pre> <p>The new image has its own repository key and timestamp key. The <code>latest</code> tag is signed with both of these.</p> <h3 id=\"pull-image-content\">Pull image content</h3> <p>A common way to consume an image is to <code>pull</code> it. With content trust enabled, the Docker client only allows <code>docker pull</code> to retrieve signed images.</p> <pre>$  docker pull docker/seaside\nUsing default tag: latest\nPull (1 of 1): docker/trusttest:latest@sha256:d149ab53f871\n...\nTagging docker/trusttest@sha256:d149ab53f871 as docker/trusttest:latest\n</pre> <p>The <code>seaside:latest</code> image is signed. In the following example, the command does not specify a tag, so the system uses the <code>latest</code> tag by default again and the <code>docker/cliffs:latest</code> tag is not signed.</p> <pre>$ docker pull docker/cliffs\nUsing default tag: latest\nno trust data available\n</pre> <p>Because the tag <code>docker/cliffs:latest</code> is not trusted, the <code>pull</code> fails.</p> <h3 id=\"disable-content-trust-for-specific-operations\">Disable content trust for specific operations</h3> <p>A user who wants to disable content trust for a particular operation can use the <code>--disable-content-trust</code> flag. <strong>Warning: this flag disables content trust for this operation</strong>. With this flag, Docker will ignore content-trust and allow all operations to be done without verifying any signatures. If we wanted the previous untrusted build to succeed we could do:</p> <pre>$  cat Dockerfile\nFROM docker/trusttest:notrust\nRUN echo\n$  docker build --disable-content-trust -t docker/trusttest:testing .\nSending build context to Docker daemon 42.84 MB\n...\nSuccessfully built f21b872447dc\n</pre> <p>The same is true for all the other commands, such as <code>pull</code> and <code>push</code>:</p> <pre>$  docker pull --disable-content-trust docker/trusttest:untrusted\n...\n$  docker push --disable-content-trust docker/trusttest:untrusted\n...\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../trust_key_mng/index\">Manage keys for content trust</a></li> <li><a href=\"../trust_automation/index\">Automation with content trust</a></li> <li><a href=\"../trust_delegation/index\">Delegations for content trust</a></li> <li><a href=\"../trust_sandbox/index\">Play in a content trust sandbox</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/trust/content_trust/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/trust/content_trust/</a>\n  </p>\n</div>\n","engine/security/trust/deploying_notary/index":"<h1 id=\"deploying-notary-server-with-compose\">Deploying Notary Server with Compose</h1> <p>The easiest way to deploy Notary Server is by using Docker Compose. To follow the procedure on this page, you must have already <a href=\"../../../../compose/install/index\">installed Docker Compose</a>.</p> <ol> <li>\n<p>Clone the Notary repository</p> <pre>git clone git@github.com:docker/notary.git\n</pre>\n</li> <li>\n<p>Build and start Notary Server with the sample certificates.</p> <pre>docker-compose up -d\n</pre> <p>For more detailed documentation about how to deploy Notary Server see the <a href=\"https://docs.docker.com/v1.11/notary/running_a_service/\">instructions to run a Notary service</a> as well as <a href=\"https://github.com/docker/notary\">https://github.com/docker/notary</a> for more information.</p>\n</li> <li><p>Make sure that your Docker or Notary client trusts Notary Server’s certificate before you try to interact with the Notary server.</p></li> </ol> <p>See the instructions for <a href=\"../../../reference/commandline/cli/index#notary\">Docker</a> or for <a href=\"https://github.com/docker/notary#using-notary\">Notary</a> depending on which one you are using.</p> <h2 id=\"if-you-want-to-use-notary-in-production\">If you want to use Notary in production</h2> <p>Please check back here for instructions after Notary Server has an official stable release. To get a head start on deploying Notary in production see <a href=\"https://github.com/docker/notary\">https://github.com/docker/notary</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/trust/deploying_notary/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/trust/deploying_notary/</a>\n  </p>\n</div>\n","engine/security/trust/trust_key_mng/index":"<h1 id=\"manage-keys-for-content-trust\">Manage keys for content trust</h1> <p>Trust for an image tag is managed through the use of keys. Docker’s content trust makes use of five different types of keys:</p> <table> <thead> <tr> <th>Key</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>root key</td> <td>Root of content trust for an image tag. When content trust is enabled, you create the root key once. Also known as the offline key, because it should be kept offline.</td> </tr> <tr> <td>targets</td> <td>This key allows you to sign image tags, to manage delegations including delegated keys or permitted delegation paths. Also known as the repository key, since this key determines what tags can be signed into an image repository.</td> </tr> <tr> <td>snapshot</td> <td>This key signs the current collection of image tags, preventing mix and match attacks.</td> </tr> <tr> <td>timestamp</td> <td>This key allows Docker image repositories to have freshness security guarantees without requiring periodic content refreshes on the client’s side.</td> </tr> <tr> <td>delegation</td> <td>Delegation keys are optional tagging keys and allow you to delegate signing image tags to other publishers without having to share your targets key.</td> </tr> </tbody> </table> <p>When doing a <code>docker push</code> with Content Trust enabled for the first time, the root, targets, snapshot, and timestamp keys are generated automatically for the image repository:</p> <ul> <li><p>The root and targets key are generated and stored locally client-side.</p></li> <li><p>The timestamp and snapshot keys are safely generated and stored in a signing server that is deployed alongside the Docker registry. These keys are generated in a backend service that isn’t directly exposed to the internet and are encrypted at rest.</p></li> </ul> <p>Delegation keys are optional, and not generated as part of the normal <code>docker</code> workflow. They need to be <a href=\"../trust_delegation/index#generating-delegation-keys\">manually generated and added to the repository</a>.</p> <p>Note: Prior to Docker Engine 1.11, the snapshot key was also generated and stored locally client-side. <a href=\"https://docs.docker.com/v1.11/notary/advanced_usage/#rotate-keys\">Use the Notary CLI to manage your snapshot key locally again</a> for repositories created with newer versions of Docker.</p> <h2 id=\"choosing-a-passphrase\">Choosing a passphrase</h2> <p>The passphrases you chose for both the root key and your repository key should be randomly generated and stored in a password manager. Having the repository key allow users to sign image tags on a repository. Passphrases are used to encrypt your keys at rest and ensures that a lost laptop or an unintended backup doesn’t put the private key material at risk.</p> <h2 id=\"back-up-your-keys\">Back up your keys</h2> <p>All the Docker trust keys are stored encrypted using the passphrase you provide on creation. Even so, you should still take care of the location where you back them up. Good practice is to create two encrypted USB keys.</p> <p>It is very important that you backup your keys to a safe, secure location. Loss of the repository key is recoverable; loss of the root key is not.</p> <p>The Docker client stores the keys in the <code>~/.docker/trust/private</code> directory. Before backing them up, you should <code>tar</code> them into an archive:</p> <pre>$ umask 077; tar -zcvf private_keys_backup.tar.gz ~/.docker/trust/private; umask 022\n</pre> <h2 id=\"hardware-storage-and-signing\">Hardware storage and signing</h2> <p>Docker Content Trust can store and sign with root keys from a Yubikey 4. The Yubikey is prioritized over keys stored in the filesystem. When you initialize a new repository with content trust, Docker Engine looks for a root key locally. If a key is not found and the Yubikey 4 exists, Docker Engine creates a root key in the Yubikey 4. Please consult the <a href=\"https://docs.docker.com/v1.11/notary/advanced_usage/#use-a-yubikey\">Notary documentation</a> for more details.</p> <p>Prior to Docker Engine 1.11, this feature was only in the experimental branch.</p> <h2 id=\"lost-keys\">Lost keys</h2> <p>If a publisher loses keys it means losing the ability to sign trusted content for your repositories. If you lose a key, contact <a href=\"https://support.docker.com\">Docker Support</a> (support@docker.com) to reset the repository state.</p> <p>This loss also requires <strong>manual intervention</strong> from every consumer that pulled the tagged image prior to the loss. Image consumers would get an error for content that they already downloaded:</p> <pre>could not validate the path to a trusted root: failed to validate data with current trusted certificates\n</pre> <p>To correct this, they need to download a new image tag with that is signed with the new key.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../content_trust/index\">Content trust in Docker</a></li> <li><a href=\"../trust_automation/index\">Automation with content trust</a></li> <li><a href=\"../trust_delegation/index\">Delegations for content trust</a></li> <li><a href=\"../trust_sandbox/index\">Play in a content trust sandbox</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/trust/trust_key_mng/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/trust/trust_key_mng/</a>\n  </p>\n</div>\n","engine/security/trust/trust_sandbox/index":"<h1 id=\"play-in-a-content-trust-sandbox\">Play in a content trust sandbox</h1> <p>This page explains how to set up and use a sandbox for experimenting with trust. The sandbox allows you to configure and try trust operations locally without impacting your production images.</p> <p>Before working through this sandbox, you should have read through the <a href=\"../content_trust/index\">trust overview</a>.</p> <h3 id=\"prerequisites\">Prerequisites</h3> <p>These instructions assume you are running in Linux or Mac OS X. You can run this sandbox on a local machine or on a virtual machine. You will need to have <code>sudo</code> privileges on your local machine or in the VM.</p> <p>This sandbox requires you to install two Docker tools: Docker Engine and Docker Compose. To install the Docker Engine, choose from the <a href=\"../../../installation/index\">list of supported platforms</a>. To install Docker Compose, see the <a href=\"https://docs.docker.com/compose/install/\">detailed instructions here</a>.</p> <p>Finally, you’ll need to have <code>git</code> installed on your local system or VM.</p> <h2 id=\"what-is-in-the-sandbox\">What is in the sandbox?</h2> <p>If you are just using trust out-of-the-box you only need your Docker Engine client and access to the Docker hub. The sandbox mimics a production trust environment, and requires these additional components:</p> <table> <thead> <tr> <th>Container</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>notarysandbox</td> <td>A container with the latest version of Docker Engine and with some preconfigured certifications. This is your sandbox where you can use the <code>docker</code> client to test trust operations.</td> </tr> <tr> <td>Registry server</td> <td>A local registry service.</td> </tr> <tr> <td>Notary server</td> <td>The service that does all the heavy-lifting of managing trust</td> </tr> <tr> <td>Notary signer</td> <td>A service that ensures that your keys are secure.</td> </tr> <tr> <td>MySQL</td> <td>The database where all of the trust information will be stored</td> </tr> </tbody> </table> <p>The sandbox uses the Docker daemon on your local system. Within the <code>notarysandbox</code> you interact with a local registry rather than the Docker Hub. This means your everyday image repositories are not used. They are protected while you play.</p> <p>When you play in the sandbox, you’ll also create root and repository keys. The sandbox is configured to store all the keys and files inside the <code>notarysandbox</code> container. Since the keys you create in the sandbox are for play only, destroying the container destroys them as well.</p> <h2 id=\"build-the-sandbox\">Build the sandbox</h2> <p>In this section, you build the Docker components for your trust sandbox. If you work exclusively with the Docker Hub, you would not need with these components. They are built into the Docker Hub for you. For the sandbox, however, you must build your own entire, mock production environment and registry.</p> <h3 id=\"configure-etc-hosts\">Configure /etc/hosts</h3> <p>The sandbox’ <code>notaryserver</code> and <code>sandboxregistry</code> run on your local server. The client inside the <code>notarysandbox</code> container connects to them over your network. So, you’ll need an entry for both the servers in your local <code>/etc/hosts</code> file.</p> <ol> <li>\n<p>Add an entry for the <code>notaryserver</code> to <code>/etc/hosts</code>.</p> <pre>$ sudo sh -c 'echo \"127.0.0.1 notaryserver\" &gt;&gt; /etc/hosts'\n</pre>\n</li> <li>\n<p>Add an entry for the <code>sandboxregistry</code> to <code>/etc/hosts</code>.</p> <pre>$ sudo sh -c 'echo \"127.0.0.1 sandboxregistry\" &gt;&gt; /etc/hosts'\n</pre>\n</li> </ol> <h3 id=\"build-the-notarytest-image\">Build the notarytest image</h3> <ol> <li>\n<p>Create a <code>notarytest</code> directory on your system.</p> <pre>$ mkdir notarysandbox\n</pre>\n</li> <li>\n<p>Change into your <code>notarysandbox</code> directory.</p> <pre>$ cd notarysandbox\n</pre>\n</li> <li>\n<p>Create a <code>notarytest</code> directory then change into that.</p> <pre>$ mkdir notarytest\n$ cd notarytest\n</pre>\n</li> <li><p>Create a filed called <code>Dockerfile</code> with your favorite editor.</p></li> <li>\n<p>Add the following to the new file.</p> <pre>FROM debian:jessie\n\nADD https://master.dockerproject.org/linux/amd64/docker /usr/bin/docker\nRUN chmod +x /usr/bin/docker \\\n  &amp;&amp; apt-get update \\\n  &amp;&amp; apt-get install -y \\\n  tree \\\n  vim \\\n  git \\\n  ca-certificates \\\n  --no-install-recommends\n\nWORKDIR /root\nRUN git clone -b trust-sandbox https://github.com/docker/notary.git\nRUN cp /root/notary/fixtures/root-ca.crt /usr/local/share/ca-certificates/root-ca.crt\nRUN update-ca-certificates\n\nENTRYPOINT [\"bash\"]\n</pre>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>Build the testing container.</p> <pre>$ docker build -t notarysandbox .\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM debian:jessie\n ...\n Successfully built 5683f17e9d72\n</pre>\n</li> </ol> <h3 id=\"build-and-start-up-the-trust-servers\">Build and start up the trust servers</h3> <p>In this step, you get the source code for your notary and registry services. Then, you’ll use Docker Compose to build and start them on your local system.</p> <ol> <li>\n<p>Change to back to the root of your <code>notarysandbox</code> directory.</p> <pre>$ cd notarysandbox\n</pre>\n</li> <li>\n<p>Clone the <code>notary</code> project.</p> <pre>  $ git clone -b trust-sandbox https://github.com/docker/notary.git\n</pre>\n</li> <li>\n<p>Clone the <code>distribution</code> project.</p> <pre>$ git clone https://github.com/docker/distribution.git\n</pre>\n</li> <li>\n<p>Change to the <code>notary</code> project directory.</p> <pre>$ cd notary\n</pre>\n</li> </ol> <p>The directory contains a <code>docker-compose</code> file that you’ll use to run a notary server together with a notary signer and the corresponding MySQL databases. The databases store the trust information for an image.</p> <ol> <li>\n<p>Build the server images.</p> <pre>$  docker-compose build\n</pre> <p>The first time you run this, the build takes some time.</p>\n</li> <li>\n<p>Run the server containers on your local system.</p> <pre>$ docker-compose up -d\n</pre> <p>Once the trust services are up, you’ll setup a local version of the Docker Registry v2.</p>\n</li> <li><p>Change to the <code>notarysandbox/distribution</code> directory.</p></li> <li>\n<p>Build the <code>sandboxregistry</code> server.</p> <pre>$ docker build -t sandboxregistry .\n</pre>\n</li> <li>\n<p>Start the <code>sandboxregistry</code> server running.</p> <pre>$ docker run -p 5000:5000 --name sandboxregistry sandboxregistry &amp;\n</pre>\n</li> </ol> <h2 id=\"playing-in-the-sandbox\">Playing in the sandbox</h2> <p>Now that everything is setup, you can go into your <code>notarysandbox</code> container and start testing Docker content trust.</p> <h3 id=\"start-the-notarysandbox-container\">Start the notarysandbox container</h3> <p>In this procedure, you start the <code>notarysandbox</code> and link it to the running <code>notary_notaryserver_1</code> and <code>sandboxregistry</code> containers. The links allow communication among the containers.</p> <pre>$ docker run -it -v /var/run/docker.sock:/var/run/docker.sock --link notary_notaryserver_1:notaryserver --link sandboxregistry:sandboxregistry notarysandbox\nroot@0710762bb59a:/#\n</pre> <p>Mounting the <code>docker.sock</code> gives the <code>notarysandbox</code> access to the <code>docker</code> daemon on your host, while storing all the keys and files inside the sandbox container. When you destroy the container, you destroy the “play” keys.</p> <h3 id=\"test-some-trust-operations\">Test some trust operations</h3> <p>Now, you’ll pull some images.</p> <ol> <li>\n<p>Download a <code>docker</code> image to test with.</p> <pre># docker pull docker/trusttest\ndocker pull docker/trusttest\nUsing default tag: latest\nlatest: Pulling from docker/trusttest\n\nb3dbab3810fc: Pull complete\na9539b34a6ab: Pull complete\nDigest: sha256:d149ab53f8718e987c3a3024bb8aa0e2caadf6c0328f1d9d850b2a2a67f2819a\nStatus: Downloaded newer image for docker/trusttest:latest\n</pre>\n</li> <li>\n<p>Tag it to be pushed to our sandbox registry:</p> <pre># docker tag docker/trusttest sandboxregistry:5000/test/trusttest:latest\n</pre>\n</li> <li>\n<p>Enable content trust.</p> <pre># export DOCKER_CONTENT_TRUST=1\n</pre>\n</li> <li>\n<p>Identify the trust server.</p> <pre># export DOCKER_CONTENT_TRUST_SERVER=https://notaryserver:4443\n</pre> <p>This step is only necessary because the sandbox is using its own server. Normally, if you are using the Docker Public Hub this step isn’t necessary.</p>\n</li> <li>\n<p>Pull the test image.</p> <pre># docker pull sandboxregistry:5000/test/trusttest\nUsing default tag: latest\nno trust data available\n</pre> <p>You see an error, because this content doesn’t exist on the <code>sandboxregistry</code> yet.</p>\n</li> <li>\n<p>Push the trusted image.</p> <pre># docker push sandboxregistry:5000/test/trusttest:latest\nThe push refers to a repository [sandboxregistry:5000/test/trusttest] (len: 1)\na9539b34a6ab: Image successfully pushed\nb3dbab3810fc: Image successfully pushed\nlatest: digest: sha256:1d871dcb16805f0604f10d31260e79c22070b35abc71a3d1e7ee54f1042c8c7c size: 3348\nSigning and pushing trust metadata\nYou are about to create a new root signing key passphrase. This passphrase\nwill be used to protect the most sensitive key in your signing system. Please\nchoose a long, complex passphrase and be careful to keep the password and the\nkey file itself secure and backed up. It is highly recommended that you use a\npassword manager to generate the passphrase and keep it safe. There will be no\nway to recover this key. You can find the key in your config directory.\nEnter passphrase for new root key with id 8c69e04:\nRepeat passphrase for new root key with id 8c69e04:\nEnter passphrase for new repository key with id sandboxregistry:5000/test/trusttest (93c362a):\nRepeat passphrase for new repository key with id sandboxregistry:5000/test/trusttest (93c362a):\nFinished initializing \"sandboxregistry:5000/test/trusttest\"\nlatest: digest: sha256:d149ab53f8718e987c3a3024bb8aa0e2caadf6c0328f1d9d850b2a2a67f2819a size: 3355\nSigning and pushing trust metadata\n</pre>\n</li> <li>\n<p>Try pulling the image you just pushed:</p> <pre># docker pull sandboxregistry:5000/test/trusttest\nUsing default tag: latest\nPull (1 of 1): sandboxregistry:5000/test/trusttest:latest@sha256:1d871dcb16805f0604f10d31260e79c22070b35abc71a3d1e7ee54f1042c8c7c\nsha256:1d871dcb16805f0604f10d31260e79c22070b35abc71a3d1e7ee54f1042c8c7c: Pulling from test/trusttest\nb3dbab3810fc: Already exists\na9539b34a6ab: Already exists\nDigest: sha256:1d871dcb16805f0604f10d31260e79c22070b35abc71a3d1e7ee54f1042c8c7c\nStatus: Downloaded newer image for sandboxregistry:5000/test/trusttest@sha256:1d871dcb16805f0604f10d31260e79c22070b35abc71a3d1e7ee54f1042c8c7c\nTagging sandboxregistry:5000/test/trusttest@sha256:1d871dcb16805f0604f10d31260e79c22070b35abc71a3d1e7ee54f1042c8c7c as sandboxregistry:5000/test/trusttest:latest\n</pre>\n</li> </ol> <h3 id=\"test-with-malicious-images\">Test with malicious images</h3> <p>What happens when data is corrupted and you try to pull it when trust is enabled? In this section, you go into the <code>sandboxregistry</code> and tamper with some data. Then, you try and pull it.</p> <ol> <li><p>Leave the sandbox container running.</p></li> <li>\n<p>Open a new bash terminal from your host into the <code>sandboxregistry</code>.</p> <pre>$ docker exec -it sandboxregistry bash\n296db6068327#\n</pre>\n</li> <li>\n<p>Change into the registry storage.</p> <p>You’ll need to provide the <code>sha</code> you received when you pushed the image.</p> <pre># cd /var/lib/registry/docker/registry/v2/blobs/sha256/aa/aac0c133338db2b18ff054943cee3267fe50c75cdee969aed88b1992539ed042\n</pre>\n</li> <li>\n<p>Add malicious data to one of the trusttest layers:</p> <pre># echo \"Malicious data\" &gt; data\n</pre>\n</li> <li><p>Got back to your sandbox terminal.</p></li> <li>\n<p>List the trusttest image.</p> <pre># docker images | grep trusttest\ndocker/trusttest                 latest              a9539b34a6ab        7 weeks ago         5.025 MB\nsandboxregistry:5000/test/trusttest   latest              a9539b34a6ab        7 weeks ago         5.025 MB\nsandboxregistry:5000/test/trusttest   &lt;none&gt;              a9539b34a6ab        7 weeks ago         5.025 MB\n</pre>\n</li> <li>\n<p>Remove the <code>trusttest:latest</code> image.</p> <pre># docker rmi -f a9539b34a6ab\nUntagged: docker/trusttest:latest\nUntagged: sandboxregistry:5000/test/trusttest:latest\nUntagged: sandboxregistry:5000/test/trusttest@sha256:1d871dcb16805f0604f10d31260e79c22070b35abc71a3d1e7ee54f1042c8c7c\nDeleted: a9539b34a6aba01d3942605dfe09ab821cd66abf3cf07755b0681f25ad81f675\nDeleted: b3dbab3810fc299c21f0894d39a7952b363f14520c2f3d13443c669b63b6aa20\n</pre>\n</li> <li>\n<p>Pull the image again.</p> <pre># docker pull sandboxregistry:5000/test/trusttest\nUsing default tag: latest\n...\nb3dbab3810fc: Verifying Checksum\na9539b34a6ab: Pulling fs layer\nfilesystem layer verification failed for digest sha256:aac0c133338db2b18ff054943cee3267fe50c75cdee969aed88b1992539ed042\n</pre> <p>You’ll see the pull did not complete because the trust system was unable to verify the image.</p>\n</li> </ol> <h2 id=\"more-play-in-the-sandbox\">More play in the sandbox</h2> <p>Now, that you have a full Docker content trust sandbox on your local system, feel free to play with it and see how it behaves. If you find any security issues with Docker, feel free to send us an email at <a href=\"mailto:security@docker.com\">security@docker.com</a>.</p> <div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/trust/trust_sandbox/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/trust/trust_sandbox/</a>\n  </p>\n</div>\n","engine/security/apparmor/index":"<h1 id=\"apparmor-security-profiles-for-docker\">AppArmor security profiles for Docker</h1> <p>AppArmor (Application Armor) is a Linux security module that protects an operating system and its applications from security threats. To use it, a system administrator associates an AppArmor security profile with each program. Docker expects to find an AppArmor policy loaded and enforced.</p> <p>Docker automatically loads container profiles. The Docker binary installs a <code>docker-default</code> profile in the <code>/etc/apparmor.d/docker</code> file. This profile is used on containers, <em>not</em> on the Docker Daemon.</p> <p>A profile for the Docker Engine daemon exists but it is not currently installed with the <code>deb</code> packages. If you are interested in the source for the daemon profile, it is located in <a href=\"https://github.com/docker/docker/tree/master/contrib/apparmor\">contrib/apparmor</a> in the Docker Engine source repository.</p> <h2 id=\"understand-the-policies\">Understand the policies</h2> <p>The <code>docker-default</code> profile is the default for running containers. It is moderately protective while providing wide application compatibility. The profile is the following:</p> <pre>#include &lt;tunables/global&gt;\n\n\nprofile docker-default flags=(attach_disconnected,mediate_deleted) {\n\n  #include &lt;abstractions/base&gt;\n\n\n  network,\n  capability,\n  file,\n  umount,\n\n  deny @{PROC}/{*,**^[0-9*],sys/kernel/shm*} wkx,\n  deny @{PROC}/sysrq-trigger rwklx,\n  deny @{PROC}/mem rwklx,\n  deny @{PROC}/kmem rwklx,\n  deny @{PROC}/kcore rwklx,\n\n  deny mount,\n\n  deny /sys/[^f]*/** wklx,\n  deny /sys/f[^s]*/** wklx,\n  deny /sys/fs/[^c]*/** wklx,\n  deny /sys/fs/c[^g]*/** wklx,\n  deny /sys/fs/cg[^r]*/** wklx,\n  deny /sys/firmware/efi/efivars/** rwklx,\n  deny /sys/kernel/security/** rwklx,\n}\n</pre> <p>When you run a container, it uses the <code>docker-default</code> policy unless you override it with the <code>security-opt</code> option. For example, the following explicitly specifies the default policy:</p> <pre>$ docker run --rm -it --security-opt apparmor=docker-default hello-world\n</pre> <h2 id=\"load-and-unload-profiles\">Load and unload profiles</h2> <p>To load a new profile into AppArmor for use with containers:</p> <pre>$ apparmor_parser -r -W /path/to/your_profile\n</pre> <p>Then, run the custom profile with <code>--security-opt</code> like so:</p> <pre>$ docker run --rm -it --security-opt apparmor=your_profile hello-world\n</pre> <p>To unload a profile from AppArmor:</p> <pre># stop apparmor\n$ /etc/init.d/apparmor stop\n# unload the profile\n$ apparmor_parser -R /path/to/profile\n# start apparmor\n$ /etc/init.d/apparmor start\n</pre> <h3 id=\"resources-for-writing-profiles\">Resources for writing profiles</h3> <p>The syntax for file globbing in AppArmor is a bit different than some other globbing implementations. It is highly suggested you take a look at some of the below resources with regard to AppArmor profile syntax.</p> <ul> <li><a href=\"http://wiki.apparmor.net/index.php/QuickProfileLanguage\">Quick Profile Language</a></li> <li><a href=\"http://wiki.apparmor.net/index.php/AppArmor_Core_Policy_Reference#AppArmor_globbing_syntax\">Globbing Syntax</a></li> </ul> <h2 id=\"nginx-example-profile\">Nginx example profile</h2> <p>In this example, you create a custom AppArmor profile for Nginx. Below is the custom profile.</p> <pre>#include &lt;tunables/global&gt;\n\n\nprofile docker-nginx flags=(attach_disconnected,mediate_deleted) {\n  #include &lt;abstractions/base&gt;\n\n  network inet tcp,\n  network inet udp,\n  network inet icmp,\n\n  deny network raw,\n\n  deny network packet,\n\n  file,\n  umount,\n\n  deny /bin/** wl,\n  deny /boot/** wl,\n  deny /dev/** wl,\n  deny /etc/** wl,\n  deny /home/** wl,\n  deny /lib/** wl,\n  deny /lib64/** wl,\n  deny /media/** wl,\n  deny /mnt/** wl,\n  deny /opt/** wl,\n  deny /proc/** wl,\n  deny /root/** wl,\n  deny /sbin/** wl,\n  deny /srv/** wl,\n  deny /tmp/** wl,\n  deny /sys/** wl,\n  deny /usr/** wl,\n\n  audit /** w,\n\n  /var/run/nginx.pid w,\n\n  /usr/sbin/nginx ix,\n\n  deny /bin/dash mrwklx,\n  deny /bin/sh mrwklx,\n  deny /usr/bin/top mrwklx,\n\n\n  capability chown,\n  capability dac_override,\n  capability setuid,\n  capability setgid,\n  capability net_bind_service,\n\n  deny @{PROC}/{*,**^[0-9*],sys/kernel/shm*} wkx,\n  deny @{PROC}/sysrq-trigger rwklx,\n  deny @{PROC}/mem rwklx,\n  deny @{PROC}/kmem rwklx,\n  deny @{PROC}/kcore rwklx,\n  deny mount,\n  deny /sys/[^f]*/** wklx,\n  deny /sys/f[^s]*/** wklx,\n  deny /sys/fs/[^c]*/** wklx,\n  deny /sys/fs/c[^g]*/** wklx,\n  deny /sys/fs/cg[^r]*/** wklx,\n  deny /sys/firmware/efi/efivars/** rwklx,\n  deny /sys/kernel/security/** rwklx,\n}\n</pre> <ol> <li>\n<p>Save the custom profile to disk in the <code>/etc/apparmor.d/containers/docker-nginx</code> file.</p> <p>The file path in this example is not a requirement. In production, you could use another.</p>\n</li> <li>\n<p>Load the profile.</p> <pre>$ sudo apparmor_parser -r -W /etc/apparmor.d/containers/docker-nginx\n</pre>\n</li> <li>\n<p>Run a container with the profile.</p> <p>To run nginx in detached mode:</p> <pre>$ docker run --security-opt \"apparmor=docker-nginx\" \\\n    -p 80:80 -d --name apparmor-nginx nginx\n</pre>\n</li> <li>\n<p>Exec into the running container</p> <pre>$ docker exec -it apparmor-nginx bash\n</pre>\n</li> <li>\n<p>Try some operations to test the profile.</p> <pre>root@6da5a2a930b9:~# ping 8.8.8.8\nping: Lacking privilege for raw socket.\n\nroot@6da5a2a930b9:/# top\nbash: /usr/bin/top: Permission denied\n\nroot@6da5a2a930b9:~# touch ~/thing\ntouch: cannot touch 'thing': Permission denied\n\nroot@6da5a2a930b9:/# sh\nbash: /bin/sh: Permission denied\n\nroot@6da5a2a930b9:/# dash\nbash: /bin/dash: Permission denied\n</pre>\n</li> </ol> <p>Congrats! You just deployed a container secured with a custom apparmor profile!</p> <h2 id=\"debug-apparmor\">Debug AppArmor</h2> <p>You can use <code>dmesg</code> to debug problems and <code>aa-status</code> check the loaded profiles.</p> <h3 id=\"use-dmesg\">Use dmesg</h3> <p>Here are some helpful tips for debugging any problems you might be facing with regard to AppArmor.</p> <p>AppArmor sends quite verbose messaging to <code>dmesg</code>. Usually an AppArmor line looks like the following:</p> <pre>[ 5442.864673] audit: type=1400 audit(1453830992.845:37): apparmor=\"ALLOWED\" operation=\"open\" profile=\"/usr/bin/docker\" name=\"/home/jessie/docker/man/man1/docker-attach.1\" pid=10923 comm=\"docker\" requested_mask=\"r\" denied_mask=\"r\" fsuid=1000 ouid=0\n</pre> <p>In the above example, you can see <code>profile=/usr/bin/docker</code>. This means the user has the <code>docker-engine</code> (Docker Engine Daemon) profile loaded.</p> <blockquote> <p><strong>Note:</strong> On version of Ubuntu &gt; 14.04 this is all fine and well, but Trusty users might run into some issues when trying to <code>docker exec</code>.</p> </blockquote> <p>Look at another log line:</p> <pre>[ 3256.689120] type=1400 audit(1405454041.341:73): apparmor=\"DENIED\" operation=\"ptrace\" profile=\"docker-default\" pid=17651 comm=\"docker\" requested_mask=\"receive\" denied_mask=\"receive\"\n</pre> <p>This time the profile is <code>docker-default</code>, which is run on containers by default unless in <code>privileged</code> mode. This line shows that apparmor has denied <code>ptrace</code> in the container. This is exactly as expected.</p> <h3 id=\"use-aa-status\">Use aa-status</h3> <p>If you need to check which profiles are loaded, you can use <code>aa-status</code>. The output looks like:</p> <pre>$ sudo aa-status\napparmor module is loaded.\n14 profiles are loaded.\n1 profiles are in enforce mode.\n   docker-default\n13 profiles are in complain mode.\n   /usr/bin/docker\n   /usr/bin/docker///bin/cat\n   /usr/bin/docker///bin/ps\n   /usr/bin/docker///sbin/apparmor_parser\n   /usr/bin/docker///sbin/auplink\n   /usr/bin/docker///sbin/blkid\n   /usr/bin/docker///sbin/iptables\n   /usr/bin/docker///sbin/mke2fs\n   /usr/bin/docker///sbin/modprobe\n   /usr/bin/docker///sbin/tune2fs\n   /usr/bin/docker///sbin/xtables-multi\n   /usr/bin/docker///sbin/zfs\n   /usr/bin/docker///usr/bin/xz\n38 processes have profiles defined.\n37 processes are in enforce mode.\n   docker-default (6044)\n   ...\n   docker-default (31899)\n1 processes are in complain mode.\n   /usr/bin/docker (29756)\n0 processes are unconfined but have a profile defined.\n</pre> <p>The above output shows that the <code>docker-default</code> profile running on various container PIDs is in <code>enforce</code> mode. This means AppArmor is actively blocking and auditing in <code>dmesg</code> anything outside the bounds of the <code>docker-default</code> profile.</p> <p>The output above also shows the <code>/usr/bin/docker</code> (Docker Engine daemon) profile is running in <code>complain</code> mode. This means AppArmor <em>only</em> logs to <code>dmesg</code> activity outside the bounds of the profile. (Except in the case of Ubuntu Trusty, where some interesting behaviors are enforced.)</p> <h2 id=\"contribute-docker-s-apparmor-code\">Contribute Docker’s AppArmor code</h2> <p>Advanced users and package managers can find a profile for <code>/usr/bin/docker</code> (Docker Engine Daemon) underneath <a href=\"https://github.com/docker/docker/tree/master/contrib/apparmor\">contrib/apparmor</a> in the Docker Engine source repository.</p> <p>The <code>docker-default</code> profile for containers lives in <a href=\"https://github.com/docker/docker/tree/master/profiles/apparmor\">profiles/apparmor</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/apparmor/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/apparmor/</a>\n  </p>\n</div>\n","engine/extend/plugins_authorization/index":"<h1 id=\"create-an-authorization-plugin\">Create an authorization plugin</h1> <p>Docker’s out-of-the-box authorization model is all or nothing. Any user with permission to access the Docker daemon can run any Docker client command. The same is true for callers using Docker’s remote API to contact the daemon. If you require greater access control, you can create authorization plugins and add them to your Docker daemon configuration. Using an authorization plugin, a Docker administrator can configure granular access policies for managing access to Docker daemon.</p> <p>Anyone with the appropriate skills can develop an authorization plugin. These skills, at their most basic, are knowledge of Docker, understanding of REST, and sound programming knowledge. This document describes the architecture, state, and methods information available to an authorization plugin developer.</p> <h2 id=\"basic-principles\">Basic principles</h2> <p>Docker’s <a href=\"../plugin_api/index\">plugin infrastructure</a> enables extending Docker by loading, removing and communicating with third-party components using a generic API. The access authorization subsystem was built using this mechanism.</p> <p>Using this subsystem, you don’t need to rebuild the Docker daemon to add an authorization plugin. You can add a plugin to an installed Docker daemon. You do need to restart the Docker daemon to add a new plugin.</p> <p>An authorization plugin approves or denies requests to the Docker daemon based on both the current authentication context and the command context. The authentication context contains all user details and the authentication method. The command context contains all the relevant request data.</p> <p>Authorization plugins must follow the rules described in <a href=\"../plugin_api/index\">Docker Plugin API</a>. Each plugin must reside within directories described under the <a href=\"../plugin_api/index#plugin-discovery\">Plugin discovery</a> section.</p> <p><strong>Note</strong>: the abbreviations <code>AuthZ</code> and <code>AuthN</code> mean authorization and authentication respectively.</p> <h2 id=\"basic-architecture\">Basic architecture</h2> <p>You are responsible for registering your plugin as part of the Docker daemon startup. You can install multiple plugins and chain them together. This chain can be ordered. Each request to the daemon passes in order through the chain. Only when all the plugins grant access to the resource, is the access granted.</p> <p>When an HTTP request is made to the Docker daemon through the CLI or via the remote API, the authentication subsystem passes the request to the installed authentication plugin(s). The request contains the user (caller) and command context. The plugin is responsible for deciding whether to allow or deny the request.</p> <p>The sequence diagrams below depict an allow and deny authorization flow:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/extend/images/authz_allow.png\" alt=\"Authorization Allow flow\"></p> <p><img src=\"https://docs.docker.com/v1.11/engine/extend/images/authz_deny.png\" alt=\"Authorization Deny flow\"></p> <p>Each request sent to the plugin includes the authenticated user, the HTTP headers, and the request/response body. Only the user name and the authentication method used are passed to the plugin. Most importantly, no user credentials or tokens are passed. Finally, not all request/response bodies are sent to the authorization plugin. Only those request/response bodies where the <code>Content-Type</code> is either <code>text/*</code> or <code>application/json</code> are sent.</p> <p>For commands that can potentially hijack the HTTP connection (<code>HTTP\nUpgrade</code>), such as <code>exec</code>, the authorization plugin is only called for the initial HTTP requests. Once the plugin approves the command, authorization is not applied to the rest of the flow. Specifically, the streaming data is not passed to the authorization plugins. For commands that return chunked HTTP response, such as <code>logs</code> and <code>events</code>, only the HTTP request is sent to the authorization plugins.</p> <p>During request/response processing, some authorization flows might need to do additional queries to the Docker daemon. To complete such flows, plugins can call the daemon API similar to a regular user. To enable these additional queries, the plugin must provide the means for an administrator to configure proper authentication and security policies.</p> <h2 id=\"docker-client-flows\">Docker client flows</h2> <p>To enable and configure the authorization plugin, the plugin developer must support the Docker client interactions detailed in this section.</p> <h3 id=\"setting-up-docker-daemon\">Setting up Docker daemon</h3> <p>Enable the authorization plugin with a dedicated command line flag in the <code>--authorization-plugin=PLUGIN_ID</code> format. The flag supplies a <code>PLUGIN_ID</code> value. This value can be the plugin’s socket or a path to a specification file.</p> <pre>$ docker daemon --authorization-plugin=plugin1 --authorization-plugin=plugin2,...\n</pre> <p>Docker’s authorization subsystem supports multiple <code>--authorization-plugin</code> parameters.</p> <h3 id=\"calling-authorized-command-allow\">Calling authorized command (allow)</h3> <pre>$ docker pull centos\n...\nf1b10cd84249: Pull complete\n...\n</pre> <h3 id=\"calling-unauthorized-command-deny\">Calling unauthorized command (deny)</h3> <pre>$ docker pull centos\n...\ndocker: Error response from daemon: authorization denied by plugin PLUGIN_NAME: volumes are not allowed.\n</pre> <h3 id=\"error-from-plugins\">Error from plugins</h3> <pre>$ docker pull centos\n...\ndocker: Error response from daemon: plugin PLUGIN_NAME failed with error: AuthZPlugin.AuthZReq: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.\n</pre> <h2 id=\"api-schema-and-implementation\">API schema and implementation</h2> <p>In addition to Docker’s standard plugin registration method, each plugin should implement the following two methods:</p> <ul> <li><p><code>/AuthzPlugin.AuthZReq</code> This authorize request method is called before the Docker daemon processes the client request.</p></li> <li><p><code>/AuthzPlugin.AuthZRes</code> This authorize response method is called before the response is returned from Docker daemon to the client.</p></li> </ul> <h4 id=\"authzplugin-authzreq\">/AuthzPlugin.AuthZReq</h4> <p><strong>Request</strong>:</p> <pre>{\n    \"User\":              \"The user identification\",\n    \"UserAuthNMethod\":   \"The authentication method used\",\n    \"RequestMethod\":     \"The HTTP method\",\n    \"RequestURI\":        \"The HTTP request URI\",\n    \"RequestBody\":       \"Byte array containing the raw HTTP request body\",\n    \"RequestHeader\":     \"Byte array containing the raw HTTP request header as a map[string][]string \"\n}\n</pre> <p><strong>Response</strong>:</p> <pre>{\n    \"Allow\": \"Determined whether the user is allowed or not\",\n    \"Msg\":   \"The authorization message\",\n    \"Err\":   \"The error message if things go wrong\"\n}\n</pre> <h4 id=\"authzplugin-authzres\">/AuthzPlugin.AuthZRes</h4> <p><strong>Request</strong>:</p> <pre>{\n    \"User\":              \"The user identification\",\n    \"UserAuthNMethod\":   \"The authentication method used\",\n    \"RequestMethod\":     \"The HTTP method\",\n    \"RequestURI\":        \"The HTTP request URI\",\n    \"RequestBody\":       \"Byte array containing the raw HTTP request body\",\n    \"RequestHeader\":     \"Byte array containing the raw HTTP request header as a map[string][]string\",\n    \"ResponseBody\":      \"Byte array containing the raw HTTP response body\",\n    \"ResponseHeader\":    \"Byte array containing the raw HTTP response header as a map[string][]string\",\n    \"ResponseStatusCode\":\"Response status code\"\n}\n</pre> <p><strong>Response</strong>:</p> <pre>{\n   \"Allow\":              \"Determined whether the user is allowed or not\",\n   \"Msg\":                \"The authorization message\",\n   \"Err\":                \"The error message if things go wrong\"\n}\n</pre> <h3 id=\"request-authorization\">Request authorization</h3> <p>Each plugin must support two request authorization messages formats, one from the daemon to the plugin and then from the plugin to the daemon. The tables below detail the content expected in each message.</p> <h4 id=\"daemon-plugin\">Daemon -&gt; Plugin</h4> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>User</td> <td>string</td> <td>The user identification</td> </tr> <tr> <td>Authentication method</td> <td>string</td> <td>The authentication method used</td> </tr> <tr> <td>Request method</td> <td>enum</td> <td>The HTTP method (GET/DELETE/POST)</td> </tr> <tr> <td>Request URI</td> <td>string</td> <td>The HTTP request URI including API version (e.g., v.1.17/containers/json)</td> </tr> <tr> <td>Request headers</td> <td>map[string]string</td> <td>Request headers as key value pairs (without the authorization header)</td> </tr> <tr> <td>Request body</td> <td>[]byte</td> <td>Raw request body</td> </tr> </tbody> </table> <h4 id=\"plugin-daemon\">Plugin -&gt; Daemon</h4> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Allow</td> <td>bool</td> <td>Boolean value indicating whether the request is allowed or denied</td> </tr> <tr> <td>Msg</td> <td>string</td> <td>Authorization message (will be returned to the client in case the access is denied)</td> </tr> <tr> <td>Err</td> <td>string</td> <td>Error message (will be returned to the client in case the plugin encounter an error. The string value supplied may appear in logs, so should not include confidential information)</td> </tr> </tbody> </table> <h3 id=\"response-authorization\">Response authorization</h3> <p>The plugin must support two authorization messages formats, one from the daemon to the plugin and then from the plugin to the daemon. The tables below detail the content expected in each message.</p> <h4 id=\"daemon-plugin-1\">Daemon -&gt; Plugin</h4> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>User</td> <td>string</td> <td>The user identification</td> </tr> <tr> <td>Authentication method</td> <td>string</td> <td>The authentication method used</td> </tr> <tr> <td>Request method</td> <td>string</td> <td>The HTTP method (GET/DELETE/POST)</td> </tr> <tr> <td>Request URI</td> <td>string</td> <td>The HTTP request URI including API version (e.g., v.1.17/containers/json)</td> </tr> <tr> <td>Request headers</td> <td>map[string]string</td> <td>Request headers as key value pairs (without the authorization header)</td> </tr> <tr> <td>Request body</td> <td>[]byte</td> <td>Raw request body</td> </tr> <tr> <td>Response status code</td> <td>int</td> <td>Status code from the docker daemon</td> </tr> <tr> <td>Response headers</td> <td>map[string]string</td> <td>Response headers as key value pairs</td> </tr> <tr> <td>Response body</td> <td>[]byte</td> <td>Raw docker daemon response body</td> </tr> </tbody> </table> <h4 id=\"plugin-daemon-1\">Plugin -&gt; Daemon</h4> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Allow</td> <td>bool</td> <td>Boolean value indicating whether the response is allowed or denied</td> </tr> <tr> <td>Msg</td> <td>string</td> <td>Authorization message (will be returned to the client in case the access is denied)</td> </tr> <tr> <td>Err</td> <td>string</td> <td>Error message (will be returned to the client in case the plugin encounter an error. The string value supplied may appear in logs, so should not include confidential information)</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/extend/plugins_authorization/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/extend/plugins_authorization/</a>\n  </p>\n</div>\n","engine/extend/plugins/index":"<h1 id=\"understand-engine-plugins\">Understand Engine plugins</h1> <p>You can extend the capabilities of the Docker Engine by loading third-party plugins. This page explains the types of plugins and provides links to several volume and network plugins for Docker.</p> <h2 id=\"types-of-plugins\">Types of plugins</h2> <p>Plugins extend Docker’s functionality. They come in specific types. For example, a <a href=\"../plugins_volume/index\">volume plugin</a> might enable Docker volumes to persist across multiple Docker hosts and a <a href=\"../plugins_network/index\">network plugin</a> might provide network plumbing.</p> <p>Currently Docker supports authorization, volume and network driver plugins. In the future it will support additional plugin types.</p> <h2 id=\"installing-a-plugin\">Installing a plugin</h2> <p>Follow the instructions in the plugin’s documentation.</p> <h2 id=\"finding-a-plugin\">Finding a plugin</h2> <p>The sections below provide an inexhaustive overview of available plugins.</p>  <h3 id=\"network-plugins\">Network plugins</h3> <table> <thead> <tr> <th>Plugin</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><a href=\"https://github.com/contiv/netplugin\">Contiv Networking</a></td> <td>An open source network plugin to provide infrastructure and security policies for a multi-tenant micro services deployment, while providing an integration to physical network for non-container workload. Contiv Networking implements the remote driver and IPAM APIs available in Docker 1.9 onwards.</td> </tr> <tr> <td><a href=\"https://github.com/openstack/kuryr\">Kuryr Network Plugin</a></td> <td>A network plugin is developed as part of the OpenStack Kuryr project and implements the Docker networking (libnetwork) remote driver API by utilizing Neutron, the OpenStack networking service. It includes an IPAM driver as well.</td> </tr> <tr> <td><a href=\"https://www.weave.works/docs/net/latest/introducing-weave/\">Weave Network Plugin</a></td> <td>A network plugin that creates a virtual network that connects your Docker containers - across multiple hosts or clouds and enables automatic discovery of applications. Weave networks are resilient, partition tolerant, secure and work in partially connected networks, and other adverse environments - all configured with delightful simplicity.</td> </tr> </tbody> </table> <h3 id=\"volume-plugins\">Volume plugins</h3> <table> <thead> <tr> <th>Plugin</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><a href=\"https://github.com/blockbridge/blockbridge-docker-volume\">Blockbridge plugin</a></td> <td>A volume plugin that provides access to an extensible set of container-based persistent storage options. It supports single and multi-host Docker environments with features that include tenant isolation, automated provisioning, encryption, secure deletion, snapshots and QoS.</td> </tr> <tr> <td><a href=\"https://github.com/contiv/volplugin\">Contiv Volume Plugin</a></td> <td>An open source volume plugin that provides multi-tenant, persistent, distributed storage with intent based consumption using ceph underneath.</td> </tr> <tr> <td><a href=\"https://github.com/rancher/convoy\">Convoy plugin</a></td> <td>A volume plugin for a variety of storage back-ends including device mapper and NFS. It’s a simple standalone executable written in Go and provides the framework to support vendor-specific extensions such as snapshots, backups and restore.</td> </tr> <tr> <td><a href=\"https://www.drbd.org/en/supported-projects/docker\">DRBD plugin</a></td> <td>A volume plugin that provides highly available storage replicated by <a href=\"https://www.drbd.org\">DRBD</a>. Data written to the docker volume is replicated in a cluster of DRBD nodes.</td> </tr> <tr> <td><a href=\"https://clusterhq.com/docker-plugin/\">Flocker plugin</a></td> <td>A volume plugin that provides multi-host portable volumes for Docker, enabling you to run databases and other stateful containers and move them around across a cluster of machines.</td> </tr> <tr> <td><a href=\"https://github.com/mcuadros/gce-docker\">gce-docker plugin</a></td> <td>A volume plugin able to attach, format and mount Google Compute <a href=\"https://cloud.google.com/compute/docs/disks/persistent-disks\">persistent-disks</a>.</td> </tr> <tr> <td><a href=\"https://github.com/calavera/docker-volume-glusterfs\">GlusterFS plugin</a></td> <td>A volume plugin that provides multi-host volumes management for Docker using GlusterFS.</td> </tr> <tr> <td><a href=\"https://github.com/muthu-r/horcrux\">Horcrux Volume Plugin</a></td> <td>A volume plugin that allows on-demand, version controlled access to your data. Horcrux is an open-source plugin, written in Go, and supports SCP, <a href=\"https://www.minio.io\">Minio</a> and Amazon S3.</td> </tr> <tr> <td><a href=\"http://github.com/vdemeester/docker-volume-ipfs\">IPFS Volume Plugin</a></td> <td>An open source volume plugin that allows using an <a href=\"https://ipfs.io/\">ipfs</a> filesystem as a volume.</td> </tr> <tr> <td><a href=\"https://github.com/calavera/docker-volume-keywhiz\">Keywhiz plugin</a></td> <td>A plugin that provides credentials and secret management using Keywhiz as a central repository.</td> </tr> <tr> <td><a href=\"https://github.com/CWSpear/local-persist\">Local Persist Plugin</a></td> <td>A volume plugin that extends the default <code>local</code> driver’s functionality by allowing you specify a mountpoint anywhere on the host, which enables the files to <em>always persist</em>, even if the volume is removed via <code>docker volume rm</code>.</td> </tr> <tr> <td>\n<a href=\"https://github.com/NetApp/netappdvp\">NetApp Plugin</a> (nDVP)</td> <td>A volume plugin that provides direct integration with the Docker ecosystem for the NetApp storage portfolio. The nDVP package supports the provisioning and management of storage resources from the storage platform to Docker hosts, with a robust framework for adding additional platforms in the future.</td> </tr> <tr> <td><a href=\"https://github.com/ContainX/docker-volume-netshare\">Netshare plugin</a></td> <td>A volume plugin that provides volume management for NFS ¾, AWS EFS and CIFS file systems.</td> </tr> <tr> <td><a href=\"https://github.com/libopenstorage/openstorage\">OpenStorage Plugin</a></td> <td>A cluster-aware volume plugin that provides volume management for file and block storage solutions. It implements a vendor neutral specification for implementing extensions such as CoS, encryption, and snapshots. It has example drivers based on FUSE, NFS, NBD and EBS to name a few.</td> </tr> <tr> <td><a href=\"https://github.com/quobyte/docker-volume\">Quobyte Volume Plugin</a></td> <td>A volume plugin that connects Docker to <a href=\"http://www.quobyte.com/containers\">Quobyte</a>’s data center file system, a general-purpose scalable and fault-tolerant storage platform.</td> </tr> <tr> <td><a href=\"https://github.com/emccode/rexray\">REX-Ray plugin</a></td> <td>A volume plugin which is written in Go and provides advanced storage functionality for many platforms including VirtualBox, EC2, Google Compute Engine, OpenStack, and EMC.</td> </tr> <tr> <td><a href=\"https://github.com/vmware/docker-volume-vsphere\">VMware vSphere Storage Plugin</a></td> <td>Docker Volume Driver for vSphere enables customers to address persistent storage requirements for Docker containers in vSphere environments.</td> </tr> </tbody> </table> <h3 id=\"authorization-plugins\">Authorization plugins</h3> <table> <thead> <tr> <th>Plugin</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><a href=\"https://github.com/twistlock/authz\">Twistlock AuthZ Broker</a></td> <td>A basic extendable authorization plugin that runs directly on the host or inside a container. This plugin allows you to define user policies that it evaluates during authorization. Basic authorization is provided if Docker daemon is started with the --tlsverify flag (username is extracted from the certificate common name).</td> </tr> </tbody> </table> <h2 id=\"troubleshooting-a-plugin\">Troubleshooting a plugin</h2> <p>If you are having problems with Docker after loading a plugin, ask the authors of the plugin for help. The Docker team may not be able to assist you.</p> <h2 id=\"writing-a-plugin\">Writing a plugin</h2> <p>If you are interested in writing a plugin for Docker, or seeing how they work under the hood, see the <a href=\"../plugin_api/index\">docker plugins reference</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/extend/plugins/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/extend/plugins/</a>\n  </p>\n</div>\n","engine/extend/plugins_network/index":"<h1 id=\"engine-network-driver-plugins\">Engine network driver plugins</h1> <p>Docker Engine network plugins enable Engine deployments to be extended to support a wide range of networking technologies, such as VXLAN, IPVLAN, MACVLAN or something completely different. Network driver plugins are supported via the LibNetwork project. Each plugin is implemented as a “remote driver” for LibNetwork, which shares plugin infrastructure with Engine. Effectively, network driver plugins are activated in the same way as other plugins, and use the same kind of protocol.</p> <h2 id=\"using-network-driver-plugins\">Using network driver plugins</h2> <p>The means of installing and running a network driver plugin depend on the particular plugin. So, be sure to install your plugin according to the instructions obtained from the plugin developer.</p> <p>Once running however, network driver plugins are used just like the built-in network drivers: by being mentioned as a driver in network-oriented Docker commands. For example,</p> <pre>$ docker network create --driver weave mynet\n</pre> <p>Some network driver plugins are listed in <a href=\"../plugins/index\">plugins</a></p> <p>The <code>mynet</code> network is now owned by <code>weave</code>, so subsequent commands referring to that network will be sent to the plugin,</p> <pre>$ docker run --net=mynet busybox top\n</pre> <h2 id=\"write-a-network-plugin\">Write a network plugin</h2> <p>Network plugins implement the <a href=\"https://docs.docker.com/extend/plugin_api/\">Docker plugin API</a> and the network plugin protocol</p> <h2 id=\"network-plugin-protocol\">Network plugin protocol</h2> <p>The network driver protocol, in addition to the plugin activation call, is documented as part of libnetwork: <a href=\"https://github.com/docker/libnetwork/blob/master/docs/remote.md\">https://github.com/docker/libnetwork/blob/master/docs/remote.md</a>.</p> <h1 id=\"related-information\">Related Information</h1> <p>To interact with the Docker maintainers and other interested users, see the IRC channel <code>#docker-network</code>.</p> <ul> <li><a href=\"../../userguide/networking/index\">Docker networks feature overview</a></li> <li>The <a href=\"https://github.com/docker/libnetwork\">LibNetwork</a> project</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/extend/plugins_network/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/extend/plugins_network/</a>\n  </p>\n</div>\n","engine/extend/plugins_volume/index":"<h1 id=\"write-a-volume-plugin\">Write a volume plugin</h1> <p>Docker Engine volume plugins enable Engine deployments to be integrated with external storage systems, such as Amazon EBS, and enable data volumes to persist beyond the lifetime of a single Engine host. See the <a href=\"../plugins/index\">plugin documentation</a> for more information.</p> <h2 id=\"command-line-changes\">Command-line changes</h2> <p>A volume plugin makes use of the <code>-v</code>and <code>--volume-driver</code> flag on the <code>docker run</code> command. The <code>-v</code> flag accepts a volume name and the <code>--volume-driver</code> flag a driver type, for example:</p> <pre>$ docker run -ti -v volumename:/data --volume-driver=flocker   busybox sh\n</pre> <p>This command passes the <code>volumename</code> through to the volume plugin as a user-given name for the volume. The <code>volumename</code> must not begin with a <code>/</code>.</p> <p>By having the user specify a <code>volumename</code>, a plugin can associate the volume with an external volume beyond the lifetime of a single container or container host. This can be used, for example, to move a stateful container from one server to another.</p> <p>By specifying a <code>volumedriver</code> in conjunction with a <code>volumename</code>, users can use plugins such as <a href=\"https://clusterhq.com/docker-plugin/\">Flocker</a> to manage volumes external to a single host, such as those on EBS.</p> <h2 id=\"create-a-volumedriver\">Create a VolumeDriver</h2> <p>The container creation endpoint (<code>/containers/create</code>) accepts a <code>VolumeDriver</code> field of type <code>string</code> allowing to specify the name of the driver. It’s default value of <code>\"local\"</code> (the default driver for local volumes).</p> <h2 id=\"volume-plugin-protocol\">Volume plugin protocol</h2> <p>If a plugin registers itself as a <code>VolumeDriver</code> when activated, then it is expected to provide writeable paths on the host filesystem for the Docker daemon to provide to containers to consume.</p> <p>The Docker daemon handles bind-mounting the provided paths into user containers.</p> <blockquote> <p><strong>Note</strong>: Volume plugins should <em>not</em> write data to the <code>/var/lib/docker/</code> directory, including <code>/var/lib/docker/volumes</code>. The <code>/var/lib/docker/</code> directory is reserved for Docker.</p> </blockquote> <h3 id=\"volumedriver-create\">/VolumeDriver.Create</h3> <p><strong>Request</strong>:</p> <pre>{\n    \"Name\": \"volume_name\",\n    \"Opts\": {}\n}\n</pre> <p>Instruct the plugin that the user wants to create a volume, given a user specified volume name. The plugin does not need to actually manifest the volume on the filesystem yet (until Mount is called). Opts is a map of driver specific options passed through from the user request.</p> <p><strong>Response</strong>:</p> <pre>{\n    \"Err\": \"\"\n}\n</pre> <p>Respond with a string error if an error occurred.</p> <h3 id=\"volumedriver-remove\">/VolumeDriver.Remove</h3> <p><strong>Request</strong>:</p> <pre>{\n    \"Name\": \"volume_name\"\n}\n</pre> <p>Delete the specified volume from disk. This request is issued when a user invokes <code>docker rm -v</code> to remove volumes associated with a container.</p> <p><strong>Response</strong>:</p> <pre>{\n    \"Err\": \"\"\n}\n</pre> <p>Respond with a string error if an error occurred.</p> <h3 id=\"volumedriver-mount\">/VolumeDriver.Mount</h3> <p><strong>Request</strong>:</p> <pre>{\n    \"Name\": \"volume_name\"\n}\n</pre> <p>Docker requires the plugin to provide a volume, given a user specified volume name. This is called once per container start. If the same volume_name is requested more than once, the plugin may need to keep track of each new mount request and provision at the first mount request and deprovision at the last corresponding unmount request.</p> <p><strong>Response</strong>:</p> <pre>{\n    \"Mountpoint\": \"/path/to/directory/on/host\",\n    \"Err\": \"\"\n}\n</pre> <p>Respond with the path on the host filesystem where the volume has been made available, and/or a string error if an error occurred.</p> <h3 id=\"volumedriver-path\">/VolumeDriver.Path</h3> <p><strong>Request</strong>:</p> <pre>{\n    \"Name\": \"volume_name\"\n}\n</pre> <p>Docker needs reminding of the path to the volume on the host.</p> <p><strong>Response</strong>:</p> <pre>{\n    \"Mountpoint\": \"/path/to/directory/on/host\",\n    \"Err\": \"\"\n}\n</pre> <p>Respond with the path on the host filesystem where the volume has been made available, and/or a string error if an error occurred.</p> <h3 id=\"volumedriver-unmount\">/VolumeDriver.Unmount</h3> <p><strong>Request</strong>:</p> <pre>{\n    \"Name\": \"volume_name\"\n}\n</pre> <p>Indication that Docker no longer is using the named volume. This is called once per container stop. Plugin may deduce that it is safe to deprovision it at this point.</p> <p><strong>Response</strong>:</p> <pre>{\n    \"Err\": \"\"\n}\n</pre> <p>Respond with a string error if an error occurred.</p> <h3 id=\"volumedriver-get\">/VolumeDriver.Get</h3> <p><strong>Request</strong>:</p> <pre>{\n    \"Name\": \"volume_name\"\n}\n</pre> <p>Get the volume info.</p> <p><strong>Response</strong>:</p> <pre>{\n  \"Volume\": {\n    \"Name\": \"volume_name\",\n    \"Mountpoint\": \"/path/to/directory/on/host\",\n  },\n  \"Err\": \"\"\n}\n</pre> <p>Respond with a string error if an error occurred.</p> <h3 id=\"volumedriver-list\">/VolumeDriver.List</h3> <p><strong>Request</strong>:</p> <pre>{}\n</pre> <p>Get the list of volumes registered with the plugin.</p> <p><strong>Response</strong>:</p> <pre>{\n  \"Volumes\": [\n    {\n      \"Name\": \"volume_name\",\n      \"Mountpoint\": \"/path/to/directory/on/host\"\n    }\n  ],\n  \"Err\": \"\"\n}\n</pre> <p>Respond with a string error if an error occurred.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/extend/plugins_volume/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/extend/plugins_volume/</a>\n  </p>\n</div>\n","engine/security/seccomp/index":"<h1 id=\"seccomp-security-profiles-for-docker\">Seccomp security profiles for Docker</h1> <p>Secure computing mode (Seccomp) is a Linux kernel feature. You can use it to restrict the actions available within the container. The <code>seccomp()</code> system call operates on the seccomp state of the calling process. You can use this feature to restrict your application’s access.</p> <p>This feature is available only if Docker has been built with seccomp and the kernel is configured with <code>CONFIG_SECCOMP</code> enabled. To check if your kernel supports seccomp:</p> <pre>$ cat /boot/config-`uname -r` | grep CONFIG_SECCOMP=\nCONFIG_SECCOMP=y\n</pre> <blockquote> <p><strong>Note</strong>: seccomp profiles require seccomp 2.2.1 and are only available starting with Debian 9 “Stretch”, Ubuntu 15.10 “Wily”, and Fedora 22. To use this feature on Ubuntu 14.04, Debian Wheezy, or Debian Jessie, you must download the <a href=\"../../installation/binaries/index\">latest static Docker Linux binary</a>. This feature is currently <em>not</em> available on other distributions.</p> </blockquote> <h2 id=\"passing-a-profile-for-a-container\">Passing a profile for a container</h2> <p>The default seccomp profile provides a sane default for running containers with seccomp and disables around 44 system calls out of 300+. It is moderately protective while providing wide application compatibility. The default Docker profile (found <a href=\"https://github.com/docker/docker/blob/master/profiles/seccomp/default.json\">here</a> has a JSON layout in the following form:</p> <pre>{\n\t\"defaultAction\": \"SCMP_ACT_ERRNO\",\n\t\"architectures\": [\n\t\t\"SCMP_ARCH_X86_64\",\n\t\t\"SCMP_ARCH_X86\",\n\t\t\"SCMP_ARCH_X32\"\n\t],\n\t\"syscalls\": [\n\t\t{\n\t\t\t\"name\": \"accept\",\n\t\t\t\"action\": \"SCMP_ACT_ALLOW\",\n\t\t\t\"args\": []\n\t\t},\n\t\t{\n\t\t\t\"name\": \"accept4\",\n\t\t\t\"action\": \"SCMP_ACT_ALLOW\",\n\t\t\t\"args\": []\n\t\t},\n\t\t...\n\t]\n}\n</pre> <p>When you run a container, it uses the default profile unless you override it with the <code>security-opt</code> option. For example, the following explicitly specifies the default policy:</p> <pre>$ docker run --rm -it --security-opt seccomp=/path/to/seccomp/profile.json hello-world\n</pre> <h3 id=\"significant-syscalls-blocked-by-the-default-profile\">Significant syscalls blocked by the default profile</h3> <p>Docker’s default seccomp profile is a whitelist which specifies the calls that are allowed. The table below lists the significant (but not all) syscalls that are effectively blocked because they are not on the whitelist. The table includes the reason each syscall is blocked rather than white-listed.</p> <table> <thead> <tr> <th>Syscall</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>acct</code></td> <td>Accounting syscall which could let containers disable their own resource limits or process accounting. Also gated by <code>CAP_SYS_PACCT</code>.</td> </tr> <tr> <td><code>add_key</code></td> <td>Prevent containers from using the kernel keyring, which is not namespaced.</td> </tr> <tr> <td><code>adjtimex</code></td> <td>Similar to <code>clock_settime</code> and <code>settimeofday</code>, time/date is not namespaced.</td> </tr> <tr> <td><code>bpf</code></td> <td>Deny loading potentially persistent bpf programs into kernel, already gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> <tr> <td><code>clock_adjtime</code></td> <td>Time/date is not namespaced.</td> </tr> <tr> <td><code>clock_settime</code></td> <td>Time/date is not namespaced.</td> </tr> <tr> <td><code>clone</code></td> <td>Deny cloning new namespaces. Also gated by <code>CAP_SYS_ADMIN</code> for CLONE_* flags, except <code>CLONE_USERNS</code>.</td> </tr> <tr> <td><code>create_module</code></td> <td>Deny manipulation and functions on kernel modules.</td> </tr> <tr> <td><code>delete_module</code></td> <td>Deny manipulation and functions on kernel modules. Also gated by <code>CAP_SYS_MODULE</code>.</td> </tr> <tr> <td><code>finit_module</code></td> <td>Deny manipulation and functions on kernel modules. Also gated by <code>CAP_SYS_MODULE</code>.</td> </tr> <tr> <td><code>get_kernel_syms</code></td> <td>Deny retrieval of exported kernel and module symbols.</td> </tr> <tr> <td><code>get_mempolicy</code></td> <td>Syscall that modifies kernel memory and NUMA settings. Already gated by <code>CAP_SYS_NICE</code>.</td> </tr> <tr> <td><code>init_module</code></td> <td>Deny manipulation and functions on kernel modules. Also gated by <code>CAP_SYS_MODULE</code>.</td> </tr> <tr> <td><code>ioperm</code></td> <td>Prevent containers from modifying kernel I/O privilege levels. Already gated by <code>CAP_SYS_RAWIO</code>.</td> </tr> <tr> <td><code>iopl</code></td> <td>Prevent containers from modifying kernel I/O privilege levels. Already gated by <code>CAP_SYS_RAWIO</code>.</td> </tr> <tr> <td><code>kcmp</code></td> <td>Restrict process inspection capabilities, already blocked by dropping <code>CAP_PTRACE</code>.</td> </tr> <tr> <td><code>kexec_file_load</code></td> <td>Sister syscall of <code>kexec_load</code> that does the same thing, slightly different arguments.</td> </tr> <tr> <td><code>kexec_load</code></td> <td>Deny loading a new kernel for later execution.</td> </tr> <tr> <td><code>keyctl</code></td> <td>Prevent containers from using the kernel keyring, which is not namespaced.</td> </tr> <tr> <td><code>lookup_dcookie</code></td> <td>Tracing/profiling syscall, which could leak a lot of information on the host.</td> </tr> <tr> <td><code>mbind</code></td> <td>Syscall that modifies kernel memory and NUMA settings. Already gated by <code>CAP_SYS_NICE</code>.</td> </tr> <tr> <td><code>mount</code></td> <td>Deny mounting, already gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> <tr> <td><code>move_pages</code></td> <td>Syscall that modifies kernel memory and NUMA settings.</td> </tr> <tr> <td><code>name_to_handle_at</code></td> <td>Sister syscall to <code>open_by_handle_at</code>. Already gated by <code>CAP_SYS_NICE</code>.</td> </tr> <tr> <td><code>nfsservctl</code></td> <td>Deny interaction with the kernel nfs daemon.</td> </tr> <tr> <td><code>open_by_handle_at</code></td> <td>Cause of an old container breakout. Also gated by <code>CAP_DAC_READ_SEARCH</code>.</td> </tr> <tr> <td><code>perf_event_open</code></td> <td>Tracing/profiling syscall, which could leak a lot of information on the host.</td> </tr> <tr> <td><code>personality</code></td> <td>Prevent container from enabling BSD emulation. Not inherently dangerous, but poorly tested, potential for a lot of kernel vulns.</td> </tr> <tr> <td><code>pivot_root</code></td> <td>Deny <code>pivot_root</code>, should be privileged operation.</td> </tr> <tr> <td><code>process_vm_readv</code></td> <td>Restrict process inspection capabilities, already blocked by dropping <code>CAP_PTRACE</code>.</td> </tr> <tr> <td><code>process_vm_writev</code></td> <td>Restrict process inspection capabilities, already blocked by dropping <code>CAP_PTRACE</code>.</td> </tr> <tr> <td><code>ptrace</code></td> <td>Tracing/profiling syscall, which could leak a lot of information on the host. Already blocked by dropping <code>CAP_PTRACE</code>.</td> </tr> <tr> <td><code>query_module</code></td> <td>Deny manipulation and functions on kernel modules.</td> </tr> <tr> <td><code>quotactl</code></td> <td>Quota syscall which could let containers disable their own resource limits or process accounting. Also gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> <tr> <td><code>reboot</code></td> <td>Don’t let containers reboot the host. Also gated by <code>CAP_SYS_BOOT</code>.</td> </tr> <tr> <td><code>request_key</code></td> <td>Prevent containers from using the kernel keyring, which is not namespaced.</td> </tr> <tr> <td><code>set_mempolicy</code></td> <td>Syscall that modifies kernel memory and NUMA settings. Already gated by <code>CAP_SYS_NICE</code>.</td> </tr> <tr> <td><code>setns</code></td> <td>Deny associating a thread with a namespace. Also gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> <tr> <td><code>settimeofday</code></td> <td>Time/date is not namespaced. Also gated by <code>CAP_SYS_TIME</code>.</td> </tr> <tr> <td><code>stime</code></td> <td>Time/date is not namespaced. Also gated by <code>CAP_SYS_TIME</code>.</td> </tr> <tr> <td><code>swapon</code></td> <td>Deny start/stop swapping to file/device. Also gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> <tr> <td><code>swapoff</code></td> <td>Deny start/stop swapping to file/device. Also gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> <tr> <td><code>sysfs</code></td> <td>Obsolete syscall.</td> </tr> <tr> <td><code>_sysctl</code></td> <td>Obsolete, replaced by /proc/sys.</td> </tr> <tr> <td><code>umount</code></td> <td>Should be a privileged operation. Also gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> <tr> <td><code>umount2</code></td> <td>Should be a privileged operation.</td> </tr> <tr> <td><code>unshare</code></td> <td>Deny cloning new namespaces for processes. Also gated by <code>CAP_SYS_ADMIN</code>, with the exception of <code>unshare --user</code>.</td> </tr> <tr> <td><code>uselib</code></td> <td>Older syscall related to shared libraries, unused for a long time.</td> </tr> <tr> <td><code>userfaultfd</code></td> <td>Userspace page fault handling, largely needed for process migration.</td> </tr> <tr> <td><code>ustat</code></td> <td>Obsolete syscall.</td> </tr> <tr> <td><code>vm86</code></td> <td>In kernel x86 real mode virtual machine. Also gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> <tr> <td><code>vm86old</code></td> <td>In kernel x86 real mode virtual machine. Also gated by <code>CAP_SYS_ADMIN</code>.</td> </tr> </tbody> </table> <h2 id=\"run-without-the-default-seccomp-profile\">Run without the default seccomp profile</h2> <p>You can pass <code>unconfined</code> to run a container without the default seccomp profile.</p> <pre>$ docker run --rm -it --security-opt seccomp=unconfined debian:jessie \\\n    unshare --map-root-user --user sh -c whoami\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/seccomp/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/seccomp/</a>\n  </p>\n</div>\n","engine/extend/plugin_api/index":"<h1 id=\"docker-plugin-api\">Docker Plugin API</h1> <p>Docker plugins are out-of-process extensions which add capabilities to the Docker Engine.</p> <p>This page is intended for people who want to develop their own Docker plugin. If you just want to learn about or use Docker plugins, look <a href=\"../plugins/index\">here</a>.</p> <h2 id=\"what-plugins-are\">What plugins are</h2> <p>A plugin is a process running on the same or a different host as the docker daemon, which registers itself by placing a file on the same docker host in one of the plugin directories described in <a href=\"#plugin-discovery\">Plugin discovery</a>.</p> <p>Plugins have human-readable names, which are short, lowercase strings. For example, <code>flocker</code> or <code>weave</code>.</p> <p>Plugins can run inside or outside containers. Currently running them outside containers is recommended.</p> <h2 id=\"plugin-discovery\">Plugin discovery</h2> <p>Docker discovers plugins by looking for them in the plugin directory whenever a user or container tries to use one by name.</p> <p>There are three types of files which can be put in the plugin directory.</p> <ul> <li>\n<code>.sock</code> files are UNIX domain sockets.</li> <li>\n<code>.spec</code> files are text files containing a URL, such as <code>unix:///other.sock</code> or <code>tcp://localhost:8080</code>.</li> <li>\n<code>.json</code> files are text files containing a full json specification for the plugin.</li> </ul> <p>Plugins with UNIX domain socket files must run on the same docker host, whereas plugins with spec or json files can run on a different host if a remote URL is specified.</p> <p>UNIX domain socket files must be located under <code>/run/docker/plugins</code>, whereas spec files can be located either under <code>/etc/docker/plugins</code> or <code>/usr/lib/docker/plugins</code>.</p> <p>The name of the file (excluding the extension) determines the plugin name.</p> <p>For example, the <code>flocker</code> plugin might create a UNIX socket at <code>/run/docker/plugins/flocker.sock</code>.</p> <p>You can define each plugin into a separated subdirectory if you want to isolate definitions from each other. For example, you can create the <code>flocker</code> socket under <code>/run/docker/plugins/flocker/flocker.sock</code> and only mount <code>/run/docker/plugins/flocker</code> inside the <code>flocker</code> container.</p> <p>Docker always searches for unix sockets in <code>/run/docker/plugins</code> first. It checks for spec or json files under <code>/etc/docker/plugins</code> and <code>/usr/lib/docker/plugins</code> if the socket doesn’t exist. The directory scan stops as soon as it finds the first plugin definition with the given name.</p> <h3 id=\"json-specification\">JSON specification</h3> <p>This is the JSON format for a plugin:</p> <pre>{\n  \"Name\": \"plugin-example\",\n  \"Addr\": \"https://example.com/docker/plugin\",\n  \"TLSConfig\": {\n    \"InsecureSkipVerify\": false,\n    \"CAFile\": \"/usr/shared/docker/certs/example-ca.pem\",\n    \"CertFile\": \"/usr/shared/docker/certs/example-cert.pem\",\n    \"KeyFile\": \"/usr/shared/docker/certs/example-key.pem\",\n  }\n}\n</pre> <p>The <code>TLSConfig</code> field is optional and TLS will only be verified if this configuration is present.</p> <h2 id=\"plugin-lifecycle\">Plugin lifecycle</h2> <p>Plugins should be started before Docker, and stopped after Docker. For example, when packaging a plugin for a platform which supports <code>systemd</code>, you might use <a href=\"http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Before=\"><code>systemd</code> dependencies</a> to manage startup and shutdown order.</p> <p>When upgrading a plugin, you should first stop the Docker daemon, upgrade the plugin, then start Docker again.</p> <h2 id=\"plugin-activation\">Plugin activation</h2> <p>When a plugin is first referred to -- either by a user referring to it by name (e.g. <code>docker run --volume-driver=foo</code>) or a container already configured to use a plugin being started -- Docker looks for the named plugin in the plugin directory and activates it with a handshake. See Handshake API below.</p> <p>Plugins are <em>not</em> activated automatically at Docker daemon startup. Rather, they are activated only lazily, or on-demand, when they are needed.</p> <h2 id=\"systemd-socket-activation\">Systemd socket activation</h2> <p>Plugins may also be socket activated by <code>systemd</code>. The official <a href=\"https://github.com/docker/go-plugins-helpers\">Plugins helpers</a> natively supports socket activation. In order for a plugin to be socket activated it needs a <code>service</code> file and a <code>socket</code> file.</p> <p>The <code>service</code> file (for example <code>/lib/systemd/system/your-plugin.service</code>):</p> <pre>[Unit]\nDescription=Your plugin\nBefore=docker.service\nAfter=network.target your-plugin.socket\nRequires=your-plugin.socket docker.service\n\n[Service]\nExecStart=/usr/lib/docker/your-plugin\n\n[Install]\nWantedBy=multi-user.target\n</pre> <p>The <code>socket</code> file (for example <code>/lib/systemd/system/your-plugin.socket</code>):</p> <pre>[Unit]\nDescription=Your plugin\n\n[Socket]\nListenStream=/run/docker/plugins/your-plugin.sock\n\n[Install]\nWantedBy=sockets.target\n</pre> <p>This will allow plugins to be actually started when the Docker daemon connects to the sockets they’re listening on (for instance the first time the daemon uses them or if one of the plugin goes down accidentally).</p> <h2 id=\"api-design\">API design</h2> <p>The Plugin API is RPC-style JSON over HTTP, much like webhooks.</p> <p>Requests flow <em>from</em> the Docker daemon <em>to</em> the plugin. So the plugin needs to implement an HTTP server and bind this to the UNIX socket mentioned in the “plugin discovery” section.</p> <p>All requests are HTTP <code>POST</code> requests.</p> <p>The API is versioned via an Accept header, which currently is always set to <code>application/vnd.docker.plugins.v1+json</code>.</p> <h2 id=\"handshake-api\">Handshake API</h2> <p>Plugins are activated via the following “handshake” API call.</p> <h3 id=\"plugin-activate\">/Plugin.Activate</h3> <p><strong>Request:</strong> empty body</p> <p><strong>Response:</strong></p> <pre>{\n    \"Implements\": [\"VolumeDriver\"]\n}\n</pre> <p>Responds with a list of Docker subsystems which this plugin implements. After activation, the plugin will then be sent events from this subsystem.</p> <p>Possible values are:</p> <ul> <li><a href=\"../plugins_authorization/index\"><code>authz</code></a></li> <li><a href=\"../plugins_network/index\"><code>NetworkDriver</code></a></li> <li><a href=\"../plugins_volume/index\"><code>VolumeDriver</code></a></li> </ul> <h2 id=\"plugin-retries\">Plugin retries</h2> <p>Attempts to call a method on a plugin are retried with an exponential backoff for up to 30 seconds. This may help when packaging plugins as containers, since it gives plugin containers a chance to start up before failing any user containers which depend on them.</p> <h2 id=\"plugins-helpers\">Plugins helpers</h2> <p>To ease plugins development, we’re providing an <code>sdk</code> for each kind of plugins currently supported by Docker at <a href=\"https://github.com/docker/go-plugins-helpers\">docker/go-plugins-helpers</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/extend/plugin_api/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/extend/plugin_api/</a>\n  </p>\n</div>\n","engine/examples/mongodb/index":"<h1 id=\"dockerizing-mongodb\">Dockerizing MongoDB</h1> <h2 id=\"introduction\">Introduction</h2> <p>In this example, we are going to learn how to build a Docker image with MongoDB pre-installed. We’ll also see how to <code>push</code> that image to the <a href=\"https://hub.docker.com\">Docker Hub registry</a> and share it with others!</p> <blockquote> <p><strong>Note:</strong> This guide will show the mechanics of building a MongoDB container, but you will probably want to use the official image on <a href=\"https://hub.docker.com/_/mongo/\">Docker Hub</a></p> </blockquote> <p>Using Docker and containers for deploying <a href=\"https://www.mongodb.org/\">MongoDB</a> instances will bring several benefits, such as:</p> <ul> <li>Easy to maintain, highly configurable MongoDB instances;</li> <li>Ready to run and start working within milliseconds;</li> <li>Based on globally accessible and shareable images.</li> </ul> <blockquote> <p><strong>Note:</strong></p> <p>If you do <strong><em>not</em></strong> like <code>sudo</code>, you might want to check out: <a href=\"../../installation/binaries/index#giving-non-root-access\"><em>Giving non-root access</em></a>.</p> </blockquote> <h2 id=\"creating-a-dockerfile-for-mongodb\">Creating a Dockerfile for MongoDB</h2> <p>Let’s create our <code>Dockerfile</code> and start building it:</p> <pre>$ nano Dockerfile\n</pre> <p>Although optional, it is handy to have comments at the beginning of a <code>Dockerfile</code> explaining its purpose:</p> <pre># Dockerizing MongoDB: Dockerfile for building MongoDB images\n# Based on ubuntu:latest, installs MongoDB following the instructions from:\n# http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/\n</pre> <blockquote> <p><strong>Tip:</strong> <code>Dockerfile</code>s are flexible. However, they need to follow a certain format. The first item to be defined is the name of an image, which becomes the <em>parent</em> of your <em>Dockerized MongoDB</em> image.</p> </blockquote> <p>We will build our image using the latest version of Ubuntu from the <a href=\"https://hub.docker.com/_/ubuntu/\">Docker Hub Ubuntu</a> repository.</p> <pre># Format: FROM    repository[:version]\nFROM       ubuntu:latest\n</pre> <p>Continuing, we will declare the <code>MAINTAINER</code> of the <code>Dockerfile</code>:</p> <pre># Format: MAINTAINER Name &lt;email@addr.ess&gt;\nMAINTAINER M.Y. Name &lt;myname@addr.ess&gt;\n</pre> <blockquote> <p><strong>Note:</strong> Although Ubuntu systems have MongoDB packages, they are likely to be outdated. Therefore in this example, we will use the official MongoDB packages.</p> </blockquote> <p>We will begin with importing the MongoDB public GPG key. We will also create a MongoDB repository file for the package manager.</p> <pre># Installation:\n# Import MongoDB public GPG key AND create a MongoDB list file\nRUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10\nRUN echo \"deb http://repo.mongodb.org/apt/ubuntu \"$(lsb_release -sc)\"/mongodb-org/3.0 multiverse\" | tee /etc/apt/sources.list.d/mongodb-org-3.0.list\n</pre> <p>After this initial preparation we can update our packages and install MongoDB.</p> <pre># Update apt-get sources AND install MongoDB\nRUN apt-get update &amp;&amp; apt-get install -y mongodb-org\n</pre> <blockquote> <p><strong>Tip:</strong> You can install a specific version of MongoDB by using a list of required packages with versions, e.g.:</p> <pre>RUN apt-get update &amp;&amp; apt-get install -y mongodb-org=3.0.1 mongodb-org-server=3.0.1 mongodb-org-shell=3.0.1 mongodb-org-mongos=3.0.1 mongodb-org-tools=3.0.1\n</pre> </blockquote> <p>MongoDB requires a data directory. Let’s create it as the final step of our installation instructions.</p> <pre># Create the MongoDB data directory\nRUN mkdir -p /data/db\n</pre> <p>Lastly we set the <code>ENTRYPOINT</code> which will tell Docker to run <code>mongod</code> inside the containers launched from our MongoDB image. And for ports, we will use the <code>EXPOSE</code> instruction.</p> <pre># Expose port 27017 from the container to the host\nEXPOSE 27017\n\n# Set usr/bin/mongod as the dockerized entry-point application\nENTRYPOINT [\"/usr/bin/mongod\"]\n</pre> <p>Now save the file and let’s build our image.</p> <blockquote> <p><strong>Note:</strong></p> <p>The full version of this <code>Dockerfile</code> can be found <a href=\"https://github.com/docker/docker/blob/master/docs/examples/mongodb/Dockerfile\">here</a>.</p> </blockquote> <h2 id=\"building-the-mongodb-docker-image\">Building the MongoDB Docker image</h2> <p>With our <code>Dockerfile</code>, we can now build the MongoDB image using Docker. Unless experimenting, it is always a good practice to tag Docker images by passing the <code>--tag</code> option to <code>docker build</code> command.</p> <pre># Format: docker build --tag/-t &lt;user-name&gt;/&lt;repository&gt; .\n# Example:\n$ docker build --tag my/repo .\n</pre> <p>Once this command is issued, Docker will go through the <code>Dockerfile</code> and build the image. The final image will be tagged <code>my/repo</code>.</p> <h2 id=\"pushing-the-mongodb-image-to-docker-hub\">Pushing the MongoDB image to Docker Hub</h2> <p>All Docker image repositories can be hosted and shared on <a href=\"https://hub.docker.com\">Docker Hub</a> with the <code>docker push</code> command. For this, you need to be logged-in.</p> <pre># Log-in\n$ docker login\nUsername:\n..\n\n# Push the image\n# Format: docker push &lt;user-name&gt;/&lt;repository&gt;\n$ docker push my/repo\nThe push refers to a repository [my/repo] (len: 1)\nSending image list\nPushing repository my/repo (1 tags)\n..\n</pre> <h2 id=\"using-the-mongodb-image\">Using the MongoDB image</h2> <p>Using the MongoDB image we created, we can run one or more MongoDB instances as daemon process(es).</p> <pre># Basic way\n# Usage: docker run --name &lt;name for container&gt; -d &lt;user-name&gt;/&lt;repository&gt;\n$ docker run -p 27017:27017 --name mongo_instance_001 -d my/repo\n\n# Dockerized MongoDB, lean and mean!\n# Usage: docker run --name &lt;name for container&gt; -d &lt;user-name&gt;/&lt;repository&gt; --noprealloc --smallfiles\n$ docker run -p 27017:27017 --name mongo_instance_001 -d my/repo --smallfiles\n\n# Checking out the logs of a MongoDB container\n# Usage: docker logs &lt;name for container&gt;\n$ docker logs mongo_instance_001\n\n# Playing with MongoDB\n# Usage: mongo --port &lt;port you get from `docker ps`&gt;\n$ mongo --port 27017\n\n# If using docker-machine\n# Usage: mongo --port &lt;port you get from `docker ps`&gt;  --host &lt;ip address from `docker-machine ip VM_NAME`&gt;\n$ mongo --port 27017 --host 192.168.59.103\n</pre> <blockquote> <p><strong>Tip:</strong> If you want to run two containers on the same engine, then you will need to map the exposed port to two different ports on the host</p> </blockquote> <pre># Start two containers and map the ports\n$ docker run -p 28001:27017 --name mongo_instance_001 -d my/repo\n$ docker run -p 28002:27017 --name mongo_instance_002 -d my/repo\n\n# Now you can connect to each MongoDB instance on the two ports\n$ mongo --port 28001\n$ mongo --port 28002\n</pre> <ul> <li><a href=\"../../userguide/networking/default_network/dockerlinks/index\">Linking containers</a></li> <li><a href=\"../../admin/ambassador_pattern_linking/index\">Cross-host linking containers</a></li> <li><a href=\"https://docs.docker.com/docker-hub/builds/\">Creating an Automated Build</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/mongodb/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/mongodb/</a>\n  </p>\n</div>\n","engine/examples/postgresql_service/index":"<h1 id=\"dockerizing-postgresql\">Dockerizing PostgreSQL</h1> <blockquote> <p><strong>Note</strong>: - <strong>If you don’t like sudo</strong> then see <a href=\"../../installation/binaries/index#giving-non-root-access\"><em>Giving non-root access</em></a></p> </blockquote> <h2 id=\"installing-postgresql-on-docker\">Installing PostgreSQL on Docker</h2> <p>Assuming there is no Docker image that suits your needs on the <a href=\"http://hub.docker.com\">Docker Hub</a>, you can create one yourself.</p> <p>Start by creating a new <code>Dockerfile</code>:</p> <blockquote> <p><strong>Note</strong>: This PostgreSQL setup is for development-only purposes. Refer to the PostgreSQL documentation to fine-tune these settings so that it is suitably secure.</p> </blockquote> <pre>#\n# example Dockerfile for https://docs.docker.com/examples/postgresql_service/\n#\n\nFROM ubuntu\nMAINTAINER SvenDowideit@docker.com\n\n# Add the PostgreSQL PGP key to verify their Debian packages.\n# It should be the same key as https://www.postgresql.org/media/keys/ACCC4CF8.asc\nRUN apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys B97B0AFCAA1A47F044F244A07FCC7D46ACCC4CF8\n\n# Add PostgreSQL's repository. It contains the most recent stable release\n#     of PostgreSQL, ``9.3``.\nRUN echo \"deb http://apt.postgresql.org/pub/repos/apt/ precise-pgdg main\" &gt; /etc/apt/sources.list.d/pgdg.list\n\n# Install ``python-software-properties``, ``software-properties-common`` and PostgreSQL 9.3\n#  There are some warnings (in red) that show up during the build. You can hide\n#  them by prefixing each apt-get statement with DEBIAN_FRONTEND=noninteractive\nRUN apt-get update &amp;&amp; apt-get install -y python-software-properties software-properties-common postgresql-9.3 postgresql-client-9.3 postgresql-contrib-9.3\n\n# Note: The official Debian and Ubuntu images automatically ``apt-get clean``\n# after each ``apt-get``\n\n# Run the rest of the commands as the ``postgres`` user created by the ``postgres-9.3`` package when it was ``apt-get installed``\nUSER postgres\n\n# Create a PostgreSQL role named ``docker`` with ``docker`` as the password and\n# then create a database `docker` owned by the ``docker`` role.\n# Note: here we use ``&amp;&amp;\\`` to run commands one after the other - the ``\\``\n#       allows the RUN command to span multiple lines.\nRUN    /etc/init.d/postgresql start &amp;&amp;\\\n    psql --command \"CREATE USER docker WITH SUPERUSER PASSWORD 'docker';\" &amp;&amp;\\\n    createdb -O docker docker\n\n# Adjust PostgreSQL configuration so that remote connections to the\n# database are possible.\nRUN echo \"host all  all    0.0.0.0/0  md5\" &gt;&gt; /etc/postgresql/9.3/main/pg_hba.conf\n\n# And add ``listen_addresses`` to ``/etc/postgresql/9.3/main/postgresql.conf``\nRUN echo \"listen_addresses='*'\" &gt;&gt; /etc/postgresql/9.3/main/postgresql.conf\n\n# Expose the PostgreSQL port\nEXPOSE 5432\n\n# Add VOLUMEs to allow backup of config, logs and databases\nVOLUME  [\"/etc/postgresql\", \"/var/log/postgresql\", \"/var/lib/postgresql\"]\n\n# Set the default command to run when starting the container\nCMD [\"/usr/lib/postgresql/9.3/bin/postgres\", \"-D\", \"/var/lib/postgresql/9.3/main\", \"-c\", \"config_file=/etc/postgresql/9.3/main/postgresql.conf\"]\n</pre> <p>Build an image from the Dockerfile assign it a name.</p> <pre>$ docker build -t eg_postgresql .\n</pre> <p>And run the PostgreSQL server container (in the foreground):</p> <pre>$ docker run --rm -P --name pg_test eg_postgresql\n</pre> <p>There are 2 ways to connect to the PostgreSQL server. We can use <a href=\"../../userguide/networking/default_network/dockerlinks/index\"><em>Link Containers</em></a>, or we can access it from our host (or the network).</p> <blockquote> <p><strong>Note</strong>: The <code>--rm</code> removes the container and its image when the container exits successfully.</p> </blockquote> <h3 id=\"using-container-linking\">Using container linking</h3> <p>Containers can be linked to another container’s ports directly using <code>-link remote_name:local_alias</code> in the client’s <code>docker run</code>. This will set a number of environment variables that can then be used to connect:</p> <pre>$ docker run --rm -t -i --link pg_test:pg eg_postgresql bash\n\npostgres@7ef98b1b7243:/$ psql -h $PG_PORT_5432_TCP_ADDR -p $PG_PORT_5432_TCP_PORT -d docker -U docker --password\n</pre> <h3 id=\"connecting-from-your-host-system\">Connecting from your host system</h3> <p>Assuming you have the postgresql-client installed, you can use the host-mapped port to test as well. You need to use <code>docker ps</code> to find out what local host port the container is mapped to first:</p> <pre>$ docker ps\nCONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS              PORTS                                      NAMES\n5e24362f27f6        eg_postgresql:latest   /usr/lib/postgresql/   About an hour ago   Up About an hour    0.0.0.0:49153-&gt;5432/tcp                    pg_test\n$ psql -h localhost -p 49153 -d docker -U docker --password\n</pre> <h3 id=\"testing-the-database\">Testing the database</h3> <p>Once you have authenticated and have a <code>docker =#</code> prompt, you can create a table and populate it.</p> <pre>psql (9.3.1)\nType \"help\" for help.\n\n$ docker=# CREATE TABLE cities (\ndocker(#     name            varchar(80),\ndocker(#     location        point\ndocker(# );\nCREATE TABLE\n$ docker=# INSERT INTO cities VALUES ('San Francisco', '(-194.0, 53.0)');\nINSERT 0 1\n$ docker=# select * from cities;\n     name      | location\n---------------+-----------\n San Francisco | (-194,53)\n(1 row)\n</pre> <h3 id=\"using-the-container-volumes\">Using the container volumes</h3> <p>You can use the defined volumes to inspect the PostgreSQL log files and to backup your configuration and data:</p> <pre>$ docker run --rm --volumes-from pg_test -t -i busybox sh\n\n/ # ls\nbin      etc      lib      linuxrc  mnt      proc     run      sys      usr\ndev      home     lib64    media    opt      root     sbin     tmp      var\n/ # ls /etc/postgresql/9.3/main/\nenvironment      pg_hba.conf      postgresql.conf\npg_ctl.conf      pg_ident.conf    start.conf\n/tmp # ls /var/log\nldconfig    postgresql\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/postgresql_service/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/postgresql_service/</a>\n  </p>\n</div>\n","engine/examples/couchdb_data_volumes/index":"<h1 id=\"dockerizing-a-couchdb-service\">Dockerizing a CouchDB service</h1> <blockquote> <p><strong>Note</strong>: - <strong>If you don’t like sudo</strong> then see <a href=\"../../installation/binaries/index#giving-non-root-access\"><em>Giving non-root access</em></a></p> </blockquote> <p>Here’s an example of using data volumes to share the same data between two CouchDB containers. This could be used for hot upgrades, testing different versions of CouchDB on the same data, etc.</p> <h2 id=\"create-first-database\">Create first database</h2> <p>Note that we’re marking <code>/var/lib/couchdb</code> as a data volume.</p> <pre>$ COUCH1=$(docker run -d -p 5984 -v /var/lib/couchdb shykes/couchdb:2013-05-03)\n</pre> <h2 id=\"add-data-to-the-first-database\">Add data to the first database</h2> <p>We’re assuming your Docker host is reachable at <code>localhost</code>. If not, replace <code>localhost</code> with the public IP of your Docker host.</p> <pre>$ HOST=localhost\n$ URL=\"http://$HOST:$(docker port $COUCH1 5984 | grep -o '[1-9][0-9]*$')/_utils/\"\n$ echo \"Navigate to $URL in your browser, and use the couch interface to add data\"\n</pre> <h2 id=\"create-second-database\">Create second database</h2> <p>This time, we’re requesting shared access to <code>$COUCH1</code>’s volumes.</p> <pre>$ COUCH2=$(docker run -d -p 5984 --volumes-from $COUCH1 shykes/couchdb:2013-05-03)\n</pre> <h2 id=\"browse-data-on-the-second-database\">Browse data on the second database</h2> <pre>$ HOST=localhost\n$ URL=\"http://$HOST:$(docker port $COUCH2 5984 | grep -o '[1-9][0-9]*$')/_utils/\"\n$ echo \"Navigate to $URL in your browser. You should see the same data as in the first database\"'!'\n</pre> <p>Congratulations, you are now running two Couchdb containers, completely isolated from each other <em>except</em> for their data.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/couchdb_data_volumes/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/couchdb_data_volumes/</a>\n  </p>\n</div>\n","engine/examples/couchbase/index":"<h1 id=\"dockerizing-a-couchbase-service\">Dockerizing a Couchbase service</h1> <p>This example shows how to start a <a href=\"http://couchbase.com\">Couchbase</a> server using Docker Compose, configure it using its <a href=\"http://developer.couchbase.com/documentation/server/4.0/rest-api/rest-endpoints-all.html\">REST API</a>, and query it.</p> <p>Couchbase is an open source, document-oriented NoSQL database for modern web, mobile, and IoT applications. It is designed for ease of development and Internet-scale performance.</p> <h2 id=\"start-couchbase-server\">Start Couchbase server</h2> <p>Couchbase Docker images are published at <a href=\"https://hub.docker.com/_/couchbase/\">Docker Hub</a>.</p> <p>Start Couchbase server as:</p> <pre>docker run -d --name db -p 8091-8093:8091-8093 -p 11210:11210 couchbase\n</pre> <p>The purpose of each port exposed is explained at <a href=\"http://developer.couchbase.com/documentation/server/4.1/install/install-ports.html\">Couchbase Developer Portal - Network Configuration</a>.</p> <p>Logs can be seen as:</p> <pre>docker logs db\nStarting Couchbase Server -- Web UI available at http://&lt;ip&gt;:8091\n</pre> <blockquote> <p><strong>Note</strong>: The examples on this page assume that the Docker Host is reachable on <code>192.168.99.100</code>. Substitute <code>192.168.99.100</code> with the actual IP address of your Docker Host. If you’re running Docker using Docker machine, you can obtain the IP address of the Docker host using <code>docker-machine ip &lt;MACHINE-NAME&gt;</code>.</p> </blockquote> <p>The logs show that Couchbase console can be accessed at <code>http://192.168.99.100:8091</code>. The default username is <code>Administrator</code> and the password is <code>password</code>.</p> <h2 id=\"configure-couchbase-docker-container\">Configure Couchbase Docker container</h2> <p>By default, Couchbase server needs to be configured using the console before it can be used. This can be simplified by configuring it using the REST API.</p> <h3 id=\"configure-memory-for-data-and-index-service\">Configure memory for Data and Index service</h3> <p>Data, Query and Index are three different services that can be configured on a Couchbase instance. Each service has different operating needs. For example, Query is CPU intensive operation and so requires a faster processor. Index is disk heavy and so requires a faster solid state drive. Data needs to be read/written fast and so requires more memory.</p> <p>Memory needs to be configured for Data and Index service only.</p> <pre>curl -v -X POST http://192.168.99.100:8091/pools/default -d memoryQuota=300 -d indexMemoryQuota=300\n* Hostname was NOT found in DNS cache\n*   Trying 192.168.99.100...\n* Connected to 192.168.99.100 (192.168.99.100) port 8091 (#0)\n&gt; POST /pools/default HTTP/1.1\n&gt; User-Agent: curl/7.37.1\n&gt; Host: 192.168.99.100:8091\n&gt; Accept: */*\n&gt; Content-Length: 36\n&gt; Content-Type: application/x-www-form-urlencoded\n&gt;\n* upload completely sent off: 36 out of 36 bytes\n&lt; HTTP/1.1 401 Unauthorized\n&lt; WWW-Authenticate: Basic realm=\"Couchbase Server Admin / REST\"\n* Server Couchbase Server is not blacklisted\n&lt; Server: Couchbase Server\n&lt; Pragma: no-cache\n&lt; Date: Wed, 25 Nov 2015 22:48:16 GMT\n&lt; Content-Length: 0\n&lt; Cache-Control: no-cache\n&lt;\n* Connection #0 to host 192.168.99.100 left intact\n</pre> <p>The command shows an HTTP POST request to the REST endpoint <code>/pools/default</code>. The host is the IP address of the Docker machine. The port is the exposed port of Couchbase server. The memory and index quota for the server are passed in the request.</p> <h3 id=\"configure-data-query-and-index-services\">Configure Data, Query, and Index services</h3> <p>All three services, or only one of them, can be configured on each instance. This allows different Couchbase instances to use affinities and setup services accordingly. For example, if Docker host is running a machine with solid-state drive then only Data service can be started.</p> <pre>curl -v http://192.168.99.100:8091/node/controller/setupServices -d 'services=kv%2Cn1ql%2Cindex'\n* Hostname was NOT found in DNS cache\n*   Trying 192.168.99.100...\n* Connected to 192.168.99.100 (192.168.99.100) port 8091 (#0)\n&gt; POST /node/controller/setupServices HTTP/1.1\n&gt; User-Agent: curl/7.37.1\n&gt; Host: 192.168.99.100:8091\n&gt; Accept: */*\n&gt; Content-Length: 26\n&gt; Content-Type: application/x-www-form-urlencoded\n&gt;\n* upload completely sent off: 26 out of 26 bytes\n&lt; HTTP/1.1 200 OK\n* Server Couchbase Server is not blacklisted\n&lt; Server: Couchbase Server\n&lt; Pragma: no-cache\n&lt; Date: Wed, 25 Nov 2015 22:49:51 GMT\n&lt; Content-Length: 0\n&lt; Cache-Control: no-cache\n&lt;\n* Connection #0 to host 192.168.99.100 left intact\n</pre> <p>The command shows an HTTP POST request to the REST endpoint <code>/node/controller/setupServices</code>. The command shows that all three services are configured for the Couchbase server. The Data service is identified by <code>kv</code>, Query service is identified by <code>n1ql</code> and Index service identified by <code>index</code>.</p> <h3 id=\"setup-credentials-for-the-couchbase-server\">Setup credentials for the Couchbase server</h3> <p>Sets the username and password credentials that will subsequently be used for managing the Couchbase server.</p> <pre>curl -v -X POST http://192.168.99.100:8091/settings/web -d port=8091 -d username=Administrator -d password=password\n* Hostname was NOT found in DNS cache\n*   Trying 192.168.99.100...\n* Connected to 192.168.99.100 (192.168.99.100) port 8091 (#0)\n&gt; POST /settings/web HTTP/1.1\n&gt; User-Agent: curl/7.37.1\n&gt; Host: 192.168.99.100:8091\n&gt; Accept: */*\n&gt; Content-Length: 50\n&gt; Content-Type: application/x-www-form-urlencoded\n&gt;\n* upload completely sent off: 50 out of 50 bytes\n&lt; HTTP/1.1 200 OK\n* Server Couchbase Server is not blacklisted\n&lt; Server: Couchbase Server\n&lt; Pragma: no-cache\n&lt; Date: Wed, 25 Nov 2015 22:50:43 GMT\n&lt; Content-Type: application/json\n&lt; Content-Length: 44\n&lt; Cache-Control: no-cache\n&lt;\n* Connection #0 to host 192.168.99.100 left intact\n{\"newBaseUri\":\"http://192.168.99.100:8091/\"}\n</pre> <p>The command shows an HTTP POST request to the REST endpoint <code>/settings/web</code>. The user name and password credentials are passed in the request.</p> <h3 id=\"install-sample-data\">Install sample data</h3> <p>The Couchbase server can be easily load some sample data in the Couchbase instance.</p> <pre>curl -v -u Administrator:password -X POST http://192.168.99.100:8091/sampleBuckets/install -d '[\"travel-sample\"]'\n* Hostname was NOT found in DNS cache\n*   Trying 192.168.99.100...\n* Connected to 192.168.99.100 (192.168.99.100) port 8091 (#0)\n* Server auth using Basic with user 'Administrator'\n&gt; POST /sampleBuckets/install HTTP/1.1\n&gt; Authorization: Basic QWRtaW5pc3RyYXRvcjpwYXNzd29yZA==\n&gt; User-Agent: curl/7.37.1\n&gt; Host: 192.168.99.100:8091\n&gt; Accept: */*\n&gt; Content-Length: 17\n&gt; Content-Type: application/x-www-form-urlencoded\n&gt;\n* upload completely sent off: 17 out of 17 bytes\n&lt; HTTP/1.1 202 Accepted\n* Server Couchbase Server is not blacklisted\n&lt; Server: Couchbase Server\n&lt; Pragma: no-cache\n&lt; Date: Wed, 25 Nov 2015 22:51:51 GMT\n&lt; Content-Type: application/json\n&lt; Content-Length: 2\n&lt; Cache-Control: no-cache\n&lt;\n* Connection #0 to host 192.168.99.100 left intact\n[]\n</pre> <p>The command shows an HTTP POST request to the REST endpoint <code>/sampleBuckets/install</code>. The name of the sample bucket is passed in the request.</p> <p>Congratulations, you are now running a Couchbase container, fully configured using the REST API.</p> <h2 id=\"query-couchbase-using-cbq\">Query Couchbase using CBQ</h2> <p><a href=\"http://developer.couchbase.com/documentation/server/4.1/cli/cbq-tool.html\">CBQ</a>, short for Couchbase Query, is a CLI tool that allows to create, read, update, and delete JSON documents on a Couchbase server. This tool is installed as part of the Couchbase Docker image.</p> <p>Run CBQ tool:</p> <pre>docker run -it --link db:db couchbase cbq --engine http://db:8093\nCouchbase query shell connected to http://db:8093/ . Type Ctrl-D to exit.\ncbq&gt;\n</pre> <p><code>--engine</code> parameter to CBQ allows to specify the Couchbase server host and port running on the Docker host. For host, typically the host name or IP address of the host where Couchbase server is running is provided. In this case, the container name used when starting the container, <code>db</code>, can be used. <code>8093</code> port listens for all incoming queries.</p> <p>Couchbase allows to query JSON documents using <a href=\"http://developer.couchbase.com/documentation/server/4.1/n1ql/n1ql-language-reference/index.html\">N1QL</a>. N1QL is a comprehensive, declarative query language that brings SQL-like query capabilities to JSON documents.</p> <p>Query the database by running a N1QL query:</p> <pre>cbq&gt; select * from `travel-sample` limit 1;\n{\n    \"requestID\": \"97816771-3c25-4a1d-9ea8-eb6ad8a51919\",\n    \"signature\": {\n        \"*\": \"*\"\n    },\n    \"results\": [\n        {\n            \"travel-sample\": {\n                \"callsign\": \"MILE-AIR\",\n                \"country\": \"United States\",\n                \"iata\": \"Q5\",\n                \"icao\": \"MLA\",\n                \"id\": 10,\n                \"name\": \"40-Mile Air\",\n                \"type\": \"airline\"\n            }\n        }\n    ],\n    \"status\": \"success\",\n    \"metrics\": {\n        \"elapsedTime\": \"60.872423ms\",\n        \"executionTime\": \"60.792258ms\",\n        \"resultCount\": 1,\n        \"resultSize\": 300\n    }\n}\n</pre> <h2 id=\"couchbase-web-console\">Couchbase Web Console</h2> <p><a href=\"http://developer.couchbase.com/documentation/server/4.1/admin/ui-intro.html\">Couchbase Web Console</a> is a console that allows to manage a Couchbase instance. It can be seen at:</p> <p><code>http://192.168.99.100:8091/</code></p> <p>Make sure to replace the IP address with the IP address of your Docker Machine or <code>localhost</code> if Docker is running locally.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/examples/couchbase/web-console.png\" alt=\"Couchbase Web Console\"></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/couchbase/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/couchbase/</a>\n  </p>\n</div>\n","engine/examples/running_redis_service/index":"<h1 id=\"dockerizing-a-redis-service\">Dockerizing a Redis service</h1> <p>Very simple, no frills, Redis service attached to a web application using a link.</p> <h2 id=\"create-a-docker-container-for-redis\">Create a Docker container for Redis</h2> <p>Firstly, we create a <code>Dockerfile</code> for our new Redis image.</p> <pre>FROM        ubuntu:14.04\nRUN         apt-get update &amp;&amp; apt-get install -y redis-server\nEXPOSE      6379\nENTRYPOINT  [\"/usr/bin/redis-server\"]\n</pre> <p>Next we build an image from our <code>Dockerfile</code>. Replace <code>&lt;your username&gt;</code> with your own user name.</p> <pre>$ docker build -t &lt;your username&gt;/redis .\n</pre> <h2 id=\"run-the-service\">Run the service</h2> <p>Use the image we’ve just created and name your container <code>redis</code>.</p> <p>Running the service with <code>-d</code> runs the container in detached mode, leaving the container running in the background.</p> <p>Importantly, we’re not exposing any ports on our container. Instead we’re going to use a container link to provide access to our Redis database.</p> <pre>$ docker run --name redis -d &lt;your username&gt;/redis\n</pre> <h2 id=\"create-your-web-application-container\">Create your web application container</h2> <p>Next we can create a container for our application. We’re going to use the <code>-link</code> flag to create a link to the <code>redis</code> container we’ve just created with an alias of <code>db</code>. This will create a secure tunnel to the <code>redis</code> container and expose the Redis instance running inside that container to only this container.</p> <pre>$ docker run --link redis:db -i -t ubuntu:14.04 /bin/bash\n</pre> <p>Once inside our freshly created container we need to install Redis to get the <code>redis-cli</code> binary to test our connection.</p> <pre>$ sudo apt-get update\n$ sudo apt-get install redis-server\n$ sudo service redis-server stop\n</pre> <p>As we’ve used the <code>--link redis:db</code> option, Docker has created some environment variables in our web application container.</p> <pre>$ env | grep DB_\n\n# Should return something similar to this with your values\nDB_NAME=/violet_wolf/db\nDB_PORT_6379_TCP_PORT=6379\nDB_PORT=tcp://172.17.0.33:6379\nDB_PORT_6379_TCP=tcp://172.17.0.33:6379\nDB_PORT_6379_TCP_ADDR=172.17.0.33\nDB_PORT_6379_TCP_PROTO=tcp\n</pre> <p>We can see that we’ve got a small list of environment variables prefixed with <code>DB</code>. The <code>DB</code> comes from the link alias specified when we launched the container. Let’s use the <code>DB_PORT_6379_TCP_ADDR</code> variable to connect to our Redis container.</p> <pre>$ redis-cli -h $DB_PORT_6379_TCP_ADDR\n$ redis 172.17.0.33:6379&gt;\n$ redis 172.17.0.33:6379&gt; set docker awesome\nOK\n$ redis 172.17.0.33:6379&gt; get docker\n\"awesome\"\n$ redis 172.17.0.33:6379&gt; exit\n</pre> <p>We could easily use this or other environment variables in our web application to make a connection to our <code>redis</code> container.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/running_redis_service/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/running_redis_service/</a>\n  </p>\n</div>\n","engine/examples/running_ssh_service/index":"<h1 id=\"dockerizing-an-ssh-daemon-service\">Dockerizing an SSH daemon service</h1> <h2 id=\"build-an-eg-sshd-image\">Build an <code>eg_sshd</code> image</h2> <p>The following <code>Dockerfile</code> sets up an SSHd service in a container that you can use to connect to and inspect other container’s volumes, or to get quick access to a test container.</p> <pre># sshd\n#\n# VERSION               0.0.2\n\nFROM ubuntu:14.04\nMAINTAINER Sven Dowideit &lt;SvenDowideit@docker.com&gt;\n\nRUN apt-get update &amp;&amp; apt-get install -y openssh-server\nRUN mkdir /var/run/sshd\nRUN echo 'root:screencast' | chpasswd\nRUN sed -i 's/PermitRootLogin without-password/PermitRootLogin yes/' /etc/ssh/sshd_config\n\n# SSH login fix. Otherwise user is kicked off after login\nRUN sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd\n\nENV NOTVISIBLE \"in users profile\"\nRUN echo \"export VISIBLE=now\" &gt;&gt; /etc/profile\n\nEXPOSE 22\nCMD [\"/usr/sbin/sshd\", \"-D\"]\n</pre> <p>Build the image using:</p> <pre>$ docker build -t eg_sshd .\n</pre> <h2 id=\"run-a-test-sshd-container\">Run a <code>test_sshd</code> container</h2> <p>Then run it. You can then use <code>docker port</code> to find out what host port the container’s port 22 is mapped to:</p> <pre>$ docker run -d -P --name test_sshd eg_sshd\n$ docker port test_sshd 22\n0.0.0.0:49154\n</pre> <p>And now you can ssh as <code>root</code> on the container’s IP address (you can find it with <code>docker inspect</code>) or on port <code>49154</code> of the Docker daemon’s host IP address (<code>ip address</code> or <code>ifconfig</code> can tell you that) or <code>localhost</code> if on the Docker daemon host:</p> <pre>$ ssh root@192.168.1.2 -p 49154\n# The password is ``screencast``.\n$$\n</pre> <h2 id=\"environment-variables\">Environment variables</h2> <p>Using the <code>sshd</code> daemon to spawn shells makes it complicated to pass environment variables to the user’s shell via the normal Docker mechanisms, as <code>sshd</code> scrubs the environment before it starts the shell.</p> <p>If you’re setting values in the <code>Dockerfile</code> using <code>ENV</code>, you’ll need to push them to a shell initialization file like the <code>/etc/profile</code> example in the <code>Dockerfile</code> above.</p> <p>If you need to pass<code>docker run -e ENV=value</code> values, you will need to write a short script to do the same before you start <code>sshd -D</code> and then replace the <code>CMD</code> with that script.</p> <h2 id=\"clean-up\">Clean up</h2> <p>Finally, clean up after your test by stopping and removing the container, and then removing the image.</p> <pre>$ docker stop test_sshd\n$ docker rm test_sshd\n$ docker rmi eg_sshd\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/running_ssh_service/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/running_ssh_service/</a>\n  </p>\n</div>\n","engine/examples/running_riak_service/index":"<h1 id=\"dockerizing-a-riak-service\">Dockerizing a Riak service</h1> <p>The goal of this example is to show you how to build a Docker image with Riak pre-installed.</p> <h2 id=\"creating-a-dockerfile\">Creating a Dockerfile</h2> <p>Create an empty file called <code>Dockerfile</code>:</p> <pre>$ touch Dockerfile\n</pre> <p>Next, define the parent image you want to use to build your image on top of. We’ll use <a href=\"https://hub.docker.com/_/ubuntu/\">Ubuntu</a> (tag: <code>trusty</code>), which is available on <a href=\"https://hub.docker.com\">Docker Hub</a>:</p> <pre># Riak\n#\n# VERSION       0.1.1\n\n# Use the Ubuntu base image provided by dotCloud\nFROM ubuntu:trusty\nMAINTAINER Hector Castro hector@basho.com\n</pre> <p>After that, we install the curl which is used to download the repository setup script and we download the setup script and run it.</p> <pre># Install Riak repository before we do apt-get update, so that update happens\n# in a single step\nRUN apt-get install -q -y curl &amp;&amp; \\\n    curl -fsSL https://packagecloud.io/install/repositories/basho/riak/script.deb | sudo bash\n</pre> <p>Then we install and setup a few dependencies:</p> <ul> <li>\n<code>supervisor</code> is used manage the Riak processes</li> <li>\n<code>riak=2.0.5-1</code> is the Riak package coded to version 2.0.5</li> </ul>  <pre># Install and setup project dependencies\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y supervisor riak=2.0.5-1\n\nRUN mkdir -p /var/log/supervisor\n\nRUN locale-gen en_US en_US.UTF-8\n\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n</pre> <p>After that, we modify Riak’s configuration:</p> <pre># Configure Riak to accept connections from any host\nRUN sed -i \"s|listener.http.internal = 127.0.0.1:8098|listener.http.internal = 0.0.0.0:8098|\" /etc/riak/riak.conf\nRUN sed -i \"s|listener.protobuf.internal = 127.0.0.1:8087|listener.protobuf.internal = 0.0.0.0:8087|\" /etc/riak/riak.conf\n</pre> <p>Then, we expose the Riak Protocol Buffers and HTTP interfaces:</p> <pre># Expose Riak Protocol Buffers and HTTP interfaces\nEXPOSE 8087 8098\n</pre> <p>Finally, run <code>supervisord</code> so that Riak is started:</p> <pre>CMD [\"/usr/bin/supervisord\"]\n</pre> <h2 id=\"create-a-supervisord-configuration-file\">Create a supervisord configuration file</h2> <p>Create an empty file called <code>supervisord.conf</code>. Make sure it’s at the same directory level as your <code>Dockerfile</code>:</p> <pre>touch supervisord.conf\n</pre> <p>Populate it with the following program definitions:</p> <pre>[supervisord]\nnodaemon=true\n\n[program:riak]\ncommand=bash -c \"/usr/sbin/riak console\"\nnumprocs=1\nautostart=true\nautorestart=true\nuser=riak\nenvironment=HOME=\"/var/lib/riak\"\nstdout_logfile=/var/log/supervisor/%(program_name)s.log\nstderr_logfile=/var/log/supervisor/%(program_name)s.log\n</pre> <h2 id=\"build-the-docker-image-for-riak\">Build the Docker image for Riak</h2> <p>Now you should be able to build a Docker image for Riak:</p> <pre>$ docker build -t \"&lt;yourname&gt;/riak\" .\n</pre> <h2 id=\"next-steps\">Next steps</h2> <p>Riak is a distributed database. Many production deployments consist of <a href=\"http://basho.com/why-your-riak-cluster-should-have-at-least-five-nodes/\">at least five nodes</a>. See the <a href=\"https://github.com/hectcastro/docker-riak\">docker-riak</a> project details on how to deploy a Riak cluster using Docker and Pipework.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/running_riak_service/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/running_riak_service/</a>\n  </p>\n</div>\n","engine/examples/apt-cacher-ng/index":"<h1 id=\"dockerizing-an-apt-cacher-ng-service\">Dockerizing an apt-cacher-ng service</h1> <blockquote> <p><strong>Note</strong>: - <strong>If you don’t like sudo</strong> then see <a href=\"../../installation/binaries/index#giving-non-root-access\"><em>Giving non-root access</em></a>. - <strong>If you’re using OS X or docker via TCP</strong> then you shouldn’t use sudo.</p> </blockquote> <p>When you have multiple Docker servers, or build unrelated Docker containers which can’t make use of the Docker build cache, it can be useful to have a caching proxy for your packages. This container makes the second download of any package almost instant.</p> <p>Use the following Dockerfile:</p> <pre>#\n# Build: docker build -t apt-cacher .\n# Run: docker run -d -p 3142:3142 --name apt-cacher-run apt-cacher\n#\n# and then you can run containers with:\n#   docker run -t -i --rm -e http_proxy http://dockerhost:3142/ debian bash\n#\n# Here, `dockerhost` is the IP address or FQDN of a host running the Docker daemon\n# which acts as an APT proxy server.\nFROM        ubuntu\nMAINTAINER  SvenDowideit@docker.com\n\nVOLUME      [\"/var/cache/apt-cacher-ng\"]\nRUN     apt-get update &amp;&amp; apt-get install -y apt-cacher-ng\n\nEXPOSE      3142\nCMD     chmod 777 /var/cache/apt-cacher-ng &amp;&amp; /etc/init.d/apt-cacher-ng start &amp;&amp; tail -f /var/log/apt-cacher-ng/*\n</pre> <p>To build the image using:</p> <pre>$ docker build -t eg_apt_cacher_ng .\n</pre> <p>Then run it, mapping the exposed port to one on the host</p> <pre>$ docker run -d -p 3142:3142 --name test_apt_cacher_ng eg_apt_cacher_ng\n</pre> <p>To see the logfiles that are <code>tailed</code> in the default command, you can use:</p> <pre>$ docker logs -f test_apt_cacher_ng\n</pre> <p>To get your Debian-based containers to use the proxy, you have following options</p> <ol> <li>Add an apt Proxy setting <code>echo 'Acquire::http { Proxy \"http://dockerhost:3142\"; };' &gt;&gt; /etc/apt/conf.d/01proxy</code>\n</li> <li>Set an environment variable: <code>http_proxy=http://dockerhost:3142/</code>\n</li> <li>Change your <code>sources.list</code> entries to start with <code>http://dockerhost:3142/</code>\n</li> <li>Link Debian-based containers to the APT proxy container using <code>--link</code>\n</li> <li>Create a custom network of an APT proxy container with Debian-based containers.</li> </ol> <p><strong>Option 1</strong> injects the settings safely into your apt configuration in a local version of a common base:</p> <pre>FROM ubuntu\nRUN  echo 'Acquire::http { Proxy \"http://dockerhost:3142\"; };' &gt;&gt; /etc/apt/apt.conf.d/01proxy\nRUN apt-get update &amp;&amp; apt-get install -y vim git\n\n# docker build -t my_ubuntu .\n</pre> <p><strong>Option 2</strong> is good for testing, but will break other HTTP clients which obey <code>http_proxy</code>, such as <code>curl</code>, <code>wget</code> and others:</p> <pre>$ docker run --rm -t -i -e http_proxy=http://dockerhost:3142/ debian bash\n</pre> <p><strong>Option 3</strong> is the least portable, but there will be times when you might need to do it and you can do it from your <code>Dockerfile</code> too.</p> <p><strong>Option 4</strong> links Debian-containers to the proxy server using following command:</p> <pre>$ docker run -i -t --link test_apt_cacher_ng:apt_proxy -e http_proxy=http://apt_proxy:3142/ debian bash\n</pre> <p><strong>Option 5</strong> creates a custom network of APT proxy server and Debian-based containers:</p> <pre>$ docker network create mynetwork\n$ docker run -d -p 3142:3142 --net=mynetwork --name test_apt_cacher_ng eg_apt_cacher_ng\n$ docker run --rm -it --net=mynetwork -e http_proxy=http://test_apt_cacher_ng:3142/ debian bash\n</pre> <p>Apt-cacher-ng has some tools that allow you to manage the repository, and they can be used by leveraging the <code>VOLUME</code> instruction, and the image we built to run the service:</p> <pre>$ docker run --rm -t -i --volumes-from test_apt_cacher_ng eg_apt_cacher_ng bash\n\n$$ /usr/lib/apt-cacher-ng/distkill.pl\nScanning /var/cache/apt-cacher-ng, please wait...\nFound distributions:\nbla, taggedcount: 0\n     1. precise-security (36 index files)\n     2. wheezy (25 index files)\n     3. precise-updates (36 index files)\n     4. precise (36 index files)\n     5. wheezy-updates (18 index files)\n\nFound architectures:\n     6. amd64 (36 index files)\n     7. i386 (24 index files)\n\nWARNING: The removal action may wipe out whole directories containing\n         index files. Select d to see detailed list.\n\n(Number nn: tag distribution or architecture nn; 0: exit; d: show details; r: remove tagged; q: quit): q\n</pre> <p>Finally, clean up after your test by stopping and removing the container, and then removing the image.</p> <pre>$ docker stop test_apt_cacher_ng\n$ docker rm test_apt_cacher_ng\n$ docker rmi eg_apt_cacher_ng\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/apt-cacher-ng/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/apt-cacher-ng/</a>\n  </p>\n</div>\n","engine/reference/commandline/cli/index":"<h1 id=\"use-the-docker-command-line\">Use the Docker command line</h1> <p>To list available commands, either run <code>docker</code> with no parameters or execute <code>docker help</code>:</p> <pre>$ docker\n  Usage: docker [OPTIONS] COMMAND [arg...]\n         docker daemon [ --help | ... ]\n         docker [ --help | -v | --version ]\n\n    -H, --host=[]: The socket(s) to talk to the Docker daemon in the format of tcp://host:port/path, unix:///path/to/socket, fd://* or fd://socketfd.\n\n  A self-sufficient runtime for Linux containers.\n\n  ...\n</pre> <p>Depending on your Docker system configuration, you may be required to preface each <code>docker</code> command with <code>sudo</code>. To avoid having to use <code>sudo</code> with the <code>docker</code> command, your system administrator can create a Unix group called <code>docker</code> and add users to it.</p> <p>For more information about installing Docker or <code>sudo</code> configuration, refer to the <a href=\"../../../installation/index\">installation</a> instructions for your operating system.</p> <h2 id=\"environment-variables\">Environment variables</h2> <p>For easy reference, the following list of environment variables are supported by the <code>docker</code> command line:</p> <ul> <li>\n<code>DOCKER_API_VERSION</code> The API version to use (e.g. <code>1.19</code>)</li> <li>\n<code>DOCKER_CONFIG</code> The location of your client configuration files.</li> <li>\n<code>DOCKER_CERT_PATH</code> The location of your authentication keys.</li> <li>\n<code>DOCKER_DRIVER</code> The graph driver to use.</li> <li>\n<code>DOCKER_HOST</code> Daemon socket to connect to.</li> <li>\n<code>DOCKER_NOWARN_KERNEL_VERSION</code> Prevent warnings that your Linux kernel is unsuitable for Docker.</li> <li>\n<code>DOCKER_RAMDISK</code> If set this will disable ‘pivot_root’.</li> <li>\n<code>DOCKER_TLS_VERIFY</code> When set Docker uses TLS and verifies the remote.</li> <li>\n<code>DOCKER_CONTENT_TRUST</code> When set Docker uses notary to sign and verify images. Equates to <code>--disable-content-trust=false</code> for build, create, pull, push, run.</li> <li>\n<code>DOCKER_CONTENT_TRUST_SERVER</code> The URL of the Notary server to use. This defaults to the same URL as the registry.</li> <li>\n<code>DOCKER_TMPDIR</code> Location for temporary Docker files.</li> </ul> <p>Because Docker is developed using ‘Go’, you can also use any environment variables used by the ‘Go’ runtime. In particular, you may find these useful:</p> <ul> <li><code>HTTP_PROXY</code></li> <li><code>HTTPS_PROXY</code></li> <li><code>NO_PROXY</code></li> </ul> <p>These Go environment variables are case-insensitive. See the <a href=\"http://golang.org/pkg/net/http/\">Go specification</a> for details on these variables.</p> <h2 id=\"configuration-files\">Configuration files</h2> <p>By default, the Docker command line stores its configuration files in a directory called <code>.docker</code> within your <code>$HOME</code> directory. However, you can specify a different location via the <code>DOCKER_CONFIG</code> environment variable or the <code>--config</code> command line option. If both are specified, then the <code>--config</code> option overrides the <code>DOCKER_CONFIG</code> environment variable. For example:</p> <pre>docker --config ~/testconfigs/ ps\n</pre> <p>Instructs Docker to use the configuration files in your <code>~/testconfigs/</code> directory when running the <code>ps</code> command.</p> <p>Docker manages most of the files in the configuration directory and you should not modify them. However, you <em>can modify</em> the <code>config.json</code> file to control certain aspects of how the <code>docker</code> command behaves.</p> <p>Currently, you can modify the <code>docker</code> command behavior using environment variables or command-line options. You can also use options within <code>config.json</code> to modify some of the same behavior. When using these mechanisms, you must keep in mind the order of precedence among them. Command line options override environment variables and environment variables override properties you specify in a <code>config.json</code> file.</p> <p>The <code>config.json</code> file stores a JSON encoding of several properties:</p> <p>The property <code>HttpHeaders</code> specifies a set of headers to include in all messages sent from the Docker client to the daemon. Docker does not try to interpret or understand these header; it simply puts them into the messages. Docker does not allow these headers to change any headers it sets for itself.</p> <p>The property <code>psFormat</code> specifies the default format for <code>docker ps</code> output. When the <code>--format</code> flag is not provided with the <code>docker ps</code> command, Docker’s client uses this property. If this property is not set, the client falls back to the default table format. For a list of supported formatting directives, see the <a href=\"../ps/index\"><strong>Formatting</strong> section in the <code>docker ps</code> documentation</a></p> <p>Once attached to a container, users detach from it and leave it running using the using <code>CTRL-p CTRL-q</code> key sequence. This detach key sequence is customizable using the <code>detachKeys</code> property. Specify a <code>&lt;sequence&gt;</code> value for the property. The format of the <code>&lt;sequence&gt;</code> is a comma-separated list of either a letter [a-Z], or the <code>ctrl-</code> combined with any of the following:</p> <ul> <li>\n<code>a-z</code> (a single lowercase alpha character )</li> <li>\n<code>@</code> (at sign)</li> <li>\n<code>[</code> (left bracket)</li> <li>\n<code>\\\\</code> (two backward slashes)</li> <li>\n<code>_</code> (underscore)</li> <li>\n<code>^</code> (caret)</li> </ul> <p>Your customization applies to all containers started in with your Docker client. Users can override your custom or the default key sequence on a per-container basis. To do this, the user specifies the <code>--detach-keys</code> flag with the <code>docker\nattach</code>, <code>docker exec</code>, <code>docker run</code> or <code>docker start</code> command.</p> <p>The property <code>imagesFormat</code> specifies the default format for <code>docker images</code> output. When the <code>--format</code> flag is not provided with the <code>docker images</code> command, Docker’s client uses this property. If this property is not set, the client falls back to the default table format. For a list of supported formatting directives, see the <a href=\"../images/index\"><strong>Formatting</strong> section in the <code>docker images</code> documentation</a></p> <p>Following is a sample <code>config.json</code> file:</p> <pre>{\n  \"HttpHeaders\": {\n    \"MyHeader\": \"MyValue\"\n  },\n  \"psFormat\": \"table {{.ID}}\\\\t{{.Image}}\\\\t{{.Command}}\\\\t{{.Labels}}\",\n  \"imagesFormat\": \"table {{.ID}}\\\\t{{.Repository}}\\\\t{{.Tag}}\\\\t{{.CreatedAt}}\",\n  \"detachKeys\": \"ctrl-e,e\"\n}\n</pre> <h3 id=\"notary\">Notary</h3> <p>If using your own notary server and a self-signed certificate or an internal Certificate Authority, you need to place the certificate at <code>tls/&lt;registry_url&gt;/ca.crt</code> in your docker config directory.</p> <p>Alternatively you can trust the certificate globally by adding it to your system’s list of root Certificate Authorities.</p> <h2 id=\"help\">Help</h2> <p>To list the help on any command just execute the command, followed by the <code>--help</code> option.</p> <pre>$ docker run --help\n\nUsage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n\nRun a command in a new container\n\n  -a, --attach=[]            Attach to STDIN, STDOUT or STDERR\n  --cpu-shares=0             CPU shares (relative weight)\n...\n</pre> <h2 id=\"option-types\">Option types</h2> <p>Single character command line options can be combined, so rather than typing <code>docker run -i -t --name test busybox sh</code>, you can write <code>docker run -it --name test busybox sh</code>.</p> <h3 id=\"boolean\">Boolean</h3> <p>Boolean options take the form <code>-d=false</code>. The value you see in the help text is the default value which is set if you do <strong>not</strong> specify that flag. If you specify a Boolean flag without a value, this will set the flag to <code>true</code>, irrespective of the default value.</p> <p>For example, running <code>docker run -d</code> will set the value to <code>true</code>, so your container <strong>will</strong> run in “detached” mode, in the background.</p> <p>Options which default to <code>true</code> (e.g., <code>docker build --rm=true</code>) can only be set to the non-default value by explicitly setting them to <code>false</code>:</p> <pre>$ docker build --rm=false .\n</pre> <h3 id=\"multi\">Multi</h3> <p>You can specify options like <code>-a=[]</code> multiple times in a single command line, for example in these commands:</p> <pre>$ docker run -a stdin -a stdout -i -t ubuntu /bin/bash\n$ docker run -a stdin -a stdout -a stderr ubuntu /bin/ls\n</pre> <p>Sometimes, multiple options can call for a more complex value string as for <code>-v</code>:</p> <pre>$ docker run -v /host:/container example/mysql\n</pre> <blockquote> <p><strong>Note:</strong> Do not use the <code>-t</code> and <code>-a stderr</code> options together due to limitations in the <code>pty</code> implementation. All <code>stderr</code> in <code>pty</code> mode simply goes to <code>stdout</code>.</p> </blockquote> <h3 id=\"strings-and-integers\">Strings and Integers</h3> <p>Options like <code>--name=\"\"</code> expect a string, and they can only be specified once. Options like <code>-c=0</code> expect an integer, and they can only be specified once.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/cli/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/cli/</a>\n  </p>\n</div>\n","engine/reference/builder/index":"<h1 id=\"dockerfile-reference\">Dockerfile reference</h1> <p>Docker can build images automatically by reading the instructions from a <code>Dockerfile</code>. A <code>Dockerfile</code> is a text document that contains all the commands a user could call on the command line to assemble an image. Using <code>docker build</code> users can create an automated build that executes several command-line instructions in succession.</p> <p>This page describes the commands you can use in a <code>Dockerfile</code>. When you are done reading this page, refer to the <a href=\"../../userguide/eng-image/dockerfile_best-practices/index\"><code>Dockerfile</code> Best Practices</a> for a tip-oriented guide.</p> <h2 id=\"usage\">Usage</h2> <p>The <a href=\"../commandline/build/index\"><code>docker build</code></a> command builds an image from a <code>Dockerfile</code> and a <em>context</em>. The build’s context is the files at a specified location <code>PATH</code> or <code>URL</code>. The <code>PATH</code> is a directory on your local filesystem. The <code>URL</code> is a the location of a Git repository.</p> <p>A context is processed recursively. So, a <code>PATH</code> includes any subdirectories and the <code>URL</code> includes the repository and its submodules. A simple build command that uses the current directory as context:</p> <pre>$ docker build .\nSending build context to Docker daemon  6.51 MB\n...\n</pre> <p>The build is run by the Docker daemon, not by the CLI. The first thing a build process does is send the entire context (recursively) to the daemon. In most cases, it’s best to start with an empty directory as context and keep your Dockerfile in that directory. Add only the files needed for building the Dockerfile.</p> <blockquote> <p><strong>Warning</strong>: Do not use your root directory, <code>/</code>, as the <code>PATH</code> as it causes the build to transfer the entire contents of your hard drive to the Docker daemon.</p> </blockquote> <p>To use a file in the build context, the <code>Dockerfile</code> refers to the file specified in an instruction, for example, a <code>COPY</code> instruction. To increase the build’s performance, exclude files and directories by adding a <code>.dockerignore</code> file to the context directory. For information about how to <a href=\"#dockerignore-file\">create a <code>.dockerignore</code> file</a> see the documentation on this page.</p> <p>Traditionally, the <code>Dockerfile</code> is called <code>Dockerfile</code> and located in the root of the context. You use the <code>-f</code> flag with <code>docker build</code> to point to a Dockerfile anywhere in your file system.</p> <pre>$ docker build -f /path/to/a/Dockerfile .\n</pre> <p>You can specify a repository and tag at which to save the new image if the build succeeds:</p> <pre>$ docker build -t shykes/myapp .\n</pre> <p>To tag the image into multiple repositories after the build, add multiple <code>-t</code> parameters when you run the <code>build</code> command:</p> <pre>$ docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest .\n</pre> <p>The Docker daemon runs the instructions in the <code>Dockerfile</code> one-by-one, committing the result of each instruction to a new image if necessary, before finally outputting the ID of your new image. The Docker daemon will automatically clean up the context you sent.</p> <p>Note that each instruction is run independently, and causes a new image to be created - so <code>RUN cd /tmp</code> will not have any effect on the next instructions.</p> <p>Whenever possible, Docker will re-use the intermediate images (cache), to accelerate the <code>docker build</code> process significantly. This is indicated by the <code>Using cache</code> message in the console output. (For more information, see the <a href=\"../../userguide/eng-image/dockerfile_best-practices/index#build-cache\">Build cache section</a>) in the <code>Dockerfile</code> best practices guide:</p> <pre>$ docker build -t svendowideit/ambassador .\nSending build context to Docker daemon 15.36 kB\nStep 0 : FROM alpine:3.2\n ---&gt; 31f630c65071\nStep 1 : MAINTAINER SvenDowideit@home.org.au\n ---&gt; Using cache\n ---&gt; 2a1c91448f5f\nStep 2 : RUN apk update &amp;&amp;      apk add socat &amp;&amp;        rm -r /var/cache/\n ---&gt; Using cache\n ---&gt; 21ed6e7fbb73\nStep 3 : CMD env | grep _TCP= | (sed 's/.*_PORT_\\([0-9]*\\)_TCP=tcp:\\/\\/\\(.*\\):\\(.*\\)/socat -t 100000000 TCP4-LISTEN:\\1,fork,reuseaddr TCP4:\\2:\\3 \\&amp;/' &amp;&amp; echo wait) | sh\n ---&gt; Using cache\n ---&gt; 7ea8aef582cc\nSuccessfully built 7ea8aef582cc\n</pre> <p>When you’re done with your build, you’re ready to look into <a href=\"../../userguide/containers/dockerrepos/index#contributing-to-docker-hub\"><em>Pushing a repository to its registry</em></a>.</p> <h2 id=\"format\">Format</h2> <p>Here is the format of the <code>Dockerfile</code>:</p> <pre># Comment\nINSTRUCTION arguments\n</pre> <p>The instruction is not case-sensitive, however convention is for them to be UPPERCASE in order to distinguish them from arguments more easily.</p> <p>Docker runs the instructions in a <code>Dockerfile</code> in order. <strong>The first instruction must be `FROM`</strong> in order to specify the <a href=\"../glossary/index#base-image\"><em>Base Image</em></a> from which you are building.</p> <p>Docker will treat lines that <em>begin</em> with <code>#</code> as a comment. A <code>#</code> marker anywhere else in the line will be treated as an argument. This allows statements like:</p> <pre># Comment\nRUN echo 'we are running some # of cool things'\n</pre> <p>Here is the set of instructions you can use in a <code>Dockerfile</code> for building images.</p> <h3 id=\"environment-replacement\">Environment replacement</h3> <p>Environment variables (declared with <a href=\"#env\">the <code>ENV</code> statement</a>) can also be used in certain instructions as variables to be interpreted by the <code>Dockerfile</code>. Escapes are also handled for including variable-like syntax into a statement literally.</p> <p>Environment variables are notated in the <code>Dockerfile</code> either with <code>$variable_name</code> or <code>${variable_name}</code>. They are treated equivalently and the brace syntax is typically used to address issues with variable names with no whitespace, like <code>${foo}_bar</code>.</p> <p>The <code>${variable_name}</code> syntax also supports a few of the standard <code>bash</code> modifiers as specified below:</p> <ul> <li>\n<code>${variable:-word}</code> indicates that if <code>variable</code> is set then the result will be that value. If <code>variable</code> is not set then <code>word</code> will be the result.</li> <li>\n<code>${variable:+word}</code> indicates that if <code>variable</code> is set then <code>word</code> will be the result, otherwise the result is the empty string.</li> </ul> <p>In all cases, <code>word</code> can be any string, including additional environment variables.</p> <p>Escaping is possible by adding a <code>\\</code> before the variable: <code>\\$foo</code> or <code>\\${foo}</code>, for example, will translate to <code>$foo</code> and <code>${foo}</code> literals respectively.</p> <p>Example (parsed representation is displayed after the <code>#</code>):</p> <pre>FROM busybox\nENV foo /bar\nWORKDIR ${foo}   # WORKDIR /bar\nADD . $foo       # ADD . /bar\nCOPY \\$foo /quux # COPY $foo /quux\n</pre> <p>Environment variables are supported by the following list of instructions in the <code>Dockerfile</code>:</p> <ul> <li><code>ADD</code></li> <li><code>COPY</code></li> <li><code>ENV</code></li> <li><code>EXPOSE</code></li> <li><code>LABEL</code></li> <li><code>USER</code></li> <li><code>WORKDIR</code></li> <li><code>VOLUME</code></li> <li><code>STOPSIGNAL</code></li> </ul> <p>as well as:</p> <ul> <li>\n<code>ONBUILD</code> (when combined with one of the supported instructions above)</li> </ul> <blockquote> <p><strong>Note</strong>: prior to 1.4, <code>ONBUILD</code> instructions did <strong>NOT</strong> support environment variable, even when combined with any of the instructions listed above.</p> </blockquote> <p>Environment variable substitution will use the same value for each variable throughout the entire command. In other words, in this example:</p> <pre>ENV abc=hello\nENV abc=bye def=$abc\nENV ghi=$abc\n</pre> <p>will result in <code>def</code> having a value of <code>hello</code>, not <code>bye</code>. However, <code>ghi</code> will have a value of <code>bye</code> because it is not part of the same command that set <code>abc</code> to <code>bye</code>.</p> <h3 id=\"dockerignore-file\">.dockerignore file</h3> <p>Before the docker CLI sends the context to the docker daemon, it looks for a file named <code>.dockerignore</code> in the root directory of the context. If this file exists, the CLI modifies the context to exclude files and directories that match patterns in it. This helps to avoid unnecessarily sending large or sensitive files and directories to the daemon and potentially adding them to images using <code>ADD</code> or <code>COPY</code>.</p> <p>The CLI interprets the <code>.dockerignore</code> file as a newline-separated list of patterns similar to the file globs of Unix shells. For the purposes of matching, the root of the context is considered to be both the working and the root directory. For example, the patterns <code>/foo/bar</code> and <code>foo/bar</code> both exclude a file or directory named <code>bar</code> in the <code>foo</code> subdirectory of <code>PATH</code> or in the root of the git repository located at <code>URL</code>. Neither excludes anything else.</p> <p>Here is an example <code>.dockerignore</code> file:</p> <pre>    */temp*\n    */*/temp*\n    temp?\n</pre> <p>This file causes the following build behavior:</p> <table> <thead> <tr> <th>Rule</th> <th>Behavior</th> </tr> </thead> <tbody> <tr> <td><code>*/temp*</code></td> <td>Exclude files and directories whose names start with <code>temp</code> in any immediate subdirectory of the root. For example, the plain file <code>/somedir/temporary.txt</code> is excluded, as is the directory <code>/somedir/temp</code>.</td> </tr> <tr> <td><code>*/*/temp*</code></td> <td>Exclude files and directories starting with <code>temp</code> from any subdirectory that is two levels below the root. For example, <code>/somedir/subdir/temporary.txt</code> is excluded.</td> </tr> <tr> <td><code>temp?</code></td> <td>Exclude files and directories in the root directory whose names are a one-character extension of <code>temp</code>. For example, <code>/tempa</code> and <code>/tempb</code> are excluded.</td> </tr> </tbody> </table> <p>Matching is done using Go’s <a href=\"http://golang.org/pkg/path/filepath#Match\">filepath.Match</a> rules. A preprocessing step removes leading and trailing whitespace and eliminates <code>.</code> and <code>..</code> elements using Go’s <a href=\"http://golang.org/pkg/path/filepath/#Clean\">filepath.Clean</a>. Lines that are blank after preprocessing are ignored.</p> <p>Beyond Go’s filepath.Match rules, Docker also supports a special wildcard string <code>**</code> that matches any number of directories (including zero). For example, <code>**/*.go</code> will exclude all files that end with <code>.go</code> that are found in all directories, including the root of the build context.</p> <p>Lines starting with <code>!</code> (exclamation mark) can be used to make exceptions to exclusions. The following is an example <code>.dockerignore</code> file that uses this mechanism:</p> <pre>    *.md\n    !README.md\n</pre> <p>All markdown files <em>except</em> <code>README.md</code> are excluded from the context.</p> <p>The placement of <code>!</code> exception rules influences the behavior: the last line of the <code>.dockerignore</code> that matches a particular file determines whether it is included or excluded. Consider the following example:</p> <pre>    *.md\n    !README*.md\n    README-secret.md\n</pre> <p>No markdown files are included in the context except README files other than <code>README-secret.md</code>.</p> <p>Now consider this example:</p> <pre>    *.md\n    README-secret.md\n    !README*.md\n</pre> <p>All of the README files are included. The middle line has no effect because <code>!README*.md</code> matches <code>README-secret.md</code> and comes last.</p> <p>You can even use the <code>.dockerignore</code> file to exclude the <code>Dockerfile</code> and <code>.dockerignore</code> files. These files are still sent to the daemon because it needs them to do its job. But the <code>ADD</code> and <code>COPY</code> commands do not copy them to the image.</p> <p>Finally, you may want to specify which files to include in the context, rather than which to exclude. To achieve this, specify <code>*</code> as the first pattern, followed by one or more <code>!</code> exception patterns.</p> <p><strong>Note</strong>: For historical reasons, the pattern <code>.</code> is ignored.</p> <h2 id=\"from\">FROM</h2> <pre>FROM &lt;image&gt;\n</pre> <p>Or</p> <pre>FROM &lt;image&gt;:&lt;tag&gt;\n</pre> <p>Or</p> <pre>FROM &lt;image&gt;@&lt;digest&gt;\n</pre> <p>The <code>FROM</code> instruction sets the <a href=\"../glossary/index#base-image\"><em>Base Image</em></a> for subsequent instructions. As such, a valid <code>Dockerfile</code> must have <code>FROM</code> as its first instruction. The image can be any valid image – it is especially easy to start by <strong>pulling an image</strong> from the <a href=\"../../userguide/containers/dockerrepos/index\"><em>Public Repositories</em></a>.</p> <ul> <li><p><code>FROM</code> must be the first non-comment instruction in the <code>Dockerfile</code>.</p></li> <li><p><code>FROM</code> can appear multiple times within a single <code>Dockerfile</code> in order to create multiple images. Simply make a note of the last image ID output by the commit before each new <code>FROM</code> command.</p></li> <li><p>The <code>tag</code> or <code>digest</code> values are optional. If you omit either of them, the builder assumes a <code>latest</code> by default. The builder returns an error if it cannot match the <code>tag</code> value.</p></li> </ul> <h2 id=\"maintainer\">MAINTAINER</h2> <pre>MAINTAINER &lt;name&gt;\n</pre> <p>The <code>MAINTAINER</code> instruction allows you to set the <em>Author</em> field of the generated images.</p> <h2 id=\"run\">RUN</h2> <p>RUN has 2 forms:</p> <ul> <li>\n<code>RUN &lt;command&gt;</code> (<em>shell</em> form, the command is run in a shell - <code>/bin/sh -c</code>)</li> <li>\n<code>RUN [\"executable\", \"param1\", \"param2\"]</code> (<em>exec</em> form)</li> </ul> <p>The <code>RUN</code> instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the <code>Dockerfile</code>.</p> <p>Layering <code>RUN</code> instructions and generating commits conforms to the core concepts of Docker where commits are cheap and containers can be created from any point in an image’s history, much like source control.</p> <p>The <em>exec</em> form makes it possible to avoid shell string munging, and to <code>RUN</code> commands using a base image that does not contain <code>/bin/sh</code>.</p> <p>In the <em>shell</em> form you can use a <code>\\</code> (backslash) to continue a single RUN instruction onto the next line. For example, consider these two lines:</p> <pre>RUN /bin/bash -c 'source $HOME/.bashrc ;\\\necho $HOME'\n</pre> <p>Together they are equivalent to this single line:</p> <pre>RUN /bin/bash -c 'source $HOME/.bashrc ; echo $HOME'\n</pre> <blockquote> <p><strong>Note</strong>: To use a different shell, other than ‘/bin/sh’, use the <em>exec</em> form passing in the desired shell. For example, <code>RUN [\"/bin/bash\", \"-c\", \"echo hello\"]</code></p> <p><strong>Note</strong>: The <em>exec</em> form is parsed as a JSON array, which means that you must use double-quotes (“) around words not single-quotes (‘).</p> <p><strong>Note</strong>: Unlike the <em>shell</em> form, the <em>exec</em> form does not invoke a command shell. This means that normal shell processing does not happen. For example, <code>RUN [ \"echo\", \"$HOME\" ]</code> will not do variable substitution on <code>$HOME</code>. If you want shell processing then either use the <em>shell</em> form or execute a shell directly, for example: <code>RUN [ \"sh\", \"-c\", \"echo $HOME\" ]</code>.</p> <p><strong>Note</strong>: In the <em>JSON</em> form, it is necessary to escape backslashes. This is particularly relevant on Windows where the backslash is the path seperator. The following line would otherwise be treated as <em>shell</em> form due to not being valid JSON, and fail in an unexpected way: <code>RUN [\"c:\\windows\\system32\\tasklist.exe\"]</code> The correct syntax for this example is: <code>RUN [\"c:\\\\windows\\\\system32\\\\tasklist.exe\"]</code></p> </blockquote> <p>The cache for <code>RUN</code> instructions isn’t invalidated automatically during the next build. The cache for an instruction like <code>RUN apt-get dist-upgrade -y</code> will be reused during the next build. The cache for <code>RUN</code> instructions can be invalidated by using the <code>--no-cache</code> flag, for example <code>docker build --no-cache</code>.</p> <p>See the <a href=\"../../userguide/eng-image/dockerfile_best-practices/index#build-cache\"><code>Dockerfile</code> Best Practices guide</a> for more information.</p> <p>The cache for <code>RUN</code> instructions can be invalidated by <code>ADD</code> instructions. See <a href=\"#add\">below</a> for details.</p> <h3 id=\"known-issues-run\">Known issues (RUN)</h3> <ul> <li>\n<a href=\"https://github.com/docker/docker/issues/783\">Issue 783</a> is about file permissions problems that can occur when using the AUFS file system. You might notice it during an attempt to <code>rm</code> a file, for example.</li> </ul> <p>For systems that have recent aufs version (i.e., <code>dirperm1</code> mount option can be set), docker will attempt to fix the issue automatically by mounting the layers with <code>dirperm1</code> option. More details on <code>dirperm1</code> option can be found at <a href=\"http://aufs.sourceforge.net/aufs3/man.html\"><code>aufs</code> man page</a></p> <p>If your system doesn’t have support for <code>dirperm1</code>, the issue describes a workaround.</p> <h2 id=\"cmd\">CMD</h2> <p>The <code>CMD</code> instruction has three forms:</p> <ul> <li>\n<code>CMD [\"executable\",\"param1\",\"param2\"]</code> (<em>exec</em> form, this is the preferred form)</li> <li>\n<code>CMD [\"param1\",\"param2\"]</code> (as <em>default parameters to ENTRYPOINT</em>)</li> <li>\n<code>CMD command param1 param2</code> (<em>shell</em> form)</li> </ul> <p>There can only be one <code>CMD</code> instruction in a <code>Dockerfile</code>. If you list more than one <code>CMD</code> then only the last <code>CMD</code> will take effect.</p> <p><strong>The main purpose of a <code>CMD</code> is to provide defaults for an executing container.</strong> These defaults can include an executable, or they can omit the executable, in which case you must specify an <code>ENTRYPOINT</code> instruction as well.</p> <blockquote> <p><strong>Note</strong>: If <code>CMD</code> is used to provide default arguments for the <code>ENTRYPOINT</code> instruction, both the <code>CMD</code> and <code>ENTRYPOINT</code> instructions should be specified with the JSON array format.</p> <p><strong>Note</strong>: The <em>exec</em> form is parsed as a JSON array, which means that you must use double-quotes (“) around words not single-quotes (‘).</p> <p><strong>Note</strong>: Unlike the <em>shell</em> form, the <em>exec</em> form does not invoke a command shell. This means that normal shell processing does not happen. For example, <code>CMD [ \"echo\", \"$HOME\" ]</code> will not do variable substitution on <code>$HOME</code>. If you want shell processing then either use the <em>shell</em> form or execute a shell directly, for example: <code>CMD [ \"sh\", \"-c\", \"echo\", \"$HOME\" ]</code>.</p> </blockquote> <p>When used in the shell or exec formats, the <code>CMD</code> instruction sets the command to be executed when running the image.</p> <p>If you use the <em>shell</em> form of the <code>CMD</code>, then the <code>&lt;command&gt;</code> will execute in <code>/bin/sh -c</code>:</p> <pre>FROM ubuntu\nCMD echo \"This is a test.\" | wc -\n</pre> <p>If you want to <strong>run your</strong> <code>&lt;command&gt;</code> <strong>without a shell</strong> then you must express the command as a JSON array and give the full path to the executable. <strong>This array form is the preferred format of <code>CMD</code>.</strong> Any additional parameters must be individually expressed as strings in the array:</p> <pre>FROM ubuntu\nCMD [\"/usr/bin/wc\",\"--help\"]\n</pre> <p>If you would like your container to run the same executable every time, then you should consider using <code>ENTRYPOINT</code> in combination with <code>CMD</code>. See <a href=\"#entrypoint\"><em>ENTRYPOINT</em></a>.</p> <p>If the user specifies arguments to <code>docker run</code> then they will override the default specified in <code>CMD</code>.</p> <blockquote> <p><strong>Note</strong>: don’t confuse <code>RUN</code> with <code>CMD</code>. <code>RUN</code> actually runs a command and commits the result; <code>CMD</code> does not execute anything at build time, but specifies the intended command for the image.</p> </blockquote> <h2 id=\"label\">LABEL</h2> <pre>LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...\n</pre> <p>The <code>LABEL</code> instruction adds metadata to an image. A <code>LABEL</code> is a key-value pair. To include spaces within a <code>LABEL</code> value, use quotes and backslashes as you would in command-line parsing. A few usage examples:</p> <pre>LABEL \"com.example.vendor\"=\"ACME Incorporated\"\nLABEL com.example.label-with-value=\"foo\"\nLABEL version=\"1.0\"\nLABEL description=\"This text illustrates \\\nthat label-values can span multiple lines.\"\n</pre> <p>An image can have more than one label. To specify multiple labels, Docker recommends combining labels into a single <code>LABEL</code> instruction where possible. Each <code>LABEL</code> instruction produces a new layer which can result in an inefficient image if you use many labels. This example results in a single image layer.</p> <pre>LABEL multi.label1=\"value1\" multi.label2=\"value2\" other=\"value3\"\n</pre> <p>The above can also be written as:</p> <pre>LABEL multi.label1=\"value1\" \\\n      multi.label2=\"value2\" \\\n      other=\"value3\"\n</pre> <p>Labels are additive including <code>LABEL</code>s in <code>FROM</code> images. If Docker encounters a label/key that already exists, the new value overrides any previous labels with identical keys.</p> <p>To view an image’s labels, use the <code>docker inspect</code> command.</p> <pre>\"Labels\": {\n    \"com.example.vendor\": \"ACME Incorporated\"\n    \"com.example.label-with-value\": \"foo\",\n    \"version\": \"1.0\",\n    \"description\": \"This text illustrates that label-values can span multiple lines.\",\n    \"multi.label1\": \"value1\",\n    \"multi.label2\": \"value2\",\n    \"other\": \"value3\"\n},\n</pre> <h2 id=\"expose\">EXPOSE</h2> <pre>EXPOSE &lt;port&gt; [&lt;port&gt;...]\n</pre> <p>The <code>EXPOSE</code> instruction informs Docker that the container listens on the specified network ports at runtime. <code>EXPOSE</code> does not make the ports of the container accessible to the host. To do that, you must use either the <code>-p</code> flag to publish a range of ports or the <code>-P</code> flag to publish all of the exposed ports. You can expose one port number and publish it externally under another number.</p> <p>To set up port redirection on the host system, see <a href=\"../run/index#expose-incoming-ports\">using the -P flag</a>. The Docker network feature supports creating networks without the need to expose ports within the network, for detailed information see the <a href=\"../../userguide/networking/index\">overview of this feature</a>).</p> <h2 id=\"env\">ENV</h2> <pre>ENV &lt;key&gt; &lt;value&gt;\nENV &lt;key&gt;=&lt;value&gt; ...\n</pre> <p>The <code>ENV</code> instruction sets the environment variable <code>&lt;key&gt;</code> to the value <code>&lt;value&gt;</code>. This value will be in the environment of all “descendant” <code>Dockerfile</code> commands and can be <a href=\"#environment-replacement\">replaced inline</a> in many as well.</p> <p>The <code>ENV</code> instruction has two forms. The first form, <code>ENV &lt;key&gt; &lt;value&gt;</code>, will set a single variable to a value. The entire string after the first space will be treated as the <code>&lt;value&gt;</code> - including characters such as spaces and quotes.</p> <p>The second form, <code>ENV &lt;key&gt;=&lt;value&gt; ...</code>, allows for multiple variables to be set at one time. Notice that the second form uses the equals sign (=) in the syntax, while the first form does not. Like command line parsing, quotes and backslashes can be used to include spaces within values.</p> <p>For example:</p> <pre>ENV myName=\"John Doe\" myDog=Rex\\ The\\ Dog \\\n    myCat=fluffy\n</pre> <p>and</p> <pre>ENV myName John Doe\nENV myDog Rex The Dog\nENV myCat fluffy\n</pre> <p>will yield the same net results in the final container, but the first form is preferred because it produces a single cache layer.</p> <p>The environment variables set using <code>ENV</code> will persist when a container is run from the resulting image. You can view the values using <code>docker inspect</code>, and change them using <code>docker run --env &lt;key&gt;=&lt;value&gt;</code>.</p> <blockquote> <p><strong>Note</strong>: Environment persistence can cause unexpected side effects. For example, setting <code>ENV DEBIAN_FRONTEND noninteractive</code> may confuse apt-get users on a Debian-based image. To set a value for a single command, use <code>RUN &lt;key&gt;=&lt;value&gt; &lt;command&gt;</code>.</p> </blockquote> <h2 id=\"add\">ADD</h2> <p>ADD has two forms:</p> <ul> <li><code>ADD &lt;src&gt;... &lt;dest&gt;</code></li> <li>\n<code>ADD [\"&lt;src&gt;\",... \"&lt;dest&gt;\"]</code> (this form is required for paths containing whitespace)</li> </ul> <p>The <code>ADD</code> instruction copies new files, directories or remote file URLs from <code>&lt;src&gt;</code> and adds them to the filesystem of the container at the path <code>&lt;dest&gt;</code>.</p> <p>Multiple <code>&lt;src&gt;</code> resource may be specified but if they are files or directories then they must be relative to the source directory that is being built (the context of the build).</p> <p>Each <code>&lt;src&gt;</code> may contain wildcards and matching will be done using Go’s <a href=\"http://golang.org/pkg/path/filepath#Match\">filepath.Match</a> rules. For example:</p> <pre>ADD hom* /mydir/        # adds all files starting with \"hom\"\nADD hom?.txt /mydir/    # ? is replaced with any single character, e.g., \"home.txt\"\n</pre> <p>The <code>&lt;dest&gt;</code> is an absolute path, or a path relative to <code>WORKDIR</code>, into which the source will be copied inside the destination container.</p> <pre>ADD test relativeDir/          # adds \"test\" to `WORKDIR`/relativeDir/\nADD test /absoluteDir/         # adds \"test\" to /absoluteDir/\n</pre> <p>All new files and directories are created with a UID and GID of 0.</p> <p>In the case where <code>&lt;src&gt;</code> is a remote file URL, the destination will have permissions of 600. If the remote file being retrieved has an HTTP <code>Last-Modified</code> header, the timestamp from that header will be used to set the <code>mtime</code> on the destination file. However, like any other file processed during an <code>ADD</code>, <code>mtime</code> will not be included in the determination of whether or not the file has changed and the cache should be updated.</p> <blockquote> <p><strong>Note</strong>: If you build by passing a <code>Dockerfile</code> through STDIN (<code>docker\nbuild - &lt; somefile</code>), there is no build context, so the <code>Dockerfile</code> can only contain a URL based <code>ADD</code> instruction. You can also pass a compressed archive through STDIN: (<code>docker build - &lt; archive.tar.gz</code>), the <code>Dockerfile</code> at the root of the archive and the rest of the archive will get used at the context of the build.</p> <p><strong>Note</strong>: If your URL files are protected using authentication, you will need to use <code>RUN wget</code>, <code>RUN curl</code> or use another tool from within the container as the <code>ADD</code> instruction does not support authentication.</p> <p><strong>Note</strong>: The first encountered <code>ADD</code> instruction will invalidate the cache for all following instructions from the Dockerfile if the contents of <code>&lt;src&gt;</code> have changed. This includes invalidating the cache for <code>RUN</code> instructions. See the <a href=\"../../userguide/eng-image/dockerfile_best-practices/index#build-cache\"><code>Dockerfile</code> Best Practices guide</a> for more information.</p> </blockquote> <p><code>ADD</code> obeys the following rules:</p> <ul> <li><p>The <code>&lt;src&gt;</code> path must be inside the <em>context</em> of the build; you cannot <code>ADD ../something /something</code>, because the first step of a <code>docker build</code> is to send the context directory (and subdirectories) to the docker daemon.</p></li> <li><p>If <code>&lt;src&gt;</code> is a URL and <code>&lt;dest&gt;</code> does not end with a trailing slash, then a file is downloaded from the URL and copied to <code>&lt;dest&gt;</code>.</p></li> <li><p>If <code>&lt;src&gt;</code> is a URL and <code>&lt;dest&gt;</code> does end with a trailing slash, then the filename is inferred from the URL and the file is downloaded to <code>&lt;dest&gt;/&lt;filename&gt;</code>. For instance, <code>ADD http://example.com/foobar /</code> would create the file <code>/foobar</code>. The URL must have a nontrivial path so that an appropriate filename can be discovered in this case (<code>http://example.com</code> will not work).</p></li> <li><p>If <code>&lt;src&gt;</code> is a directory, the entire contents of the directory are copied, including filesystem metadata.</p></li> </ul> <blockquote> <p><strong>Note</strong>: The directory itself is not copied, just its contents.</p> </blockquote> <ul> <li>\n<p>If <code>&lt;src&gt;</code> is a <em>local</em> tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from <em>remote</em> URLs are <strong>not</strong> decompressed. When a directory is copied or unpacked, it has the same behavior as <code>tar -x</code>: the result is the union of:</p> <ol> <li>Whatever existed at the destination path and</li> <li>The contents of the source tree, with conflicts resolved in favor of “2.” on a file-by-file basis.</li> </ol>\n</li> </ul> <blockquote> <p><strong>Note</strong>: Whether a file is identified as a recognized compression format or not is done solely based on the contents of the file, not the name of the file. For example, if an empty file happens to end with <code>.tar.gz</code> this will not be recognized as a compressed file and <strong>will not</strong> generate any kind of decompression error message, rather the file will simply be copied to the destination.</p> </blockquote> <ul> <li><p>If <code>&lt;src&gt;</code> is any other kind of file, it is copied individually along with its metadata. In this case, if <code>&lt;dest&gt;</code> ends with a trailing slash <code>/</code>, it will be considered a directory and the contents of <code>&lt;src&gt;</code> will be written at <code>&lt;dest&gt;/base(&lt;src&gt;)</code>.</p></li> <li><p>If multiple <code>&lt;src&gt;</code> resources are specified, either directly or due to the use of a wildcard, then <code>&lt;dest&gt;</code> must be a directory, and it must end with a slash <code>/</code>.</p></li> <li><p>If <code>&lt;dest&gt;</code> does not end with a trailing slash, it will be considered a regular file and the contents of <code>&lt;src&gt;</code> will be written at <code>&lt;dest&gt;</code>.</p></li> <li><p>If <code>&lt;dest&gt;</code> doesn’t exist, it is created along with all missing directories in its path.</p></li> </ul> <h2 id=\"copy\">COPY</h2> <p>COPY has two forms:</p> <ul> <li><code>COPY &lt;src&gt;... &lt;dest&gt;</code></li> <li>\n<code>COPY [\"&lt;src&gt;\",... \"&lt;dest&gt;\"]</code> (this form is required for paths containing whitespace)</li> </ul> <p>The <code>COPY</code> instruction copies new files or directories from <code>&lt;src&gt;</code> and adds them to the filesystem of the container at the path <code>&lt;dest&gt;</code>.</p> <p>Multiple <code>&lt;src&gt;</code> resource may be specified but they must be relative to the source directory that is being built (the context of the build).</p> <p>Each <code>&lt;src&gt;</code> may contain wildcards and matching will be done using Go’s <a href=\"http://golang.org/pkg/path/filepath#Match\">filepath.Match</a> rules. For example:</p> <pre>COPY hom* /mydir/        # adds all files starting with \"hom\"\nCOPY hom?.txt /mydir/    # ? is replaced with any single character, e.g., \"home.txt\"\n</pre> <p>The <code>&lt;dest&gt;</code> is an absolute path, or a path relative to <code>WORKDIR</code>, into which the source will be copied inside the destination container.</p> <pre>COPY test relativeDir/   # adds \"test\" to `WORKDIR`/relativeDir/\nCOPY test /absoluteDir/  # adds \"test\" to /absoluteDir/\n</pre> <p>All new files and directories are created with a UID and GID of 0.</p> <blockquote> <p><strong>Note</strong>: If you build using STDIN (<code>docker build - &lt; somefile</code>), there is no build context, so <code>COPY</code> can’t be used.</p> </blockquote> <p><code>COPY</code> obeys the following rules:</p> <ul> <li><p>The <code>&lt;src&gt;</code> path must be inside the <em>context</em> of the build; you cannot <code>COPY ../something /something</code>, because the first step of a <code>docker build</code> is to send the context directory (and subdirectories) to the docker daemon.</p></li> <li><p>If <code>&lt;src&gt;</code> is a directory, the entire contents of the directory are copied, including filesystem metadata.</p></li> </ul> <blockquote> <p><strong>Note</strong>: The directory itself is not copied, just its contents.</p> </blockquote> <ul> <li><p>If <code>&lt;src&gt;</code> is any other kind of file, it is copied individually along with its metadata. In this case, if <code>&lt;dest&gt;</code> ends with a trailing slash <code>/</code>, it will be considered a directory and the contents of <code>&lt;src&gt;</code> will be written at <code>&lt;dest&gt;/base(&lt;src&gt;)</code>.</p></li> <li><p>If multiple <code>&lt;src&gt;</code> resources are specified, either directly or due to the use of a wildcard, then <code>&lt;dest&gt;</code> must be a directory, and it must end with a slash <code>/</code>.</p></li> <li><p>If <code>&lt;dest&gt;</code> does not end with a trailing slash, it will be considered a regular file and the contents of <code>&lt;src&gt;</code> will be written at <code>&lt;dest&gt;</code>.</p></li> <li><p>If <code>&lt;dest&gt;</code> doesn’t exist, it is created along with all missing directories in its path.</p></li> </ul> <h2 id=\"entrypoint\">ENTRYPOINT</h2> <p>ENTRYPOINT has two forms:</p> <ul> <li>\n<code>ENTRYPOINT [\"executable\", \"param1\", \"param2\"]</code> (<em>exec</em> form, preferred)</li> <li>\n<code>ENTRYPOINT command param1 param2</code> (<em>shell</em> form)</li> </ul> <p>An <code>ENTRYPOINT</code> allows you to configure a container that will run as an executable.</p> <p>For example, the following will start nginx with its default content, listening on port 80:</p> <pre>docker run -i -t --rm -p 80:80 nginx\n</pre> <p>Command line arguments to <code>docker run &lt;image&gt;</code> will be appended after all elements in an <em>exec</em> form <code>ENTRYPOINT</code>, and will override all elements specified using <code>CMD</code>. This allows arguments to be passed to the entry point, i.e., <code>docker run &lt;image&gt; -d</code> will pass the <code>-d</code> argument to the entry point. You can override the <code>ENTRYPOINT</code> instruction using the <code>docker run --entrypoint</code> flag.</p> <p>The <em>shell</em> form prevents any <code>CMD</code> or <code>run</code> command line arguments from being used, but has the disadvantage that your <code>ENTRYPOINT</code> will be started as a subcommand of <code>/bin/sh -c</code>, which does not pass signals. This means that the executable will not be the container’s <code>PID 1</code> - and will <em>not</em> receive Unix signals - so your executable will not receive a <code>SIGTERM</code> from <code>docker stop &lt;container&gt;</code>.</p> <p>Only the last <code>ENTRYPOINT</code> instruction in the <code>Dockerfile</code> will have an effect.</p> <h3 id=\"exec-form-entrypoint-example\">Exec form ENTRYPOINT example</h3> <p>You can use the <em>exec</em> form of <code>ENTRYPOINT</code> to set fairly stable default commands and arguments and then use either form of <code>CMD</code> to set additional defaults that are more likely to be changed.</p> <pre>FROM ubuntu\nENTRYPOINT [\"top\", \"-b\"]\nCMD [\"-c\"]\n</pre> <p>When you run the container, you can see that <code>top</code> is the only process:</p> <pre>$ docker run -it --rm --name test  top -H\ntop - 08:25:00 up  7:27,  0 users,  load average: 0.00, 0.01, 0.05\nThreads:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem:   2056668 total,  1616832 used,   439836 free,    99352 buffers\nKiB Swap:  1441840 total,        0 used,  1441840 free.  1324440 cached Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\n    1 root      20   0   19744   2336   2080 R  0.0  0.1   0:00.04 top\n</pre> <p>To examine the result further, you can use <code>docker exec</code>:</p> <pre>$ docker exec -it test ps aux\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  2.6  0.1  19752  2352 ?        Ss+  08:24   0:00 top -b -H\nroot         7  0.0  0.1  15572  2164 ?        R+   08:25   0:00 ps aux\n</pre> <p>And you can gracefully request <code>top</code> to shut down using <code>docker stop test</code>.</p> <p>The following <code>Dockerfile</code> shows using the <code>ENTRYPOINT</code> to run Apache in the foreground (i.e., as <code>PID 1</code>):</p> <pre>FROM debian:stable\nRUN apt-get update &amp;&amp; apt-get install -y --force-yes apache2\nEXPOSE 80 443\nVOLUME [\"/var/www\", \"/var/log/apache2\", \"/etc/apache2\"]\nENTRYPOINT [\"/usr/sbin/apache2ctl\", \"-D\", \"FOREGROUND\"]\n</pre> <p>If you need to write a starter script for a single executable, you can ensure that the final executable receives the Unix signals by using <code>exec</code> and <code>gosu</code> commands:</p> <pre>#!/bin/bash\nset -e\n\nif [ \"$1\" = 'postgres' ]; then\n    chown -R postgres \"$PGDATA\"\n\n    if [ -z \"$(ls -A \"$PGDATA\")\" ]; then\n        gosu postgres initdb\n    fi\n\n    exec gosu postgres \"$@\"\nfi\n\nexec \"$@\"\n</pre> <p>Lastly, if you need to do some extra cleanup (or communicate with other containers) on shutdown, or are co-ordinating more than one executable, you may need to ensure that the <code>ENTRYPOINT</code> script receives the Unix signals, passes them on, and then does some more work:</p> <pre>#!/bin/sh\n# Note: I've written this using sh so it works in the busybox container too\n\n# USE the trap if you need to also do manual cleanup after the service is stopped,\n#     or need to start multiple services in the one container\ntrap \"echo TRAPed signal\" HUP INT QUIT TERM\n\n# start service in background here\n/usr/sbin/apachectl start\n\necho \"[hit enter key to exit] or run 'docker stop &lt;container&gt;'\"\nread\n\n# stop service and clean up here\necho \"stopping apache\"\n/usr/sbin/apachectl stop\n\necho \"exited $0\"\n</pre> <p>If you run this image with <code>docker run -it --rm -p 80:80 --name test apache</code>, you can then examine the container’s processes with <code>docker exec</code>, or <code>docker top</code>, and then ask the script to stop Apache:</p> <pre>$ docker exec -it test ps aux\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.1  0.0   4448   692 ?        Ss+  00:42   0:00 /bin/sh /run.sh 123 cmd cmd2\nroot        19  0.0  0.2  71304  4440 ?        Ss   00:42   0:00 /usr/sbin/apache2 -k start\nwww-data    20  0.2  0.2 360468  6004 ?        Sl   00:42   0:00 /usr/sbin/apache2 -k start\nwww-data    21  0.2  0.2 360468  6000 ?        Sl   00:42   0:00 /usr/sbin/apache2 -k start\nroot        81  0.0  0.1  15572  2140 ?        R+   00:44   0:00 ps aux\n$ docker top test\nPID                 USER                COMMAND\n10035               root                {run.sh} /bin/sh /run.sh 123 cmd cmd2\n10054               root                /usr/sbin/apache2 -k start\n10055               33                  /usr/sbin/apache2 -k start\n10056               33                  /usr/sbin/apache2 -k start\n$ /usr/bin/time docker stop test\ntest\nreal\t0m 0.27s\nuser\t0m 0.03s\nsys\t0m 0.03s\n</pre> <blockquote> <p><strong>Note:</strong> you can over ride the <code>ENTRYPOINT</code> setting using <code>--entrypoint</code>, but this can only set the binary to <em>exec</em> (no <code>sh -c</code> will be used).</p> <p><strong>Note</strong>: The <em>exec</em> form is parsed as a JSON array, which means that you must use double-quotes (“) around words not single-quotes (‘).</p> <p><strong>Note</strong>: Unlike the <em>shell</em> form, the <em>exec</em> form does not invoke a command shell. This means that normal shell processing does not happen. For example, <code>ENTRYPOINT [ \"echo\", \"$HOME\" ]</code> will not do variable substitution on <code>$HOME</code>. If you want shell processing then either use the <em>shell</em> form or execute a shell directly, for example: <code>ENTRYPOINT [ \"sh\", \"-c\", \"echo\", \"$HOME\" ]</code>. Variables that are defined in the <code>Dockerfile</code>using <code>ENV</code>, will be substituted by the <code>Dockerfile</code> parser.</p> </blockquote> <h3 id=\"shell-form-entrypoint-example\">Shell form ENTRYPOINT example</h3> <p>You can specify a plain string for the <code>ENTRYPOINT</code> and it will execute in <code>/bin/sh -c</code>. This form will use shell processing to substitute shell environment variables, and will ignore any <code>CMD</code> or <code>docker run</code> command line arguments. To ensure that <code>docker stop</code> will signal any long running <code>ENTRYPOINT</code> executable correctly, you need to remember to start it with <code>exec</code>:</p> <pre>FROM ubuntu\nENTRYPOINT exec top -b\n</pre> <p>When you run this image, you’ll see the single <code>PID 1</code> process:</p> <pre>$ docker run -it --rm --name test top\nMem: 1704520K used, 352148K free, 0K shrd, 0K buff, 140368121167873K cached\nCPU:   5% usr   0% sys   0% nic  94% idle   0% io   0% irq   0% sirq\nLoad average: 0.08 0.03 0.05 2/98 6\n  PID  PPID USER     STAT   VSZ %VSZ %CPU COMMAND\n    1     0 root     R     3164   0%   0% top -b\n</pre> <p>Which will exit cleanly on <code>docker stop</code>:</p> <pre>$ /usr/bin/time docker stop test\ntest\nreal    0m 0.20s\nuser    0m 0.02s\nsys 0m 0.04s\n</pre> <p>If you forget to add <code>exec</code> to the beginning of your <code>ENTRYPOINT</code>:</p> <pre>FROM ubuntu\nENTRYPOINT top -b\nCMD --ignored-param1\n</pre> <p>You can then run it (giving it a name for the next step):</p> <pre>$ docker run -it --name test top --ignored-param2\nMem: 1704184K used, 352484K free, 0K shrd, 0K buff, 140621524238337K cached\nCPU:   9% usr   2% sys   0% nic  88% idle   0% io   0% irq   0% sirq\nLoad average: 0.01 0.02 0.05 2/101 7\n  PID  PPID USER     STAT   VSZ %VSZ %CPU COMMAND\n    1     0 root     S     3168   0%   0% /bin/sh -c top -b cmd cmd2\n    7     1 root     R     3164   0%   0% top -b\n</pre> <p>You can see from the output of <code>top</code> that the specified <code>ENTRYPOINT</code> is not <code>PID 1</code>.</p> <p>If you then run <code>docker stop test</code>, the container will not exit cleanly - the <code>stop</code> command will be forced to send a <code>SIGKILL</code> after the timeout:</p> <pre>$ docker exec -it test ps aux\nPID   USER     COMMAND\n    1 root     /bin/sh -c top -b cmd cmd2\n    7 root     top -b\n    8 root     ps aux\n$ /usr/bin/time docker stop test\ntest\nreal    0m 10.19s\nuser    0m 0.04s\nsys 0m 0.03s\n</pre> <h3 id=\"understand-how-cmd-and-entrypoint-interact\">Understand how CMD and ENTRYPOINT interact</h3> <p>Both <code>CMD</code> and <code>ENTRYPOINT</code> instructions define what command gets executed when running a container. There are few rules that describe their co-operation.</p> <ol> <li><p>Dockerfile should specify at least one of <code>CMD</code> or <code>ENTRYPOINT</code> commands.</p></li> <li><p><code>ENTRYPOINT</code> should be defined when using the container as an executable.</p></li> <li><p><code>CMD</code> should be used as a way of defining default arguments for an <code>ENTRYPOINT</code> command or for executing an ad-hoc command in a container.</p></li> <li><p><code>CMD</code> will be overridden when running the container with alternative arguments.</p></li> </ol> <p>The table below shows what command is executed for different <code>ENTRYPOINT</code> / <code>CMD</code> combinations:</p> <table> <thead> <tr> <th></th> <th>No ENTRYPOINT</th> <th>ENTRYPOINT exec_entry p1_entry</th> <th>ENTRYPOINT [“exec_entry”, “p1_entry”]</th> </tr> </thead> <tbody> <tr> <td><strong>No CMD</strong></td> <td><em>error, not allowed</em></td> <td>/bin/sh -c exec_entry p1_entry</td> <td>exec_entry p1_entry</td> </tr> <tr> <td><strong>CMD [“exec_cmd”, “p1_cmd”]</strong></td> <td>exec_cmd p1_cmd</td> <td>/bin/sh -c exec_entry p1_entry exec_cmd p1_cmd</td> <td>exec_entry p1_entry exec_cmd p1_cmd</td> </tr> <tr> <td><strong>CMD [“p1_cmd”, “p2_cmd”]</strong></td> <td>p1_cmd p2_cmd</td> <td>/bin/sh -c exec_entry p1_entry p1_cmd p2_cmd</td> <td>exec_entry p1_entry p1_cmd p2_cmd</td> </tr> <tr> <td><strong>CMD exec_cmd p1_cmd</strong></td> <td>/bin/sh -c exec_cmd p1_cmd</td> <td>/bin/sh -c exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd</td> <td>exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd</td> </tr> </tbody> </table> <h2 id=\"volume\">VOLUME</h2> <pre>VOLUME [\"/data\"]\n</pre> <p>The <code>VOLUME</code> instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. The value can be a JSON array, <code>VOLUME [\"/var/log/\"]</code>, or a plain string with multiple arguments, such as <code>VOLUME /var/log</code> or <code>VOLUME /var/log\n/var/db</code>. For more information/examples and mounting instructions via the Docker client, refer to <a href=\"../../userguide/containers/dockervolumes/index#mount-a-host-directory-as-a-data-volume\"><em>Share Directories via Volumes</em></a> documentation.</p> <p>The <code>docker run</code> command initializes the newly created volume with any data that exists at the specified location within the base image. For example, consider the following Dockerfile snippet:</p> <pre>FROM ubuntu\nRUN mkdir /myvol\nRUN echo \"hello world\" &gt; /myvol/greeting\nVOLUME /myvol\n</pre> <p>This Dockerfile results in an image that causes <code>docker run</code>, to create a new mount point at <code>/myvol</code> and copy the <code>greeting</code> file into the newly created volume.</p> <blockquote> <p><strong>Note</strong>: If any build steps change the data within the volume after it has been declared, those changes will be discarded.</p> <p><strong>Note</strong>: The list is parsed as a JSON array, which means that you must use double-quotes (“) around words not single-quotes (‘).</p> </blockquote> <h2 id=\"user\">USER</h2> <pre>USER daemon\n</pre> <p>The <code>USER</code> instruction sets the user name or UID to use when running the image and for any <code>RUN</code>, <code>CMD</code> and <code>ENTRYPOINT</code> instructions that follow it in the <code>Dockerfile</code>.</p> <h2 id=\"workdir\">WORKDIR</h2> <pre>WORKDIR /path/to/workdir\n</pre> <p>The <code>WORKDIR</code> instruction sets the working directory for any <code>RUN</code>, <code>CMD</code>, <code>ENTRYPOINT</code>, <code>COPY</code> and <code>ADD</code> instructions that follow it in the <code>Dockerfile</code>. If the <code>WORKDIR</code> doesn’t exist, it will be created even if its not used in any subsequent <code>Dockerfile</code> instruction.</p> <p>It can be used multiple times in the one <code>Dockerfile</code>. If a relative path is provided, it will be relative to the path of the previous <code>WORKDIR</code> instruction. For example:</p> <pre>WORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n</pre> <p>The output of the final <code>pwd</code> command in this <code>Dockerfile</code> would be <code>/a/b/c</code>.</p> <p>The <code>WORKDIR</code> instruction can resolve environment variables previously set using <code>ENV</code>. You can only use environment variables explicitly set in the <code>Dockerfile</code>. For example:</p> <pre>ENV DIRPATH /path\nWORKDIR $DIRPATH/$DIRNAME\nRUN pwd\n</pre> <p>The output of the final <code>pwd</code> command in this <code>Dockerfile</code> would be <code>/path/$DIRNAME</code></p> <h2 id=\"arg\">ARG</h2> <pre>ARG &lt;name&gt;[=&lt;default value&gt;]\n</pre> <p>The <code>ARG</code> instruction defines a variable that users can pass at build-time to the builder with the <code>docker build</code> command using the <code>--build-arg\n&lt;varname&gt;=&lt;value&gt;</code> flag. If a user specifies a build argument that was not defined in the Dockerfile, the build outputs an error.</p> <pre>One or more build-args were not consumed, failing build.\n</pre> <p>The Dockerfile author can define a single variable by specifying <code>ARG</code> once or many variables by specifying <code>ARG</code> more than once. For example, a valid Dockerfile:</p> <pre>FROM busybox\nARG user1\nARG buildno\n...\n</pre> <p>A Dockerfile author may optionally specify a default value for an <code>ARG</code> instruction:</p> <pre>FROM busybox\nARG user1=someuser\nARG buildno=1\n...\n</pre> <p>If an <code>ARG</code> value has a default and if there is no value passed at build-time, the builder uses the default.</p> <p>An <code>ARG</code> variable definition comes into effect from the line on which it is defined in the <code>Dockerfile</code> not from the argument’s use on the command-line or elsewhere. For example, consider this Dockerfile:</p> <pre>1 FROM busybox\n2 USER ${user:-some_user}\n3 ARG user\n4 USER $user\n...\n</pre> <p>A user builds this file by calling:</p> <pre>$ docker build --build-arg user=what_user Dockerfile\n</pre> <p>The <code>USER</code> at line 2 evaluates to <code>some_user</code> as the <code>user</code> variable is defined on the subsequent line 3. The <code>USER</code> at line 4 evaluates to <code>what_user</code> as <code>user</code> is defined and the <code>what_user</code> value was passed on the command line. Prior to its definition by an <code>ARG</code> instruction, any use of a variable results in an empty string.</p> <blockquote> <p><strong>Note:</strong> It is not recommended to use build-time variables for passing secrets like github keys, user credentials etc.</p> </blockquote> <p>You can use an <code>ARG</code> or an <code>ENV</code> instruction to specify variables that are available to the <code>RUN</code> instruction. Environment variables defined using the <code>ENV</code> instruction always override an <code>ARG</code> instruction of the same name. Consider this Dockerfile with an <code>ENV</code> and <code>ARG</code> instruction.</p> <pre>1 FROM ubuntu\n2 ARG CONT_IMG_VER\n3 ENV CONT_IMG_VER v1.0.0\n4 RUN echo $CONT_IMG_VER\n</pre> <p>Then, assume this image is built with this command:</p> <pre>$ docker build --build-arg CONT_IMG_VER=v2.0.1 Dockerfile\n</pre> <p>In this case, the <code>RUN</code> instruction uses <code>v1.0.0</code> instead of the <code>ARG</code> setting passed by the user:<code>v2.0.1</code> This behavior is similar to a shell script where a locally scoped variable overrides the variables passed as arguments or inherited from environment, from its point of definition.</p> <p>Using the example above but a different <code>ENV</code> specification you can create more useful interactions between <code>ARG</code> and <code>ENV</code> instructions:</p> <pre>1 FROM ubuntu\n2 ARG CONT_IMG_VER\n3 ENV CONT_IMG_VER ${CONT_IMG_VER:-v1.0.0}\n4 RUN echo $CONT_IMG_VER\n</pre> <p>Unlike an <code>ARG</code> instruction, <code>ENV</code> values are always persisted in the built image. Consider a docker build without the --build-arg flag:</p> <pre>$ docker build Dockerfile\n</pre> <p>Using this Dockerfile example, <code>CONT_IMG_VER</code> is still persisted in the image but its value would be <code>v1.0.0</code> as it is the default set in line 3 by the <code>ENV</code> instruction.</p> <p>The variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the <code>ENV</code> instruction. Variable expansion is only supported for <a href=\"#environment-replacement\">a limited set of Dockerfile instructions.</a></p> <p>Docker has a set of predefined <code>ARG</code> variables that you can use without a corresponding <code>ARG</code> instruction in the Dockerfile.</p> <ul> <li><code>HTTP_PROXY</code></li> <li><code>http_proxy</code></li> <li><code>HTTPS_PROXY</code></li> <li><code>https_proxy</code></li> <li><code>FTP_PROXY</code></li> <li><code>ftp_proxy</code></li> <li><code>NO_PROXY</code></li> <li><code>no_proxy</code></li> </ul> <p>To use these, simply pass them on the command line using the <code>--build-arg\n&lt;varname&gt;=&lt;value&gt;</code> flag.</p> <h3 id=\"impact-on-build-caching\">Impact on build caching</h3> <p><code>ARG</code> variables are not persisted into the built image as <code>ENV</code> variables are. However, <code>ARG</code> variables do impact the build cache in similar ways. If a Dockerfile defines an <code>ARG</code> variable whose value is different from a previous build, then a “cache miss” occurs upon first use of the <code>ARG</code> variable. The declaration of the <code>ARG</code> variable does not count as a use.</p> <p>For example, consider these two Dockerfile:</p> <pre>1 FROM ubuntu\n2 ARG CONT_IMG_VER\n3 RUN echo $CONT_IMG_VER\n</pre> <pre>1 FROM ubuntu\n2 ARG CONT_IMG_VER\n3 RUN echo hello\n</pre> <p>If you specify <code>--build-arg CONT_IMG_VER=&lt;value&gt;</code> on the command line, in both cases, the specification on line 2 does not cause a cache miss; line 3 does cause a cache miss.<code>ARG CONT_IMG_VER</code> causes the RUN line to be identified as the same as running <code>CONT_IMG_VER=&lt;value&gt;</code> echo hello, so if the <code>&lt;value&gt;</code> changes, we get a cache miss.</p> <p>Consider another example under the same command line:</p> <pre>1 FROM ubuntu\n2 ARG CONT_IMG_VER\n3 ENV CONT_IMG_VER $CONT_IMG_VER\n4 RUN echo $CONT_IMG_VER\n</pre> <p>In this example, the cache miss occurs on line 3. The miss happens because the variable’s value in the <code>ENV</code> references the <code>ARG</code> variable and that variable is changed through the command line. In this example, the <code>ENV</code> command causes the image to include the value.</p> <p>If an <code>ENV</code> instruction overrides an <code>ARG</code> instruction of the same name, like this Dockerfile:</p> <pre>1 FROM ubuntu\n2 ARG CONT_IMG_VER\n3 ENV CONT_IMG_VER hello\n4 RUN echo $CONT_IMG_VER\n</pre> <p>Line 3 does not cause a cache miss because the value of <code>CONT_IMG_VER</code> is a constant (<code>hello</code>). As a result, the environment variables and values used on the <code>RUN</code> (line 4) doesn’t change between builds.</p> <h2 id=\"onbuild\">ONBUILD</h2> <pre>ONBUILD [INSTRUCTION]\n</pre> <p>The <code>ONBUILD</code> instruction adds to the image a <em>trigger</em> instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the <code>FROM</code> instruction in the downstream <code>Dockerfile</code>.</p> <p>Any build instruction can be registered as a trigger.</p> <p>This is useful if you are building an image which will be used as a base to build other images, for example an application build environment or a daemon which may be customized with user-specific configuration.</p> <p>For example, if your image is a reusable Python application builder, it will require application source code to be added in a particular directory, and it might require a build script to be called <em>after</em> that. You can’t just call <code>ADD</code> and <code>RUN</code> now, because you don’t yet have access to the application source code, and it will be different for each application build. You could simply provide application developers with a boilerplate <code>Dockerfile</code> to copy-paste into their application, but that is inefficient, error-prone and difficult to update because it mixes with application-specific code.</p> <p>The solution is to use <code>ONBUILD</code> to register advance instructions to run later, during the next build stage.</p> <p>Here’s how it works:</p> <ol> <li>When it encounters an <code>ONBUILD</code> instruction, the builder adds a trigger to the metadata of the image being built. The instruction does not otherwise affect the current build.</li> <li>At the end of the build, a list of all triggers is stored in the image manifest, under the key <code>OnBuild</code>. They can be inspected with the <code>docker inspect</code> command.</li> <li>Later the image may be used as a base for a new build, using the <code>FROM</code> instruction. As part of processing the <code>FROM</code> instruction, the downstream builder looks for <code>ONBUILD</code> triggers, and executes them in the same order they were registered. If any of the triggers fail, the <code>FROM</code> instruction is aborted which in turn causes the build to fail. If all triggers succeed, the <code>FROM</code> instruction completes and the build continues as usual.</li> <li>Triggers are cleared from the final image after being executed. In other words they are not inherited by “grand-children” builds.</li> </ol> <p>For example you might add something like this:</p> <pre>[...]\nONBUILD ADD . /app/src\nONBUILD RUN /usr/local/bin/python-build --dir /app/src\n[...]\n</pre> <blockquote> <p><strong>Warning</strong>: Chaining <code>ONBUILD</code> instructions using <code>ONBUILD ONBUILD</code> isn’t allowed.</p> <p><strong>Warning</strong>: The <code>ONBUILD</code> instruction may not trigger <code>FROM</code> or <code>MAINTAINER</code> instructions.</p> </blockquote> <h2 id=\"stopsignal\">STOPSIGNAL</h2> <pre>STOPSIGNAL signal\n</pre> <p>The <code>STOPSIGNAL</code> instruction sets the system call signal that will be sent to the container to exit. This signal can be a valid unsigned number that matches a position in the kernel’s syscall table, for instance 9, or a signal name in the format SIGNAME, for instance SIGKILL.</p> <h2 id=\"dockerfile-examples\">Dockerfile examples</h2> <p>Below you can see some examples of Dockerfile syntax. If you’re interested in something more realistic, take a look at the list of <a href=\"../../examples/index\">Dockerization examples</a>.</p> <pre># Nginx\n#\n# VERSION               0.0.1\n\nFROM      ubuntu\nMAINTAINER Victor Vieux &lt;victor@docker.com&gt;\n\nLABEL Description=\"This image is used to start the foobar executable\" Vendor=\"ACME Products\" Version=\"1.0\"\nRUN apt-get update &amp;&amp; apt-get install -y inotify-tools nginx apache2 openssh-server\n</pre> <pre># Firefox over VNC\n#\n# VERSION               0.3\n\nFROM ubuntu\n\n# Install vnc, xvfb in order to create a 'fake' display and firefox\nRUN apt-get update &amp;&amp; apt-get install -y x11vnc xvfb firefox\nRUN mkdir ~/.vnc\n# Setup a password\nRUN x11vnc -storepasswd 1234 ~/.vnc/passwd\n# Autostart firefox (might not be the best way, but it does the trick)\nRUN bash -c 'echo \"firefox\" &gt;&gt; /.bashrc'\n\nEXPOSE 5900\nCMD    [\"x11vnc\", \"-forever\", \"-usepw\", \"-create\"]\n</pre> <pre># Multiple images example\n#\n# VERSION               0.1\n\nFROM ubuntu\nRUN echo foo &gt; bar\n# Will output something like ===&gt; 907ad6c2736f\n\nFROM ubuntu\nRUN echo moo &gt; oink\n# Will output something like ===&gt; 695d7793cbe4\n\n# You᾿ll now have two images, 907ad6c2736f with /bar, and 695d7793cbe4 with\n# /oink.\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/builder/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/builder/</a>\n  </p>\n</div>\n","engine/reference/commandline/attach/index":"<h1 id=\"attach\">attach</h1> <pre>Usage: docker attach [OPTIONS] CONTAINER\n\nAttach to a running container\n\n  --detach-keys=\"&lt;sequence&gt;\"       Set up escape key sequence\n  --help                           Print usage\n  --no-stdin                       Do not attach STDIN\n  --sig-proxy=true                 Proxy all received signals to the process\n</pre> <p>The <code>docker attach</code> command allows you to attach to a running container using the container’s ID or name, either to view its ongoing output or to control it interactively. You can attach to the same contained process multiple times simultaneously, screen sharing style, or quickly view the progress of your detached process.</p> <p>To stop a container, use <code>CTRL-c</code>. This key sequence sends <code>SIGKILL</code> to the container. If <code>--sig-proxy</code> is true (the default),<code>CTRL-c</code> sends a <code>SIGINT</code> to the container. You can detach from a container and leave it running using the <code>CTRL-p CTRL-q</code> key sequence.</p> <blockquote> <p><strong>Note:</strong> A process running as PID 1 inside a container is treated specially by Linux: it ignores any signal with the default action. So, the process will not terminate on <code>SIGINT</code> or <code>SIGTERM</code> unless it is coded to do so.</p> </blockquote> <p>It is forbidden to redirect the standard input of a <code>docker attach</code> command while attaching to a tty-enabled container (i.e.: launched with <code>-t</code>).</p> <p>While a client is connected to container’s stdio using <code>docker attach</code>, Docker uses a ~1MB memory buffer to maximize the throughput of the application. If this buffer is filled, the speed of the API connection will start to have an effect on the process output writing speed. This is similar to other applications like SSH. Because of this, it is not recommended to run performance critical applications that generate a lot of output in the foreground over a slow client connection. Instead, users should use the <code>docker logs</code> command to get access to the logs.</p> <h2 id=\"override-the-detach-sequence\">Override the detach sequence</h2> <p>If you want, you can configure an override the Docker key sequence for detach. This is useful if the Docker default sequence conflicts with key sequence you use for other applications. There are two ways to defines a your own detach key sequence, as a per-container override or as a configuration property on your entire configuration.</p> <p>To override the sequence for an individual container, use the <code>--detach-keys=\"&lt;sequence&gt;\"</code> flag with the <code>docker attach</code> command. The format of the <code>&lt;sequence&gt;</code> is either a letter [a-Z], or the <code>ctrl-</code> combined with any of the following:</p> <ul> <li>\n<code>a-z</code> (a single lowercase alpha character )</li> <li>\n<code>@</code> (at sign)</li> <li>\n<code>[</code> (left bracket)</li> <li>\n<code>\\\\</code> (two backward slashes)</li> <li>\n<code>_</code> (underscore)</li> <li>\n<code>^</code> (caret)</li> </ul> <p>These <code>a</code>, <code>ctrl-a</code>, <code>X</code>, or <code>ctrl-\\\\</code> values are all examples of valid key sequences. To configure a different configuration default key sequence for all containers, see <a href=\"../cli/index#configuration-files\"><strong>Configuration file</strong> section</a>.</p> <h4 id=\"examples\">Examples</h4> <pre>$ docker run -d --name topdemo ubuntu /usr/bin/top -b\n$ docker attach topdemo\ntop - 02:05:52 up  3:05,  0 users,  load average: 0.01, 0.02, 0.05\nTasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\nCpu(s):  0.1%us,  0.2%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\nMem:    373572k total,   355560k used,    18012k free,    27872k buffers\nSwap:   786428k total,        0k used,   786428k free,   221740k cached\n\nPID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND\n 1 root      20   0 17200 1116  912 R    0  0.3   0:00.03 top\n\n top - 02:05:55 up  3:05,  0 users,  load average: 0.01, 0.02, 0.05\n Tasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\n Cpu(s):  0.0%us,  0.2%sy,  0.0%ni, 99.8%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n Mem:    373572k total,   355244k used,    18328k free,    27872k buffers\n Swap:   786428k total,        0k used,   786428k free,   221776k cached\n\n   PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND\n       1 root      20   0 17208 1144  932 R    0  0.3   0:00.03 top\n\n\n top - 02:05:58 up  3:06,  0 users,  load average: 0.01, 0.02, 0.05\n Tasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\n Cpu(s):  0.2%us,  0.3%sy,  0.0%ni, 99.5%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n Mem:    373572k total,   355780k used,    17792k free,    27880k buffers\n Swap:   786428k total,        0k used,   786428k free,   221776k cached\n\n PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND\n      1 root      20   0 17208 1144  932 R    0  0.3   0:00.03 top\n^C$\n$ echo $?\n0\n$ docker ps -a | grep topdemo\n7998ac8581f9        ubuntu:14.04        \"/usr/bin/top -b\"   38 seconds ago      Exited (0) 21 seconds ago                          topdemo\n</pre> <p>And in this second example, you can see the exit code returned by the <code>bash</code> process is returned by the <code>docker attach</code> command to its caller too:</p> <pre>$ docker run --name test -d -it debian\n275c44472aebd77c926d4527885bb09f2f6db21d878c75f0a1c212c03d3bcfab\n$ docker attach test\n$$ exit 13\nexit\n$ echo $?\n13\n$ docker ps -a | grep test\n275c44472aeb        debian:7            \"/bin/bash\"         26 seconds ago      Exited (13) 17 seconds ago                         test\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/attach/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/attach/</a>\n  </p>\n</div>\n","engine/reference/commandline/cp/index":"<h1 id=\"cp\">cp</h1> <pre>Usage: docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH | -\n       docker cp [OPTIONS] SRC_PATH | - CONTAINER:DEST_PATH\n\nCopy files/folders between a container and the local filesystem\n\n  -L, --follow-link          Always follow symbol link in SRC_PATH\n  --help                     Print usage\n</pre> <p>The <code>docker cp</code> utility copies the contents of <code>SRC_PATH</code> to the <code>DEST_PATH</code>. You can copy from the container’s file system to the local machine or the reverse, from the local filesystem to the container. If <code>-</code> is specified for either the <code>SRC_PATH</code> or <code>DEST_PATH</code>, you can also stream a tar archive from <code>STDIN</code> or to <code>STDOUT</code>. The <code>CONTAINER</code> can be a running or stopped container. The <code>SRC_PATH</code> or <code>DEST_PATH</code> can be a file or directory.</p> <p>The <code>docker cp</code> command assumes container paths are relative to the container’s <code>/</code> (root) directory. This means supplying the initial forward slash is optional; The command sees <code>compassionate_darwin:/tmp/foo/myfile.txt</code> and <code>compassionate_darwin:tmp/foo/myfile.txt</code> as identical. Local machine paths can be an absolute or relative value. The command interprets a local machine’s relative paths as relative to the current working directory where <code>docker cp</code> is run.</p> <p>The <code>cp</code> command behaves like the Unix <code>cp -a</code> command in that directories are copied recursively with permissions preserved if possible. Ownership is set to the user and primary group at the destination. For example, files copied to a container are created with <code>UID:GID</code> of the root user. Files copied to the local machine are created with the <code>UID:GID</code> of the user which invoked the <code>docker cp</code> command. If you specify the <code>-L</code> option, <code>docker cp</code> follows any symbolic link in the <code>SRC_PATH</code>. <code>docker cp</code> does <em>not</em> create parent directories for <code>DEST_PATH</code> if they do not exist.</p> <p>Assuming a path separator of <code>/</code>, a first argument of <code>SRC_PATH</code> and second argument of <code>DEST_PATH</code>, the behavior is as follows:</p> <ul> <li>\n<code>SRC_PATH</code> specifies a file <ul> <li>\n<code>DEST_PATH</code> does not exist <ul> <li>the file is saved to a file created at <code>DEST_PATH</code>\n</li> </ul>\n</li> <li>\n<code>DEST_PATH</code> does not exist and ends with <code>/</code> <ul> <li>Error condition: the destination directory must exist.</li> </ul>\n</li> <li>\n<code>DEST_PATH</code> exists and is a file <ul> <li>the destination is overwritten with the source file’s contents</li> </ul>\n</li> <li>\n<code>DEST_PATH</code> exists and is a directory <ul> <li>the file is copied into this directory using the basename from <code>SRC_PATH</code>\n</li> </ul>\n</li> </ul>\n</li> <li>\n<code>SRC_PATH</code> specifies a directory <ul> <li>\n<code>DEST_PATH</code> does not exist <ul> <li>\n<code>DEST_PATH</code> is created as a directory and the <em>contents</em> of the source directory are copied into this directory</li> </ul>\n</li> <li>\n<code>DEST_PATH</code> exists and is a file <ul> <li>Error condition: cannot copy a directory to a file</li> </ul>\n</li> <li>\n<code>DEST_PATH</code> exists and is a directory <ul> <li>\n<code>SRC_PATH</code> does not end with <code>/.</code> <ul> <li>the source directory is copied into this directory</li> </ul>\n</li> <li>\n<code>SRC_PATH</code> does end with <code>/.</code> <ul> <li>the <em>content</em> of the source directory is copied into this directory</li> </ul>\n</li> </ul>\n</li> </ul>\n</li> </ul> <p>The command requires <code>SRC_PATH</code> and <code>DEST_PATH</code> to exist according to the above rules. If <code>SRC_PATH</code> is local and is a symbolic link, the symbolic link, not the target, is copied by default. To copy the link target and not the link, specify the <code>-L</code> option.</p> <p>A colon (<code>:</code>) is used as a delimiter between <code>CONTAINER</code> and its path. You can also use <code>:</code> when specifying paths to a <code>SRC_PATH</code> or <code>DEST_PATH</code> on a local machine, for example <code>file:name.txt</code>. If you use a <code>:</code> in a local machine path, you must be explicit with a relative or absolute path, for example:</p> <pre>`/path/to/file:name.txt` or `./file:name.txt`\n</pre> <p>It is not possible to copy certain system files such as resources under <code>/proc</code>, <code>/sys</code>, <code>/dev</code>, <a href=\"../run/index#mount-tmpfs-tmpfs\">tmpfs</a>, and mounts created by the user in the container. However, you can still copy such files by manually running <code>tar</code> in <code>docker exec</code>. For example (consider <code>SRC_PATH</code> and <code>DEST_PATH</code> are directories):</p> <pre>$ docker exec foo tar Ccf $(dirname SRC_PATH) - $(basename SRC_PATH) | tar Cxf DEST_PATH -\n</pre> <p>or</p> <pre>$ tar Ccf $(dirname SRC_PATH) - $(basename SRC_PATH) | docker exec -i foo tar Cxf DEST_PATH -\n</pre> <p>Using <code>-</code> as the <code>SRC_PATH</code> streams the contents of <code>STDIN</code> as a tar archive. The command extracts the content of the tar to the <code>DEST_PATH</code> in container’s filesystem. In this case, <code>DEST_PATH</code> must specify a directory. Using <code>-</code> as the <code>DEST_PATH</code> streams the contents of the resource as a tar archive to <code>STDOUT</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/cp/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/cp/</a>\n  </p>\n</div>\n","engine/reference/commandline/diff/index":"<h1 id=\"diff\">diff</h1> <pre>Usage: docker diff [OPTIONS] CONTAINER\n\nInspect changes on a container's filesystem\n\n  --help              Print usage\n</pre> <p>List the changed files and directories in a container᾿s filesystem There are 3 events that are listed in the <code>diff</code>:</p> <ol> <li>\n<code>A</code> - Add</li> <li>\n<code>D</code> - Delete</li> <li>\n<code>C</code> - Change</li> </ol> <p>For example:</p> <pre>$ docker diff 7bb0e258aefe\n\nC /dev\nA /dev/kmsg\nC /etc\nA /etc/mtab\nA /go\nA /go/src\nA /go/src/github.com\nA /go/src/github.com/docker\nA /go/src/github.com/docker/docker\nA /go/src/github.com/docker/docker/.git\n....\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/diff/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/diff/</a>\n  </p>\n</div>\n","engine/reference/commandline/commit/index":"<h1 id=\"commit\">commit</h1> <pre>Usage: docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\n\nCreate a new image from a container's changes\n\n  -a, --author=\"\"     Author (e.g., \"John Hannibal Smith &lt;hannibal@a-team.com&gt;\")\n  -c, --change=[]     Apply specified Dockerfile instructions while committing the image\n  --help              Print usage\n  -m, --message=\"\"    Commit message\n  -p, --pause=true    Pause container during commit\n</pre> <p>It can be useful to commit a container’s file changes or settings into a new image. This allows you debug a container by running an interactive shell, or to export a working dataset to another server. Generally, it is better to use Dockerfiles to manage your images in a documented and maintainable way.</p> <p>The commit operation will not include any data contained in volumes mounted inside the container.</p> <p>By default, the container being committed and its processes will be paused while the image is committed. This reduces the likelihood of encountering data corruption during the process of creating the commit. If this behavior is undesired, set the <code>--pause</code> option to false.</p> <p>The <code>--change</code> option will apply <code>Dockerfile</code> instructions to the image that is created. Supported <code>Dockerfile</code> instructions: <code>CMD</code>|<code>ENTRYPOINT</code>|<code>ENV</code>|<code>EXPOSE</code>|<code>LABEL</code>|<code>ONBUILD</code>|<code>USER</code>|<code>VOLUME</code>|<code>WORKDIR</code></p> <h2 id=\"commit-a-container\">Commit a container</h2> <pre>$ docker ps\nID                  IMAGE               COMMAND             CREATED             STATUS              PORTS\nc3f279d17e0a        ubuntu:12.04        /bin/bash           7 days ago          Up 25 hours\n197387f1b436        ubuntu:12.04        /bin/bash           7 days ago          Up 25 hours\n$ docker commit c3f279d17e0a  svendowideit/testimage:version3\nf5283438590d\n$ docker images\nREPOSITORY                        TAG                 ID                  CREATED             SIZE\nsvendowideit/testimage            version3            f5283438590d        16 seconds ago      335.7 MB\n</pre> <h2 id=\"commit-a-container-with-new-configurations\">Commit a container with new configurations</h2> <pre>$ docker ps\nID                  IMAGE               COMMAND             CREATED             STATUS              PORTS\nc3f279d17e0a        ubuntu:12.04        /bin/bash           7 days ago          Up 25 hours\n197387f1b436        ubuntu:12.04        /bin/bash           7 days ago          Up 25 hours\n$ docker inspect -f \"{{ .Config.Env }}\" c3f279d17e0a\n[HOME=/ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin]\n$ docker commit --change \"ENV DEBUG true\" c3f279d17e0a  svendowideit/testimage:version3\nf5283438590d\n$ docker inspect -f \"{{ .Config.Env }}\" f5283438590d\n[HOME=/ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBUG=true]\n</pre> <h2 id=\"commit-a-container-with-new-cmd-and-expose-instructions\">Commit a container with new <code>CMD</code> and <code>EXPOSE</code> instructions</h2> <pre>$ docker ps\nID                  IMAGE               COMMAND             CREATED             STATUS              PORTS\nc3f279d17e0a        ubuntu:12.04        /bin/bash           7 days ago          Up 25 hours\n197387f1b436        ubuntu:12.04        /bin/bash           7 days ago          Up 25 hours\n\n$ docker commit --change='CMD [\"apachectl\", \"-DFOREGROUND\"]' -c \"EXPOSE 80\" c3f279d17e0a  svendowideit/testimage:version4\nf5283438590d\n\n$ docker run -d svendowideit/testimage:version4\n89373736e2e7f00bc149bd783073ac43d0507da250e999f3f1036e0db60817c0\n\n$ docker ps\nID                  IMAGE               COMMAND                 CREATED             STATUS              PORTS\n89373736e2e7        testimage:version4  \"apachectl -DFOREGROU\"  3 seconds ago       Up 2 seconds        80/tcp\nc3f279d17e0a        ubuntu:12.04        /bin/bash               7 days ago          Up 25 hours\n197387f1b436        ubuntu:12.04        /bin/bash               7 days ago          Up 25 hours\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/commit/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/commit/</a>\n  </p>\n</div>\n","engine/reference/commandline/create/index":"<h1 id=\"create\">create</h1> <p>Creates a new container.</p> <pre>Usage: docker create [OPTIONS] IMAGE [COMMAND] [ARG...]\n\nCreate a new container\n\n  -a, --attach=[]               Attach to STDIN, STDOUT or STDERR\n  --add-host=[]                 Add a custom host-to-IP mapping (host:ip)\n  --blkio-weight=0              Block IO weight (relative weight)\n  --blkio-weight-device=[]      Block IO weight (relative device weight, format: `DEVICE_NAME:WEIGHT`)\n  --cpu-shares=0                CPU shares (relative weight)\n  --cap-add=[]                  Add Linux capabilities\n  --cap-drop=[]                 Drop Linux capabilities\n  --cgroup-parent=\"\"            Optional parent cgroup for the container\n  --cidfile=\"\"                  Write the container ID to the file\n  --cpu-period=0                Limit CPU CFS (Completely Fair Scheduler) period\n  --cpu-quota=0                 Limit CPU CFS (Completely Fair Scheduler) quota\n  --cpuset-cpus=\"\"              CPUs in which to allow execution (0-3, 0,1)\n  --cpuset-mems=\"\"              Memory nodes (MEMs) in which to allow execution (0-3, 0,1)\n  --device=[]                   Add a host device to the container\n  --device-read-bps=[]          Limit read rate (bytes per second) from a device (e.g., --device-read-bps=/dev/sda:1mb)\n  --device-read-iops=[]         Limit read rate (IO per second) from a device (e.g., --device-read-iops=/dev/sda:1000)\n  --device-write-bps=[]         Limit write rate (bytes per second) to a device (e.g., --device-write-bps=/dev/sda:1mb)\n  --device-write-iops=[]        Limit write rate (IO per second) to a device (e.g., --device-write-iops=/dev/sda:1000)\n  --disable-content-trust=true  Skip image verification\n  --dns=[]                      Set custom DNS servers\n  --dns-opt=[]                  Set custom DNS options\n  --dns-search=[]               Set custom DNS search domains\n  -e, --env=[]                  Set environment variables\n  --entrypoint=\"\"               Overwrite the default ENTRYPOINT of the image\n  --env-file=[]                 Read in a file of environment variables\n  --expose=[]                   Expose a port or a range of ports\n  --group-add=[]                Add additional groups to join\n  -h, --hostname=\"\"             Container host name\n  --help                        Print usage\n  -i, --interactive             Keep STDIN open even if not attached\n  --ip=\"\"                       Container IPv4 address (e.g. 172.30.100.104)\n  --ip6=\"\"                      Container IPv6 address (e.g. 2001:db8::33)\n  --ipc=\"\"                      IPC namespace to use\n  --isolation=\"\"                Container isolation technology\n  --kernel-memory=\"\"            Kernel memory limit\n  -l, --label=[]                Set metadata on the container (e.g., --label=com.example.key=value)\n  --label-file=[]               Read in a line delimited file of labels\n  --link=[]                     Add link to another container\n  --log-driver=\"\"               Logging driver for container\n  --log-opt=[]                  Log driver specific options\n  -m, --memory=\"\"               Memory limit\n  --mac-address=\"\"              Container MAC address (e.g. 92:d0:c6:0a:29:33)\n  --memory-reservation=\"\"       Memory soft limit\n  --memory-swap=\"\"              A positive integer equal to memory plus swap. Specify -1 to enable unlimited swap.\n  --memory-swappiness=\"\"        Tune a container's memory swappiness behavior. Accepts an integer between 0 and 100.\n  --name=\"\"                     Assign a name to the container\n  --net=\"bridge\"                Connect a container to a network\n                                'bridge': create a network stack on the default Docker bridge\n                                'none': no networking\n                                'container:&lt;name|id&gt;': reuse another container's network stack\n                                'host': use the Docker host network stack\n                                '&lt;network-name&gt;|&lt;network-id&gt;': connect to a user-defined network\n  --net-alias=[]                Add network-scoped alias for the container\n  --oom-kill-disable            Whether to disable OOM Killer for the container or not\n  --oom-score-adj=0             Tune the host's OOM preferences for containers (accepts -1000 to 1000)\n  -P, --publish-all             Publish all exposed ports to random ports\n  -p, --publish=[]              Publish a container's port(s) to the host\n  --pid=\"\"                      PID namespace to use\n  --pids-limit=-1                Tune container pids limit (set -1 for unlimited), kernel &gt;= 4.3\n  --privileged                  Give extended privileges to this container\n  --read-only                   Mount the container's root filesystem as read only\n  --restart=\"no\"                Restart policy (no, on-failure[:max-retry], always, unless-stopped)\n  --security-opt=[]             Security options\n  --stop-signal=\"SIGTERM\"       Signal to stop a container\n  --shm-size=[]                 Size of `/dev/shm`. The format is `&lt;number&gt;&lt;unit&gt;`. `number` must be greater than `0`.  Unit is optional and can be `b` (bytes), `k` (kilobytes), `m` (megabytes), or `g` (gigabytes). If you omit the unit, the system uses bytes. If you omit the size entirely, the system uses `64m`.\n  -t, --tty                     Allocate a pseudo-TTY\n  -u, --user=\"\"                 Username or UID\n  --userns=\"\"                   Container user namespace\n                                'host': Use the Docker host user namespace\n                                '': Use the Docker daemon user namespace specified by `--userns-remap` option.\n  --ulimit=[]                   Ulimit options\n  --uts=\"\"                      UTS namespace to use\n  -v, --volume=[host-src:]container-dest[:&lt;options&gt;]\n                                Bind mount a volume. The comma-delimited\n                                `options` are [rw|ro], [z|Z],\n                                [[r]shared|[r]slave|[r]private], and\n                                [nocopy]. The 'host-src' is an absolute path\n                                or a name value.\n  --volume-driver=\"\"            Container's volume driver\n  --volumes-from=[]             Mount volumes from the specified container(s)\n  -w, --workdir=\"\"              Working directory inside the container\n</pre> <p>The <code>docker create</code> command creates a writeable container layer over the specified image and prepares it for running the specified command. The container ID is then printed to <code>STDOUT</code>. This is similar to <code>docker run -d</code> except the container is never started. You can then use the <code>docker start &lt;container_id&gt;</code> command to start the container at any point.</p> <p>This is useful when you want to set up a container configuration ahead of time so that it is ready to start when you need it. The initial status of the new container is <code>created</code>.</p> <p>Please see the <a href=\"../run/index\">run command</a> section and the <a href=\"../../run/index\">Docker run reference</a> for more details.</p> <h2 id=\"examples\">Examples</h2> <pre>$ docker create -t -i fedora bash\n6d8af538ec541dd581ebc2a24153a28329acb5268abe5ef868c1f1a261221752\n$ docker start -a -i 6d8af538ec5\nbash-4.2#\n</pre> <p>As of v1.4.0 container volumes are initialized during the <code>docker create</code> phase (i.e., <code>docker run</code> too). For example, this allows you to <code>create</code> the <code>data</code> volume container, and then use it from another container:</p> <pre>$ docker create -v /data --name data ubuntu\n240633dfbb98128fa77473d3d9018f6123b99c454b3251427ae190a7d951ad57\n$ docker run --rm --volumes-from data ubuntu ls -la /data\ntotal 8\ndrwxr-xr-x  2 root root 4096 Dec  5 04:10 .\ndrwxr-xr-x 48 root root 4096 Dec  5 04:11 ..\n</pre> <p>Similarly, <code>create</code> a host directory bind mounted volume container, which can then be used from the subsequent container:</p> <pre>$ docker create -v /home/docker:/docker --name docker ubuntu\n9aa88c08f319cd1e4515c3c46b0de7cc9aa75e878357b1e96f91e2c773029f03\n$ docker run --rm --volumes-from docker ubuntu ls -la /docker\ntotal 20\ndrwxr-sr-x  5 1000 staff  180 Dec  5 04:00 .\ndrwxr-xr-x 48 root root  4096 Dec  5 04:13 ..\n-rw-rw-r--  1 1000 staff 3833 Dec  5 04:01 .ash_history\n-rw-r--r--  1 1000 staff  446 Nov 28 11:51 .ashrc\n-rw-r--r--  1 1000 staff   25 Dec  5 04:00 .gitconfig\ndrwxr-sr-x  3 1000 staff   60 Dec  1 03:28 .local\n-rw-r--r--  1 1000 staff  920 Nov 28 11:51 .profile\ndrwx--S---  2 1000 staff  460 Dec  5 00:51 .ssh\ndrwxr-xr-x 32 1000 staff 1140 Dec  5 04:01 docker\n</pre> <h3 id=\"specify-isolation-technology-for-container-isolation\">Specify isolation technology for container (--isolation)</h3> <p>This option is useful in situations where you are running Docker containers on Windows. The <code>--isolation=&lt;value&gt;</code> option sets a container’s isolation technology. On Linux, the only supported is the <code>default</code> option which uses Linux namespaces. On Microsoft Windows, you can specify these values:</p> <table> <thead> <tr> <th>Value</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>default</code></td> <td>Use the value specified by the Docker daemon’s <code>--exec-opt</code> . If the <code>daemon</code> does not specify an isolation technology, Microsoft Windows uses <code>process</code> as its default value.</td> </tr> <tr> <td><code>process</code></td> <td>Namespace isolation only.</td> </tr> <tr> <td><code>hyperv</code></td> <td>Hyper-V hypervisor partition-based isolation.</td> </tr> </tbody> </table> <p>Specifying the <code>--isolation</code> flag without a value is the same as setting <code>--isolation=\"default\"</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/create/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/create/</a>\n  </p>\n</div>\n","engine/reference/run/index":" <h1 id=\"docker-run-reference\">Docker run reference</h1> <p>Docker runs processes in isolated containers. A container is a process which runs on a host. The host may be local or remote. When an operator executes <code>docker run</code>, the container process that runs is isolated in that it has its own file system, its own networking, and its own isolated process tree separate from the host.</p> <p>This page details how to use the <code>docker run</code> command to define the container’s resources at runtime.</p> <h2 id=\"general-form\">General form</h2> <p>The basic <code>docker run</code> command takes this form:</p> <pre>$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n</pre> <p>The <code>docker run</code> command must specify an <a href=\"../glossary/index#image\"><em>IMAGE</em></a> to derive the container from. An image developer can define image defaults related to:</p> <ul> <li>detached or foreground running</li> <li>container identification</li> <li>network settings</li> <li>runtime constraints on CPU and memory</li> </ul> <p>With the <code>docker run [OPTIONS]</code> an operator can add to or override the image defaults set by a developer. And, additionally, operators can override nearly all the defaults set by the Docker runtime itself. The operator’s ability to override image and Docker runtime defaults is why <a href=\"../commandline/run/index\"><em>run</em></a> has more options than any other <code>docker</code> command.</p> <p>To learn how to interpret the types of <code>[OPTIONS]</code>, see <a href=\"../commandline/cli/index#option-types\"><em>Option types</em></a>.</p> <blockquote> <p><strong>Note</strong>: Depending on your Docker system configuration, you may be required to preface the <code>docker run</code> command with <code>sudo</code>. To avoid having to use <code>sudo</code> with the <code>docker</code> command, your system administrator can create a Unix group called <code>docker</code> and add users to it. For more information about this configuration, refer to the Docker installation documentation for your operating system.</p> </blockquote> <h2 id=\"operator-exclusive-options\">Operator exclusive options</h2> <p>Only the operator (the person executing <code>docker run</code>) can set the following options.</p> <ul> <li>\n<a href=\"#detached-vs-foreground\">Detached vs foreground</a> <ul> <li><a href=\"#detached-d\">Detached (-d)</a></li> <li><a href=\"#foreground\">Foreground</a></li> </ul>\n</li> <li>\n<a href=\"#container-identification\">Container identification</a> <ul> <li><a href=\"#name-name\">Name (--name)</a></li> <li><a href=\"#pid-equivalent\">PID equivalent</a></li> </ul>\n</li> <li><a href=\"#ipc-settings-ipc\">IPC settings (--ipc)</a></li> <li><a href=\"#network-settings\">Network settings</a></li> <li><a href=\"#restart-policies-restart\">Restart policies (--restart)</a></li> <li><a href=\"#clean-up-rm\">Clean up (--rm)</a></li> <li><a href=\"#runtime-constraints-on-resources\">Runtime constraints on resources</a></li> <li><a href=\"#runtime-privilege-and-linux-capabilities\">Runtime privilege and Linux capabilities</a></li> </ul> <h2 id=\"detached-vs-foreground\">Detached vs foreground</h2> <p>When starting a Docker container, you must first decide if you want to run the container in the background in a “detached” mode or in the default foreground mode:</p> <pre>-d=false: Detached mode: Run container in the background, print new container id\n</pre> <h3 id=\"detached-d\">Detached (-d)</h3> <p>To start a container in detached mode, you use <code>-d=true</code> or just <code>-d</code> option. By design, containers started in detached mode exit when the root process used to run the container exits. A container in detached mode cannot be automatically removed when it stops, this means you cannot use the <code>--rm</code> option with <code>-d</code> option.</p> <p>Do not pass a <code>service x start</code> command to a detached container. For example, this command attempts to start the <code>nginx</code> service.</p> <pre>$ docker run -d -p 80:80 my_image service nginx start\n</pre> <p>This succeeds in starting the <code>nginx</code> service inside the container. However, it fails the detached container paradigm in that, the root process (<code>service nginx\nstart</code>) returns and the detached container stops as designed. As a result, the <code>nginx</code> service is started but could not be used. Instead, to start a process such as the <code>nginx</code> web server do the following:</p> <pre>$ docker run -d -p 80:80 my_image nginx -g 'daemon off;'\n</pre> <p>To do input/output with a detached container use network connections or shared volumes. These are required because the container is no longer listening to the command line where <code>docker run</code> was run.</p> <p>To reattach to a detached container, use <code>docker</code> <a href=\"../commandline/attach/index\"><em>attach</em></a> command.</p> <h3 id=\"foreground\">Foreground</h3> <p>In foreground mode (the default when <code>-d</code> is not specified), <code>docker\nrun</code> can start the process in the container and attach the console to the process’s standard input, output, and standard error. It can even pretend to be a TTY (this is what most command line executables expect) and pass along signals. All of that is configurable:</p> <pre>-a=[]           : Attach to `STDIN`, `STDOUT` and/or `STDERR`\n-t              : Allocate a pseudo-tty\n--sig-proxy=true: Proxy all received signals to the process (non-TTY mode only)\n-i              : Keep STDIN open even if not attached\n</pre> <p>If you do not specify <code>-a</code> then Docker will <a href=\"https://github.com/docker/docker/blob/75a7f4d90cde0295bcfb7213004abce8d4779b75/commands.go#L1797\">attach all standard streams</a>. You can specify to which of the three standard streams (<code>STDIN</code>, <code>STDOUT</code>, <code>STDERR</code>) you’d like to connect instead, as in:</p> <pre>$ docker run -a stdin -a stdout -i -t ubuntu /bin/bash\n</pre> <p>For interactive processes (like a shell), you must use <code>-i -t</code> together in order to allocate a tty for the container process. <code>-i -t</code> is often written <code>-it</code> as you’ll see in later examples. Specifying <code>-t</code> is forbidden when the client standard output is redirected or piped, such as in:</p> <pre>$ echo test | docker run -i busybox cat\n</pre> <blockquote> <p><strong>Note</strong>: A process running as PID 1 inside a container is treated specially by Linux: it ignores any signal with the default action. So, the process will not terminate on <code>SIGINT</code> or <code>SIGTERM</code> unless it is coded to do so.</p> </blockquote> <h2 id=\"container-identification\">Container identification</h2> <h3 id=\"name-name\">Name (--name)</h3> <p>The operator can identify a container in three ways:</p> <table> <thead> <tr> <th>Identifier type</th> <th>Example value</th> </tr> </thead> <tbody> <tr> <td>UUID long identifier</td> <td>“f78375b1c487e03c9438c729345e54db9d20cfa2ac1fc3494b6eb60872e74778”</td> </tr> <tr> <td>UUID short identifier</td> <td>“f78375b1c487”</td> </tr> <tr> <td>Name</td> <td>“evil_ptolemy”</td> </tr> </tbody> </table> <p>The UUID identifiers come from the Docker daemon. If you do not assign a container name with the <code>--name</code> option, then the daemon generates a random string name for you. Defining a <code>name</code> can be a handy way to add meaning to a container. If you specify a <code>name</code>, you can use it when referencing the container within a Docker network. This works for both background and foreground Docker containers.</p> <blockquote> <p><strong>Note</strong>: Containers on the default bridge network must be linked to communicate by name.</p> </blockquote> <h3 id=\"pid-equivalent\">PID equivalent</h3> <p>Finally, to help with automation, you can have Docker write the container ID out to a file of your choosing. This is similar to how some programs might write out their process ID to a file (you’ve seen them as PID files):</p> <pre>--cidfile=\"\": Write the container ID to the file\n</pre> <h3 id=\"image-tag\">Image[:tag]</h3> <p>While not strictly a means of identifying a container, you can specify a version of an image you’d like to run the container with by adding <code>image[:tag]</code> to the command. For example, <code>docker run ubuntu:14.04</code>.</p> <h3 id=\"image-digest\">Image[@digest]</h3> <p>Images using the v2 or later image format have a content-addressable identifier called a digest. As long as the input used to generate the image is unchanged, the digest value is predictable and referenceable.</p> <h2 id=\"pid-settings-pid\">PID settings (--pid)</h2> <pre>--pid=\"\"  : Set the PID (Process) Namespace mode for the container,\n       'host': use the host's PID namespace inside the container\n</pre> <p>By default, all containers have the PID namespace enabled.</p> <p>PID namespace provides separation of processes. The PID Namespace removes the view of the system processes, and allows process ids to be reused including pid 1.</p> <p>In certain cases you want your container to share the host’s process namespace, basically allowing processes within the container to see all of the processes on the system. For example, you could build a container with debugging tools like <code>strace</code> or <code>gdb</code>, but want to use these tools when debugging processes within the container.</p> <h3 id=\"example-run-htop-inside-a-container\">Example: run htop inside a container</h3> <p>Create this Dockerfile:</p> <pre>FROM alpine:latest\nRUN apk add --update htop &amp;&amp; rm -rf /var/cache/apk/*\nCMD [\"htop\"]\n</pre> <p>Build the Dockerfile and tag the image as <code>myhtop</code>:</p> <pre>$ docker build -t myhtop .\n</pre> <p>Use the following command to run <code>htop</code> inside a container:</p> <pre>$ docker run -it --rm --pid=host myhtop\n</pre> <h2 id=\"uts-settings-uts\">UTS settings (--uts)</h2> <pre>--uts=\"\"  : Set the UTS namespace mode for the container,\n       'host': use the host's UTS namespace inside the container\n</pre> <p>The UTS namespace is for setting the hostname and the domain that is visible to running processes in that namespace. By default, all containers, including those with <code>--net=host</code>, have their own UTS namespace. The <code>host</code> setting will result in the container using the same UTS namespace as the host. Note that <code>--hostname</code> is invalid in <code>host</code> UTS mode.</p> <p>You may wish to share the UTS namespace with the host if you would like the hostname of the container to change as the hostname of the host changes. A more advanced use case would be changing the host’s hostname from a container.</p> <h2 id=\"ipc-settings-ipc\">IPC settings (--ipc)</h2> <pre>--ipc=\"\"  : Set the IPC mode for the container,\n             'container:&lt;name|id&gt;': reuses another container's IPC namespace\n             'host': use the host's IPC namespace inside the container\n</pre> <p>By default, all containers have the IPC namespace enabled.</p> <p>IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues.</p> <p>Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. Shared memory is commonly used by databases and custom-built (typically C/OpenMPI, C++/using boost libraries) high performance applications for scientific computing and financial services industries. If these types of applications are broken into multiple containers, you might need to share the IPC mechanisms of the containers.</p> <h2 id=\"network-settings\">Network settings</h2> <pre>--dns=[]         : Set custom dns servers for the container\n--net=\"bridge\"   : Connect a container to a network\n                    'bridge': create a network stack on the default Docker bridge\n                    'none': no networking\n                    'container:&lt;name|id&gt;': reuse another container's network stack\n                    'host': use the Docker host network stack\n                    '&lt;network-name&gt;|&lt;network-id&gt;': connect to a user-defined network\n--net-alias=[]   : Add network-scoped alias for the container\n--add-host=\"\"    : Add a line to /etc/hosts (host:IP)\n--mac-address=\"\" : Sets the container's Ethernet device's MAC address\n--ip=\"\"          : Sets the container's Ethernet device's IPv4 address\n--ip6=\"\"         : Sets the container's Ethernet device's IPv6 address\n</pre> <p>By default, all containers have networking enabled and they can make any outgoing connections. The operator can completely disable networking with <code>docker run --net none</code> which disables all incoming and outgoing networking. In cases like this, you would perform I/O through files or <code>STDIN</code> and <code>STDOUT</code> only.</p> <p>Publishing ports and linking to other containers only works with the default (bridge). The linking feature is a legacy feature. You should always prefer using Docker network drivers over linking.</p> <p>Your container will use the same DNS servers as the host by default, but you can override this with <code>--dns</code>.</p> <p>By default, the MAC address is generated using the IP address allocated to the container. You can set the container’s MAC address explicitly by providing a MAC address via the <code>--mac-address</code> parameter (format:<code>12:34:56:78:9a:bc</code>).Be aware that Docker does not check if manually specified MAC addresses are unique.</p> <p>Supported networks :</p> <table> <thead> <tr> <th class=\"no-wrap\">Network</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td class=\"no-wrap\"><strong>none</strong></td> <td> No networking in the container. </td> </tr> <tr> <td class=\"no-wrap\">\n<strong>bridge</strong> (default)</td> <td> Connect the container to the bridge via veth interfaces. </td> </tr> <tr> <td class=\"no-wrap\"><strong>host</strong></td> <td> Use the host's network stack inside the container. </td> </tr> <tr> <td class=\"no-wrap\">\n<strong>container</strong>:&lt;name|id&gt;</td> <td> Use the network stack of another container, specified via its *name* or *id*. </td> </tr> <tr> <td class=\"no-wrap\"><strong>NETWORK</strong></td> <td> Connects the container to a user created network (using `docker network create` command) </td> </tr> </tbody> </table> <h4 id=\"network-none\">Network: none</h4> <p>With the network is <code>none</code> a container will not have access to any external routes. The container will still have a <code>loopback</code> interface enabled in the container but it does not have any routes to external traffic.</p> <h4 id=\"network-bridge\">Network: bridge</h4> <p>With the network set to <code>bridge</code> a container will use docker’s default networking setup. A bridge is setup on the host, commonly named <code>docker0</code>, and a pair of <code>veth</code> interfaces will be created for the container. One side of the <code>veth</code> pair will remain on the host attached to the bridge while the other side of the pair will be placed inside the container’s namespaces in addition to the <code>loopback</code> interface. An IP address will be allocated for containers on the bridge’s network and traffic will be routed though this bridge to the container.</p> <p>Containers can communicate via their IP addresses by default. To communicate by name, they must be linked.</p> <h4 id=\"network-host\">Network: host</h4> <p>With the network set to <code>host</code> a container will share the host’s network stack and all interfaces from the host will be available to the container. The container’s hostname will match the hostname on the host system. Note that <code>--add-host</code> <code>--dns</code> <code>--dns-search</code> <code>--dns-opt</code> and <code>--mac-address</code> are invalid in <code>host</code> netmode. Even in <code>host</code> network mode a container has its own UTS namespace by default. As such <code>--hostname</code> is allowed in <code>host</code> network mode and will only change the hostname inside the container.</p> <p>Compared to the default <code>bridge</code> mode, the <code>host</code> mode gives <em>significantly</em> better networking performance since it uses the host’s native networking stack whereas the bridge has to go through one level of virtualization through the docker daemon. It is recommended to run containers in this mode when their networking performance is critical, for example, a production Load Balancer or a High Performance Web Server.</p> <blockquote> <p><strong>Note</strong>: <code>--net=\"host\"</code> gives the container full access to local system services such as D-bus and is therefore considered insecure.</p> </blockquote> <h4 id=\"network-container\">Network: container</h4> <p>With the network set to <code>container</code> a container will share the network stack of another container. The other container’s name must be provided in the format of <code>--net container:&lt;name|id&gt;</code>. Note that <code>--add-host</code> <code>--hostname</code> <code>--dns</code> <code>--dns-search</code> <code>--dns-opt</code> and <code>--mac-address</code> are invalid in <code>container</code> netmode, and <code>--publish</code> <code>--publish-all</code> <code>--expose</code> are also invalid in <code>container</code> netmode.</p> <p>Example running a Redis container with Redis binding to <code>localhost</code> then running the <code>redis-cli</code> command and connecting to the Redis server over the <code>localhost</code> interface.</p> <pre>$ docker run -d --name redis example/redis --bind 127.0.0.1\n$ # use the redis container's network stack to access localhost\n$ docker run --rm -it --net container:redis example/redis-cli -h 127.0.0.1\n</pre> <h4 id=\"user-defined-network\">User-defined network</h4> <p>You can create a network using a Docker network driver or an external network driver plugin. You can connect multiple containers to the same network. Once connected to a user-defined network, the containers can communicate easily using only another container’s IP address or name.</p> <p>For <code>overlay</code> networks or custom plugins that support multi-host connectivity, containers connected to the same multi-host network but launched from different Engines can also communicate in this way.</p> <p>The following example creates a network using the built-in <code>bridge</code> network driver and running a container in the created network</p> <pre>$ docker network create -d bridge my-net\n$ docker run --net=my-net -itd --name=container3 busybox\n</pre> <h3 id=\"managing-etc-hosts\">Managing /etc/hosts</h3> <p>Your container will have lines in <code>/etc/hosts</code> which define the hostname of the container itself as well as <code>localhost</code> and a few other common things. The <code>--add-host</code> flag can be used to add additional lines to <code>/etc/hosts</code>.</p> <pre>$ docker run -it --add-host db-static:86.75.30.9 ubuntu cat /etc/hosts\n172.17.0.22     09d03f76bf2c\nfe00::0         ip6-localnet\nff00::0         ip6-mcastprefix\nff02::1         ip6-allnodes\nff02::2         ip6-allrouters\n127.0.0.1       localhost\n::1             localhost ip6-localhost ip6-loopback\n86.75.30.9      db-static\n</pre> <p>If a container is connected to the default bridge network and <code>linked</code> with other containers, then the container’s <code>/etc/hosts</code> file is updated with the linked container’s name.</p> <p>If the container is connected to user-defined network, the container’s <code>/etc/hosts</code> file is updated with names of all other containers in that user-defined network.</p> <blockquote> <p><strong>Note</strong> Since Docker may live update the container’s <code>/etc/hosts</code> file, there may be situations when processes inside the container can end up reading an empty or incomplete <code>/etc/hosts</code> file. In most cases, retrying the read again should fix the problem.</p> </blockquote> <h2 id=\"restart-policies-restart\">Restart policies (--restart)</h2> <p>Using the <code>--restart</code> flag on Docker run you can specify a restart policy for how a container should or should not be restarted on exit.</p> <p>When a restart policy is active on a container, it will be shown as either <code>Up</code> or <code>Restarting</code> in <a href=\"../commandline/ps/index\"><code>docker ps</code></a>. It can also be useful to use <a href=\"../commandline/events/index\"><code>docker events</code></a> to see the restart policy in effect.</p> <p>Docker supports the following restart policies:</p> <table> <thead> <tr> <th>Policy</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td><strong>no</strong></td> <td> Do not automatically restart the container when it exits. This is the default. </td> </tr> <tr> <td> <span style=\"white-space: nowrap\"> <strong>on-failure</strong>[:max-retries] </span> </td> <td> Restart only if the container exits with a non-zero exit status. Optionally, limit the number of restart retries the Docker daemon attempts. </td> </tr> <tr> <td><strong>always</strong></td> <td> Always restart the container regardless of the exit status. When you specify always, the Docker daemon will try to restart the container indefinitely. The container will also always start on daemon startup, regardless of the current state of the container. </td> </tr> <tr> <td><strong>unless-stopped</strong></td> <td> Always restart the container regardless of the exit status, but do not start it on daemon startup if the container has been put to a stopped state before. </td> </tr> </tbody> </table> <p>An ever increasing delay (double the previous delay, starting at 100 milliseconds) is added before each restart to prevent flooding the server. This means the daemon will wait for 100 ms, then 200 ms, 400, 800, 1600, and so on until either the <code>on-failure</code> limit is hit, or when you <code>docker stop</code> or <code>docker rm -f</code> the container.</p> <p>If a container is successfully restarted (the container is started and runs for at least 10 seconds), the delay is reset to its default value of 100 ms.</p> <p>You can specify the maximum amount of times Docker will try to restart the container when using the <strong>on-failure</strong> policy. The default is that Docker will try forever to restart the container. The number of (attempted) restarts for a container can be obtained via <a href=\"../commandline/inspect/index\"><code>docker inspect</code></a>. For example, to get the number of restarts for container “my-container”;</p> <pre>$ docker inspect -f \"{{ .RestartCount }}\" my-container\n# 2\n</pre> <p>Or, to get the last time the container was (re)started;</p> <pre>$ docker inspect -f \"{{ .State.StartedAt }}\" my-container\n# 2015-03-04T23:47:07.691840179Z\n</pre> <p>Combining <code>--restart</code> (restart policy) with the <code>--rm</code> (clean up) flag results in an error. On container restart, attached clients are disconnected. See the examples on using the <a href=\"#clean-up-rm\"><code>--rm</code> (clean up)</a> flag later in this page.</p> <h3 id=\"examples\">Examples</h3> <pre>$ docker run --restart=always redis\n</pre> <p>This will run the <code>redis</code> container with a restart policy of <strong>always</strong> so that if the container exits, Docker will restart it.</p> <pre>$ docker run --restart=on-failure:10 redis\n</pre> <p>This will run the <code>redis</code> container with a restart policy of <strong>on-failure</strong> and a maximum restart count of 10. If the <code>redis</code> container exits with a non-zero exit status more than 10 times in a row Docker will abort trying to restart the container. Providing a maximum restart limit is only valid for the <strong>on-failure</strong> policy.</p> <h2 id=\"exit-status\">Exit Status</h2> <p>The exit code from <code>docker run</code> gives information about why the container failed to run or why it exited. When <code>docker run</code> exits with a non-zero code, the exit codes follow the <code>chroot</code> standard, see below:</p> <p><strong><em>125</em></strong> if the error is with Docker daemon <strong><em>itself</em></strong></p> <pre>$ docker run --foo busybox; echo $?\n# flag provided but not defined: --foo\n  See 'docker run --help'.\n  125\n</pre> <p><strong><em>126</em></strong> if the <strong><em>contained command</em></strong> cannot be invoked</p> <pre>$ docker run busybox /etc; echo $?\n# docker: Error response from daemon: Container command '/etc' could not be invoked.\n  126\n</pre> <p><strong><em>127</em></strong> if the <strong><em>contained command</em></strong> cannot be found</p> <pre>$ docker run busybox foo; echo $?\n# docker: Error response from daemon: Container command 'foo' not found or does not exist.\n  127\n</pre> <p><strong><em>Exit code</em></strong> of <strong><em>contained command</em></strong> otherwise</p> <pre>$ docker run busybox /bin/sh -c 'exit 3'; echo $?\n# 3\n</pre> <h2 id=\"clean-up-rm\">Clean up (--rm)</h2> <p>By default a container’s file system persists even after the container exits. This makes debugging a lot easier (since you can inspect the final state) and you retain all your data by default. But if you are running short-term <strong>foreground</strong> processes, these container file systems can really pile up. If instead you’d like Docker to <strong>automatically clean up the container and remove the file system when the container exits</strong>, you can add the <code>--rm</code> flag:</p> <pre>--rm=false: Automatically remove the container when it exits (incompatible with -d)\n</pre> <blockquote> <p><strong>Note</strong>: When you set the <code>--rm</code> flag, Docker also removes the volumes associated with the container when the container is removed. This is similar to running <code>docker rm -v my-container</code>. Only volumes that are specified without a name are removed. For example, with <code>docker run --rm -v /foo -v awesome:/bar busybox top</code>, the volume for <code>/foo</code> will be removed, but the volume for <code>/bar</code> will not. Volumes inherited via <code>--volumes-from</code> will be removed with the same logic -- if the original volume was specified with a name it will <strong>not</strong> be removed.</p> </blockquote> <h2 id=\"security-configuration\">Security configuration</h2> <pre>--security-opt=\"label=user:USER\"   : Set the label user for the container\n--security-opt=\"label=role:ROLE\"   : Set the label role for the container\n--security-opt=\"label=type:TYPE\"   : Set the label type for the container\n--security-opt=\"label=level:LEVEL\" : Set the label level for the container\n--security-opt=\"label=disable\"     : Turn off label confinement for the container\n--security-opt=\"apparmor=PROFILE\"  : Set the apparmor profile to be applied\n                                     to the container\n--security-opt=\"no-new-privileges\" : Disable container processes from gaining\n                                     new privileges\n--security-opt=\"seccomp=unconfined\": Turn off seccomp confinement for the container\n--security-opt=\"seccomp=profile.json: White listed syscalls seccomp Json file to be used as a seccomp filter\n</pre> <p>You can override the default labeling scheme for each container by specifying the <code>--security-opt</code> flag. Specifying the level in the following command allows you to share the same content between containers.</p> <pre>$ docker run --security-opt label=level:s0:c100,c200 -it fedora bash\n</pre> <blockquote> <p><strong>Note</strong>: Automatic translation of MLS labels is not currently supported.</p> </blockquote> <p>To disable the security labeling for this container versus running with the <code>--permissive</code> flag, use the following command:</p> <pre>$ docker run --security-opt label=disable -it fedora bash\n</pre> <p>If you want a tighter security policy on the processes within a container, you can specify an alternate type for the container. You could run a container that is only allowed to listen on Apache ports by executing the following command:</p> <pre>$ docker run --security-opt label=type:svirt_apache_t -it centos bash\n</pre> <blockquote> <p><strong>Note</strong>: You would have to write policy defining a <code>svirt_apache_t</code> type.</p> </blockquote> <p>If you want to prevent your container processes from gaining additional privileges, you can execute the following command:</p> <pre>$ docker run --security-opt no-new-privileges -it centos bash\n</pre> <p>For more details, see <a href=\"https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt\">kernel documentation</a>.</p> <h2 id=\"specifying-custom-cgroups\">Specifying custom cgroups</h2> <p>Using the <code>--cgroup-parent</code> flag, you can pass a specific cgroup to run a container in. This allows you to create and manage cgroups on their own. You can define custom resources for those cgroups and put containers under a common parent group.</p> <h2 id=\"runtime-constraints-on-resources\">Runtime constraints on resources</h2> <p>The operator can also adjust the performance parameters of the container:</p> <table> <thead> <tr> <th>Option</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>\n<code>-m</code>, <code>--memory=\"\"</code>\n</td> <td>Memory limit (format: <code>&lt;number&gt;[&lt;unit&gt;]</code>). Number is a positive integer. Unit can be one of <code>b</code>, <code>k</code>, <code>m</code>, or <code>g</code>. Minimum is 4M.</td> </tr> <tr> <td><code>--memory-swap=\"\"</code></td> <td>Total memory limit (memory + swap, format: <code>&lt;number&gt;[&lt;unit&gt;]</code>). Number is a positive integer. Unit can be one of <code>b</code>, <code>k</code>, <code>m</code>, or <code>g</code>.</td> </tr> <tr> <td><code>--memory-reservation=\"\"</code></td> <td>Memory soft limit (format: <code>&lt;number&gt;[&lt;unit&gt;]</code>). Number is a positive integer. Unit can be one of <code>b</code>, <code>k</code>, <code>m</code>, or <code>g</code>.</td> </tr> <tr> <td><code>--kernel-memory=\"\"</code></td> <td>Kernel memory limit (format: <code>&lt;number&gt;[&lt;unit&gt;]</code>). Number is a positive integer. Unit can be one of <code>b</code>, <code>k</code>, <code>m</code>, or <code>g</code>. Minimum is 4M.</td> </tr> <tr> <td>\n<code>-c</code>, <code>--cpu-shares=0</code>\n</td> <td>CPU shares (relative weight)</td> </tr> <tr> <td><code>--cpu-period=0</code></td> <td>Limit the CPU CFS (Completely Fair Scheduler) period</td> </tr> <tr> <td><code>--cpuset-cpus=\"\"</code></td> <td>CPUs in which to allow execution (0-3, 0,1)</td> </tr> <tr> <td><code>--cpuset-mems=\"\"</code></td> <td>Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems.</td> </tr> <tr> <td><code>--cpu-quota=0</code></td> <td>Limit the CPU CFS (Completely Fair Scheduler) quota</td> </tr> <tr> <td><code>--blkio-weight=0</code></td> <td>Block IO weight (relative weight) accepts a weight value between 10 and 1000.</td> </tr> <tr> <td><code>--blkio-weight-device=\"\"</code></td> <td>Block IO weight (relative device weight, format: <code>DEVICE_NAME:WEIGHT</code>)</td> </tr> <tr> <td><code>--device-read-bps=\"\"</code></td> <td>Limit read rate from a device (format: <code>&lt;device-path&gt;:&lt;number&gt;[&lt;unit&gt;]</code>). Number is a positive integer. Unit can be one of <code>kb</code>, <code>mb</code>, or <code>gb</code>.</td> </tr> <tr> <td><code>--device-write-bps=\"\"</code></td> <td>Limit write rate to a device (format: <code>&lt;device-path&gt;:&lt;number&gt;[&lt;unit&gt;]</code>). Number is a positive integer. Unit can be one of <code>kb</code>, <code>mb</code>, or <code>gb</code>.</td> </tr> <tr> <td><code>--device-read-iops=\"\"</code></td> <td>Limit read rate (IO per second) from a device (format: <code>&lt;device-path&gt;:&lt;number&gt;</code>). Number is a positive integer.</td> </tr> <tr> <td><code>--device-write-iops=\"\"</code></td> <td>Limit write rate (IO per second) to a device (format: <code>&lt;device-path&gt;:&lt;number&gt;</code>). Number is a positive integer.</td> </tr> <tr> <td><code>--oom-kill-disable=false</code></td> <td>Whether to disable OOM Killer for the container or not.</td> </tr> <tr> <td><code>--memory-swappiness=\"\"</code></td> <td>Tune a container’s memory swappiness behavior. Accepts an integer between 0 and 100.</td> </tr> <tr> <td><code>--shm-size=\"\"</code></td> <td>Size of <code>/dev/shm</code>. The format is <code>&lt;number&gt;&lt;unit&gt;</code>. <code>number</code> must be greater than <code>0</code>. Unit is optional and can be <code>b</code> (bytes), <code>k</code> (kilobytes), <code>m</code> (megabytes), or <code>g</code> (gigabytes). If you omit the unit, the system uses bytes. If you omit the size entirely, the system uses <code>64m</code>.</td> </tr> </tbody> </table> <h3 id=\"user-memory-constraints\">User memory constraints</h3> <p>We have four ways to set user memory usage:</p> <table> <thead> <tr> <th>Option</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td class=\"no-wrap\"> <strong>memory=inf, memory-swap=inf</strong> (default) </td> <td> There is no memory limit for the container. The container can use as much memory as needed. </td> </tr> <tr> <td class=\"no-wrap\"><strong>memory=L&lt;inf, memory-swap=inf</strong></td> <td> (specify memory and set memory-swap as <code>-1</code>) The container is not allowed to use more than L bytes of memory, but can use as much swap as is needed (if the host supports swap memory). </td> </tr> <tr> <td class=\"no-wrap\"><strong>memory=L&lt;inf, memory-swap=2*L</strong></td> <td> (specify memory without memory-swap) The container is not allowed to use more than L bytes of memory, swap *plus* memory usage is double of that. </td> </tr> <tr> <td class=\"no-wrap\"> <strong>memory=L&lt;inf, memory-swap=S&lt;inf, L&lt;=S</strong> </td> <td> (specify both memory and memory-swap) The container is not allowed to use more than L bytes of memory, swap *plus* memory usage is limited by S. </td> </tr> </tbody> </table> <p>Examples:</p> <pre>$ docker run -it ubuntu:14.04 /bin/bash\n</pre> <p>We set nothing about memory, this means the processes in the container can use as much memory and swap memory as they need.</p> <pre>$ docker run -it -m 300M --memory-swap -1 ubuntu:14.04 /bin/bash\n</pre> <p>We set memory limit and disabled swap memory limit, this means the processes in the container can use 300M memory and as much swap memory as they need (if the host supports swap memory).</p> <pre>$ docker run -it -m 300M ubuntu:14.04 /bin/bash\n</pre> <p>We set memory limit only, this means the processes in the container can use 300M memory and 300M swap memory, by default, the total virtual memory size (--memory-swap) will be set as double of memory, in this case, memory + swap would be 2*300M, so processes can use 300M swap memory as well.</p> <pre>$ docker run -it -m 300M --memory-swap 1G ubuntu:14.04 /bin/bash\n</pre> <p>We set both memory and swap memory, so the processes in the container can use 300M memory and 700M swap memory.</p> <p>Memory reservation is a kind of memory soft limit that allows for greater sharing of memory. Under normal circumstances, containers can use as much of the memory as needed and are constrained only by the hard limits set with the <code>-m</code>/<code>--memory</code> option. When memory reservation is set, Docker detects memory contention or low memory and forces containers to restrict their consumption to a reservation limit.</p> <p>Always set the memory reservation value below the hard limit, otherwise the hard limit takes precedence. A reservation of 0 is the same as setting no reservation. By default (without reservation set), memory reservation is the same as the hard memory limit.</p> <p>Memory reservation is a soft-limit feature and does not guarantee the limit won’t be exceeded. Instead, the feature attempts to ensure that, when memory is heavily contended for, memory is allocated based on the reservation hints/setup.</p> <p>The following example limits the memory (<code>-m</code>) to 500M and sets the memory reservation to 200M.</p> <pre>$ docker run -it -m 500M --memory-reservation 200M ubuntu:14.04 /bin/bash\n</pre> <p>Under this configuration, when the container consumes memory more than 200M and less than 500M, the next system memory reclaim attempts to shrink container memory below 200M.</p> <p>The following example set memory reservation to 1G without a hard memory limit.</p> <pre>$ docker run -it --memory-reservation 1G ubuntu:14.04 /bin/bash\n</pre> <p>The container can use as much memory as it needs. The memory reservation setting ensures the container doesn’t consume too much memory for long time, because every memory reclaim shrinks the container’s consumption to the reservation.</p> <p>By default, kernel kills processes in a container if an out-of-memory (OOM) error occurs. To change this behaviour, use the <code>--oom-kill-disable</code> option. Only disable the OOM killer on containers where you have also set the <code>-m/--memory</code> option. If the <code>-m</code> flag is not set, this can result in the host running out of memory and require killing the host’s system processes to free memory.</p> <p>The following example limits the memory to 100M and disables the OOM killer for this container:</p> <pre>$ docker run -it -m 100M --oom-kill-disable ubuntu:14.04 /bin/bash\n</pre> <p>The following example, illustrates a dangerous way to use the flag:</p> <pre>$ docker run -it --oom-kill-disable ubuntu:14.04 /bin/bash\n</pre> <p>The container has unlimited memory which can cause the host to run out memory and require killing system processes to free memory.</p> <h3 id=\"kernel-memory-constraints\">Kernel memory constraints</h3> <p>Kernel memory is fundamentally different than user memory as kernel memory can’t be swapped out. The inability to swap makes it possible for the container to block system services by consuming too much kernel memory. Kernel memory includes：</p> <ul> <li>stack pages</li> <li>slab pages</li> <li>sockets memory pressure</li> <li>tcp memory pressure</li> </ul> <p>You can setup kernel memory limit to constrain these kinds of memory. For example, every process consumes some stack pages. By limiting kernel memory, you can prevent new processes from being created when the kernel memory usage is too high.</p> <p>Kernel memory is never completely independent of user memory. Instead, you limit kernel memory in the context of the user memory limit. Assume “U” is the user memory limit and “K” the kernel limit. There are three possible ways to set limits:</p> <table> <thead> <tr> <th>Option</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td class=\"no-wrap\">\n<strong>U != 0, K = inf</strong> (default)</td> <td> This is the standard memory limitation mechanism already present before using kernel memory. Kernel memory is completely ignored. </td> </tr> <tr> <td class=\"no-wrap\"><strong>U != 0, K &lt; U</strong></td> <td> Kernel memory is a subset of the user memory. This setup is useful in deployments where the total amount of memory per-cgroup is overcommitted. Overcommitting kernel memory limits is definitely not recommended, since the box can still run out of non-reclaimable memory. In this case, you can configure K so that the sum of all groups is never greater than the total memory. Then, freely set U at the expense of the system's service quality. </td> </tr> <tr> <td class=\"no-wrap\"><strong>U != 0, K &gt; U</strong></td> <td> Since kernel memory charges are also fed to the user counter and reclamation is triggered for the container for both kinds of memory. This configuration gives the admin a unified view of memory. It is also useful for people who just want to track kernel memory usage. </td> </tr> </tbody> </table> <p>Examples:</p> <pre>$ docker run -it -m 500M --kernel-memory 50M ubuntu:14.04 /bin/bash\n</pre> <p>We set memory and kernel memory, so the processes in the container can use 500M memory in total, in this 500M memory, it can be 50M kernel memory tops.</p> <pre>$ docker run -it --kernel-memory 50M ubuntu:14.04 /bin/bash\n</pre> <p>We set kernel memory without <strong>-m</strong>, so the processes in the container can use as much memory as they want, but they can only use 50M kernel memory.</p> <h3 id=\"swappiness-constraint\">Swappiness constraint</h3> <p>By default, a container’s kernel can swap out a percentage of anonymous pages. To set this percentage for a container, specify a <code>--memory-swappiness</code> value between 0 and 100. A value of 0 turns off anonymous page swapping. A value of 100 sets all anonymous pages as swappable. By default, if you are not using <code>--memory-swappiness</code>, memory swappiness value will be inherited from the parent.</p> <p>For example, you can set:</p> <pre>$ docker run -it --memory-swappiness=0 ubuntu:14.04 /bin/bash\n</pre> <p>Setting the <code>--memory-swappiness</code> option is helpful when you want to retain the container’s working set and to avoid swapping performance penalties.</p> <h3 id=\"cpu-share-constraint\">CPU share constraint</h3> <p>By default, all containers get the same proportion of CPU cycles. This proportion can be modified by changing the container’s CPU share weighting relative to the weighting of all other running containers.</p> <p>To modify the proportion from the default of 1024, use the <code>-c</code> or <code>--cpu-shares</code> flag to set the weighting to 2 or higher. If 0 is set, the system will ignore the value and use the default of 1024.</p> <p>The proportion will only apply when CPU-intensive processes are running. When tasks in one container are idle, other containers can use the left-over CPU time. The actual amount of CPU time will vary depending on the number of containers running on the system.</p> <p>For example, consider three containers, one has a cpu-share of 1024 and two others have a cpu-share setting of 512. When processes in all three containers attempt to use 100% of CPU, the first container would receive 50% of the total CPU time. If you add a fourth container with a cpu-share of 1024, the first container only gets 33% of the CPU. The remaining containers receive 16.5%, 16.5% and 33% of the CPU.</p> <p>On a multi-core system, the shares of CPU time are distributed over all CPU cores. Even if a container is limited to less than 100% of CPU time, it can use 100% of each individual CPU core.</p> <p>For example, consider a system with more than three cores. If you start one container <code>{C0}</code> with <code>-c=512</code> running one process, and another container <code>{C1}</code> with <code>-c=1024</code> running two processes, this can result in the following division of CPU shares:</p> <pre>PID    container    CPU CPU share\n100    {C0}     0   100% of CPU0\n101    {C1}     1   100% of CPU1\n102    {C1}     2   100% of CPU2\n</pre> <h3 id=\"cpu-period-constraint\">CPU period constraint</h3> <p>The default CPU CFS (Completely Fair Scheduler) period is 100ms. We can use <code>--cpu-period</code> to set the period of CPUs to limit the container’s CPU usage. And usually <code>--cpu-period</code> should work with <code>--cpu-quota</code>.</p> <p>Examples:</p> <pre>$ docker run -it --cpu-period=50000 --cpu-quota=25000 ubuntu:14.04 /bin/bash\n</pre> <p>If there is 1 CPU, this means the container can get 50% CPU worth of run-time every 50ms.</p> <p>For more information, see the <a href=\"https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt\">CFS documentation on bandwidth limiting</a>.</p> <h3 id=\"cpuset-constraint\">Cpuset constraint</h3> <p>We can set cpus in which to allow execution for containers.</p> <p>Examples:</p> <pre>$ docker run -it --cpuset-cpus=\"1,3\" ubuntu:14.04 /bin/bash\n</pre> <p>This means processes in container can be executed on cpu 1 and cpu 3.</p> <pre>$ docker run -it --cpuset-cpus=\"0-2\" ubuntu:14.04 /bin/bash\n</pre> <p>This means processes in container can be executed on cpu 0, cpu 1 and cpu 2.</p> <p>We can set mems in which to allow execution for containers. Only effective on NUMA systems.</p> <p>Examples:</p> <pre>$ docker run -it --cpuset-mems=\"1,3\" ubuntu:14.04 /bin/bash\n</pre> <p>This example restricts the processes in the container to only use memory from memory nodes 1 and 3.</p> <pre>$ docker run -it --cpuset-mems=\"0-2\" ubuntu:14.04 /bin/bash\n</pre> <p>This example restricts the processes in the container to only use memory from memory nodes 0, 1 and 2.</p> <h3 id=\"cpu-quota-constraint\">CPU quota constraint</h3> <p>The <code>--cpu-quota</code> flag limits the container’s CPU usage. The default 0 value allows the container to take 100% of a CPU resource (1 CPU). The CFS (Completely Fair Scheduler) handles resource allocation for executing processes and is default Linux Scheduler used by the kernel. Set this value to 50000 to limit the container to 50% of a CPU resource. For multiple CPUs, adjust the <code>--cpu-quota</code> as necessary. For more information, see the <a href=\"https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt\">CFS documentation on bandwidth limiting</a>.</p> <h3 id=\"block-io-bandwidth-blkio-constraint\">Block IO bandwidth (Blkio) constraint</h3> <p>By default, all containers get the same proportion of block IO bandwidth (blkio). This proportion is 500. To modify this proportion, change the container’s blkio weight relative to the weighting of all other running containers using the <code>--blkio-weight</code> flag.</p> <blockquote> <p><strong>Note:</strong> The blkio weight setting is only available for direct IO. Buffered IO is not currently supported.</p> </blockquote> <p>The <code>--blkio-weight</code> flag can set the weighting to a value between 10 to 1000. For example, the commands below create two containers with different blkio weight:</p> <pre>$ docker run -it --name c1 --blkio-weight 300 ubuntu:14.04 /bin/bash\n$ docker run -it --name c2 --blkio-weight 600 ubuntu:14.04 /bin/bash\n</pre> <p>If you do block IO in the two containers at the same time, by, for example:</p> <pre>$ time dd if=/mnt/zerofile of=test.out bs=1M count=1024 oflag=direct\n</pre> <p>You’ll find that the proportion of time is the same as the proportion of blkio weights of the two containers.</p> <p>The <code>--blkio-weight-device=\"DEVICE_NAME:WEIGHT\"</code> flag sets a specific device weight. The <code>DEVICE_NAME:WEIGHT</code> is a string containing a colon-separated device name and weight. For example, to set <code>/dev/sda</code> device weight to <code>200</code>:</p> <pre>$ docker run -it \\\n    --blkio-weight-device \"/dev/sda:200\" \\\n    ubuntu\n</pre> <p>If you specify both the <code>--blkio-weight</code> and <code>--blkio-weight-device</code>, Docker uses the <code>--blkio-weight</code> as the default weight and uses <code>--blkio-weight-device</code> to override this default with a new value on a specific device. The following example uses a default weight of <code>300</code> and overrides this default on <code>/dev/sda</code> setting that weight to <code>200</code>:</p> <pre>$ docker run -it \\\n    --blkio-weight 300 \\\n    --blkio-weight-device \"/dev/sda:200\" \\\n    ubuntu\n</pre> <p>The <code>--device-read-bps</code> flag limits the read rate (bytes per second) from a device. For example, this command creates a container and limits the read rate to <code>1mb</code> per second from <code>/dev/sda</code>:</p> <pre>$ docker run -it --device-read-bps /dev/sda:1mb ubuntu\n</pre> <p>The <code>--device-write-bps</code> flag limits the write rate (bytes per second)to a device. For example, this command creates a container and limits the write rate to <code>1mb</code> per second for <code>/dev/sda</code>:</p> <pre>$ docker run -it --device-write-bps /dev/sda:1mb ubuntu\n</pre> <p>Both flags take limits in the <code>&lt;device-path&gt;:&lt;limit&gt;[unit]</code> format. Both read and write rates must be a positive integer. You can specify the rate in <code>kb</code> (kilobytes), <code>mb</code> (megabytes), or <code>gb</code> (gigabytes).</p> <p>The <code>--device-read-iops</code> flag limits read rate (IO per second) from a device. For example, this command creates a container and limits the read rate to <code>1000</code> IO per second from <code>/dev/sda</code>:</p> <pre>$ docker run -ti --device-read-iops /dev/sda:1000 ubuntu\n</pre> <p>The <code>--device-write-iops</code> flag limits write rate (IO per second) to a device. For example, this command creates a container and limits the write rate to <code>1000</code> IO per second to <code>/dev/sda</code>:</p> <pre>$ docker run -ti --device-write-iops /dev/sda:1000 ubuntu\n</pre> <p>Both flags take limits in the <code>&lt;device-path&gt;:&lt;limit&gt;</code> format. Both read and write rates must be a positive integer.</p> <h2 id=\"additional-groups\">Additional groups</h2> <pre>--group-add: Add additional groups to run as \n</pre> <p>By default, the docker container process runs with the supplementary groups looked up for the specified user. If one wants to add more to that list of groups, then one can use this flag:</p> <pre>$ docker run --rm --group-add audio --group-add nogroup --group-add 777 busybox id\nuid=0(root) gid=0(root) groups=10(wheel),29(audio),99(nogroup),777\n</pre> <h2 id=\"runtime-privilege-and-linux-capabilities\">Runtime privilege and Linux capabilities</h2> <pre>--cap-add: Add Linux capabilities\n--cap-drop: Drop Linux capabilities\n--privileged=false: Give extended privileges to this container\n--device=[]: Allows you to run devices inside the container without the --privileged flag.\n</pre> <blockquote> <p><strong>Note:</strong> With Docker 1.10 and greater, the default seccomp profile will also block syscalls, regardless of <code>--cap-add</code> passed to the container. We recommend in these cases to create your own custom seccomp profile based off our <a href=\"https://github.com/docker/docker/blob/master/profiles/seccomp/default.json\">default</a>. Or if you don’t want to run with the default seccomp profile, you can pass <code>--security-opt=seccomp=unconfined</code> on run.</p> </blockquote> <p>By default, Docker containers are “unprivileged” and cannot, for example, run a Docker daemon inside a Docker container. This is because by default a container is not allowed to access any devices, but a “privileged” container is given access to all devices (see the documentation on <a href=\"https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt\">cgroups devices</a>).</p> <p>When the operator executes <code>docker run --privileged</code>, Docker will enable to access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host. Additional information about running with <code>--privileged</code> is available on the <a href=\"http://blog.docker.com/2013/09/docker-can-now-run-within-docker/\">Docker Blog</a>.</p> <p>If you want to limit access to a specific device or devices you can use the <code>--device</code> flag. It allows you to specify one or more devices that will be accessible within the container.</p> <pre>$ docker run --device=/dev/snd:/dev/snd ...\n</pre> <p>By default, the container will be able to <code>read</code>, <code>write</code>, and <code>mknod</code> these devices. This can be overridden using a third <code>:rwm</code> set of options to each <code>--device</code> flag:</p> <pre>$ docker run --device=/dev/sda:/dev/xvdc --rm -it ubuntu fdisk  /dev/xvdc\n\nCommand (m for help): q\n$ docker run --device=/dev/sda:/dev/xvdc:r --rm -it ubuntu fdisk  /dev/xvdc\nYou will not be able to write the partition table.\n\nCommand (m for help): q\n\n$ docker run --device=/dev/sda:/dev/xvdc:w --rm -it ubuntu fdisk  /dev/xvdc\n    crash....\n\n$ docker run --device=/dev/sda:/dev/xvdc:m --rm -it ubuntu fdisk  /dev/xvdc\nfdisk: unable to open /dev/xvdc: Operation not permitted\n</pre> <p>In addition to <code>--privileged</code>, the operator can have fine grain control over the capabilities using <code>--cap-add</code> and <code>--cap-drop</code>. By default, Docker has a default list of capabilities that are kept. The following table lists the Linux capability options which can be added or dropped.</p> <table> <thead> <tr> <th>Capability Key</th> <th>Capability Description</th> </tr> </thead> <tbody> <tr> <td>SETPCAP</td> <td>Modify process capabilities.</td> </tr> <tr> <td>SYS_MODULE</td> <td>Load and unload kernel modules.</td> </tr> <tr> <td>SYS_RAWIO</td> <td>Perform I/O port operations (iopl(2) and ioperm(2)).</td> </tr> <tr> <td>SYS_PACCT</td> <td>Use acct(2), switch process accounting on or off.</td> </tr> <tr> <td>SYS_ADMIN</td> <td>Perform a range of system administration operations.</td> </tr> <tr> <td>SYS_NICE</td> <td>Raise process nice value (nice(2), setpriority(2)) and change the nice value for arbitrary processes.</td> </tr> <tr> <td>SYS_RESOURCE</td> <td>Override resource Limits.</td> </tr> <tr> <td>SYS_TIME</td> <td>Set system clock (settimeofday(2), stime(2), adjtimex(2)); set real-time (hardware) clock.</td> </tr> <tr> <td>SYS_TTY_CONFIG</td> <td>Use vhangup(2); employ various privileged ioctl(2) operations on virtual terminals.</td> </tr> <tr> <td>MKNOD</td> <td>Create special files using mknod(2).</td> </tr> <tr> <td>AUDIT_WRITE</td> <td>Write records to kernel auditing log.</td> </tr> <tr> <td>AUDIT_CONTROL</td> <td>Enable and disable kernel auditing; change auditing filter rules; retrieve auditing status and filtering rules.</td> </tr> <tr> <td>MAC_OVERRIDE</td> <td>Allow MAC configuration or state changes. Implemented for the Smack LSM.</td> </tr> <tr> <td>MAC_ADMIN</td> <td>Override Mandatory Access Control (MAC). Implemented for the Smack Linux Security Module (LSM).</td> </tr> <tr> <td>NET_ADMIN</td> <td>Perform various network-related operations.</td> </tr> <tr> <td>SYSLOG</td> <td>Perform privileged syslog(2) operations.</td> </tr> <tr> <td>CHOWN</td> <td>Make arbitrary changes to file UIDs and GIDs (see chown(2)).</td> </tr> <tr> <td>NET_RAW</td> <td>Use RAW and PACKET sockets.</td> </tr> <tr> <td>DAC_OVERRIDE</td> <td>Bypass file read, write, and execute permission checks.</td> </tr> <tr> <td>FOWNER</td> <td>Bypass permission checks on operations that normally require the file system UID of the process to match the UID of the file.</td> </tr> <tr> <td>DAC_READ_SEARCH</td> <td>Bypass file read permission checks and directory read and execute permission checks.</td> </tr> <tr> <td>FSETID</td> <td>Don’t clear set-user-ID and set-group-ID permission bits when a file is modified.</td> </tr> <tr> <td>KILL</td> <td>Bypass permission checks for sending signals.</td> </tr> <tr> <td>SETGID</td> <td>Make arbitrary manipulations of process GIDs and supplementary GID list.</td> </tr> <tr> <td>SETUID</td> <td>Make arbitrary manipulations of process UIDs.</td> </tr> <tr> <td>LINUX_IMMUTABLE</td> <td>Set the FS_APPEND_FL and FS_IMMUTABLE_FL i-node flags.</td> </tr> <tr> <td>NET_BIND_SERVICE</td> <td>Bind a socket to internet domain privileged ports (port numbers less than 1024).</td> </tr> <tr> <td>NET_BROADCAST</td> <td>Make socket broadcasts, and listen to multicasts.</td> </tr> <tr> <td>IPC_LOCK</td> <td>Lock memory (mlock(2), mlockall(2), mmap(2), shmctl(2)).</td> </tr> <tr> <td>IPC_OWNER</td> <td>Bypass permission checks for operations on System V IPC objects.</td> </tr> <tr> <td>SYS_CHROOT</td> <td>Use chroot(2), change root directory.</td> </tr> <tr> <td>SYS_PTRACE</td> <td>Trace arbitrary processes using ptrace(2).</td> </tr> <tr> <td>SYS_BOOT</td> <td>Use reboot(2) and kexec_load(2), reboot and load a new kernel for later execution.</td> </tr> <tr> <td>LEASE</td> <td>Establish leases on arbitrary files (see fcntl(2)).</td> </tr> <tr> <td>SETFCAP</td> <td>Set file capabilities.</td> </tr> <tr> <td>WAKE_ALARM</td> <td>Trigger something that will wake up the system.</td> </tr> <tr> <td>BLOCK_SUSPEND</td> <td>Employ features that can block system suspend.</td> </tr> </tbody> </table> <p>Further reference information is available on the <a href=\"http://linux.die.net/man/7/capabilities\">capabilities(7) - Linux man page</a></p> <p>Both flags support the value <code>ALL</code>, so if the operator wants to have all capabilities but <code>MKNOD</code> they could use:</p> <pre>$ docker run --cap-add=ALL --cap-drop=MKNOD ...\n</pre> <p>For interacting with the network stack, instead of using <code>--privileged</code> they should use <code>--cap-add=NET_ADMIN</code> to modify the network interfaces.</p> <pre>$ docker run -it --rm  ubuntu:14.04 ip link add dummy0 type dummy\nRTNETLINK answers: Operation not permitted\n$ docker run -it --rm --cap-add=NET_ADMIN ubuntu:14.04 ip link add dummy0 type dummy\n</pre> <p>To mount a FUSE based filesystem, you need to combine both <code>--cap-add</code> and <code>--device</code>:</p> <pre>$ docker run --rm -it --cap-add SYS_ADMIN sshfs sshfs sven@10.10.10.20:/home/sven /mnt\nfuse: failed to open /dev/fuse: Operation not permitted\n$ docker run --rm -it --device /dev/fuse sshfs sshfs sven@10.10.10.20:/home/sven /mnt\nfusermount: mount failed: Operation not permitted\n$ docker run --rm -it --cap-add SYS_ADMIN --device /dev/fuse sshfs\n# sshfs sven@10.10.10.20:/home/sven /mnt\nThe authenticity of host '10.10.10.20 (10.10.10.20)' can't be established.\nECDSA key fingerprint is 25:34:85:75:25:b0:17:46:05:19:04:93:b5:dd:5f:c6.\nAre you sure you want to continue connecting (yes/no)? yes\nsven@10.10.10.20's password:\nroot@30aa0cfaf1b5:/# ls -la /mnt/src/docker\ntotal 1516\ndrwxrwxr-x 1 1000 1000   4096 Dec  4 06:08 .\ndrwxrwxr-x 1 1000 1000   4096 Dec  4 11:46 ..\n-rw-rw-r-- 1 1000 1000     16 Oct  8 00:09 .dockerignore\n-rwxrwxr-x 1 1000 1000    464 Oct  8 00:09 .drone.yml\ndrwxrwxr-x 1 1000 1000   4096 Dec  4 06:11 .git\n-rw-rw-r-- 1 1000 1000    461 Dec  4 06:08 .gitignore\n....\n</pre> <h2 id=\"logging-drivers-log-driver\">Logging drivers (--log-driver)</h2> <p>The container can have a different logging driver than the Docker daemon. Use the <code>--log-driver=VALUE</code> with the <code>docker run</code> command to configure the container’s logging driver. The following options are supported:</p> <table> <thead> <tr> <th>Driver</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>none</code></td> <td>Disables any logging for the container. <code>docker logs</code> won’t be available with this driver.</td> </tr> <tr> <td><code>json-file</code></td> <td>Default logging driver for Docker. Writes JSON messages to file. No logging options are supported for this driver.</td> </tr> <tr> <td><code>syslog</code></td> <td>Syslog logging driver for Docker. Writes log messages to syslog.</td> </tr> <tr> <td><code>journald</code></td> <td>Journald logging driver for Docker. Writes log messages to <code>journald</code>.</td> </tr> <tr> <td><code>gelf</code></td> <td>Graylog Extended Log Format (GELF) logging driver for Docker. Writes log messages to a GELF endpoint likeGraylog or Logstash.</td> </tr> <tr> <td><code>fluentd</code></td> <td>Fluentd logging driver for Docker. Writes log messages to <code>fluentd</code> (forward input).</td> </tr> <tr> <td><code>awslogs</code></td> <td>Amazon CloudWatch Logs logging driver for Docker. Writes log messages to Amazon CloudWatch Logs</td> </tr> <tr> <td><code>splunk</code></td> <td>Splunk logging driver for Docker. Writes log messages to <code>splunk</code> using Event Http Collector.</td> </tr> </tbody> </table> <p>The <code>docker logs</code> command is available only for the <code>json-file</code> and <code>journald</code> logging drivers. For detailed information on working with logging drivers, see <a href=\"../../admin/logging/overview/index\">Configure a logging driver</a>.</p> <h2 id=\"overriding-dockerfile-image-defaults\">Overriding Dockerfile image defaults</h2> <p>When a developer builds an image from a <a href=\"../builder/index\"><em>Dockerfile</em></a> or when she commits it, the developer can set a number of default parameters that take effect when the image starts up as a container.</p> <p>Four of the Dockerfile commands cannot be overridden at runtime: <code>FROM</code>, <code>MAINTAINER</code>, <code>RUN</code>, and <code>ADD</code>. Everything else has a corresponding override in <code>docker run</code>. We’ll go through what the developer might have set in each Dockerfile instruction and how the operator can override that setting.</p> <ul> <li><a href=\"#cmd-default-command-or-options\">CMD (Default Command or Options)</a></li> <li><a href=\"#entrypoint-default-command-to-execute-at-runtime\">ENTRYPOINT (Default Command to Execute at Runtime)</a></li> <li><a href=\"#expose-incoming-ports\">EXPOSE (Incoming Ports)</a></li> <li><a href=\"#env-environment-variables\">ENV (Environment Variables)</a></li> <li><a href=\"#volume-shared-filesystems\">VOLUME (Shared Filesystems)</a></li> <li><a href=\"#user\">USER</a></li> <li><a href=\"#workdir\">WORKDIR</a></li> </ul> <h3 id=\"cmd-default-command-or-options\">CMD (default command or options)</h3> <p>Recall the optional <code>COMMAND</code> in the Docker commandline:</p> <pre>$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n</pre> <p>This command is optional because the person who created the <code>IMAGE</code> may have already provided a default <code>COMMAND</code> using the Dockerfile <code>CMD</code> instruction. As the operator (the person running a container from the image), you can override that <code>CMD</code> instruction just by specifying a new <code>COMMAND</code>.</p> <p>If the image also specifies an <code>ENTRYPOINT</code> then the <code>CMD</code> or <code>COMMAND</code> get appended as arguments to the <code>ENTRYPOINT</code>.</p> <h3 id=\"entrypoint-default-command-to-execute-at-runtime\">ENTRYPOINT (default command to execute at runtime)</h3> <pre>--entrypoint=\"\": Overwrite the default entrypoint set by the image\n</pre> <p>The <code>ENTRYPOINT</code> of an image is similar to a <code>COMMAND</code> because it specifies what executable to run when the container starts, but it is (purposely) more difficult to override. The <code>ENTRYPOINT</code> gives a container its default nature or behavior, so that when you set an <code>ENTRYPOINT</code> you can run the container <em>as if it were that binary</em>, complete with default options, and you can pass in more options via the <code>COMMAND</code>. But, sometimes an operator may want to run something else inside the container, so you can override the default <code>ENTRYPOINT</code> at runtime by using a string to specify the new <code>ENTRYPOINT</code>. Here is an example of how to run a shell in a container that has been set up to automatically run something else (like <code>/usr/bin/redis-server</code>):</p> <pre>$ docker run -it --entrypoint /bin/bash example/redis\n</pre> <p>or two examples of how to pass more parameters to that ENTRYPOINT:</p> <pre>$ docker run -it --entrypoint /bin/bash example/redis -c ls -l\n$ docker run -it --entrypoint /usr/bin/redis-cli example/redis --help\n</pre> <h3 id=\"expose-incoming-ports\">EXPOSE (incoming ports)</h3> <p>The following <code>run</code> command options work with container networking:</p> <pre>--expose=[]: Expose a port or a range of ports inside the container.\n             These are additional to those exposed by the `EXPOSE` instruction\n-P         : Publish all exposed ports to the host interfaces\n-p=[]      : Publish a container᾿s port or a range of ports to the host\n               format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort\n               Both hostPort and containerPort can be specified as a\n               range of ports. When specifying ranges for both, the\n               number of container ports in the range must match the\n               number of host ports in the range, for example:\n                   -p 1234-1236:1234-1236/tcp\n\n               When specifying a range for hostPort only, the\n               containerPort must not be a range.  In this case the\n               container port is published somewhere within the\n               specified hostPort range. (e.g., `-p 1234-1236:1234/tcp`)\n\n               (use 'docker port' to see the actual mapping)\n\n--link=\"\"  : Add link to another container (&lt;name or id&gt;:alias or &lt;name or id&gt;)\n</pre> <p>With the exception of the <code>EXPOSE</code> directive, an image developer hasn’t got much control over networking. The <code>EXPOSE</code> instruction defines the initial incoming ports that provide services. These ports are available to processes inside the container. An operator can use the <code>--expose</code> option to add to the exposed ports.</p> <p>To expose a container’s internal port, an operator can start the container with the <code>-P</code> or <code>-p</code> flag. The exposed port is accessible on the host and the ports are available to any client that can reach the host.</p> <p>The <code>-P</code> option publishes all the ports to the host interfaces. Docker binds each exposed port to a random port on the host. The range of ports are within an <em>ephemeral port range</em> defined by <code>/proc/sys/net/ipv4/ip_local_port_range</code>. Use the <code>-p</code> flag to explicitly map a single port or range of ports.</p> <p>The port number inside the container (where the service listens) does not need to match the port number exposed on the outside of the container (where clients connect). For example, inside the container an HTTP service is listening on port 80 (and so the image developer specifies <code>EXPOSE 80</code> in the Dockerfile). At runtime, the port might be bound to 42800 on the host. To find the mapping between the host ports and the exposed ports, use <code>docker port</code>.</p> <p>If the operator uses <code>--link</code> when starting a new client container in the default bridge network, then the client container can access the exposed port via a private networking interface. If <code>--link</code> is used when starting a container in a user-defined network as described in <a href=\"../../userguide/networking/index\"><em>Docker network overview</em></a>), it will provide a named alias for the container being linked to.</p> <h3 id=\"env-environment-variables\">ENV (environment variables)</h3> <p>When a new container is created, Docker will set the following environment variables automatically:</p> <table> <tr> <th>Variable</th> <th>Value</th> </tr> <tr> <td><code>HOME</code></td> <td> Set based on the value of <code>USER</code> </td> </tr> <tr> <td><code>HOSTNAME</code></td> <td> The hostname associated with the container </td> </tr> <tr> <td><code>PATH</code></td> <td> Includes popular directories, such as :<br> <code>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</code> </td> </tr>\n<tr> <td><code>TERM</code></td> <td>\n<code>xterm</code> if the container is allocated a pseudo-TTY</td> </tr> </table> <p>Additionally, the operator can <strong>set any environment variable</strong> in the container by using one or more <code>-e</code> flags, even overriding those mentioned above, or already defined by the developer with a Dockerfile <code>ENV</code>:</p> <pre>$ docker run -e \"deep=purple\" --rm ubuntu /bin/bash -c export\ndeclare -x HOME=\"/\"\ndeclare -x HOSTNAME=\"85bc26a0e200\"\ndeclare -x OLDPWD\ndeclare -x PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\ndeclare -x PWD=\"/\"\ndeclare -x SHLVL=\"1\"\ndeclare -x deep=\"purple\"\n</pre> <p>Similarly the operator can set the <strong>hostname</strong> with <code>-h</code>.</p> <h3 id=\"tmpfs-mount-tmpfs-filesystems\">TMPFS (mount tmpfs filesystems)</h3> <pre>--tmpfs=[]: Create a tmpfs mount with: container-dir[:&lt;options&gt;],\n            where the options are identical to the Linux\n            'mount -t tmpfs -o' command.\n</pre> <p>The example below mounts an empty tmpfs into the container with the <code>rw</code>, <code>noexec</code>, <code>nosuid</code>, and <code>size=65536k</code> options.</p> <pre>$ docker run -d --tmpfs /run:rw,noexec,nosuid,size=65536k my_image\n</pre> <h3 id=\"volume-shared-filesystems\">VOLUME (shared filesystems)</h3> <pre>-v, --volume=[host-src:]container-dest[:&lt;options&gt;]: Bind mount a volume.\nThe comma-delimited `options` are [rw|ro], [z|Z],\n[[r]shared|[r]slave|[r]private], and [nocopy].\nThe 'host-src' is an absolute path or a name value.\n\nIf neither 'rw' or 'ro' is specified then the volume is mounted in\nread-write mode.\n\nThe `nocopy` modes is used to disable automatic copying requested volume\npath in the container to the volume storage location.\nFor named volumes, `copy` is the default mode. Copy modes are not supported\nfor bind-mounted volumes.\n\n--volumes-from=\"\": Mount all volumes from the given container(s)\n</pre> <blockquote> <p><strong>Note</strong>: When using systemd to manage the Docker daemon’s start and stop, in the systemd unit file there is an option to control mount propagation for the Docker daemon itself, called <code>MountFlags</code>. The value of this setting may cause Docker to not see mount propagation changes made on the mount point. For example, if this value is <code>slave</code>, you may not be able to use the <code>shared</code> or <code>rshared</code> propagation on a volume.</p> </blockquote> <p>The volumes commands are complex enough to have their own documentation in section <a href=\"../../userguide/containers/dockervolumes/index\"><em>Managing data in containers</em></a>. A developer can define one or more <code>VOLUME</code>’s associated with an image, but only the operator can give access from one container to another (or from a container to a volume mounted on the host).</p> <p>The <code>container-dest</code> must always be an absolute path such as <code>/src/docs</code>. The <code>host-src</code> can either be an absolute path or a <code>name</code> value. If you supply an absolute path for the <code>host-dir</code>, Docker bind-mounts to the path you specify. If you supply a <code>name</code>, Docker creates a named volume by that <code>name</code>.</p> <p>A <code>name</code> value must start with an alphanumeric character, followed by <code>a-z0-9</code>, <code>_</code> (underscore), <code>.</code> (period) or <code>-</code> (hyphen). An absolute path starts with a <code>/</code> (forward slash).</p> <p>For example, you can specify either <code>/foo</code> or <code>foo</code> for a <code>host-src</code> value. If you supply the <code>/foo</code> value, Docker creates a bind-mount. If you supply the <code>foo</code> specification, Docker creates a named volume.</p> <h3 id=\"user\">USER</h3> <p><code>root</code> (id = 0) is the default user within a container. The image developer can create additional users. Those users are accessible by name. When passing a numeric ID, the user does not have to exist in the container.</p> <p>The developer can set a default user to run the first process with the Dockerfile <code>USER</code> instruction. When starting a container, the operator can override the <code>USER</code> instruction by passing the <code>-u</code> option.</p> <pre>-u=\"\", --user=\"\": Sets the username or UID used and optionally the groupname or GID for the specified command.\n\nThe followings examples are all valid:\n--user=[ user | user:group | uid | uid:gid | user:gid | uid:group ]\n</pre> <blockquote> <p><strong>Note:</strong> if you pass a numeric uid, it must be in the range of 0-2147483647.</p> </blockquote> <h3 id=\"workdir\">WORKDIR</h3> <p>The default working directory for running binaries within a container is the root directory (<code>/</code>), but the developer can set a different default with the Dockerfile <code>WORKDIR</code> command. The operator can override this with:</p> <pre>-w=\"\": Working directory inside the container\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/run/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/run/</a>\n  </p>\n</div>\n","engine/reference/commandline/build/index":"<h1 id=\"build\">build</h1> <pre>Usage: docker build [OPTIONS] PATH | URL | -\n\nBuild a new image from the source code at PATH\n\n  --build-arg=[]                  Set build-time variables\n  --cpu-shares                    CPU Shares (relative weight)\n  --cgroup-parent=\"\"              Optional parent cgroup for the container\n  --cpu-period=0                  Limit the CPU CFS (Completely Fair Scheduler) period\n  --cpu-quota=0                   Limit the CPU CFS (Completely Fair Scheduler) quota\n  --cpuset-cpus=\"\"                CPUs in which to allow execution, e.g. `0-3`, `0,1`\n  --cpuset-mems=\"\"                MEMs in which to allow execution, e.g. `0-3`, `0,1`\n  --disable-content-trust=true    Skip image verification\n  -f, --file=\"\"                   Name of the Dockerfile (Default is 'PATH/Dockerfile')\n  --force-rm                      Always remove intermediate containers\n  --help                          Print usage\n  --isolation=\"\"                  Container isolation technology\n  --label=[]                      Set metadata for an image\n  -m, --memory=\"\"                 Memory limit for all build containers\n  --memory-swap=\"\"                A positive integer equal to memory plus swap. Specify -1 to enable unlimited swap.\n  --no-cache                      Do not use cache when building the image\n  --pull                          Always attempt to pull a newer version of the image\n  -q, --quiet                     Suppress the build output and print image ID on success\n  --rm=true                       Remove intermediate containers after a successful build\n  --shm-size=[]                   Size of `/dev/shm`. The format is `&lt;number&gt;&lt;unit&gt;`. `number` must be greater than `0`.  Unit is optional and can be `b` (bytes), `k` (kilobytes), `m` (megabytes), or `g` (gigabytes). If you omit the unit, the system uses bytes. If you omit the size entirely, the system uses `64m`.\n  -t, --tag=[]                    Name and optionally a tag in the 'name:tag' format\n  --ulimit=[]                     Ulimit options\n</pre> <p>Builds Docker images from a Dockerfile and a “context”. A build’s context is the files located in the specified <code>PATH</code> or <code>URL</code>. The build process can refer to any of the files in the context. For example, your build can use an <a href=\"../../builder/index#add\"><em>ADD</em></a> instruction to reference a file in the context.</p> <p>The <code>URL</code> parameter can specify the location of a Git repository; the repository acts as the build context. The system recursively clones the repository and its submodules using a <code>git clone --depth 1 --recursive</code> command. This command runs in a temporary directory on your local host. After the command succeeds, the directory is sent to the Docker daemon as the context. Local clones give you the ability to access private repositories using local user credentials, VPNs, and so forth.</p> <p>Git URLs accept context configuration in their fragment section, separated by a colon <code>:</code>. The first part represents the reference that Git will check out, this can be either a branch, a tag, or a commit SHA. The second part represents a subdirectory inside the repository that will be used as a build context.</p> <p>For example, run this command to use a directory called <code>docker</code> in the branch <code>container</code>:</p> <pre>  $ docker build https://github.com/docker/rootfs.git#container:docker\n</pre> <p>The following table represents all the valid suffixes with their build contexts:</p> <table> <thead> <tr> <th>Build Syntax Suffix</th> <th>Commit Used</th> <th>Build Context Used</th> </tr> </thead> <tbody> <tr> <td><code>myrepo.git</code></td> <td><code>refs/heads/master</code></td> <td><code>/</code></td> </tr> <tr> <td><code>myrepo.git#mytag</code></td> <td><code>refs/tags/mytag</code></td> <td><code>/</code></td> </tr> <tr> <td><code>myrepo.git#mybranch</code></td> <td><code>refs/heads/mybranch</code></td> <td><code>/</code></td> </tr> <tr> <td><code>myrepo.git#abcdef</code></td> <td><code>sha1 = abcdef</code></td> <td><code>/</code></td> </tr> <tr> <td><code>myrepo.git#:myfolder</code></td> <td><code>refs/heads/master</code></td> <td><code>/myfolder</code></td> </tr> <tr> <td><code>myrepo.git#master:myfolder</code></td> <td><code>refs/heads/master</code></td> <td><code>/myfolder</code></td> </tr> <tr> <td><code>myrepo.git#mytag:myfolder</code></td> <td><code>refs/tags/mytag</code></td> <td><code>/myfolder</code></td> </tr> <tr> <td><code>myrepo.git#mybranch:myfolder</code></td> <td><code>refs/heads/mybranch</code></td> <td><code>/myfolder</code></td> </tr> <tr> <td><code>myrepo.git#abcdef:myfolder</code></td> <td><code>sha1 = abcdef</code></td> <td><code>/myfolder</code></td> </tr> </tbody> </table> <p>Instead of specifying a context, you can pass a single Dockerfile in the <code>URL</code> or pipe the file in via <code>STDIN</code>. To pipe a Dockerfile from <code>STDIN</code>:</p> <pre>$ docker build - &lt; Dockerfile\n</pre> <p>With Powershell on Windows, you can run:</p> <pre>Get-Content Dockerfile | docker build -\n</pre> <p>If you use STDIN or specify a <code>URL</code>, the system places the contents into a file called <code>Dockerfile</code>, and any <code>-f</code>, <code>--file</code> option is ignored. In this scenario, there is no context.</p> <p>By default the <code>docker build</code> command will look for a <code>Dockerfile</code> at the root of the build context. The <code>-f</code>, <code>--file</code>, option lets you specify the path to an alternative file to use instead. This is useful in cases where the same set of files are used for multiple builds. The path must be to a file within the build context. If a relative path is specified then it must to be relative to the current directory.</p> <p>In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a <code>.dockerignore</code> file to that directory as well. For information on creating one, see the <a href=\"../../builder/index#dockerignore-file\">.dockerignore file</a>.</p> <p>If the Docker client loses connection to the daemon, the build is canceled. This happens if you interrupt the Docker client with <code>CTRL-c</code> or if the Docker client is killed for any reason. If the build initiated a pull which is still running at the time the build is cancelled, the pull is cancelled as well.</p> <h2 id=\"return-code\">Return code</h2> <p>On a successful build, a return code of success <code>0</code> will be returned. When the build fails, a non-zero failure code will be returned.</p> <p>There should be informational output of the reason for failure output to <code>STDERR</code>:</p> <pre>$ docker build -t fail .\nSending build context to Docker daemon 2.048 kB\nSending build context to Docker daemon\nStep 1 : FROM busybox\n ---&gt; 4986bf8c1536\nStep 2 : RUN exit 13\n ---&gt; Running in e26670ec7a0a\nINFO[0000] The command [/bin/sh -c exit 13] returned a non-zero code: 13\n$ echo $?\n1\n</pre> <p>See also:</p> <p><a href=\"../../builder/index\"><em>Dockerfile Reference</em></a>.</p> <h2 id=\"examples\">Examples</h2> <h3 id=\"build-with-path\">Build with PATH</h3> <pre>$ docker build .\nUploading context 10240 bytes\nStep 1 : FROM busybox\nPulling repository busybox\n ---&gt; e9aa60c60128MB/2.284 MB (100%) endpoint: https://cdn-registry-1.docker.io/v1/\nStep 2 : RUN ls -lh /\n ---&gt; Running in 9c9e81692ae9\ntotal 24\ndrwxr-xr-x    2 root     root        4.0K Mar 12  2013 bin\ndrwxr-xr-x    5 root     root        4.0K Oct 19 00:19 dev\ndrwxr-xr-x    2 root     root        4.0K Oct 19 00:19 etc\ndrwxr-xr-x    2 root     root        4.0K Nov 15 23:34 lib\nlrwxrwxrwx    1 root     root           3 Mar 12  2013 lib64 -&gt; lib\ndr-xr-xr-x  116 root     root           0 Nov 15 23:34 proc\nlrwxrwxrwx    1 root     root           3 Mar 12  2013 sbin -&gt; bin\ndr-xr-xr-x   13 root     root           0 Nov 15 23:34 sys\ndrwxr-xr-x    2 root     root        4.0K Mar 12  2013 tmp\ndrwxr-xr-x    2 root     root        4.0K Nov 15 23:34 usr\n ---&gt; b35f4035db3f\nStep 3 : CMD echo Hello world\n ---&gt; Running in 02071fceb21b\n ---&gt; f52f38b7823e\nSuccessfully built f52f38b7823e\nRemoving intermediate container 9c9e81692ae9\nRemoving intermediate container 02071fceb21b\n</pre> <p>This example specifies that the <code>PATH</code> is <code>.</code>, and so all the files in the local directory get <code>tar</code>d and sent to the Docker daemon. The <code>PATH</code> specifies where to find the files for the “context” of the build on the Docker daemon. Remember that the daemon could be running on a remote machine and that no parsing of the Dockerfile happens at the client side (where you’re running <code>docker build</code>). That means that <em>all</em> the files at <code>PATH</code> get sent, not just the ones listed to <a href=\"../../builder/index#add\"><em>ADD</em></a> in the Dockerfile.</p> <p>The transfer of context from the local machine to the Docker daemon is what the <code>docker</code> client means when you see the “Sending build context” message.</p> <p>If you wish to keep the intermediate containers after the build is complete, you must use <code>--rm=false</code>. This does not affect the build cache.</p> <h3 id=\"build-with-url\">Build with URL</h3> <pre>$ docker build github.com/creack/docker-firefox\n</pre> <p>This will clone the GitHub repository and use the cloned repository as context. The Dockerfile at the root of the repository is used as Dockerfile. Note that you can specify an arbitrary Git repository by using the <code>git://</code> or <code>git@</code> schema.</p> <h3 id=\"build-with\">Build with -</h3> <pre>$ docker build - &lt; Dockerfile\n</pre> <p>This will read a Dockerfile from <code>STDIN</code> without context. Due to the lack of a context, no contents of any local directory will be sent to the Docker daemon. Since there is no context, a Dockerfile <code>ADD</code> only works if it refers to a remote URL.</p> <pre>$ docker build - &lt; context.tar.gz\n</pre> <p>This will build an image for a compressed context read from <code>STDIN</code>. Supported formats are: bzip2, gzip and xz.</p> <h3 id=\"usage-of-dockerignore\">Usage of .dockerignore</h3> <pre>$ docker build .\nUploading context 18.829 MB\nUploading context\nStep 1 : FROM busybox\n ---&gt; 769b9341d937\nStep 2 : CMD echo Hello world\n ---&gt; Using cache\n ---&gt; 99cc1ad10469\nSuccessfully built 99cc1ad10469\n$ echo \".git\" &gt; .dockerignore\n$ docker build .\nUploading context  6.76 MB\nUploading context\nStep 1 : FROM busybox\n ---&gt; 769b9341d937\nStep 2 : CMD echo Hello world\n ---&gt; Using cache\n ---&gt; 99cc1ad10469\nSuccessfully built 99cc1ad10469\n</pre> <p>This example shows the use of the <code>.dockerignore</code> file to exclude the <code>.git</code> directory from the context. Its effect can be seen in the changed size of the uploaded context. The builder reference contains detailed information on <a href=\"../../builder/index#dockerignore-file\">creating a .dockerignore file</a></p> <h3 id=\"tag-image-t\">Tag image (-t)</h3> <pre>$ docker build -t vieux/apache:2.0 .\n</pre> <p>This will build like the previous example, but it will then tag the resulting image. The repository name will be <code>vieux/apache</code> and the tag will be <code>2.0</code></p> <p>You can apply multiple tags to an image. For example, you can apply the <code>latest</code> tag to a newly built image and add another tag that references a specific version. For example, to tag an image both as <code>whenry/fedora-jboss:latest</code> and <code>whenry/fedora-jboss:v2.1</code>, use the following:</p> <pre>$ docker build -t whenry/fedora-jboss:latest -t whenry/fedora-jboss:v2.1 .\n</pre> <h3 id=\"specify-dockerfile-f\">Specify Dockerfile (-f)</h3> <pre>$ docker build -f Dockerfile.debug .\n</pre> <p>This will use a file called <code>Dockerfile.debug</code> for the build instructions instead of <code>Dockerfile</code>.</p> <pre>$ docker build -f dockerfiles/Dockerfile.debug -t myapp_debug .\n$ docker build -f dockerfiles/Dockerfile.prod  -t myapp_prod .\n</pre> <p>The above commands will build the current build context (as specified by the <code>.</code>) twice, once using a debug version of a <code>Dockerfile</code> and once using a production version.</p> <pre>$ cd /home/me/myapp/some/dir/really/deep\n$ docker build -f /home/me/myapp/dockerfiles/debug /home/me/myapp\n$ docker build -f ../../../../dockerfiles/debug /home/me/myapp\n</pre> <p>These two <code>docker build</code> commands do the exact same thing. They both use the contents of the <code>debug</code> file instead of looking for a <code>Dockerfile</code> and will use <code>/home/me/myapp</code> as the root of the build context. Note that <code>debug</code> is in the directory structure of the build context, regardless of how you refer to it on the command line.</p> <blockquote> <p><strong>Note:</strong> <code>docker build</code> will return a <code>no such file or directory</code> error if the file or directory does not exist in the uploaded context. This may happen if there is no context, or if you specify a file that is elsewhere on the Host system. The context is limited to the current directory (and its children) for security reasons, and to ensure repeatable builds on remote Docker hosts. This is also the reason why <code>ADD ../file</code> will not work.</p> </blockquote> <h3 id=\"optional-parent-cgroup-cgroup-parent\">Optional parent cgroup (--cgroup-parent)</h3> <p>When <code>docker build</code> is run with the <code>--cgroup-parent</code> option the containers used in the build will be run with the <a href=\"../../run/index#specifying-custom-cgroups\">corresponding <code>docker run</code> flag</a>.</p> <h3 id=\"set-ulimits-in-container-ulimit\">Set ulimits in container (--ulimit)</h3> <p>Using the <code>--ulimit</code> option with <code>docker build</code> will cause each build step’s container to be started using those <a href=\"../run/index#set-ulimits-in-container-ulimit\"><code>--ulimit</code> flag values</a>.</p> <h3 id=\"set-build-time-variables-build-arg\">Set build-time variables (--build-arg)</h3> <p>You can use <code>ENV</code> instructions in a Dockerfile to define variable values. These values persist in the built image. However, often persistence is not what you want. Users want to specify variables differently depending on which host they build an image on.</p> <p>A good example is <code>http_proxy</code> or source versions for pulling intermediate files. The <code>ARG</code> instruction lets Dockerfile authors define values that users can set at build-time using the <code>--build-arg</code> flag:</p> <pre>$ docker build --build-arg HTTP_PROXY=http://10.20.30.2:1234 .\n</pre> <p>This flag allows you to pass the build-time variables that are accessed like regular environment variables in the <code>RUN</code> instruction of the Dockerfile. Also, these values don’t persist in the intermediate or final images like <code>ENV</code> values do.</p> <p>Using this flag will not alter the output you see when the <code>ARG</code> lines from the Dockerfile are echoed during the build process.</p> <p>For detailed information on using <code>ARG</code> and <code>ENV</code> instructions, see the <a href=\"../../builder/index\">Dockerfile reference</a>.</p> <h3 id=\"specify-isolation-technology-for-container-isolation\">Specify isolation technology for container (--isolation)</h3> <p>This option is useful in situations where you are running Docker containers on Windows. The <code>--isolation=&lt;value&gt;</code> option sets a container’s isolation technology. On Linux, the only supported is the <code>default</code> option which uses Linux namespaces. On Microsoft Windows, you can specify these values:</p> <table> <thead> <tr> <th>Value</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>default</code></td> <td>Use the value specified by the Docker daemon’s <code>--exec-opt</code> . If the <code>daemon</code> does not specify an isolation technology, Microsoft Windows uses <code>process</code> as its default value.</td> </tr> <tr> <td><code>process</code></td> <td>Namespace isolation only.</td> </tr> <tr> <td><code>hyperv</code></td> <td>Hyper-V hypervisor partition-based isolation.</td> </tr> </tbody> </table> <p>Specifying the <code>--isolation</code> flag without a value is the same as setting <code>--isolation=\"default\"</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/build/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/build/</a>\n  </p>\n</div>\n","engine/reference/commandline/daemon/index":"<h1 id=\"daemon\">daemon</h1> <pre>Usage: docker daemon [OPTIONS]\n\nA self-sufficient runtime for linux containers.\n\nOptions:\n  --api-cors-header=\"\"                   Set CORS headers in the remote API\n  --authorization-plugin=[]              Set authorization plugins to load\n  -b, --bridge=\"\"                        Attach containers to a network bridge\n  --bip=\"\"                               Specify network bridge IP\n  --cgroup-parent=                       Set parent cgroup for all containers\n  --cluster-store=\"\"                     URL of the distributed storage backend\n  --cluster-advertise=\"\"                 Address of the daemon instance on the cluster\n  --cluster-store-opt=map[]              Set cluster options\n  --config-file=/etc/docker/daemon.json  Daemon configuration file\n  --containerd                           Path to containerd socket\n  -D, --debug                            Enable debug mode\n  --default-gateway=\"\"                   Container default gateway IPv4 address\n  --default-gateway-v6=\"\"                Container default gateway IPv6 address\n  --dns=[]                               DNS server to use\n  --dns-opt=[]                           DNS options to use\n  --dns-search=[]                        DNS search domains to use\n  --default-ulimit=[]                    Set default ulimit settings for containers\n  --exec-opt=[]                          Set runtime execution options\n  --exec-root=\"/var/run/docker\"          Root directory for execution state files\n  --fixed-cidr=\"\"                        IPv4 subnet for fixed IPs\n  --fixed-cidr-v6=\"\"                     IPv6 subnet for fixed IPs\n  -G, --group=\"docker\"                   Group for the unix socket\n  -g, --graph=\"/var/lib/docker\"          Root of the Docker runtime\n  -H, --host=[]                          Daemon socket(s) to connect to\n  --help                                 Print usage\n  --icc=true                             Enable inter-container communication\n  --insecure-registry=[]                 Enable insecure registry communication\n  --ip=0.0.0.0                           Default IP when binding container ports\n  --ip-forward=true                      Enable net.ipv4.ip_forward\n  --ip-masq=true                         Enable IP masquerading\n  --iptables=true                        Enable addition of iptables rules\n  --ipv6                                 Enable IPv6 networking\n  -l, --log-level=\"info\"                 Set the logging level\n  --label=[]                             Set key=value labels to the daemon\n  --log-driver=\"json-file\"               Default driver for container logs\n  --log-opt=[]                           Log driver specific options\n  --mtu=0                                Set the containers network MTU\n  --disable-legacy-registry              Do not contact legacy registries\n  -p, --pidfile=\"/var/run/docker.pid\"    Path to use for daemon PID file\n  --raw-logs                             Full timestamps without ANSI coloring\n  --registry-mirror=[]                   Preferred Docker registry mirror\n  -s, --storage-driver=\"\"                Storage driver to use\n  --selinux-enabled                      Enable selinux support\n  --storage-opt=[]                       Set storage driver options\n  --tls                                  Use TLS; implied by --tlsverify\n  --tlscacert=\"~/.docker/ca.pem\"         Trust certs signed only by this CA\n  --tlscert=\"~/.docker/cert.pem\"         Path to TLS certificate file\n  --tlskey=\"~/.docker/key.pem\"           Path to TLS key file\n  --tlsverify                            Use TLS and verify the remote\n  --userns-remap=\"default\"               Enable user namespace remapping\n  --userland-proxy=true                  Use userland proxy for loopback traffic\n</pre> <p>Options with [] may be specified multiple times.</p> <p>The Docker daemon is the persistent process that manages containers. Docker uses the same binary for both the daemon and client. To run the daemon you type <code>docker daemon</code>.</p> <p>To run the daemon with debug output, use <code>docker daemon -D</code>.</p> <h2 id=\"daemon-socket-option\">Daemon socket option</h2> <p>The Docker daemon can listen for <a href=\"../../api/docker_remote_api/index\">Docker Remote API</a> requests via three different types of Socket: <code>unix</code>, <code>tcp</code>, and <code>fd</code>.</p> <p>By default, a <code>unix</code> domain socket (or IPC socket) is created at <code>/var/run/docker.sock</code>, requiring either <code>root</code> permission, or <code>docker</code> group membership.</p> <p>If you need to access the Docker daemon remotely, you need to enable the <code>tcp</code> Socket. Beware that the default setup provides un-encrypted and un-authenticated direct access to the Docker daemon - and should be secured either using the <a href=\"../../../security/https/index\">built in HTTPS encrypted socket</a>, or by putting a secure web proxy in front of it. You can listen on port <code>2375</code> on all network interfaces with <code>-H tcp://0.0.0.0:2375</code>, or on a particular network interface using its IP address: <code>-H tcp://192.168.59.103:2375</code>. It is conventional to use port <code>2375</code> for un-encrypted, and port <code>2376</code> for encrypted communication with the daemon.</p> <blockquote> <p><strong>Note:</strong> If you’re using an HTTPS encrypted socket, keep in mind that only TLS1.0 and greater are supported. Protocols SSLv3 and under are not supported anymore for security reasons.</p> </blockquote> <p>On Systemd based systems, you can communicate with the daemon via <a href=\"http://0pointer.de/blog/projects/socket-activation.html\">Systemd socket activation</a>, use <code>docker daemon -H fd://</code>. Using <code>fd://</code> will work perfectly for most setups but you can also specify individual sockets: <code>docker daemon -H fd://3</code>. If the specified socket activated files aren’t found, then Docker will exit. You can find examples of using Systemd socket activation with Docker and Systemd in the <a href=\"https://github.com/docker/docker/tree/master/contrib/init/systemd/\">Docker source tree</a>.</p> <p>You can configure the Docker daemon to listen to multiple sockets at the same time using multiple <code>-H</code> options:</p> <pre># listen using the default unix socket, and on 2 specific IP addresses on this host.\ndocker daemon -H unix:///var/run/docker.sock -H tcp://192.168.59.106 -H tcp://10.10.10.2\n</pre> <p>The Docker client will honor the <code>DOCKER_HOST</code> environment variable to set the <code>-H</code> flag for the client.</p> <pre>$ docker -H tcp://0.0.0.0:2375 ps\n# or\n$ export DOCKER_HOST=\"tcp://0.0.0.0:2375\"\n$ docker ps\n# both are equal\n</pre> <p>Setting the <code>DOCKER_TLS_VERIFY</code> environment variable to any value other than the empty string is equivalent to setting the <code>--tlsverify</code> flag. The following are equivalent:</p> <pre>$ docker --tlsverify ps\n# or\n$ export DOCKER_TLS_VERIFY=1\n$ docker ps\n</pre> <p>The Docker client will honor the <code>HTTP_PROXY</code>, <code>HTTPS_PROXY</code>, and <code>NO_PROXY</code> environment variables (or the lowercase versions thereof). <code>HTTPS_PROXY</code> takes precedence over <code>HTTP_PROXY</code>.</p> <h3 id=\"daemon-storage-driver-option\">Daemon storage-driver option</h3> <p>The Docker daemon has support for several different image layer storage drivers: <code>aufs</code>, <code>devicemapper</code>, <code>btrfs</code>, <code>zfs</code> and <code>overlay</code>.</p> <p>The <code>aufs</code> driver is the oldest, but is based on a Linux kernel patch-set that is unlikely to be merged into the main kernel. These are also known to cause some serious kernel crashes. However, <code>aufs</code> is also the only storage driver that allows containers to share executable and shared library memory, so is a useful choice when running thousands of containers with the same program or libraries.</p> <p>The <code>devicemapper</code> driver uses thin provisioning and Copy on Write (CoW) snapshots. For each devicemapper graph location – typically <code>/var/lib/docker/devicemapper</code> – a thin pool is created based on two block devices, one for data and one for metadata. By default, these block devices are created automatically by using loopback mounts of automatically created sparse files. Refer to <a href=\"#storage-driver-options\">Storage driver options</a> below for a way how to customize this setup. <a href=\"http://jpetazzo.github.io/2014/01/29/docker-device-mapper-resize/\">~jpetazzo/Resizing Docker containers with the Device Mapper plugin</a> article explains how to tune your existing setup without the use of options.</p> <p>The <code>btrfs</code> driver is very fast for <code>docker build</code> - but like <code>devicemapper</code> does not share executable memory between devices. Use <code>docker daemon -s btrfs -g /mnt/btrfs_partition</code>.</p> <p>The <code>zfs</code> driver is probably not as fast as <code>btrfs</code> but has a longer track record on stability. Thanks to <code>Single Copy ARC</code> shared blocks between clones will be cached only once. Use <code>docker daemon -s zfs</code>. To select a different zfs filesystem set <code>zfs.fsname</code> option as described in <a href=\"#storage-driver-options\">Storage driver options</a>.</p> <p>The <code>overlay</code> is a very fast union filesystem. It is now merged in the main Linux kernel as of <a href=\"https://lkml.org/lkml/2014/10/26/137\">3.18.0</a>. Call <code>docker daemon -s overlay</code> to use it.</p> <blockquote> <p><strong>Note:</strong> As promising as <code>overlay</code> is, the feature is still quite young and should not be used in production. Most notably, using <code>overlay</code> can cause excessive inode consumption (especially as the number of images grows), as well as being incompatible with the use of RPMs.</p> <p><strong>Note:</strong> It is currently unsupported on <code>btrfs</code> or any Copy on Write filesystem and should only be used over <code>ext4</code> partitions.</p> </blockquote> <h3 id=\"storage-driver-options\">Storage driver options</h3> <p>Particular storage-driver can be configured with options specified with <code>--storage-opt</code> flags. Options for <code>devicemapper</code> are prefixed with <code>dm</code> and options for <code>zfs</code> start with <code>zfs</code>.</p> <ul> <li>\n<p><code>dm.thinpooldev</code></p> <p>Specifies a custom block storage device to use for the thin pool.</p> <p>If using a block device for device mapper storage, it is best to use <code>lvm</code> to create and manage the thin-pool volume. This volume is then handed to Docker to exclusively create snapshot volumes needed for images and containers.</p> <p>Managing the thin-pool outside of Engine makes for the most feature-rich method of having Docker utilize device mapper thin provisioning as the backing storage for Docker containers. The highlights of the lvm-based thin-pool management feature include: automatic or interactive thin-pool resize support, dynamically changing thin-pool features, automatic thinp metadata checking when lvm activates the thin-pool, etc.</p> <p>As a fallback if no thin pool is provided, loopback files are created. Loopback is very slow, but can be used without any pre-configuration of storage. It is strongly recommended that you do not use loopback in production. Ensure your Engine daemon has a <code>--storage-opt dm.thinpooldev</code> argument provided.</p> <p>Example use:</p> <pre>$ docker daemon \\\n      --storage-opt dm.thinpooldev=/dev/mapper/thin-pool\n</pre>\n</li> <li>\n<p><code>dm.basesize</code></p> <p>Specifies the size to use when creating the base device, which limits the size of images and containers. The default value is 10G. Note, thin devices are inherently “sparse”, so a 10G device which is mostly empty doesn’t use 10 GB of space on the pool. However, the filesystem will use more space for the empty case the larger the device is.</p> <p>The base device size can be increased at daemon restart which will allow all future images and containers (based on those new images) to be of the new base device size.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.basesize=50G\n</pre> <p>This will increase the base device size to 50G. The Docker daemon will throw an error if existing base device size is larger than 50G. A user can use this option to expand the base device size however shrinking is not permitted.</p> <p>This value affects the system-wide “base” empty filesystem that may already be initialized and inherited by pulled images. Typically, a change to this value requires additional steps to take effect:</p> <pre>$ sudo service docker stop\n$ sudo rm -rf /var/lib/docker\n$ sudo service docker start\n</pre> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.basesize=20G\n</pre>\n</li> <li>\n<p><code>dm.loopdatasize</code></p> <blockquote> <p><strong>Note</strong>: This option configures devicemapper loopback, which should not be used in production.</p> </blockquote> <p>Specifies the size to use when creating the loopback file for the “data” device which is used for the thin pool. The default size is 100G. The file is sparse, so it will not initially take up this much space.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.loopdatasize=200G\n</pre>\n</li> <li>\n<p><code>dm.loopmetadatasize</code></p> <blockquote> <p><strong>Note</strong>: This option configures devicemapper loopback, which should not be used in production.</p> </blockquote> <p>Specifies the size to use when creating the loopback file for the “metadata” device which is used for the thin pool. The default size is 2G. The file is sparse, so it will not initially take up this much space.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.loopmetadatasize=4G\n</pre>\n</li> <li>\n<p><code>dm.fs</code></p> <p>Specifies the filesystem type to use for the base device. The supported options are “ext4” and “xfs”. The default is “xfs”</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.fs=ext4\n</pre>\n</li> <li>\n<p><code>dm.mkfsarg</code></p> <p>Specifies extra mkfs arguments to be used when creating the base device.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt \"dm.mkfsarg=-O ^has_journal\"\n</pre>\n</li> <li>\n<p><code>dm.mountopt</code></p> <p>Specifies extra mount options used when mounting the thin devices.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.mountopt=nodiscard\n</pre>\n</li> <li>\n<p><code>dm.datadev</code></p> <p>(Deprecated, use <code>dm.thinpooldev</code>)</p> <p>Specifies a custom blockdevice to use for data for the thin pool.</p> <p>If using a block device for device mapper storage, ideally both datadev and metadatadev should be specified to completely avoid using the loopback device.</p> <p>Example use:</p> <pre>$ docker daemon \\\n      --storage-opt dm.datadev=/dev/sdb1 \\\n      --storage-opt dm.metadatadev=/dev/sdc1\n</pre>\n</li> <li>\n<p><code>dm.metadatadev</code></p> <p>(Deprecated, use <code>dm.thinpooldev</code>)</p> <p>Specifies a custom blockdevice to use for metadata for the thin pool.</p> <p>For best performance the metadata should be on a different spindle than the data, or even better on an SSD.</p> <p>If setting up a new metadata pool it is required to be valid. This can be achieved by zeroing the first 4k to indicate empty metadata, like this:</p> <pre>$ dd if=/dev/zero of=$metadata_dev bs=4096 count=1\n</pre> <p>Example use:</p> <pre>$ docker daemon \\\n      --storage-opt dm.datadev=/dev/sdb1 \\\n      --storage-opt dm.metadatadev=/dev/sdc1\n</pre>\n</li> <li>\n<p><code>dm.blocksize</code></p> <p>Specifies a custom blocksize to use for the thin pool. The default blocksize is 64K.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.blocksize=512K\n</pre>\n</li> <li>\n<p><code>dm.blkdiscard</code></p> <p>Enables or disables the use of blkdiscard when removing devicemapper devices. This is enabled by default (only) if using loopback devices and is required to resparsify the loopback file on image/container removal.</p> <p>Disabling this on loopback can lead to <em>much</em> faster container removal times, but will make the space used in <code>/var/lib/docker</code> directory not be returned to the system for other use when containers are removed.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.blkdiscard=false\n</pre>\n</li> <li>\n<p><code>dm.override_udev_sync_check</code></p> <p>Overrides the <code>udev</code> synchronization checks between <code>devicemapper</code> and <code>udev</code>. <code>udev</code> is the device manager for the Linux kernel.</p> <p>To view the <code>udev</code> sync support of a Docker daemon that is using the <code>devicemapper</code> driver, run:</p> <pre>$ docker info\n[...]\nUdev Sync Supported: true\n[...]\n</pre> <p>When <code>udev</code> sync support is <code>true</code>, then <code>devicemapper</code> and udev can coordinate the activation and deactivation of devices for containers.</p> <p>When <code>udev</code> sync support is <code>false</code>, a race condition occurs between the<code>devicemapper</code> and <code>udev</code> during create and cleanup. The race condition results in errors and failures. (For information on these failures, see <a href=\"https://github.com/docker/docker/issues/4036\">docker#4036</a>)</p> <p>To allow the <code>docker</code> daemon to start, regardless of <code>udev</code> sync not being supported, set <code>dm.override_udev_sync_check</code> to true:</p> <pre>$ docker daemon --storage-opt dm.override_udev_sync_check=true\n</pre> <p>When this value is <code>true</code>, the <code>devicemapper</code> continues and simply warns you the errors are happening.</p> <blockquote> <p><strong>Note:</strong> The ideal is to pursue a <code>docker</code> daemon and environment that does support synchronizing with <code>udev</code>. For further discussion on this topic, see <a href=\"https://github.com/docker/docker/issues/4036\">docker#4036</a>. Otherwise, set this flag for migrating existing Docker daemons to a daemon with a supported environment.</p> </blockquote>\n</li> <li>\n<p><code>dm.use_deferred_removal</code></p> <p>Enables use of deferred device removal if <code>libdm</code> and the kernel driver support the mechanism.</p> <p>Deferred device removal means that if device is busy when devices are being removed/deactivated, then a deferred removal is scheduled on device. And devices automatically go away when last user of the device exits.</p> <p>For example, when a container exits, its associated thin device is removed. If that device has leaked into some other mount namespace and can’t be removed, the container exit still succeeds and this option causes the system to schedule the device for deferred removal. It does not wait in a loop trying to remove a busy device.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.use_deferred_removal=true\n</pre>\n</li> <li>\n<p><code>dm.use_deferred_deletion</code></p> <p>Enables use of deferred device deletion for thin pool devices. By default, thin pool device deletion is synchronous. Before a container is deleted, the Docker daemon removes any associated devices. If the storage driver can not remove a device, the container deletion fails and daemon returns.</p> <pre>Error deleting container: Error response from daemon: Cannot destroy container\n</pre> <p>To avoid this failure, enable both deferred device deletion and deferred device removal on the daemon.</p> <pre>$ docker daemon \\\n      --storage-opt dm.use_deferred_deletion=true \\\n      --storage-opt dm.use_deferred_removal=true\n</pre> <p>With these two options enabled, if a device is busy when the driver is deleting a container, the driver marks the device as deleted. Later, when the device isn’t in use, the driver deletes it.</p> <p>In general it should be safe to enable this option by default. It will help when unintentional leaking of mount point happens across multiple mount namespaces.</p>\n</li> <li>\n<p><code>dm.min_free_space</code></p> <p>Specifies the min free space percent in a thin pool require for new device creation to succeed. This check applies to both free data space as well as free metadata space. Valid values are from 0% - 99%. Value 0% disables free space checking logic. If user does not specify a value for this option, the Engine uses a default value of 10%.</p> <p>Whenever a new a thin pool device is created (during <code>docker pull</code> or during container creation), the Engine checks if the minimum free space is available. If sufficient space is unavailable, then device creation fails and any relevant <code>docker</code> operation fails.</p> <p>To recover from this error, you must create more free space in the thin pool to recover from the error. You can create free space by deleting some images and containers from the thin pool. You can also add more storage to the thin pool.</p> <p>To add more space to a LVM (logical volume management) thin pool, just add more storage to the volume group container thin pool; this should automatically resolve any errors. If your configuration uses loop devices, then stop the Engine daemon, grow the size of loop files and restart the daemon to resolve the issue.</p> <p>Example use:</p> <pre>$ docker daemon --storage-opt dm.min_free_space=10%\n</pre>\n</li> </ul> <p>Currently supported options of <code>zfs</code>:</p> <ul> <li>\n<p><code>zfs.fsname</code></p> <p>Set zfs filesystem under which docker will create its own datasets. By default docker will pick up the zfs filesystem where docker graph (<code>/var/lib/docker</code>) is located.</p> <p>Example use:</p> <pre>$ docker daemon -s zfs --storage-opt zfs.fsname=zroot/docker\n</pre>\n</li> </ul> <h2 id=\"docker-runtime-execution-options\">Docker runtime execution options</h2> <p>The Docker daemon relies on a <a href=\"https://github.com/opencontainers/specs\">OCI</a> compliant runtime (invoked via the <code>containerd</code> daemon) as its interface to the Linux kernel <code>namespaces</code>, <code>cgroups</code>, and <code>SELinux</code>.</p> <h2 id=\"options-for-the-runtime\">Options for the runtime</h2> <p>You can configure the runtime using options specified with the <code>--exec-opt</code> flag. All the flag’s options have the <code>native</code> prefix. A single <code>native.cgroupdriver</code> option is available.</p> <p>The <code>native.cgroupdriver</code> option specifies the management of the container’s cgroups. You can specify only specify <code>cgroupfs</code> or <code>systemd</code>. If you specify <code>systemd</code> and it is not available, the system errors out. If you omit the <code>native.cgroupdriver</code> option,<code>cgroupfs</code> is used.</p> <p>This example sets the <code>cgroupdriver</code> to <code>systemd</code>:</p> <pre>$ sudo docker daemon --exec-opt native.cgroupdriver=systemd\n</pre> <p>Setting this option applies to all containers the daemon launches.</p> <p>Also Windows Container makes use of <code>--exec-opt</code> for special purpose. Docker user can specify default container isolation technology with this, for example:</p> <pre>$ docker daemon --exec-opt isolation=hyperv\n</pre> <p>Will make <code>hyperv</code> the default isolation technology on Windows, without specifying isolation value on daemon start, Windows isolation technology will default to <code>process</code>.</p> <h2 id=\"daemon-dns-options\">Daemon DNS options</h2> <p>To set the DNS server for all Docker containers, use <code>docker daemon --dns 8.8.8.8</code>.</p> <p>To set the DNS search domain for all Docker containers, use <code>docker daemon --dns-search example.com</code>.</p> <h2 id=\"insecure-registries\">Insecure registries</h2> <p>Docker considers a private registry either secure or insecure. In the rest of this section, <em>registry</em> is used for <em>private registry</em>, and <code>myregistry:5000</code> is a placeholder example for a private registry.</p> <p>A secure registry uses TLS and a copy of its CA certificate is placed on the Docker host at <code>/etc/docker/certs.d/myregistry:5000/ca.crt</code>. An insecure registry is either not using TLS (i.e., listening on plain text HTTP), or is using TLS with a CA certificate not known by the Docker daemon. The latter can happen when the certificate was not found under <code>/etc/docker/certs.d/myregistry:5000/</code>, or if the certificate verification failed (i.e., wrong CA).</p> <p>By default, Docker assumes all, but local (see local registries below), registries are secure. Communicating with an insecure registry is not possible if Docker assumes that registry is secure. In order to communicate with an insecure registry, the Docker daemon requires <code>--insecure-registry</code> in one of the following two forms:</p> <ul> <li>\n<code>--insecure-registry myregistry:5000</code> tells the Docker daemon that myregistry:5000 should be considered insecure.</li> <li>\n<code>--insecure-registry 10.1.0.0/16</code> tells the Docker daemon that all registries whose domain resolve to an IP address is part of the subnet described by the CIDR syntax, should be considered insecure.</li> </ul> <p>The flag can be used multiple times to allow multiple registries to be marked as insecure.</p> <p>If an insecure registry is not marked as insecure, <code>docker pull</code>, <code>docker push</code>, and <code>docker search</code> will result in an error message prompting the user to either secure or pass the <code>--insecure-registry</code> flag to the Docker daemon as described above.</p> <p>Local registries, whose IP address falls in the 127.0.0.0/8 range, are automatically marked as insecure as of Docker 1.3.2. It is not recommended to rely on this, as it may change in the future.</p> <p>Enabling <code>--insecure-registry</code>, i.e., allowing un-encrypted and/or untrusted communication, can be useful when running a local registry. However, because its use creates security vulnerabilities it should ONLY be enabled for testing purposes. For increased security, users should add their CA to their system’s list of trusted CAs instead of enabling <code>--insecure-registry</code>.</p> <h2 id=\"legacy-registries\">Legacy Registries</h2> <p>Enabling <code>--disable-legacy-registry</code> forces a docker daemon to only interact with registries which support the V2 protocol. Specifically, the daemon will not attempt <code>push</code>, <code>pull</code> and <code>login</code> to v1 registries. The exception to this is <code>search</code> which can still be performed on v1 registries.</p> <h2 id=\"running-a-docker-daemon-behind-an-https-proxy\">Running a Docker daemon behind an HTTPS_PROXY</h2> <p>When running inside a LAN that uses an <code>HTTPS</code> proxy, the Docker Hub certificates will be replaced by the proxy’s certificates. These certificates need to be added to your Docker host’s configuration:</p> <ol> <li>Install the <code>ca-certificates</code> package for your distribution</li> <li>Ask your network admin for the proxy’s CA certificate and append them to <code>/etc/pki/tls/certs/ca-bundle.crt</code>\n</li> <li>Then start your Docker daemon with <code>HTTPS_PROXY=http://username:password@proxy:port/ docker daemon</code>. The <code>username:</code> and <code>password@</code> are optional - and are only needed if your proxy is set up to require authentication.</li> </ol> <p>This will only add the proxy and authentication to the Docker daemon’s requests - your <code>docker build</code>s and running containers will need extra configuration to use the proxy</p> <h2 id=\"default-ulimits\">Default Ulimits</h2> <p><code>--default-ulimit</code> allows you to set the default <code>ulimit</code> options to use for all containers. It takes the same options as <code>--ulimit</code> for <code>docker run</code>. If these defaults are not set, <code>ulimit</code> settings will be inherited, if not set on <code>docker run</code>, from the Docker daemon. Any <code>--ulimit</code> options passed to <code>docker run</code> will overwrite these defaults.</p> <p>Be careful setting <code>nproc</code> with the <code>ulimit</code> flag as <code>nproc</code> is designed by Linux to set the maximum number of processes available to a user, not to a container. For details please check the <a href=\"../run/index\">run</a> reference.</p> <h2 id=\"nodes-discovery\">Nodes discovery</h2> <p>The <code>--cluster-advertise</code> option specifies the <code>host:port</code> or <code>interface:port</code> combination that this particular daemon instance should use when advertising itself to the cluster. The daemon is reached by remote hosts through this value. If you specify an interface, make sure it includes the IP address of the actual Docker host. For Engine installation created through <code>docker-machine</code>, the interface is typically <code>eth1</code>.</p> <p>The daemon uses <a href=\"https://github.com/docker/libkv/\">libkv</a> to advertise the node within the cluster. Some key-value backends support mutual TLS. To configure the client TLS settings used by the daemon can be configured using the <code>--cluster-store-opt</code> flag, specifying the paths to PEM encoded files. For example:</p> <pre>docker daemon \\\n    --cluster-advertise 192.168.1.2:2376 \\\n    --cluster-store etcd://192.168.1.2:2379 \\\n    --cluster-store-opt kv.cacertfile=/path/to/ca.pem \\\n    --cluster-store-opt kv.certfile=/path/to/cert.pem \\\n    --cluster-store-opt kv.keyfile=/path/to/key.pem\n</pre> <p>The currently supported cluster store options are:</p> <ul> <li>\n<p><code>discovery.heartbeat</code></p> <p>Specifies the heartbeat timer in seconds which is used by the daemon as a keepalive mechanism to make sure discovery module treats the node as alive in the cluster. If not configured, the default value is 20 seconds.</p>\n</li> <li>\n<p><code>discovery.ttl</code></p> <p>Specifies the ttl (time-to-live) in seconds which is used by the discovery module to timeout a node if a valid heartbeat is not received within the configured ttl value. If not configured, the default value is 60 seconds.</p>\n</li> <li>\n<p><code>kv.cacertfile</code></p> <p>Specifies the path to a local file with PEM encoded CA certificates to trust</p>\n</li> <li>\n<p><code>kv.certfile</code></p> <p>Specifies the path to a local file with a PEM encoded certificate. This certificate is used as the client cert for communication with the Key/Value store.</p>\n</li> <li>\n<p><code>kv.keyfile</code></p> <p>Specifies the path to a local file with a PEM encoded private key. This private key is used as the client key for communication with the Key/Value store.</p>\n</li> <li>\n<p><code>kv.path</code></p> <p>Specifies the path in the Key/Value store. If not configured, the default value is ‘docker/nodes’.</p>\n</li> </ul> <h2 id=\"access-authorization\">Access authorization</h2> <p>Docker’s access authorization can be extended by authorization plugins that your organization can purchase or build themselves. You can install one or more authorization plugins when you start the Docker <code>daemon</code> using the <code>--authorization-plugin=PLUGIN_ID</code> option.</p> <pre>docker daemon --authorization-plugin=plugin1 --authorization-plugin=plugin2,...\n</pre> <p>The <code>PLUGIN_ID</code> value is either the plugin’s name or a path to its specification file. The plugin’s implementation determines whether you can specify a name or path. Consult with your Docker administrator to get information about the plugins available to you.</p> <p>Once a plugin is installed, requests made to the <code>daemon</code> through the command line or Docker’s remote API are allowed or denied by the plugin. If you have multiple plugins installed, at least one must allow the request for it to complete.</p> <p>For information about how to create an authorization plugin, see <a href=\"../../../extend/plugins_authorization/index\">authorization plugin</a> section in the Docker extend section of this documentation.</p> <h2 id=\"daemon-user-namespace-options\">Daemon user namespace options</h2> <p>The Linux kernel <a href=\"http://man7.org/linux/man-pages/man7/user_namespaces.7.html\">user namespace support</a> provides additional security by enabling a process, and therefore a container, to have a unique range of user and group IDs which are outside the traditional user and group range utilized by the host system. Potentially the most important security improvement is that, by default, container processes running as the <code>root</code> user will have expected administrative privilege (with some restrictions) inside the container but will effectively be mapped to an unprivileged <code>uid</code> on the host.</p> <p>When user namespace support is enabled, Docker creates a single daemon-wide mapping for all containers running on the same engine instance. The mappings will utilize the existing subordinate user and group ID feature available on all modern Linux distributions. The <a href=\"http://man7.org/linux/man-pages/man5/subuid.5.html\"><code>/etc/subuid</code></a> and <a href=\"http://man7.org/linux/man-pages/man5/subgid.5.html\"><code>/etc/subgid</code></a> files will be read for the user, and optional group, specified to the <code>--userns-remap</code> parameter. If you do not wish to specify your own user and/or group, you can provide <code>default</code> as the value to this flag, and a user will be created on your behalf and provided subordinate uid and gid ranges. This default user will be named <code>dockremap</code>, and entries will be created for it in <code>/etc/passwd</code> and <code>/etc/group</code> using your distro’s standard user and group creation tools.</p> <blockquote> <p><strong>Note</strong>: The single mapping per-daemon restriction is in place for now because Docker shares image layers from its local cache across all containers running on the engine instance. Since file ownership must be the same for all containers sharing the same layer content, the decision was made to map the file ownership on <code>docker pull</code> to the daemon’s user and group mappings so that there is no delay for running containers once the content is downloaded. This design preserves the same performance for <code>docker\npull</code>, <code>docker push</code>, and container startup as users expect with user namespaces disabled.</p> </blockquote> <h3 id=\"starting-the-daemon-with-user-namespaces-enabled\">Starting the daemon with user namespaces enabled</h3> <p>To enable user namespace support, start the daemon with the <code>--userns-remap</code> flag, which accepts values in the following formats:</p> <ul> <li>uid</li> <li>uid:gid</li> <li>username</li> <li>username:groupname</li> </ul> <p>If numeric IDs are provided, translation back to valid user or group names will occur so that the subordinate uid and gid information can be read, given these resources are name-based, not id-based. If the numeric ID information provided does not exist as entries in <code>/etc/passwd</code> or <code>/etc/group</code>, daemon startup will fail with an error message.</p> <blockquote> <p><strong>Note:</strong> On Fedora 22, you have to <code>touch</code> the <code>/etc/subuid</code> and <code>/etc/subgid</code> files to have ranges assigned when users are created. This must be done <em>before</em> the <code>--userns-remap</code> option is enabled. Once these files exist, the daemon can be (re)started and range assignment on user creation works properly.</p> </blockquote> <p><em>Example: starting with default Docker user management:</em></p> <pre>$ docker daemon --userns-remap=default\n</pre> <p>When <code>default</code> is provided, Docker will create - or find the existing - user and group named <code>dockremap</code>. If the user is created, and the Linux distribution has appropriate support, the <code>/etc/subuid</code> and <code>/etc/subgid</code> files will be populated with a contiguous 65536 length range of subordinate user and group IDs, starting at an offset based on prior entries in those files. For example, Ubuntu will create the following range, based on an existing user named <code>user1</code> already owning the first 65536 range:</p> <pre>$ cat /etc/subuid\nuser1:100000:65536\ndockremap:165536:65536\n</pre> <p>If you have a preferred/self-managed user with subordinate ID mappings already configured, you can provide that username or uid to the <code>--userns-remap</code> flag. If you have a group that doesn’t match the username, you may provide the <code>gid</code> or group name as well; otherwise the username will be used as the group name when querying the system for the subordinate group ID range.</p> <h3 id=\"detailed-information-on-subuid-subgid-ranges\">Detailed information on <code>subuid</code>/<code>subgid</code> ranges</h3> <p>Given potential advanced use of the subordinate ID ranges by power users, the following paragraphs define how the Docker daemon currently uses the range entries found within the subordinate range files.</p> <p>The simplest case is that only one contiguous range is defined for the provided user or group. In this case, Docker will use that entire contiguous range for the mapping of host uids and gids to the container process. This means that the first ID in the range will be the remapped root user, and the IDs above that initial ID will map host ID 1 through the end of the range.</p> <p>From the example <code>/etc/subuid</code> content shown above, the remapped root user would be uid 165536.</p> <p>If the system administrator has set up multiple ranges for a single user or group, the Docker daemon will read all the available ranges and use the following algorithm to create the mapping ranges:</p> <ol> <li>The range segments found for the particular user will be sorted by <em>start ID</em> ascending.</li> <li>Map segments will be created from each range in increasing value with a length matching the length of each segment. Therefore the range segment with the lowest numeric starting value will be equal to the remapped root, and continue up through host uid/gid equal to the range segment length. As an example, if the lowest segment starts at ID 1000 and has a length of 100, then a map of 1000 -&gt; 0 (the remapped root) up through 1100 -&gt; 100 will be created from this segment. If the next segment starts at ID 10000, then the next map will start with mapping 10000 -&gt; 101 up to the length of this second segment. This will continue until no more segments are found in the subordinate files for this user.</li> <li>If more than five range segments exist for a single user, only the first five will be utilized, matching the kernel’s limitation of only five entries in <code>/proc/self/uid_map</code> and <code>proc/self/gid_map</code>.</li> </ol> <h3 id=\"disable-user-namespace-for-a-container\">Disable user namespace for a container</h3> <p>If you enable user namespaces on the daemon, all containers are started with user namespaces enabled. In some situations you might want to disable this feature for a container, for example, to start a privileged container (see <a href=\"#user-namespace-known-restrictions\">user namespace known restrictions</a>). To enable those advanced features for a specific container use <code>--userns=host</code> in the <code>run/exec/create</code> command. This option will completely disable user namespace mapping for the container’s user.</p> <h3 id=\"user-namespace-known-restrictions\">User namespace known restrictions</h3> <p>The following standard Docker features are currently incompatible when running a Docker daemon with user namespaces enabled:</p> <ul> <li>sharing PID or NET namespaces with the host (<code>--pid=host</code> or <code>--net=host</code>)</li> <li>A <code>--readonly</code> container filesystem (this is a Linux kernel restriction against remounting with modified flags of a currently mounted filesystem when inside a user namespace)</li> <li>external (volume or graph) drivers which are unaware/incapable of using daemon user mappings</li> <li>Using <code>--privileged</code> mode flag on <code>docker run</code> (unless also specifying <code>--userns=host</code>)</li> </ul> <p>In general, user namespaces are an advanced feature and will require coordination with other capabilities. For example, if volumes are mounted from the host, file ownership will have to be pre-arranged if the user or administrator wishes the containers to have expected access to the volume contents.</p> <p>Finally, while the <code>root</code> user inside a user namespaced container process has many of the expected admin privileges that go along with being the superuser, the Linux kernel has restrictions based on internal knowledge that this is a user namespaced process. The most notable restriction that we are aware of at this time is the inability to use <code>mknod</code>. Permission will be denied for device creation even as container <code>root</code> inside a user namespace.</p> <h2 id=\"miscellaneous-options\">Miscellaneous options</h2> <p>IP masquerading uses address translation to allow containers without a public IP to talk to other machines on the Internet. This may interfere with some network topologies and can be disabled with <code>--ip-masq=false</code>.</p> <p>Docker supports softlinks for the Docker data directory (<code>/var/lib/docker</code>) and for <code>/var/lib/docker/tmp</code>. The <code>DOCKER_TMPDIR</code> and the data directory can be set like this:</p> <pre>DOCKER_TMPDIR=/mnt/disk2/tmp /usr/local/bin/docker daemon -D -g /var/lib/docker -H unix:// &gt; /var/lib/docker-machine/docker.log 2&gt;&amp;1\n# or\nexport DOCKER_TMPDIR=/mnt/disk2/tmp\n/usr/local/bin/docker daemon -D -g /var/lib/docker -H unix:// &gt; /var/lib/docker-machine/docker.log 2&gt;&amp;1\n</pre> <h2 id=\"default-cgroup-parent\">Default cgroup parent</h2> <p>The <code>--cgroup-parent</code> option allows you to set the default cgroup parent to use for containers. If this option is not set, it defaults to <code>/docker</code> for fs cgroup driver and <code>system.slice</code> for systemd cgroup driver.</p> <p>If the cgroup has a leading forward slash (<code>/</code>), the cgroup is created under the root cgroup, otherwise the cgroup is created under the daemon cgroup.</p> <p>Assuming the daemon is running in cgroup <code>daemoncgroup</code>, <code>--cgroup-parent=/foobar</code> creates a cgroup in <code>/sys/fs/cgroup/memory/foobar</code>, whereas using <code>--cgroup-parent=foobar</code> creates the cgroup in <code>/sys/fs/cgroup/memory/daemoncgroup/foobar</code></p> <p>The systemd cgroup driver has different rules for <code>--cgroup-parent</code>. Systemd represents hierarchy by slice and the name of the slice encodes the location in the tree. So <code>--cgroup-parent</code> for systemd cgroups should be a slice name. A name can consist of a dash-separated series of names, which describes the path to the slice from the root slice. For example, <code>--cgroup-parent=user-a-b.slice</code> means the memory cgroup for the container is created in <code>/sys/fs/cgroup/memory/user.slice/user-a.slice/user-a-b.slice/docker-&lt;id&gt;.scope</code>.</p> <p>This setting can also be set per container, using the <code>--cgroup-parent</code> option on <code>docker create</code> and <code>docker run</code>, and takes precedence over the <code>--cgroup-parent</code> option on the daemon.</p> <h2 id=\"daemon-configuration-file\">Daemon configuration file</h2> <p>The <code>--config-file</code> option allows you to set any configuration option for the daemon in a JSON format. This file uses the same flag names as keys, except for flags that allow several entries, where it uses the plural of the flag name, e.g., <code>labels</code> for the <code>label</code> flag. By default, docker tries to load a configuration file from <code>/etc/docker/daemon.json</code> on Linux and <code>%programdata%\\docker\\config\\daemon.json</code> on Windows.</p> <p>The options set in the configuration file must not conflict with options set via flags. The docker daemon fails to start if an option is duplicated between the file and the flags, regardless their value. We do this to avoid silently ignore changes introduced in configuration reloads. For example, the daemon fails to start if you set daemon labels in the configuration file and also set daemon labels via the <code>--label</code> flag.</p> <p>Options that are not present in the file are ignored when the daemon starts. This is a full example of the allowed configuration options in the file:</p> <pre>{\n\t\"authorization-plugins\": [],\n\t\"dns\": [],\n\t\"dns-opts\": [],\n\t\"dns-search\": [],\n\t\"exec-opts\": [],\n\t\"exec-root\": \"\",\n\t\"storage-driver\": \"\",\n\t\"storage-opts\": [],\n\t\"labels\": [],\n\t\"log-driver\": \"\",\n\t\"log-opts\": [],\n\t\"mtu\": 0,\n\t\"pidfile\": \"\",\n\t\"graph\": \"\",\n\t\"cluster-store\": \"\",\n\t\"cluster-store-opts\": {},\n\t\"cluster-advertise\": \"\",\n\t\"debug\": true,\n\t\"hosts\": [],\n\t\"log-level\": \"\",\n\t\"tls\": true,\n\t\"tlsverify\": true,\n\t\"tlscacert\": \"\",\n\t\"tlscert\": \"\",\n\t\"tlskey\": \"\",\n\t\"api-cors-headers\": \"\",\n\t\"selinux-enabled\": false,\n\t\"userns-remap\": \"\",\n\t\"group\": \"\",\n\t\"cgroup-parent\": \"\",\n\t\"default-ulimits\": {},\n\t\"ipv6\": false,\n\t\"iptables\": false,\n\t\"ip-forward\": false,\n\t\"ip-mask\": false,\n\t\"userland-proxy\": false,\n\t\"ip\": \"0.0.0.0\",\n\t\"bridge\": \"\",\n\t\"bip\": \"\",\n\t\"fixed-cidr\": \"\",\n\t\"fixed-cidr-v6\": \"\",\n\t\"default-gateway\": \"\",\n\t\"default-gateway-v6\": \"\",\n\t\"icc\": false,\n\t\"raw-logs\": false,\n\t\"registry-mirrors\": [],\n\t\"insecure-registries\": [],\n\t\"disable-legacy-registry\": false\n}\n</pre> <h3 id=\"configuration-reloading\">Configuration reloading</h3> <p>Some options can be reconfigured when the daemon is running without requiring to restart the process. We use the <code>SIGHUP</code> signal in Linux to reload, and a global event in Windows with the key <code>Global\\docker-daemon-config-$PID</code>. The options can be modified in the configuration file but still will check for conflicts with the provided flags. The daemon fails to reconfigure itself if there are conflicts, but it won’t stop execution.</p> <p>The list of currently supported options that can be reconfigured is this:</p> <ul> <li>\n<code>debug</code>: it changes the daemon to debug mode when set to true.</li> <li>\n<code>cluster-store</code>: it reloads the discovery store with the new address.</li> <li>\n<code>cluster-store-opts</code>: it uses the new options to reload the discovery store.</li> <li>\n<code>cluster-advertise</code>: it modifies the address advertised after reloading.</li> <li>\n<code>labels</code>: it replaces the daemon labels with a new set of labels.</li> </ul> <p>Updating and reloading the cluster configurations such as <code>--cluster-store</code>, <code>--cluster-advertise</code> and <code>--cluster-store-opts</code> will take effect only if these configurations were not previously configured. If <code>--cluster-store</code> has been provided in flags and <code>cluster-advertise</code> not, <code>cluster-advertise</code> can be added in the configuration file without accompanied by <code>--cluster-store</code> Configuration reload will log a warning message if it detects a change in previously configured cluster configurations.</p> <h2 id=\"running-multiple-daemons\">Running multiple daemons</h2> <blockquote> <p><strong>Note:</strong> Running multiple daemons on a single host is considered as “experimental”. The user should be aware of unsolved problems. This solution may not work properly in some cases. Solutions are currently under development and will be delivered in the near future.</p> </blockquote> <p>This section describes how to run multiple Docker daemons on a single host. To run multiple daemons, you must configure each daemon so that it does not conflict with other daemons on the same host. You can set these options either by providing them as flags, or by using a <a href=\"#daemon-configuration-file\">daemon configuration file</a>.</p> <p>The following daemon options must be configured for each daemon:</p> <pre>-b, --bridge=                          Attach containers to a network bridge\n--exec-root=/var/run/docker            Root of the Docker execdriver\n-g, --graph=/var/lib/docker            Root of the Docker runtime\n-p, --pidfile=/var/run/docker.pid      Path to use for daemon PID file\n-H, --host=[]                          Daemon socket(s) to connect to\n--config-file=/etc/docker/daemon.json  Daemon configuration file\n--tlscacert=\"~/.docker/ca.pem\"         Trust certs signed only by this CA\n--tlscert=\"~/.docker/cert.pem\"         Path to TLS certificate file\n--tlskey=\"~/.docker/key.pem\"           Path to TLS key file\n</pre> <p>When your daemons use different values for these flags, you can run them on the same host without any problems. It is very important to properly understand the meaning of those options and to use them correctly.</p> <ul> <li>The <code>-b, --bridge=</code> flag is set to <code>docker0</code> as default bridge network. It is created automatically when you install Docker. If you are not using the default, you must create and configure the bridge manually or just set it to ‘none’: <code>--bridge=none</code>\n</li> <li>\n<code>--exec-root</code> is the path where the container state is stored. The default value is <code>/var/run/docker</code>. Specify the path for your running daemon here.</li> <li>\n<code>--graph</code> is the path where images are stored. The default value is <code>/var/lib/docker</code>. To avoid any conflict with other daemons set this parameter separately for each daemon.</li> <li>\n<code>-p, --pidfile=/var/run/docker.pid</code> is the path where the process ID of the daemon is stored. Specify the path for your pid file here.</li> <li>\n<code>--host=[]</code> specifies where the Docker daemon will listen for client connections. If unspecified, it defaults to <code>/var/run/docker.sock</code>.</li> <li>\n<code>--config-file=/etc/docker/daemon.json</code> is the path where configuration file is stored. You can use it instead of daemon flags. Specify the path for each daemon.</li> <li>\n<code>--tls*</code> Docker daemon supports <code>--tlsverify</code> mode that enforces encrypted and authenticated remote connections. The <code>--tls*</code> options enable use of specific certificates for individual daemons.</li> </ul> <p>Example script for a separate “bootstrap” instance of the Docker daemon without network:</p> <pre>$ docker daemon \\\n        -H unix:///var/run/docker-bootstrap.sock \\\n        -p /var/run/docker-bootstrap.pid \\\n        --iptables=false \\\n        --ip-masq=false \\\n        --bridge=none \\\n        --graph=/var/lib/docker-bootstrap \\\n        --exec-root=/var/run/docker-bootstrap\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/daemon/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/daemon/</a>\n  </p>\n</div>\n","engine/reference/commandline/exec/index":"<h1 id=\"exec\">exec</h1> <pre>Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...]\n\nRun a command in a running container\n\n  -d, --detach               Detached mode: run command in the background\n  --detach-keys              Specify the escape key sequence used to detach a container\n  --help                     Print usage\n  -i, --interactive          Keep STDIN open even if not attached\n  --privileged               Give extended Linux capabilities to the command\n  -t, --tty                  Allocate a pseudo-TTY\n  -u, --user=                Username or UID (format: &lt;name|uid&gt;[:&lt;group|gid&gt;])\n</pre> <p>The <code>docker exec</code> command runs a new command in a running container.</p> <p>The command started using <code>docker exec</code> only runs while the container’s primary process (<code>PID 1</code>) is running, and it is not restarted if the container is restarted.</p> <p>If the container is paused, then the <code>docker exec</code> command will fail with an error:</p> <pre>$ docker pause test\ntest\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                   PORTS               NAMES\n1ae3b36715d2        ubuntu:latest       \"bash\"              17 seconds ago      Up 16 seconds (Paused)                       test\n$ docker exec test ls\nFATA[0000] Error response from daemon: Container test is paused, unpause the container before exec\n$ echo $?\n1\n</pre> <h2 id=\"examples\">Examples</h2> <pre>$ docker run --name ubuntu_bash --rm -i -t ubuntu bash\n</pre> <p>This will create a container named <code>ubuntu_bash</code> and start a Bash session.</p> <pre>$ docker exec -d ubuntu_bash touch /tmp/execWorks\n</pre> <p>This will create a new file <code>/tmp/execWorks</code> inside the running container <code>ubuntu_bash</code>, in the background.</p> <pre>$ docker exec -it ubuntu_bash bash\n</pre> <p>This will create a new Bash session in the container <code>ubuntu_bash</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/exec/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/exec/</a>\n  </p>\n</div>\n","engine/reference/commandline/export/index":"<h1 id=\"export\">export</h1> <pre>Usage: docker export [OPTIONS] CONTAINER\n\nExport the contents of a container's filesystem as a tar archive\n\n  --help             Print usage\n  -o, --output=\"\"    Write to a file, instead of STDOUT\n</pre> <p>The <code>docker export</code> command does not export the contents of volumes associated with the container. If a volume is mounted on top of an existing directory in the container, <code>docker export</code> will export the contents of the <em>underlying</em> directory, not the contents of the volume.</p> <p>Refer to <a href=\"../../../userguide/containers/dockervolumes/index#backup-restore-or-migrate-data-volumes\">Backup, restore, or migrate data volumes</a> in the user guide for examples on exporting data in a volume.</p> <h2 id=\"examples\">Examples</h2> <pre>$ docker export red_panda &gt; latest.tar\n</pre> <p>Or</p> <pre>$ docker export --output=\"latest.tar\" red_panda\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/export/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/export/</a>\n  </p>\n</div>\n","engine/reference/commandline/history/index":"<h1 id=\"history\">history</h1> <pre>Usage: docker history [OPTIONS] IMAGE\n\nShow the history of an image\n\n  -H, --human=true     Print sizes and dates in human readable format\n  --help               Print usage\n  --no-trunc           Don't truncate output\n  -q, --quiet          Only show numeric IDs\n</pre> <p>To see how the <code>docker:latest</code> image was built:</p> <pre>$ docker history docker\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n3e23a5875458        8 days ago          /bin/sh -c #(nop) ENV LC_ALL=C.UTF-8            0 B\n8578938dd170        8 days ago          /bin/sh -c dpkg-reconfigure locales &amp;&amp;    loc   1.245 MB\nbe51b77efb42        8 days ago          /bin/sh -c apt-get update &amp;&amp; apt-get install    338.3 MB\n4b137612be55        6 weeks ago         /bin/sh -c #(nop) ADD jessie.tar.xz in /        121 MB\n750d58736b4b        6 weeks ago         /bin/sh -c #(nop) MAINTAINER Tianon Gravi &lt;ad   0 B\n511136ea3c5a        9 months ago                                                        0 B                 Imported from -\n</pre> <p>To see how the <code>docker:apache</code> image was added to a container’s base image:</p> <pre>$ docker history docker:scm\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n2ac9d1098bf1        3 months ago        /bin/bash                                       241.4 MB            Added Apache to Fedora base image\n88b42ffd1f7c        5 months ago        /bin/sh -c #(nop) ADD file:1fd8d7f9f6557cafc7   373.7 MB\nc69cab00d6ef        5 months ago        /bin/sh -c #(nop) MAINTAINER Lokesh Mandvekar   0 B\n511136ea3c5a        19 months ago                                                       0 B                 Imported from -\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/history/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/history/</a>\n  </p>\n</div>\n","engine/reference/commandline/events/index":"<h1 id=\"events\">events</h1> <pre>Usage: docker events [OPTIONS]\n\nGet real time events from the server\n\n  -f, --filter=[]    Filter output based on conditions provided\n  --help             Print usage\n  --since=\"\"         Show all events created since timestamp\n  --until=\"\"         Stream events until this timestamp\n</pre> <p>Docker containers report the following events:</p> <pre>attach, commit, copy, create, destroy, die, exec_create, exec_start, export, kill, oom, pause, rename, resize, restart, start, stop, top, unpause, update\n</pre> <p>Docker images report the following events:</p> <pre>delete, import, pull, push, tag, untag\n</pre> <p>Docker volumes report the following events:</p> <pre>create, mount, unmount, destroy\n</pre> <p>Docker networks report the following events:</p> <pre>create, connect, disconnect, destroy\n</pre> <p>The <code>--since</code> and <code>--until</code> parameters can be Unix timestamps, date formatted timestamps, or Go duration strings (e.g. <code>10m</code>, <code>1h30m</code>) computed relative to the client machine’s time. If you do not provide the <code>--since</code> option, the command returns only new and/or live events. Supported formats for date formatted time stamps include RFC3339Nano, RFC3339, <code>2006-01-02T15:04:05</code>, <code>2006-01-02T15:04:05.999999999</code>, <code>2006-01-02Z07:00</code>, and <code>2006-01-02</code>. The local timezone on the client will be used if you do not provide either a <code>Z</code> or a <code>+-00:00</code> timezone offset at the end of the timestamp. When providing Unix timestamps enter seconds[.nanoseconds], where seconds is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (aka Unix epoch or Unix time), and the optional .nanoseconds field is a fraction of a second no more than nine digits long.</p> <h2 id=\"filtering\">Filtering</h2> <p>The filtering flag (<code>-f</code> or <code>--filter</code>) format is of “key=value”. If you would like to use multiple filters, pass multiple flags (e.g., <code>--filter \"foo=bar\" --filter \"bif=baz\"</code>)</p> <p>Using the same filter multiple times will be handled as a <em>OR</em>; for example <code>--filter container=588a23dac085 --filter container=a8f7720b8c22</code> will display events for container 588a23dac085 <em>OR</em> container a8f7720b8c22</p> <p>Using multiple filters will be handled as a <em>AND</em>; for example <code>--filter container=588a23dac085 --filter event=start</code> will display events for container container 588a23dac085 <em>AND</em> the event type is <em>start</em></p> <p>The currently supported filters are:</p> <ul> <li>container (<code>container=&lt;name or id&gt;</code>)</li> <li>event (<code>event=&lt;event action&gt;</code>)</li> <li>image (<code>image=&lt;tag or id&gt;</code>)</li> <li>label (<code>label=&lt;key&gt;</code> or <code>label=&lt;key&gt;=&lt;value&gt;</code>)</li> <li>type (<code>type=&lt;container or image or volume or network&gt;</code>)</li> <li>volume (<code>volume=&lt;name or id&gt;</code>)</li> <li>network (<code>network=&lt;name or id&gt;</code>)</li> </ul> <h2 id=\"examples\">Examples</h2> <p>You’ll need two shells for this example.</p> <p><strong>Shell 1: Listening for events:</strong></p> <pre>$ docker events\n</pre> <p><strong>Shell 2: Start and Stop containers:</strong></p> <pre>$ docker start 4386fb97867d\n$ docker stop 4386fb97867d\n$ docker stop 7805c1d35632\n</pre> <p><strong>Shell 1: (Again .. now showing events):</strong></p> <pre>2015-05-12T11:51:30.999999999Z07:00 container start 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T11:51:30.999999999Z07:00 container die 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:52:12.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:53:45.999999999Z07:00 container die 7805c1d35632 (image=redis:2.8)\n2015-05-12T15:54:03.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n</pre> <p><strong>Show events in the past from a specified time:</strong></p> <pre>$ docker events --since 1378216169\n2015-05-12T11:51:30.999999999Z07:00 container die 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:52:12.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:53:45.999999999Z07:00 container die 7805c1d35632 (image=redis:2.8)\n2015-05-12T15:54:03.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n\n$ docker events --since '2013-09-03'\n2015-05-12T11:51:30.999999999Z07:00 container start 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T11:51:30.999999999Z07:00 container die 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:52:12.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:53:45.999999999Z07:00 container die 7805c1d35632 (image=redis:2.8)\n2015-05-12T15:54:03.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n\n$ docker events --since '2013-09-03T15:49:29'\n2015-05-12T11:51:30.999999999Z07:00 container die 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:52:12.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:53:45.999999999Z07:00 container die 7805c1d35632 (image=redis:2.8)\n2015-05-12T15:54:03.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n</pre> <p>This example outputs all events that were generated in the last 3 minutes, relative to the current time on the client machine:</p> <pre>$ docker events --since '3m'\n2015-05-12T11:51:30.999999999Z07:00 container die 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:52:12.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n2015-05-12T15:53:45.999999999Z07:00 container die 7805c1d35632 (image=redis:2.8)\n2015-05-12T15:54:03.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n</pre> <p><strong>Filter events:</strong></p> <pre>$ docker events --filter 'event=stop'\n2014-05-10T17:42:14.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n2014-09-03T17:42:14.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n\n$ docker events --filter 'image=ubuntu-1:14.04'\n2014-05-10T17:42:14.999999999Z07:00 container start 4386fb97867d (image=ubuntu-1:14.04)\n2014-05-10T17:42:14.999999999Z07:00 container die 4386fb97867d (image=ubuntu-1:14.04)\n2014-05-10T17:42:14.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n\n$ docker events --filter 'container=7805c1d35632'\n2014-05-10T17:42:14.999999999Z07:00 container die 7805c1d35632 (image=redis:2.8)\n2014-09-03T15:49:29.999999999Z07:00 container stop 7805c1d35632 (image= redis:2.8)\n\n$ docker events --filter 'container=7805c1d35632' --filter 'container=4386fb97867d'\n2014-09-03T15:49:29.999999999Z07:00 container die 4386fb97867d (image=ubuntu-1:14.04)\n2014-05-10T17:42:14.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n2014-05-10T17:42:14.999999999Z07:00 container die 7805c1d35632 (image=redis:2.8)\n2014-09-03T15:49:29.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n\n$ docker events --filter 'container=7805c1d35632' --filter 'event=stop'\n2014-09-03T15:49:29.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n\n$ docker events --filter 'container=container_1' --filter 'container=container_2'\n2014-09-03T15:49:29.999999999Z07:00 container die 4386fb97867d (image=ubuntu-1:14.04)\n2014-05-10T17:42:14.999999999Z07:00 container stop 4386fb97867d (image=ubuntu-1:14.04)\n2014-05-10T17:42:14.999999999Z07:00 container die 7805c1d35632 (imager=redis:2.8)\n2014-09-03T15:49:29.999999999Z07:00 container stop 7805c1d35632 (image=redis:2.8)\n\n$ docker events --filter 'type=volume'\n2015-12-23T21:05:28.136212689Z volume create test-event-volume-local (driver=local)\n2015-12-23T21:05:28.383462717Z volume mount test-event-volume-local (read/write=true, container=562fe10671e9273da25eed36cdce26159085ac7ee6707105fd534866340a5025, destination=/foo, driver=local, propagation=rprivate)\n2015-12-23T21:05:28.650314265Z volume unmount test-event-volume-local (container=562fe10671e9273da25eed36cdce26159085ac7ee6707105fd534866340a5025, driver=local)\n2015-12-23T21:05:28.716218405Z volume destroy test-event-volume-local (driver=local)\n\n$ docker events --filter 'type=network'\n2015-12-23T21:38:24.705709133Z network create 8b111217944ba0ba844a65b13efcd57dc494932ee2527577758f939315ba2c5b (name=test-event-network-local, type=bridge)\n2015-12-23T21:38:25.119625123Z network connect 8b111217944ba0ba844a65b13efcd57dc494932ee2527577758f939315ba2c5b (name=test-event-network-local, container=b4be644031a3d90b400f88ab3d4bdf4dc23adb250e696b6328b85441abe2c54e, type=bridge)\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/events/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/events/</a>\n  </p>\n</div>\n","engine/reference/commandline/import/index":"<h1 id=\"import\">import</h1> <pre>Usage: docker import file|URL|- [REPOSITORY[:TAG]]\n\nCreate an empty filesystem image and import the contents of the\ntarball (.tar, .tar.gz, .tgz, .bzip, .tar.xz, .txz) into it, then\noptionally tag it.\n\n  -c, --change=[]     Apply specified Dockerfile instructions while importing the image\n  --help              Print usage\n  -m, --message=      Set commit message for imported image\n</pre> <p>You can specify a <code>URL</code> or <code>-</code> (dash) to take data directly from <code>STDIN</code>. The <code>URL</code> can point to an archive (.tar, .tar.gz, .tgz, .bzip, .tar.xz, or .txz) containing a filesystem or to an individual file on the Docker host. If you specify an archive, Docker untars it in the container relative to the <code>/</code> (root). If you specify an individual file, you must specify the full path within the host. To import from a remote location, specify a <code>URI</code> that begins with the <code>http://</code> or <code>https://</code> protocol.</p> <p>The <code>--change</code> option will apply <code>Dockerfile</code> instructions to the image that is created. Supported <code>Dockerfile</code> instructions: <code>CMD</code>|<code>ENTRYPOINT</code>|<code>ENV</code>|<code>EXPOSE</code>|<code>ONBUILD</code>|<code>USER</code>|<code>VOLUME</code>|<code>WORKDIR</code></p> <h2 id=\"examples\">Examples</h2> <p><strong>Import from a remote location:</strong></p> <p>This will create a new untagged image.</p> <pre>$ docker import http://example.com/exampleimage.tgz\n</pre> <p><strong>Import from a local file:</strong></p> <p>Import to docker via pipe and <code>STDIN</code>.</p> <pre>$ cat exampleimage.tgz | docker import - exampleimagelocal:new\n</pre> <p>Import with a commit message.</p> <pre>$ cat exampleimage.tgz | docker import --message \"New image imported from tarball\" - exampleimagelocal:new\n</pre> <p>Import to docker from a local archive.</p> <pre>$ docker import /path/to/exampleimage.tgz\n</pre> <p><strong>Import from a local directory:</strong></p> <pre>$ sudo tar -c . | docker import - exampleimagedir\n</pre> <p><strong>Import from a local directory with new configurations:</strong></p> <pre>$ sudo tar -c . | docker import --change \"ENV DEBUG true\" - exampleimagedir\n</pre> <p>Note the <code>sudo</code> in this example – you must preserve the ownership of the files (especially root ownership) during the archiving with tar. If you are not root (or the sudo command) when you tar, then the ownerships might not get preserved.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/import/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/import/</a>\n  </p>\n</div>\n","engine/reference/commandline/images/index":"<h1 id=\"images\">images</h1> <pre>Usage: docker images [OPTIONS] [REPOSITORY[:TAG]]\n\nList images\n\n  -a, --all            Show all images (default hides intermediate images)\n  --digests            Show digests\n  -f, --filter=[]      Filter output based on conditions provided\n  --help               Print usage\n  --no-trunc           Don't truncate output\n  -q, --quiet          Only show numeric IDs\n</pre> <p>The default <code>docker images</code> will show all top level images, their repository and tags, and their size.</p> <p>Docker images have intermediate layers that increase reusability, decrease disk usage, and speed up <code>docker build</code> by allowing each step to be cached. These intermediate layers are not shown by default.</p> <p>The <code>SIZE</code> is the cumulative space taken up by the image and all its parent images. This is also the disk space used by the contents of the Tar file created when you <code>docker save</code> an image.</p> <p>An image will be listed more than once if it has multiple repository names or tags. This single image (identifiable by its matching <code>IMAGE ID</code>) uses up the <code>SIZE</code> listed only once.</p> <h3 id=\"listing-the-most-recently-created-images\">Listing the most recently created images</h3> <pre>$ docker images\nREPOSITORY                TAG                 IMAGE ID            CREATED             SIZE\n&lt;none&gt;                    &lt;none&gt;              77af4d6b9913        19 hours ago        1.089 GB\ncommitt                   latest              b6fa739cedf5        19 hours ago        1.089 GB\n&lt;none&gt;                    &lt;none&gt;              78a85c484f71        19 hours ago        1.089 GB\ndocker                    latest              30557a29d5ab        20 hours ago        1.089 GB\n&lt;none&gt;                    &lt;none&gt;              5ed6274db6ce        24 hours ago        1.089 GB\npostgres                  9                   746b819f315e        4 days ago          213.4 MB\npostgres                  9.3                 746b819f315e        4 days ago          213.4 MB\npostgres                  9.3.5               746b819f315e        4 days ago          213.4 MB\npostgres                  latest              746b819f315e        4 days ago          213.4 MB\n</pre> <h3 id=\"listing-images-by-name-and-tag\">Listing images by name and tag</h3> <p>The <code>docker images</code> command takes an optional <code>[REPOSITORY[:TAG]]</code> argument that restricts the list to images that match the argument. If you specify <code>REPOSITORY</code>but no <code>TAG</code>, the <code>docker images</code> command lists all images in the given repository.</p> <p>For example, to list all images in the “java” repository, run this command :</p> <pre>$ docker images java\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\njava                8                   308e519aac60        6 days ago          824.5 MB\njava                7                   493d82594c15        3 months ago        656.3 MB\njava                latest              2711b1d6f3aa        5 months ago        603.9 MB\n</pre> <p>The <code>[REPOSITORY[:TAG]]</code> value must be an “exact match”. This means that, for example, <code>docker images jav</code> does not match the image <code>java</code>.</p> <p>If both <code>REPOSITORY</code> and <code>TAG</code> are provided, only images matching that repository and tag are listed. To find all local images in the “java” repository with tag “8” you can use:</p> <pre>$ docker images java:8\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\njava                8                   308e519aac60        6 days ago          824.5 MB\n</pre> <p>If nothing matches <code>REPOSITORY[:TAG]</code>, the list is empty.</p> <pre>$ docker images java:0\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n</pre> <h2 id=\"listing-the-full-length-image-ids\">Listing the full length image IDs</h2> <pre>$ docker images --no-trunc\nREPOSITORY                    TAG                 IMAGE ID                                                                  CREATED             SIZE\n&lt;none&gt;                        &lt;none&gt;              sha256:77af4d6b9913e693e8d0b4b294fa62ade6054e6b2f1ffb617ac955dd63fb0182   19 hours ago        1.089 GB\ncommittest                    latest              sha256:b6fa739cedf5ea12a620a439402b6004d057da800f91c7524b5086a5e4749c9f   19 hours ago        1.089 GB\n&lt;none&gt;                        &lt;none&gt;              sha256:78a85c484f71509adeaace20e72e941f6bdd2b25b4c75da8693efd9f61a37921   19 hours ago        1.089 GB\ndocker                        latest              sha256:30557a29d5abc51e5f1d5b472e79b7e296f595abcf19fe6b9199dbbc809c6ff4   20 hours ago        1.089 GB\n&lt;none&gt;                        &lt;none&gt;              sha256:0124422dd9f9cf7ef15c0617cda3931ee68346455441d66ab8bdc5b05e9fdce5   20 hours ago        1.089 GB\n&lt;none&gt;                        &lt;none&gt;              sha256:18ad6fad340262ac2a636efd98a6d1f0ea775ae3d45240d3418466495a19a81b   22 hours ago        1.082 GB\n&lt;none&gt;                        &lt;none&gt;              sha256:f9f1e26352f0a3ba6a0ff68167559f64f3e21ff7ada60366e2d44a04befd1d3a   23 hours ago        1.089 GB\ntryout                        latest              sha256:2629d1fa0b81b222fca63371ca16cbf6a0772d07759ff80e8d1369b926940074   23 hours ago        131.5 MB\n&lt;none&gt;                        &lt;none&gt;              sha256:5ed6274db6ceb2397844896966ea239290555e74ef307030ebb01ff91b1914df   24 hours ago        1.089 GB\n</pre> <h2 id=\"listing-image-digests\">Listing image digests</h2> <p>Images that use the v2 or later format have a content-addressable identifier called a <code>digest</code>. As long as the input used to generate the image is unchanged, the digest value is predictable. To list image digest values, use the <code>--digests</code> flag:</p> <pre>$ docker images --digests\nREPOSITORY                         TAG                 DIGEST                                                                    IMAGE ID            CREATED             SIZE\nlocalhost:5000/test/busybox        &lt;none&gt;              sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf   4986bf8c1536        9 weeks ago         2.43 MB\n</pre> <p>When pushing or pulling to a 2.0 registry, the <code>push</code> or <code>pull</code> command output includes the image digest. You can <code>pull</code> using a digest value. You can also reference by digest in <code>create</code>, <code>run</code>, and <code>rmi</code> commands, as well as the <code>FROM</code> image reference in a Dockerfile.</p> <h2 id=\"filtering\">Filtering</h2> <p>The filtering flag (<code>-f</code> or <code>--filter</code>) format is of “key=value”. If there is more than one filter, then pass multiple flags (e.g., <code>--filter \"foo=bar\" --filter \"bif=baz\"</code>)</p> <p>The currently supported filters are:</p> <ul> <li>dangling (boolean - true or false)</li> <li>label (<code>label=&lt;key&gt;</code> or <code>label=&lt;key&gt;=&lt;value&gt;</code>)</li> </ul> <h5 id=\"untagged-images-dangling\">Untagged images (dangling)</h5> <pre>$ docker images --filter \"dangling=true\"\n\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n&lt;none&gt;              &lt;none&gt;              8abc22fbb042        4 weeks ago         0 B\n&lt;none&gt;              &lt;none&gt;              48e5f45168b9        4 weeks ago         2.489 MB\n&lt;none&gt;              &lt;none&gt;              bf747efa0e2f        4 weeks ago         0 B\n&lt;none&gt;              &lt;none&gt;              980fe10e5736        12 weeks ago        101.4 MB\n&lt;none&gt;              &lt;none&gt;              dea752e4e117        12 weeks ago        101.4 MB\n&lt;none&gt;              &lt;none&gt;              511136ea3c5a        8 months ago        0 B\n</pre> <p>This will display untagged images, that are the leaves of the images tree (not intermediary layers). These images occur when a new build of an image takes the <code>repo:tag</code> away from the image ID, leaving it as <code>&lt;none&gt;:&lt;none&gt;</code> or untagged. A warning will be issued if trying to remove an image when a container is presently using it. By having this flag it allows for batch cleanup.</p> <p>Ready for use by <code>docker rmi ...</code>, like:</p> <pre>$ docker rmi $(docker images -f \"dangling=true\" -q)\n\n8abc22fbb042\n48e5f45168b9\nbf747efa0e2f\n980fe10e5736\ndea752e4e117\n511136ea3c5a\n</pre> <p>NOTE: Docker will warn you if any containers exist that are using these untagged images.</p> <h5 id=\"labeled-images\">Labeled images</h5> <p>The <code>label</code> filter matches images based on the presence of a <code>label</code> alone or a <code>label</code> and a value.</p> <p>The following filter matches images with the <code>com.example.version</code> label regardless of its value.</p> <pre>$ docker images --filter \"label=com.example.version\"\n\nREPOSITORY          TAG                 IMAGE ID            CREATED              SIZE\nmatch-me-1          latest              eeae25ada2aa        About a minute ago   188.3 MB\nmatch-me-2          latest              eeae25ada2aa        About a minute ago   188.3 MB\n</pre> <p>The following filter matches images with the <code>com.example.version</code> label with the <code>1.0</code> value.</p> <pre>$ docker images --filter \"label=com.example.version=1.0\"\nREPOSITORY          TAG                 IMAGE ID            CREATED              SIZE\nmatch-me            latest              eeae25ada2aa        About a minute ago   188.3 MB\n</pre> <p>In this example, with the <code>0.1</code> value, it returns an empty set because no matches were found.</p> <pre>$ docker images --filter \"label=com.example.version=0.1\"\nREPOSITORY          TAG                 IMAGE ID            CREATED              SIZE\n</pre> <h2 id=\"formatting\">Formatting</h2> <p>The formatting option (<code>--format</code>) will pretty print container output using a Go template.</p> <p>Valid placeholders for the Go template are listed below:</p> <table> <thead> <tr> <th>Placeholder</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>.ID</code></td> <td>Image ID</td> </tr> <tr> <td><code>.Repository</code></td> <td>Image repository</td> </tr> <tr> <td><code>.Tag</code></td> <td>Image tag</td> </tr> <tr> <td><code>.Digest</code></td> <td>Image digest</td> </tr> <tr> <td><code>.CreatedSince</code></td> <td>Elapsed time since the image was created.</td> </tr> <tr> <td><code>.CreatedAt</code></td> <td>Time when the image was created.</td> </tr> <tr> <td><code>.Size</code></td> <td>Image disk size.</td> </tr> </tbody> </table> <p>When using the <code>--format</code> option, the <code>image</code> command will either output the data exactly as the template declares or, when using the <code>table</code> directive, will include column headers as well.</p> <p>The following example uses a template without headers and outputs the <code>ID</code> and <code>Repository</code> entries separated by a colon for all images:</p> <pre>$ docker images --format \"{{.ID}}: {{.Repository}}\"\n77af4d6b9913: &lt;none&gt;\nb6fa739cedf5: committ\n78a85c484f71: &lt;none&gt;\n30557a29d5ab: docker\n5ed6274db6ce: &lt;none&gt;\n746b819f315e: postgres\n746b819f315e: postgres\n746b819f315e: postgres\n746b819f315e: postgres\n</pre> <p>To list all images with their repository and tag in a table format you can use:</p> <pre>$ docker images --format \"table {{.ID}}\\t{{.Repository}}\\t{{.Tag}}\"\nIMAGE ID            REPOSITORY                TAG\n77af4d6b9913        &lt;none&gt;                    &lt;none&gt;\nb6fa739cedf5        committ                   latest\n78a85c484f71        &lt;none&gt;                    &lt;none&gt;\n30557a29d5ab        docker                    latest\n5ed6274db6ce        &lt;none&gt;                    &lt;none&gt;\n746b819f315e        postgres                  9\n746b819f315e        postgres                  9.3\n746b819f315e        postgres                  9.3.5\n746b819f315e        postgres                  latest\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/images/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/images/</a>\n  </p>\n</div>\n","engine/reference/commandline/kill/index":"<h1 id=\"kill\">kill</h1> <pre>Usage: docker kill [OPTIONS] CONTAINER [CONTAINER...]\n\nKill a running container using SIGKILL or a specified signal\n\n  --help                 Print usage\n  -s, --signal=\"KILL\"    Signal to send to the container\n</pre> <p>The main process inside the container will be sent <code>SIGKILL</code>, or any signal specified with option <code>--signal</code>.</p> <blockquote> <p><strong>Note:</strong> <code>ENTRYPOINT</code> and <code>CMD</code> in the <em>shell</em> form run as a subcommand of <code>/bin/sh -c</code>, which does not pass signals. This means that the executable is not the container’s PID 1 and does not receive Unix signals.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/kill/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/kill/</a>\n  </p>\n</div>\n","engine/reference/commandline/info/index":"<h1 id=\"info\">info</h1> <pre>Usage: docker info [OPTIONS]\n\nDisplay system-wide information\n\n  --help              Print usage\n</pre> <p>For example:</p> <pre>$ docker -D info\nContainers: 14\n Running: 3\n Paused: 1\n Stopped: 10\nImages: 52\nServer Version: 1.9.0\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 545\n Dirperm1 Supported: true\nExecution Driver: native-0.2\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge null host\nKernel Version: 3.19.0-22-generic\nOSType: linux\nArchitecture: x86_64\nOperating System: Ubuntu 15.04\nCPUs: 24\nTotal Memory: 62.86 GiB\nName: docker\nID: I54V:OLXT:HVMM:TPKO:JPHQ:CQCD:JNLC:O3BZ:4ZVJ:43XJ:PFHZ:6N2S\nDocker Root Dir: /var/lib/docker\nDebug mode (client): true\nDebug mode (server): true\n File Descriptors: 59\n Goroutines: 159\n System Time: 2015-09-23T14:04:20.699842089+08:00\n EventsListeners: 0\n Init SHA1:\n Init Path: /usr/bin/docker\n Docker Root Dir: /var/lib/docker\n Http Proxy: http://test:test@localhost:8080\n Https Proxy: https://test:test@localhost:8080\nWARNING: No swap limit support\nUsername: svendowideit\nRegistry: [https://index.docker.io/v1/]\nLabels:\n storage=ssd\n</pre> <p>The global <code>-D</code> option tells all <code>docker</code> commands to output debug information.</p> <p>When sending issue reports, please use <code>docker version</code> and <code>docker -D info</code> to ensure we know how your setup is configured.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/info/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/info/</a>\n  </p>\n</div>\n","engine/reference/commandline/load/index":"<h1 id=\"load\">load</h1> <pre>Usage: docker load [OPTIONS]\n\nLoad an image from a tar archive or STDIN\n\n  --help             Print usage\n  -i, --input=\"\"     Read from a tar archive file, instead of STDIN. The tarball may be compressed with gzip, bzip, or xz\n  -q, --quiet        Suppress the load output. Without this option, a progress bar is displayed.\n</pre> <p>Loads a tarred repository from a file or the standard input stream. Restores both images and tags.</p> <pre>$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n$ docker load &lt; busybox.tar.gz\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nbusybox             latest              769b9341d937        7 weeks ago         2.489 MB\n$ docker load --input fedora.tar\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nbusybox             latest              769b9341d937        7 weeks ago         2.489 MB\nfedora              rawhide             0d20aec6529d        7 weeks ago         387 MB\nfedora              20                  58394af37342        7 weeks ago         385.5 MB\nfedora              heisenbug           58394af37342        7 weeks ago         385.5 MB\nfedora              latest              58394af37342        7 weeks ago         385.5 MB\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/load/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/load/</a>\n  </p>\n</div>\n","engine/reference/commandline/inspect/index":"<h1 id=\"inspect\">inspect</h1> <pre>Usage: docker inspect [OPTIONS] CONTAINER|IMAGE [CONTAINER|IMAGE...]\n\nReturn low-level information on a container or image\n\n  -f, --format=\"\"         Format the output using the given go template\n  --help                  Print usage\n  --type=container|image  Return JSON for specified type, permissible\n                          values are \"image\" or \"container\"\n  -s, --size              Display total file sizes if the type is container\n</pre> <p>By default, this will render all results in a JSON array. If the container and image have the same name, this will return container JSON for unspecified type. If a format is specified, the given template will be executed for each result.</p> <p>Go’s <a href=\"http://golang.org/pkg/text/template/\">text/template</a> package describes all the details of the format.</p> <h2 id=\"examples\">Examples</h2> <p><strong>Get an instance’s IP address:</strong></p> <p>For the most part, you can pick out any field from the JSON in a fairly straightforward manner.</p> <pre>$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' $INSTANCE_ID\n</pre> <p><strong>Get an instance’s MAC Address:</strong></p> <p>For the most part, you can pick out any field from the JSON in a fairly straightforward manner.</p> <pre>$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.MacAddress}}{{end}}' $INSTANCE_ID\n</pre> <p><strong>Get an instance’s log path:</strong></p> <pre>$ docker inspect --format='{{.LogPath}}' $INSTANCE_ID\n</pre> <p><strong>List All Port Bindings:</strong></p> <p>One can loop over arrays and maps in the results to produce simple text output:</p> <pre>$ docker inspect --format='{{range $p, $conf := .NetworkSettings.Ports}} {{$p}} -&gt; {{(index $conf 0).HostPort}} {{end}}' $INSTANCE_ID\n</pre> <p><strong>Find a Specific Port Mapping:</strong></p> <p>The <code>.Field</code> syntax doesn’t work when the field name begins with a number, but the template language’s <code>index</code> function does. The <code>.NetworkSettings.Ports</code> section contains a map of the internal port mappings to a list of external address/port objects. To grab just the numeric public port, you use <code>index</code> to find the specific port map, and then <code>index</code> 0 contains the first object inside of that. Then we ask for the <code>HostPort</code> field to get the public address.</p> <pre>$ docker inspect --format='{{(index (index .NetworkSettings.Ports \"8787/tcp\") 0).HostPort}}' $INSTANCE_ID\n</pre> <p><strong>Get a subsection in JSON format:</strong></p> <p>If you request a field which is itself a structure containing other fields, by default you get a Go-style dump of the inner values. Docker adds a template function, <code>json</code>, which can be applied to get results in JSON format.</p> <pre>$ docker inspect --format='{{json .Config}}' $INSTANCE_ID\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/inspect/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/inspect/</a>\n  </p>\n</div>\n","engine/reference/commandline/login/index":"<h1 id=\"login\">login</h1> <pre>Usage: docker login [OPTIONS] [SERVER]\n\nLog in to a Docker registry server, if no server is\nspecified \"https://index.docker.io/v1/\" is the default.\n\n  --help               Print usage\n  -p, --password=\"\"    Password\n  -u, --username=\"\"    Username\n</pre> <p>If you want to login to a self-hosted registry you can specify this by adding the server name.</p> <pre>example:\n$ docker login localhost:8080\n</pre> <p><code>docker login</code> requires user to use <code>sudo</code> or be <code>root</code>, except when:</p> <ol> <li>connecting to a remote daemon, such as a <code>docker-machine</code> provisioned <code>docker engine</code>.</li> <li>user is added to the <code>docker</code> group. This will impact the security of your system; the <code>docker</code> group is <code>root</code> equivalent. See <a href=\"https://docs.docker.com/security/security/#docker-daemon-attack-surface\">Docker Daemon Attack Surface</a> for details.</li> </ol> <p>You can log into any public or private repository for which you have credentials. When you log in, the command stores encoded credentials in <code>$HOME/.docker/config.json</code> on Linux or <code>%USERPROFILE%/.docker/config.json</code> on Windows.</p> <blockquote> <p><strong>Note</strong>: When running <code>sudo docker login</code> credentials are saved in <code>/root/.docker/config.json</code>.</p> </blockquote> <h2 id=\"credentials-store\">Credentials store</h2> <p>The Docker Engine can keep user credentials in an external credentials store, such as the native keychain of the operating system. Using an external store is more secure than storing credentials in the Docker configuration file.</p> <p>To use a credentials store, you need an external helper program to interact with a specific keychain or external store. Docker requires the helper program to be in the client’s host <code>$PATH</code>.</p> <p>This is the list of currently available credentials helpers and where you can download them from:</p> <ul> <li>D-Bus Secret Service: <a href=\"https://github.com/docker/docker-credential-helpers/releases\">https://github.com/docker/docker-credential-helpers/releases</a>\n</li> <li>Apple OS X keychain: <a href=\"https://github.com/docker/docker-credential-helpers/releases\">https://github.com/docker/docker-credential-helpers/releases</a>\n</li> <li>Microsoft Windows Credential Manager: <a href=\"https://github.com/docker/docker-credential-helpers/releases\">https://github.com/docker/docker-credential-helpers/releases</a>\n</li> </ul> <h3 id=\"usage\">Usage</h3> <p>You need to speficy the credentials store in <code>$HOME/.docker/config.json</code> to tell the docker engine to use it:</p> <pre>{\n\t\"credsStore\": \"osxkeychain\"\n}\n</pre> <p>If you are currently logged in, run <code>docker logout</code> to remove the credentials from the file and run <code>docker login</code> again.</p> <h3 id=\"protocol\">Protocol</h3> <p>Credential helpers can be any program or script that follows a very simple protocol. This protocol is heavily inspired by Git, but it differs in the information shared.</p> <p>The helpers always use the first argument in the command to identify the action. There are only three possible values for that argument: <code>store</code>, <code>get</code>, and <code>erase</code>.</p> <p>The <code>store</code> command takes a JSON payload from the standard input. That payload carries the server address, to identify the credential, the user name, and either a password or an identity token.</p> <pre>{\n\t\"ServerURL\": \"https://index.docker.io/v1\",\n\t\"Username\": \"david\",\n\t\"Secret\": \"passw0rd1\"\n}\n</pre> <p>If the secret being stored is an identity token, the Username should be set to <code>&lt;token&gt;</code>.</p> <p>The <code>store</code> command can write error messages to <code>STDOUT</code> that the docker engine will show if there was an issue.</p> <p>The <code>get</code> command takes a string payload from the standard input. That payload carries the server address that the docker engine needs credentials for. This is an example of that payload: <code>https://index.docker.io/v1</code>.</p> <p>The <code>get</code> command writes a JSON payload to <code>STDOUT</code>. Docker reads the user name and password from this payload:</p> <pre>{\n\t\"Username\": \"david\",\n\t\"Secret\": \"passw0rd1\"\n}\n</pre> <p>The <code>erase</code> command takes a string payload from <code>STDIN</code>. That payload carries the server address that the docker engine wants to remove credentials for. This is an example of that payload: <code>https://index.docker.io/v1</code>.</p> <p>The <code>erase</code> command can write error messages to <code>STDOUT</code> that the docker engine will show if there was an issue.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/login/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/login/</a>\n  </p>\n</div>\n","engine/reference/commandline/logout/index":"<h1 id=\"logout\">logout</h1> <pre>Usage: docker logout [SERVER]\n\nLog out from a Docker registry, if no server is\nspecified \"https://index.docker.io/v1/\" is the default.\n\n  --help          Print usage\n</pre> <p>For example:</p> <pre>$ docker logout localhost:8080\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/logout/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/logout/</a>\n  </p>\n</div>\n","engine/reference/commandline/logs/index":"<h1 id=\"logs\">logs</h1> <pre>Usage: docker logs [OPTIONS] CONTAINER\n\nFetch the logs of a container\n\n  -f, --follow              Follow log output\n  --help                    Print usage\n  --since=\"\"                Show logs since timestamp\n  -t, --timestamps          Show timestamps\n  --tail=\"all\"              Number of lines to show from the end of the logs\n</pre> <blockquote> <p><strong>Note</strong>: this command is available only for containers with <code>json-file</code> and <code>journald</code> logging drivers.</p> </blockquote> <p>The <code>docker logs</code> command batch-retrieves logs present at the time of execution.</p> <p>The <code>docker logs --follow</code> command will continue streaming the new output from the container’s <code>STDOUT</code> and <code>STDERR</code>.</p> <p>Passing a negative number or a non-integer to <code>--tail</code> is invalid and the value is set to <code>all</code> in that case.</p> <p>The <code>docker logs --timestamps</code> command will add an <a href=\"https://golang.org/pkg/time/#pkg-constants\">RFC3339Nano timestamp</a> , for example <code>2014-09-16T06:17:46.000000000Z</code>, to each log entry. To ensure that the timestamps are aligned the nano-second part of the timestamp will be padded with zero when necessary.</p> <p>The <code>--since</code> option shows only the container logs generated after a given date. You can specify the date as an RFC 3339 date, a UNIX timestamp, or a Go duration string (e.g. <code>1m30s</code>, <code>3h</code>). Besides RFC3339 date format you may also use RFC3339Nano, <code>2006-01-02T15:04:05</code>, <code>2006-01-02T15:04:05.999999999</code>, <code>2006-01-02Z07:00</code>, and <code>2006-01-02</code>. The local timezone on the client will be used if you do not provide either a <code>Z</code> or a <code>+-00:00</code> timezone offset at the end of the timestamp. When providing Unix timestamps enter seconds[.nanoseconds], where seconds is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (aka Unix epoch or Unix time), and the optional .nanoseconds field is a fraction of a second no more than nine digits long. You can combine the <code>--since</code> option with either or both of the <code>--follow</code> or <code>--tail</code> options.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/logs/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/logs/</a>\n  </p>\n</div>\n","engine/reference/commandline/network_connect/index":"<h1 id=\"network-connect\">network connect</h1> <pre>Usage:  docker network connect [OPTIONS] NETWORK CONTAINER\n\nConnects a container to a network\n\n  --alias=[]         Add network-scoped alias for the container\n  --help             Print usage\n  --ip               IPv4 Address\n  --ip6              IPv6 Address\n  --link=[]          Add a link to another container\n</pre> <p>Connects a container to a network. You can connect a container by name or by ID. Once connected, the container can communicate with other containers in the same network.</p> <pre>$ docker network connect multi-host-network container1\n</pre> <p>You can also use the <code>docker run --net=&lt;network-name&gt;</code> option to start a container and immediately connect it to a network.</p> <pre>$ docker run -itd --net=multi-host-network busybox\n</pre> <p>You can specify the IP address you want to be assigned to the container’s interface.</p> <pre>$ docker network connect --ip 10.10.36.122 multi-host-network container2\n</pre> <p>You can use <code>--link</code> option to link another container with a preferred alias</p> <pre>$ docker network connect --link container1:c1 multi-host-network container2\n</pre> <p><code>--alias</code> option can be used to resolve the container by another name in the network being connected to.</p> <pre>$ docker network connect --alias db --alias mysql multi-host-network container2\n</pre> <p>You can pause, restart, and stop containers that are connected to a network. Paused containers remain connected and can be revealed by a <code>network inspect</code>. When the container is stopped, it does not appear on the network until you restart it.</p> <p>If specified, the container’s IP address(es) is reapplied when a stopped container is restarted. If the IP address is no longer available, the container fails to start. One way to guarantee that the IP address is available is to specify an <code>--ip-range</code> when creating the network, and choose the static IP address(es) from outside that range. This ensures that the IP address is not given to another container while this container is not on the network.</p> <pre>$ docker network create --subnet 172.20.0.0/16 --ip-range 172.20.240.0/20 multi-host-network\n</pre> <pre>$ docker network connect --ip 172.20.128.2 multi-host-network container2\n</pre> <p>To verify the container is connected, use the <code>docker network inspect</code> command. Use <code>docker network disconnect</code> to remove a container from the network.</p> <p>Once connected in network, containers can communicate using only another container’s IP address or name. For <code>overlay</code> networks or custom plugins that support multi-host connectivity, containers connected to the same multi-host network but launched from different Engines can also communicate in this way.</p> <p>You can connect a container to one or more networks. The networks need not be the same type. For example, you can connect a single container bridge and overlay networks.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../network_inspect/index\">network inspect</a></li> <li><a href=\"../network_create/index\">network create</a></li> <li><a href=\"../network_disconnect/index\">network disconnect</a></li> <li><a href=\"../network_ls/index\">network ls</a></li> <li><a href=\"../network_rm/index\">network rm</a></li> <li><a href=\"../../../userguide/networking/dockernetworks/index\">Understand Docker container networks</a></li> <li><a href=\"../../../userguide/networking/work-with-networks/index\">Work with networks</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/network_connect/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/network_connect/</a>\n  </p>\n</div>\n","engine/reference/commandline/network_inspect/index":"<h1 id=\"network-inspect\">network inspect</h1> <pre>Usage:  docker network inspect [OPTIONS] NETWORK [NETWORK..]\n\nDisplays detailed information on a network\n\n  -f, --format=       Format the output using the given go template.\n  --help             Print usage\n</pre> <p>Returns information about one or more networks. By default, this command renders all results in a JSON object. For example, if you connect two containers to the default <code>bridge</code> network:</p> <pre>$ sudo docker run -itd --name=container1 busybox\nf2870c98fd504370fb86e59f32cd0753b1ac9b69b7d80566ffc7192a82b3ed27\n\n$ sudo docker run -itd --name=container2 busybox\nbda12f8922785d1f160be70736f26c1e331ab8aaf8ed8d56728508f2e2fd4727\n</pre> <p>The <code>network inspect</code> command shows the containers, by id, in its results. For networks backed by multi-host network driver, such as Overlay, this command also shows the container endpoints in other hosts in the cluster. These endpoints are represented as “ep-{endpoint-id}” in the output. You can specify an alternate format to execute a given template for each result. Go’s <a href=\"http://golang.org/pkg/text/template/\">text/template</a> package describes all the details of the format.</p> <pre>$ sudo docker network inspect bridge\n[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"b2b1a2cba717161d984383fd68218cf70bbbd17d328496885f7c921333228b0f\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.42.1/16\",\n                    \"Gateway\": \"172.17.42.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Containers\": {\n            \"bda12f8922785d1f160be70736f26c1e331ab8aaf8ed8d56728508f2e2fd4727\": {\n                \"Name\": \"container2\",\n                \"EndpointID\": \"0aebb8fcd2b282abe1365979536f21ee4ceaf3ed56177c628eae9f706e00e019\",\n                \"MacAddress\": \"02:42:ac:11:00:02\",\n                \"IPv4Address\": \"172.17.0.2/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"f2870c98fd504370fb86e59f32cd0753b1ac9b69b7d80566ffc7192a82b3ed27\": {\n                \"Name\": \"container1\",\n                \"EndpointID\": \"a00676d9c91a96bbe5bcfb34f705387a33d7cc365bac1a29e4e9728df92d10ad\",\n                \"MacAddress\": \"02:42:ac:11:00:01\",\n                \"IPv4Address\": \"172.17.0.1/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        }\n    }\n]\n</pre> <p>Returns the information about the user-defined network:</p> <pre>$ docker network create simple-network\n69568e6336d8c96bbf57869030919f7c69524f71183b44d80948bd3927c87f6a\n$ docker network inspect simple-network\n[\n    {\n        \"Name\": \"simple-network\",\n        \"Id\": \"69568e6336d8c96bbf57869030919f7c69524f71183b44d80948bd3927c87f6a\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.22.0.0/16\",\n                    \"Gateway\": \"172.22.0.1/16\"\n                }\n            ]\n        },\n        \"Containers\": {},\n        \"Options\": {}\n    }\n]\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../network_disconnect/index\">network disconnect </a></li> <li><a href=\"../network_connect/index\">network connect</a></li> <li><a href=\"../network_create/index\">network create</a></li> <li><a href=\"../network_ls/index\">network ls</a></li> <li><a href=\"../network_rm/index\">network rm</a></li> <li><a href=\"../../../userguide/networking/dockernetworks/index\">Understand Docker container networks</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/network_inspect/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/network_inspect/</a>\n  </p>\n</div>\n","engine/reference/commandline/network_create/index":"<h1 id=\"network-create\">network create</h1> <pre>Usage:  docker network create [OPTIONS] NETWORK-NAME\n\nCreates a new network with a name specified by the user\n\n--aux-address=map[]      Auxiliary ipv4 or ipv6 addresses used by network driver\n-d --driver=DRIVER       Driver to manage the Network bridge or overlay. The default is bridge.\n--gateway=[]             ipv4 or ipv6 Gateway for the master subnet\n--help                   Print usage\n--internal               Restricts external access to the network\n--ip-range=[]            Allocate container ip from a sub-range\n--ipam-driver=default    IP Address Management Driver\n--ipam-opt=map[]         Set custom IPAM driver specific options\n--ipv6                   Enable IPv6 networking\n--label=[]               Set metadata on a network\n-o --opt=map[]           Set custom driver specific options\n--subnet=[]              Subnet in CIDR format that represents a network segment\n</pre> <p>Creates a new network. The <code>DRIVER</code> accepts <code>bridge</code> or <code>overlay</code> which are the built-in network drivers. If you have installed a third party or your own custom network driver you can specify that <code>DRIVER</code> here also. If you don’t specify the <code>--driver</code> option, the command automatically creates a <code>bridge</code> network for you. When you install Docker Engine it creates a <code>bridge</code> network automatically. This network corresponds to the <code>docker0</code> bridge that Engine has traditionally relied on. When launch a new container with <code>docker run</code> it automatically connects to this bridge network. You cannot remove this default bridge network but you can create new ones using the <code>network create</code> command.</p> <pre>$ docker network create -d bridge my-bridge-network\n</pre> <p>Bridge networks are isolated networks on a single Engine installation. If you want to create a network that spans multiple Docker hosts each running an Engine, you must create an <code>overlay</code> network. Unlike <code>bridge</code> networks overlay networks require some pre-existing conditions before you can create one. These conditions are:</p> <ul> <li>Access to a key-value store. Engine supports Consul, Etcd, and ZooKeeper (Distributed store) key-value stores.</li> <li>A cluster of hosts with connectivity to the key-value store.</li> <li>A properly configured Engine <code>daemon</code> on each host in the cluster.</li> </ul> <p>The <code>docker daemon</code> options that support the <code>overlay</code> network are:</p> <ul> <li><code>--cluster-store</code></li> <li><code>--cluster-store-opt</code></li> <li><code>--cluster-advertise</code></li> </ul> <p>To read more about these options and how to configure them, see <a href=\"../../../userguide/networking/get-started-overlay/index\">“<em>Get started with multi-host network</em>“</a>.</p> <p>It is also a good idea, though not required, that you install Docker Swarm on to manage the cluster that makes up your network. Swarm provides sophisticated discovery and server management that can assist your implementation.</p> <p>Once you have prepared the <code>overlay</code> network prerequisites you simply choose a Docker host in the cluster and issue the following to create the network:</p> <pre>$ docker network create -d overlay my-multihost-network\n</pre> <p>Network names must be unique. The Docker daemon attempts to identify naming conflicts but this is not guaranteed. It is the user’s responsibility to avoid name conflicts.</p> <h2 id=\"connect-containers\">Connect containers</h2> <p>When you start a container use the <code>--net</code> flag to connect it to a network. This adds the <code>busybox</code> container to the <code>mynet</code> network.</p> <pre>$ docker run -itd --net=mynet busybox\n</pre> <p>If you want to add a container to a network after the container is already running use the <code>docker network connect</code> subcommand.</p> <p>You can connect multiple containers to the same network. Once connected, the containers can communicate using only another container’s IP address or name. For <code>overlay</code> networks or custom plugins that support multi-host connectivity, containers connected to the same multi-host network but launched from different Engines can also communicate in this way.</p> <p>You can disconnect a container from a network using the <code>docker network\ndisconnect</code> command.</p> <h2 id=\"specifying-advanced-options\">Specifying advanced options</h2> <p>When you create a network, Engine creates a non-overlapping subnetwork for the network by default. This subnetwork is not a subdivision of an existing network. It is purely for ip-addressing purposes. You can override this default and specify subnetwork values directly using the <code>--subnet</code> option. On a <code>bridge</code> network you can only create a single subnet:</p> <pre>docker network create --driver=bridge --subnet=192.168.0.0/16 br0\n</pre> <p>Additionally, you also specify the <code>--gateway</code> <code>--ip-range</code> and <code>--aux-address</code> options.</p> <pre>network create --driver=bridge --subnet=172.28.0.0/16 --ip-range=172.28.5.0/24 --gateway=172.28.5.254 br0\n</pre> <p>If you omit the <code>--gateway</code> flag the Engine selects one for you from inside a preferred pool. For <code>overlay</code> networks and for network driver plugins that support it you can create multiple subnetworks.</p> <pre>docker network create -d overlay\n  --subnet=192.168.0.0/16 --subnet=192.170.0.0/16\n  --gateway=192.168.0.100 --gateway=192.170.0.100\n  --ip-range=192.168.1.0/24\n  --aux-address a=192.168.1.5 --aux-address b=192.168.1.6\n  --aux-address a=192.170.1.5 --aux-address b=192.170.1.6\n  my-multihost-network\n</pre> <p>Be sure that your subnetworks do not overlap. If they do, the network create fails and Engine returns an error.</p> <h1 id=\"bridge-driver-options\">Bridge driver options</h1> <p>When creating a custom network, the default network driver (i.e. <code>bridge</code>) has additional options that can be passed. The following are those options and the equivalent docker daemon flags used for docker0 bridge:</p> <table> <thead> <tr> <th>Option</th> <th>Equivalent</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>com.docker.network.bridge.name</code></td> <td>-</td> <td>bridge name to be used when creating the Linux bridge</td> </tr> <tr> <td><code>com.docker.network.bridge.enable_ip_masquerade</code></td> <td><code>--ip-masq</code></td> <td>Enable IP masquerading</td> </tr> <tr> <td><code>com.docker.network.bridge.enable_icc</code></td> <td><code>--icc</code></td> <td>Enable or Disable Inter Container Connectivity</td> </tr> <tr> <td><code>com.docker.network.bridge.host_binding_ipv4</code></td> <td><code>--ip</code></td> <td>Default IP when binding container ports</td> </tr> <tr> <td><code>com.docker.network.mtu</code></td> <td><code>--mtu</code></td> <td>Set the containers network MTU</td> </tr> </tbody> </table> <p>The following arguments can be passed to <code>docker network create</code> for any network driver, again with their approximate equivalents to <code>docker daemon</code>.</p> <table> <thead> <tr> <th>Argument</th> <th>Equivalent</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>--gateway</code></td> <td>-</td> <td>ipv4 or ipv6 Gateway for the master subnet</td> </tr> <tr> <td><code>--ip-range</code></td> <td><code>--fixed-cidr</code></td> <td>Allocate IPs from a range</td> </tr> <tr> <td><code>--internal</code></td> <td>-</td> <td>Restricts external access to the network</td> </tr> <tr> <td><code>--ipv6</code></td> <td><code>--ipv6</code></td> <td>Enable IPv6 networking</td> </tr> <tr> <td><code>--subnet</code></td> <td><code>--bip</code></td> <td>Subnet for network</td> </tr> </tbody> </table> <p>For example, let’s use <code>-o</code> or <code>--opt</code> options to specify an IP address binding when publishing ports:</p> <pre>docker network create -o \"com.docker.network.bridge.host_binding_ipv4\"=\"172.19.0.1\" simple-network\n</pre> <h3 id=\"network-internal-mode\">Network internal mode</h3> <p>By default, when you connect a container to an <code>overlay</code> network, Docker also connects a bridge network to it to provide external connectivity. If you want to create an externally isolated <code>overlay</code> network, you can specify the <code>--internal</code> option.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../network_inspect/index\">network inspect</a></li> <li><a href=\"../network_connect/index\">network connect</a></li> <li><a href=\"../network_disconnect/index\">network disconnect</a></li> <li><a href=\"../network_ls/index\">network ls</a></li> <li><a href=\"../network_rm/index\">network rm</a></li> <li><a href=\"../../../userguide/networking/dockernetworks/index\">Understand Docker container networks</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/network_create/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/network_create/</a>\n  </p>\n</div>\n","engine/reference/commandline/network_disconnect/index":"<h1 id=\"network-disconnect\">network disconnect</h1> <pre>Usage:  docker network disconnect [OPTIONS] NETWORK CONTAINER\n\n\nDisconnects a container from a network\n\n  -f, --force        Force the container to disconnect from a network\n  --help             Print usage\n</pre> <p>Disconnects a container from a network. The container must be running to disconnect it from the network.</p> <pre>  $ docker network disconnect multi-host-network container1\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../network_inspect/index\">network inspect</a></li> <li><a href=\"../network_connect/index\">network connect</a></li> <li><a href=\"../network_create/index\">network create</a></li> <li><a href=\"../network_ls/index\">network ls</a></li> <li><a href=\"../network_rm/index\">network rm</a></li> <li><a href=\"../../../userguide/networking/dockernetworks/index\">Understand Docker container networks</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/network_disconnect/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/network_disconnect/</a>\n  </p>\n</div>\n","engine/reference/commandline/network_rm/index":"<h1 id=\"network-rm\">network rm</h1> <pre>Usage:  docker network rm [OPTIONS] NETWORK [NETWORK...]\n\nDeletes one or more networks\n\n  --help             Print usage\n</pre> <p>Removes one or more networks by name or identifier. To remove a network, you must first disconnect any containers connected to it. To remove the network named ‘my-network’:</p> <pre>  $ docker network rm my-network\n</pre> <p>To delete multiple networks in a single <code>docker network rm</code> command, provide multiple network names or ids. The following example deletes a network with id <code>3695c422697f</code> and a network named <code>my-network</code>:</p> <pre>  $ docker network rm 3695c422697f my-network\n</pre> <p>When you specify multiple networks, the command attempts to delete each in turn. If the deletion of one network fails, the command continues to the next on the list and tries to delete that. The command reports success or failure for each deletion.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../network_disconnect/index\">network disconnect </a></li> <li><a href=\"../network_connect/index\">network connect</a></li> <li><a href=\"../network_create/index\">network create</a></li> <li><a href=\"../network_ls/index\">network ls</a></li> <li><a href=\"../network_inspect/index\">network inspect</a></li> <li><a href=\"../../../userguide/networking/dockernetworks/index\">Understand Docker container networks</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/network_rm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/network_rm/</a>\n  </p>\n</div>\n","engine/reference/commandline/network_ls/index":"<h1 id=\"docker-network-ls\">docker network ls</h1> <pre>Usage:  docker network ls [OPTIONS]\n\nLists all the networks created by the user\n  -f, --filter=[]       Filter output based on conditions provided\n  --help                Print usage\n  --no-trunc            Do not truncate the output\n  -q, --quiet           Only display numeric IDs\n</pre> <p>Lists all the networks the Engine <code>daemon</code> knows about. This includes the networks that span across multiple hosts in a cluster, for example:</p> <pre>    $ sudo docker network ls\n    NETWORK ID          NAME                DRIVER\n    7fca4eb8c647        bridge              bridge\n    9f904ee27bf5        none                null\n    cf03ee007fb4        host                host\n    78b03ee04fc4        multi-host          overlay\n</pre> <p>Use the <code>--no-trunc</code> option to display the full network id:</p> <pre>docker network ls --no-trunc\nNETWORK ID                                                         NAME                DRIVER\n18a2866682b85619a026c81b98a5e375bd33e1b0936a26cc497c283d27bae9b3   none                null                \nc288470c46f6c8949c5f7e5099b5b7947b07eabe8d9a27d79a9cbf111adcbf47   host                host                \n7b369448dccbf865d397c8d2be0cda7cf7edc6b0945f77d2529912ae917a0185   bridge              bridge              \n95e74588f40db048e86320c6526440c504650a1ff3e9f7d60a497c4d2163e5bd   foo                 bridge    \n63d1ff1f77b07ca51070a8c227e962238358bd310bde1529cf62e6c307ade161   dev                 bridge\n</pre> <h2 id=\"filtering\">Filtering</h2> <p>The filtering flag (<code>-f</code> or <code>--filter</code>) format is a <code>key=value</code> pair. If there is more than one filter, then pass multiple flags (e.g. <code>--filter \"foo=bar\" --filter \"bif=baz\"</code>). Multiple filter flags are combined as an <code>OR</code> filter. For example, <code>-f type=custom -f type=builtin</code> returns both <code>custom</code> and <code>builtin</code> networks.</p> <p>The currently supported filters are:</p> <ul> <li>id (network’s id)</li> <li>name (network’s name)</li> <li>type (custom|builtin)</li> </ul> <h4 id=\"type\">Type</h4> <p>The <code>type</code> filter supports two values; <code>builtin</code> displays predefined networks (<code>bridge</code>, <code>none</code>, <code>host</code>), whereas <code>custom</code> displays user defined networks.</p> <p>The following filter matches all user defined networks:</p> <pre>$ docker network ls --filter type=custom\nNETWORK ID          NAME                DRIVER\n95e74588f40d        foo                 bridge\n63d1ff1f77b0        dev                 bridge\n</pre> <p>By having this flag it allows for batch cleanup. For example, use this filter to delete all user defined networks:</p> <pre>$ docker network rm `docker network ls --filter type=custom -q`\n</pre> <p>A warning will be issued when trying to remove a network that has containers attached.</p> <h4 id=\"name\">Name</h4> <p>The <code>name</code> filter matches on all or part of a network’s name.</p> <p>The following filter matches all networks with a name containing the <code>foobar</code> string.</p> <pre>$ docker network ls --filter name=foobar\nNETWORK ID          NAME                DRIVER\n06e7eef0a170        foobar              bridge\n</pre> <p>You can also filter for a substring in a name as this shows:</p> <pre>$ docker network ls --filter name=foo\nNETWORK ID          NAME                DRIVER\n95e74588f40d        foo                 bridge\n06e7eef0a170        foobar              bridge\n</pre> <h4 id=\"id\">ID</h4> <p>The <code>id</code> filter matches on all or part of a network’s ID.</p> <p>The following filter matches all networks with an ID containing the <code>63d1ff1f77b0...</code> string.</p> <pre>$ docker network ls --filter id=63d1ff1f77b07ca51070a8c227e962238358bd310bde1529cf62e6c307ade161\nNETWORK ID          NAME                DRIVER\n63d1ff1f77b0        dev                 bridge\n</pre> <p>You can also filter for a substring in an ID as this shows:</p> <pre>$ docker network ls --filter id=95e74588f40d\nNETWORK ID          NAME                DRIVER\n95e74588f40d        foo                 bridge\n\n$ docker network ls --filter id=95e\nNETWORK ID          NAME                DRIVER\n95e74588f40d        foo                 bridge\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../network_disconnect/index\">network disconnect </a></li> <li><a href=\"../network_connect/index\">network connect</a></li> <li><a href=\"../network_create/index\">network create</a></li> <li><a href=\"../network_inspect/index\">network inspect</a></li> <li><a href=\"../network_rm/index\">network rm</a></li> <li><a href=\"../../../userguide/networking/dockernetworks/index\">Understand Docker container networks</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/network_ls/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/network_ls/</a>\n  </p>\n</div>\n","engine/reference/commandline/pause/index":"<h1 id=\"pause\">pause</h1> <pre>Usage: docker pause [OPTIONS] CONTAINER [CONTAINER...]\n\nPause all processes within a container\n\n  --help          Print usage\n</pre> <p>The <code>docker pause</code> command uses the cgroups freezer to suspend all processes in a container. Traditionally, when suspending a process the <code>SIGSTOP</code> signal is used, which is observable by the process being suspended. With the cgroups freezer the process is unaware, and unable to capture, that it is being suspended, and subsequently resumed.</p> <p>See the <a href=\"https://www.kernel.org/doc/Documentation/cgroup-v1/freezer-subsystem.txt\">cgroups freezer documentation</a> for further details.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/pause/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/pause/</a>\n  </p>\n</div>\n","engine/reference/commandline/port/index":"<h1 id=\"port\">port</h1> <pre>Usage: docker port [OPTIONS] CONTAINER [PRIVATE_PORT[/PROTO]]\n\nList port mappings for the CONTAINER, or lookup the public-facing port that is\nNAT-ed to the PRIVATE_PORT\n\n  --help          Print usage\n</pre> <p>You can find out all the ports mapped by not specifying a <code>PRIVATE_PORT</code>, or just a specific mapping:</p> <pre>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                            NAMES\nb650456536c7        busybox:latest      top                 54 minutes ago      Up 54 minutes       0.0.0.0:1234-&gt;9876/tcp, 0.0.0.0:4321-&gt;7890/tcp   test\n$ docker port test\n7890/tcp -&gt; 0.0.0.0:4321\n9876/tcp -&gt; 0.0.0.0:1234\n$ docker port test 7890/tcp\n0.0.0.0:4321\n$ docker port test 7890/udp\n2014/06/24 11:53:36 Error: No public port '7890/udp' published for test\n$ docker port test 7890\n0.0.0.0:4321\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/port/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/port/</a>\n  </p>\n</div>\n","engine/reference/commandline/pull/index":"<h1 id=\"pull\">pull</h1> <pre>Usage: docker pull [OPTIONS] NAME[:TAG] | [REGISTRY_HOST[:REGISTRY_PORT]/]NAME[:TAG]\n\nPull an image or a repository from the registry\n\n  -a, --all-tags                Download all tagged images in the repository\n  --disable-content-trust=true  Skip image verification\n  --help                        Print usage\n</pre> <p>Most of your images will be created on top of a base image from the <a href=\"https://hub.docker.com\">Docker Hub</a> registry.</p> <p><a href=\"https://hub.docker.com\">Docker Hub</a> contains many pre-built images that you can <code>pull</code> and try without needing to define and configure your own.</p> <p>To download a particular image, or set of images (i.e., a repository), use <code>docker pull</code>.</p> <h2 id=\"proxy-configuration\">Proxy configuration</h2> <p>If you are behind an HTTP proxy server, for example in corporate settings, before open a connect to registry, you may need to configure the Docker daemon’s proxy settings, using the <code>HTTP_PROXY</code>, <code>HTTPS_PROXY</code>, and <code>NO_PROXY</code> environment variables. To set these environment variables on a host using <code>systemd</code>, refer to the <a href=\"../../../admin/systemd/index#http-proxy\">control and configure Docker with systemd</a> for variables configuration.</p> <h2 id=\"examples\">Examples</h2> <h3 id=\"pull-an-image-from-docker-hub\">Pull an image from Docker Hub</h3> <p>To download a particular image, or set of images (i.e., a repository), use <code>docker pull</code>. If no tag is provided, Docker Engine uses the <code>:latest</code> tag as a default. This command pulls the <code>debian:latest</code> image:</p> <pre>$ docker pull debian\n\nUsing default tag: latest\nlatest: Pulling from library/debian\nfdd5d7827f33: Pull complete\na3ed95caeb02: Pull complete\nDigest: sha256:e7d38b3517548a1c71e41bffe9c8ae6d6d29546ce46bf62159837aad072c90aa\nStatus: Downloaded newer image for debian:latest\n</pre> <p>Docker images can consist of multiple layers. In the example above, the image consists of two layers; <code>fdd5d7827f33</code> and <code>a3ed95caeb02</code>.</p> <p>Layers can be reused by images. For example, the <code>debian:jessie</code> image shares both layers with <code>debian:latest</code>. Pulling the <code>debian:jessie</code> image therefore only pulls its metadata, but not its layers, because all layers are already present locally:</p> <pre>$ docker pull debian:jessie\n\njessie: Pulling from library/debian\nfdd5d7827f33: Already exists\na3ed95caeb02: Already exists\nDigest: sha256:a9c958be96d7d40df920e7041608f2f017af81800ca5ad23e327bc402626b58e\nStatus: Downloaded newer image for debian:jessie\n</pre> <p>To see which images are present locally, use the <a href=\"../images/index\"><code>docker images</code></a> command:</p> <pre>$ docker images\n\nREPOSITORY   TAG      IMAGE ID        CREATED      SIZE\ndebian       jessie   f50f9524513f    5 days ago   125.1 MB\ndebian       latest   f50f9524513f    5 days ago   125.1 MB\n</pre> <p>Docker uses a content-addressable image store, and the image ID is a SHA256 digest covering the image’s configuration and layers. In the example above, <code>debian:jessie</code> and <code>debian:latest</code> have the same image ID because they are actually the <em>same</em> image tagged with different names. Because they are the same image, their layers are stored only once and do not consume extra disk space.</p> <p>For more information about images, layers, and the content-addressable store, refer to <a href=\"../../../userguide/storagedriver/imagesandcontainers/index\">understand images, containers, and storage drivers</a>.</p> <h2 id=\"pull-an-image-by-digest-immutable-identifier\">Pull an image by digest (immutable identifier)</h2> <p>So far, you’ve pulled images by their name (and “tag”). Using names and tags is a convenient way to work with images. When using tags, you can <code>docker pull</code> an image again to make sure you have the most up-to-date version of that image. For example, <code>docker pull ubuntu:14.04</code> pulls the latest version of the Ubuntu 14.04 image.</p> <p>In some cases you don’t want images to be updated to newer versions, but prefer to use a fixed version of an image. Docker enables you to pull an image by its <em>digest</em>. When pulling an image by digest, you specify <em>exactly</em> which version of an image to pull. Doing so, allows you to “pin” an image to that version, and guarantee that the image you’re using is always the same.</p> <p>To know the digest of an image, pull the image first. Let’s pull the latest <code>ubuntu:14.04</code> image from Docker Hub:</p> <pre>$ docker pull ubuntu:14.04\n\n14.04: Pulling from library/ubuntu\n5a132a7e7af1: Pull complete\nfd2731e4c50c: Pull complete\n28a2f68d1120: Pull complete\na3ed95caeb02: Pull complete\nDigest: sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2\nStatus: Downloaded newer image for ubuntu:14.04\n</pre> <p>Docker prints the digest of the image after the pull has finished. In the example above, the digest of the image is:</p> <pre>sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2\n</pre> <p>Docker also prints the digest of an image when <em>pushing</em> to a registry. This may be useful if you want to pin to a version of the image you just pushed.</p> <p>A digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:</p> <pre>$ docker pull ubuntu@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2\n\nsha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2: Pulling from library/ubuntu\n5a132a7e7af1: Already exists\nfd2731e4c50c: Already exists\n28a2f68d1120: Already exists\na3ed95caeb02: Already exists\nDigest: sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2\nStatus: Downloaded newer image for ubuntu@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2\n</pre> <p>Digest can also be used in the <code>FROM</code> of a Dockerfile, for example:</p> <pre>FROM ubuntu@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2\nMAINTAINER some maintainer &lt;maintainer@example.com&gt;\n</pre> <blockquote> <p><strong>Note</strong>: Using this feature “pins” an image to a specific version in time. Docker will therefore not pull updated versions of an image, which may include security updates. If you want to pull an updated image, you need to change the digest accordingly.</p> </blockquote> <h2 id=\"pulling-from-a-different-registry\">Pulling from a different registry</h2> <p>By default, <code>docker pull</code> pulls images from Docker Hub. It is also possible to manually specify the path of a registry to pull from. For example, if you have set up a local registry, you can specify its path to pull from it. A registry path is similar to a URL, but does not contain a protocol specifier (<code>https://</code>).</p> <p>The following command pulls the <code>testing/test-image</code> image from a local registry listening on port 5000 (<code>myregistry.local:5000</code>):</p> <pre>$ docker pull myregistry.local:5000/testing/test-image\n</pre> <p>Registry credentials are managed by <a href=\"../login/index\">docker login</a>.</p> <p>Docker uses the <code>https://</code> protocol to communicate with a registry, unless the registry is allowed to be accessed over an insecure connection. Refer to the <a href=\"../daemon/index#insecure-registries\">insecure registries</a> section for more information.</p> <h2 id=\"pull-a-repository-with-multiple-images\">Pull a repository with multiple images</h2> <p>By default, <code>docker pull</code> pulls a <em>single</em> image from the registry. A repository can contain multiple images. To pull all images from a repository, provide the <code>-a</code> (or <code>--all-tags</code>) option when using <code>docker pull</code>.</p> <p>This command pulls all images from the <code>fedora</code> repository:</p> <pre>$ docker pull --all-tags fedora\n\nPulling repository fedora\nad57ef8d78d7: Download complete\n105182bb5e8b: Download complete\n511136ea3c5a: Download complete\n73bd853d2ea5: Download complete\n....\n\nStatus: Downloaded newer image for fedora\n</pre> <p>After the pull has completed use the <code>docker images</code> command to see the images that were pulled. The example below shows all the <code>fedora</code> images that are present locally:</p> <pre>$ docker images fedora\n\nREPOSITORY   TAG         IMAGE ID        CREATED      SIZE\nfedora       rawhide     ad57ef8d78d7    5 days ago   359.3 MB\nfedora       20          105182bb5e8b    5 days ago   372.7 MB\nfedora       heisenbug   105182bb5e8b    5 days ago   372.7 MB\nfedora       latest      105182bb5e8b    5 days ago   372.7 MB\n</pre> <h2 id=\"canceling-a-pull\">Canceling a pull</h2> <p>Killing the <code>docker pull</code> process, for example by pressing <code>CTRL-c</code> while it is running in a terminal, will terminate the pull operation.</p> <pre>$ docker pull fedora\n\nUsing default tag: latest\nlatest: Pulling from library/fedora\na3ed95caeb02: Pulling fs layer\n236608c7b546: Pulling fs layer\n^C\n</pre> <blockquote> <p><strong>Note</strong>: Technically, the Engine terminates a pull operation when the connection between the Docker Engine daemon and the Docker Engine client initiating the pull is lost. If the connection with the Engine daemon is lost for other reasons than a manual interaction, the pull is also aborted.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/pull/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/pull/</a>\n  </p>\n</div>\n","engine/reference/commandline/ps/index":"<h1 id=\"ps\">ps</h1> <pre>Usage: docker ps [OPTIONS]\n\nList containers\n\n  -a, --all             Show all containers (default shows just running)\n  -f, --filter=[]       Filter output based on these conditions:\n                        - exited=&lt;int&gt; an exit code of &lt;int&gt;\n                        - label=&lt;key&gt; or label=&lt;key&gt;=&lt;value&gt;\n                        - status=(created|restarting|running|paused|exited)\n                        - name=&lt;string&gt; a container's name\n                        - id=&lt;ID&gt; a container's ID\n                        - before=(&lt;container-name&gt;|&lt;container-id&gt;)\n                        - since=(&lt;container-name&gt;|&lt;container-id&gt;)\n                        - ancestor=(&lt;image-name&gt;[:tag]|&lt;image-id&gt;|&lt;image@digest&gt;) - containers created from an image or a descendant.\n                        - volume=(&lt;volume-name&gt;|&lt;mount-point&gt;)\n  --format=[]           Pretty-print containers using a Go template\n  --help                Print usage\n  -l, --latest          Show the latest created container (includes all states)\n  -n=-1                 Show n last created containers (includes all states)\n  --no-trunc            Don't truncate output\n  -q, --quiet           Only display numeric IDs\n  -s, --size            Display total file sizes\n</pre> <p>Running <code>docker ps --no-trunc</code> showing 2 linked containers.</p> <pre>$ docker ps\nCONTAINER ID        IMAGE                        COMMAND                CREATED              STATUS              PORTS               NAMES\n4c01db0b339c        ubuntu:12.04                 bash                   17 seconds ago       Up 16 seconds       3300-3310/tcp       webapp\nd7886598dbe2        crosbymichael/redis:latest   /redis-server --dir    33 minutes ago       Up 33 minutes       6379/tcp            redis,webapp/db\n</pre> <p><code>docker ps</code> will show only running containers by default. To see all containers: <code>docker ps -a</code></p> <p><code>docker ps</code> will group exposed ports into a single range if possible. E.g., a container that exposes TCP ports <code>100, 101, 102</code> will display <code>100-102/tcp</code> in the <code>PORTS</code> column.</p> <h2 id=\"filtering\">Filtering</h2> <p>The filtering flag (<code>-f</code> or <code>--filter</code>) format is a <code>key=value</code> pair. If there is more than one filter, then pass multiple flags (e.g. <code>--filter \"foo=bar\" --filter \"bif=baz\"</code>)</p> <p>The currently supported filters are:</p> <ul> <li>id (container’s id)</li> <li>label (<code>label=&lt;key&gt;</code> or <code>label=&lt;key&gt;=&lt;value&gt;</code>)</li> <li>name (container’s name)</li> <li>exited (int - the code of exited containers. Only useful with <code>--all</code>)</li> <li>status (created|restarting|running|paused|exited|dead)</li> <li>ancestor (<code>&lt;image-name&gt;[:&lt;tag&gt;]</code>, <code>&lt;image id&gt;</code> or <code>&lt;image@digest&gt;</code>) - filters containers that were created from the given image or a descendant.</li> <li>before (container’s id or name) - filters containers created before given id or name</li> <li>since (container’s id or name) - filters containers created since given id or name</li> <li>isolation (default|process|hyperv) (Windows daemon only)</li> <li>volume (volume name or mount point) - filters containers that mount volumes.</li> </ul> <h4 id=\"label\">Label</h4> <p>The <code>label</code> filter matches containers based on the presence of a <code>label</code> alone or a <code>label</code> and a value.</p> <p>The following filter matches containers with the <code>color</code> label regardless of its value.</p> <pre>$ docker ps --filter \"label=color\"\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n673394ef1d4c        busybox             \"top\"               47 seconds ago      Up 45 seconds                           nostalgic_shockley\nd85756f57265        busybox             \"top\"               52 seconds ago      Up 51 seconds                           high_albattani\n</pre> <p>The following filter matches containers with the <code>color</code> label with the <code>blue</code> value.</p> <pre>$ docker ps --filter \"label=color=blue\"\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES\nd85756f57265        busybox             \"top\"               About a minute ago   Up About a minute                       high_albattani\n</pre> <h4 id=\"name\">Name</h4> <p>The <code>name</code> filter matches on all or part of a container’s name.</p> <p>The following filter matches all containers with a name containing the <code>nostalgic_stallman</code> string.</p> <pre>$ docker ps --filter \"name=nostalgic_stallman\"\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n9b6247364a03        busybox             \"top\"               2 minutes ago       Up 2 minutes                            nostalgic_stallman\n</pre> <p>You can also filter for a substring in a name as this shows:</p> <pre>$ docker ps --filter \"name=nostalgic\"\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n715ebfcee040        busybox             \"top\"               3 seconds ago       Up 1 seconds                            i_am_nostalgic\n9b6247364a03        busybox             \"top\"               7 minutes ago       Up 7 minutes                            nostalgic_stallman\n673394ef1d4c        busybox             \"top\"               38 minutes ago      Up 38 minutes                           nostalgic_shockley\n</pre> <h4 id=\"exited\">Exited</h4> <p>The <code>exited</code> filter matches containers by exist status code. For example, to filter for containers that have exited successfully:</p> <pre>$ docker ps -a --filter 'exited=0'\nCONTAINER ID        IMAGE             COMMAND                CREATED             STATUS                   PORTS                      NAMES\nea09c3c82f6e        registry:latest   /srv/run.sh            2 weeks ago         Exited (0) 2 weeks ago   127.0.0.1:5000-&gt;5000/tcp   desperate_leakey\n106ea823fe4e        fedora:latest     /bin/sh -c 'bash -l'   2 weeks ago         Exited (0) 2 weeks ago                              determined_albattani\n48ee228c9464        fedora:20         bash                   2 weeks ago         Exited (0) 2 weeks ago                              tender_torvalds\n</pre> <h4 id=\"status\">Status</h4> <p>The <code>status</code> filter matches containers by status. You can filter using <code>created</code>, <code>restarting</code>, <code>running</code>, <code>paused</code>, <code>exited</code> and <code>dead</code>. For example, to filter for <code>running</code> containers:</p> <pre>$ docker ps --filter status=running\nCONTAINER ID        IMAGE                  COMMAND             CREATED             STATUS              PORTS               NAMES\n715ebfcee040        busybox                \"top\"               16 minutes ago      Up 16 minutes                           i_am_nostalgic\nd5c976d3c462        busybox                \"top\"               23 minutes ago      Up 23 minutes                           top\n9b6247364a03        busybox                \"top\"               24 minutes ago      Up 24 minutes                           nostalgic_stallman\n</pre> <p>To filter for <code>paused</code> containers:</p> <pre>$ docker ps --filter status=paused\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n673394ef1d4c        busybox             \"top\"               About an hour ago   Up About an hour (Paused)                       nostalgic_shockley\n</pre> <h4 id=\"ancestor\">Ancestor</h4> <p>The <code>ancestor</code> filter matches containers based on its image or a descendant of it. The filter supports the following image representation:</p> <ul> <li>image</li> <li>image:tag</li> <li>image:tag@digest</li> <li>short-id</li> <li>full-id</li> </ul> <p>If you don’t specify a <code>tag</code>, the <code>latest</code> tag is used. For example, to filter for containers that use the latest <code>ubuntu</code> image:</p> <pre>$ docker ps --filter ancestor=ubuntu\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES\n919e1179bdb8        ubuntu-c1           \"top\"               About a minute ago   Up About a minute                       admiring_lovelace\n5d1e4a540723        ubuntu-c2           \"top\"               About a minute ago   Up About a minute                       admiring_sammet\n82a598284012        ubuntu              \"top\"               3 minutes ago        Up 3 minutes                            sleepy_bose\nbab2a34ba363        ubuntu              \"top\"               3 minutes ago        Up 3 minutes                            focused_yonath\n</pre> <p>Match containers based on the <code>ubuntu-c1</code> image which, in this case, is a child of <code>ubuntu</code>:</p> <pre>$ docker ps --filter ancestor=ubuntu-c1\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES\n919e1179bdb8        ubuntu-c1           \"top\"               About a minute ago   Up About a minute                       admiring_lovelace\n</pre> <p>Match containers based on the <code>ubuntu</code> version <code>12.04.5</code> image:</p> <pre>$ docker ps --filter ancestor=ubuntu:12.04.5\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES\n82a598284012        ubuntu:12.04.5      \"top\"               3 minutes ago        Up 3 minutes                            sleepy_bose\n</pre> <p>The following matches containers based on the layer <code>d0e008c6cf02</code> or an image that have this layer in it’s layer stack.</p> <pre>$ docker ps --filter ancestor=d0e008c6cf02\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES\n82a598284012        ubuntu:12.04.5      \"top\"               3 minutes ago        Up 3 minutes                            sleepy_bose\n</pre> <h4 id=\"before\">Before</h4> <p>The <code>before</code> filter shows only containers created before the container with given id or name. For example, having these containers created:</p> <pre>$ docker ps\nCONTAINER ID        IMAGE       COMMAND       CREATED              STATUS              PORTS              NAMES\n9c3527ed70ce        busybox     \"top\"         14 seconds ago       Up 15 seconds                          desperate_dubinsky\n4aace5031105        busybox     \"top\"         48 seconds ago       Up 49 seconds                          focused_hamilton\n6e63f6ff38b0        busybox     \"top\"         About a minute ago   Up About a minute                      distracted_fermat\n</pre> <p>Filtering with <code>before</code> would give:</p> <pre>$ docker ps -f before=9c3527ed70ce\nCONTAINER ID        IMAGE       COMMAND       CREATED              STATUS              PORTS              NAMES\n4aace5031105        busybox     \"top\"         About a minute ago   Up About a minute                      focused_hamilton\n6e63f6ff38b0        busybox     \"top\"         About a minute ago   Up About a minute                      distracted_fermat\n</pre> <h4 id=\"since\">Since</h4> <p>The <code>since</code> filter shows only containers created since the container with given id or name. For example, with the same containers as in <code>before</code> filter:</p> <pre>$ docker ps -f since=6e63f6ff38b0\nCONTAINER ID        IMAGE       COMMAND       CREATED             STATUS              PORTS               NAMES\n9c3527ed70ce        busybox     \"top\"         10 minutes ago      Up 10 minutes                           desperate_dubinsky\n4aace5031105        busybox     \"top\"         10 minutes ago      Up 10 minutes                           focused_hamilton\n</pre> <h4 id=\"volume\">Volume</h4> <p>The <code>volume</code> filter shows only containers that mount a specific volume or have a volume mounted in a specific path:</p> <pre>$ docker ps --filter volume=remote-volume --format \"table {{.ID}}\\t{{.Mounts}}\"\nCONTAINER ID        MOUNTS\n9c3527ed70ce        remote-volume\n\n$ docker ps --filter volume=/data --format \"table {{.ID}}\\t{{.Mounts}}\"\nCONTAINER ID        MOUNTS\n9c3527ed70ce        remote-volume\n</pre> <h2 id=\"formatting\">Formatting</h2> <p>The formatting option (<code>--format</code>) will pretty-print container output using a Go template.</p> <p>Valid placeholders for the Go template are listed below:</p> <table> <thead> <tr> <th>Placeholder</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>.ID</code></td> <td>Container ID</td> </tr> <tr> <td><code>.Image</code></td> <td>Image ID</td> </tr> <tr> <td><code>.Command</code></td> <td>Quoted command</td> </tr> <tr> <td><code>.CreatedAt</code></td> <td>Time when the container was created.</td> </tr> <tr> <td><code>.RunningFor</code></td> <td>Elapsed time since the container was started.</td> </tr> <tr> <td><code>.Ports</code></td> <td>Exposed ports.</td> </tr> <tr> <td><code>.Status</code></td> <td>Container status.</td> </tr> <tr> <td><code>.Size</code></td> <td>Container disk size.</td> </tr> <tr> <td><code>.Names</code></td> <td>Container names.</td> </tr> <tr> <td><code>.Labels</code></td> <td>All labels assigned to the container.</td> </tr> <tr> <td><code>.Label</code></td> <td>Value of a specific label for this container. For example <code>{{.Label \"com.docker.swarm.cpu\"}}</code>\n</td> </tr> <tr> <td><code>.Mounts</code></td> <td>Names of the volumes mounted in this container.</td> </tr> </tbody> </table> <p>When using the <code>--format</code> option, the <code>ps</code> command will either output the data exactly as the template declares or, when using the <code>table</code> directive, will include column headers as well.</p> <p>The following example uses a template without headers and outputs the <code>ID</code> and <code>Command</code> entries separated by a colon for all running containers:</p> <pre>$ docker ps --format \"{{.ID}}: {{.Command}}\"\na87ecb4f327c: /bin/sh -c #(nop) MA\n01946d9d34d8: /bin/sh -c #(nop) MA\nc1d3b0166030: /bin/sh -c yum -y up\n41d50ecd2f57: /bin/sh -c #(nop) MA\n</pre> <p>To list all running containers with their labels in a table format you can use:</p> <pre>$ docker ps --format \"table {{.ID}}\\t{{.Labels}}\"\nCONTAINER ID        LABELS\na87ecb4f327c        com.docker.swarm.node=ubuntu,com.docker.swarm.storage=ssd\n01946d9d34d8\nc1d3b0166030        com.docker.swarm.node=debian,com.docker.swarm.cpu=6\n41d50ecd2f57        com.docker.swarm.node=fedora,com.docker.swarm.cpu=3,com.docker.swarm.storage=ssd\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/ps/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/ps/</a>\n  </p>\n</div>\n","engine/reference/commandline/push/index":"<h1 id=\"push\">push</h1> <pre>Usage: docker push [OPTIONS] NAME[:TAG]\n\nPush an image or a repository to the registry\n\n  --disable-content-trust=true   Skip image signing\n  --help                         Print usage\n</pre> <p>Use <code>docker push</code> to share your images to the <a href=\"https://hub.docker.com\">Docker Hub</a> registry or to a self-hosted one.</p> <p>Killing the <code>docker push</code> process, for example by pressing <code>CTRL-c</code> while it is running in a terminal, will terminate the push operation.</p> <p>Registry credentials are managed by <a href=\"../login/index\">docker login</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/push/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/push/</a>\n  </p>\n</div>\n","engine/reference/commandline/rename/index":"<h1 id=\"rename\">rename</h1> <pre>Usage: docker rename [OPTIONS] OLD_NAME NEW_NAME\n\nRename a container\n\n  --help          Print usage\n</pre> <p>The <code>docker rename</code> command allows the container to be renamed to a different name.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/rename/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/rename/</a>\n  </p>\n</div>\n","engine/reference/commandline/restart/index":"<h1 id=\"restart\">restart</h1> <pre>Usage: docker restart [OPTIONS] CONTAINER [CONTAINER...]\n\nRestart a container\n\n  --help             Print usage\n  -t, --time=10      Seconds to wait for stop before killing the container\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/restart/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/restart/</a>\n  </p>\n</div>\n","engine/reference/commandline/rm/index":"<h1 id=\"rm\">rm</h1> <pre>Usage: docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers\n\n  -f, --force            Force the removal of a running container (uses SIGKILL)\n  --help                 Print usage\n  -l, --link             Remove the specified link\n  -v, --volumes          Remove the volumes associated with the container\n</pre> <h2 id=\"examples\">Examples</h2> <pre>$ docker rm /redis\n/redis\n</pre> <p>This will remove the container referenced under the link <code>/redis</code>.</p> <pre>$ docker rm --link /webapp/redis\n/webapp/redis\n</pre> <p>This will remove the underlying link between <code>/webapp</code> and the <code>/redis</code> containers removing all network communication.</p> <pre>$ docker rm --force redis\nredis\n</pre> <p>The main process inside the container referenced under the link <code>/redis</code> will receive <code>SIGKILL</code>, then the container will be removed.</p> <pre>$ docker rm $(docker ps -a -q)\n</pre> <p>This command will delete all stopped containers. The command <code>docker ps -a -q</code> will return all existing container IDs and pass them to the <code>rm</code> command which will delete them. Any running containers will not be deleted.</p> <pre>$ docker rm -v redis\nredis\n</pre> <p>This command will remove the container and any volumes associated with it. Note that if a volume was specified with a name, it will not be removed.</p> <pre>$ docker create -v awesome:/foo -v /bar --name hello redis\nhello\n$ docker rm -v hello\n</pre> <p>In this example, the volume for <code>/foo</code> will remain intact, but the volume for <code>/bar</code> will be removed. The same behavior holds for volumes inherited with <code>--volumes-from</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/rm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/rm/</a>\n  </p>\n</div>\n","engine/reference/commandline/rmi/index":"<h1 id=\"rmi\">rmi</h1> <pre>Usage: docker rmi [OPTIONS] IMAGE [IMAGE...]\n\nRemove one or more images\n\n  -f, --force          Force removal of the image\n  --help               Print usage\n  --no-prune           Do not delete untagged parents\n</pre> <p>You can remove an image using its short or long ID, its tag, or its digest. If an image has one or more tag referencing it, you must remove all of them before the image is removed. Digest references are removed automatically when an image is removed by tag.</p> <pre>$ docker images\nREPOSITORY                TAG                 IMAGE ID            CREATED             SIZE\ntest1                     latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)\ntest                      latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)\ntest2                     latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)\n\n$ docker rmi fd484f19954f\nError: Conflict, cannot delete image fd484f19954f because it is tagged in multiple repositories, use -f to force\n2013/12/11 05:47:16 Error: failed to remove one or more images\n\n$ docker rmi test1\nUntagged: test1:latest\n$ docker rmi test2\nUntagged: test2:latest\n\n$ docker images\nREPOSITORY                TAG                 IMAGE ID            CREATED             SIZE\ntest                      latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)\n$ docker rmi test\nUntagged: test:latest\nDeleted: fd484f19954f4920da7ff372b5067f5b7ddb2fd3830cecd17b96ea9e286ba5b8\n</pre> <p>If you use the <code>-f</code> flag and specify the image’s short or long ID, then this command untags and removes all images that match the specified ID.</p> <pre>$ docker images\nREPOSITORY                TAG                 IMAGE ID            CREATED             SIZE\ntest1                     latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)\ntest                      latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)\ntest2                     latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)\n\n$ docker rmi -f fd484f19954f\nUntagged: test1:latest\nUntagged: test:latest\nUntagged: test2:latest\nDeleted: fd484f19954f4920da7ff372b5067f5b7ddb2fd3830cecd17b96ea9e286ba5b8\n</pre> <p>An image pulled by digest has no tag associated with it:</p> <pre>$ docker images --digests\nREPOSITORY                     TAG       DIGEST                                                                    IMAGE ID        CREATED         SIZE\nlocalhost:5000/test/busybox    &lt;none&gt;    sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf   4986bf8c1536    9 weeks ago     2.43 MB\n</pre> <p>To remove an image using its digest:</p> <pre>$ docker rmi localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\nUntagged: localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\nDeleted: 4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125\nDeleted: ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2\nDeleted: df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/rmi/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/rmi/</a>\n  </p>\n</div>\n","engine/reference/commandline/search/index":"<h1 id=\"search\">search</h1> <pre>Usage: docker search [OPTIONS] TERM\n\nSearch the Docker Hub for images\n\n  --automated          Only show automated builds\n  --help               Print usage\n  --no-trunc           Don't truncate output\n  -s, --stars=0        Only displays with at least x stars\n</pre> <p>Search <a href=\"https://hub.docker.com\">Docker Hub</a> for images</p> <p>See <a href=\"../../../userguide/containers/dockerrepos/index#searching-for-images\"><em>Find Public Images on Docker Hub</em></a> for more details on finding shared images from the command line.</p> <blockquote> <p><strong>Note:</strong> Search queries will only return up to 25 results</p> </blockquote> <h2 id=\"examples\">Examples</h2> <h3 id=\"search-images-by-name\">Search images by name</h3> <p>This example displays images with a name containing ‘busybox’:</p> <pre>$ docker search busybox\nNAME                             DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nbusybox                          Busybox base image.                             316       [OK]       \nprogrium/busybox                                                                 50                   [OK]\nradial/busyboxplus               Full-chain, Internet enabled, busybox made...   8                    [OK]\nodise/busybox-python                                                             2                    [OK]\nazukiapp/busybox                 This image is meant to be used as the base...   2                    [OK]\nofayau/busybox-jvm               Prepare busybox to install a 32 bits JVM.       1                    [OK]\nshingonoide/archlinux-busybox    Arch Linux, a lightweight and flexible Lin...   1                    [OK]\nodise/busybox-curl                                                               1                    [OK]\nofayau/busybox-libc32            Busybox with 32 bits (and 64 bits) libs         1                    [OK]\npeelsky/zulu-openjdk-busybox                                                     1                    [OK]\nskomma/busybox-data              Docker image suitable for data volume cont...   1                    [OK]\nelektritter/busybox-teamspeak    Leightweight teamspeak3 container based on...   1                    [OK]\nsocketplane/busybox                                                              1                    [OK]\noveits/docker-nginx-busybox      This is a tiny NginX docker image based on...   0                    [OK]\nggtools/busybox-ubuntu           Busybox ubuntu version with extra goodies       0                    [OK]\nnikfoundas/busybox-confd         Minimal busybox based distribution of confd     0                    [OK]\nopenshift/busybox-http-app                                                       0                    [OK]\njllopis/busybox                                                                  0                    [OK]\nswyckoff/busybox                                                                 0                    [OK]\npowellquiring/busybox                                                            0                    [OK]\nwilliamyeh/busybox-sh            Docker image for BusyBox's sh                   0                    [OK]\nsimplexsys/busybox-cli-powered   Docker busybox images, with a few often us...   0                    [OK]\nfhisamoto/busybox-java           Busybox java                                    0                    [OK]\nscottabernethy/busybox                                                           0                    [OK]\nmarclop/busybox-solr\n</pre> <h3 id=\"search-images-by-name-and-number-of-stars-s-stars\">Search images by name and number of stars (-s, --stars)</h3> <p>This example displays images with a name containing ‘busybox’ and at least 3 stars:</p> <pre>$ docker search --stars=3 busybox\nNAME                 DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nbusybox              Busybox base image.                             325       [OK]       \nprogrium/busybox                                                     50                   [OK]\nradial/busyboxplus   Full-chain, Internet enabled, busybox made...   8                    [OK]\n</pre> <h3 id=\"search-automated-images-automated\">Search automated images (--automated)</h3> <p>This example displays images with a name containing ‘busybox’, at least 3 stars and are automated builds:</p> <pre>$ docker search --stars=3 --automated busybox\nNAME                 DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nprogrium/busybox                                                     50                   [OK]\nradial/busyboxplus   Full-chain, Internet enabled, busybox made...   8                    [OK]\n</pre> <h3 id=\"display-non-truncated-description-no-trunc\">Display non-truncated description (--no-trunc)</h3> <p>This example displays images with a name containing ‘busybox’, at least 3 stars and the description isn’t truncated in the output:</p> <pre>$ docker search --stars=3 --no-trunc busybox\nNAME                 DESCRIPTION                                                                               STARS     OFFICIAL   AUTOMATED\nbusybox              Busybox base image.                                                                       325       [OK]       \nprogrium/busybox                                                                                               50                   [OK]\nradial/busyboxplus   Full-chain, Internet enabled, busybox made from scratch. Comes in git and cURL flavors.   8                    [OK]\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/search/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/search/</a>\n  </p>\n</div>\n","engine/reference/commandline/run/index":"<h1 id=\"run\">run</h1> <pre>Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n\nRun a command in a new container\n\n  -a, --attach=[]               Attach to STDIN, STDOUT or STDERR\n  --add-host=[]                 Add a custom host-to-IP mapping (host:ip)\n  --blkio-weight=0              Block IO weight (relative weight)\n  --blkio-weight-device=[]      Block IO weight (relative device weight, format: `DEVICE_NAME:WEIGHT`)\n  --cpu-shares=0                CPU shares (relative weight)\n  --cap-add=[]                  Add Linux capabilities\n  --cap-drop=[]                 Drop Linux capabilities\n  --cgroup-parent=\"\"            Optional parent cgroup for the container\n  --cidfile=\"\"                  Write the container ID to the file\n  --cpu-period=0                Limit CPU CFS (Completely Fair Scheduler) period\n  --cpu-quota=0                 Limit CPU CFS (Completely Fair Scheduler) quota\n  --cpuset-cpus=\"\"              CPUs in which to allow execution (0-3, 0,1)\n  --cpuset-mems=\"\"              Memory nodes (MEMs) in which to allow execution (0-3, 0,1)\n  -d, --detach                  Run container in background and print container ID\n  --detach-keys                 Specify the escape key sequence used to detach a container\n  --device=[]                   Add a host device to the container\n  --device-read-bps=[]          Limit read rate (bytes per second) from a device (e.g., --device-read-bps=/dev/sda:1mb)\n  --device-read-iops=[]         Limit read rate (IO per second) from a device (e.g., --device-read-iops=/dev/sda:1000)\n  --device-write-bps=[]         Limit write rate (bytes per second) to a device (e.g., --device-write-bps=/dev/sda:1mb)\n  --device-write-iops=[]        Limit write rate (IO per second) to a device (e.g., --device-write-bps=/dev/sda:1000)\n  --disable-content-trust=true  Skip image verification\n  --dns=[]                      Set custom DNS servers\n  --dns-opt=[]                  Set custom DNS options\n  --dns-search=[]               Set custom DNS search domains\n  -e, --env=[]                  Set environment variables\n  --entrypoint=\"\"               Overwrite the default ENTRYPOINT of the image\n  --env-file=[]                 Read in a file of environment variables\n  --expose=[]                   Expose a port or a range of ports\n  --group-add=[]                Add additional groups to run as\n  -h, --hostname=\"\"             Container host name\n  --help                        Print usage\n  -i, --interactive             Keep STDIN open even if not attached\n  --ip=\"\"                       Container IPv4 address (e.g. 172.30.100.104)\n  --ip6=\"\"                      Container IPv6 address (e.g. 2001:db8::33)\n  --ipc=\"\"                      IPC namespace to use\n  --isolation=\"\"                Container isolation technology\n  --kernel-memory=\"\"            Kernel memory limit\n  -l, --label=[]                Set metadata on the container (e.g., --label=com.example.key=value)\n  --label-file=[]               Read in a file of labels (EOL delimited)\n  --link=[]                     Add link to another container\n  --log-driver=\"\"               Logging driver for container\n  --log-opt=[]                  Log driver specific options\n  -m, --memory=\"\"               Memory limit\n  --mac-address=\"\"              Container MAC address (e.g. 92:d0:c6:0a:29:33)\n  --memory-reservation=\"\"       Memory soft limit\n  --memory-swap=\"\"              A positive integer equal to memory plus swap. Specify -1 to enable unlimited swap.\n  --memory-swappiness=\"\"        Tune a container's memory swappiness behavior. Accepts an integer between 0 and 100.\n  --name=\"\"                     Assign a name to the container\n  --net=\"bridge\"                Connect a container to a network\n                                'bridge': create a network stack on the default Docker bridge\n                                'none': no networking\n                                'container:&lt;name|id&gt;': reuse another container's network stack\n                                'host': use the Docker host network stack\n                                '&lt;network-name&gt;|&lt;network-id&gt;': connect to a user-defined network\n  --net-alias=[]                Add network-scoped alias for the container\n  --oom-kill-disable            Whether to disable OOM Killer for the container or not\n  --oom-score-adj=0             Tune the host's OOM preferences for containers (accepts -1000 to 1000)\n  -P, --publish-all             Publish all exposed ports to random ports\n  -p, --publish=[]              Publish a container's port(s) to the host\n  --pid=\"\"                      PID namespace to use\n  --pids-limit=-1                Tune container pids limit (set -1 for unlimited), kernel &gt;= 4.3\n  --privileged                  Give extended privileges to this container\n  --read-only                   Mount the container's root filesystem as read only\n  --restart=\"no\"                Restart policy (no, on-failure[:max-retry], always, unless-stopped)\n  --rm                          Automatically remove the container when it exits\n  --shm-size=[]                 Size of `/dev/shm`. The format is `&lt;number&gt;&lt;unit&gt;`. `number` must be greater than `0`.  Unit is optional and can be `b` (bytes), `k` (kilobytes), `m` (megabytes), or `g` (gigabytes). If you omit the unit, the system uses bytes. If you omit the size entirely, the system uses `64m`.\n  --security-opt=[]             Security Options\n  --sig-proxy=true              Proxy received signals to the process\n  --stop-signal=\"SIGTERM\"       Signal to stop a container\n  -t, --tty                     Allocate a pseudo-TTY\n  -u, --user=\"\"                 Username or UID (format: &lt;name|uid&gt;[:&lt;group|gid&gt;])\n  --userns=\"\"                   Container user namespace\n                                'host': Use the Docker host user namespace\n                                '': Use the Docker daemon user namespace specified by `--userns-remap` option.\n  --ulimit=[]                   Ulimit options\n  --uts=\"\"                      UTS namespace to use\n  -v, --volume=[host-src:]container-dest[:&lt;options&gt;]\n                                Bind mount a volume. The comma-delimited\n                                `options` are [rw|ro], [z|Z],\n                                [[r]shared|[r]slave|[r]private], and\n                                [nocopy]. The 'host-src' is an absolute path\n                                or a name value.\n  --volume-driver=\"\"            Container's volume driver\n  --volumes-from=[]             Mount volumes from the specified container(s)\n  -w, --workdir=\"\"              Working directory inside the container\n</pre> <p>The <code>docker run</code> command first <code>creates</code> a writeable container layer over the specified image, and then <code>starts</code> it using the specified command. That is, <code>docker run</code> is equivalent to the API <code>/containers/create</code> then <code>/containers/(id)/start</code>. A stopped container can be restarted with all its previous changes intact using <code>docker start</code>. See <code>docker ps -a</code> to view a list of all containers.</p> <p>The <code>docker run</code> command can be used in combination with <code>docker commit</code> to <a href=\"../commit/index\"><em>change the command that a container runs</em></a>. There is additional detailed information about <code>docker run</code> in the <a href=\"../../run/index\">Docker run reference</a>.</p> <p>For information on connecting a container to a network, see the <a href=\"../../../userguide/networking/index\">“<em>Docker network overview</em>“</a>.</p> <h2 id=\"examples\">Examples</h2> <h3 id=\"assign-name-and-allocate-pseudo-tty-name-it\">Assign name and allocate pseudo-TTY (--name, -it)</h3> <pre>$ docker run --name test -it debian\nroot@d6c0fe130dba:/# exit 13\n$ echo $?\n13\n$ docker ps -a | grep test\nd6c0fe130dba        debian:7            \"/bin/bash\"         26 seconds ago      Exited (13) 17 seconds ago                         test\n</pre> <p>This example runs a container named <code>test</code> using the <code>debian:latest</code> image. The <code>-it</code> instructs Docker to allocate a pseudo-TTY connected to the container’s stdin; creating an interactive <code>bash</code> shell in the container. In the example, the <code>bash</code> shell is quit by entering <code>exit 13</code>. This exit code is passed on to the caller of <code>docker run</code>, and is recorded in the <code>test</code> container’s metadata.</p> <h3 id=\"capture-container-id-cidfile\">Capture container ID (--cidfile)</h3> <pre>$ docker run --cidfile /tmp/docker_test.cid ubuntu echo \"test\"\n</pre> <p>This will create a container and print <code>test</code> to the console. The <code>cidfile</code> flag makes Docker attempt to create a new file and write the container ID to it. If the file exists already, Docker will return an error. Docker will close this file when <code>docker run</code> exits.</p> <h3 id=\"full-container-capabilities-privileged\">Full container capabilities (--privileged)</h3> <pre>$ docker run -t -i --rm ubuntu bash\nroot@bc338942ef20:/# mount -t tmpfs none /mnt\nmount: permission denied\n</pre> <p>This will <em>not</em> work, because by default, most potentially dangerous kernel capabilities are dropped; including <code>cap_sys_admin</code> (which is required to mount filesystems). However, the <code>--privileged</code> flag will allow it to run:</p> <pre>$ docker run -t -i --privileged ubuntu bash\nroot@50e3f57e16e6:/# mount -t tmpfs none /mnt\nroot@50e3f57e16e6:/# df -h\nFilesystem      Size  Used Avail Use% Mounted on\nnone            1.9G     0  1.9G   0% /mnt\n</pre> <p>The <code>--privileged</code> flag gives <em>all</em> capabilities to the container, and it also lifts all the limitations enforced by the <code>device</code> cgroup controller. In other words, the container can then do almost everything that the host can do. This flag exists to allow special use-cases, like running Docker within Docker.</p> <h3 id=\"set-working-directory-w\">Set working directory (-w)</h3> <pre>$ docker  run -w /path/to/dir/ -i -t  ubuntu pwd\n</pre> <p>The <code>-w</code> lets the command being executed inside directory given, here <code>/path/to/dir/</code>. If the path does not exists it is created inside the container.</p> <h3 id=\"mount-tmpfs-tmpfs\">Mount tmpfs (--tmpfs)</h3> <pre>$ docker run -d --tmpfs /run:rw,noexec,nosuid,size=65536k my_image\n</pre> <p>The <code>--tmpfs</code> flag mounts an empty tmpfs into the container with the <code>rw</code>, <code>noexec</code>, <code>nosuid</code>, <code>size=65536k</code> options.</p> <h3 id=\"mount-volume-v-read-only\">Mount volume (-v, --read-only)</h3> <pre>$ docker  run  -v `pwd`:`pwd` -w `pwd` -i -t  ubuntu pwd\n</pre> <p>The <code>-v</code> flag mounts the current working directory into the container. The <code>-w</code> lets the command being executed inside the current working directory, by changing into the directory to the value returned by <code>pwd</code>. So this combination executes the command using the container, but inside the current working directory.</p> <pre>$ docker run -v /doesnt/exist:/foo -w /foo -i -t ubuntu bash\n</pre> <p>When the host directory of a bind-mounted volume doesn’t exist, Docker will automatically create this directory on the host for you. In the example above, Docker will create the <code>/doesnt/exist</code> folder before starting your container.</p> <pre>$ docker run --read-only -v /icanwrite busybox touch /icanwrite here\n</pre> <p>Volumes can be used in combination with <code>--read-only</code> to control where a container writes files. The <code>--read-only</code> flag mounts the container’s root filesystem as read only prohibiting writes to locations other than the specified volumes for the container.</p> <pre>$ docker run -t -i -v /var/run/docker.sock:/var/run/docker.sock -v /path/to/static-docker-binary:/usr/bin/docker busybox sh\n</pre> <p>By bind-mounting the docker unix socket and statically linked docker binary (refer to <a href=\"../../../installation/binaries/index#get-the-linux-binary\">get the linux binary</a>), you give the container the full access to create and manipulate the host’s Docker daemon.</p> <h3 id=\"publish-or-expose-port-p-expose\">Publish or expose port (-p, --expose)</h3> <pre>$ docker run -p 127.0.0.1:80:8080 ubuntu bash\n</pre> <p>This binds port <code>8080</code> of the container to port <code>80</code> on <code>127.0.0.1</code> of the host machine. The <a href=\"../../../userguide/networking/default_network/dockerlinks/index\">Docker User Guide</a> explains in detail how to manipulate ports in Docker.</p> <pre>$ docker run --expose 80 ubuntu bash\n</pre> <p>This exposes port <code>80</code> of the container without publishing the port to the host system’s interfaces.</p> <h3 id=\"set-environment-variables-e-env-env-file\">Set environment variables (-e, --env, --env-file)</h3> <pre>$ docker run -e MYVAR1 --env MYVAR2=foo --env-file ./env.list ubuntu bash\n</pre> <p>This sets simple (non-array) environmental variables in the container. For illustration all three flags are shown here. Where <code>-e</code>, <code>--env</code> take an environment variable and value, or if no <code>=</code> is provided, then that variable’s current value, set via <code>export</code>, is passed through (i.e. <code>$MYVAR1</code> from the host is set to <code>$MYVAR1</code> in the container). When no <code>=</code> is provided and that variable is not defined in the client’s environment then that variable will be removed from the container’s list of environment variables. All three flags, <code>-e</code>, <code>--env</code> and <code>--env-file</code> can be repeated.</p> <p>Regardless of the order of these three flags, the <code>--env-file</code> are processed first, and then <code>-e</code>, <code>--env</code> flags. This way, the <code>-e</code> or <code>--env</code> will override variables as needed.</p> <pre>$ cat ./env.list\nTEST_FOO=BAR\n$ docker run --env TEST_FOO=\"This is a test\" --env-file ./env.list busybox env | grep TEST_FOO\nTEST_FOO=This is a test\n</pre> <p>The <code>--env-file</code> flag takes a filename as an argument and expects each line to be in the <code>VAR=VAL</code> format, mimicking the argument passed to <code>--env</code>. Comment lines need only be prefixed with <code>#</code></p> <p>An example of a file passed with <code>--env-file</code></p> <pre>$ cat ./env.list\nTEST_FOO=BAR\n\n# this is a comment\nTEST_APP_DEST_HOST=10.10.0.127\nTEST_APP_DEST_PORT=8888\n_TEST_BAR=FOO\nTEST_APP_42=magic\nhelloWorld=true\n123qwe=bar\norg.spring.config=something\n\n# pass through this variable from the caller\nTEST_PASSTHROUGH\n$ TEST_PASSTHROUGH=howdy docker run --env-file ./env.list busybox env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=5198e0745561\nTEST_FOO=BAR\nTEST_APP_DEST_HOST=10.10.0.127\nTEST_APP_DEST_PORT=8888\n_TEST_BAR=FOO\nTEST_APP_42=magic\nhelloWorld=true\nTEST_PASSTHROUGH=howdy\nHOME=/root\n123qwe=bar\norg.spring.config=something\n\n$ docker run --env-file ./env.list busybox env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=5198e0745561\nTEST_FOO=BAR\nTEST_APP_DEST_HOST=10.10.0.127\nTEST_APP_DEST_PORT=8888\n_TEST_BAR=FOO\nTEST_APP_42=magic\nhelloWorld=true\nTEST_PASSTHROUGH=\nHOME=/root\n123qwe=bar\norg.spring.config=something\n</pre> <h3 id=\"set-metadata-on-container-l-label-label-file\">Set metadata on container (-l, --label, --label-file)</h3> <p>A label is a <code>key=value</code> pair that applies metadata to a container. To label a container with two labels:</p> <pre>$ docker run -l my-label --label com.example.foo=bar ubuntu bash\n</pre> <p>The <code>my-label</code> key doesn’t specify a value so the label defaults to an empty string(<code>\"\"</code>). To add multiple labels, repeat the label flag (<code>-l</code> or <code>--label</code>).</p> <p>The <code>key=value</code> must be unique to avoid overwriting the label value. If you specify labels with identical keys but different values, each subsequent value overwrites the previous. Docker uses the last <code>key=value</code> you supply.</p> <p>Use the <code>--label-file</code> flag to load multiple labels from a file. Delimit each label in the file with an EOL mark. The example below loads labels from a labels file in the current directory:</p> <pre>$ docker run --label-file ./labels ubuntu bash\n</pre> <p>The label-file format is similar to the format for loading environment variables. (Unlike environment variables, labels are not visible to processes running inside a container.) The following example illustrates a label-file format:</p> <pre>com.example.label1=\"a label\"\n\n# this is a comment\ncom.example.label2=another\\ label\ncom.example.label3\n</pre> <p>You can load multiple label-files by supplying multiple <code>--label-file</code> flags.</p> <p>For additional information on working with labels, see <a href=\"../../../userguide/labels-custom-metadata/index\"><em>Labels - custom metadata in Docker</em></a> in the Docker User Guide.</p> <h3 id=\"connect-a-container-to-a-network-net\">Connect a container to a network (--net)</h3> <p>When you start a container use the <code>--net</code> flag to connect it to a network. This adds the <code>busybox</code> container to the <code>my-net</code> network.</p> <pre>$ docker run -itd --net=my-net busybox\n</pre> <p>You can also choose the IP addresses for the container with <code>--ip</code> and <code>--ip6</code> flags when you start the container on a user-defined network.</p> <pre>$ docker run -itd --net=my-net --ip=10.10.9.75 busybox\n</pre> <p>If you want to add a running container to a network use the <code>docker network connect</code> subcommand.</p> <p>You can connect multiple containers to the same network. Once connected, the containers can communicate easily need only another container’s IP address or name. For <code>overlay</code> networks or custom plugins that support multi-host connectivity, containers connected to the same multi-host network but launched from different Engines can also communicate in this way.</p> <p><strong>Note</strong>: Service discovery is unavailable on the default bridge network. Containers can communicate via their IP addresses by default. To communicate by name, they must be linked.</p> <p>You can disconnect a container from a network using the <code>docker network\ndisconnect</code> command.</p> <h3 id=\"mount-volumes-from-container-volumes-from\">Mount volumes from container (--volumes-from)</h3> <pre>$ docker run --volumes-from 777f7dc92da7 --volumes-from ba8c0c54f0f2:ro -i -t ubuntu pwd\n</pre> <p>The <code>--volumes-from</code> flag mounts all the defined volumes from the referenced containers. Containers can be specified by repetitions of the <code>--volumes-from</code> argument. The container ID may be optionally suffixed with <code>:ro</code> or <code>:rw</code> to mount the volumes in read-only or read-write mode, respectively. By default, the volumes are mounted in the same mode (read write or read only) as the reference container.</p> <p>Labeling systems like SELinux require that proper labels are placed on volume content mounted into a container. Without a label, the security system might prevent the processes running inside the container from using the content. By default, Docker does not change the labels set by the OS.</p> <p>To change the label in the container context, you can add either of two suffixes <code>:z</code> or <code>:Z</code> to the volume mount. These suffixes tell Docker to relabel file objects on the shared volumes. The <code>z</code> option tells Docker that two containers share the volume content. As a result, Docker labels the content with a shared content label. Shared volume labels allow all containers to read/write content. The <code>Z</code> option tells Docker to label the content with a private unshared label. Only the current container can use a private volume.</p> <h3 id=\"attach-to-stdin-stdout-stderr-a\">Attach to STDIN/STDOUT/STDERR (-a)</h3> <p>The <code>-a</code> flag tells <code>docker run</code> to bind to the container’s <code>STDIN</code>, <code>STDOUT</code> or <code>STDERR</code>. This makes it possible to manipulate the output and input as needed.</p> <pre>$ echo \"test\" | docker run -i -a stdin ubuntu cat -\n</pre> <p>This pipes data into a container and prints the container’s ID by attaching only to the container’s <code>STDIN</code>.</p> <pre>$ docker run -a stderr ubuntu echo test\n</pre> <p>This isn’t going to print anything unless there’s an error because we’ve only attached to the <code>STDERR</code> of the container. The container’s logs still store what’s been written to <code>STDERR</code> and <code>STDOUT</code>.</p> <pre>$ cat somefile | docker run -i -a stdin mybuilder dobuild\n</pre> <p>This is how piping a file into a container could be done for a build. The container’s ID will be printed after the build is done and the build logs could be retrieved using <code>docker logs</code>. This is useful if you need to pipe a file or something else into a container and retrieve the container’s ID once the container has finished running.</p> <h3 id=\"add-host-device-to-container-device\">Add host device to container (--device)</h3> <pre>$ docker run --device=/dev/sdc:/dev/xvdc --device=/dev/sdd --device=/dev/zero:/dev/nulo -i -t ubuntu ls -l /dev/{xvdc,sdd,nulo}\nbrw-rw---- 1 root disk 8, 2 Feb  9 16:05 /dev/xvdc\nbrw-rw---- 1 root disk 8, 3 Feb  9 16:05 /dev/sdd\ncrw-rw-rw- 1 root root 1, 5 Feb  9 16:05 /dev/nulo\n</pre> <p>It is often necessary to directly expose devices to a container. The <code>--device</code> option enables that. For example, a specific block storage device or loop device or audio device can be added to an otherwise unprivileged container (without the <code>--privileged</code> flag) and have the application directly access it.</p> <p>By default, the container will be able to <code>read</code>, <code>write</code> and <code>mknod</code> these devices. This can be overridden using a third <code>:rwm</code> set of options to each <code>--device</code> flag:</p> <pre>$ docker run --device=/dev/sda:/dev/xvdc --rm -it ubuntu fdisk  /dev/xvdc\n\nCommand (m for help): q\n$ docker run --device=/dev/sda:/dev/xvdc:r --rm -it ubuntu fdisk  /dev/xvdc\nYou will not be able to write the partition table.\n\nCommand (m for help): q\n\n$ docker run --device=/dev/sda:/dev/xvdc:rw --rm -it ubuntu fdisk  /dev/xvdc\n\nCommand (m for help): q\n\n$ docker run --device=/dev/sda:/dev/xvdc:m --rm -it ubuntu fdisk  /dev/xvdc\nfdisk: unable to open /dev/xvdc: Operation not permitted\n</pre> <blockquote> <p><strong>Note:</strong> <code>--device</code> cannot be safely used with ephemeral devices. Block devices that may be removed should not be added to untrusted containers with <code>--device</code>.</p> </blockquote> <h3 id=\"restart-policies-restart\">Restart policies (--restart)</h3> <p>Use Docker’s <code>--restart</code> to specify a container’s <em>restart policy</em>. A restart policy controls whether the Docker daemon restarts a container after exit. Docker supports the following restart policies:</p> <table> <thead> <tr> <th>Policy</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td><strong>no</strong></td> <td> Do not automatically restart the container when it exits. This is the default. </td> </tr> <tr> <td> <span style=\"white-space: nowrap\"> <strong>on-failure</strong>[:max-retries] </span> </td> <td> Restart only if the container exits with a non-zero exit status. Optionally, limit the number of restart retries the Docker daemon attempts. </td> </tr> <tr> <td><strong>always</strong></td> <td> Always restart the container regardless of the exit status. When you specify always, the Docker daemon will try to restart the container indefinitely. The container will also always start on daemon startup, regardless of the current state of the container. </td> </tr> <tr> <td><strong>unless-stopped</strong></td> <td> Always restart the container regardless of the exit status, but do not start it on daemon startup if the container has been put to a stopped state before. </td> </tr> </tbody> </table> <pre>$ docker run --restart=always redis\n</pre> <p>This will run the <code>redis</code> container with a restart policy of <strong>always</strong> so that if the container exits, Docker will restart it.</p> <p>More detailed information on restart policies can be found in the <a href=\"../../run/index#restart-policies-restart\">Restart Policies (--restart)</a> section of the Docker run reference page.</p> <h3 id=\"add-entries-to-container-hosts-file-add-host\">Add entries to container hosts file (--add-host)</h3> <p>You can add other hosts into a container’s <code>/etc/hosts</code> file by using one or more <code>--add-host</code> flags. This example adds a static address for a host named <code>docker</code>:</p> <pre>$ docker run --add-host=docker:10.180.0.1 --rm -it debian\n$$ ping docker\nPING docker (10.180.0.1): 48 data bytes\n56 bytes from 10.180.0.1: icmp_seq=0 ttl=254 time=7.600 ms\n56 bytes from 10.180.0.1: icmp_seq=1 ttl=254 time=30.705 ms\n^C--- docker ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max/stddev = 7.600/19.152/30.705/11.553 ms\n</pre> <p>Sometimes you need to connect to the Docker host from within your container. To enable this, pass the Docker host’s IP address to the container using the <code>--add-host</code> flag. To find the host’s address, use the <code>ip addr show</code> command.</p> <p>The flags you pass to <code>ip addr show</code> depend on whether you are using IPv4 or IPv6 networking in your containers. Use the following flags for IPv4 address retrieval for a network device named <code>eth0</code>:</p> <pre>$ HOSTIP=`ip -4 addr show scope global dev eth0 | grep inet | awk '{print \\$2}' | cut -d / -f 1`\n$ docker run  --add-host=docker:${HOSTIP} --rm -it debian\n</pre> <p>For IPv6 use the <code>-6</code> flag instead of the <code>-4</code> flag. For other network devices, replace <code>eth0</code> with the correct device name (for example <code>docker0</code> for the bridge device).</p> <h3 id=\"set-ulimits-in-container-ulimit\">Set ulimits in container (--ulimit)</h3> <p>Since setting <code>ulimit</code> settings in a container requires extra privileges not available in the default container, you can set these using the <code>--ulimit</code> flag. <code>--ulimit</code> is specified with a soft and hard limit as such: <code>&lt;type&gt;=&lt;soft limit&gt;[:&lt;hard limit&gt;]</code>, for example:</p> <pre>$ docker run --ulimit nofile=1024:1024 --rm debian sh -c \"ulimit -n\"\n1024\n</pre> <blockquote> <p><strong>Note:</strong> If you do not provide a <code>hard limit</code>, the <code>soft limit</code> will be used for both values. If no <code>ulimits</code> are set, they will be inherited from the default <code>ulimits</code> set on the daemon. <code>as</code> option is disabled now. In other words, the following script is not supported: <code>$ docker run -it --ulimit as=1024 fedora /bin/bash</code></p> </blockquote> <p>The values are sent to the appropriate <code>syscall</code> as they are set. Docker doesn’t perform any byte conversion. Take this into account when setting the values.</p> <h4 id=\"for-nproc-usage\">For <code>nproc</code> usage</h4> <p>Be careful setting <code>nproc</code> with the <code>ulimit</code> flag as <code>nproc</code> is designed by Linux to set the maximum number of processes available to a user, not to a container. For example, start four containers with <code>daemon</code> user:</p> <pre>docker run -d -u daemon --ulimit nproc=3 busybox top\ndocker run -d -u daemon --ulimit nproc=3 busybox top\ndocker run -d -u daemon --ulimit nproc=3 busybox top\ndocker run -d -u daemon --ulimit nproc=3 busybox top\n</pre> <p>The 4th container fails and reports “[8] System error: resource temporarily unavailable” error. This fails because the caller set <code>nproc=3</code> resulting in the first three containers using up the three processes quota set for the <code>daemon</code> user.</p> <h3 id=\"stop-container-with-signal-stop-signal\">Stop container with signal (--stop-signal)</h3> <p>The <code>--stop-signal</code> flag sets the system call signal that will be sent to the container to exit. This signal can be a valid unsigned number that matches a position in the kernel’s syscall table, for instance 9, or a signal name in the format SIGNAME, for instance SIGKILL.</p> <h3 id=\"specify-isolation-technology-for-container-isolation\">Specify isolation technology for container (--isolation)</h3> <p>This option is useful in situations where you are running Docker containers on Microsoft Windows. The <code>--isolation &lt;value&gt;</code> option sets a container’s isolation technology. On Linux, the only supported is the <code>default</code> option which uses Linux namespaces. These two commands are equivalent on Linux:</p> <pre>$ docker run -d busybox top\n$ docker run -d --isolation default busybox top\n</pre> <p>On Microsoft Windows, can take any of these values:</p> <table> <thead> <tr> <th>Value</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>default</code></td> <td>Use the value specified by the Docker daemon’s <code>--exec-opt</code> . If the <code>daemon</code> does not specify an isolation technology, Microsoft Windows uses <code>process</code> as its default value.</td> </tr> <tr> <td><code>process</code></td> <td>Namespace isolation only.</td> </tr> <tr> <td><code>hyperv</code></td> <td>Hyper-V hypervisor partition-based isolation.</td> </tr> </tbody> </table> <p>In practice, when running on Microsoft Windows without a <code>daemon</code> option set, these two commands are equivalent:</p> <pre>$ docker run -d --isolation default busybox top\n$ docker run -d --isolation process busybox top\n</pre> <p>If you have set the <code>--exec-opt isolation=hyperv</code> option on the Docker <code>daemon</code>, any of these commands also result in <code>hyperv</code> isolation:</p> <pre>$ docker run -d --isolation default busybox top\n$ docker run -d --isolation hyperv busybox top\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/run/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/run/</a>\n  </p>\n</div>\n","engine/reference/commandline/save/index":"<h1 id=\"save\">save</h1> <pre>Usage: docker save [OPTIONS] IMAGE [IMAGE...]\n\nSave one or more images to a tar archive (streamed to STDOUT by default)\n\n  --help             Print usage\n  -o, --output=\"\"    Write to a file, instead of STDOUT\n</pre> <p>Produces a tarred repository to the standard output stream. Contains all parent layers, and all tags + versions, or specified <code>repo:tag</code>, for each argument provided.</p> <p>It is used to create a backup that can then be used with <code>docker load</code></p> <pre>$ docker save busybox &gt; busybox.tar\n$ ls -sh busybox.tar\n2.7M busybox.tar\n$ docker save --output busybox.tar busybox\n$ ls -sh busybox.tar\n2.7M busybox.tar\n$ docker save -o fedora-all.tar fedora\n$ docker save -o fedora-latest.tar fedora:latest\n</pre> <p>It is even useful to cherry-pick particular tags of an image repository</p> <pre>$ docker save -o ubuntu.tar ubuntu:lucid ubuntu:saucy\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/save/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/save/</a>\n  </p>\n</div>\n","engine/reference/commandline/start/index":"<h1 id=\"start\">start</h1> <pre>Usage: docker start [OPTIONS] CONTAINER [CONTAINER...]\n\nStart one or more containers\n\n  -a, --attach               Attach STDOUT/STDERR and forward signals\n  --detach-keys              Specify the escape key sequence used to detach a container\n  --help                     Print usage\n  -i, --interactive          Attach container's STDIN\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/start/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/start/</a>\n  </p>\n</div>\n","engine/reference/commandline/stats/index":"<h1 id=\"stats\">stats</h1> <pre>Usage: docker stats [OPTIONS] [CONTAINER...]\n\nDisplay a live stream of one or more containers' resource usage statistics\n\n  -a, --all          Show all containers (default shows just running)\n  --help             Print usage\n  --no-stream        Disable streaming stats and only pull the first result\n</pre> <p>The <code>docker stats</code> command returns a live data stream for running containers. To limit data to one or more specific containers, specify a list of container names or ids separated by a space. You can specify a stopped container but stopped containers do not return any data.</p> <p>If you want more detailed information about a container’s resource usage, use the <code>/containers/(id)/stats</code> API endpoint.</p> <h2 id=\"examples\">Examples</h2> <p>Running <code>docker stats</code> on all running containers</p> <pre>$ docker stats\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O\n1285939c1fd3        0.07%               796 KB / 64 MB        1.21%               788 B / 648 B       3.568 MB / 512 KB\n9c76f7834ae2        0.07%               2.746 MB / 64 MB      4.29%               1.266 KB / 648 B    12.4 MB / 0 B\nd1ea048f04e4        0.03%               4.583 MB / 64 MB      6.30%               2.854 KB / 648 B    27.7 MB / 0 B\n</pre> <p>Running <code>docker stats</code> on multiple containers by name and id.</p> <pre>$ docker stats fervent_panini 5acfcb1b4fd1\nCONTAINER           CPU %               MEM USAGE/LIMIT     MEM %               NET I/O\n5acfcb1b4fd1        0.00%               115.2 MB/1.045 GB   11.03%              1.422 kB/648 B\nfervent_panini      0.02%               11.08 MB/1.045 GB   1.06%               648 B/648 B\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/stats/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/stats/</a>\n  </p>\n</div>\n","engine/reference/commandline/stop/index":"<h1 id=\"stop\">stop</h1> <pre>Usage: docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop a container by sending SIGTERM and then SIGKILL after a\ngrace period\n\n  --help             Print usage\n  -t, --time=10      Seconds to wait for stop before killing it\n</pre> <p>The main process inside the container will receive <code>SIGTERM</code>, and after a grace period, <code>SIGKILL</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/stop/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/stop/</a>\n  </p>\n</div>\n","engine/reference/commandline/tag/index":"<h1 id=\"tag\">tag</h1> <pre>Usage: docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]\n\nTag an image into a repository\n\n  --help               Print usage\n</pre> <p>You can group your images together using names and tags, and then upload them to <a href=\"../../../userguide/containers/dockerrepos/index#contributing-to-docker-hub\"><em>Share Images via Repositories</em></a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/tag/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/tag/</a>\n  </p>\n</div>\n","engine/reference/commandline/top/index":"<h1 id=\"top\">top</h1> <pre>Usage: docker top [OPTIONS] CONTAINER [ps OPTIONS]\n\nDisplay the running processes of a container\n\n  --help          Print usage\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/top/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/top/</a>\n  </p>\n</div>\n","engine/reference/commandline/unpause/index":"<h1 id=\"unpause\">unpause</h1> <pre>Usage: docker unpause [OPTIONS] CONTAINER [CONTAINER...]\n\nUnpause all processes within a container\n\n  --help          Print usage\n</pre> <p>The <code>docker unpause</code> command uses the cgroups freezer to un-suspend all processes in a container.</p> <p>See the <a href=\"https://www.kernel.org/doc/Documentation/cgroup-v1/freezer-subsystem.txt\">cgroups freezer documentation</a> for further details.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/unpause/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/unpause/</a>\n  </p>\n</div>\n","engine/reference/commandline/update/index":"<h1 id=\"update\">update</h1> <pre>Usage: docker update [OPTIONS] CONTAINER [CONTAINER...]\n\nUpdate configuration of one or more containers\n\n  --help=false               Print usage\n  --blkio-weight=0           Block IO (relative weight), between 10 and 1000\n  --cpu-shares=0             CPU shares (relative weight)\n  --cpu-period=0             Limit the CPU CFS (Completely Fair Scheduler) period\n  --cpu-quota=0              Limit the CPU CFS (Completely Fair Scheduler) quota\n  --cpuset-cpus=\"\"           CPUs in which to allow execution (0-3, 0,1)\n  --cpuset-mems=\"\"           Memory nodes (MEMs) in which to allow execution (0-3, 0,1)\n  -m, --memory=\"\"            Memory limit\n  --memory-reservation=\"\"    Memory soft limit\n  --memory-swap=\"\"           A positive integer equal to memory plus swap. Specify -1 to enable unlimited swap\n  --kernel-memory=\"\"         Kernel memory limit: container must be stopped\n  --restart                  Restart policy to apply when a container exits\n</pre> <p>The <code>docker update</code> command dynamically updates container configuration. You can use this command to prevent containers from consuming too many resources from their Docker host. With a single command, you can place limits on a single container or on many. To specify more than one container, provide space-separated list of container names or IDs.</p> <p>With the exception of the <code>--kernel-memory</code> value, you can specify these options on a running or a stopped container. You can only update <code>--kernel-memory</code> on a stopped container. When you run <code>docker update</code> on stopped container, the next time you restart it, the container uses those values.</p> <p>Another configuration you can change with this command is restart policy, new restart policy will take effect instantly after you run <code>docker update</code> on a container.</p> <h2 id=\"examples\">EXAMPLES</h2> <p>The following sections illustrate ways to use this command.</p> <h3 id=\"update-a-container-with-cpu-shares-512\">Update a container with cpu-shares=512</h3> <p>To limit a container’s cpu-shares to 512, first identify the container name or ID. You can use <strong>docker ps</strong> to find these values. You can also use the ID returned from the <strong>docker run</strong> command. Then, do the following:</p> <pre>$ docker update --cpu-shares 512 abebf7571666\n</pre> <h3 id=\"update-a-container-with-cpu-shares-and-memory\">Update a container with cpu-shares and memory</h3> <p>To update multiple resource configurations for multiple containers:</p> <pre>$ docker update --cpu-shares 512 -m 300M abebf7571666 hopeful_morse\n</pre> <h3 id=\"update-a-container-s-restart-policy\">Update a container’s restart policy</h3> <p>To update restart policy for one or more containers:</p> <pre>$ docker update --restart=on-failure:3 abebf7571666 hopeful_morse\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/update/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/update/</a>\n  </p>\n</div>\n","engine/reference/commandline/version/index":"<h1 id=\"version\">version</h1> <pre>Usage: docker version [OPTIONS]\n\nShow the Docker version information.\n\n  -f, --format=\"\"    Format the output using the given go template\n  --help             Print usage\n</pre> <p>By default, this will render all version information in an easy to read layout. If a format is specified, the given template will be executed instead.</p> <p>Go’s <a href=\"http://golang.org/pkg/text/template/\">text/template</a> package describes all the details of the format.</p> <h2 id=\"examples\">Examples</h2> <p><strong>Default output:</strong></p> <pre>$ docker version\nClient:\n Version:      1.8.0\n API version:  1.20\n Go version:   go1.4.2\n Git commit:   f5bae0a\n Built:        Tue Jun 23 17:56:00 UTC 2015\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.8.0\n API version:  1.20\n Go version:   go1.4.2\n Git commit:   f5bae0a\n Built:        Tue Jun 23 17:56:00 UTC 2015\n OS/Arch:      linux/amd64\n</pre> <p><strong>Get server version:</strong></p> <pre>$ docker version --format '{{.Server.Version}}'\n1.8.0\n</pre> <p><strong>Dump raw data:</strong></p> <pre>$ docker version --format '{{json .}}'\n{\"Client\":{\"Version\":\"1.8.0\",\"ApiVersion\":\"1.20\",\"GitCommit\":\"f5bae0a\",\"GoVersion\":\"go1.4.2\",\"Os\":\"linux\",\"Arch\":\"amd64\",\"BuildTime\":\"Tue Jun 23 17:56:00 UTC 2015\"},\"ServerOK\":true,\"Server\":{\"Version\":\"1.8.0\",\"ApiVersion\":\"1.20\",\"GitCommit\":\"f5bae0a\",\"GoVersion\":\"go1.4.2\",\"Os\":\"linux\",\"Arch\":\"amd64\",\"KernelVersion\":\"3.13.2-gentoo\",\"BuildTime\":\"Tue Jun 23 17:56:00 UTC 2015\"}}\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/version/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/version/</a>\n  </p>\n</div>\n","engine/reference/commandline/volume_create/index":"<h1 id=\"volume-create\">volume create</h1> <pre>Usage: docker volume create [OPTIONS]\n\nCreate a volume\n\n  -d, --driver=local    Specify volume driver name\n  --help                Print usage\n  --label=[]            Set metadata for a volume\n  --name=               Specify volume name\n  -o, --opt=map[]       Set driver specific options\n</pre> <p>Creates a new volume that containers can consume and store data in. If a name is not specified, Docker generates a random name. You create a volume and then configure the container to use it, for example:</p> <pre>$ docker volume create --name hello\nhello\n\n$ docker run -d -v hello:/world busybox ls /world\n</pre> <p>The mount is created inside the container’s <code>/world</code> directory. Docker does not support relative paths for mount points inside the container.</p> <p>Multiple containers can use the same volume in the same time period. This is useful if two containers need access to shared data. For example, if one container writes and the other reads the data.</p> <p>Volume names must be unique among drivers. This means you cannot use the same volume name with two different drivers. If you attempt this <code>docker</code> returns an error:</p> <pre>A volume named  \"hello\"  already exists with the \"some-other\" driver. Choose a different volume name.\n</pre> <p>If you specify a volume name already in use on the current driver, Docker assumes you want to re-use the existing volume and does not return an error.</p> <h2 id=\"driver-specific-options\">Driver specific options</h2> <p>Some volume drivers may take options to customize the volume creation. Use the <code>-o</code> or <code>--opt</code> flags to pass driver options:</p> <pre>$ docker volume create --driver fake --opt tardis=blue --opt timey=wimey\n</pre> <p>These options are passed directly to the volume driver. Options for different volume drivers may do different things (or nothing at all).</p> <p>The built-in <code>local</code> driver on Windows does not support any options.</p> <p>The built-in <code>local</code> driver on Linux accepts options similar to the linux <code>mount</code> command:</p> <pre>$ docker volume create --driver local --opt type=tmpfs --opt device=tmpfs --opt o=size=100m,uid=1000\n</pre> <p>Another example:</p> <pre>$ docker volume create --driver local --opt type=btrfs --opt device=/dev/sda2\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../volume_inspect/index\">volume inspect</a></li> <li><a href=\"../volume_ls/index\">volume ls</a></li> <li><a href=\"../volume_rm/index\">volume rm</a></li> <li><a href=\"../../../userguide/containers/dockervolumes/index\">Understand Data Volumes</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/volume_create/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/volume_create/</a>\n  </p>\n</div>\n","engine/reference/commandline/volume_ls/index":"<h1 id=\"volume-ls\">volume ls</h1> <pre>Usage: docker volume ls [OPTIONS]\n\nList volumes\n\n  -f, --filter=[]      Provide filter values (i.e. 'dangling=true')\n  --help               Print usage\n  -q, --quiet          Only display volume names\n</pre> <p>Lists all the volumes Docker knows about. You can filter using the <code>-f</code> or <code>--filter</code> flag. The filtering format is a <code>key=value</code> pair. To specify more than one filter, pass multiple flags (for example, <code>--filter \"foo=bar\" --filter \"bif=baz\"</code>)</p> <p>There is a single supported filter <code>dangling=value</code> which takes a boolean of <code>true</code> or <code>false</code>.</p> <p>Example output:</p> <pre>$ docker volume create --name rose\nrose\n$docker volume create --name tyler\ntyler\n$ docker volume ls\nDRIVER              VOLUME NAME\nlocal               rose\nlocal               tyler\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../volume_create/index\">volume create</a></li> <li><a href=\"../volume_inspect/index\">volume inspect</a></li> <li><a href=\"../volume_rm/index\">volume rm</a></li> <li><a href=\"../../../userguide/containers/dockervolumes/index\">Understand Data Volumes</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/volume_ls/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/volume_ls/</a>\n  </p>\n</div>\n","engine/reference/commandline/volume_rm/index":"<h1 id=\"volume-rm\">volume rm</h1> <pre>Usage: docker volume rm [OPTIONS] VOLUME [VOLUME...]\n\nRemove a volume\n\n  --help             Print usage\n</pre> <p>Removes one or more volumes. You cannot remove a volume that is in use by a container.</p> <pre>$ docker volume rm hello\nhello\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../volume_create/index\">volume create</a></li> <li><a href=\"../volume_inspect/index\">volume inspect</a></li> <li><a href=\"../volume_ls/index\">volume ls</a></li> <li><a href=\"../../../userguide/containers/dockervolumes/index\">Understand Data Volumes</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/volume_rm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/volume_rm/</a>\n  </p>\n</div>\n","engine/reference/commandline/volume_inspect/index":"<h1 id=\"volume-inspect\">volume inspect</h1> <pre>Usage: docker volume inspect [OPTIONS] VOLUME [VOLUME...]\n\nReturn low-level information on a volume\n\n  -f, --format=       Format the output using the given go template.\n  --help              Print usage\n</pre> <p>Returns information about a volume. By default, this command renders all results in a JSON array. You can specify an alternate format to execute a given template for each result. Go’s <a href=\"http://golang.org/pkg/text/template/\">text/template</a> package describes all the details of the format.</p> <p>Example output:</p> <pre>$ docker volume create\n85bffb0677236974f93955d8ecc4df55ef5070117b0e53333cc1b443777be24d\n$ docker volume inspect 85bffb0677236974f93955d8ecc4df55ef5070117b0e53333cc1b443777be24d\n[\n  {\n      \"Name\": \"85bffb0677236974f93955d8ecc4df55ef5070117b0e53333cc1b443777be24d\",\n      \"Driver\": \"local\",\n      \"Mountpoint\": \"/var/lib/docker/volumes/85bffb0677236974f93955d8ecc4df55ef5070117b0e53333cc1b443777be24d/_data\"\n  }\n]\n\n$ docker volume inspect --format '{{ .Mountpoint }}' 85bffb0677236974f93955d8ecc4df55ef5070117b0e53333cc1b443777be24d\n/var/lib/docker/volumes/85bffb0677236974f93955d8ecc4df55ef5070117b0e53333cc1b443777be24d/_data\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../volume_create/index\">volume create</a></li> <li><a href=\"../volume_ls/index\">volume ls</a></li> <li><a href=\"../volume_rm/index\">volume rm</a></li> <li><a href=\"../../../userguide/containers/dockervolumes/index\">Understand Data Volumes</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/volume_inspect/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/volume_inspect/</a>\n  </p>\n</div>\n","engine/reference/commandline/wait/index":"<h1 id=\"wait\">wait</h1> <pre>Usage: docker wait [OPTIONS] CONTAINER [CONTAINER...]\n\nBlock until a container stops, then print its exit code.\n\n  --help          Print usage\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/wait/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/commandline/wait/</a>\n  </p>\n</div>\n","engine/reference/api/docker_remote_api/index":"<h1 id=\"docker-remote-api\">Docker Remote API</h1> <p>Docker’s Remote API uses an open schema model. In this model, unknown properties in incoming messages are ignored. Client applications need to take this behavior into account to ensure they do not break when talking to newer Docker daemons.</p> <p>The API tends to be REST, but for some complex commands, like attach or pull, the HTTP connection is hijacked to transport STDOUT, STDIN, and STDERR.</p> <p>By default the Docker daemon listens on <code>unix:///var/run/docker.sock</code> and the client must have <code>root</code> access to interact with the daemon. If a group named <code>docker</code> exists on your system, <code>docker</code> applies ownership of the socket to the group.</p> <p>To connect to the Docker daemon with cURL you need to use cURL 7.40 or later, as these versions have the <code>--unix-socket</code> flag available. To run <code>curl</code> against the daemon on the default socket, use the following:</p> <pre>curl --unix-socket /var/run/docker.sock http:/containers/json\n</pre> <p>If you have bound the Docker daemon to a different socket path or TCP port, you would reference that in your cURL rather than the default.</p> <p>The current version of the API is v1.23 which means calling <code>/info</code> is the same as calling <code>/v1.23/info</code>. To call an older version of the API use <code>/v1.22/info</code>.</p> <p>Use the table below to find the API version for a Docker version:</p> <table> <thead> <tr> <th>Docker version</th> <th>API version</th> <th>Changes</th> </tr> </thead> <tbody> <tr> <td>1.11.x</td> <td><a href=\"../docker_remote_api_v1.23/index\">1.23</a></td> <td><a href=\"index#v1-23-api-changes\">API changes</a></td> </tr> <tr> <td>1.10.x</td> <td><a href=\"../docker_remote_api_v1.22/index\">1.22</a></td> <td><a href=\"index#v1-22-api-changes\">API changes</a></td> </tr> <tr> <td>1.9.x</td> <td><a href=\"../docker_remote_api_v1.21/index\">1.21</a></td> <td><a href=\"index#v1-21-api-changes\">API changes</a></td> </tr> <tr> <td>1.8.x</td> <td><a href=\"../docker_remote_api_v1.20/index\">1.20</a></td> <td><a href=\"index#v1-20-api-changes\">API changes</a></td> </tr> <tr> <td>1.7.x</td> <td><a href=\"../docker_remote_api_v1.19/index\">1.19</a></td> <td><a href=\"index#v1-19-api-changes\">API changes</a></td> </tr> <tr> <td>1.6.x</td> <td><a href=\"../docker_remote_api_v1.18/index\">1.18</a></td> <td><a href=\"index#v1-18-api-changes\">API changes</a></td> </tr> </tbody> </table> <p>Refer to the <a href=\"https://github.com/docker/docker/tree/master/docs/reference/api\">GitHub repository</a> for older releases.</p> <h2 id=\"authentication\">Authentication</h2> <p>Authentication configuration is handled client side, so the client has to send the <code>authConfig</code> as a <code>POST</code> in <code>/images/(name)/push</code>. The <code>authConfig</code>, set as the <code>X-Registry-Auth</code> header, is currently a Base64 encoded (JSON) string with the following structure:</p> <pre>{\"username\": \"string\", \"password\": \"string\", \"email\": \"string\",\n   \"serveraddress\" : \"string\", \"auth\": \"\"}\n</pre> <p>Callers should leave the <code>auth</code> empty. The <code>serveraddress</code> is a domain/ip without protocol. Throughout this structure, double quotes are required.</p> <h2 id=\"using-docker-machine-with-the-api\">Using Docker Machine with the API</h2> <p>If you are using <code>docker-machine</code>, the Docker daemon is on a host that uses an encrypted TCP socket using TLS. This means, for Docker Machine users, you need to add extra parameters to <code>curl</code> or <code>wget</code> when making test API requests, for example:</p> <pre>curl --insecure \\\n     --cert $DOCKER_CERT_PATH/cert.pem \\\n     --key $DOCKER_CERT_PATH/key.pem \\\n     https://YOUR_VM_IP:2376/images/json\n\nwget --no-check-certificate --certificate=$DOCKER_CERT_PATH/cert.pem \\\n     --private-key=$DOCKER_CERT_PATH/key.pem \\\n     https://YOUR_VM_IP:2376/images/json -O - -q\n</pre> <h2 id=\"docker-events\">Docker Events</h2> <p>The following diagram depicts the container states accessible through the API.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/reference/api/images/event_state.png\" alt=\"States\"></p> <p>Some container-related events are not affected by container state, so they are not included in this diagram. These events are:</p> <ul> <li>\n<strong>export</strong> emitted by <code>docker export</code>\n</li> <li>\n<strong>exec_create</strong> emitted by <code>docker exec</code>\n</li> <li>\n<strong>exec_start</strong> emitted by <code>docker exec</code> after <strong>exec_create</strong>\n</li> </ul> <p>Running <code>docker rmi</code> emits an <strong>untag</strong> event when removing an image name. The <code>rmi</code> command may also emit <strong>delete</strong> events when images are deleted by ID directly or by deleting the last tag referring to the image.</p> <blockquote> <p><strong>Acknowledgment</strong>: This diagram and the accompanying text were used with the permission of Matt Good and Gilder Labs. See Matt’s original blog post <a href=\"https://gliderlabs.com/blog/2015/04/14/docker-events-explained/\">Docker Events Explained</a>.</p> </blockquote> <h2 id=\"version-history\">Version history</h2> <p>This section lists each version from latest to oldest. Each listing includes a link to the full documentation set and the changes relevant in that release.</p> <h3 id=\"v1-23-api-changes\">v1.23 API changes</h3> <p><a href=\"../docker_remote_api_v1.23/index\">Docker Remote API v1.23</a> documentation</p> <ul> <li>\n<code>GET /containers/json</code> returns the state of the container, one of <code>created</code>, <code>restarting</code>, <code>running</code>, <code>paused</code>, <code>exited</code> or <code>dead</code>.</li> <li>\n<code>GET /containers/json</code> returns the mount points for the container.</li> <li>\n<code>GET /networks/(name)</code> now returns an <code>Internal</code> field showing whether the network is internal or not.</li> <li>\n<code>GET /networks/(name)</code> now returns an <code>EnableIPv6</code> field showing whether the network has ipv6 enabled or not.</li> <li>\n<code>POST /containers/(name)/update</code> now supports updating container’s restart policy.</li> <li>\n<code>POST /networks/create</code> now supports enabling ipv6 on the network by setting the <code>EnableIPv6</code> field (doing this with a label will no longer work).</li> <li>\n<code>GET /info</code> now returns <code>CgroupDriver</code> field showing what cgroup driver the daemon is using; <code>cgroupfs</code> or <code>systemd</code>.</li> <li>\n<code>GET /info</code> now returns <code>KernelMemory</code> field, showing if “kernel memory limit” is supported.</li> <li>\n<code>POST /containers/create</code> now takes <code>PidsLimit</code> field, if the kernel is &gt;= 4.3 and the pids cgroup is supported.</li> <li>\n<code>GET /containers/(id or name)/stats</code> now returns <code>pids_stats</code>, if the kernel is &gt;= 4.3 and the pids cgroup is supported.</li> <li>\n<code>POST /containers/create</code> now allows you to override usernamespaces remapping and use privileged options for the container.</li> <li>\n<code>POST /containers/create</code> now allows specifying <code>nocopy</code> for named volumes, which disables automatic copying from the container path to the volume.</li> <li>\n<code>POST /auth</code> now returns an <code>IdentityToken</code> when supported by a registry.</li> <li>\n<code>POST /containers/create</code> with both <code>Hostname</code> and <code>Domainname</code> fields specified will result in the container’s hostname being set to <code>Hostname</code>, rather than <code>Hostname.Domainname</code>.</li> </ul> <h3 id=\"v1-22-api-changes\">v1.22 API changes</h3> <p><a href=\"../docker_remote_api_v1.22/index\">Docker Remote API v1.22</a> documentation</p> <ul> <li>\n<code>POST /container/(name)/update</code> updates the resources of a container.</li> <li>\n<code>GET /containers/json</code> supports filter <code>isolation</code> on Windows.</li> <li>\n<code>GET /containers/json</code> now returns the list of networks of containers.</li> <li>\n<code>GET /info</code> Now returns <code>Architecture</code> and <code>OSType</code> fields, providing information about the host architecture and operating system type that the daemon runs on.</li> <li>\n<code>GET /networks/(name)</code> now returns a <code>Name</code> field for each container attached to the network.</li> <li>\n<code>GET /version</code> now returns the <code>BuildTime</code> field in RFC3339Nano format to make it consistent with other date/time values returned by the API.</li> <li>\n<code>AuthConfig</code> now supports a <code>registrytoken</code> for token based authentication</li> <li>\n<code>POST /containers/create</code> now has a 4M minimum value limit for <code>HostConfig.KernelMemory</code>\n</li> <li>Pushes initiated with <code>POST /images/(name)/push</code> and pulls initiated with <code>POST /images/create</code> will be cancelled if the HTTP connection making the API request is closed before the push or pull completes.</li> <li>\n<code>POST /containers/create</code> now allows you to set a read/write rate limit for a device (in bytes per second or IO per second).</li> <li>\n<code>GET /networks</code> now supports filtering by <code>name</code>, <code>id</code> and <code>type</code>.</li> <li>\n<code>POST /containers/create</code> now allows you to set the static IPv4 and/or IPv6 address for the container.</li> <li>\n<code>POST /networks/(id)/connect</code> now allows you to set the static IPv4 and/or IPv6 address for the container.</li> <li>\n<code>GET /info</code> now includes the number of containers running, stopped, and paused.</li> <li>\n<code>POST /networks/create</code> now supports restricting external access to the network by setting the <code>Internal</code> field.</li> <li>\n<code>POST /networks/(id)/disconnect</code> now includes a <code>Force</code> option to forcefully disconnect a container from network</li> <li>\n<code>GET /containers/(id)/json</code> now returns the <code>NetworkID</code> of containers.</li> <li>\n<code>POST /networks/create</code> Now supports an options field in the IPAM config that provides options for custom IPAM plugins.</li> <li>\n<code>GET /networks/{network-id}</code> Now returns IPAM config options for custom IPAM plugins if any are available.</li> <li>\n<code>GET /networks/&lt;network-id&gt;</code> now returns subnets info for user-defined networks.</li> <li>\n<code>GET /info</code> can now return a <code>SystemStatus</code> field useful for returning additional information about applications that are built on top of engine.</li> </ul> <h3 id=\"v1-21-api-changes\">v1.21 API changes</h3> <p><a href=\"../docker_remote_api_v1.21/index\">Docker Remote API v1.21</a> documentation</p> <ul> <li>\n<code>GET /volumes</code> lists volumes from all volume drivers.</li> <li>\n<code>POST /volumes/create</code> to create a volume.</li> <li>\n<code>GET /volumes/(name)</code> get low-level information about a volume.</li> <li>\n<code>DELETE /volumes/(name)</code> remove a volume with the specified name.</li> <li>\n<code>VolumeDriver</code> was moved from <code>config</code> to <code>HostConfig</code> to make the configuration portable.</li> <li>\n<code>GET /images/(name)/json</code> now returns information about an image’s <code>RepoTags</code> and <code>RepoDigests</code>.</li> <li>The <code>config</code> option now accepts the field <code>StopSignal</code>, which specifies the signal to use to kill a container.</li> <li>\n<code>GET /containers/(id)/stats</code> will return networking information respectively for each interface.</li> <li>The <code>HostConfig</code> option now includes the <code>DnsOptions</code> field to configure the container’s DNS options.</li> <li>\n<code>POST /build</code> now optionally takes a serialized map of build-time variables.</li> <li>\n<code>GET /events</code> now includes a <code>timenano</code> field, in addition to the existing <code>time</code> field.</li> <li>\n<code>GET /events</code> now supports filtering by image and container labels.</li> <li>\n<code>GET /info</code> now lists engine version information and return the information of <code>CPUShares</code> and <code>Cpuset</code>.</li> <li>\n<code>GET /containers/json</code> will return <code>ImageID</code> of the image used by container.</li> <li>\n<code>POST /exec/(name)/start</code> will now return an HTTP 409 when the container is either stopped or paused.</li> <li>\n<code>GET /containers/(name)/json</code> now accepts a <code>size</code> parameter. Setting this parameter to ‘1’ returns container size information in the <code>SizeRw</code> and <code>SizeRootFs</code> fields.</li> <li>\n<code>GET /containers/(name)/json</code> now returns a <code>NetworkSettings.Networks</code> field, detailing network settings per network. This field deprecates the <code>NetworkSettings.Gateway</code>, <code>NetworkSettings.IPAddress</code>, <code>NetworkSettings.IPPrefixLen</code>, and <code>NetworkSettings.MacAddress</code> fields, which are still returned for backward-compatibility, but will be removed in a future version.</li> <li>\n<code>GET /exec/(id)/json</code> now returns a <code>NetworkSettings.Networks</code> field, detailing networksettings per network. This field deprecates the <code>NetworkSettings.Gateway</code>, <code>NetworkSettings.IPAddress</code>, <code>NetworkSettings.IPPrefixLen</code>, and <code>NetworkSettings.MacAddress</code> fields, which are still returned for backward-compatibility, but will be removed in a future version.</li> <li>The <code>HostConfig</code> option now includes the <code>OomScoreAdj</code> field for adjusting the badness heuristic. This heuristic selects which processes the OOM killer kills under out-of-memory conditions.</li> </ul> <h3 id=\"v1-20-api-changes\">v1.20 API changes</h3> <p><a href=\"../docker_remote_api_v1.20/index\">Docker Remote API v1.20</a> documentation</p> <ul> <li>\n<code>GET /containers/(id)/archive</code> get an archive of filesystem content from a container.</li> <li>\n<code>PUT /containers/(id)/archive</code> upload an archive of content to be extracted to an existing directory inside a container’s filesystem.</li> <li>\n<code>POST /containers/(id)/copy</code> is deprecated in favor of the above <code>archive</code> endpoint which can be used to download files and directories from a container.</li> <li>The <code>hostConfig</code> option now accepts the field <code>GroupAdd</code>, which specifies a list of additional groups that the container process will run as.</li> </ul> <h3 id=\"v1-19-api-changes\">v1.19 API changes</h3> <p><a href=\"../docker_remote_api_v1.19/index\">Docker Remote API v1.19</a> documentation</p> <ul> <li>When the daemon detects a version mismatch with the client, usually when the client is newer than the daemon, an HTTP 400 is now returned instead of a 404.</li> <li>\n<code>GET /containers/(id)/stats</code> now accepts <code>stream</code> bool to get only one set of stats and disconnect.</li> <li>\n<code>GET /containers/(id)/logs</code> now accepts a <code>since</code> timestamp parameter.</li> <li>\n<code>GET /info</code> The fields <code>Debug</code>, <code>IPv4Forwarding</code>, <code>MemoryLimit</code>, and <code>SwapLimit</code> are now returned as boolean instead of as an int. In addition, the end point now returns the new boolean fields <code>CpuCfsPeriod</code>, <code>CpuCfsQuota</code>, and <code>OomKillDisable</code>.</li> <li>The <code>hostConfig</code> option now accepts the fields <code>CpuPeriod</code> and <code>CpuQuota</code>\n</li> <li>\n<code>POST /build</code> accepts <code>cpuperiod</code> and <code>cpuquota</code> options</li> </ul> <h3 id=\"v1-18-api-changes\">v1.18 API changes</h3> <p><a href=\"../docker_remote_api_v1.18/index\">Docker Remote API v1.18</a> documentation</p> <ul> <li>\n<code>GET /version</code> now returns <code>Os</code>, <code>Arch</code> and <code>KernelVersion</code>.</li> <li>\n<code>POST /containers/create</code> and <code>POST /containers/(id)/start</code>allow you to set ulimit settings for use in the container.</li> <li>\n<code>GET /info</code> now returns <code>SystemTime</code>, <code>HttpProxy</code>,<code>HttpsProxy</code> and <code>NoProxy</code>.</li> <li>\n<code>GET /images/json</code> added a <code>RepoDigests</code> field to include image digest information.</li> <li>\n<code>POST /build</code> can now set resource constraints for all containers created for the build.</li> <li>\n<code>CgroupParent</code> can be passed in the host config to setup container cgroups under a specific cgroup.</li> <li>\n<code>POST /build</code> closing the HTTP request cancels the build</li> <li>\n<code>POST /containers/(id)/exec</code> includes <code>Warnings</code> field to response.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api/</a>\n  </p>\n</div>\n","engine/breaking_changes/index":"<h1 id=\"breaking-changes-and-incompatibilities\">Breaking changes and incompatibilities</h1> <p>Every Engine release strives to be backward compatible with its predecessors. In all cases, the policy is that feature removal is communicated two releases in advance and documented as part of the <a href=\"../deprecated/index\">deprecated features</a> page.</p> <p>Unfortunately, Docker is a fast moving project, and newly introduced features may sometime introduce breaking changes and/or incompatibilities. This page documents these by Engine version.</p> <h1 id=\"engine-1-10\">Engine 1.10</h1> <p>There were two breaking changes in the 1.10 release.</p> <h2 id=\"registry\">Registry</h2> <p>Registry 2.3 includes improvements to the image manifest that have caused a breaking change. Images pushed by Engine 1.10 to a Registry 2.3 cannot be pulled by digest by older Engine versions. A <code>docker pull</code> that encounters this situation returns the following error:</p> <pre> Error response from daemon: unsupported schema version 2 for tag TAGNAME\n</pre> <p>Docker Content Trust heavily relies on pull by digest. As a result, images pushed from the Engine 1.10 CLI to a 2.3 Registry cannot be pulled by older Engine CLIs (&lt; 1.10) with Docker Content Trust enabled.</p> <p>If you are using an older Registry version (&lt; 2.3), this problem does not occur with any version of the Engine CLI; push, pull, with and without content trust work as you would expect.</p> <h2 id=\"docker-content-trust\">Docker Content Trust</h2> <p>Engine older than the current 1.10 cannot pull images from repositories that have enabled key delegation. Key delegation is a feature which requires a manual action to enable.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/breaking_changes/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/breaking_changes/</a>\n  </p>\n</div>\n","engine/migration/index":"<h1 id=\"migrate-to-engine-1-10\">Migrate to Engine 1.10</h1> <p>Starting from version 1.10 of Docker Engine, we completely change the way image data is addressed on disk. Previously, every image and layer used a randomly assigned UUID. In 1.10 we implemented a content addressable method using an ID, based on a secure hash of the image and layer data.</p> <p>The new method gives users more security, provides a built-in way to avoid ID collisions and guarantee data integrity after pull, push, load, or save. It also brings better sharing of layers by allowing many images to freely share their layers even if they didn’t come from the same build.</p> <p>Addressing images by their content also lets us more easily detect if something has already been downloaded. Because we have separated images and layers, you don’t have to pull the configurations for every image that was part of the original build chain. We also don’t need to create layers for the build instructions that didn’t modify the filesystem.</p> <p>Content addressability is the foundation for the new distribution features. The image pull and push code has been reworked to use a download/upload manager concept that makes pushing and pulling images much more stable and mitigates any parallel request issues. The download manager also brings retries on failed downloads and better prioritization for concurrent downloads.</p> <p>We are also introducing a new manifest format that is built on top of the content addressable base. It directly references the content addressable image configuration and layer checksums. The new manifest format also makes it possible for a manifest list to be used for targeting multiple architectures/platforms. Moving to the new manifest format will be completely transparent.</p> <h2 id=\"preparing-for-upgrade\">Preparing for upgrade</h2> <p>To make your current images accessible to the new model we have to migrate them to content addressable storage. This means calculating the secure checksums for your current data.</p> <p>All your current images, tags and containers are automatically migrated to the new foundation the first time you start Docker Engine 1.10. Before loading your container, the daemon will calculate all needed checksums for your current data, and after it has completed, all your images and tags will have brand new secure IDs.</p> <p><strong>While this is simple operation, calculating SHA256 checksums for your files can take time if you have lots of image data.</strong> On average you should assume that migrator can process data at a speed of 100MB/s. During this time your Docker daemon won’t be ready to respond to requests.</p> <h2 id=\"minimizing-migration-time\">Minimizing migration time</h2> <p>If you can accept this one time hit, then upgrading Docker Engine and restarting the daemon will transparently migrate your images. However, if you want to minimize the daemon’s downtime, a migration utility can be run while your old daemon is still running.</p> <p>This tool will find all your current images and calculate the checksums for them. After you upgrade and restart the daemon, the checksum data of the migrated images will already exist, freeing the daemon from that computation work. If new images appeared between the migration and the upgrade, those will be processed at time of upgrade to 1.10.</p> <p><a href=\"https://github.com/docker/v1.10-migrator/releases\">You can download the migration tool here.</a></p> <p>The migration tool can also be run as a Docker image. While running the migrator image you need to expose your Docker data directory to the container. If you use the default path then you would run:</p> <pre>$ docker run --rm -v /var/lib/docker:/var/lib/docker docker/v1.10-migrator\n</pre> <p>If you use the devicemapper storage driver, you also need to pass the flag <code>--privileged</code> to give the tool access to your storage devices.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/migration/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/migration/</a>\n  </p>\n</div>\n","engine/reference/api/remote_api_client_libraries/index":"<h1 id=\"docker-remote-api-client-libraries\">Docker Remote API client libraries</h1> <p>These libraries make it easier to build applications on top of the Docker Remote API with various programming languages. They have not been tested by the Docker maintainers for compatibility, so if you run into any issues, file them with the library maintainers.</p> <table border=\"1\" class=\"docutils\"> <colgroup> <col width=\"29%\"> <col width=\"23%\"> <col width=\"48%\"> </colgroup> <thead valign=\"bottom\"> <tr> <th class=\"head\">Language/Framework</th> <th class=\"head\">Name</th> <th class=\"head\">Repository</th> </tr> </thead> <tbody valign=\"top\"> <tr> <td>C#</td> <td>Docker.DotNet</td> <td><a class=\"reference external\" href=\"https://github.com/ahmetalpbalkan/Docker.DotNet\">https://github.com/ahmetalpbalkan/Docker.DotNet</a></td> </tr> <tr> <td>C++</td> <td>lasote/docker_client</td> <td><a class=\"reference external\" href=\"https://github.com/lasote/docker_client\">https://github.com/lasote/docker_client</a></td> </tr> <tr> <td>Erlang</td> <td>erldocker</td> <td><a class=\"reference external\" href=\"https://github.com/proger/erldocker\">https://github.com/proger/erldocker</a></td> </tr> <tr> <td>Dart</td> <td>bwu_docker</td> <td><a class=\"reference external\" href=\"https://github.com/bwu-dart/bwu_docker\">https://github.com/bwu-dart/bwu_docker</a></td> </tr> <tr> <td>Go</td> <td>engine-api</td> <td><a class=\"reference external\" href=\"https://github.com/docker/engine-api\">https://github.com/docker/engine-api</a></td> </tr> <tr> <td>Gradle</td> <td>gradle-docker-plugin</td> <td><a class=\"reference external\" href=\"https://github.com/gesellix/gradle-docker-plugin\">https://github.com/gesellix/gradle-docker-plugin</a></td> </tr> <tr> <td>Groovy</td> <td>docker-client</td> <td><a class=\"reference external\" href=\"https://github.com/gesellix/docker-client\">https://github.com/gesellix/docker-client</a></td> </tr> <tr> <td>Haskell</td> <td>docker-hs</td> <td><a class=\"reference external\" href=\"https://github.com/denibertovic/docker-hs\">https://github.com/denibertovic/docker-hs</a></td> </tr> <tr> <td>HTML (Web Components)</td> <td>docker-elements</td> <td><a class=\"reference external\" href=\"https://github.com/kapalhq/docker-elements\">https://github.com/kapalhq/docker-elements</a></td> </tr> <tr> <td>Java</td> <td>docker-java</td> <td><a class=\"reference external\" href=\"https://github.com/docker-java/docker-java\">https://github.com/docker-java/docker-java</a></td> </tr> <tr> <td>Java</td> <td>docker-client</td> <td><a class=\"reference external\" href=\"https://github.com/spotify/docker-client\">https://github.com/spotify/docker-client</a></td> </tr> <tr> <td>NodeJS</td> <td>dockerode</td> <td><a class=\"reference external\" href=\"https://github.com/apocas/dockerode\">https://github.com/apocas/dockerode</a></td> </tr> <tr> <td>Perl</td> <td>Eixo::Docker</td> <td><a class=\"reference external\" href=\"https://github.com/alambike/eixo-docker\">https://github.com/alambike/eixo-docker</a></td> </tr> <tr> <td>PHP</td> <td>Docker-PHP</td> <td><a class=\"reference external\" href=\"https://github.com/docker-php/docker-php\">https://github.com/docker-php/docker-php</a></td> </tr> <tr> <td>Python</td> <td>docker-py</td> <td><a class=\"reference external\" href=\"https://github.com/docker/docker-py\">https://github.com/docker/docker-py</a></td> </tr> <tr> <td>Ruby</td> <td>docker-api</td> <td><a class=\"reference external\" href=\"https://github.com/swipely/docker-api\">https://github.com/swipely/docker-api</a></td> </tr> <tr> <td>Rust</td> <td>docker-rust</td> <td><a class=\"reference external\" href=\"https://github.com/abh1nav/docker-rust\">https://github.com/abh1nav/docker-rust</a></td> </tr> <tr> <td>Rust</td> <td>shiplift</td> <td><a class=\"reference external\" href=\"https://github.com/softprops/shiplift\">https://github.com/softprops/shiplift</a></td> </tr> <tr> <td>Scala</td> <td>tugboat</td> <td><a class=\"reference external\" href=\"https://github.com/softprops/tugboat\">https://github.com/softprops/tugboat</a></td> </tr> <tr> <td>Scala</td> <td>reactive-docker</td> <td><a class=\"reference external\" href=\"https://github.com/almoehi/reactive-docker\">https://github.com/almoehi/reactive-docker</a></td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/remote_api_client_libraries/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/remote_api_client_libraries/</a>\n  </p>\n</div>\n","engine/reference/api/docker_io_accounts_api/index":"<h1 id=\"docker-io-accounts-api\">docker.io accounts API</h1> <h2 id=\"get-a-single-user\">Get a single user</h2> <p><code>GET /api/v1.1/users/:username/</code></p> <p>Get profile info for the specified user.</p> <p>Parameters:</p> <ul> <li>\n<strong>username</strong> – username of the user whose profile info is being requested.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>Authorization</strong> – required authentication credentials of either type HTTP Basic or OAuth Bearer Token.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – success, user data returned.</li> <li>\n<strong>401</strong> – authentication error.</li> <li>\n<strong>403</strong> – permission error, authenticated user must be the user whose data is being requested, OAuth access tokens must have <code>profile_read</code> scope.</li> <li>\n<strong>404</strong> – the specified username does not exist.</li> </ul> <p><strong>Example request</strong>:</p> <pre>    GET /api/v1.1/users/janedoe/ HTTP/1.1\n    Host: www.docker.io\n    Accept: application/json\n    Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\n        \"id\": 2,\n        \"username\": \"janedoe\",\n        \"url\": \"https://www.docker.io/api/v1.1/users/janedoe/\",\n        \"date_joined\": \"2014-02-12T17:58:01.431312Z\",\n        \"type\": \"User\",\n        \"full_name\": \"Jane Doe\",\n        \"location\": \"San Francisco, CA\",\n        \"company\": \"Success, Inc.\",\n        \"profile_url\": \"https://docker.io/\",\n        \"gravatar_url\": \"https://secure.gravatar.com/avatar/0212b397124be4acd4e7dea9aa357.jpg?s=80&amp;r=g&amp;d=mm\"\n        \"email\": \"jane.doe@example.com\",\n        \"is_active\": true\n    }\n</pre> <h2 id=\"update-a-single-user\">Update a single user</h2> <p><code>PATCH /api/v1.1/users/:username/</code></p> <p>Update profile info for the specified user.</p> <p>Parameters:</p> <ul> <li>\n<strong>username</strong> – username of the user whose profile info is being updated.</li> </ul> <p>Json Parameters:</p> <ul> <li>\n<strong>full_name</strong> (<em>string</em>) – (optional) the new name of the user.</li> <li>\n<strong>location</strong> (<em>string</em>) – (optional) the new location.</li> <li>\n<strong>company</strong> (<em>string</em>) – (optional) the new company of the user.</li> <li>\n<strong>profile_url</strong> (<em>string</em>) – (optional) the new profile url.</li> <li>\n<strong>gravatar_email</strong> (<em>string</em>) – (optional) the new Gravatar email address.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>Authorization</strong> – required authentication credentials of either type HTTP Basic or OAuth Bearer Token.</li> <li>\n<strong>Content-Type</strong> – MIME Type of post data. JSON, url-encoded form data, etc.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – success, user data updated.</li> <li>\n<strong>400</strong> – post data validation error.</li> <li>\n<strong>401</strong> – authentication error.</li> <li>\n<strong>403</strong> – permission error, authenticated user must be the user whose data is being updated, OAuth access tokens must have <code>profile_write</code> scope.</li> <li>\n<strong>404</strong> – the specified username does not exist.</li> </ul> <p><strong>Example request</strong>:</p> <pre>    PATCH /api/v1.1/users/janedoe/ HTTP/1.1\n    Host: www.docker.io\n    Accept: application/json\n    Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=\n\n    {\n        \"location\": \"Private Island\",\n        \"profile_url\": \"http://janedoe.com/\",\n        \"company\": \"Retired\",\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\n        \"id\": 2,\n        \"username\": \"janedoe\",\n        \"url\": \"https://www.docker.io/api/v1.1/users/janedoe/\",\n        \"date_joined\": \"2014-02-12T17:58:01.431312Z\",\n        \"type\": \"User\",\n        \"full_name\": \"Jane Doe\",\n        \"location\": \"Private Island\",\n        \"company\": \"Retired\",\n        \"profile_url\": \"http://janedoe.com/\",\n        \"gravatar_url\": \"https://secure.gravatar.com/avatar/0212b397124be4acd4e7dea9aa357.jpg?s=80&amp;r=g&amp;d=mm\"\n        \"email\": \"jane.doe@example.com\",\n        \"is_active\": true\n    }\n</pre> <h2 id=\"list-email-addresses-for-a-user\">List email addresses for a user</h2> <p><code>GET /api/v1.1/users/:username/emails/</code></p> <p>List email info for the specified user.</p> <p>Parameters:</p> <ul> <li>\n<strong>username</strong> – username of the user whose profile info is being updated.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>Authorization</strong> – required authentication credentials of either type HTTP Basic or OAuth Bearer Token</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – success, user data updated.</li> <li>\n<strong>401</strong> – authentication error.</li> <li>\n<strong>403</strong> – permission error, authenticated user must be the user whose data is being requested, OAuth access tokens must have <code>email_read</code> scope.</li> <li>\n<strong>404</strong> – the specified username does not exist.</li> </ul> <p><strong>Example request</strong>:</p> <pre>    GET /api/v1.1/users/janedoe/emails/ HTTP/1.1\n    Host: www.docker.io\n    Accept: application/json\n    Authorization: Bearer zAy0BxC1wDv2EuF3tGs4HrI6qJp6KoL7nM\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    [\n        {\n            \"email\": \"jane.doe@example.com\",\n            \"verified\": true,\n            \"primary\": true\n        }\n    ]\n</pre> <h2 id=\"add-email-address-for-a-user\">Add email address for a user</h2> <p><code>POST /api/v1.1/users/:username/emails/</code></p> <p>Add a new email address to the specified user’s account. The email address must be verified separately, a confirmation email is not automatically sent.</p> <p>Json Parameters:</p> <ul> <li>\n<strong>email</strong> (<em>string</em>) – email address to be added.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>Authorization</strong> – required authentication credentials of either type HTTP Basic or OAuth Bearer Token.</li> <li>\n<strong>Content-Type</strong> – MIME Type of post data. JSON, url-encoded form data, etc.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – success, new email added.</li> <li>\n<strong>400</strong> – data validation error.</li> <li>\n<strong>401</strong> – authentication error.</li> <li>\n<strong>403</strong> – permission error, authenticated user must be the user whose data is being requested, OAuth access tokens must have <code>email_write</code> scope.</li> <li>\n<strong>404</strong> – the specified username does not exist.</li> </ul> <p><strong>Example request</strong>:</p> <pre>    POST /api/v1.1/users/janedoe/emails/ HTTP/1.1\n    Host: www.docker.io\n    Accept: application/json\n    Content-Type: application/json\n    Authorization: Bearer zAy0BxC1wDv2EuF3tGs4HrI6qJp6KoL7nM\n\n    {\n        \"email\": \"jane.doe+other@example.com\"\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 201 Created\n    Content-Type: application/json\n\n    {\n        \"email\": \"jane.doe+other@example.com\",\n        \"verified\": false,\n        \"primary\": false\n    }\n</pre> <h2 id=\"delete-email-address-for-a-user\">Delete email address for a user</h2> <p><code>DELETE /api/v1.1/users/:username/emails/</code></p> <p>Delete an email address from the specified user’s account. You cannot delete a user’s primary email address.</p> <p>Json Parameters:</p> <ul> <li>\n<strong>email</strong> (<em>string</em>) – email address to be deleted.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>Authorization</strong> – required authentication credentials of either type HTTP Basic or OAuth Bearer Token.</li> <li>\n<strong>Content-Type</strong> – MIME Type of post data. JSON, url-encoded form data, etc.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – success, email address removed.</li> <li>\n<strong>400</strong> – validation error.</li> <li>\n<strong>401</strong> – authentication error.</li> <li>\n<strong>403</strong> – permission error, authenticated user must be the user whose data is being requested, OAuth access tokens must have <code>email_write</code> scope.</li> <li>\n<strong>404</strong> – the specified username or email address does not exist.</li> </ul> <p><strong>Example request</strong>:</p> <pre>    DELETE /api/v1.1/users/janedoe/emails/ HTTP/1.1\n    Host: www.docker.io\n    Accept: application/json\n    Content-Type: application/json\n    Authorization: Bearer zAy0BxC1wDv2EuF3tGs4HrI6qJp6KoL7nM\n\n    {\n        \"email\": \"jane.doe+other@example.com\"\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 NO CONTENT\n    Content-Length: 0\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/docker_io_accounts_api/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/docker_io_accounts_api/</a>\n  </p>\n</div>\n","engine/deprecated/index":"<h1 id=\"deprecated-engine-features\">Deprecated Engine Features</h1> <p>The following list of features are deprecated in Engine.</p> <h3 id=\"e-and-email-flags-on-docker-login\">\n<code>-e</code> and <code>--email</code> flags on <code>docker login</code>\n</h3> <p><strong>Deprecated In Release: v1.11</strong></p> <p><strong>Target For Removal In Release: v1.13</strong></p> <p>The docker login command is removing the ability to automatically register for an account with the target registry if the given username doesn’t exist. Due to this change, the email flag is no longer required, and will be deprecated.</p> <h3 id=\"separator-of-security-opt-flag-on-docker-run\">Separator (<code>:</code>) of <code>--security-opt</code> flag on <code>docker run</code>\n</h3> <p><strong>Deprecated In Release: v1.11</strong></p> <p><strong>Target For Removal In Release: v1.13</strong></p> <p>The flag <code>--security-opt</code> doesn’t use the colon separator(<code>:</code>) anymore to divide keys and values, it uses the equal symbol(<code>=</code>) for consinstency with other similar flags, like <code>--storage-opt</code>.</p> <h3 id=\"ambiguous-event-fields-in-api\">Ambiguous event fields in API</h3> <p><strong>Deprecated In Release: v1.10</strong></p> <p>The fields <code>ID</code>, <code>Status</code> and <code>From</code> in the events API have been deprecated in favor of a more rich structure. See the events API documentation for the new format.</p> <h3 id=\"f-flag-on-docker-tag\">\n<code>-f</code> flag on <code>docker tag</code>\n</h3> <p><strong>Deprecated In Release: v1.10</strong></p> <p><strong>Target For Removal In Release: v1.12</strong></p> <p>To make tagging consistent across the various <code>docker</code> commands, the <code>-f</code> flag on the <code>docker tag</code> command is deprecated. It is not longer necessary to specify <code>-f</code> to move a tag from one image to another. Nor will <code>docker</code> generate an error if the <code>-f</code> flag is missing and the specified tag is already in use.</p> <h3 id=\"hostconfig-at-api-container-start\">HostConfig at API container start</h3> <p><strong>Deprecated In Release: v1.10</strong></p> <p><strong>Target For Removal In Release: v1.12</strong></p> <p>Passing an <code>HostConfig</code> to <code>POST /containers/{name}/start</code> is deprecated in favor of defining it at container creation (<code>POST /containers/create</code>).</p> <h3 id=\"docker-ps-before-and-since-options\">Docker ps ‘before’ and ‘since’ options</h3> <p><strong>Deprecated In Release: <a href=\"https://github.com/docker/docker/releases/tag/v1.10.0\">v1.10.0</a></strong></p> <p><strong>Target For Removal In Release: v1.12</strong></p> <p>The <code>docker ps --before</code> and <code>docker ps --since</code> options are deprecated. Use <code>docker ps --filter=before=...</code> and <code>docker ps --filter=since=...</code> instead.</p> <h3 id=\"command-line-short-variant-options\">Command line short variant options</h3> <p><strong>Deprecated In Release: v1.9</strong></p> <p><strong>Target For Removal In Release: v1.11</strong></p> <p>The following short variant options are deprecated in favor of their long variants:</p> <pre>docker run -c (--cpu-shares)\ndocker build -c (--cpu-shares)\ndocker create -c (--cpu-shares)\n</pre> <h3 id=\"driver-specific-log-tags\">Driver Specific Log Tags</h3> <p><strong>Deprecated In Release: v1.9</strong></p> <p><strong>Target For Removal In Release: v1.11</strong></p> <p>Log tags are now generated in a standard way across different logging drivers. Because of which, the driver specific log tag options <code>syslog-tag</code>, <code>gelf-tag</code> and <code>fluentd-tag</code> have been deprecated in favor of the generic <code>tag</code> option.</p> <pre>docker --log-driver=syslog --log-opt tag=\"{{.ImageName}}/{{.Name}}/{{.ID}}\"\n</pre> <h3 id=\"lxc-built-in-exec-driver\">LXC built-in exec driver</h3> <p><strong>Deprecated In Release: v1.8</strong></p> <p><strong>Removed In Release: v1.10</strong></p> <p>The built-in LXC execution driver, the lxc-conf flag, and API fields have been removed.</p> <h3 id=\"old-command-line-options\">Old Command Line Options</h3> <p><strong>Deprecated In Release: <a href=\"https://github.com/docker/docker/releases/tag/v1.8.0\">v1.8.0</a></strong></p> <p><strong>Removed In Release: <a href=\"https://github.com/docker/docker/releases/tag/v1.10.0\">v1.10.0</a></strong></p> <p>The flags <code>-d</code> and <code>--daemon</code> are deprecated in favor of the <code>daemon</code> subcommand:</p> <pre>docker daemon -H ...\n</pre> <p>The following single-dash (<code>-opt</code>) variant of certain command line options are deprecated and replaced with double-dash options (<code>--opt</code>):</p> <pre>docker attach -nostdin\ndocker attach -sig-proxy\ndocker build -no-cache\ndocker build -rm\ndocker commit -author\ndocker commit -run\ndocker events -since\ndocker history -notrunc\ndocker images -notrunc\ndocker inspect -format\ndocker ps -beforeId\ndocker ps -notrunc\ndocker ps -sinceId\ndocker rm -link\ndocker run -cidfile\ndocker run -dns\ndocker run -entrypoint\ndocker run -expose\ndocker run -link\ndocker run -lxc-conf\ndocker run -n\ndocker run -privileged\ndocker run -volumes-from\ndocker search -notrunc\ndocker search -stars\ndocker search -t\ndocker search -trusted\ndocker tag -force\n</pre> <p>The following double-dash options are deprecated and have no replacement:</p> <pre>docker run --cpuset\ndocker run --networking\ndocker ps --since-id\ndocker ps --before-id\ndocker search --trusted\n</pre> <h3 id=\"interacting-with-v1-registries\">Interacting with V1 registries</h3> <p>Version 1.9 adds a flag (<code>--disable-legacy-registry=false</code>) which prevents the docker daemon from <code>pull</code>, <code>push</code>, and <code>login</code> operations against v1 registries. Though disabled by default, this signals the intent to deprecate the v1 protocol.</p> <h3 id=\"docker-content-trust-env-passphrase-variables-name-change\">Docker Content Trust ENV passphrase variables name change</h3> <p><strong>Deprecated In Release: v1.9</strong></p> <p><strong>Target For Removal In Release: v1.10</strong></p> <p>As of 1.9, Docker Content Trust Offline key will be renamed to Root key and the Tagging key will be renamed to Repository key. Due to this renaming, we’re also changing the corresponding environment variables</p> <ul> <li>DOCKER_CONTENT_TRUST_OFFLINE_PASSPHRASE will now be named DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE</li> <li>DOCKER_CONTENT_TRUST_TAGGING_PASSPHRASE will now be named DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/deprecated/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/deprecated/</a>\n  </p>\n</div>\n","engine/faq/index":"<h1 id=\"frequently-asked-questions-faq\">Frequently Asked Questions (FAQ)</h1> <p>If you don’t see your question here, feel free to submit new ones to <a href=\"mailto:docs@docker.com\">docs@docker.com</a>. Or, you can fork <a href=\"https://github.com/docker/docker\">the repo</a> and contribute them yourself by editing the documentation sources.</p> <h3 id=\"how-much-does-engine-cost\">How much does Engine cost?</h3> <p>Docker Engine is 100% free. It is open source, so you can use it without paying.</p> <h3 id=\"what-open-source-license-are-you-using\">What open source license are you using?</h3> <p>We are using the Apache License Version 2.0, see it here: <a href=\"https://github.com/docker/docker/blob/master/LICENSE\">https://github.com/docker/docker/blob/master/LICENSE</a></p> <h3 id=\"does-docker-run-on-mac-os-x-or-windows\">Does Docker run on Mac OS X or Windows?</h3> <p>Docker Engine currently runs only on Linux, but you can use VirtualBox to run Engine in a virtual machine on your box, and get the best of both worlds. Check out the <a href=\"../installation/mac/index\"><em>Mac OS X</em></a> and <a href=\"../installation/windows/index\"><em>Microsoft Windows</em></a> installation guides. The small Linux distribution boot2docker can be set up using the Docker Machine tool to be run inside virtual machines on these two operating systems.</p> <blockquote> <p><strong>Note:</strong> if you are using a remote Docker Engine daemon on a VM through Docker Machine, then <em>do not</em> type the <code>sudo</code> before the <code>docker</code> commands shown in the documentation’s examples.</p> </blockquote> <h3 id=\"how-do-containers-compare-to-virtual-machines\">How do containers compare to virtual machines?</h3> <p>They are complementary. VMs are best used to allocate chunks of hardware resources. Containers operate at the process level, which makes them very lightweight and perfect as a unit of software delivery.</p> <h3 id=\"what-does-docker-technology-add-to-just-plain-lxc\">What does Docker technology add to just plain LXC?</h3> <p>Docker technology is not a replacement for LXC. “LXC” refers to capabilities of the Linux kernel (specifically namespaces and control groups) which allow sandboxing processes from one another, and controlling their resource allocations. On top of this low-level foundation of kernel features, Docker offers a high-level tool with several powerful functionalities:</p> <ul> <li><p><em>Portable deployment across machines.</em> Docker defines a format for bundling an application and all its dependencies into a single object which can be transferred to any Docker-enabled machine, and executed there with the guarantee that the execution environment exposed to the application will be the same. LXC implements process sandboxing, which is an important pre-requisite for portable deployment, but that alone is not enough for portable deployment. If you sent me a copy of your application installed in a custom LXC configuration, it would almost certainly not run on my machine the way it does on yours, because it is tied to your machine’s specific configuration: networking, storage, logging, distro, etc. Docker defines an abstraction for these machine-specific settings, so that the exact same Docker container can run - unchanged - on many different machines, with many different configurations.</p></li> <li><p><em>Application-centric.</em> Docker is optimized for the deployment of applications, as opposed to machines. This is reflected in its API, user interface, design philosophy and documentation. By contrast, the <code>lxc</code> helper scripts focus on containers as lightweight machines - basically servers that boot faster and need less RAM. We think there’s more to containers than just that.</p></li> <li><p><em>Automatic build.</em> Docker includes <a href=\"../reference/builder/index\"><em>a tool for developers to automatically assemble a container from their source code</em></a>, with full control over application dependencies, build tools, packaging etc. They are free to use <code>make</code>, <code>maven</code>, <code>chef</code>, <code>puppet</code>, <code>salt,</code> Debian packages, RPMs, source tarballs, or any combination of the above, regardless of the configuration of the machines.</p></li> <li><p><em>Versioning.</em> Docker includes git-like capabilities for tracking successive versions of a container, inspecting the diff between versions, committing new versions, rolling back etc. The history also includes how a container was assembled and by whom, so you get full traceability from the production server all the way back to the upstream developer. Docker also implements incremental uploads and downloads, similar to <code>git pull</code>, so new versions of a container can be transferred by only sending diffs.</p></li> <li><p><em>Component re-use.</em> Any container can be used as a <a href=\"../reference/glossary/index#image\"><em>“base image”</em></a> to create more specialized components. This can be done manually or as part of an automated build. For example you can prepare the ideal Python environment, and use it as a base for 10 different applications. Your ideal PostgreSQL setup can be re-used for all your future projects. And so on.</p></li> <li><p><em>Sharing.</em> Docker has access to a public registry <a href=\"https://hub.docker.com/\">on Docker Hub</a> where thousands of people have uploaded useful images: anything from Redis, CouchDB, PostgreSQL to IRC bouncers to Rails app servers to Hadoop to base images for various Linux distros. The <a href=\"https://docs.docker.com/registry/\"><em>registry</em></a> also includes an official “standard library” of useful containers maintained by the Docker team. The registry itself is open-source, so anyone can deploy their own registry to store and transfer private containers, for internal server deployments for example.</p></li> <li><p><em>Tool ecosystem.</em> Docker defines an API for automating and customizing the creation and deployment of containers. There are a huge number of tools integrating with Docker to extend its capabilities. PaaS-like deployment (Dokku, Deis, Flynn), multi-node orchestration (Maestro, Salt, Mesos, Openstack Nova), management dashboards (docker-ui, Openstack Horizon, Shipyard), configuration management (Chef, Puppet), continuous integration (Jenkins, Strider, Travis), etc. Docker is rapidly establishing itself as the standard for container-based tooling.</p></li> </ul> <h3 id=\"what-is-different-between-a-docker-container-and-a-vm\">What is different between a Docker container and a VM?</h3> <p>There’s a great StackOverflow answer <a href=\"http://stackoverflow.com/questions/16047306/how-is-docker-io-different-from-a-normal-virtual-machine\">showing the differences</a>.</p> <h3 id=\"do-i-lose-my-data-when-the-container-exits\">Do I lose my data when the container exits?</h3> <p>Not at all! Any data that your application writes to disk gets preserved in its container until you explicitly delete the container. The file system for the container persists even after the container halts.</p> <h3 id=\"how-far-do-docker-containers-scale\">How far do Docker containers scale?</h3> <p>Some of the largest server farms in the world today are based on containers. Large web deployments like Google and Twitter, and platform providers such as Heroku and dotCloud all run on container technology, at a scale of hundreds of thousands or even millions of containers running in parallel.</p> <h3 id=\"how-do-i-connect-docker-containers\">How do I connect Docker containers?</h3> <p>Currently the recommended way to connect containers is via the Docker network feature. You can see details of how to <a href=\"../userguide/networking/work-with-networks/index\">work with Docker networks here</a>.</p> <p>Also useful for more flexible service portability is the <a href=\"../admin/ambassador_pattern_linking/index\">Ambassador linking pattern</a>.</p> <h3 id=\"how-do-i-run-more-than-one-process-in-a-docker-container\">How do I run more than one process in a Docker container?</h3> <p>Any capable process supervisor such as <a href=\"http://supervisord.org/\">http://supervisord.org/</a>, runit, s6, or daemontools can do the trick. Docker will start up the process management daemon which will then fork to run additional processes. As long as the processor manager daemon continues to run, the container will continue to as well. You can see a more substantial example <a href=\"../admin/using_supervisord/index\">that uses supervisord here</a>.</p> <h3 id=\"what-platforms-does-docker-run-on\">What platforms does Docker run on?</h3> <p>Linux:</p> <ul> <li>Ubuntu 12.04, 13.04 et al</li> <li>Fedora 19/20+</li> <li>RHEL 6.5+</li> <li>CentOS 6+</li> <li>Gentoo</li> <li>ArchLinux</li> <li>openSUSE 12.3+</li> <li>CRUX 3.0+</li> </ul> <p>Cloud:</p> <ul> <li>Amazon EC2</li> <li>Google Compute Engine</li> <li>Microsoft Azure</li> <li>Rackspace</li> </ul> <h3 id=\"how-do-i-report-a-security-issue-with-docker\">How do I report a security issue with Docker?</h3> <p>You can learn about the project’s security policy <a href=\"https://www.docker.com/security/\">here</a> and report security issues to this <a href=\"mailto:security@docker.com\">mailbox</a>.</p> <h3 id=\"why-do-i-need-to-sign-my-commits-to-docker-with-the-dco\">Why do I need to sign my commits to Docker with the DCO?</h3> <p>Please read <a href=\"http://blog.docker.com/2014/01/docker-code-contributions-require-developer-certificate-of-origin/\">our blog post</a> on the introduction of the DCO.</p> <h3 id=\"when-building-an-image-should-i-prefer-system-libraries-or-bundled-ones\">When building an image, should I prefer system libraries or bundled ones?</h3> <p><em>This is a summary of a discussion on the <a href=\"https://groups.google.com/forum/#!topic/docker-dev/L2RBSPDu1L0\">docker-dev mailing list</a>.</em></p> <p>Virtually all programs depend on third-party libraries. Most frequently, they will use dynamic linking and some kind of package dependency, so that when multiple programs need the same library, it is installed only once.</p> <p>Some programs, however, will bundle their third-party libraries, because they rely on very specific versions of those libraries. For instance, Node.js bundles OpenSSL; MongoDB bundles V8 and Boost (among others).</p> <p>When creating a Docker image, is it better to use the bundled libraries, or should you build those programs so that they use the default system libraries instead?</p> <p>The key point about system libraries is not about saving disk or memory space. It is about security. All major distributions handle security seriously, by having dedicated security teams, following up closely with published vulnerabilities, and disclosing advisories themselves. (Look at the <a href=\"https://www.debian.org/security/\">Debian Security Information</a> for an example of those procedures.) Upstream developers, however, do not always implement similar practices.</p> <p>Before setting up a Docker image to compile a program from source, if you want to use bundled libraries, you should check if the upstream authors provide a convenient way to announce security vulnerabilities, and if they update their bundled libraries in a timely manner. If they don’t, you are exposing yourself (and the users of your image) to security vulnerabilities.</p> <p>Likewise, before using packages built by others, you should check if the channels providing those packages implement similar security best practices. Downloading and installing an “all-in-one” .deb or .rpm sounds great at first, except if you have no way to figure out that it contains a copy of the OpenSSL library vulnerable to the <a href=\"http://heartbleed.com/\">Heartbleed</a> bug.</p> <h3 id=\"why-is-debian-frontend-noninteractive-discouraged-in-dockerfiles\">Why is <code>DEBIAN_FRONTEND=noninteractive</code> discouraged in Dockerfiles?</h3> <p>When building Docker images on Debian and Ubuntu you may have seen errors like:</p> <pre>unable to initialize frontend: Dialog\n</pre> <p>These errors don’t stop the image from being built but inform you that the installation process tried to open a dialog box, but was unable to. Generally, these errors are safe to ignore.</p> <p>Some people circumvent these errors by changing the <code>DEBIAN_FRONTEND</code> environment variable inside the Dockerfile using:</p> <pre>ENV DEBIAN_FRONTEND=noninteractive\n</pre> <p>This prevents the installer from opening dialog boxes during installation which stops the errors.</p> <p>While this may sound like a good idea, it <em>may</em> have side effects. The <code>DEBIAN_FRONTEND</code> environment variable will be inherited by all images and containers built from your image, effectively changing their behavior. People using those images will run into problems when installing software interactively, because installers will not show any dialog boxes.</p> <p>Because of this, and because setting <code>DEBIAN_FRONTEND</code> to <code>noninteractive</code> is mainly a ‘cosmetic’ change, we <em>discourage</em> changing it.</p> <p>If you <em>really</em> need to change its setting, make sure to change it back to its <a href=\"https://www.debian.org/releases/stable/i386/ch05s03.html.en\">default value</a> afterwards.</p> <h3 id=\"why-do-i-get-connection-reset-by-peer-when-making-a-request-to-a-service-running-in-a-container\">Why do I get <code>Connection reset by peer</code> when making a request to a service running in a container?</h3> <p>Typically, this message is returned if the service is already bound to your localhost. As a result, requests coming to the container from outside are dropped. To correct this problem, change the service’s configuration on your localhost so that the service accepts requests from all IPs. If you aren’t sure how to do this, check the documentation for your OS.</p> <h3 id=\"why-do-i-get-cannot-connect-to-the-docker-daemon-is-the-docker-daemon-running-on-this-host-when-using-docker-machine\">Why do I get <code>Cannot connect to the Docker daemon. Is the docker daemon running on this host?</code> when using docker-machine?</h3> <p>This error points out that the docker client cannot connect to the virtual machine. This means that either the virtual machine that works underneath <code>docker-machine</code> is not running or that the client doesn’t correctly point at it.</p> <p>To verify that the docker machine is running you can use the <code>docker-machine ls</code> command and start it with <code>docker-machine start</code> if needed.</p> <pre>$ docker-machine ls\nNAME             ACTIVE   DRIVER       STATE     URL   SWARM                   DOCKER    ERRORS\ndefault          -        virtualbox   Stopped                                 Unknown\n\n$ docker-machine start default\n</pre> <p>You have to tell Docker to talk to that machine. You can do this with the <code>docker-machine env</code> command. For example,</p> <pre>$ eval \"$(docker-machine env default)\"\n$ docker ps\n</pre> <h3 id=\"where-can-i-find-more-answers\">Where can I find more answers?</h3> <p>You can find more answers on:</p> <ul> <li><a href=\"https://groups.google.com/d/forum/docker-user\">Docker user mailinglist</a></li> <li><a href=\"https://groups.google.com/d/forum/docker-dev\">Docker developer mailinglist</a></li> <li><a href=\"irc://chat.freenode.net#docker\">IRC, docker on freenode</a></li> <li><a href=\"https://github.com/docker/docker\">GitHub</a></li> <li><a href=\"http://stackoverflow.com/search?q=docker\">Ask questions on Stackoverflow</a></li> <li><a href=\"http://twitter.com/docker\">Join the conversation on Twitter</a></li> </ul> <p>Looking for something else to read? Checkout the <a href=\"../userguide/index\">User Guide</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/faq/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/faq/</a>\n  </p>\n</div>\n","swarm/overview/index":"<h1 id=\"docker-swarm-overview\">Docker Swarm overview</h1> <p>Docker Swarm is native clustering for Docker. It turns a pool of Docker hosts into a single, virtual Docker host. Because Docker Swarm serves the standard Docker API, any tool that already communicates with a Docker daemon can use Swarm to transparently scale to multiple hosts. Supported tools include, but are not limited to, the following:</p> <ul> <li>Dokku</li> <li>Docker Compose</li> <li>Docker Machine</li> <li>Jenkins</li> </ul> <p>And of course, the Docker client itself is also supported.</p> <p>Like other Docker projects, Docker Swarm follows the “swap, plug, and play” principle. As initial development settles, an API will develop to enable pluggable backends. This means you can swap out the scheduling backend Docker Swarm uses out-of-the-box with a backend you prefer. Swarm’s swappable design provides a smooth out-of-box experience for most use cases, and allows large-scale production deployments to swap for more powerful backends, like Mesos.</p> <h2 id=\"understand-swarm-cluster-creation\">Understand Swarm cluster creation</h2> <p>The first step to creating a Swarm cluster on your network is to pull the Docker Swarm image. Then, using Docker, you configure the Swarm manager and all the nodes to run Docker Swarm. This method requires that you:</p> <ul> <li>open a TCP port on each node for communication with the Swarm manager</li> <li>install Docker on each node</li> <li>create and manage TLS certificates to secure your cluster</li> </ul> <p>As a starting point, the manual method is best suited for experienced administrators or programmers contributing to Docker Swarm. The alternative is to use <code>docker-machine</code> to install a cluster.</p> <p>Using Docker Machine, you can quickly install a Docker Swarm on cloud providers or inside your own data center. If you have VirtualBox installed on your local machine, you can quickly build and explore Docker Swarm in your local environment. This method automatically generates a certificate to secure your cluster.</p> <p>Using Docker Machine is the best method for users getting started with Swarm for the first time. To try the recommended method of getting started, see <a href=\"../install-w-machine/index\">Get Started with Docker Swarm</a>.</p> <p>If you are interested manually installing or interested in contributing, see <a href=\"../install-manual/index\">Build a Swarm cluster for production</a>.</p> <h2 id=\"discovery-services\">Discovery services</h2> <p>To dynamically configure and manage the services in your containers, you use a discovery backend with Docker Swarm. For information on which backends are available, see the <a href=\"../discovery/index\">Discovery service</a> documentation.</p> <h2 id=\"advanced-scheduling\">Advanced Scheduling</h2> <p>To learn more about advanced scheduling, see the <a href=\"../scheduler/strategy/index\">strategies</a> and <a href=\"../scheduler/filter/index\">filters</a> documents.</p> <h2 id=\"swarm-api\">Swarm API</h2> <p>The <a href=\"../swarm-api/index\">Docker Swarm API</a> is compatible with the <a href=\"http://docs.docker.com/reference/api/docker_remote_api/\">Docker remote API</a>, and extends it with some new endpoints.</p> <h2 id=\"getting-help\">Getting help</h2> <p>Docker Swarm is still in its infancy and under active development. If you need help, would like to contribute, or simply want to talk about the project with like-minded individuals, we have a number of open channels for communication.</p> <ul> <li><p>To report bugs or file feature requests: please use the <a href=\"https://github.com/docker/swarm/issues\">issue tracker on Github</a>.</p></li> <li><p>To talk about the project with people in real time: please join the <code>#docker-swarm</code> channel on IRC.</p></li> <li><p>To contribute code or documentation changes: please submit a <a href=\"https://github.com/docker/swarm/pulls\">pull request on Github</a>.</p></li> </ul> <p>For more information and resources, please visit the <a href=\"https://docs.docker.com/project/get-help/\">Getting Help project page</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/overview/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/overview/</a>\n  </p>\n</div>\n","swarm/get-swarm/index":"<h1 id=\"how-to-get-docker-swarm\">How to get Docker Swarm</h1> <p>You can create a Docker Swarm cluster using the <code>swarm</code> executable image from a container or using an executable <code>swarm</code> binary you install on your system. This page introduces the two methods and discusses their pros and cons.</p> <h2 id=\"create-a-cluster-with-an-interactive-container\">Create a cluster with an interactive container</h2> <p>You can use the Docker Swarm official image to create a cluster. The image is built by Docker and updated regularly through an automated build. To use the image, you run it a container via the Engine <code>docker run</code> command. The image has multiple options and subcommands you can use to create and manage a Swarm cluster.</p> <p>The first time you use any image, Docker Engine checks to see if you already have the image in your environment. By default Docker runs the <code>swarm:latest</code> version but you can also specify a tag other than <code>latest</code>. If you have an image locally but a newer one exists on Docker Hub, Engine downloads it.</p> <h3 id=\"run-the-swarm-image-from-a-container\">Run the Swarm image from a container</h3> <ol> <li>\n<p>Open a terminal on a host running Engine.</p> <p>If you are using Mac or Windows, then you must make sure you have started a Docker Engine host running and pointed your terminal environment to it with the Docker Machine commands. If you aren’t sure, you can verify:</p> <pre>$ docker-machine ls\nNAME      ACTIVE   URL          STATE     URL                         SWARM   DOCKER    ERRORS\ndefault   *       virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1    \n</pre> <p>This shows an environment running an Engine host on the <code>default</code> instance.</p>\n</li> <li>\n<p>Use the <code>swarm</code> image to execute a command.</p> <p>The easiest command is to get the help for the image. This command shows all the options that are available with the image.</p> <pre>$ docker run swarm --help\nUnable to find image 'swarm:latest' locally\nlatest: Pulling from library/swarm\nd681c900c6e3: Pull complete\n188de6f24f3f: Pull complete\n90b2ffb8d338: Pull complete\n237af4efea94: Pull complete\n3b3fc6f62107: Pull complete\n7e6c9135b308: Pull complete\n986340ab62f0: Pull complete\na9975e2cc0a3: Pull complete\nDigest: sha256:c21fd414b0488637b1f05f13a59b032a3f9da5d818d31da1a4ca98a84c0c781b\nStatus: Downloaded newer image for swarm:latest\nUsage: swarm [OPTIONS] COMMAND [arg...]\n\nA Docker-native clustering system\n\nVersion: 1.0.1 (744e3a3)\n\nOptions:\n  --debug           debug mode [$DEBUG]\n  --log-level, -l \"info\"    Log level (options: debug, info, warn, error, fatal, panic)\n  --help, -h            show help\n  --version, -v         print the version\n\nCommands:\n  create, c Create a cluster\n  list, l   List nodes in a cluster\n  manage, m Manage a docker cluster\n  join, j   join a docker cluster\n  help, h   Shows a list of commands or help for one command\n\nRun 'swarm COMMAND --help' for more information on a command.\n</pre> <p>In this example, the <code>swarm</code> image did not exist on the Engine host, so the Engine downloaded it. After it downloaded, the image executed the <code>help</code> subcommand to display the help text. After displaying the help, the <code>swarm</code> image exits and returns you to your terminal command line.</p>\n</li> <li>\n<p>List the running containers on your Engine host.</p> <pre>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</pre> <p>Swarm is no longer running. The <code>swarm</code> image exits after you issue it a command.</p>\n</li> </ol> <h3 id=\"why-use-the-image\">Why use the image?</h3> <p>Using a Swarm container has three key benefits over other methods:</p> <ul> <li>You don’t need to install a binary on the system to use the image.</li> <li>The single command <code>docker run</code> command gets and run the most recent version of the image every time.</li> <li>The container isolates Swarm from your host environment. You don’t need to perform or maintain shell paths and environments.</li> </ul> <p>Running the Swarm image is the recommended way to create and manage your Swarm cluster. All of Docker’s documentation and tutorials use this method.</p> <h2 id=\"run-a-swarm-binary\">Run a Swarm binary</h2> <p>Before you run a Swarm binary directly on a host operating system (OS), you compile the binary from the source code or get a trusted copy from another location. Then you run the Swarm binary.</p> <p>To compile Swarm from source code, refer to the instructions in <a href=\"http://github.com/docker/swarm/blob/master/CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p> <h3 id=\"why-use-the-binary\">Why use the binary?</h3> <p>Using a Swarm binary this way has one key benefit over other methods: If you are a developer who contributes to the Swarm project, you can test your code changes without “containerizing” the binary before you run it.</p> <p>Running a Swarm binary on the host OS has disadvantages:</p> <ul> <li>Compilation from source is a burden.</li> <li>The binary doesn’t have the benefits that Docker containers provide, such as isolation.</li> <li>Most Docker documentation and tutorials don’t show this method of running swarm.</li> </ul> <p>Lastly, because the Swarm nodes don’t use Engine, you can’t use Docker-based software tools, such as Docker Engine CLI at the node level.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li>\n<a href=\"https://hub.docker.com/_/swarm/\">Docker Swarm official image</a> repository on Docker Hub</li> <li><a href=\"../provision-with-machine/index\">Provision a Swarm with Docker Machine</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/get-swarm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/get-swarm/</a>\n  </p>\n</div>\n","engine/reference/api/docker_remote_api_v1.23/index":"<h1 id=\"docker-remote-api-v1-23\">Docker Remote API v1.23</h1> <h2 id=\"1-brief-introduction\">1. Brief introduction</h2> <ul> <li>The Remote API has replaced <code>rcli</code>.</li> <li>The daemon listens on <code>unix:///var/run/docker.sock</code> but you can <a href=\"../../../quickstart/index#bind-docker-to-another-host-port-or-a-unix-socket\">Bind Docker to another host/port or a Unix socket</a>.</li> <li>The API tends to be REST. However, for some complex commands, like <code>attach</code> or <code>pull</code>, the HTTP connection is hijacked to transport <code>stdout</code>, <code>stdin</code> and <code>stderr</code>.</li> <li>When the client API version is newer than the daemon’s, these calls return an HTTP <code>400 Bad Request</code> error message.</li> </ul> <h1 id=\"2-endpoints\">2. Endpoints</h1> <h2 id=\"2-1-containers\">2.1 Containers</h2> <h3 id=\"list-containers\">List containers</h3> <p><code>GET /containers/json</code></p> <p>List containers</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/json?all=1&amp;before=8dfafdbc3a40&amp;size=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Id\": \"8dfafdbc3a40\",\n             \"Names\":[\"/boring_feynman\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 1\",\n             \"Created\": 1367854155,\n             \"State\": \"Exited\",\n             \"Status\": \"Exit 0\",\n             \"Ports\": [{\"PrivatePort\": 2222, \"PublicPort\": 3333, \"Type\": \"tcp\"}],\n             \"Labels\": {\n                     \"com.example.vendor\": \"Acme\",\n                     \"com.example.license\": \"GPL\",\n                     \"com.example.version\": \"1.0\"\n             },\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"2cdc4edb1ded3631c81f57966563e5c8525b81121bb3706a9a9a3ae102711f3f\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.2\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:02\"\n                              }\n                     }\n             },\n             \"Mounts\": [\n                     {\n                              \"Name\": \"fac362...80535\",\n                              \"Source\": \"/data\",\n                              \"Destination\": \"/data\",\n                              \"Driver\": \"local\",\n                              \"Mode\": \"ro,Z\",\n                              \"RW\": false,\n                              \"Propagation\": \"\"\n                     }\n             ]\n     },\n     {\n             \"Id\": \"9cd87474be90\",\n             \"Names\":[\"/coolName\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 222222\",\n             \"Created\": 1367854155,\n             \"State\": \"Exited\",\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"88eaed7b37b38c2a3f0c4bc796494fdf51b270c2d22656412a2ca5d559a64d7a\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.8\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:08\"\n                              }\n                     }\n             },\n             \"Mounts\": []\n     },\n     {\n             \"Id\": \"3176a2479c92\",\n             \"Names\":[\"/sleepy_dog\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 3333333333333333\",\n             \"Created\": 1367854154,\n             \"State\": \"Exited\",\n             \"Status\": \"Exit 0\",\n             \"Ports\":[],\n             \"Labels\": {},\n             \"SizeRw\":12288,\n             \"SizeRootFs\":0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"8b27c041c30326d59cd6e6f510d4f8d1d570a228466f956edf7815508f78e30d\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.6\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:06\"\n                              }\n                     }\n             },\n             \"Mounts\": []\n     },\n     {\n             \"Id\": \"4cb07b47f9fb\",\n             \"Names\":[\"/running_cat\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 444444444444444444444444444444444\",\n             \"Created\": 1367854152,\n             \"State\": \"Exited\",\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"d91c7b2f0644403d7ef3095985ea0e2370325cd2332ff3a3225c4247328e66e9\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.5\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:05\"\n                              }\n                     }\n             },\n             \"Mounts\": []\n     }\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, Show all containers. Only running containers are shown by default (i.e., this defaults to false)</li> <li>\n<strong>limit</strong> – Show <code>limit</code> last created containers, include non-running ones.</li> <li>\n<strong>since</strong> – Show only containers created since Id, include non-running ones.</li> <li>\n<strong>before</strong> – Show only containers created before Id, include non-running ones.</li> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, Show the containers sizes</li> <li>\n<strong>filters</strong> - a JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the containers list. Available filters: <ul> <li>\n<code>exited=&lt;int&gt;</code>; -- containers with exit code of <code>&lt;int&gt;</code> ;</li> <li>\n<code>status=</code>(<code>created</code>|<code>restarting</code>|<code>running</code>|<code>paused</code>|<code>exited</code>|<code>dead</code>)</li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of a container label</li> <li>\n<code>isolation=</code>(<code>default</code>|<code>process</code>|<code>hyperv</code>) (Windows daemon only)</li> <li>\n<code>ancestor</code>=(<code>&lt;image-name&gt;[:&lt;tag&gt;]</code>, <code>&lt;image id&gt;</code> or <code>&lt;image@digest&gt;</code>)</li> <li>\n<code>before</code>=(<code>&lt;container id&gt;</code> or <code>&lt;container name&gt;</code>)</li> <li>\n<code>since</code>=(<code>&lt;container id&gt;</code> or <code>&lt;container name&gt;</code>)</li> <li>\n<code>volume</code>=(<code>&lt;volume name&gt;</code> or <code>&lt;mount point destination&gt;</code>)</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-a-container\">Create a container</h3> <p><code>POST /containers/create</code></p> <p>Create a container</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/create HTTP/1.1\nContent-Type: application/json\n\n{\n       \"Hostname\": \"\",\n       \"Domainname\": \"\",\n       \"User\": \"\",\n       \"AttachStdin\": false,\n       \"AttachStdout\": true,\n       \"AttachStderr\": true,\n       \"Tty\": false,\n       \"OpenStdin\": false,\n       \"StdinOnce\": false,\n       \"Env\": [\n               \"FOO=bar\",\n               \"BAZ=quux\"\n       ],\n       \"Cmd\": [\n               \"date\"\n       ],\n       \"Entrypoint\": \"\",\n       \"Image\": \"ubuntu\",\n       \"Labels\": {\n               \"com.example.vendor\": \"Acme\",\n               \"com.example.license\": \"GPL\",\n               \"com.example.version\": \"1.0\"\n       },\n       \"Volumes\": {\n         \"/volumes/data\": {}\n       },\n       \"WorkingDir\": \"\",\n       \"NetworkDisabled\": false,\n       \"MacAddress\": \"12:34:56:78:9a:bc\",\n       \"ExposedPorts\": {\n               \"22/tcp\": {}\n       },\n       \"StopSignal\": \"SIGTERM\",\n       \"HostConfig\": {\n         \"Binds\": [\"/tmp:/tmp\"],\n         \"Links\": [\"redis3:redis\"],\n         \"Memory\": 0,\n         \"MemorySwap\": 0,\n         \"MemoryReservation\": 0,\n         \"KernelMemory\": 0,\n         \"CpuShares\": 512,\n         \"CpuPeriod\": 100000,\n         \"CpuQuota\": 50000,\n         \"CpusetCpus\": \"0,1\",\n         \"CpusetMems\": \"0,1\",\n         \"BlkioWeight\": 300,\n         \"BlkioWeightDevice\": [{}],\n         \"BlkioDeviceReadBps\": [{}],\n         \"BlkioDeviceReadIOps\": [{}],\n         \"BlkioDeviceWriteBps\": [{}],\n         \"BlkioDeviceWriteIOps\": [{}],\n         \"MemorySwappiness\": 60,\n         \"OomKillDisable\": false,\n         \"OomScoreAdj\": 500,\n         \"PidsLimit\": -1,\n         \"PortBindings\": { \"22/tcp\": [{ \"HostPort\": \"11022\" }] },\n         \"PublishAllPorts\": false,\n         \"Privileged\": false,\n         \"ReadonlyRootfs\": false,\n         \"Dns\": [\"8.8.8.8\"],\n         \"DnsOptions\": [\"\"],\n         \"DnsSearch\": [\"\"],\n         \"ExtraHosts\": null,\n         \"VolumesFrom\": [\"parent\", \"other:ro\"],\n         \"CapAdd\": [\"NET_ADMIN\"],\n         \"CapDrop\": [\"MKNOD\"],\n         \"GroupAdd\": [\"newgroup\"],\n         \"RestartPolicy\": { \"Name\": \"\", \"MaximumRetryCount\": 0 },\n         \"NetworkMode\": \"bridge\",\n         \"Devices\": [],\n         \"Ulimits\": [{}],\n         \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} },\n         \"SecurityOpt\": [],\n         \"CgroupParent\": \"\",\n         \"VolumeDriver\": \"\",\n         \"ShmSize\": 67108864\n      },\n      \"NetworkingConfig\": {\n      \"EndpointsConfig\": {\n          \"isolated_nw\" : {\n              \"IPAMConfig\": {\n                  \"IPv4Address\":\"172.20.30.33\",\n                  \"IPv6Address\":\"2001:db8:abcd::3033\"\n              },\n              \"Links\":[\"container_1\", \"container_2\"],\n              \"Aliases\":[\"server_x\", \"server_y\"]\n          }\n      }\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 201 Created\n  Content-Type: application/json\n\n  {\n       \"Id\":\"e90e34656806\",\n       \"Warnings\":[]\n  }\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Hostname</strong> - A string value containing the hostname to use for the container.</li> <li>\n<strong>Domainname</strong> - A string value containing the domain name to use for the container.</li> <li>\n<strong>User</strong> - A string value specifying the user inside the container.</li> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code>.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code>.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code>.</li> <li>\n<strong>Tty</strong> - Boolean value, Attach standard streams to a <code>tty</code>, including <code>stdin</code> if it is not closed.</li> <li>\n<strong>OpenStdin</strong> - Boolean value, opens stdin,</li> <li>\n<strong>StdinOnce</strong> - Boolean value, close <code>stdin</code> after the 1 attached client disconnects.</li> <li>\n<strong>Env</strong> - A list of environment variables in the form of <code>[\"VAR=value\"[,\"VAR2=value2\"]]</code>\n</li> <li>\n<strong>Labels</strong> - Adds a map of labels to a container. To specify a map: <code>{\"key\":\"value\"[,\"key2\":\"value2\"]}</code>\n</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> <li>\n<strong>Entrypoint</strong> - Set the entry point for the container as a string or an array of strings.</li> <li>\n<strong>Image</strong> - A string specifying the image name to use for the container.</li> <li>\n<strong>Volumes</strong> - An object mapping mount point paths (strings) inside the container to empty objects.</li> <li>\n<strong>WorkingDir</strong> - A string specifying the working directory for commands to run in.</li> <li>\n<strong>NetworkDisabled</strong> - Boolean value, when true disables networking for the container</li> <li>\n<strong>ExposedPorts</strong> - An object mapping ports to an empty object in the form of: <code>\"ExposedPorts\": { \"&lt;port&gt;/&lt;tcp|udp&gt;: {}\" }</code>\n</li> <li>\n<strong>StopSignal</strong> - Signal to stop a container as a string or unsigned integer. <code>SIGTERM</code> by default.</li> <li>\n<strong>HostConfig</strong> <ul> <li>\n<strong>Binds</strong> – A list of volume bindings for this container. Each volume binding is a string in one of these forms: <ul> <li>\n<code>host_path:container_path</code> to bind-mount a host path into the container</li> <li>\n<code>host_path:container_path:ro</code> to make the bind-mount read-only inside the container.</li> <li>\n<code>volume_name:container_path</code> to bind-mount a volume managed by a volume plugin into the container.</li> <li>\n<code>volume_name:container_path:ro</code> to make the bind mount read-only inside the container.</li> </ul>\n</li> <li>\n<strong>Links</strong> - A list of links for the container. Each link entry should be in the form of <code>container_name:alias</code>.</li> <li>\n<strong>Memory</strong> - Memory limit in bytes.</li> <li>\n<strong>MemorySwap</strong> - Total memory limit (memory + swap); set <code>-1</code> to enable unlimited swap. You must use this with <code>memory</code> and make the swap value larger than <code>memory</code>.</li> <li>\n<strong>MemoryReservation</strong> - Memory soft limit in bytes.</li> <li>\n<strong>KernelMemory</strong> - Kernel memory limit in bytes.</li> <li>\n<strong>CpuShares</strong> - An integer value containing the container’s CPU Shares (ie. the relative weight vs other containers).</li> <li>\n<strong>CpuPeriod</strong> - The length of a CPU period in microseconds.</li> <li>\n<strong>CpuQuota</strong> - Microseconds of CPU time that the container can get in a CPU period.</li> <li>\n<strong>CpusetCpus</strong> - String value containing the <code>cgroups CpusetCpus</code> to use.</li> <li>\n<strong>CpusetMems</strong> - Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems.</li> <li>\n<strong>BlkioWeight</strong> - Block IO weight (relative weight) accepts a weight value between 10 and 1000.</li> <li>\n<strong>BlkioWeightDevice</strong> - Block IO weight (relative device weight) in the form of: <code>\"BlkioWeightDevice\": [{\"Path\": \"device_path\", \"Weight\": weight}]</code>\n</li> <li>\n<strong>BlkioDeviceReadBps</strong> - Limit read rate (bytes per second) from a device in the form of: <code>\"BlkioDeviceReadBps\": [{\"Path\": \"device_path\", \"Rate\": rate}]</code>, for example: <code>\"BlkioDeviceReadBps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1024\"}]\"</code>\n</li> <li>\n<strong>BlkioDeviceWriteBps</strong> - Limit write rate (bytes per second) to a device in the form of: <code>\"BlkioDeviceWriteBps\": [{\"Path\": \"device_path\", \"Rate\": rate}]</code>, for example: <code>\"BlkioDeviceWriteBps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1024\"}]\"</code>\n</li> <li>\n<strong>BlkioDeviceReadIOps</strong> - Limit read rate (IO per second) from a device in the form of: <code>\"BlkioDeviceReadIOps\": [{\"Path\": \"device_path\", \"Rate\": rate}]</code>, for example: <code>\"BlkioDeviceReadIOps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1000\"}]</code>\n</li> <li>\n<strong>BlkioDeviceWiiteIOps</strong> - Limit write rate (IO per second) to a device in the form of: <code>\"BlkioDeviceWriteIOps\": [{\"Path\": \"device_path\", \"Rate\": rate}]</code>, for example: <code>\"BlkioDeviceWriteIOps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1000\"}]</code>\n</li> <li>\n<strong>MemorySwappiness</strong> - Tune a container’s memory swappiness behavior. Accepts an integer between 0 and 100.</li> <li>\n<strong>OomKillDisable</strong> - Boolean value, whether to disable OOM Killer for the container or not.</li> <li>\n<strong>OomScoreAdj</strong> - An integer value containing the score given to the container in order to tune OOM killer preferences.</li> <li>\n<strong>PidsLimit</strong> - Tune a container’s pids limit. Set -1 for unlimited.</li> <li>\n<strong>PortBindings</strong> - A map of exposed container ports and the host port they should map to. A JSON object in the form <code>{ &lt;port&gt;/&lt;protocol&gt;: [{ \"HostPort\": \"&lt;port&gt;\" }] }</code> Take note that <code>port</code> is specified as a string and not an integer value.</li> <li>\n<strong>PublishAllPorts</strong> - Allocates a random host port for all of a container’s exposed ports. Specified as a boolean value.</li> <li>\n<strong>Privileged</strong> - Gives the container full access to the host. Specified as a boolean value.</li> <li>\n<strong>ReadonlyRootfs</strong> - Mount the container’s root filesystem as read only. Specified as a boolean value.</li> <li>\n<strong>Dns</strong> - A list of DNS servers for the container to use.</li> <li>\n<strong>DnsOptions</strong> - A list of DNS options</li> <li>\n<strong>DnsSearch</strong> - A list of DNS search domains</li> <li>\n<strong>ExtraHosts</strong> - A list of hostnames/IP mappings to add to the container’s <code>/etc/hosts</code> file. Specified in the form <code>[\"hostname:IP\"]</code>.</li> <li>\n<strong>VolumesFrom</strong> - A list of volumes to inherit from another container. Specified in the form <code>&lt;container name&gt;[:&lt;ro|rw&gt;]</code>\n</li> <li>\n<strong>CapAdd</strong> - A list of kernel capabilities to add to the container.</li> <li>\n<strong>Capdrop</strong> - A list of kernel capabilities to drop from the container.</li> <li>\n<strong>GroupAdd</strong> - A list of additional groups that the container process will run as</li> <li>\n<strong>RestartPolicy</strong> – The behavior to apply when the container exits. The value is an object with a <code>Name</code> property of either <code>\"always\"</code> to always restart, <code>\"unless-stopped\"</code> to restart always except when user has manually stopped the container or <code>\"on-failure\"</code> to restart only when the container exit code is non-zero. If <code>on-failure</code> is used, <code>MaximumRetryCount</code> controls the number of times to retry before giving up. The default is not to restart. (optional) An ever increasing delay (double the previous delay, starting at 100mS) is added before each restart to prevent flooding the server.</li> <li>\n<strong>UsernsMode</strong> - Sets the usernamespace mode for the container when usernamespace remapping option is enabled. supported values are: <code>host</code>.</li> <li>\n<strong>NetworkMode</strong> - Sets the networking mode for the container. Supported standard values are: <code>bridge</code>, <code>host</code>, <code>none</code>, and <code>container:&lt;name|id&gt;</code>. Any other value is taken as a custom network’s name to which this container should connect to.</li> <li>\n<strong>Devices</strong> - A list of devices to add to the container specified as a JSON object in the form <code>{ \"PathOnHost\": \"/dev/deviceName\", \"PathInContainer\": \"/dev/deviceName\", \"CgroupPermissions\": \"mrw\"}</code>\n</li> <li>\n<strong>Ulimits</strong> - A list of ulimits to set in the container, specified as <code>{ \"Name\": &lt;name&gt;, \"Soft\": &lt;soft limit&gt;, \"Hard\": &lt;hard limit&gt; }</code>, for example: <code>Ulimits: { \"Name\": \"nofile\", \"Soft\": 1024, \"Hard\": 2048 }</code>\n</li> <li>\n<strong>SecurityOpt</strong>: A list of string values to customize labels for MLS systems, such as SELinux.</li> <li>\n<strong>LogConfig</strong> - Log configuration for the container, specified as a JSON object in the form <code>{ \"Type\": \"&lt;driver_name&gt;\", \"Config\": {\"key1\": \"val1\"}}</code>. Available types: <code>json-file</code>, <code>syslog</code>, <code>journald</code>, <code>gelf</code>, <code>fluentd</code>, <code>awslogs</code>, <code>splunk</code>, <code>etwlogs</code>, <code>none</code>. <code>json-file</code> logging driver.</li> <li>\n<strong>CgroupParent</strong> - Path to <code>cgroups</code> under which the container’s <code>cgroup</code> is created. If the path is not absolute, the path is considered to be relative to the <code>cgroups</code> path of the init process. Cgroups are created if they do not already exist.</li> <li>\n<strong>VolumeDriver</strong> - Driver that this container users to mount volumes.</li> <li>\n<strong>ShmSize</strong> - Size of <code>/dev/shm</code> in bytes. The size must be greater than 0. If omitted the system uses 64MB.</li> </ul>\n</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – Assign the specified name to the container. Must match <code>/?[a-zA-Z0-9_-]+</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>406</strong> – impossible to attach (container not running)</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-a-container\">Inspect a container</h3> <p><code>GET /containers/(id or name)/json</code></p> <p>Return low-level information on the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>  GET /containers/4fa6e0f0c678/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"AppArmorProfile\": \"\",\n    \"Args\": [\n        \"-c\",\n        \"exit 9\"\n    ],\n    \"Config\": {\n        \"AttachStderr\": true,\n        \"AttachStdin\": false,\n        \"AttachStdout\": true,\n        \"Cmd\": [\n            \"/bin/sh\",\n            \"-c\",\n            \"exit 9\"\n        ],\n        \"Domainname\": \"\",\n        \"Entrypoint\": null,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": null,\n        \"Hostname\": \"ba033ac44011\",\n        \"Image\": \"ubuntu\",\n        \"Labels\": {\n            \"com.example.vendor\": \"Acme\",\n            \"com.example.license\": \"GPL\",\n            \"com.example.version\": \"1.0\"\n        },\n        \"MacAddress\": \"\",\n        \"NetworkDisabled\": false,\n        \"OnBuild\": null,\n        \"OpenStdin\": false,\n        \"StdinOnce\": false,\n        \"Tty\": false,\n        \"User\": \"\",\n        \"Volumes\": {\n            \"/volumes/data\": {}\n        },\n        \"WorkingDir\": \"\",\n        \"StopSignal\": \"SIGTERM\"\n    },\n    \"Created\": \"2015-01-06T15:47:31.485331387Z\",\n    \"Driver\": \"devicemapper\",\n    \"ExecDriver\": \"native-0.2\",\n    \"ExecIDs\": null,\n    \"HostConfig\": {\n        \"Binds\": null,\n        \"BlkioWeight\": 0,\n        \"BlkioWeightDevice\": [{}],\n        \"BlkioDeviceReadBps\": [{}],\n        \"BlkioDeviceWriteBps\": [{}],\n        \"BlkioDeviceReadIOps\": [{}],\n        \"BlkioDeviceWriteIOps\": [{}],\n        \"CapAdd\": null,\n        \"CapDrop\": null,\n        \"ContainerIDFile\": \"\",\n        \"CpusetCpus\": \"\",\n        \"CpusetMems\": \"\",\n        \"CpuShares\": 0,\n        \"CpuPeriod\": 100000,\n        \"Devices\": [],\n        \"Dns\": null,\n        \"DnsOptions\": null,\n        \"DnsSearch\": null,\n        \"ExtraHosts\": null,\n        \"IpcMode\": \"\",\n        \"Links\": null,\n        \"LxcConf\": [],\n        \"Memory\": 0,\n        \"MemorySwap\": 0,\n        \"MemoryReservation\": 0,\n        \"KernelMemory\": 0,\n        \"OomKillDisable\": false,\n        \"OomScoreAdj\": 500,\n        \"NetworkMode\": \"bridge\",\n        \"PortBindings\": {},\n        \"Privileged\": false,\n        \"ReadonlyRootfs\": false,\n        \"PublishAllPorts\": false,\n        \"RestartPolicy\": {\n            \"MaximumRetryCount\": 2,\n            \"Name\": \"on-failure\"\n        },\n        \"LogConfig\": {\n            \"Config\": null,\n            \"Type\": \"json-file\"\n        },\n        \"SecurityOpt\": null,\n        \"VolumesFrom\": null,\n        \"Ulimits\": [{}],\n        \"VolumeDriver\": \"\",\n        \"ShmSize\": 67108864\n    },\n    \"HostnamePath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hostname\",\n    \"HostsPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Id\": \"ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39\",\n    \"Image\": \"04c5d3b7b0656168630d3ba35d8889bd0e9caafcaeb3004d2bfbc47e7c5d35d2\",\n    \"MountLabel\": \"\",\n    \"Name\": \"/boring_euclid\",\n    \"NetworkSettings\": {\n        \"Bridge\": \"\",\n        \"SandboxID\": \"\",\n        \"HairpinMode\": false,\n        \"LinkLocalIPv6Address\": \"\",\n        \"LinkLocalIPv6PrefixLen\": 0,\n        \"Ports\": null,\n        \"SandboxKey\": \"\",\n        \"SecondaryIPAddresses\": null,\n        \"SecondaryIPv6Addresses\": null,\n        \"EndpointID\": \"\",\n        \"Gateway\": \"\",\n        \"GlobalIPv6Address\": \"\",\n        \"GlobalIPv6PrefixLen\": 0,\n        \"IPAddress\": \"\",\n        \"IPPrefixLen\": 0,\n        \"IPv6Gateway\": \"\",\n        \"MacAddress\": \"\",\n        \"Networks\": {\n            \"bridge\": {\n                \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                \"EndpointID\": \"7587b82f0dada3656fda26588aee72630c6fab1536d36e394b2bfbcf898c971d\",\n                \"Gateway\": \"172.17.0.1\",\n                \"IPAddress\": \"172.17.0.2\",\n                \"IPPrefixLen\": 16,\n                \"IPv6Gateway\": \"\",\n                \"GlobalIPv6Address\": \"\",\n                \"GlobalIPv6PrefixLen\": 0,\n                \"MacAddress\": \"02:42:ac:12:00:02\"\n            }\n        }\n    },\n    \"Path\": \"/bin/sh\",\n    \"ProcessLabel\": \"\",\n    \"ResolvConfPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/resolv.conf\",\n    \"RestartCount\": 1,\n    \"State\": {\n        \"Error\": \"\",\n        \"ExitCode\": 9,\n        \"FinishedAt\": \"2015-01-06T15:47:32.080254511Z\",\n        \"OOMKilled\": false,\n        \"Dead\": false,\n        \"Paused\": false,\n        \"Pid\": 0,\n        \"Restarting\": false,\n        \"Running\": true,\n        \"StartedAt\": \"2015-01-06T15:47:32.072697474Z\",\n        \"Status\": \"running\"\n    },\n    \"Mounts\": [\n        {\n            \"Name\": \"fac362...80535\",\n            \"Source\": \"/data\",\n            \"Destination\": \"/data\",\n            \"Driver\": \"local\",\n            \"Mode\": \"ro,Z\",\n            \"RW\": false,\n            \"Propagation\": \"\"\n        }\n    ]\n}\n</pre> <p><strong>Example request, with size information</strong>:</p> <pre>GET /containers/4fa6e0f0c678/json?size=1 HTTP/1.1\n</pre> <p><strong>Example response, with size information</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n....\n\"SizeRw\": 0,\n\"SizeRootFs\": 972,\n....\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, return container size information. Default is <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"list-processes-running-inside-a-container\">List processes running inside a container</h3> <p><code>GET /containers/(id or name)/top</code></p> <p>List processes running inside the container <code>id</code>. On Unix systems this is done by running the <code>ps</code> command. This endpoint is not supported on Windows.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Titles\" : [\n     \"UID\", \"PID\", \"PPID\", \"C\", \"STIME\", \"TTY\", \"TIME\", \"CMD\"\n   ],\n   \"Processes\" : [\n     [\n       \"root\", \"13642\", \"882\", \"0\", \"17:03\", \"pts/0\", \"00:00:00\", \"/bin/bash\"\n     ],\n     [\n       \"root\", \"13735\", \"13642\", \"0\", \"17:06\", \"pts/0\", \"00:00:00\", \"sleep 10\"\n     ]\n   ]\n}\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top?ps_args=aux HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Titles\" : [\n    \"USER\",\"PID\",\"%CPU\",\"%MEM\",\"VSZ\",\"RSS\",\"TTY\",\"STAT\",\"START\",\"TIME\",\"COMMAND\"\n  ]\n  \"Processes\" : [\n    [\n      \"root\",\"13642\",\"0.0\",\"0.1\",\"18172\",\"3184\",\"pts/0\",\"Ss\",\"17:03\",\"0:00\",\"/bin/bash\"\n    ],\n    [\n      \"root\",\"13895\",\"0.0\",\"0.0\",\"4348\",\"692\",\"pts/0\",\"S+\",\"17:15\",\"0:00\",\"sleep 10\"\n    ]\n  ],\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>ps_args</strong> – <code>ps</code> arguments to use (e.g., <code>aux</code>), defaults to <code>-ef</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-logs\">Get container logs</h3> <p><code>GET /containers/(id or name)/logs</code></p> <p>Get <code>stdout</code> and <code>stderr</code> logs from the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: This endpoint works only for containers with the <code>json-file</code> or <code>journald</code> logging drivers.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre> GET /containers/4fa6e0f0c678/logs?stderr=1&amp;stdout=1&amp;timestamps=1&amp;follow=1&amp;tail=10&amp;since=1428990821 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre> HTTP/1.1 101 UPGRADED\n Content-Type: application/vnd.docker.raw-stream\n Connection: Upgrade\n Upgrade: tcp\n\n {{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>follow</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, show <code>stdout</code> log. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, show <code>stderr</code> log. Default <code>false</code>.</li> <li>\n<strong>since</strong> – UNIX timestamp (integer) to filter logs. Specifying a timestamp will only output log-entries since that timestamp. Default: 0 (unfiltered)</li> <li>\n<strong>timestamps</strong> – 1/True/true or 0/False/false, print timestamps for every log line. Default <code>false</code>.</li> <li>\n<strong>tail</strong> – Output specified number of lines at the end of logs: <code>all</code> or <code>&lt;number&gt;</code>. Default all.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-changes-on-a-container-s-filesystem\">Inspect changes on a container’s filesystem</h3> <p><code>GET /containers/(id or name)/changes</code></p> <p>Inspect changes on container <code>id</code>’s filesystem</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/changes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Path\": \"/dev\",\n             \"Kind\": 0\n     },\n     {\n             \"Path\": \"/dev/kmsg\",\n             \"Kind\": 1\n     },\n     {\n             \"Path\": \"/test\",\n             \"Kind\": 1\n     }\n]\n</pre> <p>Values for <code>Kind</code>:</p> <ul> <li>\n<code>0</code>: Modify</li> <li>\n<code>1</code>: Add</li> <li>\n<code>2</code>: Delete</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"export-a-container\">Export a container</h3> <p><code>GET /containers/(id or name)/export</code></p> <p>Export the contents of container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/export HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/octet-stream\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-stats-based-on-resource-usage\">Get container stats based on resource usage</h3> <p><code>GET /containers/(id or name)/stats</code></p> <p>This endpoint returns a live stream of a container’s resource usage statistics.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/redis1/stats HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Type: application/json\n\n  {\n     \"read\" : \"2015-01-08T22:57:31.547920715Z\",\n     \"pids_stats\": {\n        \"current\": 3\n     },\n     \"networks\": {\n             \"eth0\": {\n                 \"rx_bytes\": 5338,\n                 \"rx_dropped\": 0,\n                 \"rx_errors\": 0,\n                 \"rx_packets\": 36,\n                 \"tx_bytes\": 648,\n                 \"tx_dropped\": 0,\n                 \"tx_errors\": 0,\n                 \"tx_packets\": 8\n             },\n             \"eth5\": {\n                 \"rx_bytes\": 4641,\n                 \"rx_dropped\": 0,\n                 \"rx_errors\": 0,\n                 \"rx_packets\": 26,\n                 \"tx_bytes\": 690,\n                 \"tx_dropped\": 0,\n                 \"tx_errors\": 0,\n                 \"tx_packets\": 9\n             }\n     },\n     \"memory_stats\" : {\n        \"stats\" : {\n           \"total_pgmajfault\" : 0,\n           \"cache\" : 0,\n           \"mapped_file\" : 0,\n           \"total_inactive_file\" : 0,\n           \"pgpgout\" : 414,\n           \"rss\" : 6537216,\n           \"total_mapped_file\" : 0,\n           \"writeback\" : 0,\n           \"unevictable\" : 0,\n           \"pgpgin\" : 477,\n           \"total_unevictable\" : 0,\n           \"pgmajfault\" : 0,\n           \"total_rss\" : 6537216,\n           \"total_rss_huge\" : 6291456,\n           \"total_writeback\" : 0,\n           \"total_inactive_anon\" : 0,\n           \"rss_huge\" : 6291456,\n           \"hierarchical_memory_limit\" : 67108864,\n           \"total_pgfault\" : 964,\n           \"total_active_file\" : 0,\n           \"active_anon\" : 6537216,\n           \"total_active_anon\" : 6537216,\n           \"total_pgpgout\" : 414,\n           \"total_cache\" : 0,\n           \"inactive_anon\" : 0,\n           \"active_file\" : 0,\n           \"pgfault\" : 964,\n           \"inactive_file\" : 0,\n           \"total_pgpgin\" : 477\n        },\n        \"max_usage\" : 6651904,\n        \"usage\" : 6537216,\n        \"failcnt\" : 0,\n        \"limit\" : 67108864\n     },\n     \"blkio_stats\" : {},\n     \"cpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24472255,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100215355,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 739306590000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     },\n     \"precpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24350896,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100093996,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 9492140000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     }\n  }\n</pre> <p>The precpu_stats is the cpu statistic of last read, which is used for calculating the cpu usage percent. It is not the exact copy of the “cpu_stats” field.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, pull stats once then disconnect. Default <code>true</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"resize-a-container-tty\">Resize a container TTY</h3> <p><code>POST /containers/(id or name)/resize</code></p> <p>Resize the TTY for container with <code>id</code>. The unit is number of characters. You must restart the container for the resize to take effect.</p> <p><strong>Example request</strong>:</p> <pre>  POST /containers/4fa6e0f0c678/resize?h=40&amp;w=80 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Length: 0\n  Content-Type: text/plain; charset=utf-8\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of <code>tty</code> session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – No such container</li> <li>\n<strong>500</strong> – Cannot resize container</li> </ul> <h3 id=\"start-a-container\">Start a container</h3> <p><code>POST /containers/(id or name)/start</code></p> <p>Start the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: For backwards compatibility, this endpoint accepts a <code>HostConfig</code> as JSON-encoded request body. See <a href=\"#create-a-container\">create a container</a> for details.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/start HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>detachKeys</strong> – Override the key sequence for detaching a container. Format is a single character <code>[a-Z]</code> or <code>ctrl-&lt;value&gt;</code> where <code>&lt;value&gt;</code> is one of: <code>a-z</code>, <code>@</code>, <code>^</code>, <code>[</code>, <code>,</code> or <code>_</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already started</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"stop-a-container\">Stop a container</h3> <p><code>POST /containers/(id or name)/stop</code></p> <p>Stop the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/stop?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already stopped</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"restart-a-container\">Restart a container</h3> <p><code>POST /containers/(id or name)/restart</code></p> <p>Restart the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/restart?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"kill-a-container\">Kill a container</h3> <p><code>POST /containers/(id or name)/kill</code></p> <p>Kill the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/kill HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters</p> <ul> <li>\n<strong>signal</strong> - Signal to send to the container: integer or string like <code>SIGINT</code>. When not set, <code>SIGKILL</code> is assumed and the call waits for the container to exit.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"update-a-container\">Update a container</h3> <p><code>POST /containers/(id or name)/update</code></p> <p>Update configuration of one or more containers.</p> <p><strong>Example request</strong>:</p> <pre>   POST /containers/e90e34656806/update HTTP/1.1\n   Content-Type: application/json\n\n   {\n     \"BlkioWeight\": 300,\n     \"CpuShares\": 512,\n     \"CpuPeriod\": 100000,\n     \"CpuQuota\": 50000,\n     \"CpusetCpus\": \"0,1\",\n     \"CpusetMems\": \"0\",\n     \"Memory\": 314572800,\n     \"MemorySwap\": 514288000,\n     \"MemoryReservation\": 209715200,\n     \"KernelMemory\": 52428800,\n     \"RestartPolicy\": {\n       \"MaximumRetryCount\": 4,\n       \"Name\": \"on-failure\"\n     },\n   }\n</pre> <p><strong>Example response</strong>:</p> <pre>   HTTP/1.1 200 OK\n   Content-Type: application/json\n\n   {\n       \"Warnings\": []\n   }\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"rename-a-container\">Rename a container</h3> <p><code>POST /containers/(id or name)/rename</code></p> <p>Rename the container <code>id</code> to a <code>new_name</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/rename?name=new_name HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – new name for the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - conflict name already assigned</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"pause-a-container\">Pause a container</h3> <p><code>POST /containers/(id or name)/pause</code></p> <p>Pause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/pause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"unpause-a-container\">Unpause a container</h3> <p><code>POST /containers/(id or name)/unpause</code></p> <p>Unpause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/unpause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"attach-to-a-container\">Attach to a container</h3> <p><code>POST /containers/(id or name)/attach</code></p> <p>Attach to the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/attach?logs=1&amp;stream=0&amp;stdout=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 101 UPGRADED\nContent-Type: application/vnd.docker.raw-stream\nConnection: Upgrade\nUpgrade: tcp\n\n{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>detachKeys</strong> – Override the key sequence for detaching a container. Format is a single character <code>[a-Z]</code> or <code>ctrl-&lt;value&gt;</code> where <code>&lt;value&gt;</code> is one of: <code>a-z</code>, <code>@</code>, <code>^</code>, <code>[</code>, <code>,</code> or <code>_</code>.</li> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<p><strong>500</strong> – server error</p> <p><strong>Stream details</strong>:</p> <p>When using the TTY setting is enabled in <a href=\"#create-a-container\"><code>POST /containers/create</code> </a>, the stream is the raw data from the process PTY and client’s <code>stdin</code>. When the TTY is disabled, then the stream is multiplexed to separate <code>stdout</code> and <code>stderr</code>.</p> <p>The format is a <strong>Header</strong> and a <strong>Payload</strong> (frame).</p> <p><strong>HEADER</strong></p> <p>The header contains the information which the stream writes (<code>stdout</code> or <code>stderr</code>). It also contains the size of the associated frame encoded in the last four bytes (<code>uint32</code>).</p> <p>It is encoded on the first eight bytes like this:</p> <pre>header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}\n</pre> <p><code>STREAM_TYPE</code> can be:</p>\n</li> <li><p>0: <code>stdin</code> (is written on <code>stdout</code>)</p></li> <li><p>1: <code>stdout</code></p></li> <li>\n<p>2: <code>stderr</code></p> <p><code>SIZE1, SIZE2, SIZE3, SIZE4</code> are the four bytes of the <code>uint32</code> size encoded as big endian.</p> <p><strong>PAYLOAD</strong></p> <p>The payload is the raw stream.</p> <p><strong>IMPLEMENTATION</strong></p> <p>The simplest way to implement the Attach protocol is the following:</p> <ol> <li>Read eight bytes.</li> <li>Choose <code>stdout</code> or <code>stderr</code> depending on the first byte.</li> <li>Extract the frame size from the last four bytes.</li> <li>Read the extracted size and output it on the correct output.</li> <li>Goto 1.</li> </ol>\n</li> </ul> <h3 id=\"attach-to-a-container-websocket\">Attach to a container (websocket)</h3> <p><code>GET /containers/(id or name)/attach/ws</code></p> <p>Attach to the container <code>id</code> via websocket</p> <p>Implements websocket protocol handshake according to <a href=\"http://tools.ietf.org/html/rfc6455\">RFC 6455</a></p> <p><strong>Example request</strong></p> <pre>GET /containers/e90e34656806/attach/ws?logs=0&amp;stream=1&amp;stdin=1&amp;stdout=1&amp;stderr=1 HTTP/1.1\n</pre> <p><strong>Example response</strong></p> <pre>{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>detachKeys</strong> – Override the key sequence for detaching a container. Format is a single character <code>[a-Z]</code> or <code>ctrl-&lt;value&gt;</code> where <code>&lt;value&gt;</code> is one of: <code>a-z</code>, <code>@</code>, <code>^</code>, <code>[</code>, <code>,</code> or <code>_</code>.</li> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"wait-a-container\">Wait a container</h3> <p><code>POST /containers/(id or name)/wait</code></p> <p>Block until container <code>id</code> stops, then returns the exit code</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/wait HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"StatusCode\": 0}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-a-container\">Remove a container</h3> <p><code>DELETE /containers/(id or name)</code></p> <p>Remove the container <code>id</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /containers/16253994b7c4?v=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>v</strong> – 1/True/true or 0/False/false, Remove the volumes associated to the container. Default <code>false</code>.</li> <li>\n<strong>force</strong> - 1/True/true or 0/False/false, Kill then remove the container. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"copy-files-or-folders-from-a-container\">Copy files or folders from a container</h3> <p><code>POST /containers/(id or name)/copy</code></p> <p>Copy files or folders of container <code>id</code></p> <p><strong>Deprecated</strong> in favor of the <code>archive</code> endpoint below.</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/4fa6e0f0c678/copy HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Resource\": \"test.txt\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"retrieving-information-about-files-and-folders-in-a-container\">Retrieving information about files and folders in a container</h3> <p><code>HEAD /containers/(id or name)/archive</code></p> <p>See the description of the <code>X-Docker-Container-Path-Stat</code> header in the following section.</p> <h3 id=\"get-an-archive-of-a-filesystem-resource-in-a-container\">Get an archive of a filesystem resource in a container</h3> <p><code>GET /containers/(id or name)/archive</code></p> <p>Get a tar archive of a resource in the filesystem of container <code>id</code>.</p> <p>Query Parameters:</p> <ul> <li>\n<p><strong>path</strong> - resource in the container’s filesystem to archive. Required.</p> <p>If not an absolute path, it is relative to the container’s root directory. The resource specified by <strong>path</strong> must exist. To assert that the resource is expected to be a directory, <strong>path</strong> should end in <code>/</code> or <code>/.</code> (assuming a path separator of <code>/</code>). If <strong>path</strong> ends in <code>/.</code> then this indicates that only the contents of the <strong>path</strong> directory should be copied. A symlink is always resolved to its target.</p> <p><strong>Note</strong>: It is not possible to copy certain system files such as resources under <code>/proc</code>, <code>/sys</code>, <code>/dev</code>, and mounts created by the user in the container.</p>\n</li> </ul> <p><strong>Example request</strong>:</p> <pre>    GET /containers/8cce319429b2/archive?path=/root HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/x-tar\n    X-Docker-Container-Path-Stat: eyJuYW1lIjoicm9vdCIsInNpemUiOjQwOTYsIm1vZGUiOjIxNDc0ODQwOTYsIm10aW1lIjoiMjAxNC0wMi0yN1QyMDo1MToyM1oiLCJsaW5rVGFyZ2V0IjoiIn0=\n\n    {{ TAR STREAM }}\n</pre> <p>On success, a response header <code>X-Docker-Container-Path-Stat</code> will be set to a base64-encoded JSON object containing some filesystem header information about the archived resource. The above example value would decode to the following JSON object (whitespace added for readability):</p> <pre>    {\n        \"name\": \"root\",\n        \"size\": 4096,\n        \"mode\": 2147484096,\n        \"mtime\": \"2014-02-27T20:51:23Z\",\n        \"linkTarget\": \"\"\n    }\n</pre> <p>A <code>HEAD</code> request can also be made to this endpoint if only this information is desired.</p> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - success, returns archive of copied resource</li> <li>\n<strong>400</strong> - client error, bad parameter, details in JSON response body, one of: <ul> <li>must specify path parameter (<strong>path</strong> cannot be empty)</li> <li>not a directory (<strong>path</strong> was asserted to be a directory but exists as a file)</li> </ul>\n</li> <li>\n<strong>404</strong> - client error, resource not found, one of: – no such container (container <code>id</code> does not exist) <ul> <li>no such file or directory (<strong>path</strong> does not exist)</li> </ul>\n</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"extract-an-archive-of-files-or-folders-to-a-directory-in-a-container\">Extract an archive of files or folders to a directory in a container</h3> <p><code>PUT /containers/(id or name)/archive</code></p> <p>Upload a tar archive to be extracted to a path in the filesystem of container <code>id</code>.</p> <p>Query Parameters:</p> <ul> <li>\n<p><strong>path</strong> - path to a directory in the container to extract the archive’s contents into. Required.</p> <p>If not an absolute path, it is relative to the container’s root directory. The <strong>path</strong> resource must exist.</p>\n</li> <li><p><strong>noOverwriteDirNonDir</strong> - If “1”, “true”, or “True” then it will be an error if unpacking the given content would cause an existing directory to be replaced with a non-directory and vice versa.</p></li> </ul> <p><strong>Example request</strong>:</p> <pre>PUT /containers/8cce319429b2/archive?path=/vol1 HTTP/1.1\nContent-Type: application/x-tar\n\n{{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – the content was extracted successfully</li> <li>\n<strong>400</strong> - client error, bad parameter, details in JSON response body, one of: <ul> <li>must specify path parameter (<strong>path</strong> cannot be empty)</li> <li>not a directory (<strong>path</strong> should be a directory but exists as a file)</li> <li>unable to overwrite existing directory with non-directory (if <strong>noOverwriteDirNonDir</strong>)</li> <li>unable to overwrite existing non-directory with directory (if <strong>noOverwriteDirNonDir</strong>)</li> </ul>\n</li> <li>\n<strong>403</strong> - client error, permission denied, the volume or container rootfs is marked as read-only.</li> <li>\n<strong>404</strong> - client error, resource not found, one of: – no such container (container <code>id</code> does not exist) <ul> <li>no such file or directory (<strong>path</strong> resource does not exist)</li> </ul>\n</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-2-images\">2.2 Images</h2> <h3 id=\"list-images\">List Images</h3> <p><code>GET /images/json</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/json?all=0 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.04\",\n       \"ubuntu:precise\",\n       \"ubuntu:latest\"\n     ],\n     \"Id\": \"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\n     \"Created\": 1365714795,\n     \"Size\": 131506275,\n     \"VirtualSize\": 131506275,\n     \"Labels\": {}\n  },\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.10\",\n       \"ubuntu:quantal\"\n     ],\n     \"ParentId\": \"27cf784147099545\",\n     \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n     \"Created\": 1364102658,\n     \"Size\": 24653,\n     \"VirtualSize\": 180116135,\n     \"Labels\": {\n        \"com.example.version\": \"v1\"\n     }\n  }\n]\n</pre> <p><strong>Example request, with digest information</strong>:</p> <pre>GET /images/json?digests=1 HTTP/1.1\n</pre> <p><strong>Example response, with digest information</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Created\": 1420064636,\n    \"Id\": \"4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125\",\n    \"ParentId\": \"ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2\",\n    \"RepoDigests\": [\n      \"localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n    ],\n    \"RepoTags\": [\n      \"localhost:5000/test/busybox:latest\",\n      \"playdate:latest\"\n    ],\n    \"Size\": 0,\n    \"VirtualSize\": 2429728,\n    \"Labels\": {}\n  }\n]\n</pre> <p>The response shows a single image <code>Id</code> associated with two repositories (<code>RepoTags</code>): <code>localhost:5000/test/busybox</code>: and <code>playdate</code>. A caller can use either of the <code>RepoTags</code> values <code>localhost:5000/test/busybox:latest</code> or <code>playdate:latest</code> to reference the image.</p> <p>You can also use <code>RepoDigests</code> values to reference an image. In this response, the array has only one reference and that is to the <code>localhost:5000/test/busybox</code> repository; the <code>playdate</code> repository has no digest. You can reference this digest using the value: <code>localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d...</code></p> <p>See the <code>docker run</code> and <code>docker build</code> commands for examples of digest and tag references on the command line.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>filters</strong> – a JSON encoded value of the filters (a map[string][]string) to process on the images list. Available filters: <ul> <li><code>dangling=true</code></li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of an image label</li> </ul>\n</li> <li>\n<strong>filter</strong> - only return images with the specified name</li> </ul> <h3 id=\"build-image-from-a-dockerfile\">Build image from a Dockerfile</h3> <p><code>POST /build</code></p> <p>Build an image from a Dockerfile</p> <p><strong>Example request</strong>:</p> <pre>POST /build HTTP/1.1\n\n{{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"stream\": \"Step 1...\"}\n{\"stream\": \"...\"}\n{\"error\": \"Error...\", \"errorDetail\": {\"code\": 123, \"message\": \"Error...\"}}\n</pre> <p>The input stream must be a <code>tar</code> archive compressed with one of the following algorithms: <code>identity</code> (no compression), <code>gzip</code>, <code>bzip2</code>, <code>xz</code>.</p> <p>The archive must include a build instructions file, typically called <code>Dockerfile</code> at the archive’s root. The <code>dockerfile</code> parameter may be used to specify a different build instructions file. To do this, its value must be the path to the alternate build instructions file to use.</p> <p>The archive may include any number of other files, which are accessible in the build context (See the <a href=\"../../builder/index#dockerbuilder\"><em>ADD build command</em></a>).</p> <p>The build is canceled if the client drops the connection by quitting or being killed.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>dockerfile</strong> - Path within the build context to the Dockerfile. This is ignored if <code>remote</code> is specified and points to an individual filename.</li> <li>\n<strong>t</strong> – A name and optional tag to apply to the image in the <code>name:tag</code> format. If you omit the <code>tag</code> the default <code>latest</code> value is assumed. You can provide one or more <code>t</code> parameters.</li> <li>\n<strong>remote</strong> – A Git repository URI or HTTP/HTTPS URI build source. If the URI specifies a filename, the file’s contents are placed into a file called <code>Dockerfile</code>.</li> <li>\n<strong>q</strong> – Suppress verbose build output.</li> <li>\n<strong>nocache</strong> – Do not use the cache when building the image.</li> <li>\n<strong>pull</strong> - Attempt to pull the image even if an older image exists locally.</li> <li>\n<strong>rm</strong> - Remove intermediate containers after a successful build (default behavior).</li> <li>\n<strong>forcerm</strong> - Always remove intermediate containers (includes <code>rm</code>).</li> <li>\n<strong>memory</strong> - Set memory limit for build.</li> <li>\n<strong>memswap</strong> - Total memory (memory + swap), <code>-1</code> to enable unlimited swap.</li> <li>\n<strong>cpushares</strong> - CPU shares (relative weight).</li> <li>\n<strong>cpusetcpus</strong> - CPUs in which to allow execution (e.g., <code>0-3</code>, <code>0,1</code>).</li> <li>\n<strong>cpuperiod</strong> - The length of a CPU period in microseconds.</li> <li>\n<strong>cpuquota</strong> - Microseconds of CPU time that the container can get in a CPU period.</li> <li>\n<strong>buildargs</strong> – JSON map of string pairs for build-time variables. Users pass these values at build-time. Docker uses the <code>buildargs</code> as the environment context for command(s) run via the Dockerfile’s <code>RUN</code> instruction or for variable expansion in other Dockerfile instructions. This is not meant for passing secret values. <a href=\"../../builder/index#arg\">Read more about the buildargs instruction</a>\n</li> <li>\n<strong>shmsize</strong> - Size of <code>/dev/shm</code> in bytes. The size must be greater than 0. If omitted the system uses 64MB.</li> <li>\n<p><strong>labels</strong> – JSON map of string pairs for labels to set on the image.</p> <p>Request Headers:</p>\n</li> <li><p><strong>Content-type</strong> – Set to <code>\"application/tar\"</code>.</p></li> <li>\n<p><strong>X-Registry-Config</strong> – A base64-url-safe-encoded Registry Auth Config JSON object with the following structure:</p> <pre>    {\n        \"docker.example.com\": {\n            \"username\": \"janedoe\",\n            \"password\": \"hunter2\"\n        },\n        \"https://index.docker.io/v1/\": {\n            \"username\": \"mobydock\",\n            \"password\": \"conta1n3rize14\"\n        }\n    }\n</pre> <p>This object maps the hostname of a registry to an object containing the “username” and “password” for that registry. Multiple registries may be specified as the build may be based on an image requiring authentication to pull from any arbitrary registry. Only the registry domain name (and port if not the default “443”) are required. However (for legacy reasons) the “official” Docker, Inc. hosted registry must be specified with both a “https://” prefix and a “/v1/” suffix even though Docker will prefer to use the v2 registry API.</p>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-an-image\">Create an image</h3> <p><code>POST /images/create</code></p> <p>Create an image either by pulling it from the registry or by importing it</p> <p><strong>Example request</strong>:</p> <pre>POST /images/create?fromImage=ubuntu HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pulling...\"}\n{\"status\": \"Pulling\", \"progress\": \"1 B/ 100 B\", \"progressDetail\": {\"current\": 1, \"total\": 100}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>When using this endpoint to pull an image from the registry, the <code>X-Registry-Auth</code> header can be used to include a base64-encoded AuthConfig object.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>fromImage</strong> – Name of the image to pull. The name may include a tag or digest. This parameter may only be used when pulling an image. The pull is cancelled if the HTTP connection is closed.</li> <li>\n<strong>fromSrc</strong> – Source to import. The value may be a URL from which the image can be retrieved or <code>-</code> to read the image from the request body. This parameter may only be used when importing an image.</li> <li>\n<strong>repo</strong> – Repository name given to an image when it is imported. The repo may include a tag. This parameter may only be used when importing an image.</li> <li>\n<p><strong>tag</strong> – Tag or digest.</p> <p>Request Headers:</p>\n</li> <li>\n<p><strong>X-Registry-Auth</strong> – base64-encoded AuthConfig object, containing either login information, or a token</p> <ul> <li>\n<p>Credential based login:</p> <pre>{\n    \"username\": \"jdoe\",\n    \"password\": \"secret\",\n    \"email\": \"jdoe@acme.com\",\n}\n</pre>\n</li> <li>\n<p>Token based login:</p> <pre>{\n    \"registrytoken\": \"9cbaf023786cd7...\"\n}\n</pre>\n</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-an-image\">Inspect an image</h3> <p><code>GET /images/(name)/json</code></p> <p>Return low-level information on the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/example/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Id\" : \"sha256:85f05633ddc1c50679be2b16a0479ab6f7637f8884e0cfe0f4d20e1ebb3d6e7c\",\n   \"Container\" : \"cb91e48a60d01f1e27028b4fc6819f4f290b3cf12496c8176ec714d0d390984a\",\n   \"Comment\" : \"\",\n   \"Os\" : \"linux\",\n   \"Architecture\" : \"amd64\",\n   \"Parent\" : \"sha256:91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n   \"ContainerConfig\" : {\n      \"Tty\" : false,\n      \"Hostname\" : \"e611e15f9c9d\",\n      \"Volumes\" : null,\n      \"Domainname\" : \"\",\n      \"AttachStdout\" : false,\n      \"PublishService\" : \"\",\n      \"AttachStdin\" : false,\n      \"OpenStdin\" : false,\n      \"StdinOnce\" : false,\n      \"NetworkDisabled\" : false,\n      \"OnBuild\" : [],\n      \"Image\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n      \"User\" : \"\",\n      \"WorkingDir\" : \"\",\n      \"Entrypoint\" : null,\n      \"MacAddress\" : \"\",\n      \"AttachStderr\" : false,\n      \"Labels\" : {\n         \"com.example.license\" : \"GPL\",\n         \"com.example.version\" : \"1.0\",\n         \"com.example.vendor\" : \"Acme\"\n      },\n      \"Env\" : [\n         \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n      ],\n      \"ExposedPorts\" : null,\n      \"Cmd\" : [\n         \"/bin/sh\",\n         \"-c\",\n         \"#(nop) LABEL com.example.vendor=Acme com.example.license=GPL com.example.version=1.0\"\n      ]\n   },\n   \"DockerVersion\" : \"1.9.0-dev\",\n   \"VirtualSize\" : 188359297,\n   \"Size\" : 0,\n   \"Author\" : \"\",\n   \"Created\" : \"2015-09-10T08:30:53.26995814Z\",\n   \"GraphDriver\" : {\n      \"Name\" : \"aufs\",\n      \"Data\" : null\n   },\n   \"RepoDigests\" : [\n      \"localhost:5000/test/busybox/example@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n   ],\n   \"RepoTags\" : [\n      \"example:1.0\",\n      \"example:latest\",\n      \"example:stable\"\n   ],\n   \"Config\" : {\n      \"Image\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n      \"NetworkDisabled\" : false,\n      \"OnBuild\" : [],\n      \"StdinOnce\" : false,\n      \"PublishService\" : \"\",\n      \"AttachStdin\" : false,\n      \"OpenStdin\" : false,\n      \"Domainname\" : \"\",\n      \"AttachStdout\" : false,\n      \"Tty\" : false,\n      \"Hostname\" : \"e611e15f9c9d\",\n      \"Volumes\" : null,\n      \"Cmd\" : [\n         \"/bin/bash\"\n      ],\n      \"ExposedPorts\" : null,\n      \"Env\" : [\n         \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n      ],\n      \"Labels\" : {\n         \"com.example.vendor\" : \"Acme\",\n         \"com.example.version\" : \"1.0\",\n         \"com.example.license\" : \"GPL\"\n      },\n      \"Entrypoint\" : null,\n      \"MacAddress\" : \"\",\n      \"AttachStderr\" : false,\n      \"WorkingDir\" : \"\",\n      \"User\" : \"\"\n   },\n   \"RootFS\": {\n       \"Type\": \"layers\",\n       \"Layers\": [\n           \"sha256:1834950e52ce4d5a88a1bbd131c537f4d0e56d10ff0dd69e66be3b7dfa9df7e6\",\n           \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\"\n       ]\n   }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-the-history-of-an-image\">Get the history of an image</h3> <p><code>GET /images/(name)/history</code></p> <p>Return the history of the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/ubuntu/history HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n    {\n        \"Id\": \"3db9c44f45209632d6050b35958829c3a2aa256d81b9a7be45b362ff85c54710\",\n        \"Created\": 1398108230,\n        \"CreatedBy\": \"/bin/sh -c #(nop) ADD file:eb15dbd63394e063b805a3c32ca7bf0266ef64676d5a6fab4801f2e81e2a5148 in /\",\n        \"Tags\": [\n            \"ubuntu:lucid\",\n            \"ubuntu:10.04\"\n        ],\n        \"Size\": 182964289,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"6cfa4d1f33fb861d4d114f43b25abd0ac737509268065cdfd69d544a59c85ab8\",\n        \"Created\": 1398108222,\n        \"CreatedBy\": \"/bin/sh -c #(nop) MAINTAINER Tianon Gravi &lt;admwiggin@gmail.com&gt; - mkimage-debootstrap.sh -i iproute,iputils-ping,ubuntu-minimal -t lucid.tar.xz lucid http://archive.ubuntu.com/ubuntu/\",\n        \"Tags\": null,\n        \"Size\": 0,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\n        \"Created\": 1371157430,\n        \"CreatedBy\": \"\",\n        \"Tags\": [\n            \"scratch12:latest\",\n            \"scratch:latest\"\n        ],\n        \"Size\": 0,\n        \"Comment\": \"Imported from -\"\n    }\n]\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"push-an-image-on-the-registry\">Push an image on the registry</h3> <p><code>POST /images/(name)/push</code></p> <p>Push the image <code>name</code> on the registry</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/push HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pushing...\"}\n{\"status\": \"Pushing\", \"progress\": \"1/? (n/a)\", \"progressDetail\": {\"current\": 1}}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>If you wish to push an image on to a private registry, that image must already have a tag into a repository which references that registry <code>hostname</code> and <code>port</code>. This repository name should then be used in the URL. This duplicates the command line’s flow.</p> <p>The push is cancelled if the HTTP connection is closed.</p> <p><strong>Example request</strong>:</p> <pre>POST /images/registry.acme.com:5000/test/push HTTP/1.1\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>tag</strong> – The tag to associate with the image on the registry. This is optional.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<p><strong>X-Registry-Auth</strong> – base64-encoded AuthConfig object, containing either login information, or a token</p> <ul> <li>\n<p>Credential based login:</p> <pre>{\n    \"username\": \"jdoe\",\n    \"password\": \"secret\",\n    \"email\": \"jdoe@acme.com\",\n}\n</pre>\n</li> <li>\n<p>Identity token based login:</p> <pre>{\n    \"identitytoken\": \"9cbaf023786cd7...\"\n}\n</pre>\n</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"tag-an-image-into-a-repository\">Tag an image into a repository</h3> <p><code>POST /images/(name)/tag</code></p> <p>Tag the image <code>name</code> into a repository</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/tag?repo=myrepo&amp;force=0&amp;tag=v42 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>repo</strong> – The repository to tag in</li> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>tag</strong> - The new tag name</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-an-image\">Remove an image</h3> <p><code>DELETE /images/(name)</code></p> <p>Remove the image <code>name</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /images/test HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-type: application/json\n\n[\n {\"Untagged\": \"3e2f21a89f\"},\n {\"Deleted\": \"3e2f21a89f\"},\n {\"Deleted\": \"53b4f83ac9\"}\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>noprune</strong> – 1/True/true or 0/False/false, default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"search-images\">Search images</h3> <p><code>GET /images/search</code></p> <p>Search for an image on <a href=\"https://hub.docker.com\">Docker Hub</a>.</p> <blockquote> <p><strong>Note</strong>: The response keys have changed from API v1.6 to reflect the JSON sent by the registry server to the docker daemon’s request.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>GET /images/search?term=sshd HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"wma55/u1210sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"jdswinbank/sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"vgauthier/sshd\",\n            \"star_count\": 0\n        }\n...\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>term</strong> – term to search</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-3-misc\">2.3 Misc</h2> <h3 id=\"check-auth-configuration\">Check auth configuration</h3> <p><code>POST /auth</code></p> <p>Validate credentials for a registry and get identity token, if available, for accessing the registry without password.</p> <p><strong>Example request</strong>:</p> <pre>POST /auth HTTP/1.1\nContent-Type: application/json\n\n{\n     \"username\": \"hannibal\",\n     \"password\": \"xxxx\",\n     \"serveraddress\": \"https://index.docker.io/v1/\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n\n{\n     \"Status\": \"Login Succeeded\",\n     \"IdentityToken\": \"9cbaf023786cd7...\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"display-system-wide-information\">Display system-wide information</h3> <p><code>GET /info</code></p> <p>Display system-wide information</p> <p><strong>Example request</strong>:</p> <pre>GET /info HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"Architecture\": \"x86_64\",\n    \"ClusterStore\": \"etcd://localhost:2379\",\n    \"CgroupDriver\": \"cgroupfs\",\n    \"Containers\": 11,\n    \"ContainersRunning\": 7,\n    \"ContainersStopped\": 3,\n    \"ContainersPaused\": 1,\n    \"CpuCfsPeriod\": true,\n    \"CpuCfsQuota\": true,\n    \"Debug\": false,\n    \"DockerRootDir\": \"/var/lib/docker\",\n    \"Driver\": \"btrfs\",\n    \"DriverStatus\": [[\"\"]],\n    \"ExecutionDriver\": \"native-0.1\",\n    \"ExperimentalBuild\": false,\n    \"HttpProxy\": \"http://test:test@localhost:8080\",\n    \"HttpsProxy\": \"https://test:test@localhost:8080\",\n    \"ID\": \"7TRN:IPZB:QYBB:VPBQ:UMPP:KARE:6ZNR:XE6T:7EWV:PKF4:ZOJD:TPYS\",\n    \"IPv4Forwarding\": true,\n    \"Images\": 16,\n    \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n    \"InitPath\": \"/usr/bin/docker\",\n    \"InitSha1\": \"\",\n    \"KernelMemory\": true,\n    \"KernelVersion\": \"3.12.0-1-amd64\",\n    \"Labels\": [\n        \"storage=ssd\"\n    ],\n    \"MemTotal\": 2099236864,\n    \"MemoryLimit\": true,\n    \"NCPU\": 1,\n    \"NEventsListener\": 0,\n    \"NFd\": 11,\n    \"NGoroutines\": 21,\n    \"Name\": \"prod-server-42\",\n    \"NoProxy\": \"9.81.1.160\",\n    \"OomKillDisable\": true,\n    \"OSType\": \"linux\",\n    \"OperatingSystem\": \"Boot2Docker\",\n    \"Plugins\": {\n        \"Volume\": [\n            \"local\"\n        ],\n        \"Network\": [\n            \"null\",\n            \"host\",\n            \"bridge\"\n        ]\n    },\n    \"RegistryConfig\": {\n        \"IndexConfigs\": {\n            \"docker.io\": {\n                \"Mirrors\": null,\n                \"Name\": \"docker.io\",\n                \"Official\": true,\n                \"Secure\": true\n            }\n        },\n        \"InsecureRegistryCIDRs\": [\n            \"127.0.0.0/8\"\n        ]\n    },\n    \"ServerVersion\": \"1.9.0\",\n    \"SwapLimit\": false,\n    \"SystemStatus\": [[\"State\", \"Healthy\"]],\n    \"SystemTime\": \"2015-03-10T11:11:23.730591467-07:00\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"show-the-docker-version-information\">Show the docker version information</h3> <p><code>GET /version</code></p> <p>Show the docker version information</p> <p><strong>Example request</strong>:</p> <pre>GET /version HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n     \"Version\": \"1.10.0-dev\",\n     \"Os\": \"linux\",\n     \"KernelVersion\": \"3.19.0-23-generic\",\n     \"GoVersion\": \"go1.4.2\",\n     \"GitCommit\": \"e75da4b\",\n     \"Arch\": \"amd64\",\n     \"ApiVersion\": \"1.23\",\n     \"BuildTime\": \"2015-12-01T07:09:13.444803460+00:00\",\n     \"Experimental\": true\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"ping-the-docker-server\">Ping the docker server</h3> <p><code>GET /_ping</code></p> <p>Ping the docker server</p> <p><strong>Example request</strong>:</p> <pre>GET /_ping HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: text/plain\n\nOK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-new-image-from-a-container-s-changes\">Create a new image from a container’s changes</h3> <p><code>POST /commit</code></p> <p>Create a new image from a container’s changes</p> <p><strong>Example request</strong>:</p> <pre>POST /commit?container=44c004db4b17&amp;comment=message&amp;repo=myrepo HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Hostname\": \"\",\n     \"Domainname\": \"\",\n     \"User\": \"\",\n     \"AttachStdin\": false,\n     \"AttachStdout\": true,\n     \"AttachStderr\": true,\n     \"Tty\": false,\n     \"OpenStdin\": false,\n     \"StdinOnce\": false,\n     \"Env\": null,\n     \"Cmd\": [\n             \"date\"\n     ],\n     \"Mounts\": [\n       {\n         \"Source\": \"/data\",\n         \"Destination\": \"/data\",\n         \"Mode\": \"ro,Z\",\n         \"RW\": false\n       }\n     ],\n     \"Labels\": {\n             \"key1\": \"value1\",\n             \"key2\": \"value2\"\n      },\n     \"WorkingDir\": \"\",\n     \"NetworkDisabled\": false,\n     \"ExposedPorts\": {\n             \"22/tcp\": {}\n     }\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\"Id\": \"596069db4bf5\"}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>config</strong> - the container’s configuration</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>container</strong> – source container</li> <li>\n<strong>repo</strong> – repository</li> <li>\n<strong>tag</strong> – tag</li> <li>\n<strong>comment</strong> – commit message</li> <li>\n<strong>author</strong> – author (e.g., “John Hannibal Smith &lt;<a href=\"mailto:hannibal%40a-team.com\">hannibal@a-team.com</a>&gt;“)</li> <li>\n<strong>pause</strong> – 1/True/true or 0/False/false, whether to pause the container before committing</li> <li>\n<strong>changes</strong> – Dockerfile instructions to apply while committing</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"monitor-docker-s-events\">Monitor Docker’s events</h3> <p><code>GET /events</code></p> <p>Get container events from docker, either in real time via streaming, or via polling (using since).</p> <p>Docker containers report the following events:</p> <pre>attach, commit, copy, create, destroy, die, exec_create, exec_start, export, kill, oom, pause, rename, resize, restart, start, stop, top, unpause, update\n</pre> <p>Docker images report the following events:</p> <pre>delete, import, pull, push, tag, untag\n</pre> <p>Docker volumes report the following events:</p> <pre>create, mount, unmount, destroy\n</pre> <p>Docker networks report the following events:</p> <pre>create, connect, disconnect, destroy\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /events?since=1374067924\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\nServer: Docker/1.11.0 (linux)\nDate: Fri, 29 Apr 2016 15:18:06 GMT\nTransfer-Encoding: chunked\n\n{\n  \"status\": \"pull\",\n  \"id\": \"alpine:latest\",\n  \"Type\": \"image\",\n  \"Action\": \"pull\",\n  \"Actor\": {\n    \"ID\": \"alpine:latest\",\n    \"Attributes\": {\n      \"name\": \"alpine\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101301854122\n}\n{\n  \"status\": \"create\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"create\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101381709551\n}\n{\n  \"status\": \"attach\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"attach\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101383858412\n}\n{\n  \"Type\": \"network\",\n  \"Action\": \"connect\",\n  \"Actor\": {\n    \"ID\": \"7dc8ac97d5d29ef6c31b6052f3938c1e8f2749abbd17d1bd1febf2608db1b474\",\n    \"Attributes\": {\n      \"container\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n      \"name\": \"bridge\",\n      \"type\": \"bridge\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101394865557\n}\n{\n  \"status\": \"start\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"start\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101607533796\n}\n{\n  \"status\": \"resize\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"resize\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"height\": \"46\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\",\n      \"width\": \"204\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101610269268\n}\n{\n  \"status\": \"die\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"die\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"exitCode\": \"0\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105079144137\n}\n{\n  \"Type\": \"network\",\n  \"Action\": \"disconnect\",\n  \"Actor\": {\n    \"ID\": \"7dc8ac97d5d29ef6c31b6052f3938c1e8f2749abbd17d1bd1febf2608db1b474\",\n    \"Attributes\": {\n      \"container\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n      \"name\": \"bridge\",\n      \"type\": \"bridge\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105230860245\n}\n{\n  \"status\": \"destroy\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"destroy\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105338056026\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>since</strong> – Timestamp used for polling</li> <li>\n<strong>until</strong> – Timestamp used for polling</li> <li>\n<strong>filters</strong> – A json encoded value of the filters (a map[string][]string) to process on the event list. Available filters: <ul> <li>\n<code>container=&lt;string&gt;</code>; -- container to filter</li> <li>\n<code>event=&lt;string&gt;</code>; -- event to filter</li> <li>\n<code>image=&lt;string&gt;</code>; -- image to filter</li> <li>\n<code>label=&lt;string&gt;</code>; -- image and container label to filter</li> <li>\n<code>type=&lt;string&gt;</code>; -- either <code>container</code> or <code>image</code> or <code>volume</code> or <code>network</code>\n</li> <li>\n<code>volume=&lt;string&gt;</code>; -- volume to filter</li> <li>\n<code>network=&lt;string&gt;</code>; -- network to filter</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images-in-a-repository\">Get a tarball containing all images in a repository</h3> <p><code>GET /images/(name)/get</code></p> <p>Get a tarball containing all images and metadata for the repository specified by <code>name</code>.</p> <p>If <code>name</code> is a specific name and tag (e.g. ubuntu:latest), then only that image (and its parents) are returned. If <code>name</code> is an image ID, similarly only that image (and its parents) are returned, but with the exclusion of the ‘repositories’ file in the tarball, as there were no image names referenced.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/ubuntu/get\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images\">Get a tarball containing all images.</h3> <p><code>GET /images/get</code></p> <p>Get a tarball containing all images and metadata for one or more repositories.</p> <p>For each value of the <code>names</code> parameter: if it is a specific name and tag (e.g. <code>ubuntu:latest</code>), then only that image (and its parents) are returned; if it is an image ID, similarly only that image (and its parents) are returned and there would be no names referenced in the ‘repositories’ file for this image ID.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/get?names=myname%2Fmyapp%3Alatest&amp;names=busybox\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"load-a-tarball-with-a-set-of-images-and-tags-into-docker\">Load a tarball with a set of images and tags into docker</h3> <p><code>POST /images/load</code></p> <p>Load a set of images and tags into a Docker repository. See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>POST /images/load\n\nTarball in body\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"image-tarball-format\">Image tarball format</h3> <p>An image tarball contains one directory per image layer (named using its long ID), each containing these files:</p> <ul> <li>\n<code>VERSION</code>: currently <code>1.0</code> - the file format version</li> <li>\n<code>json</code>: detailed layer information, similar to <code>docker inspect layer_id</code>\n</li> <li>\n<code>layer.tar</code>: A tarfile containing the filesystem changes in this layer</li> </ul> <p>The <code>layer.tar</code> file contains <code>aufs</code> style <code>.wh..wh.aufs</code> files and directories for storing attribute changes and deletions.</p> <p>If the tarball defines a repository, the tarball should also include a <code>repositories</code> file at the root that contains a list of repository and tag names mapped to layer IDs.</p> <pre>{\"hello-world\":\n    {\"latest\": \"565a9d68a73f6706862bfe8409a7f659776d4d60a8d096eb4a3cbce6999cc2a1\"}\n}\n</pre> <h3 id=\"exec-create\">Exec Create</h3> <p><code>POST /containers/(id or name)/exec</code></p> <p>Sets up an exec instance in a running container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/exec HTTP/1.1\nContent-Type: application/json\n\n  {\n   \"AttachStdin\": false,\n   \"AttachStdout\": true,\n   \"AttachStderr\": true,\n   \"DetachKeys\": \"ctrl-p,ctrl-q\",\n   \"Tty\": false,\n   \"Cmd\": [\n                 \"date\"\n         ]\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n     \"Id\": \"f90e34656806\",\n     \"Warnings\":[]\n}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code> of the <code>exec</code> command.</li> <li>\n<strong>DetachKeys</strong> – Override the key sequence for detaching a container. Format is a single character <code>[a-Z]</code> or <code>ctrl-&lt;value&gt;</code> where <code>&lt;value&gt;</code> is one of: <code>a-z</code>, <code>@</code>, <code>^</code>, <code>[</code>, <code>,</code> or <code>_</code>.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - container is paused</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"exec-start\">Exec Start</h3> <p><code>POST /exec/(id)/start</code></p> <p>Starts a previously set up <code>exec</code> instance <code>id</code>. If <code>detach</code> is true, this API returns after starting the <code>exec</code> command. Otherwise, this API sets up an interactive session with the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/start HTTP/1.1\nContent-Type: application/json\n\n{\n \"Detach\": false,\n \"Tty\": false\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/vnd.docker.raw-stream\n\n{{ STREAM }}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Detach</strong> - Detach from the <code>exec</code> command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<p><strong>409</strong> - container is paused</p> <p><strong>Stream details</strong>: Similar to the stream behavior of <code>POST /containers/(id or name)/attach</code> API</p>\n</li> </ul> <h3 id=\"exec-resize\">Exec Resize</h3> <p><code>POST /exec/(id)/resize</code></p> <p>Resizes the <code>tty</code> session used by the <code>exec</code> command <code>id</code>. The unit is number of characters. This API is valid only if <code>tty</code> was specified as part of creating and starting the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/resize?h=40&amp;w=80 HTTP/1.1\nContent-Type: text/plain\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: text/plain\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of <code>tty</code> session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> </ul> <h3 id=\"exec-inspect\">Exec Inspect</h3> <p><code>GET /exec/(id)/json</code></p> <p>Return low-level information about the <code>exec</code> command <code>id</code>.</p> <p><strong>Example request</strong>:</p> <pre>GET /exec/11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"CanRemove\": false,\n    \"ContainerID\": \"b53ee82b53a40c7dca428523e34f741f3abc51d9f297a14ff874bf761b995126\",\n    \"DetachKeys\": \"\",\n    \"ExitCode\": 2,\n    \"ID\": \"f33bbfb39f5b142420f4759b2348913bd4a8d1a6d7fd56499cb41a1bb91d7b3b\",\n    \"OpenStderr\": true,\n    \"OpenStdin\": true,\n    \"OpenStdout\": true,\n    \"ProcessConfig\": {\n        \"arguments\": [\n            \"-c\",\n            \"exit 2\"\n        ],\n        \"entrypoint\": \"sh\",\n        \"privileged\": false,\n        \"tty\": true,\n        \"user\": \"1000\"\n    },\n    \"Running\": false\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<strong>500</strong> - server error</li> </ul> <h2 id=\"2-4-volumes\">2.4 Volumes</h2> <h3 id=\"list-volumes\">List volumes</h3> <p><code>GET /volumes</code></p> <p><strong>Example request</strong>:</p> <pre>GET /volumes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Volumes\": [\n    {\n      \"Name\": \"tardis\",\n      \"Driver\": \"local\",\n      \"Mountpoint\": \"/var/lib/docker/volumes/tardis\"\n    }\n  ],\n  \"Warnings\": []\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>filters</strong> - JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the volumes list. There is one available filter: <code>dangling=true</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-volume\">Create a volume</h3> <p><code>POST /volumes/create</code></p> <p>Create a volume</p> <p><strong>Example request</strong>:</p> <pre>POST /volumes/create HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\",\n  \"Labels\": {\n    \"com.example.some-label\": \"some-value\",\n    \"com.example.some-other-label\": \"some-other-value\"\n  },\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\",\n  \"Driver\": \"local\",\n  \"Mountpoint\": \"/var/lib/docker/volumes/tardis\",\n  \"Labels\": {\n    \"com.example.some-label\": \"some-value\",\n    \"com.example.some-other-label\": \"some-other-value\"\n  },\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Name</strong> - The new volume’s name. If not specified, Docker generates a name.</li> <li>\n<strong>Driver</strong> - Name of the volume driver to use. Defaults to <code>local</code> for the name.</li> <li>\n<strong>DriverOpts</strong> - A mapping of driver options and values. These options are passed directly to the driver and are driver specific.</li> <li>\n<strong>Labels</strong> - Labels to set on the volume, specified as a map: <code>{\"key\":\"value\" [,\"key2\":\"value2\"]}</code>\n</li> </ul> <h3 id=\"inspect-a-volume\">Inspect a volume</h3> <p><code>GET /volumes/(name)</code></p> <p>Return low-level information on the volume <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /volumes/tardis\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"Name\": \"tardis\",\n    \"Driver\": \"local\",\n    \"Mountpoint\": \"/var/lib/docker/volumes/tardis/_data\",\n    \"Labels\": {\n        \"com.example.some-label\": \"some-value\",\n        \"com.example.some-other-label\": \"some-other-value\"\n    }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - no such volume</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"remove-a-volume\">Remove a volume</h3> <p><code>DELETE /volumes/(name)</code></p> <p>Instruct the driver to remove the volume (<code>name</code>).</p> <p><strong>Example request</strong>:</p> <pre>DELETE /volumes/tardis HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes</p> <ul> <li>\n<strong>204</strong> - no error</li> <li>\n<strong>404</strong> - no such volume or volume driver</li> <li>\n<strong>409</strong> - volume is in use and cannot be removed</li> <li>\n<strong>500</strong> - server error</li> </ul> <h2 id=\"2-5-networks\">2.5 Networks</h2> <h3 id=\"list-networks\">List networks</h3> <p><code>GET /networks</code></p> <p><strong>Example request</strong>:</p> <pre>GET /networks?filters={\"type\":{\"custom\":true}} HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Name\": \"bridge\",\n    \"Id\": \"f2de39df4171b0dc801e8002d1d999b77256983dfc63041c0f34030aa3977566\",\n    \"Scope\": \"local\",\n    \"Driver\": \"bridge\",\n    \"EnableIPv6\": false,\n    \"Internal\": false,\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": [\n        {\n          \"Subnet\": \"172.17.0.0/16\"\n        }\n      ]\n    },\n    \"Containers\": {\n      \"39b69226f9d79f5634485fb236a23b2fe4e96a0a94128390a7fbbcc167065867\": {\n        \"EndpointID\": \"ed2419a97c1d9954d05b46e462e7002ea552f216e9b136b80a7db8d98b442eda\",\n        \"MacAddress\": \"02:42:ac:11:00:02\",\n        \"IPv4Address\": \"172.17.0.2/16\",\n        \"IPv6Address\": \"\"\n      }\n    },\n    \"Options\": {\n      \"com.docker.network.bridge.default_bridge\": \"true\",\n      \"com.docker.network.bridge.enable_icc\": \"true\",\n      \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n      \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n      \"com.docker.network.bridge.name\": \"docker0\",\n      \"com.docker.network.driver.mtu\": \"1500\"\n    }\n  },\n  {\n    \"Name\": \"none\",\n    \"Id\": \"e086a3893b05ab69242d3c44e49483a3bbbd3a26b46baa8f61ab797c1088d794\",\n    \"Scope\": \"local\",\n    \"Driver\": \"null\",\n    \"EnableIPv6\": false,\n    \"Internal\": false,\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": []\n    },\n    \"Containers\": {},\n    \"Options\": {}\n  },\n  {\n    \"Name\": \"host\",\n    \"Id\": \"13e871235c677f196c4e1ecebb9dc733b9b2d2ab589e30c539efeda84a24215e\",\n    \"Scope\": \"local\",\n    \"Driver\": \"host\",\n    \"EnableIPv6\": false,\n    \"Internal\": false,\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": []\n    },\n    \"Containers\": {},\n    \"Options\": {}\n  }\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>filters</strong> - JSON encoded network list filter. The filter value is one of: <ul> <li>\n<code>name=&lt;network-name&gt;</code> Matches all or part of a network name.</li> <li>\n<code>id=&lt;network-id&gt;</code> Matches all or part of a network id.</li> <li>\n<code>type=[\"custom\"|\"builtin\"]</code> Filters networks by type. The <code>custom</code> keyword returns all user-defined networks.</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"inspect-network\">Inspect network</h3> <p><code>GET /networks/&lt;network-id&gt;</code></p> <p><strong>Example request</strong>:</p> <pre>GET /networks/7d86d31b1478e7cca9ebed7e73aa0fdeec46c5ca29497431d3007d2d9e15ed99 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Name\": \"net01\",\n  \"Id\": \"7d86d31b1478e7cca9ebed7e73aa0fdeec46c5ca29497431d3007d2d9e15ed99\",\n  \"Scope\": \"local\",\n  \"Driver\": \"bridge\",\n  \"EnableIPv6\": false,\n  \"IPAM\": {\n    \"Driver\": \"default\",\n    \"Config\": [\n      {\n        \"Subnet\": \"172.19.0.0/16\",\n        \"Gateway\": \"172.19.0.1/16\"\n      }\n    ],\n    \"Options\": {\n        \"foo\": \"bar\"\n    }\n  },\n  \"Internal\": false,\n  \"Containers\": {\n    \"19a4d5d687db25203351ed79d478946f861258f018fe384f229f2efa4b23513c\": {\n      \"Name\": \"test\",\n      \"EndpointID\": \"628cadb8bcb92de107b2a1e516cbffe463e321f548feb37697cce00ad694f21a\",\n      \"MacAddress\": \"02:42:ac:13:00:02\",\n      \"IPv4Address\": \"172.19.0.2/16\",\n      \"IPv6Address\": \"\"\n    }\n  },\n  \"Options\": {\n    \"com.docker.network.bridge.default_bridge\": \"true\",\n    \"com.docker.network.bridge.enable_icc\": \"true\",\n    \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n    \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n    \"com.docker.network.bridge.name\": \"docker0\",\n    \"com.docker.network.driver.mtu\": \"1500\"\n  },\n  \"Labels\": {\n    \"com.example.some-label\": \"some-value\",\n    \"com.example.some-other-label\": \"some-other-value\"\n  }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network not found</li> </ul> <h3 id=\"create-a-network\">Create a network</h3> <p><code>POST /networks/create</code></p> <p>Create a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/create HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Name\":\"isolated_nw\",\n  \"CheckDuplicate\":false,\n  \"Driver\":\"bridge\",\n  \"EnableIPv6\": true,\n  \"IPAM\":{\n    \"Config\":[\n       {\n          \"Subnet\":\"172.20.0.0/16\",\n          \"IPRange\":\"172.20.10.0/24\",\n          \"Gateway\":\"172.20.10.11\"\n        },\n        {\n          \"Subnet\":\"2001:db8:abcd::/64\",\n          \"Gateway\":\"2001:db8:abcd::1011\"\n        }\n    ],\n    \"Options\": {\n        \"foo\": \"bar\"\n    }\n  },\n  \"Internal\":true,\n  \"Options\": {\n    \"com.docker.network.bridge.default_bridge\": \"true\",\n    \"com.docker.network.bridge.enable_icc\": \"true\",\n    \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n    \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n    \"com.docker.network.bridge.name\": \"docker0\",\n    \"com.docker.network.driver.mtu\": \"1500\"\n  },\n  \"Labels\": {\n    \"com.example.some-label\": \"some-value\",\n    \"com.example.some-other-label\": \"some-other-value\"\n  }\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n  \"Id\": \"22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30\",\n  \"Warning\": \"\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> - no error</li> <li>\n<strong>404</strong> - plugin not found</li> <li>\n<strong>500</strong> - server error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Name</strong> - The new network’s name. this is a mandatory field</li> <li>\n<strong>CheckDuplicate</strong> - Requests daemon to check for networks with same name</li> <li>\n<strong>Driver</strong> - Name of the network driver plugin to use. Defaults to <code>bridge</code> driver</li> <li>\n<strong>Internal</strong> - Restrict external access to the network</li> <li>\n<strong>IPAM</strong> - Optional custom IP scheme for the network</li> <li>\n<strong>EnableIPv6</strong> - Enable IPv6 on the network</li> <li>\n<strong>Options</strong> - Network specific options to be used by the drivers</li> <li>\n<strong>Labels</strong> - Labels to set on the network, specified as a map: <code>{\"key\":\"value\" [,\"key2\":\"value2\"]}</code>\n</li> </ul> <h3 id=\"connect-a-container-to-a-network\">Connect a container to a network</h3> <p><code>POST /networks/(id)/connect</code></p> <p>Connect a container to a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30/connect HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Container\":\"3613f73ba0e4\",\n  \"EndpointConfig\": {\n    \"IPAMConfig\": {\n        \"IPv4Address\":\"172.24.56.89\",\n        \"IPv6Address\":\"2001:db8::5689\"\n    }\n  }\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network or container is not found</li> <li>\n<strong>500</strong> - Internal Server Error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>container</strong> - container-id/name to be connected to the network</li> </ul> <h3 id=\"disconnect-a-container-from-a-network\">Disconnect a container from a network</h3> <p><code>POST /networks/(id)/disconnect</code></p> <p>Disconnect a container from a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30/disconnect HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Container\":\"3613f73ba0e4\",\n  \"Force\":false\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network or container not found</li> <li>\n<strong>500</strong> - Internal Server Error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Container</strong> - container-id/name to be disconnected from a network</li> <li>\n<strong>Force</strong> - Force the container to disconnect from a network</li> </ul> <h3 id=\"remove-a-network\">Remove a network</h3> <p><code>DELETE /networks/(id)</code></p> <p>Instruct the driver to remove the network (<code>id</code>).</p> <p><strong>Example request</strong>:</p> <pre>DELETE /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes</p> <ul> <li>\n<strong>204</strong> - no error</li> <li>\n<strong>404</strong> - no such network</li> <li>\n<strong>500</strong> - server error</li> </ul> <h1 id=\"3-going-further\">3. Going further</h1> <h2 id=\"3-1-inside-docker-run\">3.1 Inside <code>docker run</code>\n</h2> <p>As an example, the <code>docker run</code> command line makes the following API calls:</p> <ul> <li><p>Create the container</p></li> <li>\n<p>If the status code is 404, it means the image doesn’t exist:</p> <ul> <li>Try to pull it.</li> <li>Then, retry to create the container.</li> </ul>\n</li> <li><p>Start the container.</p></li> <li><p>If you are not in detached mode:</p></li> <li><p>Attach to the container, using <code>logs=1</code> (to have <code>stdout</code> and <code>stderr</code> from the container’s start) and <code>stream=1</code></p></li> <li><p>If in detached mode or only <code>stdin</code> is attached, display the container’s id.</p></li> </ul> <h2 id=\"3-2-hijacking\">3.2 Hijacking</h2> <p>In this version of the API, <code>/attach</code>, uses hijacking to transport <code>stdin</code>, <code>stdout</code>, and <code>stderr</code> on the same socket.</p> <p>To hint potential proxies about connection hijacking, Docker client sends connection upgrade headers similarly to websocket.</p> <pre>Upgrade: tcp\nConnection: Upgrade\n</pre> <p>When Docker daemon detects the <code>Upgrade</code> header, it switches its status code from <strong>200 OK</strong> to <strong>101 UPGRADED</strong> and resends the same headers.</p> <h2 id=\"3-3-cors-requests\">3.3 CORS Requests</h2> <p>To set cross origin requests to the remote api please give values to <code>--api-cors-header</code> when running Docker in daemon mode. Set * (asterisk) allows all, default or blank means CORS disabled</p> <pre>$ docker daemon -H=\"192.168.1.9:2375\" --api-cors-header=\"http://foo.bar\"\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.23/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.23/</a>\n  </p>\n</div>\n","engine/reference/api/docker_remote_api_v1.22/index":"<h1 id=\"docker-remote-api-v1-22\">Docker Remote API v1.22</h1> <h2 id=\"1-brief-introduction\">1. Brief introduction</h2> <ul> <li>The Remote API has replaced <code>rcli</code>.</li> <li>The daemon listens on <code>unix:///var/run/docker.sock</code> but you can <a href=\"../../../quickstart/index#bind-docker-to-another-host-port-or-a-unix-socket\">Bind Docker to another host/port or a Unix socket</a>.</li> <li>The API tends to be REST. However, for some complex commands, like <code>attach</code> or <code>pull</code>, the HTTP connection is hijacked to transport <code>stdout</code>, <code>stdin</code> and <code>stderr</code>.</li> <li>When the client API version is newer than the daemon’s, these calls return an HTTP <code>400 Bad Request</code> error message.</li> </ul> <h1 id=\"2-endpoints\">2. Endpoints</h1> <h2 id=\"2-1-containers\">2.1 Containers</h2> <h3 id=\"list-containers\">List containers</h3> <p><code>GET /containers/json</code></p> <p>List containers</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/json?all=1&amp;before=8dfafdbc3a40&amp;size=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Id\": \"8dfafdbc3a40\",\n             \"Names\":[\"/boring_feynman\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 1\",\n             \"Created\": 1367854155,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [{\"PrivatePort\": 2222, \"PublicPort\": 3333, \"Type\": \"tcp\"}],\n             \"Labels\": {\n                     \"com.example.vendor\": \"Acme\",\n                     \"com.example.license\": \"GPL\",\n                     \"com.example.version\": \"1.0\"\n             },\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"2cdc4edb1ded3631c81f57966563e5c8525b81121bb3706a9a9a3ae102711f3f\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.2\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:02\"\n                              }\n                     }\n             }\n     },\n     {\n             \"Id\": \"9cd87474be90\",\n             \"Names\":[\"/coolName\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 222222\",\n             \"Created\": 1367854155,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"88eaed7b37b38c2a3f0c4bc796494fdf51b270c2d22656412a2ca5d559a64d7a\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.8\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:08\"\n                              }\n                     }\n             }\n\n     },\n     {\n             \"Id\": \"3176a2479c92\",\n             \"Names\":[\"/sleepy_dog\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 3333333333333333\",\n             \"Created\": 1367854154,\n             \"Status\": \"Exit 0\",\n             \"Ports\":[],\n             \"Labels\": {},\n             \"SizeRw\":12288,\n             \"SizeRootFs\":0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"8b27c041c30326d59cd6e6f510d4f8d1d570a228466f956edf7815508f78e30d\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.6\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:06\"\n                              }\n                     }\n             }\n\n     },\n     {\n             \"Id\": \"4cb07b47f9fb\",\n             \"Names\":[\"/running_cat\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 444444444444444444444444444444444\",\n             \"Created\": 1367854152,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"d91c7b2f0644403d7ef3095985ea0e2370325cd2332ff3a3225c4247328e66e9\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.5\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:05\"\n                              }\n                     }\n             }\n\n     }\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, Show all containers. Only running containers are shown by default (i.e., this defaults to false)</li> <li>\n<strong>limit</strong> – Show <code>limit</code> last created containers, include non-running ones.</li> <li>\n<strong>since</strong> – Show only containers created since Id, include non-running ones.</li> <li>\n<strong>before</strong> – Show only containers created before Id, include non-running ones.</li> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, Show the containers sizes</li> <li>\n<strong>filters</strong> - a JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the containers list. Available filters: <ul> <li>\n<code>exited=&lt;int&gt;</code>; -- containers with exit code of <code>&lt;int&gt;</code> ;</li> <li>\n<code>status=</code>(<code>created</code>|<code>restarting</code>|<code>running</code>|<code>paused</code>|<code>exited</code>|<code>dead</code>)</li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of a container label</li> <li>\n<code>isolation=</code>(<code>default</code>|<code>process</code>|<code>hyperv</code>) (Windows daemon only)</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-a-container\">Create a container</h3> <p><code>POST /containers/create</code></p> <p>Create a container</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/create HTTP/1.1\nContent-Type: application/json\n\n{\n       \"Hostname\": \"\",\n       \"Domainname\": \"\",\n       \"User\": \"\",\n       \"AttachStdin\": false,\n       \"AttachStdout\": true,\n       \"AttachStderr\": true,\n       \"Tty\": false,\n       \"OpenStdin\": false,\n       \"StdinOnce\": false,\n       \"Env\": [\n               \"FOO=bar\",\n               \"BAZ=quux\"\n       ],\n       \"Cmd\": [\n               \"date\"\n       ],\n       \"Entrypoint\": \"\",\n       \"Image\": \"ubuntu\",\n       \"Labels\": {\n               \"com.example.vendor\": \"Acme\",\n               \"com.example.license\": \"GPL\",\n               \"com.example.version\": \"1.0\"\n       },\n       \"Volumes\": {\n         \"/volumes/data\": {}\n       },\n       \"WorkingDir\": \"\",\n       \"NetworkDisabled\": false,\n       \"MacAddress\": \"12:34:56:78:9a:bc\",\n       \"ExposedPorts\": {\n               \"22/tcp\": {}\n       },\n       \"StopSignal\": \"SIGTERM\",\n       \"HostConfig\": {\n         \"Binds\": [\"/tmp:/tmp\"],\n         \"Links\": [\"redis3:redis\"],\n         \"Memory\": 0,\n         \"MemorySwap\": 0,\n         \"MemoryReservation\": 0,\n         \"KernelMemory\": 0,\n         \"CpuShares\": 512,\n         \"CpuPeriod\": 100000,\n         \"CpuQuota\": 50000,\n         \"CpusetCpus\": \"0,1\",\n         \"CpusetMems\": \"0,1\",\n         \"BlkioWeight\": 300,\n         \"BlkioWeightDevice\": [{}],\n         \"BlkioDeviceReadBps\": [{}],\n         \"BlkioDeviceReadIOps\": [{}],\n         \"BlkioDeviceWriteBps\": [{}],\n         \"BlkioDeviceWriteIOps\": [{}],\n         \"MemorySwappiness\": 60,\n         \"OomKillDisable\": false,\n         \"OomScoreAdj\": 500,\n         \"PortBindings\": { \"22/tcp\": [{ \"HostPort\": \"11022\" }] },\n         \"PublishAllPorts\": false,\n         \"Privileged\": false,\n         \"ReadonlyRootfs\": false,\n         \"Dns\": [\"8.8.8.8\"],\n         \"DnsOptions\": [\"\"],\n         \"DnsSearch\": [\"\"],\n         \"ExtraHosts\": null,\n         \"VolumesFrom\": [\"parent\", \"other:ro\"],\n         \"CapAdd\": [\"NET_ADMIN\"],\n         \"CapDrop\": [\"MKNOD\"],\n         \"GroupAdd\": [\"newgroup\"],\n         \"RestartPolicy\": { \"Name\": \"\", \"MaximumRetryCount\": 0 },\n         \"NetworkMode\": \"bridge\",\n         \"Devices\": [],\n         \"Ulimits\": [{}],\n         \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} },\n         \"SecurityOpt\": [],\n         \"CgroupParent\": \"\",\n         \"VolumeDriver\": \"\",\n         \"ShmSize\": 67108864\n      },\n      \"NetworkingConfig\": {\n      \"EndpointsConfig\": {\n          \"isolated_nw\" : {\n              \"IPAMConfig\": {\n                  \"IPv4Address\":\"172.20.30.33\",\n                  \"IPv6Address\":\"2001:db8:abcd::3033\"\n              },\n              \"Links\":[\"container_1\", \"container_2\"],\n              \"Aliases\":[\"server_x\", \"server_y\"]\n          }\n      }\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 201 Created\n  Content-Type: application/json\n\n  {\n       \"Id\":\"e90e34656806\",\n       \"Warnings\":[]\n  }\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Hostname</strong> - A string value containing the hostname to use for the container.</li> <li>\n<strong>Domainname</strong> - A string value containing the domain name to use for the container.</li> <li>\n<strong>User</strong> - A string value specifying the user inside the container.</li> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code>.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code>.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code>.</li> <li>\n<strong>Tty</strong> - Boolean value, Attach standard streams to a <code>tty</code>, including <code>stdin</code> if it is not closed.</li> <li>\n<strong>OpenStdin</strong> - Boolean value, opens stdin,</li> <li>\n<strong>StdinOnce</strong> - Boolean value, close <code>stdin</code> after the 1 attached client disconnects.</li> <li>\n<strong>Env</strong> - A list of environment variables in the form of <code>[\"VAR=value\"[,\"VAR2=value2\"]]</code>\n</li> <li>\n<strong>Labels</strong> - Adds a map of labels to a container. To specify a map: <code>{\"key\":\"value\"[,\"key2\":\"value2\"]}</code>\n</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> <li>\n<strong>Entrypoint</strong> - Set the entry point for the container as a string or an array of strings.</li> <li>\n<strong>Image</strong> - A string specifying the image name to use for the container.</li> <li>\n<strong>Volumes</strong> - An object mapping mount point paths (strings) inside the container to empty objects.</li> <li>\n<strong>WorkingDir</strong> - A string specifying the working directory for commands to run in.</li> <li>\n<strong>NetworkDisabled</strong> - Boolean value, when true disables networking for the container</li> <li>\n<strong>ExposedPorts</strong> - An object mapping ports to an empty object in the form of: <code>\"ExposedPorts\": { \"&lt;port&gt;/&lt;tcp|udp&gt;: {}\" }</code>\n</li> <li>\n<strong>StopSignal</strong> - Signal to stop a container as a string or unsigned integer. <code>SIGTERM</code> by default.</li> <li>\n<strong>HostConfig</strong> <ul> <li>\n<strong>Binds</strong> – A list of volume bindings for this container. Each volume binding is a string in one of these forms: <ul> <li>\n<code>host_path:container_path</code> to bind-mount a host path into the container</li> <li>\n<code>host_path:container_path:ro</code> to make the bind-mount read-only inside the container.</li> <li>\n<code>volume_name:container_path</code> to bind-mount a volume managed by a volume plugin into the container.</li> <li>\n<code>volume_name:container_path:ro</code> to make the bind mount read-only inside the container.</li> </ul>\n</li> <li>\n<strong>Links</strong> - A list of links for the container. Each link entry should be in the form of <code>container_name:alias</code>.</li> <li>\n<strong>Memory</strong> - Memory limit in bytes.</li> <li>\n<strong>MemorySwap</strong> - Total memory limit (memory + swap); set <code>-1</code> to enable unlimited swap. You must use this with <code>memory</code> and make the swap value larger than <code>memory</code>.</li> <li>\n<strong>MemoryReservation</strong> - Memory soft limit in bytes.</li> <li>\n<strong>KernelMemory</strong> - Kernel memory limit in bytes.</li> <li>\n<strong>CpuShares</strong> - An integer value containing the container’s CPU Shares (ie. the relative weight vs other containers).</li> <li>\n<strong>CpuPeriod</strong> - The length of a CPU period in microseconds.</li> <li>\n<strong>CpuQuota</strong> - Microseconds of CPU time that the container can get in a CPU period.</li> <li>\n<strong>CpusetCpus</strong> - String value containing the <code>cgroups CpusetCpus</code> to use.</li> <li>\n<strong>CpusetMems</strong> - Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems.</li> <li>\n<strong>BlkioWeight</strong> - Block IO weight (relative weight) accepts a weight value between 10 and 1000.</li> <li>\n<strong>BlkioWeightDevice</strong> - Block IO weight (relative device weight) in the form of: <code>\"BlkioWeightDevice\": [{\"Path\": \"device_path\", \"Weight\": weight}]</code>\n</li> <li>\n<strong>BlkioDeviceReadBps</strong> - Limit read rate (bytes per second) from a device in the form of: <code>\"BlkioDeviceReadBps\": [{\"Path\": \"device_path\", \"Rate\": rate}]</code>, for example: <code>\"BlkioDeviceReadBps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1024\"}]\"</code>\n</li> <li>\n<strong>BlkioDeviceWriteBps</strong> - Limit write rate (bytes per second) to a device in the form of: <code>\"BlkioDeviceWriteBps\": [{\"Path\": \"device_path\", \"Rate\": rate}]</code>, for example: <code>\"BlkioDeviceWriteBps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1024\"}]\"</code>\n</li> <li>\n<strong>BlkioDeviceReadIOps</strong> - Limit read rate (IO per second) from a device in the form of: <code>\"BlkioDeviceReadIOps\": [{\"Path\": \"device_path\", \"Rate\": rate}]</code>, for example: <code>\"BlkioDeviceReadIOps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1000\"}]</code>\n</li> <li>\n<strong>BlkioDeviceWiiteIOps</strong> - Limit write rate (IO per second) to a device in the form of: <code>\"BlkioDeviceWriteIOps\": [{\"Path\": \"device_path\", \"Rate\": rate}]</code>, for example: <code>\"BlkioDeviceWriteIOps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1000\"}]</code>\n</li> <li>\n<strong>MemorySwappiness</strong> - Tune a container’s memory swappiness behavior. Accepts an integer between 0 and 100.</li> <li>\n<strong>OomKillDisable</strong> - Boolean value, whether to disable OOM Killer for the container or not.</li> <li>\n<strong>OomScoreAdj</strong> - An integer value containing the score given to the container in order to tune OOM killer preferences.</li> <li>\n<strong>PortBindings</strong> - A map of exposed container ports and the host port they should map to. A JSON object in the form <code>{ &lt;port&gt;/&lt;protocol&gt;: [{ \"HostPort\": \"&lt;port&gt;\" }] }</code> Take note that <code>port</code> is specified as a string and not an integer value.</li> <li>\n<strong>PublishAllPorts</strong> - Allocates a random host port for all of a container’s exposed ports. Specified as a boolean value.</li> <li>\n<strong>Privileged</strong> - Gives the container full access to the host. Specified as a boolean value.</li> <li>\n<strong>ReadonlyRootfs</strong> - Mount the container’s root filesystem as read only. Specified as a boolean value.</li> <li>\n<strong>Dns</strong> - A list of DNS servers for the container to use.</li> <li>\n<strong>DnsOptions</strong> - A list of DNS options</li> <li>\n<strong>DnsSearch</strong> - A list of DNS search domains</li> <li>\n<strong>ExtraHosts</strong> - A list of hostnames/IP mappings to add to the container’s <code>/etc/hosts</code> file. Specified in the form <code>[\"hostname:IP\"]</code>.</li> <li>\n<strong>VolumesFrom</strong> - A list of volumes to inherit from another container. Specified in the form <code>&lt;container name&gt;[:&lt;ro|rw&gt;]</code>\n</li> <li>\n<strong>CapAdd</strong> - A list of kernel capabilities to add to the container.</li> <li>\n<strong>Capdrop</strong> - A list of kernel capabilities to drop from the container.</li> <li>\n<strong>GroupAdd</strong> - A list of additional groups that the container process will run as</li> <li>\n<strong>RestartPolicy</strong> – The behavior to apply when the container exits. The value is an object with a <code>Name</code> property of either <code>\"always\"</code> to always restart, <code>\"unless-stopped\"</code> to restart always except when user has manually stopped the container or <code>\"on-failure\"</code> to restart only when the container exit code is non-zero. If <code>on-failure</code> is used, <code>MaximumRetryCount</code> controls the number of times to retry before giving up. The default is not to restart. (optional) An ever increasing delay (double the previous delay, starting at 100mS) is added before each restart to prevent flooding the server.</li> <li>\n<strong>NetworkMode</strong> - Sets the networking mode for the container. Supported standard values are: <code>bridge</code>, <code>host</code>, <code>none</code>, and <code>container:&lt;name|id&gt;</code>. Any other value is taken as a custom network’s name to which this container should connect to.</li> <li>\n<strong>Devices</strong> - A list of devices to add to the container specified as a JSON object in the form <code>{ \"PathOnHost\": \"/dev/deviceName\", \"PathInContainer\": \"/dev/deviceName\", \"CgroupPermissions\": \"mrw\"}</code>\n</li> <li>\n<strong>Ulimits</strong> - A list of ulimits to set in the container, specified as <code>{ \"Name\": &lt;name&gt;, \"Soft\": &lt;soft limit&gt;, \"Hard\": &lt;hard limit&gt; }</code>, for example: <code>Ulimits: { \"Name\": \"nofile\", \"Soft\": 1024, \"Hard\": 2048 }</code>\n</li> <li>\n<strong>SecurityOpt</strong>: A list of string values to customize labels for MLS systems, such as SELinux.</li> <li>\n<strong>LogConfig</strong> - Log configuration for the container, specified as a JSON object in the form <code>{ \"Type\": \"&lt;driver_name&gt;\", \"Config\": {\"key1\": \"val1\"}}</code>. Available types: <code>json-file</code>, <code>syslog</code>, <code>journald</code>, <code>gelf</code>, <code>awslogs</code>, <code>splunk</code>, <code>none</code>. <code>json-file</code> logging driver.</li> <li>\n<strong>CgroupParent</strong> - Path to <code>cgroups</code> under which the container’s <code>cgroup</code> is created. If the path is not absolute, the path is considered to be relative to the <code>cgroups</code> path of the init process. Cgroups are created if they do not already exist.</li> <li>\n<strong>VolumeDriver</strong> - Driver that this container users to mount volumes.</li> <li>\n<strong>ShmSize</strong> - Size of <code>/dev/shm</code> in bytes. The size must be greater than 0. If omitted the system uses 64MB.</li> </ul>\n</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – Assign the specified name to the container. Must match <code>/?[a-zA-Z0-9_-]+</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>406</strong> – impossible to attach (container not running)</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-a-container\">Inspect a container</h3> <p><code>GET /containers/(id or name)/json</code></p> <p>Return low-level information on the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>  GET /containers/4fa6e0f0c678/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"AppArmorProfile\": \"\",\n    \"Args\": [\n        \"-c\",\n        \"exit 9\"\n    ],\n    \"Config\": {\n        \"AttachStderr\": true,\n        \"AttachStdin\": false,\n        \"AttachStdout\": true,\n        \"Cmd\": [\n            \"/bin/sh\",\n            \"-c\",\n            \"exit 9\"\n        ],\n        \"Domainname\": \"\",\n        \"Entrypoint\": null,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": null,\n        \"Hostname\": \"ba033ac44011\",\n        \"Image\": \"ubuntu\",\n        \"Labels\": {\n            \"com.example.vendor\": \"Acme\",\n            \"com.example.license\": \"GPL\",\n            \"com.example.version\": \"1.0\"\n        },\n        \"MacAddress\": \"\",\n        \"NetworkDisabled\": false,\n        \"OnBuild\": null,\n        \"OpenStdin\": false,\n        \"StdinOnce\": false,\n        \"Tty\": false,\n        \"User\": \"\",\n        \"Volumes\": {\n                      \"/volumes/data\": {}\n                    },\n        \"WorkingDir\": \"\",\n        \"StopSignal\": \"SIGTERM\"\n    },\n    \"Created\": \"2015-01-06T15:47:31.485331387Z\",\n    \"Driver\": \"devicemapper\",\n    \"ExecDriver\": \"native-0.2\",\n    \"ExecIDs\": null,\n    \"HostConfig\": {\n        \"Binds\": null,\n        \"BlkioWeight\": 0,\n        \"BlkioWeightDevice\": [{}],\n        \"BlkioDeviceReadBps\": [{}],\n        \"BlkioDeviceWriteBps\": [{}],\n        \"BlkioDeviceReadIOps\": [{}],\n        \"BlkioDeviceWriteIOps\": [{}],\n        \"CapAdd\": null,\n        \"CapDrop\": null,\n        \"ContainerIDFile\": \"\",\n        \"CpusetCpus\": \"\",\n        \"CpusetMems\": \"\",\n        \"CpuShares\": 0,\n        \"CpuPeriod\": 100000,\n        \"Devices\": [],\n        \"Dns\": null,\n        \"DnsOptions\": null,\n        \"DnsSearch\": null,\n        \"ExtraHosts\": null,\n        \"IpcMode\": \"\",\n        \"Links\": null,\n        \"LxcConf\": [],\n        \"Memory\": 0,\n        \"MemorySwap\": 0,\n        \"MemoryReservation\": 0,\n        \"KernelMemory\": 0,\n        \"OomKillDisable\": false,\n        \"OomScoreAdj\": 500,\n        \"NetworkMode\": \"bridge\",\n        \"PortBindings\": {},\n        \"Privileged\": false,\n        \"ReadonlyRootfs\": false,\n        \"PublishAllPorts\": false,\n        \"RestartPolicy\": {\n            \"MaximumRetryCount\": 2,\n            \"Name\": \"on-failure\"\n        },\n        \"LogConfig\": {\n            \"Config\": null,\n            \"Type\": \"json-file\"\n        },\n        \"SecurityOpt\": null,\n        \"VolumesFrom\": null,\n        \"Ulimits\": [{}],\n        \"VolumeDriver\": \"\",\n        \"ShmSize\": 67108864\n    },\n    \"HostnamePath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hostname\",\n    \"HostsPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Id\": \"ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39\",\n    \"Image\": \"04c5d3b7b0656168630d3ba35d8889bd0e9caafcaeb3004d2bfbc47e7c5d35d2\",\n    \"MountLabel\": \"\",\n    \"Name\": \"/boring_euclid\",\n    \"NetworkSettings\": {\n        \"Bridge\": \"\",\n        \"SandboxID\": \"\",\n        \"HairpinMode\": false,\n        \"LinkLocalIPv6Address\": \"\",\n        \"LinkLocalIPv6PrefixLen\": 0,\n        \"Ports\": null,\n        \"SandboxKey\": \"\",\n        \"SecondaryIPAddresses\": null,\n        \"SecondaryIPv6Addresses\": null,\n        \"EndpointID\": \"\",\n        \"Gateway\": \"\",\n        \"GlobalIPv6Address\": \"\",\n        \"GlobalIPv6PrefixLen\": 0,\n        \"IPAddress\": \"\",\n        \"IPPrefixLen\": 0,\n        \"IPv6Gateway\": \"\",\n        \"MacAddress\": \"\",\n        \"Networks\": {\n            \"bridge\": {\n                \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                \"EndpointID\": \"7587b82f0dada3656fda26588aee72630c6fab1536d36e394b2bfbcf898c971d\",\n                \"Gateway\": \"172.17.0.1\",\n                \"IPAddress\": \"172.17.0.2\",\n                \"IPPrefixLen\": 16,\n                \"IPv6Gateway\": \"\",\n                \"GlobalIPv6Address\": \"\",\n                \"GlobalIPv6PrefixLen\": 0,\n                \"MacAddress\": \"02:42:ac:12:00:02\"\n            }\n        }\n    },\n    \"Path\": \"/bin/sh\",\n    \"ProcessLabel\": \"\",\n    \"ResolvConfPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/resolv.conf\",\n    \"RestartCount\": 1,\n    \"State\": {\n        \"Error\": \"\",\n        \"ExitCode\": 9,\n        \"FinishedAt\": \"2015-01-06T15:47:32.080254511Z\",\n        \"OOMKilled\": false,\n        \"Dead\": false,\n        \"Paused\": false,\n        \"Pid\": 0,\n        \"Restarting\": false,\n        \"Running\": true,\n        \"StartedAt\": \"2015-01-06T15:47:32.072697474Z\",\n        \"Status\": \"running\"\n    },\n    \"Mounts\": [\n        {\n            \"Name\": \"fac362...80535\",\n            \"Source\": \"/data\",\n            \"Destination\": \"/data\",\n            \"Driver\": \"local\",\n            \"Mode\": \"ro,Z\",\n            \"RW\": false,\n            \"Propagation\": \"\"\n        }\n    ]\n}\n</pre> <p><strong>Example request, with size information</strong>:</p> <pre>GET /containers/4fa6e0f0c678/json?size=1 HTTP/1.1\n</pre> <p><strong>Example response, with size information</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n....\n\"SizeRw\": 0,\n\"SizeRootFs\": 972,\n....\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, return container size information. Default is <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"list-processes-running-inside-a-container\">List processes running inside a container</h3> <p><code>GET /containers/(id or name)/top</code></p> <p>List processes running inside the container <code>id</code>. On Unix systems this is done by running the <code>ps</code> command. This endpoint is not supported on Windows.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Titles\" : [\n     \"UID\", \"PID\", \"PPID\", \"C\", \"STIME\", \"TTY\", \"TIME\", \"CMD\"\n   ],\n   \"Processes\" : [\n     [\n       \"root\", \"13642\", \"882\", \"0\", \"17:03\", \"pts/0\", \"00:00:00\", \"/bin/bash\"\n     ],\n     [\n       \"root\", \"13735\", \"13642\", \"0\", \"17:06\", \"pts/0\", \"00:00:00\", \"sleep 10\"\n     ]\n   ]\n}\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top?ps_args=aux HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Titles\" : [\n    \"USER\",\"PID\",\"%CPU\",\"%MEM\",\"VSZ\",\"RSS\",\"TTY\",\"STAT\",\"START\",\"TIME\",\"COMMAND\"\n  ]\n  \"Processes\" : [\n    [\n      \"root\",\"13642\",\"0.0\",\"0.1\",\"18172\",\"3184\",\"pts/0\",\"Ss\",\"17:03\",\"0:00\",\"/bin/bash\"\n    ],\n    [\n      \"root\",\"13895\",\"0.0\",\"0.0\",\"4348\",\"692\",\"pts/0\",\"S+\",\"17:15\",\"0:00\",\"sleep 10\"\n    ]\n  ],\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>ps_args</strong> – <code>ps</code> arguments to use (e.g., <code>aux</code>), defaults to <code>-ef</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-logs\">Get container logs</h3> <p><code>GET /containers/(id or name)/logs</code></p> <p>Get <code>stdout</code> and <code>stderr</code> logs from the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: This endpoint works only for containers with the <code>json-file</code> or <code>journald</code> logging drivers.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre> GET /containers/4fa6e0f0c678/logs?stderr=1&amp;stdout=1&amp;timestamps=1&amp;follow=1&amp;tail=10&amp;since=1428990821 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre> HTTP/1.1 101 UPGRADED\n Content-Type: application/vnd.docker.raw-stream\n Connection: Upgrade\n Upgrade: tcp\n\n {{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>follow</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, show <code>stdout</code> log. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, show <code>stderr</code> log. Default <code>false</code>.</li> <li>\n<strong>since</strong> – UNIX timestamp (integer) to filter logs. Specifying a timestamp will only output log-entries since that timestamp. Default: 0 (unfiltered)</li> <li>\n<strong>timestamps</strong> – 1/True/true or 0/False/false, print timestamps for every log line. Default <code>false</code>.</li> <li>\n<strong>tail</strong> – Output specified number of lines at the end of logs: <code>all</code> or <code>&lt;number&gt;</code>. Default all.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-changes-on-a-container-s-filesystem\">Inspect changes on a container’s filesystem</h3> <p><code>GET /containers/(id or name)/changes</code></p> <p>Inspect changes on container <code>id</code>’s filesystem</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/changes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Path\": \"/dev\",\n             \"Kind\": 0\n     },\n     {\n             \"Path\": \"/dev/kmsg\",\n             \"Kind\": 1\n     },\n     {\n             \"Path\": \"/test\",\n             \"Kind\": 1\n     }\n]\n</pre> <p>Values for <code>Kind</code>:</p> <ul> <li>\n<code>0</code>: Modify</li> <li>\n<code>1</code>: Add</li> <li>\n<code>2</code>: Delete</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"export-a-container\">Export a container</h3> <p><code>GET /containers/(id or name)/export</code></p> <p>Export the contents of container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/export HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/octet-stream\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-stats-based-on-resource-usage\">Get container stats based on resource usage</h3> <p><code>GET /containers/(id or name)/stats</code></p> <p>This endpoint returns a live stream of a container’s resource usage statistics.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/redis1/stats HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Type: application/json\n\n  {\n     \"read\" : \"2015-01-08T22:57:31.547920715Z\",\n     \"networks\": {\n             \"eth0\": {\n                 \"rx_bytes\": 5338,\n                 \"rx_dropped\": 0,\n                 \"rx_errors\": 0,\n                 \"rx_packets\": 36,\n                 \"tx_bytes\": 648,\n                 \"tx_dropped\": 0,\n                 \"tx_errors\": 0,\n                 \"tx_packets\": 8\n             },\n             \"eth5\": {\n                 \"rx_bytes\": 4641,\n                 \"rx_dropped\": 0,\n                 \"rx_errors\": 0,\n                 \"rx_packets\": 26,\n                 \"tx_bytes\": 690,\n                 \"tx_dropped\": 0,\n                 \"tx_errors\": 0,\n                 \"tx_packets\": 9\n             }\n     },\n     \"memory_stats\" : {\n        \"stats\" : {\n           \"total_pgmajfault\" : 0,\n           \"cache\" : 0,\n           \"mapped_file\" : 0,\n           \"total_inactive_file\" : 0,\n           \"pgpgout\" : 414,\n           \"rss\" : 6537216,\n           \"total_mapped_file\" : 0,\n           \"writeback\" : 0,\n           \"unevictable\" : 0,\n           \"pgpgin\" : 477,\n           \"total_unevictable\" : 0,\n           \"pgmajfault\" : 0,\n           \"total_rss\" : 6537216,\n           \"total_rss_huge\" : 6291456,\n           \"total_writeback\" : 0,\n           \"total_inactive_anon\" : 0,\n           \"rss_huge\" : 6291456,\n           \"hierarchical_memory_limit\" : 67108864,\n           \"total_pgfault\" : 964,\n           \"total_active_file\" : 0,\n           \"active_anon\" : 6537216,\n           \"total_active_anon\" : 6537216,\n           \"total_pgpgout\" : 414,\n           \"total_cache\" : 0,\n           \"inactive_anon\" : 0,\n           \"active_file\" : 0,\n           \"pgfault\" : 964,\n           \"inactive_file\" : 0,\n           \"total_pgpgin\" : 477\n        },\n        \"max_usage\" : 6651904,\n        \"usage\" : 6537216,\n        \"failcnt\" : 0,\n        \"limit\" : 67108864\n     },\n     \"blkio_stats\" : {},\n     \"cpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24472255,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100215355,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 739306590000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     },\n     \"precpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24350896,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100093996,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 9492140000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     }\n  }\n</pre> <p>The precpu_stats is the cpu statistic of last read, which is used for calculating the cpu usage percent. It is not the exact copy of the “cpu_stats” field.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, pull stats once then disconnect. Default <code>true</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"resize-a-container-tty\">Resize a container TTY</h3> <p><code>POST /containers/(id or name)/resize</code></p> <p>Resize the TTY for container with <code>id</code>. The unit is number of characters. You must restart the container for the resize to take effect.</p> <p><strong>Example request</strong>:</p> <pre>  POST /containers/4fa6e0f0c678/resize?h=40&amp;w=80 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Length: 0\n  Content-Type: text/plain; charset=utf-8\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of <code>tty</code> session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – No such container</li> <li>\n<strong>500</strong> – Cannot resize container</li> </ul> <h3 id=\"start-a-container\">Start a container</h3> <p><code>POST /containers/(id or name)/start</code></p> <p>Start the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: For backwards compatibility, this endpoint accepts a <code>HostConfig</code> as JSON-encoded request body. See <a href=\"#create-a-container\">create a container</a> for details.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/start HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>detachKeys</strong> – Override the key sequence for detaching a container. Format is a single character <code>[a-Z]</code> or <code>ctrl-&lt;value&gt;</code> where <code>&lt;value&gt;</code> is one of: <code>a-z</code>, <code>@</code>, <code>^</code>, <code>[</code>, <code>,</code> or <code>_</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already started</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"stop-a-container\">Stop a container</h3> <p><code>POST /containers/(id or name)/stop</code></p> <p>Stop the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/stop?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already stopped</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"restart-a-container\">Restart a container</h3> <p><code>POST /containers/(id or name)/restart</code></p> <p>Restart the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/restart?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"kill-a-container\">Kill a container</h3> <p><code>POST /containers/(id or name)/kill</code></p> <p>Kill the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/kill HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters</p> <ul> <li>\n<strong>signal</strong> - Signal to send to the container: integer or string like <code>SIGINT</code>. When not set, <code>SIGKILL</code> is assumed and the call waits for the container to exit.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"update-a-container\">Update a container</h3> <p><code>POST /containers/(id or name)/update</code></p> <p>Update resource configs of one or more containers.</p> <p><strong>Example request</strong>:</p> <pre>   POST /containers/e90e34656806/update HTTP/1.1\n   Content-Type: application/json\n\n   {\n     \"BlkioWeight\": 300,\n     \"CpuShares\": 512,\n     \"CpuPeriod\": 100000,\n     \"CpuQuota\": 50000,\n     \"CpusetCpus\": \"0,1\",\n     \"CpusetMems\": \"0\",\n     \"Memory\": 314572800,\n     \"MemorySwap\": 514288000,\n     \"MemoryReservation\": 209715200,\n     \"KernelMemory\": 52428800,\n   }\n</pre> <p><strong>Example response</strong>:</p> <pre>   HTTP/1.1 200 OK\n   Content-Type: application/json\n\n   {\n       \"Warnings\": []\n   }\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"rename-a-container\">Rename a container</h3> <p><code>POST /containers/(id or name)/rename</code></p> <p>Rename the container <code>id</code> to a <code>new_name</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/rename?name=new_name HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – new name for the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - conflict name already assigned</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"pause-a-container\">Pause a container</h3> <p><code>POST /containers/(id or name)/pause</code></p> <p>Pause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/pause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"unpause-a-container\">Unpause a container</h3> <p><code>POST /containers/(id or name)/unpause</code></p> <p>Unpause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/unpause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"attach-to-a-container\">Attach to a container</h3> <p><code>POST /containers/(id or name)/attach</code></p> <p>Attach to the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/attach?logs=1&amp;stream=0&amp;stdout=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 101 UPGRADED\nContent-Type: application/vnd.docker.raw-stream\nConnection: Upgrade\nUpgrade: tcp\n\n{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>detachKeys</strong> – Override the key sequence for detaching a container. Format is a single character <code>[a-Z]</code> or <code>ctrl-&lt;value&gt;</code> where <code>&lt;value&gt;</code> is one of: <code>a-z</code>, <code>@</code>, <code>^</code>, <code>[</code>, <code>,</code> or <code>_</code>.</li> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<p><strong>500</strong> – server error</p> <p><strong>Stream details</strong>:</p> <p>When using the TTY setting is enabled in <a href=\"#create-a-container\"><code>POST /containers/create</code> </a>, the stream is the raw data from the process PTY and client’s <code>stdin</code>. When the TTY is disabled, then the stream is multiplexed to separate <code>stdout</code> and <code>stderr</code>.</p> <p>The format is a <strong>Header</strong> and a <strong>Payload</strong> (frame).</p> <p><strong>HEADER</strong></p> <p>The header contains the information which the stream writes (<code>stdout</code> or <code>stderr</code>). It also contains the size of the associated frame encoded in the last four bytes (<code>uint32</code>).</p> <p>It is encoded on the first eight bytes like this:</p> <pre>header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}\n</pre> <p><code>STREAM_TYPE</code> can be:</p>\n</li> <li><p>0: <code>stdin</code> (is written on <code>stdout</code>)</p></li> <li><p>1: <code>stdout</code></p></li> <li>\n<p>2: <code>stderr</code></p> <p><code>SIZE1, SIZE2, SIZE3, SIZE4</code> are the four bytes of the <code>uint32</code> size encoded as big endian.</p> <p><strong>PAYLOAD</strong></p> <p>The payload is the raw stream.</p> <p><strong>IMPLEMENTATION</strong></p> <p>The simplest way to implement the Attach protocol is the following:</p> <ol> <li>Read eight bytes.</li> <li>Choose <code>stdout</code> or <code>stderr</code> depending on the first byte.</li> <li>Extract the frame size from the last four bytes.</li> <li>Read the extracted size and output it on the correct output.</li> <li>Goto 1.</li> </ol>\n</li> </ul> <h3 id=\"attach-to-a-container-websocket\">Attach to a container (websocket)</h3> <p><code>GET /containers/(id or name)/attach/ws</code></p> <p>Attach to the container <code>id</code> via websocket</p> <p>Implements websocket protocol handshake according to <a href=\"http://tools.ietf.org/html/rfc6455\">RFC 6455</a></p> <p><strong>Example request</strong></p> <pre>GET /containers/e90e34656806/attach/ws?logs=0&amp;stream=1&amp;stdin=1&amp;stdout=1&amp;stderr=1 HTTP/1.1\n</pre> <p><strong>Example response</strong></p> <pre>{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>detachKeys</strong> – Override the key sequence for detaching a container. Format is a single character <code>[a-Z]</code> or <code>ctrl-&lt;value&gt;</code> where <code>&lt;value&gt;</code> is one of: <code>a-z</code>, <code>@</code>, <code>^</code>, <code>[</code>, <code>,</code> or <code>_</code>.</li> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"wait-a-container\">Wait a container</h3> <p><code>POST /containers/(id or name)/wait</code></p> <p>Block until container <code>id</code> stops, then returns the exit code</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/wait HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"StatusCode\": 0}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-a-container\">Remove a container</h3> <p><code>DELETE /containers/(id or name)</code></p> <p>Remove the container <code>id</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /containers/16253994b7c4?v=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>v</strong> – 1/True/true or 0/False/false, Remove the volumes associated to the container. Default <code>false</code>.</li> <li>\n<strong>force</strong> - 1/True/true or 0/False/false, Kill then remove the container. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"copy-files-or-folders-from-a-container\">Copy files or folders from a container</h3> <p><code>POST /containers/(id or name)/copy</code></p> <p>Copy files or folders of container <code>id</code></p> <p><strong>Deprecated</strong> in favor of the <code>archive</code> endpoint below.</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/4fa6e0f0c678/copy HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Resource\": \"test.txt\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"retrieving-information-about-files-and-folders-in-a-container\">Retrieving information about files and folders in a container</h3> <p><code>HEAD /containers/(id or name)/archive</code></p> <p>See the description of the <code>X-Docker-Container-Path-Stat</code> header in the following section.</p> <h3 id=\"get-an-archive-of-a-filesystem-resource-in-a-container\">Get an archive of a filesystem resource in a container</h3> <p><code>GET /containers/(id or name)/archive</code></p> <p>Get a tar archive of a resource in the filesystem of container <code>id</code>.</p> <p>Query Parameters:</p> <ul> <li>\n<p><strong>path</strong> - resource in the container’s filesystem to archive. Required.</p> <p>If not an absolute path, it is relative to the container’s root directory. The resource specified by <strong>path</strong> must exist. To assert that the resource is expected to be a directory, <strong>path</strong> should end in <code>/</code> or <code>/.</code> (assuming a path separator of <code>/</code>). If <strong>path</strong> ends in <code>/.</code> then this indicates that only the contents of the <strong>path</strong> directory should be copied. A symlink is always resolved to its target.</p> <p><strong>Note</strong>: It is not possible to copy certain system files such as resources under <code>/proc</code>, <code>/sys</code>, <code>/dev</code>, and mounts created by the user in the container.</p>\n</li> </ul> <p><strong>Example request</strong>:</p> <pre>    GET /containers/8cce319429b2/archive?path=/root HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/x-tar\n    X-Docker-Container-Path-Stat: eyJuYW1lIjoicm9vdCIsInNpemUiOjQwOTYsIm1vZGUiOjIxNDc0ODQwOTYsIm10aW1lIjoiMjAxNC0wMi0yN1QyMDo1MToyM1oiLCJsaW5rVGFyZ2V0IjoiIn0=\n\n    {{ TAR STREAM }}\n</pre> <p>On success, a response header <code>X-Docker-Container-Path-Stat</code> will be set to a base64-encoded JSON object containing some filesystem header information about the archived resource. The above example value would decode to the following JSON object (whitespace added for readability):</p> <pre>    {\n        \"name\": \"root\",\n        \"size\": 4096,\n        \"mode\": 2147484096,\n        \"mtime\": \"2014-02-27T20:51:23Z\",\n        \"linkTarget\": \"\"\n    }\n</pre> <p>A <code>HEAD</code> request can also be made to this endpoint if only this information is desired.</p> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - success, returns archive of copied resource</li> <li>\n<strong>400</strong> - client error, bad parameter, details in JSON response body, one of: <ul> <li>must specify path parameter (<strong>path</strong> cannot be empty)</li> <li>not a directory (<strong>path</strong> was asserted to be a directory but exists as a file)</li> </ul>\n</li> <li>\n<strong>404</strong> - client error, resource not found, one of: – no such container (container <code>id</code> does not exist) <ul> <li>no such file or directory (<strong>path</strong> does not exist)</li> </ul>\n</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"extract-an-archive-of-files-or-folders-to-a-directory-in-a-container\">Extract an archive of files or folders to a directory in a container</h3> <p><code>PUT /containers/(id or name)/archive</code></p> <p>Upload a tar archive to be extracted to a path in the filesystem of container <code>id</code>.</p> <p>Query Parameters:</p> <ul> <li>\n<p><strong>path</strong> - path to a directory in the container to extract the archive’s contents into. Required.</p> <p>If not an absolute path, it is relative to the container’s root directory. The <strong>path</strong> resource must exist.</p>\n</li> <li><p><strong>noOverwriteDirNonDir</strong> - If “1”, “true”, or “True” then it will be an error if unpacking the given content would cause an existing directory to be replaced with a non-directory and vice versa.</p></li> </ul> <p><strong>Example request</strong>:</p> <pre>PUT /containers/8cce319429b2/archive?path=/vol1 HTTP/1.1\nContent-Type: application/x-tar\n\n{{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – the content was extracted successfully</li> <li>\n<strong>400</strong> - client error, bad parameter, details in JSON response body, one of: <ul> <li>must specify path parameter (<strong>path</strong> cannot be empty)</li> <li>not a directory (<strong>path</strong> should be a directory but exists as a file)</li> <li>unable to overwrite existing directory with non-directory (if <strong>noOverwriteDirNonDir</strong>)</li> <li>unable to overwrite existing non-directory with directory (if <strong>noOverwriteDirNonDir</strong>)</li> </ul>\n</li> <li>\n<strong>403</strong> - client error, permission denied, the volume or container rootfs is marked as read-only.</li> <li>\n<strong>404</strong> - client error, resource not found, one of: – no such container (container <code>id</code> does not exist) <ul> <li>no such file or directory (<strong>path</strong> resource does not exist)</li> </ul>\n</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-2-images\">2.2 Images</h2> <h3 id=\"list-images\">List Images</h3> <p><code>GET /images/json</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/json?all=0 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.04\",\n       \"ubuntu:precise\",\n       \"ubuntu:latest\"\n     ],\n     \"Id\": \"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\n     \"Created\": 1365714795,\n     \"Size\": 131506275,\n     \"VirtualSize\": 131506275,\n     \"Labels\": {}\n  },\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.10\",\n       \"ubuntu:quantal\"\n     ],\n     \"ParentId\": \"27cf784147099545\",\n     \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n     \"Created\": 1364102658,\n     \"Size\": 24653,\n     \"VirtualSize\": 180116135,\n     \"Labels\": {\n        \"com.example.version\": \"v1\"\n     }\n  }\n]\n</pre> <p><strong>Example request, with digest information</strong>:</p> <pre>GET /images/json?digests=1 HTTP/1.1\n</pre> <p><strong>Example response, with digest information</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Created\": 1420064636,\n    \"Id\": \"4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125\",\n    \"ParentId\": \"ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2\",\n    \"RepoDigests\": [\n      \"localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n    ],\n    \"RepoTags\": [\n      \"localhost:5000/test/busybox:latest\",\n      \"playdate:latest\"\n    ],\n    \"Size\": 0,\n    \"VirtualSize\": 2429728,\n    \"Labels\": {}\n  }\n]\n</pre> <p>The response shows a single image <code>Id</code> associated with two repositories (<code>RepoTags</code>): <code>localhost:5000/test/busybox</code>: and <code>playdate</code>. A caller can use either of the <code>RepoTags</code> values <code>localhost:5000/test/busybox:latest</code> or <code>playdate:latest</code> to reference the image.</p> <p>You can also use <code>RepoDigests</code> values to reference an image. In this response, the array has only one reference and that is to the <code>localhost:5000/test/busybox</code> repository; the <code>playdate</code> repository has no digest. You can reference this digest using the value: <code>localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d...</code></p> <p>See the <code>docker run</code> and <code>docker build</code> commands for examples of digest and tag references on the command line.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>filters</strong> – a JSON encoded value of the filters (a map[string][]string) to process on the images list. Available filters: <ul> <li><code>dangling=true</code></li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of an image label</li> </ul>\n</li> <li>\n<strong>filter</strong> - only return images with the specified name</li> </ul> <h3 id=\"build-image-from-a-dockerfile\">Build image from a Dockerfile</h3> <p><code>POST /build</code></p> <p>Build an image from a Dockerfile</p> <p><strong>Example request</strong>:</p> <pre>POST /build HTTP/1.1\n\n{{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"stream\": \"Step 1...\"}\n{\"stream\": \"...\"}\n{\"error\": \"Error...\", \"errorDetail\": {\"code\": 123, \"message\": \"Error...\"}}\n</pre> <p>The input stream must be a <code>tar</code> archive compressed with one of the following algorithms: <code>identity</code> (no compression), <code>gzip</code>, <code>bzip2</code>, <code>xz</code>.</p> <p>The archive must include a build instructions file, typically called <code>Dockerfile</code> at the archive’s root. The <code>dockerfile</code> parameter may be used to specify a different build instructions file. To do this, its value must be the path to the alternate build instructions file to use.</p> <p>The archive may include any number of other files, which are accessible in the build context (See the <a href=\"../../builder/index#dockerbuilder\"><em>ADD build command</em></a>).</p> <p>The build is canceled if the client drops the connection by quitting or being killed.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>dockerfile</strong> - Path within the build context to the Dockerfile. This is ignored if <code>remote</code> is specified and points to an individual filename.</li> <li>\n<strong>t</strong> – A name and optional tag to apply to the image in the <code>name:tag</code> format. If you omit the <code>tag</code> the default <code>latest</code> value is assumed. You can provide one or more <code>t</code> parameters.</li> <li>\n<strong>remote</strong> – A Git repository URI or HTTP/HTTPS URI build source. If the URI specifies a filename, the file’s contents are placed into a file called <code>Dockerfile</code>.</li> <li>\n<strong>q</strong> – Suppress verbose build output.</li> <li>\n<strong>nocache</strong> – Do not use the cache when building the image.</li> <li>\n<strong>pull</strong> - Attempt to pull the image even if an older image exists locally.</li> <li>\n<strong>rm</strong> - Remove intermediate containers after a successful build (default behavior).</li> <li>\n<strong>forcerm</strong> - Always remove intermediate containers (includes <code>rm</code>).</li> <li>\n<strong>memory</strong> - Set memory limit for build.</li> <li>\n<strong>memswap</strong> - Total memory (memory + swap), <code>-1</code> to enable unlimited swap.</li> <li>\n<strong>cpushares</strong> - CPU shares (relative weight).</li> <li>\n<strong>cpusetcpus</strong> - CPUs in which to allow execution (e.g., <code>0-3</code>, <code>0,1</code>).</li> <li>\n<strong>cpuperiod</strong> - The length of a CPU period in microseconds.</li> <li>\n<strong>cpuquota</strong> - Microseconds of CPU time that the container can get in a CPU period.</li> <li>\n<strong>buildargs</strong> – JSON map of string pairs for build-time variables. Users pass these values at build-time. Docker uses the <code>buildargs</code> as the environment context for command(s) run via the Dockerfile’s <code>RUN</code> instruction or for variable expansion in other Dockerfile instructions. This is not meant for passing secret values. <a href=\"../../builder/index#arg\">Read more about the buildargs instruction</a>\n</li> <li>\n<p><strong>shmsize</strong> - Size of <code>/dev/shm</code> in bytes. The size must be greater than 0. If omitted the system uses 64MB.</p> <p>Request Headers:</p>\n</li> <li><p><strong>Content-type</strong> – Set to <code>\"application/tar\"</code>.</p></li> <li>\n<p><strong>X-Registry-Config</strong> – A base64-url-safe-encoded Registry Auth Config JSON object with the following structure:</p> <pre>    {\n        \"docker.example.com\": {\n            \"username\": \"janedoe\",\n            \"password\": \"hunter2\"\n        },\n        \"https://index.docker.io/v1/\": {\n            \"username\": \"mobydock\",\n            \"password\": \"conta1n3rize14\"\n        }\n    }\n</pre> <p>This object maps the hostname of a registry to an object containing the “username” and “password” for that registry. Multiple registries may be specified as the build may be based on an image requiring authentication to pull from any arbitrary registry. Only the registry domain name (and port if not the default “443”) are required. However (for legacy reasons) the “official” Docker, Inc. hosted registry must be specified with both a “https://” prefix and a “/v1/” suffix even though Docker will prefer to use the v2 registry API.</p>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-an-image\">Create an image</h3> <p><code>POST /images/create</code></p> <p>Create an image either by pulling it from the registry or by importing it</p> <p><strong>Example request</strong>:</p> <pre>POST /images/create?fromImage=ubuntu HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pulling...\"}\n{\"status\": \"Pulling\", \"progress\": \"1 B/ 100 B\", \"progressDetail\": {\"current\": 1, \"total\": 100}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>When using this endpoint to pull an image from the registry, the <code>X-Registry-Auth</code> header can be used to include a base64-encoded AuthConfig object.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>fromImage</strong> – Name of the image to pull. The name may include a tag or digest. This parameter may only be used when pulling an image. The pull is cancelled if the HTTP connection is closed.</li> <li>\n<strong>fromSrc</strong> – Source to import. The value may be a URL from which the image can be retrieved or <code>-</code> to read the image from the request body. This parameter may only be used when importing an image.</li> <li>\n<strong>repo</strong> – Repository name given to an image when it is imported. The repo may include a tag. This parameter may only be used when importing an image.</li> <li>\n<p><strong>tag</strong> – Tag or digest.</p> <p>Request Headers:</p>\n</li> <li>\n<p><strong>X-Registry-Auth</strong> – base64-encoded AuthConfig object, containing either login information, or a token</p> <ul> <li>\n<p>Credential based login:</p> <pre>{\n    \"username\": \"jdoe\",\n    \"password\": \"secret\",\n    \"email\": \"jdoe@acme.com\",\n}\n</pre>\n</li> <li>\n<p>Token based login:</p> <pre>{\n    \"registrytoken\": \"9cbaf023786cd7...\"\n}\n</pre>\n</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-an-image\">Inspect an image</h3> <p><code>GET /images/(name)/json</code></p> <p>Return low-level information on the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/example/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Id\" : \"85f05633ddc1c50679be2b16a0479ab6f7637f8884e0cfe0f4d20e1ebb3d6e7c\",\n   \"Container\" : \"cb91e48a60d01f1e27028b4fc6819f4f290b3cf12496c8176ec714d0d390984a\",\n   \"Comment\" : \"\",\n   \"Os\" : \"linux\",\n   \"Architecture\" : \"amd64\",\n   \"Parent\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n   \"ContainerConfig\" : {\n      \"Tty\" : false,\n      \"Hostname\" : \"e611e15f9c9d\",\n      \"Volumes\" : null,\n      \"Domainname\" : \"\",\n      \"AttachStdout\" : false,\n      \"PublishService\" : \"\",\n      \"AttachStdin\" : false,\n      \"OpenStdin\" : false,\n      \"StdinOnce\" : false,\n      \"NetworkDisabled\" : false,\n      \"OnBuild\" : [],\n      \"Image\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n      \"User\" : \"\",\n      \"WorkingDir\" : \"\",\n      \"Entrypoint\" : null,\n      \"MacAddress\" : \"\",\n      \"AttachStderr\" : false,\n      \"Labels\" : {\n         \"com.example.license\" : \"GPL\",\n         \"com.example.version\" : \"1.0\",\n         \"com.example.vendor\" : \"Acme\"\n      },\n      \"Env\" : [\n         \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n      ],\n      \"ExposedPorts\" : null,\n      \"Cmd\" : [\n         \"/bin/sh\",\n         \"-c\",\n         \"#(nop) LABEL com.example.vendor=Acme com.example.license=GPL com.example.version=1.0\"\n      ]\n   },\n   \"DockerVersion\" : \"1.9.0-dev\",\n   \"VirtualSize\" : 188359297,\n   \"Size\" : 0,\n   \"Author\" : \"\",\n   \"Created\" : \"2015-09-10T08:30:53.26995814Z\",\n   \"GraphDriver\" : {\n      \"Name\" : \"aufs\",\n      \"Data\" : null\n   },\n   \"RepoDigests\" : [\n      \"localhost:5000/test/busybox/example@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n   ],\n   \"RepoTags\" : [\n      \"example:1.0\",\n      \"example:latest\",\n      \"example:stable\"\n   ],\n   \"Config\" : {\n      \"Image\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n      \"NetworkDisabled\" : false,\n      \"OnBuild\" : [],\n      \"StdinOnce\" : false,\n      \"PublishService\" : \"\",\n      \"AttachStdin\" : false,\n      \"OpenStdin\" : false,\n      \"Domainname\" : \"\",\n      \"AttachStdout\" : false,\n      \"Tty\" : false,\n      \"Hostname\" : \"e611e15f9c9d\",\n      \"Volumes\" : null,\n      \"Cmd\" : [\n         \"/bin/bash\"\n      ],\n      \"ExposedPorts\" : null,\n      \"Env\" : [\n         \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n      ],\n      \"Labels\" : {\n         \"com.example.vendor\" : \"Acme\",\n         \"com.example.version\" : \"1.0\",\n         \"com.example.license\" : \"GPL\"\n      },\n      \"Entrypoint\" : null,\n      \"MacAddress\" : \"\",\n      \"AttachStderr\" : false,\n      \"WorkingDir\" : \"\",\n      \"User\" : \"\"\n   }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-the-history-of-an-image\">Get the history of an image</h3> <p><code>GET /images/(name)/history</code></p> <p>Return the history of the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/ubuntu/history HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n    {\n        \"Id\": \"3db9c44f45209632d6050b35958829c3a2aa256d81b9a7be45b362ff85c54710\",\n        \"Created\": 1398108230,\n        \"CreatedBy\": \"/bin/sh -c #(nop) ADD file:eb15dbd63394e063b805a3c32ca7bf0266ef64676d5a6fab4801f2e81e2a5148 in /\",\n        \"Tags\": [\n            \"ubuntu:lucid\",\n            \"ubuntu:10.04\"\n        ],\n        \"Size\": 182964289,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"6cfa4d1f33fb861d4d114f43b25abd0ac737509268065cdfd69d544a59c85ab8\",\n        \"Created\": 1398108222,\n        \"CreatedBy\": \"/bin/sh -c #(nop) MAINTAINER Tianon Gravi &lt;admwiggin@gmail.com&gt; - mkimage-debootstrap.sh -i iproute,iputils-ping,ubuntu-minimal -t lucid.tar.xz lucid http://archive.ubuntu.com/ubuntu/\",\n        \"Tags\": null,\n        \"Size\": 0,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\n        \"Created\": 1371157430,\n        \"CreatedBy\": \"\",\n        \"Tags\": [\n            \"scratch12:latest\",\n            \"scratch:latest\"\n        ],\n        \"Size\": 0,\n        \"Comment\": \"Imported from -\"\n    }\n]\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"push-an-image-on-the-registry\">Push an image on the registry</h3> <p><code>POST /images/(name)/push</code></p> <p>Push the image <code>name</code> on the registry</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/push HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pushing...\"}\n{\"status\": \"Pushing\", \"progress\": \"1/? (n/a)\", \"progressDetail\": {\"current\": 1}}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>If you wish to push an image on to a private registry, that image must already have a tag into a repository which references that registry <code>hostname</code> and <code>port</code>. This repository name should then be used in the URL. This duplicates the command line’s flow.</p> <p>The push is cancelled if the HTTP connection is closed.</p> <p><strong>Example request</strong>:</p> <pre>POST /images/registry.acme.com:5000/test/push HTTP/1.1\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>tag</strong> – The tag to associate with the image on the registry. This is optional.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<p><strong>X-Registry-Auth</strong> – base64-encoded AuthConfig object, containing either login information, or a token</p> <ul> <li>\n<p>Credential based login:</p> <pre>{\n    \"username\": \"jdoe\",\n    \"password\": \"secret\",\n    \"email\": \"jdoe@acme.com\",\n}\n</pre>\n</li> <li>\n<p>Token based login:</p> <pre>{\n    \"registrytoken\": \"9cbaf023786cd7...\"\n}\n</pre>\n</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"tag-an-image-into-a-repository\">Tag an image into a repository</h3> <p><code>POST /images/(name)/tag</code></p> <p>Tag the image <code>name</code> into a repository</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/tag?repo=myrepo&amp;force=0&amp;tag=v42 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>repo</strong> – The repository to tag in</li> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>tag</strong> - The new tag name</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-an-image\">Remove an image</h3> <p><code>DELETE /images/(name)</code></p> <p>Remove the image <code>name</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /images/test HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-type: application/json\n\n[\n {\"Untagged\": \"3e2f21a89f\"},\n {\"Deleted\": \"3e2f21a89f\"},\n {\"Deleted\": \"53b4f83ac9\"}\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>noprune</strong> – 1/True/true or 0/False/false, default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"search-images\">Search images</h3> <p><code>GET /images/search</code></p> <p>Search for an image on <a href=\"https://hub.docker.com\">Docker Hub</a>.</p> <blockquote> <p><strong>Note</strong>: The response keys have changed from API v1.6 to reflect the JSON sent by the registry server to the docker daemon’s request.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>GET /images/search?term=sshd HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"wma55/u1210sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"jdswinbank/sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"vgauthier/sshd\",\n            \"star_count\": 0\n        }\n...\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>term</strong> – term to search</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-3-misc\">2.3 Misc</h2> <h3 id=\"check-auth-configuration\">Check auth configuration</h3> <p><code>POST /auth</code></p> <p>Get the default username and email</p> <p><strong>Example request</strong>:</p> <pre>POST /auth HTTP/1.1\nContent-Type: application/json\n\n{\n     \"username\":\" hannibal\",\n     \"password: \"xxxx\",\n     \"email\": \"hannibal@a-team.com\",\n     \"serveraddress\": \"https://index.docker.io/v1/\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"display-system-wide-information\">Display system-wide information</h3> <p><code>GET /info</code></p> <p>Display system-wide information</p> <p><strong>Example request</strong>:</p> <pre>GET /info HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"Architecture\": \"x86_64\",\n    \"ClusterStore\": \"etcd://localhost:2379\",\n    \"Containers\": 11,\n    \"ContainersRunning\": 7,\n    \"ContainersStopped\": 3,\n    \"ContainersPaused\": 1,\n    \"CpuCfsPeriod\": true,\n    \"CpuCfsQuota\": true,\n    \"Debug\": false,\n    \"DockerRootDir\": \"/var/lib/docker\",\n    \"Driver\": \"btrfs\",\n    \"DriverStatus\": [[\"\"]],\n    \"ExecutionDriver\": \"native-0.1\",\n    \"ExperimentalBuild\": false,\n    \"HttpProxy\": \"http://test:test@localhost:8080\",\n    \"HttpsProxy\": \"https://test:test@localhost:8080\",\n    \"ID\": \"7TRN:IPZB:QYBB:VPBQ:UMPP:KARE:6ZNR:XE6T:7EWV:PKF4:ZOJD:TPYS\",\n    \"IPv4Forwarding\": true,\n    \"Images\": 16,\n    \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n    \"InitPath\": \"/usr/bin/docker\",\n    \"InitSha1\": \"\",\n    \"KernelVersion\": \"3.12.0-1-amd64\",\n    \"Labels\": [\n        \"storage=ssd\"\n    ],\n    \"MemTotal\": 2099236864,\n    \"MemoryLimit\": true,\n    \"NCPU\": 1,\n    \"NEventsListener\": 0,\n    \"NFd\": 11,\n    \"NGoroutines\": 21,\n    \"Name\": \"prod-server-42\",\n    \"NoProxy\": \"9.81.1.160\",\n    \"OomKillDisable\": true,\n    \"OSType\": \"linux\",\n    \"OperatingSystem\": \"Boot2Docker\",\n    \"Plugins\": {\n        \"Volume\": [\n            \"local\"\n        ],\n        \"Network\": [\n            \"null\",\n            \"host\",\n            \"bridge\"\n        ]\n    },\n    \"RegistryConfig\": {\n        \"IndexConfigs\": {\n            \"docker.io\": {\n                \"Mirrors\": null,\n                \"Name\": \"docker.io\",\n                \"Official\": true,\n                \"Secure\": true\n            }\n        },\n        \"InsecureRegistryCIDRs\": [\n            \"127.0.0.0/8\"\n        ]\n    },\n    \"ServerVersion\": \"1.9.0\",\n    \"SwapLimit\": false,\n    \"SystemStatus\": [[\"State\", \"Healthy\"]],\n    \"SystemTime\": \"2015-03-10T11:11:23.730591467-07:00\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"show-the-docker-version-information\">Show the docker version information</h3> <p><code>GET /version</code></p> <p>Show the docker version information</p> <p><strong>Example request</strong>:</p> <pre>GET /version HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n     \"Version\": \"1.10.0-dev\",\n     \"Os\": \"linux\",\n     \"KernelVersion\": \"3.19.0-23-generic\",\n     \"GoVersion\": \"go1.4.2\",\n     \"GitCommit\": \"e75da4b\",\n     \"Arch\": \"amd64\",\n     \"ApiVersion\": \"1.22\",\n     \"BuildTime\": \"2015-12-01T07:09:13.444803460+00:00\",\n     \"Experimental\": true\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"ping-the-docker-server\">Ping the docker server</h3> <p><code>GET /_ping</code></p> <p>Ping the docker server</p> <p><strong>Example request</strong>:</p> <pre>GET /_ping HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: text/plain\n\nOK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-new-image-from-a-container-s-changes\">Create a new image from a container’s changes</h3> <p><code>POST /commit</code></p> <p>Create a new image from a container’s changes</p> <p><strong>Example request</strong>:</p> <pre>POST /commit?container=44c004db4b17&amp;comment=message&amp;repo=myrepo HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Hostname\": \"\",\n     \"Domainname\": \"\",\n     \"User\": \"\",\n     \"AttachStdin\": false,\n     \"AttachStdout\": true,\n     \"AttachStderr\": true,\n     \"Tty\": false,\n     \"OpenStdin\": false,\n     \"StdinOnce\": false,\n     \"Env\": null,\n     \"Cmd\": [\n             \"date\"\n     ],\n     \"Mounts\": [\n       {\n         \"Source\": \"/data\",\n         \"Destination\": \"/data\",\n         \"Mode\": \"ro,Z\",\n         \"RW\": false\n       }\n     ],\n     \"Labels\": {\n             \"key1\": \"value1\",\n             \"key2\": \"value2\"\n      },\n     \"WorkingDir\": \"\",\n     \"NetworkDisabled\": false,\n     \"ExposedPorts\": {\n             \"22/tcp\": {}\n     }\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\"Id\": \"596069db4bf5\"}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>config</strong> - the container’s configuration</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>container</strong> – source container</li> <li>\n<strong>repo</strong> – repository</li> <li>\n<strong>tag</strong> – tag</li> <li>\n<strong>comment</strong> – commit message</li> <li>\n<strong>author</strong> – author (e.g., “John Hannibal Smith &lt;<a href=\"mailto:hannibal%40a-team.com\">hannibal@a-team.com</a>&gt;“)</li> <li>\n<strong>pause</strong> – 1/True/true or 0/False/false, whether to pause the container before committing</li> <li>\n<strong>changes</strong> – Dockerfile instructions to apply while committing</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"monitor-docker-s-events\">Monitor Docker’s events</h3> <p><code>GET /events</code></p> <p>Get container events from docker, either in real time via streaming, or via polling (using since).</p> <p>Docker containers report the following events:</p> <pre>attach, commit, copy, create, destroy, die, exec_create, exec_start, export, kill, oom, pause, rename, resize, restart, start, stop, top, unpause, update\n</pre> <p>Docker images report the following events:</p> <pre>delete, import, pull, push, tag, untag\n</pre> <p>Docker volumes report the following events:</p> <pre>create, mount, unmount, destroy\n</pre> <p>Docker networks report the following events:</p> <pre>create, connect, disconnect, destroy\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /events?since=1374067924\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\nServer: Docker/1.10.0 (linux)\nDate: Fri, 29 Apr 2016 15:18:06 GMT\nTransfer-Encoding: chunked\n\n{\n  \"status\": \"pull\",\n  \"id\": \"alpine:latest\",\n  \"Type\": \"image\",\n  \"Action\": \"pull\",\n  \"Actor\": {\n    \"ID\": \"alpine:latest\",\n    \"Attributes\": {\n      \"name\": \"alpine\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101301854122\n}\n{\n  \"status\": \"create\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"create\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101381709551\n}\n{\n  \"status\": \"attach\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"attach\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101383858412\n}\n{\n  \"Type\": \"network\",\n  \"Action\": \"connect\",\n  \"Actor\": {\n    \"ID\": \"7dc8ac97d5d29ef6c31b6052f3938c1e8f2749abbd17d1bd1febf2608db1b474\",\n    \"Attributes\": {\n      \"container\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n      \"name\": \"bridge\",\n      \"type\": \"bridge\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101394865557\n}\n{\n  \"status\": \"start\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"start\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101607533796\n}\n{\n  \"status\": \"resize\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"resize\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"height\": \"46\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\",\n      \"width\": \"204\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101610269268\n}\n{\n  \"status\": \"die\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"die\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"exitCode\": \"0\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105079144137\n}\n{\n  \"Type\": \"network\",\n  \"Action\": \"disconnect\",\n  \"Actor\": {\n    \"ID\": \"7dc8ac97d5d29ef6c31b6052f3938c1e8f2749abbd17d1bd1febf2608db1b474\",\n    \"Attributes\": {\n      \"container\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n      \"name\": \"bridge\",\n      \"type\": \"bridge\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105230860245\n}\n{\n  \"status\": \"destroy\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"destroy\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105338056026\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>since</strong> – Timestamp used for polling</li> <li>\n<strong>until</strong> – Timestamp used for polling</li> <li>\n<strong>filters</strong> – A json encoded value of the filters (a map[string][]string) to process on the event list. Available filters: <ul> <li>\n<code>container=&lt;string&gt;</code>; -- container to filter</li> <li>\n<code>event=&lt;string&gt;</code>; -- event to filter</li> <li>\n<code>image=&lt;string&gt;</code>; -- image to filter</li> <li>\n<code>label=&lt;string&gt;</code>; -- image and container label to filter</li> <li>\n<code>type=&lt;string&gt;</code>; -- either <code>container</code> or <code>image</code> or <code>volume</code> or <code>network</code>\n</li> <li>\n<code>volume=&lt;string&gt;</code>; -- volume to filter</li> <li>\n<code>network=&lt;string&gt;</code>; -- network to filter</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images-in-a-repository\">Get a tarball containing all images in a repository</h3> <p><code>GET /images/(name)/get</code></p> <p>Get a tarball containing all images and metadata for the repository specified by <code>name</code>.</p> <p>If <code>name</code> is a specific name and tag (e.g. ubuntu:latest), then only that image (and its parents) are returned. If <code>name</code> is an image ID, similarly only that image (and its parents) are returned, but with the exclusion of the ‘repositories’ file in the tarball, as there were no image names referenced.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/ubuntu/get\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images\">Get a tarball containing all images.</h3> <p><code>GET /images/get</code></p> <p>Get a tarball containing all images and metadata for one or more repositories.</p> <p>For each value of the <code>names</code> parameter: if it is a specific name and tag (e.g. <code>ubuntu:latest</code>), then only that image (and its parents) are returned; if it is an image ID, similarly only that image (and its parents) are returned and there would be no names referenced in the ‘repositories’ file for this image ID.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/get?names=myname%2Fmyapp%3Alatest&amp;names=busybox\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"load-a-tarball-with-a-set-of-images-and-tags-into-docker\">Load a tarball with a set of images and tags into docker</h3> <p><code>POST /images/load</code></p> <p>Load a set of images and tags into a Docker repository. See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>POST /images/load\n\nTarball in body\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"image-tarball-format\">Image tarball format</h3> <p>An image tarball contains one directory per image layer (named using its long ID), each containing these files:</p> <ul> <li>\n<code>VERSION</code>: currently <code>1.0</code> - the file format version</li> <li>\n<code>json</code>: detailed layer information, similar to <code>docker inspect layer_id</code>\n</li> <li>\n<code>layer.tar</code>: A tarfile containing the filesystem changes in this layer</li> </ul> <p>The <code>layer.tar</code> file contains <code>aufs</code> style <code>.wh..wh.aufs</code> files and directories for storing attribute changes and deletions.</p> <p>If the tarball defines a repository, the tarball should also include a <code>repositories</code> file at the root that contains a list of repository and tag names mapped to layer IDs.</p> <pre>{\"hello-world\":\n    {\"latest\": \"565a9d68a73f6706862bfe8409a7f659776d4d60a8d096eb4a3cbce6999cc2a1\"}\n}\n</pre> <h3 id=\"exec-create\">Exec Create</h3> <p><code>POST /containers/(id or name)/exec</code></p> <p>Sets up an exec instance in a running container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/exec HTTP/1.1\nContent-Type: application/json\n\n  {\n   \"AttachStdin\": false,\n   \"AttachStdout\": true,\n   \"AttachStderr\": true,\n   \"DetachKeys\": \"ctrl-p,ctrl-q\",\n   \"Tty\": false,\n   \"Cmd\": [\n                 \"date\"\n         ]\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n     \"Id\": \"f90e34656806\",\n     \"Warnings\":[]\n}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code> of the <code>exec</code> command.</li> <li>\n<strong>DetachKeys</strong> – Override the key sequence for detaching a container. Format is a single character <code>[a-Z]</code> or <code>ctrl-&lt;value&gt;</code> where <code>&lt;value&gt;</code> is one of: <code>a-z</code>, <code>@</code>, <code>^</code>, <code>[</code>, <code>,</code> or <code>_</code>.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - container is paused</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"exec-start\">Exec Start</h3> <p><code>POST /exec/(id)/start</code></p> <p>Starts a previously set up <code>exec</code> instance <code>id</code>. If <code>detach</code> is true, this API returns after starting the <code>exec</code> command. Otherwise, this API sets up an interactive session with the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/start HTTP/1.1\nContent-Type: application/json\n\n{\n \"Detach\": false,\n \"Tty\": false\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/vnd.docker.raw-stream\n\n{{ STREAM }}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Detach</strong> - Detach from the <code>exec</code> command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<p><strong>409</strong> - container is paused</p> <p><strong>Stream details</strong>: Similar to the stream behavior of <code>POST /containers/(id or name)/attach</code> API</p>\n</li> </ul> <h3 id=\"exec-resize\">Exec Resize</h3> <p><code>POST /exec/(id)/resize</code></p> <p>Resizes the <code>tty</code> session used by the <code>exec</code> command <code>id</code>. The unit is number of characters. This API is valid only if <code>tty</code> was specified as part of creating and starting the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/resize?h=40&amp;w=80 HTTP/1.1\nContent-Type: text/plain\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: text/plain\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of <code>tty</code> session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> </ul> <h3 id=\"exec-inspect\">Exec Inspect</h3> <p><code>GET /exec/(id)/json</code></p> <p>Return low-level information about the <code>exec</code> command <code>id</code>.</p> <p><strong>Example request</strong>:</p> <pre>GET /exec/11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"CanRemove\": false,\n    \"ContainerID\": \"b53ee82b53a40c7dca428523e34f741f3abc51d9f297a14ff874bf761b995126\",\n    \"DetachKeys\": \"\",\n    \"ExitCode\": 2,\n    \"ID\": \"f33bbfb39f5b142420f4759b2348913bd4a8d1a6d7fd56499cb41a1bb91d7b3b\",\n    \"OpenStderr\": true,\n    \"OpenStdin\": true,\n    \"OpenStdout\": true,\n    \"ProcessConfig\": {\n        \"arguments\": [\n            \"-c\",\n            \"exit 2\"\n        ],\n        \"entrypoint\": \"sh\",\n        \"privileged\": false,\n        \"tty\": true,\n        \"user\": \"1000\"\n    },\n    \"Running\": false\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<strong>500</strong> - server error</li> </ul> <h2 id=\"2-4-volumes\">2.4 Volumes</h2> <h3 id=\"list-volumes\">List volumes</h3> <p><code>GET /volumes</code></p> <p><strong>Example request</strong>:</p> <pre>GET /volumes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Volumes\": [\n    {\n      \"Name\": \"tardis\",\n      \"Driver\": \"local\",\n      \"Mountpoint\": \"/var/lib/docker/volumes/tardis\"\n    }\n  ],\n  \"Warnings\": []\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>filters</strong> - JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the volumes list. There is one available filter: <code>dangling=true</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-volume\">Create a volume</h3> <p><code>POST /volumes/create</code></p> <p>Create a volume</p> <p><strong>Example request</strong>:</p> <pre>POST /volumes/create HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\",\n  \"Driver\": \"local\",\n  \"Mountpoint\": \"/var/lib/docker/volumes/tardis\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Name</strong> - The new volume’s name. If not specified, Docker generates a name.</li> <li>\n<strong>Driver</strong> - Name of the volume driver to use. Defaults to <code>local</code> for the name.</li> <li>\n<strong>DriverOpts</strong> - A mapping of driver options and values. These options are passed directly to the driver and are driver specific.</li> </ul> <h3 id=\"inspect-a-volume\">Inspect a volume</h3> <p><code>GET /volumes/(name)</code></p> <p>Return low-level information on the volume <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /volumes/tardis\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\",\n  \"Driver\": \"local\",\n  \"Mountpoint\": \"/var/lib/docker/volumes/tardis\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - no such volume</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"remove-a-volume\">Remove a volume</h3> <p><code>DELETE /volumes/(name)</code></p> <p>Instruct the driver to remove the volume (<code>name</code>).</p> <p><strong>Example request</strong>:</p> <pre>DELETE /volumes/tardis HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes</p> <ul> <li>\n<strong>204</strong> - no error</li> <li>\n<strong>404</strong> - no such volume or volume driver</li> <li>\n<strong>409</strong> - volume is in use and cannot be removed</li> <li>\n<strong>500</strong> - server error</li> </ul> <h2 id=\"2-5-networks\">2.5 Networks</h2> <h3 id=\"list-networks\">List networks</h3> <p><code>GET /networks</code></p> <p><strong>Example request</strong>:</p> <pre>GET /networks?filters={\"type\":{\"custom\":true}} HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Name\": \"bridge\",\n    \"Id\": \"f2de39df4171b0dc801e8002d1d999b77256983dfc63041c0f34030aa3977566\",\n    \"Scope\": \"local\",\n    \"Driver\": \"bridge\",\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": [\n        {\n          \"Subnet\": \"172.17.0.0/16\"\n        }\n      ]\n    },\n    \"Containers\": {\n      \"39b69226f9d79f5634485fb236a23b2fe4e96a0a94128390a7fbbcc167065867\": {\n        \"EndpointID\": \"ed2419a97c1d9954d05b46e462e7002ea552f216e9b136b80a7db8d98b442eda\",\n        \"MacAddress\": \"02:42:ac:11:00:02\",\n        \"IPv4Address\": \"172.17.0.2/16\",\n        \"IPv6Address\": \"\"\n      }\n    },\n    \"Options\": {\n      \"com.docker.network.bridge.default_bridge\": \"true\",\n      \"com.docker.network.bridge.enable_icc\": \"true\",\n      \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n      \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n      \"com.docker.network.bridge.name\": \"docker0\",\n      \"com.docker.network.driver.mtu\": \"1500\"\n    }\n  },\n  {\n    \"Name\": \"none\",\n    \"Id\": \"e086a3893b05ab69242d3c44e49483a3bbbd3a26b46baa8f61ab797c1088d794\",\n    \"Scope\": \"local\",\n    \"Driver\": \"null\",\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": []\n    },\n    \"Containers\": {},\n    \"Options\": {}\n  },\n  {\n    \"Name\": \"host\",\n    \"Id\": \"13e871235c677f196c4e1ecebb9dc733b9b2d2ab589e30c539efeda84a24215e\",\n    \"Scope\": \"local\",\n    \"Driver\": \"host\",\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": []\n    },\n    \"Containers\": {},\n    \"Options\": {}\n  }\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>filters</strong> - JSON encoded network list filter. The filter value is one of: <ul> <li>\n<code>name=&lt;network-name&gt;</code> Matches all or part of a network name.</li> <li>\n<code>id=&lt;network-id&gt;</code> Matches all or part of a network id.</li> <li>\n<code>type=[\"custom\"|\"builtin\"]</code> Filters networks by type. The <code>custom</code> keyword returns all user-defined networks.</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"inspect-network\">Inspect network</h3> <p><code>GET /networks/&lt;network-id&gt;</code></p> <p><strong>Example request</strong>:</p> <pre>GET /networks/7d86d31b1478e7cca9ebed7e73aa0fdeec46c5ca29497431d3007d2d9e15ed99 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Name\": \"net01\",\n  \"Id\": \"7d86d31b1478e7cca9ebed7e73aa0fdeec46c5ca29497431d3007d2d9e15ed99\",\n  \"Scope\": \"local\",\n  \"Driver\": \"bridge\",\n  \"IPAM\": {\n    \"Driver\": \"default\",\n    \"Config\": [\n      {\n        \"Subnet\": \"172.19.0.0/16\",\n        \"Gateway\": \"172.19.0.1/16\"\n      }\n    ],\n    \"Options\": {\n        \"foo\": \"bar\"\n    }\n  },\n  \"Containers\": {\n    \"19a4d5d687db25203351ed79d478946f861258f018fe384f229f2efa4b23513c\": {\n      \"Name\": \"test\",\n      \"EndpointID\": \"628cadb8bcb92de107b2a1e516cbffe463e321f548feb37697cce00ad694f21a\",\n      \"MacAddress\": \"02:42:ac:13:00:02\",\n      \"IPv4Address\": \"172.19.0.2/16\",\n      \"IPv6Address\": \"\"\n    }\n  },\n  \"Options\": {\n    \"com.docker.network.bridge.default_bridge\": \"true\",\n    \"com.docker.network.bridge.enable_icc\": \"true\",\n    \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n    \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n    \"com.docker.network.bridge.name\": \"docker0\",\n    \"com.docker.network.driver.mtu\": \"1500\"\n  }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network not found</li> </ul> <h3 id=\"create-a-network\">Create a network</h3> <p><code>POST /networks/create</code></p> <p>Create a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/create HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Name\":\"isolated_nw\",\n  \"Driver\":\"bridge\",\n  \"IPAM\":{\n    \"Config\":[\n       {\n          \"Subnet\":\"172.20.0.0/16\",\n          \"IPRange\":\"172.20.10.0/24\",\n          \"Gateway\":\"172.20.10.11\"\n        },\n        {\n          \"Subnet\":\"2001:db8:abcd::/64\",\n          \"Gateway\":\"2001:db8:abcd::1011\"\n        }\n    ],\n    \"Options\": {\n        \"foo\": \"bar\"\n    }\n  },\n  \"Internal\":true\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n  \"Id\": \"22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30\",\n  \"Warning\": \"\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> - no error</li> <li>\n<strong>404</strong> - plugin not found</li> <li>\n<strong>500</strong> - server error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Name</strong> - The new network’s name. this is a mandatory field</li> <li>\n<strong>Driver</strong> - Name of the network driver plugin to use. Defaults to <code>bridge</code> driver</li> <li>\n<strong>IPAM</strong> - Optional custom IP scheme for the network</li> <li>\n<strong>Options</strong> - Network specific options to be used by the drivers</li> <li>\n<strong>CheckDuplicate</strong> - Requests daemon to check for networks with same name</li> </ul> <h3 id=\"connect-a-container-to-a-network\">Connect a container to a network</h3> <p><code>POST /networks/(id)/connect</code></p> <p>Connect a container to a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30/connect HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Container\":\"3613f73ba0e4\",\n  \"EndpointConfig\": {\n    \"IPAMConfig\": {\n        \"IPv4Address\":\"172.24.56.89\",\n        \"IPv6Address\":\"2001:db8::5689\"\n    }\n  }\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network or container is not found</li> <li>\n<strong>500</strong> - Internal Server Error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>container</strong> - container-id/name to be connected to the network</li> </ul> <h3 id=\"disconnect-a-container-from-a-network\">Disconnect a container from a network</h3> <p><code>POST /networks/(id)/disconnect</code></p> <p>Disconnect a container from a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30/disconnect HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Container\":\"3613f73ba0e4\",\n  \"Force\":false\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network or container not found</li> <li>\n<strong>500</strong> - Internal Server Error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Container</strong> - container-id/name to be disconnected from a network</li> <li>\n<strong>Force</strong> - Force the container to disconnect from a network</li> </ul> <h3 id=\"remove-a-network\">Remove a network</h3> <p><code>DELETE /networks/(id)</code></p> <p>Instruct the driver to remove the network (<code>id</code>).</p> <p><strong>Example request</strong>:</p> <pre>DELETE /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - no such network</li> <li>\n<strong>500</strong> - server error</li> </ul> <h1 id=\"3-going-further\">3. Going further</h1> <h2 id=\"3-1-inside-docker-run\">3.1 Inside <code>docker run</code>\n</h2> <p>As an example, the <code>docker run</code> command line makes the following API calls:</p> <ul> <li><p>Create the container</p></li> <li>\n<p>If the status code is 404, it means the image doesn’t exist:</p> <ul> <li>Try to pull it.</li> <li>Then, retry to create the container.</li> </ul>\n</li> <li><p>Start the container.</p></li> <li><p>If you are not in detached mode:</p></li> <li><p>Attach to the container, using <code>logs=1</code> (to have <code>stdout</code> and <code>stderr</code> from the container’s start) and <code>stream=1</code></p></li> <li><p>If in detached mode or only <code>stdin</code> is attached, display the container’s id.</p></li> </ul> <h2 id=\"3-2-hijacking\">3.2 Hijacking</h2> <p>In this version of the API, <code>/attach</code>, uses hijacking to transport <code>stdin</code>, <code>stdout</code>, and <code>stderr</code> on the same socket.</p> <p>To hint potential proxies about connection hijacking, Docker client sends connection upgrade headers similarly to websocket.</p> <pre>Upgrade: tcp\nConnection: Upgrade\n</pre> <p>When Docker daemon detects the <code>Upgrade</code> header, it switches its status code from <strong>200 OK</strong> to <strong>101 UPGRADED</strong> and resends the same headers.</p> <h2 id=\"3-3-cors-requests\">3.3 CORS Requests</h2> <p>To set cross origin requests to the remote api please give values to <code>--api-cors-header</code> when running Docker in daemon mode. Set * (asterisk) allows all, default or blank means CORS disabled</p> <pre>$ docker daemon -H=\"192.168.1.9:2375\" --api-cors-header=\"http://foo.bar\"\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.22/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.22/</a>\n  </p>\n</div>\n","engine/reference/api/docker_remote_api_v1.20/index":"<h1 id=\"docker-remote-api-v1-20\">Docker Remote API v1.20</h1> <h2 id=\"1-brief-introduction\">1. Brief introduction</h2> <ul> <li>The Remote API has replaced <code>rcli</code>.</li> <li>The daemon listens on <code>unix:///var/run/docker.sock</code> but you can <a href=\"../../../quickstart/index#bind-docker-to-another-host-port-or-a-unix-socket\">Bind Docker to another host/port or a Unix socket</a>.</li> <li>The API tends to be REST. However, for some complex commands, like <code>attach</code> or <code>pull</code>, the HTTP connection is hijacked to transport <code>stdout</code>, <code>stdin</code> and <code>stderr</code>.</li> <li>When the client API version is newer than the daemon’s, these calls return an HTTP <code>400 Bad Request</code> error message.</li> </ul> <h1 id=\"2-endpoints\">2. Endpoints</h1> <h2 id=\"2-1-containers\">2.1 Containers</h2> <h3 id=\"list-containers\">List containers</h3> <p><code>GET /containers/json</code></p> <p>List containers</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/json?all=1&amp;before=8dfafdbc3a40&amp;size=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Id\": \"8dfafdbc3a40\",\n             \"Names\":[\"/boring_feynman\"],\n             \"Image\": \"ubuntu:latest\",\n             \"Command\": \"echo 1\",\n             \"Created\": 1367854155,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [{\"PrivatePort\": 2222, \"PublicPort\": 3333, \"Type\": \"tcp\"}],\n             \"Labels\": {\n                     \"com.example.vendor\": \"Acme\",\n                     \"com.example.license\": \"GPL\",\n                     \"com.example.version\": \"1.0\"\n             },\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     },\n     {\n             \"Id\": \"9cd87474be90\",\n             \"Names\":[\"/coolName\"],\n             \"Image\": \"ubuntu:latest\",\n             \"Command\": \"echo 222222\",\n             \"Created\": 1367854155,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     },\n     {\n             \"Id\": \"3176a2479c92\",\n             \"Names\":[\"/sleepy_dog\"],\n             \"Image\": \"ubuntu:latest\",\n             \"Command\": \"echo 3333333333333333\",\n             \"Created\": 1367854154,\n             \"Status\": \"Exit 0\",\n             \"Ports\":[],\n             \"Labels\": {},\n             \"SizeRw\":12288,\n             \"SizeRootFs\":0\n     },\n     {\n             \"Id\": \"4cb07b47f9fb\",\n             \"Names\":[\"/running_cat\"],\n             \"Image\": \"ubuntu:latest\",\n             \"Command\": \"echo 444444444444444444444444444444444\",\n             \"Created\": 1367854152,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     }\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, Show all containers. Only running containers are shown by default (i.e., this defaults to false)</li> <li>\n<strong>limit</strong> – Show <code>limit</code> last created containers, include non-running ones.</li> <li>\n<strong>since</strong> – Show only containers created since Id, include non-running ones.</li> <li>\n<strong>before</strong> – Show only containers created before Id, include non-running ones.</li> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, Show the containers sizes</li> <li>\n<strong>filters</strong> - a JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the containers list. Available filters: <ul> <li>\n<code>exited=&lt;int&gt;</code>; -- containers with exit code of <code>&lt;int&gt;</code> ;</li> <li>\n<code>status=</code>(<code>created</code>|<code>restarting</code>|<code>running</code>|<code>paused</code>|<code>exited</code>)</li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of a container label</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-a-container\">Create a container</h3> <p><code>POST /containers/create</code></p> <p>Create a container</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/create HTTP/1.1\nContent-Type: application/json\n\n{\n       \"Hostname\": \"\",\n       \"Domainname\": \"\",\n       \"User\": \"\",\n       \"AttachStdin\": false,\n       \"AttachStdout\": true,\n       \"AttachStderr\": true,\n       \"Tty\": false,\n       \"OpenStdin\": false,\n       \"StdinOnce\": false,\n       \"Env\": [\n               \"FOO=bar\",\n               \"BAZ=quux\"\n       ],\n       \"Cmd\": [\n               \"date\"\n       ],\n       \"Entrypoint\": \"\",\n       \"Image\": \"ubuntu\",\n       \"Labels\": {\n               \"com.example.vendor\": \"Acme\",\n               \"com.example.license\": \"GPL\",\n               \"com.example.version\": \"1.0\"\n       },\n       \"Volumes\": {\n         \"/volumes/data\": {}\n       },\n       \"WorkingDir\": \"\",\n       \"NetworkDisabled\": false,\n       \"MacAddress\": \"12:34:56:78:9a:bc\",\n       \"ExposedPorts\": {\n               \"22/tcp\": {}\n       },\n       \"HostConfig\": {\n         \"Binds\": [\"/tmp:/tmp\"],\n         \"Links\": [\"redis3:redis\"],\n         \"LxcConf\": {\"lxc.utsname\":\"docker\"},\n         \"Memory\": 0,\n         \"MemorySwap\": 0,\n         \"CpuShares\": 512,\n         \"CpuPeriod\": 100000,\n         \"CpuQuota\": 50000,\n         \"CpusetCpus\": \"0,1\",\n         \"CpusetMems\": \"0,1\",\n         \"BlkioWeight\": 300,\n         \"MemorySwappiness\": 60,\n         \"OomKillDisable\": false,\n         \"PortBindings\": { \"22/tcp\": [{ \"HostPort\": \"11022\" }] },\n         \"PublishAllPorts\": false,\n         \"Privileged\": false,\n         \"ReadonlyRootfs\": false,\n         \"Dns\": [\"8.8.8.8\"],\n         \"DnsSearch\": [\"\"],\n         \"ExtraHosts\": null,\n         \"VolumesFrom\": [\"parent\", \"other:ro\"],\n         \"CapAdd\": [\"NET_ADMIN\"],\n         \"CapDrop\": [\"MKNOD\"],\n         \"GroupAdd\": [\"newgroup\"],\n         \"RestartPolicy\": { \"Name\": \"\", \"MaximumRetryCount\": 0 },\n         \"NetworkMode\": \"bridge\",\n         \"Devices\": [],\n         \"Ulimits\": [{}],\n         \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} },\n         \"SecurityOpt\": [],\n         \"CgroupParent\": \"\"\n      }\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 201 Created\n  Content-Type: application/json\n\n  {\n       \"Id\":\"e90e34656806\",\n       \"Warnings\":[]\n  }\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Hostname</strong> - A string value containing the hostname to use for the container.</li> <li>\n<strong>Domainname</strong> - A string value containing the domain name to use for the container.</li> <li>\n<strong>User</strong> - A string value specifying the user inside the container.</li> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code>.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code>.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code>.</li> <li>\n<strong>Tty</strong> - Boolean value, Attach standard streams to a <code>tty</code>, including <code>stdin</code> if it is not closed.</li> <li>\n<strong>OpenStdin</strong> - Boolean value, opens stdin,</li> <li>\n<strong>StdinOnce</strong> - Boolean value, close <code>stdin</code> after the 1 attached client disconnects.</li> <li>\n<strong>Env</strong> - A list of environment variables in the form of <code>[\"VAR=value\"[,\"VAR2=value2\"]]</code>\n</li> <li>\n<strong>Labels</strong> - Adds a map of labels to a container. To specify a map: <code>{\"key\":\"value\"[,\"key2\":\"value2\"]}</code>\n</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> <li>\n<strong>Entrypoint</strong> - Set the entry point for the container as a string or an array of strings.</li> <li>\n<strong>Image</strong> - A string specifying the image name to use for the container.</li> <li>\n<strong>Volumes</strong> - An object mapping mount point paths (strings) inside the container to empty objects.</li> <li>\n<strong>WorkingDir</strong> - A string specifying the working directory for commands to run in.</li> <li>\n<strong>NetworkDisabled</strong> - Boolean value, when true disables networking for the container</li> <li>\n<strong>ExposedPorts</strong> - An object mapping ports to an empty object in the form of: <code>\"ExposedPorts\": { \"&lt;port&gt;/&lt;tcp|udp&gt;: {}\" }</code>\n</li> <li>\n<strong>HostConfig</strong> <ul> <li>\n<strong>Binds</strong> – A list of volume bindings for this container. Each volume binding is a string in one of these forms: <ul> <li>\n<code>container_path</code> to create a new volume for the container</li> <li>\n<code>host_path:container_path</code> to bind-mount a host path into the container</li> <li>\n<code>host_path:container_path:ro</code> to make the bind-mount read-only inside the container.</li> </ul>\n</li> <li>\n<strong>Links</strong> - A list of links for the container. Each link entry should be in the form of <code>container_name:alias</code>.</li> <li>\n<strong>LxcConf</strong> - LXC specific configurations. These configurations only work when using the <code>lxc</code> execution driver.</li> <li>\n<strong>Memory</strong> - Memory limit in bytes.</li> <li>\n<strong>MemorySwap</strong> - Total memory limit (memory + swap); set <code>-1</code> to enable unlimited swap. You must use this with <code>memory</code> and make the swap value larger than <code>memory</code>.</li> <li>\n<strong>CpuShares</strong> - An integer value containing the container’s CPU Shares (ie. the relative weight vs other containers).</li> <li>\n<strong>CpuPeriod</strong> - The length of a CPU period in microseconds.</li> <li>\n<strong>CpuQuota</strong> - Microseconds of CPU time that the container can get in a CPU period.</li> <li>\n<strong>CpusetCpus</strong> - String value containing the <code>cgroups CpusetCpus</code> to use.</li> <li>\n<strong>CpusetMems</strong> - Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems.</li> <li>\n<strong>BlkioWeight</strong> - Block IO weight (relative weight) accepts a weight value between 10 and 1000.</li> <li>\n<strong>MemorySwappiness</strong> - Tune a container’s memory swappiness behavior. Accepts an integer between 0 and 100.</li> <li>\n<strong>OomKillDisable</strong> - Boolean value, whether to disable OOM Killer for the container or not.</li> <li>\n<strong>PortBindings</strong> - A map of exposed container ports and the host port they should map to. A JSON object in the form <code>{ &lt;port&gt;/&lt;protocol&gt;: [{ \"HostPort\": \"&lt;port&gt;\" }] }</code> Take note that <code>port</code> is specified as a string and not an integer value.</li> <li>\n<strong>PublishAllPorts</strong> - Allocates a random host port for all of a container’s exposed ports. Specified as a boolean value.</li> <li>\n<strong>Privileged</strong> - Gives the container full access to the host. Specified as a boolean value.</li> <li>\n<strong>ReadonlyRootfs</strong> - Mount the container’s root filesystem as read only. Specified as a boolean value.</li> <li>\n<strong>Dns</strong> - A list of DNS servers for the container to use.</li> <li>\n<strong>DnsSearch</strong> - A list of DNS search domains</li> <li>\n<strong>ExtraHosts</strong> - A list of hostnames/IP mappings to add to the container’s <code>/etc/hosts</code> file. Specified in the form <code>[\"hostname:IP\"]</code>.</li> <li>\n<strong>VolumesFrom</strong> - A list of volumes to inherit from another container. Specified in the form <code>&lt;container name&gt;[:&lt;ro|rw&gt;]</code>\n</li> <li>\n<strong>CapAdd</strong> - A list of kernel capabilities to add to the container.</li> <li>\n<strong>Capdrop</strong> - A list of kernel capabilities to drop from the container.</li> <li>\n<strong>GroupAdd</strong> - A list of additional groups that the container process will run as</li> <li>\n<strong>RestartPolicy</strong> – The behavior to apply when the container exits. The value is an object with a <code>Name</code> property of either <code>\"always\"</code> to always restart or <code>\"on-failure\"</code> to restart only when the container exit code is non-zero. If <code>on-failure</code> is used, <code>MaximumRetryCount</code> controls the number of times to retry before giving up. The default is not to restart. (optional) An ever increasing delay (double the previous delay, starting at 100mS) is added before each restart to prevent flooding the server.</li> <li>\n<strong>NetworkMode</strong> - Sets the networking mode for the container. Supported values are: <code>bridge</code>, <code>host</code>, <code>none</code>, and <code>container:&lt;name|id&gt;</code>\n</li> <li>\n<strong>Devices</strong> - A list of devices to add to the container specified as a JSON object in the form <code>{ \"PathOnHost\": \"/dev/deviceName\", \"PathInContainer\": \"/dev/deviceName\", \"CgroupPermissions\": \"mrw\"}</code>\n</li> <li>\n<strong>Ulimits</strong> - A list of ulimits to set in the container, specified as <code>{ \"Name\": &lt;name&gt;, \"Soft\": &lt;soft limit&gt;, \"Hard\": &lt;hard limit&gt; }</code>, for example: <code>Ulimits: { \"Name\": \"nofile\", \"Soft\": 1024, \"Hard\": 2048 }</code>\n</li> <li>\n<strong>SecurityOpt</strong>: A list of string values to customize labels for MLS systems, such as SELinux.</li> <li>\n<strong>LogConfig</strong> - Log configuration for the container, specified as a JSON object in the form <code>{ \"Type\": \"&lt;driver_name&gt;\", \"Config\": {\"key1\": \"val1\"}}</code>. Available types: <code>json-file</code>, <code>syslog</code>, <code>journald</code>, <code>gelf</code>, <code>none</code>. <code>json-file</code> logging driver.</li> <li>\n<strong>CgroupParent</strong> - Path to <code>cgroups</code> under which the container’s <code>cgroup</code> is created. If the path is not absolute, the path is considered to be relative to the <code>cgroups</code> path of the init process. Cgroups are created if they do not already exist.</li> </ul>\n</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – Assign the specified name to the container. Must match <code>/?[a-zA-Z0-9_-]+</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>406</strong> – impossible to attach (container not running)</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-a-container\">Inspect a container</h3> <p><code>GET /containers/(id or name)/json</code></p> <p>Return low-level information on the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>  GET /containers/4fa6e0f0c678/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"AppArmorProfile\": \"\",\n    \"Args\": [\n        \"-c\",\n        \"exit 9\"\n    ],\n    \"Config\": {\n        \"AttachStderr\": true,\n        \"AttachStdin\": false,\n        \"AttachStdout\": true,\n        \"Cmd\": [\n            \"/bin/sh\",\n            \"-c\",\n            \"exit 9\"\n        ],\n        \"Domainname\": \"\",\n        \"Entrypoint\": null,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": null,\n        \"Hostname\": \"ba033ac44011\",\n        \"Image\": \"ubuntu\",\n        \"Labels\": {\n            \"com.example.vendor\": \"Acme\",\n            \"com.example.license\": \"GPL\",\n            \"com.example.version\": \"1.0\"\n        },\n        \"MacAddress\": \"\",\n        \"NetworkDisabled\": false,\n        \"OnBuild\": null,\n        \"OpenStdin\": false,\n        \"StdinOnce\": false,\n        \"Tty\": false,\n        \"User\": \"\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\"\n    },\n    \"Created\": \"2015-01-06T15:47:31.485331387Z\",\n    \"Driver\": \"devicemapper\",\n    \"ExecDriver\": \"native-0.2\",\n    \"ExecIDs\": null,\n    \"HostConfig\": {\n        \"Binds\": null,\n        \"BlkioWeight\": 0,\n        \"CapAdd\": null,\n        \"CapDrop\": null,\n        \"ContainerIDFile\": \"\",\n        \"CpusetCpus\": \"\",\n        \"CpusetMems\": \"\",\n        \"CpuShares\": 0,\n        \"CpuPeriod\": 100000,\n        \"Devices\": [],\n        \"Dns\": null,\n        \"DnsSearch\": null,\n        \"ExtraHosts\": null,\n        \"IpcMode\": \"\",\n        \"Links\": null,\n        \"LxcConf\": [],\n        \"Memory\": 0,\n        \"MemorySwap\": 0,\n        \"OomKillDisable\": false,\n        \"NetworkMode\": \"bridge\",\n        \"PortBindings\": {},\n        \"Privileged\": false,\n        \"ReadonlyRootfs\": false,\n        \"PublishAllPorts\": false,\n        \"RestartPolicy\": {\n            \"MaximumRetryCount\": 2,\n            \"Name\": \"on-failure\"\n        },\n        \"LogConfig\": {\n            \"Config\": null,\n            \"Type\": \"json-file\"\n        },\n        \"SecurityOpt\": null,\n        \"VolumesFrom\": null,\n        \"Ulimits\": [{}]\n    },\n    \"HostnamePath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hostname\",\n    \"HostsPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Id\": \"ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39\",\n    \"Image\": \"04c5d3b7b0656168630d3ba35d8889bd0e9caafcaeb3004d2bfbc47e7c5d35d2\",\n    \"MountLabel\": \"\",\n    \"Name\": \"/boring_euclid\",\n    \"NetworkSettings\": {\n        \"Bridge\": \"\",\n        \"Gateway\": \"\",\n        \"IPAddress\": \"\",\n        \"IPPrefixLen\": 0,\n        \"MacAddress\": \"\",\n        \"PortMapping\": null,\n        \"Ports\": null\n    },\n    \"Path\": \"/bin/sh\",\n    \"ProcessLabel\": \"\",\n    \"ResolvConfPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/resolv.conf\",\n    \"RestartCount\": 1,\n    \"State\": {\n        \"Error\": \"\",\n        \"ExitCode\": 9,\n        \"FinishedAt\": \"2015-01-06T15:47:32.080254511Z\",\n        \"OOMKilled\": false,\n        \"Paused\": false,\n        \"Pid\": 0,\n        \"Restarting\": false,\n        \"Running\": false,\n        \"StartedAt\": \"2015-01-06T15:47:32.072697474Z\"\n    },\n    \"Mounts\": [\n        {\n            \"Source\": \"/data\",\n            \"Destination\": \"/data\",\n            \"Mode\": \"ro,Z\",\n            \"RW\": false\n        }\n    ]\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"list-processes-running-inside-a-container\">List processes running inside a container</h3> <p><code>GET /containers/(id or name)/top</code></p> <p>List processes running inside the container <code>id</code>. On Unix systems this is done by running the <code>ps</code> command. This endpoint is not supported on Windows.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Titles\" : [\n     \"UID\", \"PID\", \"PPID\", \"C\", \"STIME\", \"TTY\", \"TIME\", \"CMD\"\n   ],\n   \"Processes\" : [\n     [\n       \"root\", \"13642\", \"882\", \"0\", \"17:03\", \"pts/0\", \"00:00:00\", \"/bin/bash\"\n     ],\n     [\n       \"root\", \"13735\", \"13642\", \"0\", \"17:06\", \"pts/0\", \"00:00:00\", \"sleep 10\"\n     ]\n   ]\n}\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top?ps_args=aux HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Titles\" : [\n    \"USER\",\"PID\",\"%CPU\",\"%MEM\",\"VSZ\",\"RSS\",\"TTY\",\"STAT\",\"START\",\"TIME\",\"COMMAND\"\n  ]\n  \"Processes\" : [\n    [\n      \"root\",\"13642\",\"0.0\",\"0.1\",\"18172\",\"3184\",\"pts/0\",\"Ss\",\"17:03\",\"0:00\",\"/bin/bash\"\n    ],\n    [\n      \"root\",\"13895\",\"0.0\",\"0.0\",\"4348\",\"692\",\"pts/0\",\"S+\",\"17:15\",\"0:00\",\"sleep 10\"\n    ]\n  ],\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>ps_args</strong> – <code>ps</code> arguments to use (e.g., <code>aux</code>), defaults to <code>-ef</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-logs\">Get container logs</h3> <p><code>GET /containers/(id or name)/logs</code></p> <p>Get <code>stdout</code> and <code>stderr</code> logs from the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: This endpoint works only for containers with the <code>json-file</code> or <code>journald</code> logging drivers.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre> GET /containers/4fa6e0f0c678/logs?stderr=1&amp;stdout=1&amp;timestamps=1&amp;follow=1&amp;tail=10&amp;since=1428990821 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre> HTTP/1.1 101 UPGRADED\n Content-Type: application/vnd.docker.raw-stream\n Connection: Upgrade\n Upgrade: tcp\n\n {{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>follow</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, show <code>stdout</code> log. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, show <code>stderr</code> log. Default <code>false</code>.</li> <li>\n<strong>since</strong> – UNIX timestamp (integer) to filter logs. Specifying a timestamp will only output log-entries since that timestamp. Default: 0 (unfiltered)</li> <li>\n<strong>timestamps</strong> – 1/True/true or 0/False/false, print timestamps for every log line. Default <code>false</code>.</li> <li>\n<strong>tail</strong> – Output specified number of lines at the end of logs: <code>all</code> or <code>&lt;number&gt;</code>. Default all.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-changes-on-a-container-s-filesystem\">Inspect changes on a container’s filesystem</h3> <p><code>GET /containers/(id or name)/changes</code></p> <p>Inspect changes on container <code>id</code>’s filesystem</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/changes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Path\": \"/dev\",\n             \"Kind\": 0\n     },\n     {\n             \"Path\": \"/dev/kmsg\",\n             \"Kind\": 1\n     },\n     {\n             \"Path\": \"/test\",\n             \"Kind\": 1\n     }\n]\n</pre> <p>Values for <code>Kind</code>:</p> <ul> <li>\n<code>0</code>: Modify</li> <li>\n<code>1</code>: Add</li> <li>\n<code>2</code>: Delete</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"export-a-container\">Export a container</h3> <p><code>GET /containers/(id or name)/export</code></p> <p>Export the contents of container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/export HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/octet-stream\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-stats-based-on-resource-usage\">Get container stats based on resource usage</h3> <p><code>GET /containers/(id or name)/stats</code></p> <p>This endpoint returns a live stream of a container’s resource usage statistics.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/redis1/stats HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Type: application/json\n\n  {\n     \"read\" : \"2015-01-08T22:57:31.547920715Z\",\n     \"network\" : {\n        \"rx_dropped\" : 0,\n        \"rx_bytes\" : 648,\n        \"rx_errors\" : 0,\n        \"tx_packets\" : 8,\n        \"tx_dropped\" : 0,\n        \"rx_packets\" : 8,\n        \"tx_errors\" : 0,\n        \"tx_bytes\" : 648\n     },\n     \"memory_stats\" : {\n        \"stats\" : {\n           \"total_pgmajfault\" : 0,\n           \"cache\" : 0,\n           \"mapped_file\" : 0,\n           \"total_inactive_file\" : 0,\n           \"pgpgout\" : 414,\n           \"rss\" : 6537216,\n           \"total_mapped_file\" : 0,\n           \"writeback\" : 0,\n           \"unevictable\" : 0,\n           \"pgpgin\" : 477,\n           \"total_unevictable\" : 0,\n           \"pgmajfault\" : 0,\n           \"total_rss\" : 6537216,\n           \"total_rss_huge\" : 6291456,\n           \"total_writeback\" : 0,\n           \"total_inactive_anon\" : 0,\n           \"rss_huge\" : 6291456,\n           \"hierarchical_memory_limit\" : 67108864,\n           \"total_pgfault\" : 964,\n           \"total_active_file\" : 0,\n           \"active_anon\" : 6537216,\n           \"total_active_anon\" : 6537216,\n           \"total_pgpgout\" : 414,\n           \"total_cache\" : 0,\n           \"inactive_anon\" : 0,\n           \"active_file\" : 0,\n           \"pgfault\" : 964,\n           \"inactive_file\" : 0,\n           \"total_pgpgin\" : 477\n        },\n        \"max_usage\" : 6651904,\n        \"usage\" : 6537216,\n        \"failcnt\" : 0,\n        \"limit\" : 67108864\n     },\n     \"blkio_stats\" : {},\n     \"cpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24472255,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100215355,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 739306590000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     },\n     \"precpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24350896,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100093996,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 9492140000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     }\n  }\n</pre> <p>The precpu_stats is the cpu statistic of last read, which is used for calculating the cpu usage percent. It is not the exact copy of the “cpu_stats” field.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, pull stats once then disconnect. Default <code>true</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"resize-a-container-tty\">Resize a container TTY</h3> <p><code>POST /containers/(id or name)/resize?h=&lt;height&gt;&amp;w=&lt;width&gt;</code></p> <p>Resize the TTY for container with <code>id</code>. You must restart the container for the resize to take effect.</p> <p><strong>Example request</strong>:</p> <pre>  POST /containers/4fa6e0f0c678/resize?h=40&amp;w=80 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Length: 0\n  Content-Type: text/plain; charset=utf-8\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – No such container</li> <li>\n<strong>500</strong> – Cannot resize container</li> </ul> <h3 id=\"start-a-container\">Start a container</h3> <p><code>POST /containers/(id or name)/start</code></p> <p>Start the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: For backwards compatibility, this endpoint accepts a <code>HostConfig</code> as JSON-encoded request body. See <a href=\"#create-a-container\">create a container</a> for details.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/start HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already started</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"stop-a-container\">Stop a container</h3> <p><code>POST /containers/(id or name)/stop</code></p> <p>Stop the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/stop?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already stopped</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"restart-a-container\">Restart a container</h3> <p><code>POST /containers/(id or name)/restart</code></p> <p>Restart the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/restart?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"kill-a-container\">Kill a container</h3> <p><code>POST /containers/(id or name)/kill</code></p> <p>Kill the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/kill HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters</p> <ul> <li>\n<strong>signal</strong> - Signal to send to the container: integer or string like <code>SIGINT</code>. When not set, <code>SIGKILL</code> is assumed and the call waits for the container to exit.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"rename-a-container\">Rename a container</h3> <p><code>POST /containers/(id or name)/rename</code></p> <p>Rename the container <code>id</code> to a <code>new_name</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/rename?name=new_name HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – new name for the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - conflict name already assigned</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"pause-a-container\">Pause a container</h3> <p><code>POST /containers/(id or name)/pause</code></p> <p>Pause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/pause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"unpause-a-container\">Unpause a container</h3> <p><code>POST /containers/(id or name)/unpause</code></p> <p>Unpause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/unpause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"attach-to-a-container\">Attach to a container</h3> <p><code>POST /containers/(id or name)/attach</code></p> <p>Attach to the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/attach?logs=1&amp;stream=0&amp;stdout=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 101 UPGRADED\nContent-Type: application/vnd.docker.raw-stream\nConnection: Upgrade\nUpgrade: tcp\n\n{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<p><strong>500</strong> – server error</p> <p><strong>Stream details</strong>:</p> <p>When using the TTY setting is enabled in <a href=\"#create-a-container\"><code>POST /containers/create</code> </a>, the stream is the raw data from the process PTY and client’s <code>stdin</code>. When the TTY is disabled, then the stream is multiplexed to separate <code>stdout</code> and <code>stderr</code>.</p> <p>The format is a <strong>Header</strong> and a <strong>Payload</strong> (frame).</p> <p><strong>HEADER</strong></p> <p>The header contains the information which the stream writes (<code>stdout</code> or <code>stderr</code>). It also contains the size of the associated frame encoded in the last four bytes (<code>uint32</code>).</p> <p>It is encoded on the first eight bytes like this:</p> <pre>header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}\n</pre> <p><code>STREAM_TYPE</code> can be:</p>\n</li> <li><p>0: <code>stdin</code> (is written on <code>stdout</code>)</p></li> <li><p>1: <code>stdout</code></p></li> <li>\n<p>2: <code>stderr</code></p> <p><code>SIZE1, SIZE2, SIZE3, SIZE4</code> are the four bytes of the <code>uint32</code> size encoded as big endian.</p> <p><strong>PAYLOAD</strong></p> <p>The payload is the raw stream.</p> <p><strong>IMPLEMENTATION</strong></p> <p>The simplest way to implement the Attach protocol is the following:</p> <ol> <li>Read eight bytes.</li> <li>Choose <code>stdout</code> or <code>stderr</code> depending on the first byte.</li> <li>Extract the frame size from the last four bytes.</li> <li>Read the extracted size and output it on the correct output.</li> <li>Goto 1.</li> </ol>\n</li> </ul> <h3 id=\"attach-to-a-container-websocket\">Attach to a container (websocket)</h3> <p><code>GET /containers/(id or name)/attach/ws</code></p> <p>Attach to the container <code>id</code> via websocket</p> <p>Implements websocket protocol handshake according to <a href=\"http://tools.ietf.org/html/rfc6455\">RFC 6455</a></p> <p><strong>Example request</strong></p> <pre>GET /containers/e90e34656806/attach/ws?logs=0&amp;stream=1&amp;stdin=1&amp;stdout=1&amp;stderr=1 HTTP/1.1\n</pre> <p><strong>Example response</strong></p> <pre>{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"wait-a-container\">Wait a container</h3> <p><code>POST /containers/(id or name)/wait</code></p> <p>Block until container <code>id</code> stops, then returns the exit code</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/wait HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"StatusCode\": 0}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-a-container\">Remove a container</h3> <p><code>DELETE /containers/(id or name)</code></p> <p>Remove the container <code>id</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /containers/16253994b7c4?v=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>v</strong> – 1/True/true or 0/False/false, Remove the volumes associated to the container. Default <code>false</code>.</li> <li>\n<strong>force</strong> - 1/True/true or 0/False/false, Kill then remove the container. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"copy-files-or-folders-from-a-container\">Copy files or folders from a container</h3> <p><code>POST /containers/(id or name)/copy</code></p> <p>Copy files or folders of container <code>id</code></p> <p><strong>Deprecated</strong> in favor of the <code>archive</code> endpoint below.</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/4fa6e0f0c678/copy HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Resource\": \"test.txt\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"retrieving-information-about-files-and-folders-in-a-container\">Retrieving information about files and folders in a container</h3> <p><code>HEAD /containers/(id or name)/archive</code></p> <p>See the description of the <code>X-Docker-Container-Path-Stat</code> header in the following section.</p> <h3 id=\"get-an-archive-of-a-filesystem-resource-in-a-container\">Get an archive of a filesystem resource in a container</h3> <p><code>GET /containers/(id or name)/archive</code></p> <p>Get a tar archive of a resource in the filesystem of container <code>id</code>.</p> <p>Query Parameters:</p> <ul> <li>\n<p><strong>path</strong> - resource in the container’s filesystem to archive. Required.</p> <p>If not an absolute path, it is relative to the container’s root directory. The resource specified by <strong>path</strong> must exist. To assert that the resource is expected to be a directory, <strong>path</strong> should end in <code>/</code> or <code>/.</code> (assuming a path separator of <code>/</code>). If <strong>path</strong> ends in <code>/.</code> then this indicates that only the contents of the <strong>path</strong> directory should be copied. A symlink is always resolved to its target.</p> <p><strong>Note</strong>: It is not possible to copy certain system files such as resources under <code>/proc</code>, <code>/sys</code>, <code>/dev</code>, and mounts created by the user in the container.</p>\n</li> </ul> <p><strong>Example request</strong>:</p> <pre>    GET /containers/8cce319429b2/archive?path=/root HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/x-tar\n    X-Docker-Container-Path-Stat: eyJuYW1lIjoicm9vdCIsInNpemUiOjQwOTYsIm1vZGUiOjIxNDc0ODQwOTYsIm10aW1lIjoiMjAxNC0wMi0yN1QyMDo1MToyM1oiLCJsaW5rVGFyZ2V0IjoiIn0=\n\n    {{ TAR STREAM }}\n</pre> <p>On success, a response header <code>X-Docker-Container-Path-Stat</code> will be set to a base64-encoded JSON object containing some filesystem header information about the archived resource. The above example value would decode to the following JSON object (whitespace added for readability):</p> <pre>    {\n        \"name\": \"root\",\n        \"size\": 4096,\n        \"mode\": 2147484096,\n        \"mtime\": \"2014-02-27T20:51:23Z\",\n        \"linkTarget\": \"\"\n    }\n</pre> <p>A <code>HEAD</code> request can also be made to this endpoint if only this information is desired.</p> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - success, returns archive of copied resource</li> <li>\n<strong>400</strong> - client error, bad parameter, details in JSON response body, one of: <ul> <li>must specify path parameter (<strong>path</strong> cannot be empty)</li> <li>not a directory (<strong>path</strong> was asserted to be a directory but exists as a file)</li> </ul>\n</li> <li>\n<strong>404</strong> - client error, resource not found, one of: – no such container (container <code>id</code> does not exist) <ul> <li>no such file or directory (<strong>path</strong> does not exist)</li> </ul>\n</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"extract-an-archive-of-files-or-folders-to-a-directory-in-a-container\">Extract an archive of files or folders to a directory in a container</h3> <p><code>PUT /containers/(id or name)/archive</code></p> <p>Upload a tar archive to be extracted to a path in the filesystem of container <code>id</code>.</p> <p>Query Parameters:</p> <ul> <li>\n<p><strong>path</strong> - path to a directory in the container to extract the archive’s contents into. Required.</p> <p>If not an absolute path, it is relative to the container’s root directory. The <strong>path</strong> resource must exist.</p>\n</li> <li><p><strong>noOverwriteDirNonDir</strong> - If “1”, “true”, or “True” then it will be an error if unpacking the given content would cause an existing directory to be replaced with a non-directory and vice versa.</p></li> </ul> <p><strong>Example request</strong>:</p> <pre>    PUT /containers/8cce319429b2/archive?path=/vol1 HTTP/1.1\n    Content-Type: application/x-tar\n\n    {{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – the content was extracted successfully</li> <li>\n<strong>400</strong> - client error, bad parameter, details in JSON response body, one of: <ul> <li>must specify path parameter (<strong>path</strong> cannot be empty)</li> <li>not a directory (<strong>path</strong> should be a directory but exists as a file)</li> <li>unable to overwrite existing directory with non-directory (if <strong>noOverwriteDirNonDir</strong>)</li> <li>unable to overwrite existing non-directory with directory (if <strong>noOverwriteDirNonDir</strong>)</li> </ul>\n</li> <li>\n<strong>403</strong> - client error, permission denied, the volume or container rootfs is marked as read-only.</li> <li>\n<strong>404</strong> - client error, resource not found, one of: – no such container (container <code>id</code> does not exist) <ul> <li>no such file or directory (<strong>path</strong> resource does not exist)</li> </ul>\n</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-2-images\">2.2 Images</h2> <h3 id=\"list-images\">List Images</h3> <p><code>GET /images/json</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/json?all=0 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.04\",\n       \"ubuntu:precise\",\n       \"ubuntu:latest\"\n     ],\n     \"Id\": \"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\n     \"Created\": 1365714795,\n     \"Size\": 131506275,\n     \"VirtualSize\": 131506275,\n     \"Labels\": {}\n  },\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.10\",\n       \"ubuntu:quantal\"\n     ],\n     \"ParentId\": \"27cf784147099545\",\n     \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n     \"Created\": 1364102658,\n     \"Size\": 24653,\n     \"VirtualSize\": 180116135,\n     \"Labels\": {\n        \"com.example.version\": \"v1\"\n     }\n  }\n]\n</pre> <p><strong>Example request, with digest information</strong>:</p> <pre>GET /images/json?digests=1 HTTP/1.1\n</pre> <p><strong>Example response, with digest information</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Created\": 1420064636,\n    \"Id\": \"4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125\",\n    \"ParentId\": \"ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2\",\n    \"RepoDigests\": [\n      \"localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n    ],\n    \"RepoTags\": [\n      \"localhost:5000/test/busybox:latest\",\n      \"playdate:latest\"\n    ],\n    \"Size\": 0,\n    \"VirtualSize\": 2429728,\n    \"Labels\": {}\n  }\n]\n</pre> <p>The response shows a single image <code>Id</code> associated with two repositories (<code>RepoTags</code>): <code>localhost:5000/test/busybox</code>: and <code>playdate</code>. A caller can use either of the <code>RepoTags</code> values <code>localhost:5000/test/busybox:latest</code> or <code>playdate:latest</code> to reference the image.</p> <p>You can also use <code>RepoDigests</code> values to reference an image. In this response, the array has only one reference and that is to the <code>localhost:5000/test/busybox</code> repository; the <code>playdate</code> repository has no digest. You can reference this digest using the value: <code>localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d...</code></p> <p>See the <code>docker run</code> and <code>docker build</code> commands for examples of digest and tag references on the command line.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>filters</strong> – a JSON encoded value of the filters (a map[string][]string) to process on the images list. Available filters: <ul> <li><code>dangling=true</code></li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of an image label</li> </ul>\n</li> <li>\n<strong>filter</strong> - only return images with the specified name</li> </ul> <h3 id=\"build-image-from-a-dockerfile\">Build image from a Dockerfile</h3> <p><code>POST /build</code></p> <p>Build an image from a Dockerfile</p> <p><strong>Example request</strong>:</p> <pre>POST /build HTTP/1.1\n\n{{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"stream\": \"Step 1...\"}\n{\"stream\": \"...\"}\n{\"error\": \"Error...\", \"errorDetail\": {\"code\": 123, \"message\": \"Error...\"}}\n</pre> <p>The input stream must be a <code>tar</code> archive compressed with one of the following algorithms: <code>identity</code> (no compression), <code>gzip</code>, <code>bzip2</code>, <code>xz</code>.</p> <p>The archive must include a build instructions file, typically called <code>Dockerfile</code> at the archive’s root. The <code>dockerfile</code> parameter may be used to specify a different build instructions file. To do this, its value must be the path to the alternate build instructions file to use.</p> <p>The archive may include any number of other files, which are accessible in the build context (See the <a href=\"../../builder/index#dockerbuilder\"><em>ADD build command</em></a>).</p> <p>The build is canceled if the client drops the connection by quitting or being killed.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>dockerfile</strong> - Path within the build context to the Dockerfile. This is ignored if <code>remote</code> is specified and points to an individual filename.</li> <li>\n<strong>t</strong> – A repository name (and optionally a tag) to apply to the resulting image in case of success.</li> <li>\n<strong>remote</strong> – A Git repository URI or HTTP/HTTPS URI build source. If the URI specifies a filename, the file’s contents are placed into a file called <code>Dockerfile</code>.</li> <li>\n<strong>q</strong> – Suppress verbose build output.</li> <li>\n<strong>nocache</strong> – Do not use the cache when building the image.</li> <li>\n<strong>pull</strong> - Attempt to pull the image even if an older image exists locally.</li> <li>\n<strong>rm</strong> - Remove intermediate containers after a successful build (default behavior).</li> <li>\n<strong>forcerm</strong> - Always remove intermediate containers (includes <code>rm</code>).</li> <li>\n<strong>memory</strong> - Set memory limit for build.</li> <li>\n<strong>memswap</strong> - Total memory (memory + swap), <code>-1</code> to enable unlimited swap.</li> <li>\n<strong>cpushares</strong> - CPU shares (relative weight).</li> <li>\n<strong>cpusetcpus</strong> - CPUs in which to allow execution (e.g., <code>0-3</code>, <code>0,1</code>).</li> <li>\n<strong>cpuperiod</strong> - The length of a CPU period in microseconds.</li> <li>\n<p><strong>cpuquota</strong> - Microseconds of CPU time that the container can get in a CPU period.</p> <p>Request Headers:</p>\n</li> <li><p><strong>Content-type</strong> – Set to <code>\"application/tar\"</code>.</p></li> <li>\n<p><strong>X-Registry-Config</strong> – A base64-url-safe-encoded Registry Auth Config JSON object with the following structure:</p> <pre>    {\n        \"docker.example.com\": {\n            \"username\": \"janedoe\",\n            \"password\": \"hunter2\"\n        },\n        \"https://index.docker.io/v1/\": {\n            \"username\": \"mobydock\",\n            \"password\": \"conta1n3rize14\"\n        }\n    }\n</pre> <p>This object maps the hostname of a registry to an object containing the “username” and “password” for that registry. Multiple registries may be specified as the build may be based on an image requiring authentication to pull from any arbitrary registry. Only the registry domain name (and port if not the default “443”) are required. However (for legacy reasons) the “official” Docker, Inc. hosted registry must be specified with both a “https://” prefix and a “/v1/” suffix even though Docker will prefer to use the v2 registry API.</p>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-an-image\">Create an image</h3> <p><code>POST /images/create</code></p> <p>Create an image either by pulling it from the registry or by importing it</p> <p><strong>Example request</strong>:</p> <pre>POST /images/create?fromImage=ubuntu HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pulling...\"}\n{\"status\": \"Pulling\", \"progress\": \"1 B/ 100 B\", \"progressDetail\": {\"current\": 1, \"total\": 100}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>When using this endpoint to pull an image from the registry, the <code>X-Registry-Auth</code> header can be used to include a base64-encoded AuthConfig object.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>fromImage</strong> – Name of the image to pull.</li> <li>\n<strong>fromSrc</strong> – Source to import. The value may be a URL from which the image can be retrieved or <code>-</code> to read the image from the request body.</li> <li>\n<strong>repo</strong> – Repository name.</li> <li>\n<p><strong>tag</strong> – Tag.</p> <p>Request Headers:</p>\n</li> <li><p><strong>X-Registry-Auth</strong> – base64-encoded AuthConfig object</p></li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-an-image\">Inspect an image</h3> <p><code>GET /images/(name)/json</code></p> <p>Return low-level information on the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/ubuntu/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n     \"Created\": \"2013-03-23T22:24:18.818426-07:00\",\n     \"Container\": \"3d67245a8d72ecf13f33dffac9f79dcdf70f75acb84d308770391510e0c23ad0\",\n     \"ContainerConfig\":\n             {\n                     \"Hostname\": \"\",\n                     \"User\": \"\",\n                     \"AttachStdin\": false,\n                     \"AttachStdout\": false,\n                     \"AttachStderr\": false,\n                     \"Tty\": true,\n                     \"OpenStdin\": true,\n                     \"StdinOnce\": false,\n                     \"Env\": null,\n                     \"Cmd\": [\"/bin/bash\"],\n                     \"Dns\": null,\n                     \"Image\": \"ubuntu\",\n                     \"Labels\": {\n                         \"com.example.vendor\": \"Acme\",\n                         \"com.example.license\": \"GPL\",\n                         \"com.example.version\": \"1.0\"\n                     },\n                     \"Volumes\": null,\n                     \"VolumesFrom\": \"\",\n                     \"WorkingDir\": \"\"\n             },\n     \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n     \"Parent\": \"27cf784147099545\",\n     \"Size\": 6824592\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-the-history-of-an-image\">Get the history of an image</h3> <p><code>GET /images/(name)/history</code></p> <p>Return the history of the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/ubuntu/history HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n    {\n        \"Id\": \"3db9c44f45209632d6050b35958829c3a2aa256d81b9a7be45b362ff85c54710\",\n        \"Created\": 1398108230,\n        \"CreatedBy\": \"/bin/sh -c #(nop) ADD file:eb15dbd63394e063b805a3c32ca7bf0266ef64676d5a6fab4801f2e81e2a5148 in /\",\n        \"Tags\": [\n            \"ubuntu:lucid\",\n            \"ubuntu:10.04\"\n        ],\n        \"Size\": 182964289,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"6cfa4d1f33fb861d4d114f43b25abd0ac737509268065cdfd69d544a59c85ab8\",\n        \"Created\": 1398108222,\n        \"CreatedBy\": \"/bin/sh -c #(nop) MAINTAINER Tianon Gravi &lt;admwiggin@gmail.com&gt; - mkimage-debootstrap.sh -i iproute,iputils-ping,ubuntu-minimal -t lucid.tar.xz lucid http://archive.ubuntu.com/ubuntu/\",\n        \"Tags\": null,\n        \"Size\": 0,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\n        \"Created\": 1371157430,\n        \"CreatedBy\": \"\",\n        \"Tags\": [\n            \"scratch12:latest\",\n            \"scratch:latest\"\n        ],\n        \"Size\": 0,\n        \"Comment\": \"Imported from -\"\n    }\n]\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"push-an-image-on-the-registry\">Push an image on the registry</h3> <p><code>POST /images/(name)/push</code></p> <p>Push the image <code>name</code> on the registry</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/push HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pushing...\"}\n{\"status\": \"Pushing\", \"progress\": \"1/? (n/a)\", \"progressDetail\": {\"current\": 1}}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>If you wish to push an image on to a private registry, that image must already have a tag into a repository which references that registry <code>hostname</code> and <code>port</code>. This repository name should then be used in the URL. This duplicates the command line’s flow.</p> <p><strong>Example request</strong>:</p> <pre>POST /images/registry.acme.com:5000/test/push HTTP/1.1\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>tag</strong> – The tag to associate with the image on the registry. This is optional.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>X-Registry-Auth</strong> – Include a base64-encoded AuthConfig. object.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"tag-an-image-into-a-repository\">Tag an image into a repository</h3> <p><code>POST /images/(name)/tag</code></p> <p>Tag the image <code>name</code> into a repository</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/tag?repo=myrepo&amp;force=0&amp;tag=v42 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>repo</strong> – The repository to tag in</li> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>tag</strong> - The new tag name</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-an-image\">Remove an image</h3> <p><code>DELETE /images/(name)</code></p> <p>Remove the image <code>name</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /images/test HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-type: application/json\n\n[\n {\"Untagged\": \"3e2f21a89f\"},\n {\"Deleted\": \"3e2f21a89f\"},\n {\"Deleted\": \"53b4f83ac9\"}\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>noprune</strong> – 1/True/true or 0/False/false, default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"search-images\">Search images</h3> <p><code>GET /images/search</code></p> <p>Search for an image on <a href=\"https://hub.docker.com\">Docker Hub</a>.</p> <blockquote> <p><strong>Note</strong>: The response keys have changed from API v1.6 to reflect the JSON sent by the registry server to the docker daemon’s request.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>GET /images/search?term=sshd HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"wma55/u1210sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"jdswinbank/sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"vgauthier/sshd\",\n            \"star_count\": 0\n        }\n...\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>term</strong> – term to search</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-3-misc\">2.3 Misc</h2> <h3 id=\"check-auth-configuration\">Check auth configuration</h3> <p><code>POST /auth</code></p> <p>Get the default username and email</p> <p><strong>Example request</strong>:</p> <pre>POST /auth HTTP/1.1\nContent-Type: application/json\n\n{\n     \"username\":\" hannibal\",\n     \"password: \"xxxx\",\n     \"email\": \"hannibal@a-team.com\",\n     \"serveraddress\": \"https://index.docker.io/v1/\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"display-system-wide-information\">Display system-wide information</h3> <p><code>GET /info</code></p> <p>Display system-wide information</p> <p><strong>Example request</strong>:</p> <pre>GET /info HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"Containers\": 11,\n    \"CpuCfsPeriod\": true,\n    \"CpuCfsQuota\": true,\n    \"Debug\": false,\n    \"DockerRootDir\": \"/var/lib/docker\",\n    \"Driver\": \"btrfs\",\n    \"DriverStatus\": [[\"\"]],\n    \"ExecutionDriver\": \"native-0.1\",\n    \"ExperimentalBuild\": false,\n    \"HttpProxy\": \"http://test:test@localhost:8080\",\n    \"HttpsProxy\": \"https://test:test@localhost:8080\",\n    \"ID\": \"7TRN:IPZB:QYBB:VPBQ:UMPP:KARE:6ZNR:XE6T:7EWV:PKF4:ZOJD:TPYS\",\n    \"IPv4Forwarding\": true,\n    \"Images\": 16,\n    \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n    \"InitPath\": \"/usr/bin/docker\",\n    \"InitSha1\": \"\",\n    \"KernelVersion\": \"3.12.0-1-amd64\",\n    \"Labels\": [\n        \"storage=ssd\"\n    ],\n    \"MemTotal\": 2099236864,\n    \"MemoryLimit\": true,\n    \"NCPU\": 1,\n    \"NEventsListener\": 0,\n    \"NFd\": 11,\n    \"NGoroutines\": 21,\n    \"Name\": \"prod-server-42\",\n    \"NoProxy\": \"9.81.1.160\",\n    \"OomKillDisable\": true,\n    \"OperatingSystem\": \"Boot2Docker\",\n    \"RegistryConfig\": {\n        \"IndexConfigs\": {\n            \"docker.io\": {\n                \"Mirrors\": null,\n                \"Name\": \"docker.io\",\n                \"Official\": true,\n                \"Secure\": true\n            }\n        },\n        \"InsecureRegistryCIDRs\": [\n            \"127.0.0.0/8\"\n        ]\n    },\n    \"SwapLimit\": false,\n    \"SystemTime\": \"2015-03-10T11:11:23.730591467-07:00\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"show-the-docker-version-information\">Show the docker version information</h3> <p><code>GET /version</code></p> <p>Show the docker version information</p> <p><strong>Example request</strong>:</p> <pre>GET /version HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n     \"Version\": \"1.5.0\",\n     \"Os\": \"linux\",\n     \"KernelVersion\": \"3.18.5-tinycore64\",\n     \"GoVersion\": \"go1.4.1\",\n     \"GitCommit\": \"a8a31ef\",\n     \"Arch\": \"amd64\",\n     \"ApiVersion\": \"1.20\",\n     \"Experimental\": false\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"ping-the-docker-server\">Ping the docker server</h3> <p><code>GET /_ping</code></p> <p>Ping the docker server</p> <p><strong>Example request</strong>:</p> <pre>GET /_ping HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: text/plain\n\nOK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-new-image-from-a-container-s-changes\">Create a new image from a container’s changes</h3> <p><code>POST /commit</code></p> <p>Create a new image from a container’s changes</p> <p><strong>Example request</strong>:</p> <pre>POST /commit?container=44c004db4b17&amp;comment=message&amp;repo=myrepo HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Hostname\": \"\",\n     \"Domainname\": \"\",\n     \"User\": \"\",\n     \"AttachStdin\": false,\n     \"AttachStdout\": true,\n     \"AttachStderr\": true,\n     \"Tty\": false,\n     \"OpenStdin\": false,\n     \"StdinOnce\": false,\n     \"Env\": null,\n     \"Cmd\": [\n             \"date\"\n     ],\n     \"Mounts\": [\n       {\n         \"Source\": \"/data\",\n         \"Destination\": \"/data\",\n         \"Mode\": \"ro,Z\",\n         \"RW\": false\n       }\n     ],\n     \"Labels\": {\n             \"key1\": \"value1\",\n             \"key2\": \"value2\"\n      },\n     \"WorkingDir\": \"\",\n     \"NetworkDisabled\": false,\n     \"ExposedPorts\": {\n             \"22/tcp\": {}\n     }\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\"Id\": \"596069db4bf5\"}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>config</strong> - the container’s configuration</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>container</strong> – source container</li> <li>\n<strong>repo</strong> – repository</li> <li>\n<strong>tag</strong> – tag</li> <li>\n<strong>comment</strong> – commit message</li> <li>\n<strong>author</strong> – author (e.g., “John Hannibal Smith &lt;<a href=\"mailto:hannibal%40a-team.com\">hannibal@a-team.com</a>&gt;“)</li> <li>\n<strong>pause</strong> – 1/True/true or 0/False/false, whether to pause the container before committing</li> <li>\n<strong>changes</strong> – Dockerfile instructions to apply while committing</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"monitor-docker-s-events\">Monitor Docker’s events</h3> <p><code>GET /events</code></p> <p>Get container events from docker, either in real time via streaming, or via polling (using since).</p> <p>Docker containers report the following events:</p> <pre>attach, commit, copy, create, destroy, die, exec_create, exec_start, export, kill, oom, pause, rename, resize, restart, start, stop, top, unpause\n</pre> <p>and Docker images report:</p> <pre>delete, import, pull, push, tag, untag\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /events?since=1374067924\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"create\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067924}\n{\"status\": \"start\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067924}\n{\"status\": \"stop\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067966}\n{\"status\": \"destroy\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067970}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>since</strong> – Timestamp used for polling</li> <li>\n<strong>until</strong> – Timestamp used for polling</li> <li>\n<strong>filters</strong> – A json encoded value of the filters (a map[string][]string) to process on the event list. Available filters: <ul> <li>\n<code>event=&lt;string&gt;</code>; -- event to filter</li> <li>\n<code>image=&lt;string&gt;</code>; -- image to filter</li> <li>\n<code>container=&lt;string&gt;</code>; -- container to filter</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images-in-a-repository\">Get a tarball containing all images in a repository</h3> <p><code>GET /images/(name)/get</code></p> <p>Get a tarball containing all images and metadata for the repository specified by <code>name</code>.</p> <p>If <code>name</code> is a specific name and tag (e.g. ubuntu:latest), then only that image (and its parents) are returned. If <code>name</code> is an image ID, similarly only that image (and its parents) are returned, but with the exclusion of the ‘repositories’ file in the tarball, as there were no image names referenced.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/ubuntu/get\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images\">Get a tarball containing all images.</h3> <p><code>GET /images/get</code></p> <p>Get a tarball containing all images and metadata for one or more repositories.</p> <p>For each value of the <code>names</code> parameter: if it is a specific name and tag (e.g. <code>ubuntu:latest</code>), then only that image (and its parents) are returned; if it is an image ID, similarly only that image (and its parents) are returned and there would be no names referenced in the ‘repositories’ file for this image ID.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/get?names=myname%2Fmyapp%3Alatest&amp;names=busybox\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"load-a-tarball-with-a-set-of-images-and-tags-into-docker\">Load a tarball with a set of images and tags into docker</h3> <p><code>POST /images/load</code></p> <p>Load a set of images and tags into a Docker repository. See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>POST /images/load\n\nTarball in body\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"image-tarball-format\">Image tarball format</h3> <p>An image tarball contains one directory per image layer (named using its long ID), each containing these files:</p> <ul> <li>\n<code>VERSION</code>: currently <code>1.0</code> - the file format version</li> <li>\n<code>json</code>: detailed layer information, similar to <code>docker inspect layer_id</code>\n</li> <li>\n<code>layer.tar</code>: A tarfile containing the filesystem changes in this layer</li> </ul> <p>The <code>layer.tar</code> file contains <code>aufs</code> style <code>.wh..wh.aufs</code> files and directories for storing attribute changes and deletions.</p> <p>If the tarball defines a repository, the tarball should also include a <code>repositories</code> file at the root that contains a list of repository and tag names mapped to layer IDs.</p> <pre>{\"hello-world\":\n    {\"latest\": \"565a9d68a73f6706862bfe8409a7f659776d4d60a8d096eb4a3cbce6999cc2a1\"}\n}\n</pre> <h3 id=\"exec-create\">Exec Create</h3> <p><code>POST /containers/(id or name)/exec</code></p> <p>Sets up an exec instance in a running container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/exec HTTP/1.1\nContent-Type: application/json\n\n  {\n   \"AttachStdin\": false,\n   \"AttachStdout\": true,\n   \"AttachStderr\": true,\n   \"Tty\": false,\n   \"Cmd\": [\n                 \"date\"\n         ]\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n     \"Id\": \"f90e34656806\",\n     \"Warnings\":[]\n}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code> of the <code>exec</code> command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> </ul> <h3 id=\"exec-start\">Exec Start</h3> <p><code>POST /exec/(id)/start</code></p> <p>Starts a previously set up <code>exec</code> instance <code>id</code>. If <code>detach</code> is true, this API returns after starting the <code>exec</code> command. Otherwise, this API sets up an interactive session with the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/start HTTP/1.1\nContent-Type: application/json\n\n{\n \"Detach\": false,\n \"Tty\": false\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/vnd.docker.raw-stream\n\n{{ STREAM }}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Detach</strong> - Detach from the <code>exec</code> command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<p><strong>404</strong> – no such exec instance</p> <p><strong>Stream details</strong>: Similar to the stream behavior of <code>POST /containers/(id or name)/attach</code> API</p>\n</li> </ul> <h3 id=\"exec-resize\">Exec Resize</h3> <p><code>POST /exec/(id)/resize</code></p> <p>Resizes the <code>tty</code> session used by the <code>exec</code> command <code>id</code>. The unit is number of characters. This API is valid only if <code>tty</code> was specified as part of creating and starting the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/resize?h=40&amp;w=80 HTTP/1.1\nContent-Type: text/plain\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: text/plain\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of <code>tty</code> session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> </ul> <h3 id=\"exec-inspect\">Exec Inspect</h3> <p><code>GET /exec/(id)/json</code></p> <p>Return low-level information about the <code>exec</code> command <code>id</code>.</p> <p><strong>Example request</strong>:</p> <pre>GET /exec/11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: plain/text\n\n{\n  \"ID\" : \"11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39\",\n  \"Running\" : false,\n  \"ExitCode\" : 2,\n  \"ProcessConfig\" : {\n    \"privileged\" : false,\n    \"user\" : \"\",\n    \"tty\" : false,\n    \"entrypoint\" : \"sh\",\n    \"arguments\" : [\n      \"-c\",\n      \"exit 2\"\n    ]\n  },\n  \"OpenStdin\" : false,\n  \"OpenStderr\" : false,\n  \"OpenStdout\" : false,\n  \"Container\" : {\n    \"State\" : {\n      \"Running\" : true,\n      \"Paused\" : false,\n      \"Restarting\" : false,\n      \"OOMKilled\" : false,\n      \"Pid\" : 3650,\n      \"ExitCode\" : 0,\n      \"Error\" : \"\",\n      \"StartedAt\" : \"2014-11-17T22:26:03.717657531Z\",\n      \"FinishedAt\" : \"0001-01-01T00:00:00Z\"\n    },\n    \"ID\" : \"8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c\",\n    \"Created\" : \"2014-11-17T22:26:03.626304998Z\",\n    \"Path\" : \"date\",\n    \"Args\" : [],\n    \"Config\" : {\n      \"Hostname\" : \"8f177a186b97\",\n      \"Domainname\" : \"\",\n      \"User\" : \"\",\n      \"AttachStdin\" : false,\n      \"AttachStdout\" : false,\n      \"AttachStderr\" : false,\n      \"ExposedPorts\" : null,\n      \"Tty\" : false,\n      \"OpenStdin\" : false,\n      \"StdinOnce\" : false,\n      \"Env\" : [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ],\n      \"Cmd\" : [\n        \"date\"\n      ],\n      \"Image\" : \"ubuntu\",\n      \"Volumes\" : null,\n      \"WorkingDir\" : \"\",\n      \"Entrypoint\" : null,\n      \"NetworkDisabled\" : false,\n      \"MacAddress\" : \"\",\n      \"OnBuild\" : null,\n      \"SecurityOpt\" : null\n    },\n    \"Image\" : \"5506de2b643be1e6febbf3b8a240760c6843244c41e12aa2f60ccbb7153d17f5\",\n    \"NetworkSettings\" : {\n      \"IPAddress\" : \"172.17.0.2\",\n      \"IPPrefixLen\" : 16,\n      \"MacAddress\" : \"02:42:ac:11:00:02\",\n      \"Gateway\" : \"172.17.42.1\",\n      \"Bridge\" : \"docker0\",\n      \"PortMapping\" : null,\n      \"Ports\" : {}\n    },\n    \"ResolvConfPath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/resolv.conf\",\n    \"HostnamePath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/hostname\",\n    \"HostsPath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Name\" : \"/test\",\n    \"Driver\" : \"aufs\",\n    \"ExecDriver\" : \"native-0.2\",\n    \"MountLabel\" : \"\",\n    \"ProcessLabel\" : \"\",\n    \"AppArmorProfile\" : \"\",\n    \"RestartCount\" : 0,\n    \"Mounts\" : []\n  }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<strong>500</strong> - server error</li> </ul> <h1 id=\"3-going-further\">3. Going further</h1> <h2 id=\"3-1-inside-docker-run\">3.1 Inside <code>docker run</code>\n</h2> <p>As an example, the <code>docker run</code> command line makes the following API calls:</p> <ul> <li><p>Create the container</p></li> <li>\n<p>If the status code is 404, it means the image doesn’t exist:</p> <ul> <li>Try to pull it.</li> <li>Then, retry to create the container.</li> </ul>\n</li> <li><p>Start the container.</p></li> <li><p>If you are not in detached mode:</p></li> <li><p>Attach to the container, using <code>logs=1</code> (to have <code>stdout</code> and <code>stderr</code> from the container’s start) and <code>stream=1</code></p></li> <li><p>If in detached mode or only <code>stdin</code> is attached, display the container’s id.</p></li> </ul> <h2 id=\"3-2-hijacking\">3.2 Hijacking</h2> <p>In this version of the API, <code>/attach</code>, uses hijacking to transport <code>stdin</code>, <code>stdout</code>, and <code>stderr</code> on the same socket.</p> <p>To hint potential proxies about connection hijacking, Docker client sends connection upgrade headers similarly to websocket.</p> <pre>Upgrade: tcp\nConnection: Upgrade\n</pre> <p>When Docker daemon detects the <code>Upgrade</code> header, it switches its status code from <strong>200 OK</strong> to <strong>101 UPGRADED</strong> and resends the same headers.</p> <h2 id=\"3-3-cors-requests\">3.3 CORS Requests</h2> <p>To set cross origin requests to the remote api please give values to <code>--api-cors-header</code> when running Docker in daemon mode. Set * (asterisk) allows all, default or blank means CORS disabled</p> <pre>$ docker daemon -H=\"192.168.1.9:2375\" --api-cors-header=\"http://foo.bar\"\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.20/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.20/</a>\n  </p>\n</div>\n","engine/reference/api/docker_remote_api_v1.19/index":"<h1 id=\"docker-remote-api-v1-19\">Docker Remote API v1.19</h1> <h2 id=\"1-brief-introduction\">1. Brief introduction</h2> <ul> <li>The Remote API has replaced <code>rcli</code>.</li> <li>The daemon listens on <code>unix:///var/run/docker.sock</code> but you can <a href=\"../../../quickstart/index#bind-docker-to-another-host-port-or-a-unix-socket\">Bind Docker to another host/port or a Unix socket</a>.</li> <li>The API tends to be REST. However, for some complex commands, like <code>attach</code> or <code>pull</code>, the HTTP connection is hijacked to transport <code>stdout</code>, <code>stdin</code> and <code>stderr</code>.</li> <li>When the client API version is newer than the daemon’s, these calls return an HTTP <code>400 Bad Request</code> error message.</li> </ul> <h1 id=\"2-endpoints\">2. Endpoints</h1> <h2 id=\"2-1-containers\">2.1 Containers</h2> <h3 id=\"list-containers\">List containers</h3> <p><code>GET /containers/json</code></p> <p>List containers</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/json?all=1&amp;before=8dfafdbc3a40&amp;size=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Id\": \"8dfafdbc3a40\",\n             \"Names\":[\"/boring_feynman\"],\n             \"Image\": \"ubuntu:latest\",\n             \"Command\": \"echo 1\",\n             \"Created\": 1367854155,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [{\"PrivatePort\": 2222, \"PublicPort\": 3333, \"Type\": \"tcp\"}],\n             \"Labels\": {\n                     \"com.example.vendor\": \"Acme\",\n                     \"com.example.license\": \"GPL\",\n                     \"com.example.version\": \"1.0\"\n             },\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     },\n     {\n             \"Id\": \"9cd87474be90\",\n             \"Names\":[\"/coolName\"],\n             \"Image\": \"ubuntu:latest\",\n             \"Command\": \"echo 222222\",\n             \"Created\": 1367854155,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     },\n     {\n             \"Id\": \"3176a2479c92\",\n             \"Names\":[\"/sleepy_dog\"],\n             \"Image\": \"ubuntu:latest\",\n             \"Command\": \"echo 3333333333333333\",\n             \"Created\": 1367854154,\n             \"Status\": \"Exit 0\",\n             \"Ports\":[],\n             \"Labels\": {},\n             \"SizeRw\":12288,\n             \"SizeRootFs\":0\n     },\n     {\n             \"Id\": \"4cb07b47f9fb\",\n             \"Names\":[\"/running_cat\"],\n             \"Image\": \"ubuntu:latest\",\n             \"Command\": \"echo 444444444444444444444444444444444\",\n             \"Created\": 1367854152,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     }\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, Show all containers. Only running containers are shown by default (i.e., this defaults to false)</li> <li>\n<strong>limit</strong> – Show <code>limit</code> last created containers, include non-running ones.</li> <li>\n<strong>since</strong> – Show only containers created since Id, include non-running ones.</li> <li>\n<strong>before</strong> – Show only containers created before Id, include non-running ones.</li> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, Show the containers sizes</li> <li>\n<strong>filters</strong> - a JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the containers list. Available filters: <ul> <li>\n<code>exited=&lt;int&gt;</code>; -- containers with exit code of <code>&lt;int&gt;</code> ;</li> <li>\n<code>status=</code>(<code>restarting</code>|<code>running</code>|<code>paused</code>|<code>exited</code>)</li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of a container label</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-a-container\">Create a container</h3> <p><code>POST /containers/create</code></p> <p>Create a container</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/create HTTP/1.1\nContent-Type: application/json\n\n{\n       \"Hostname\": \"\",\n       \"Domainname\": \"\",\n       \"User\": \"\",\n       \"AttachStdin\": false,\n       \"AttachStdout\": true,\n       \"AttachStderr\": true,\n       \"Tty\": false,\n       \"OpenStdin\": false,\n       \"StdinOnce\": false,\n       \"Env\": [\n               \"FOO=bar\",\n               \"BAZ=quux\"\n       ],\n       \"Cmd\": [\n               \"date\"\n       ],\n       \"Entrypoint\": \"\",\n       \"Image\": \"ubuntu\",\n       \"Labels\": {\n               \"com.example.vendor\": \"Acme\",\n               \"com.example.license\": \"GPL\",\n               \"com.example.version\": \"1.0\"\n       },\n       \"Volumes\": {\n               \"/tmp\": {}\n       },\n       \"WorkingDir\": \"\",\n       \"NetworkDisabled\": false,\n       \"MacAddress\": \"12:34:56:78:9a:bc\",\n       \"ExposedPorts\": {\n               \"22/tcp\": {}\n       },\n       \"HostConfig\": {\n         \"Binds\": [\"/tmp:/tmp\"],\n         \"Links\": [\"redis3:redis\"],\n         \"LxcConf\": {\"lxc.utsname\":\"docker\"},\n         \"Memory\": 0,\n         \"MemorySwap\": 0,\n         \"CpuShares\": 512,\n         \"CpuPeriod\": 100000,\n         \"CpuQuota\": 50000,\n         \"CpusetCpus\": \"0,1\",\n         \"CpusetMems\": \"0,1\",\n         \"BlkioWeight\": 300,\n         \"OomKillDisable\": false,\n         \"PortBindings\": { \"22/tcp\": [{ \"HostPort\": \"11022\" }] },\n         \"PublishAllPorts\": false,\n         \"Privileged\": false,\n         \"ReadonlyRootfs\": false,\n         \"Dns\": [\"8.8.8.8\"],\n         \"DnsSearch\": [\"\"],\n         \"ExtraHosts\": null,\n         \"VolumesFrom\": [\"parent\", \"other:ro\"],\n         \"CapAdd\": [\"NET_ADMIN\"],\n         \"CapDrop\": [\"MKNOD\"],\n         \"RestartPolicy\": { \"Name\": \"\", \"MaximumRetryCount\": 0 },\n         \"NetworkMode\": \"bridge\",\n         \"Devices\": [],\n         \"Ulimits\": [{}],\n         \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} },\n         \"SecurityOpt\": [],\n         \"CgroupParent\": \"\"\n      }\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 201 Created\n  Content-Type: application/json\n\n  {\n       \"Id\":\"e90e34656806\",\n       \"Warnings\":[]\n  }\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Hostname</strong> - A string value containing the hostname to use for the container.</li> <li>\n<strong>Domainname</strong> - A string value containing the domain name to use for the container.</li> <li>\n<strong>User</strong> - A string value specifying the user inside the container.</li> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code>.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code>.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code>.</li> <li>\n<strong>Tty</strong> - Boolean value, Attach standard streams to a <code>tty</code>, including <code>stdin</code> if it is not closed.</li> <li>\n<strong>OpenStdin</strong> - Boolean value, opens stdin,</li> <li>\n<strong>StdinOnce</strong> - Boolean value, close <code>stdin</code> after the 1 attached client disconnects.</li> <li>\n<strong>Env</strong> - A list of environment variables in the form of <code>[\"VAR=value\"[,\"VAR2=value2\"]]</code>\n</li> <li>\n<strong>Labels</strong> - Adds a map of labels to a container. To specify a map: <code>{\"key\":\"value\"[,\"key2\":\"value2\"]}</code>\n</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> <li>\n<strong>Entrypoint</strong> - Set the entry point for the container as a string or an array of strings.</li> <li>\n<strong>Image</strong> - A string specifying the image name to use for the container.</li> <li>\n<strong>Volumes</strong> – An object mapping mount point paths (strings) inside the container to empty objects.</li> <li>\n<strong>WorkingDir</strong> - A string specifying the working directory for commands to run in.</li> <li>\n<strong>NetworkDisabled</strong> - Boolean value, when true disables networking for the container</li> <li>\n<strong>ExposedPorts</strong> - An object mapping ports to an empty object in the form of: <code>\"ExposedPorts\": { \"&lt;port&gt;/&lt;tcp|udp&gt;: {}\" }</code>\n</li> <li>\n<strong>HostConfig</strong> <ul> <li>\n<strong>Binds</strong> – A list of volume bindings for this container. Each volume binding is a string in one of these forms: <ul> <li>\n<code>container_path</code> to create a new volume for the container</li> <li>\n<code>host_path:container_path</code> to bind-mount a host path into the container</li> <li>\n<code>host_path:container_path:ro</code> to make the bind-mount read-only inside the container.</li> </ul>\n</li> <li>\n<strong>Links</strong> - A list of links for the container. Each link entry should be in the form of <code>container_name:alias</code>.</li> <li>\n<strong>LxcConf</strong> - LXC specific configurations. These configurations only work when using the <code>lxc</code> execution driver.</li> <li>\n<strong>Memory</strong> - Memory limit in bytes.</li> <li>\n<strong>MemorySwap</strong> - Total memory limit (memory + swap); set <code>-1</code> to enable unlimited swap. You must use this with <code>memory</code> and make the swap value larger than <code>memory</code>.</li> <li>\n<strong>CpuShares</strong> - An integer value containing the container’s CPU Shares (ie. the relative weight vs other containers).</li> <li>\n<strong>CpuPeriod</strong> - The length of a CPU period in microseconds.</li> <li>\n<strong>CpuQuota</strong> - Microseconds of CPU time that the container can get in a CPU period.</li> <li>\n<strong>CpusetCpus</strong> - String value containing the <code>cgroups CpusetCpus</code> to use.</li> <li>\n<strong>CpusetMems</strong> - Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems.</li> <li>\n<strong>BlkioWeight</strong> - Block IO weight (relative weight) accepts a weight value between 10 and 1000.</li> <li>\n<strong>OomKillDisable</strong> - Boolean value, whether to disable OOM Killer for the container or not.</li> <li>\n<strong>PortBindings</strong> - A map of exposed container ports and the host port they should map to. A JSON object in the form <code>{ &lt;port&gt;/&lt;protocol&gt;: [{ \"HostPort\": \"&lt;port&gt;\" }] }</code> Take note that <code>port</code> is specified as a string and not an integer value.</li> <li>\n<strong>PublishAllPorts</strong> - Allocates a random host port for all of a container’s exposed ports. Specified as a boolean value.</li> <li>\n<strong>Privileged</strong> - Gives the container full access to the host. Specified as a boolean value.</li> <li>\n<strong>ReadonlyRootfs</strong> - Mount the container’s root filesystem as read only. Specified as a boolean value.</li> <li>\n<strong>Dns</strong> - A list of DNS servers for the container to use.</li> <li>\n<strong>DnsSearch</strong> - A list of DNS search domains</li> <li>\n<strong>ExtraHosts</strong> - A list of hostnames/IP mappings to add to the container’s <code>/etc/hosts</code> file. Specified in the form <code>[\"hostname:IP\"]</code>.</li> <li>\n<strong>VolumesFrom</strong> - A list of volumes to inherit from another container. Specified in the form <code>&lt;container name&gt;[:&lt;ro|rw&gt;]</code>\n</li> <li>\n<strong>CapAdd</strong> - A list of kernel capabilities to add to the container.</li> <li>\n<strong>Capdrop</strong> - A list of kernel capabilities to drop from the container.</li> <li>\n<strong>RestartPolicy</strong> – The behavior to apply when the container exits. The value is an object with a <code>Name</code> property of either <code>\"always\"</code> to always restart or <code>\"on-failure\"</code> to restart only when the container exit code is non-zero. If <code>on-failure</code> is used, <code>MaximumRetryCount</code> controls the number of times to retry before giving up. The default is not to restart. (optional) An ever increasing delay (double the previous delay, starting at 100mS) is added before each restart to prevent flooding the server.</li> <li>\n<strong>NetworkMode</strong> - Sets the networking mode for the container. Supported values are: <code>bridge</code>, <code>host</code>, <code>none</code>, and <code>container:&lt;name|id&gt;</code>\n</li> <li>\n<strong>Devices</strong> - A list of devices to add to the container specified as a JSON object in the form <code>{ \"PathOnHost\": \"/dev/deviceName\", \"PathInContainer\": \"/dev/deviceName\", \"CgroupPermissions\": \"mrw\"}</code>\n</li> <li>\n<strong>Ulimits</strong> - A list of ulimits to set in the container, specified as <code>{ \"Name\": &lt;name&gt;, \"Soft\": &lt;soft limit&gt;, \"Hard\": &lt;hard limit&gt; }</code>, for example: <code>Ulimits: { \"Name\": \"nofile\", \"Soft\": 1024, \"Hard\": 2048 }</code>\n</li> <li>\n<strong>SecurityOpt</strong>: A list of string values to customize labels for MLS systems, such as SELinux.</li> <li>\n<strong>LogConfig</strong> - Log configuration for the container, specified as a JSON object in the form <code>{ \"Type\": \"&lt;driver_name&gt;\", \"Config\": {\"key1\": \"val1\"}}</code>. Available types: <code>json-file</code>, <code>syslog</code>, <code>journald</code>, <code>none</code>. <code>syslog</code> available options are: <code>address</code>.</li> <li>\n<strong>CgroupParent</strong> - Path to cgroups under which the cgroup for the container will be created. If the path is not absolute, the path is considered to be relative to the cgroups path of the init process. Cgroups will be created if they do not already exist.</li> </ul>\n</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – Assign the specified name to the container. Must match <code>/?[a-zA-Z0-9_-]+</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>406</strong> – impossible to attach (container not running)</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-a-container\">Inspect a container</h3> <p><code>GET /containers/(id or name)/json</code></p> <p>Return low-level information on the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>  GET /containers/4fa6e0f0c678/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"AppArmorProfile\": \"\",\n    \"Args\": [\n        \"-c\",\n        \"exit 9\"\n    ],\n    \"Config\": {\n        \"AttachStderr\": true,\n        \"AttachStdin\": false,\n        \"AttachStdout\": true,\n        \"Cmd\": [\n            \"/bin/sh\",\n            \"-c\",\n            \"exit 9\"\n        ],\n        \"Domainname\": \"\",\n        \"Entrypoint\": null,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": null,\n        \"Hostname\": \"ba033ac44011\",\n        \"Image\": \"ubuntu\",\n        \"Labels\": {\n            \"com.example.vendor\": \"Acme\",\n            \"com.example.license\": \"GPL\",\n            \"com.example.version\": \"1.0\"\n        },\n        \"MacAddress\": \"\",\n        \"NetworkDisabled\": false,\n        \"OnBuild\": null,\n        \"OpenStdin\": false,\n        \"PortSpecs\": null,\n        \"StdinOnce\": false,\n        \"Tty\": false,\n        \"User\": \"\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\"\n    },\n    \"Created\": \"2015-01-06T15:47:31.485331387Z\",\n    \"Driver\": \"devicemapper\",\n    \"ExecDriver\": \"native-0.2\",\n    \"ExecIDs\": null,\n    \"HostConfig\": {\n        \"Binds\": null,\n        \"BlkioWeight\": 0,\n        \"CapAdd\": null,\n        \"CapDrop\": null,\n        \"ContainerIDFile\": \"\",\n        \"CpusetCpus\": \"\",\n        \"CpusetMems\": \"\",\n        \"CpuShares\": 0,\n        \"CpuPeriod\": 100000,\n        \"Devices\": [],\n        \"Dns\": null,\n        \"DnsSearch\": null,\n        \"ExtraHosts\": null,\n        \"IpcMode\": \"\",\n        \"Links\": null,\n        \"LxcConf\": [],\n        \"Memory\": 0,\n        \"MemorySwap\": 0,\n        \"OomKillDisable\": false,\n        \"NetworkMode\": \"bridge\",\n        \"PortBindings\": {},\n        \"Privileged\": false,\n        \"ReadonlyRootfs\": false,\n        \"PublishAllPorts\": false,\n        \"RestartPolicy\": {\n            \"MaximumRetryCount\": 2,\n            \"Name\": \"on-failure\"\n        },\n        \"LogConfig\": {\n            \"Config\": null,\n            \"Type\": \"json-file\"\n        },\n        \"SecurityOpt\": null,\n        \"VolumesFrom\": null,\n        \"Ulimits\": [{}]\n    },\n    \"HostnamePath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hostname\",\n    \"HostsPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Id\": \"ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39\",\n    \"Image\": \"04c5d3b7b0656168630d3ba35d8889bd0e9caafcaeb3004d2bfbc47e7c5d35d2\",\n    \"MountLabel\": \"\",\n    \"Name\": \"/boring_euclid\",\n    \"NetworkSettings\": {\n        \"Bridge\": \"\",\n        \"Gateway\": \"\",\n        \"IPAddress\": \"\",\n        \"IPPrefixLen\": 0,\n        \"MacAddress\": \"\",\n        \"PortMapping\": null,\n        \"Ports\": null\n    },\n    \"Path\": \"/bin/sh\",\n    \"ProcessLabel\": \"\",\n    \"ResolvConfPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/resolv.conf\",\n    \"RestartCount\": 1,\n    \"State\": {\n        \"Error\": \"\",\n        \"ExitCode\": 9,\n        \"FinishedAt\": \"2015-01-06T15:47:32.080254511Z\",\n        \"OOMKilled\": false,\n        \"Paused\": false,\n        \"Pid\": 0,\n        \"Restarting\": false,\n        \"Running\": false,\n        \"StartedAt\": \"2015-01-06T15:47:32.072697474Z\"\n    },\n    \"Volumes\": {},\n    \"VolumesRW\": {}\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"list-processes-running-inside-a-container\">List processes running inside a container</h3> <p><code>GET /containers/(id or name)/top</code></p> <p>List processes running inside the container <code>id</code>. On Unix systems this is done by running the <code>ps</code> command. This endpoint is not supported on Windows.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Titles\" : [\n     \"UID\", \"PID\", \"PPID\", \"C\", \"STIME\", \"TTY\", \"TIME\", \"CMD\"\n   ],\n   \"Processes\" : [\n     [\n       \"root\", \"13642\", \"882\", \"0\", \"17:03\", \"pts/0\", \"00:00:00\", \"/bin/bash\"\n     ],\n     [\n       \"root\", \"13735\", \"13642\", \"0\", \"17:06\", \"pts/0\", \"00:00:00\", \"sleep 10\"\n     ]\n   ]\n}\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top?ps_args=aux HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Titles\" : [\n    \"USER\",\"PID\",\"%CPU\",\"%MEM\",\"VSZ\",\"RSS\",\"TTY\",\"STAT\",\"START\",\"TIME\",\"COMMAND\"\n  ]\n  \"Processes\" : [\n    [\n      \"root\",\"13642\",\"0.0\",\"0.1\",\"18172\",\"3184\",\"pts/0\",\"Ss\",\"17:03\",\"0:00\",\"/bin/bash\"\n    ],\n    [\n      \"root\",\"13895\",\"0.0\",\"0.0\",\"4348\",\"692\",\"pts/0\",\"S+\",\"17:15\",\"0:00\",\"sleep 10\"\n    ]\n  ],\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>ps_args</strong> – <code>ps</code> arguments to use (e.g., <code>aux</code>), defaults to <code>-ef</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-logs\">Get container logs</h3> <p><code>GET /containers/(id or name)/logs</code></p> <p>Get <code>stdout</code> and <code>stderr</code> logs from the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: This endpoint works only for containers with the <code>json-file</code> or <code>journald</code> logging drivers.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre> GET /containers/4fa6e0f0c678/logs?stderr=1&amp;stdout=1&amp;timestamps=1&amp;follow=1&amp;tail=10&amp;since=1428990821 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre> HTTP/1.1 101 UPGRADED\n Content-Type: application/vnd.docker.raw-stream\n Connection: Upgrade\n Upgrade: tcp\n\n {{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>follow</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, show <code>stdout</code> log. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, show <code>stderr</code> log. Default <code>false</code>.</li> <li>\n<strong>since</strong> – UNIX timestamp (integer) to filter logs. Specifying a timestamp will only output log-entries since that timestamp. Default: 0 (unfiltered)</li> <li>\n<strong>timestamps</strong> – 1/True/true or 0/False/false, print timestamps for every log line. Default <code>false</code>.</li> <li>\n<strong>tail</strong> – Output specified number of lines at the end of logs: <code>all</code> or <code>&lt;number&gt;</code>. Default all.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-changes-on-a-container-s-filesystem\">Inspect changes on a container’s filesystem</h3> <p><code>GET /containers/(id or name)/changes</code></p> <p>Inspect changes on container <code>id</code>’s filesystem</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/changes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Path\": \"/dev\",\n             \"Kind\": 0\n     },\n     {\n             \"Path\": \"/dev/kmsg\",\n             \"Kind\": 1\n     },\n     {\n             \"Path\": \"/test\",\n             \"Kind\": 1\n     }\n]\n</pre> <p>Values for <code>Kind</code>:</p> <ul> <li>\n<code>0</code>: Modify</li> <li>\n<code>1</code>: Add</li> <li>\n<code>2</code>: Delete</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"export-a-container\">Export a container</h3> <p><code>GET /containers/(id or name)/export</code></p> <p>Export the contents of container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/export HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/octet-stream\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-stats-based-on-resource-usage\">Get container stats based on resource usage</h3> <p><code>GET /containers/(id or name)/stats</code></p> <p>This endpoint returns a live stream of a container’s resource usage statistics.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/redis1/stats HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Type: application/json\n\n  {\n     \"read\" : \"2015-01-08T22:57:31.547920715Z\",\n     \"network\" : {\n        \"rx_dropped\" : 0,\n        \"rx_bytes\" : 648,\n        \"rx_errors\" : 0,\n        \"tx_packets\" : 8,\n        \"tx_dropped\" : 0,\n        \"rx_packets\" : 8,\n        \"tx_errors\" : 0,\n        \"tx_bytes\" : 648\n     },\n     \"memory_stats\" : {\n        \"stats\" : {\n           \"total_pgmajfault\" : 0,\n           \"cache\" : 0,\n           \"mapped_file\" : 0,\n           \"total_inactive_file\" : 0,\n           \"pgpgout\" : 414,\n           \"rss\" : 6537216,\n           \"total_mapped_file\" : 0,\n           \"writeback\" : 0,\n           \"unevictable\" : 0,\n           \"pgpgin\" : 477,\n           \"total_unevictable\" : 0,\n           \"pgmajfault\" : 0,\n           \"total_rss\" : 6537216,\n           \"total_rss_huge\" : 6291456,\n           \"total_writeback\" : 0,\n           \"total_inactive_anon\" : 0,\n           \"rss_huge\" : 6291456,\n           \"hierarchical_memory_limit\" : 67108864,\n           \"total_pgfault\" : 964,\n           \"total_active_file\" : 0,\n           \"active_anon\" : 6537216,\n           \"total_active_anon\" : 6537216,\n           \"total_pgpgout\" : 414,\n           \"total_cache\" : 0,\n           \"inactive_anon\" : 0,\n           \"active_file\" : 0,\n           \"pgfault\" : 964,\n           \"inactive_file\" : 0,\n           \"total_pgpgin\" : 477\n        },\n        \"max_usage\" : 6651904,\n        \"usage\" : 6537216,\n        \"failcnt\" : 0,\n        \"limit\" : 67108864\n     },\n     \"blkio_stats\" : {},\n     \"cpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24472255,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100215355,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 739306590000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     },\n     \"precpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24350896,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100093996,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 9492140000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     }\n  }\n</pre> <p>The precpu_stats is the cpu statistic of last read, which is used for calculating the cpu usage percent. It is not the exact copy of the “cpu_stats” field.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, pull stats once then disconnect. Default <code>true</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"resize-a-container-tty\">Resize a container TTY</h3> <p><code>POST /containers/(id or name)/resize?h=&lt;height&gt;&amp;w=&lt;width&gt;</code></p> <p>Resize the TTY for container with <code>id</code>. You must restart the container for the resize to take effect.</p> <p><strong>Example request</strong>:</p> <pre>  POST /containers/4fa6e0f0c678/resize?h=40&amp;w=80 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Length: 0\n  Content-Type: text/plain; charset=utf-8\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – No such container</li> <li>\n<strong>500</strong> – Cannot resize container</li> </ul> <h3 id=\"start-a-container\">Start a container</h3> <p><code>POST /containers/(id or name)/start</code></p> <p>Start the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: For backwards compatibility, this endpoint accepts a <code>HostConfig</code> as JSON-encoded request body. See <a href=\"#create-a-container\">create a container</a> for details.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre> POST /containers/e90e34656806/start HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre> HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already started</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"stop-a-container\">Stop a container</h3> <p><code>POST /containers/(id or name)/stop</code></p> <p>Stop the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/stop?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already stopped</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"restart-a-container\">Restart a container</h3> <p><code>POST /containers/(id or name)/restart</code></p> <p>Restart the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/restart?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"kill-a-container\">Kill a container</h3> <p><code>POST /containers/(id or name)/kill</code></p> <p>Kill the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/kill HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters</p> <ul> <li>\n<strong>signal</strong> - Signal to send to the container: integer or string like <code>SIGINT</code>. When not set, <code>SIGKILL</code> is assumed and the call waits for the container to exit.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"rename-a-container\">Rename a container</h3> <p><code>POST /containers/(id or name)/rename</code></p> <p>Rename the container <code>id</code> to a <code>new_name</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/rename?name=new_name HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – new name for the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - conflict name already assigned</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"pause-a-container\">Pause a container</h3> <p><code>POST /containers/(id or name)/pause</code></p> <p>Pause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/pause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"unpause-a-container\">Unpause a container</h3> <p><code>POST /containers/(id or name)/unpause</code></p> <p>Unpause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/unpause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"attach-to-a-container\">Attach to a container</h3> <p><code>POST /containers/(id or name)/attach</code></p> <p>Attach to the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/attach?logs=1&amp;stream=0&amp;stdout=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 101 UPGRADED\nContent-Type: application/vnd.docker.raw-stream\nConnection: Upgrade\nUpgrade: tcp\n\n{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<p><strong>500</strong> – server error</p> <p><strong>Stream details</strong>:</p> <p>When using the TTY setting is enabled in <a href=\"#create-a-container\"><code>POST /containers/create</code> </a>, the stream is the raw data from the process PTY and client’s <code>stdin</code>. When the TTY is disabled, then the stream is multiplexed to separate <code>stdout</code> and <code>stderr</code>.</p> <p>The format is a <strong>Header</strong> and a <strong>Payload</strong> (frame).</p> <p><strong>HEADER</strong></p> <p>The header contains the information which the stream writes (<code>stdout</code> or <code>stderr</code>). It also contains the size of the associated frame encoded in the last four bytes (<code>uint32</code>).</p> <p>It is encoded on the first eight bytes like this:</p> <pre>header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}\n</pre> <p><code>STREAM_TYPE</code> can be:</p>\n</li> <li><p>0: <code>stdin</code> (is written on <code>stdout</code>)</p></li> <li><p>1: <code>stdout</code></p></li> <li>\n<p>2: <code>stderr</code></p> <p><code>SIZE1, SIZE2, SIZE3, SIZE4</code> are the four bytes of the <code>uint32</code> size encoded as big endian.</p> <p><strong>PAYLOAD</strong></p> <p>The payload is the raw stream.</p> <p><strong>IMPLEMENTATION</strong></p> <p>The simplest way to implement the Attach protocol is the following:</p> <ol> <li>Read eight bytes.</li> <li>Choose <code>stdout</code> or <code>stderr</code> depending on the first byte.</li> <li>Extract the frame size from the last four bytes.</li> <li>Read the extracted size and output it on the correct output.</li> <li>Goto 1.</li> </ol>\n</li> </ul> <h3 id=\"attach-to-a-container-websocket\">Attach to a container (websocket)</h3> <p><code>GET /containers/(id or name)/attach/ws</code></p> <p>Attach to the container <code>id</code> via websocket</p> <p>Implements websocket protocol handshake according to <a href=\"http://tools.ietf.org/html/rfc6455\">RFC 6455</a></p> <p><strong>Example request</strong></p> <pre>GET /containers/e90e34656806/attach/ws?logs=0&amp;stream=1&amp;stdin=1&amp;stdout=1&amp;stderr=1 HTTP/1.1\n</pre> <p><strong>Example response</strong></p> <pre>{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"wait-a-container\">Wait a container</h3> <p><code>POST /containers/(id or name)/wait</code></p> <p>Block until container <code>id</code> stops, then returns the exit code</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/wait HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"StatusCode\": 0}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-a-container\">Remove a container</h3> <p><code>DELETE /containers/(id or name)</code></p> <p>Remove the container <code>id</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /containers/16253994b7c4?v=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>v</strong> – 1/True/true or 0/False/false, Remove the volumes associated to the container. Default <code>false</code>.</li> <li>\n<strong>force</strong> - 1/True/true or 0/False/false, Kill then remove the container. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"copy-files-or-folders-from-a-container\">Copy files or folders from a container</h3> <p><code>POST /containers/(id or name)/copy</code></p> <p>Copy files or folders of container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/4fa6e0f0c678/copy HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Resource\": \"test.txt\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-2-images\">2.2 Images</h2> <h3 id=\"list-images\">List Images</h3> <p><code>GET /images/json</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/json?all=0 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.04\",\n       \"ubuntu:precise\",\n       \"ubuntu:latest\"\n     ],\n     \"Id\": \"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\n     \"Created\": 1365714795,\n     \"Size\": 131506275,\n     \"VirtualSize\": 131506275,\n     \"Labels\": {}\n  },\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.10\",\n       \"ubuntu:quantal\"\n     ],\n     \"ParentId\": \"27cf784147099545\",\n     \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n     \"Created\": 1364102658,\n     \"Size\": 24653,\n     \"VirtualSize\": 180116135,\n     \"Labels\": {\n        \"com.example.version\": \"v1\"\n     }\n  }\n]\n</pre> <p><strong>Example request, with digest information</strong>:</p> <pre>GET /images/json?digests=1 HTTP/1.1\n</pre> <p><strong>Example response, with digest information</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Created\": 1420064636,\n    \"Id\": \"4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125\",\n    \"ParentId\": \"ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2\",\n    \"RepoDigests\": [\n      \"localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n    ],\n    \"RepoTags\": [\n      \"localhost:5000/test/busybox:latest\",\n      \"playdate:latest\"\n    ],\n    \"Size\": 0,\n    \"VirtualSize\": 2429728,\n    \"Labels\": {}\n  }\n]\n</pre> <p>The response shows a single image <code>Id</code> associated with two repositories (<code>RepoTags</code>): <code>localhost:5000/test/busybox</code>: and <code>playdate</code>. A caller can use either of the <code>RepoTags</code> values <code>localhost:5000/test/busybox:latest</code> or <code>playdate:latest</code> to reference the image.</p> <p>You can also use <code>RepoDigests</code> values to reference an image. In this response, the array has only one reference and that is to the <code>localhost:5000/test/busybox</code> repository; the <code>playdate</code> repository has no digest. You can reference this digest using the value: <code>localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d...</code></p> <p>See the <code>docker run</code> and <code>docker build</code> commands for examples of digest and tag references on the command line.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>filters</strong> – a JSON encoded value of the filters (a map[string][]string) to process on the images list. Available filters: <ul> <li><code>dangling=true</code></li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of an image label</li> </ul>\n</li> <li>\n<strong>filter</strong> - only return images with the specified name</li> </ul> <h3 id=\"build-image-from-a-dockerfile\">Build image from a Dockerfile</h3> <p><code>POST /build</code></p> <p>Build an image from a Dockerfile</p> <p><strong>Example request</strong>:</p> <pre>POST /build HTTP/1.1\n\n{{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"stream\": \"Step 1...\"}\n{\"stream\": \"...\"}\n{\"error\": \"Error...\", \"errorDetail\": {\"code\": 123, \"message\": \"Error...\"}}\n</pre> <p>The input stream must be a <code>tar</code> archive compressed with one of the following algorithms: <code>identity</code> (no compression), <code>gzip</code>, <code>bzip2</code>, <code>xz</code>.</p> <p>The archive must include a build instructions file, typically called <code>Dockerfile</code> at the archive’s root. The <code>dockerfile</code> parameter may be used to specify a different build instructions file. To do this, its value must be the path to the alternate build instructions file to use.</p> <p>The archive may include any number of other files, which are accessible in the build context (See the <a href=\"../../builder/index#dockerbuilder\"><em>ADD build command</em></a>).</p> <p>The build is canceled if the client drops the connection by quitting or being killed.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>dockerfile</strong> - Path within the build context to the <code>Dockerfile</code>. This is ignored if <code>remote</code> is specified and points to an external <code>Dockerfile</code>.</li> <li>\n<strong>t</strong> – Repository name (and optionally a tag) to be applied to the resulting image in case of success.</li> <li>\n<strong>remote</strong> – A Git repository URI or HTTP/HTTPS context URI. If the URI points to a single text file, the file’s contents are placed into a file called <code>Dockerfile</code> and the image is built from that file. If the URI points to a tarball, the file is downloaded by the daemon and the contents therein used as the context for the build. If the URI points to a tarball and the <code>dockerfile</code> parameter is also specified, there must be a file with the corresponding path inside the tarball.</li> <li>\n<strong>q</strong> – Suppress verbose build output.</li> <li>\n<strong>nocache</strong> – Do not use the cache when building the image.</li> <li>\n<strong>pull</strong> - Attempt to pull the image even if an older image exists locally.</li> <li>\n<strong>rm</strong> - Remove intermediate containers after a successful build (default behavior).</li> <li>\n<strong>forcerm</strong> - Always remove intermediate containers (includes <code>rm</code>).</li> <li>\n<strong>memory</strong> - Set memory limit for build.</li> <li>\n<strong>memswap</strong> - Total memory (memory + swap), <code>-1</code> to enable unlimited swap.</li> <li>\n<strong>cpushares</strong> - CPU shares (relative weight).</li> <li>\n<strong>cpusetcpus</strong> - CPUs in which to allow execution (e.g., <code>0-3</code>, <code>0,1</code>).</li> <li>\n<strong>cpuperiod</strong> - The length of a CPU period in microseconds.</li> <li>\n<p><strong>cpuquota</strong> - Microseconds of CPU time that the container can get in a CPU period.</p> <p>Request Headers:</p>\n</li> <li><p><strong>Content-type</strong> – Set to <code>\"application/tar\"</code>.</p></li> <li><p><strong>X-Registry-Config</strong> – base64-encoded ConfigFile object</p></li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-an-image\">Create an image</h3> <p><code>POST /images/create</code></p> <p>Create an image either by pulling it from the registry or by importing it</p> <p><strong>Example request</strong>:</p> <pre>POST /images/create?fromImage=ubuntu HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pulling...\"}\n{\"status\": \"Pulling\", \"progress\": \"1 B/ 100 B\", \"progressDetail\": {\"current\": 1, \"total\": 100}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>When using this endpoint to pull an image from the registry, the <code>X-Registry-Auth</code> header can be used to include a base64-encoded AuthConfig object.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>fromImage</strong> – Name of the image to pull.</li> <li>\n<strong>fromSrc</strong> – Source to import. The value may be a URL from which the image can be retrieved or <code>-</code> to read the image from the request body.</li> <li>\n<strong>repo</strong> – Repository name.</li> <li>\n<p><strong>tag</strong> – Tag.</p> <p>Request Headers:</p>\n</li> <li><p><strong>X-Registry-Auth</strong> – base64-encoded AuthConfig object</p></li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-an-image\">Inspect an image</h3> <p><code>GET /images/(name)/json</code></p> <p>Return low-level information on the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/ubuntu/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n     \"Created\": \"2013-03-23T22:24:18.818426-07:00\",\n     \"Container\": \"3d67245a8d72ecf13f33dffac9f79dcdf70f75acb84d308770391510e0c23ad0\",\n     \"ContainerConfig\":\n             {\n                     \"Hostname\": \"\",\n                     \"User\": \"\",\n                     \"AttachStdin\": false,\n                     \"AttachStdout\": false,\n                     \"AttachStderr\": false,\n                     \"PortSpecs\": null,\n                     \"Tty\": true,\n                     \"OpenStdin\": true,\n                     \"StdinOnce\": false,\n                     \"Env\": null,\n                     \"Cmd\": [\"/bin/bash\"],\n                     \"Dns\": null,\n                     \"Image\": \"ubuntu\",\n                     \"Labels\": {\n                         \"com.example.vendor\": \"Acme\",\n                         \"com.example.license\": \"GPL\",\n                         \"com.example.version\": \"1.0\"\n                     },\n                     \"Volumes\": null,\n                     \"VolumesFrom\": \"\",\n                     \"WorkingDir\": \"\"\n             },\n     \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n     \"Parent\": \"27cf784147099545\",\n     \"Size\": 6824592\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-the-history-of-an-image\">Get the history of an image</h3> <p><code>GET /images/(name)/history</code></p> <p>Return the history of the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/ubuntu/history HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n    {\n        \"Id\": \"3db9c44f45209632d6050b35958829c3a2aa256d81b9a7be45b362ff85c54710\",\n        \"Created\": 1398108230,\n        \"CreatedBy\": \"/bin/sh -c #(nop) ADD file:eb15dbd63394e063b805a3c32ca7bf0266ef64676d5a6fab4801f2e81e2a5148 in /\",\n        \"Tags\": [\n            \"ubuntu:lucid\",\n            \"ubuntu:10.04\"\n        ],\n        \"Size\": 182964289,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"6cfa4d1f33fb861d4d114f43b25abd0ac737509268065cdfd69d544a59c85ab8\",\n        \"Created\": 1398108222,\n        \"CreatedBy\": \"/bin/sh -c #(nop) MAINTAINER Tianon Gravi &lt;admwiggin@gmail.com&gt; - mkimage-debootstrap.sh -i iproute,iputils-ping,ubuntu-minimal -t lucid.tar.xz lucid http://archive.ubuntu.com/ubuntu/\",\n        \"Tags\": null,\n        \"Size\": 0,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\n        \"Created\": 1371157430,\n        \"CreatedBy\": \"\",\n        \"Tags\": [\n            \"scratch12:latest\",\n            \"scratch:latest\"\n        ],\n        \"Size\": 0,\n        \"Comment\": \"Imported from -\"\n    }\n]\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"push-an-image-on-the-registry\">Push an image on the registry</h3> <p><code>POST /images/(name)/push</code></p> <p>Push the image <code>name</code> on the registry</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/push HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pushing...\"}\n{\"status\": \"Pushing\", \"progress\": \"1/? (n/a)\", \"progressDetail\": {\"current\": 1}}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>If you wish to push an image on to a private registry, that image must already have a tag into a repository which references that registry <code>hostname</code> and <code>port</code>. This repository name should then be used in the URL. This duplicates the command line’s flow.</p> <p><strong>Example request</strong>:</p> <pre>POST /images/registry.acme.com:5000/test/push HTTP/1.1\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>tag</strong> – The tag to associate with the image on the registry. This is optional.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>X-Registry-Auth</strong> – Include a base64-encoded AuthConfig. object.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"tag-an-image-into-a-repository\">Tag an image into a repository</h3> <p><code>POST /images/(name)/tag</code></p> <p>Tag the image <code>name</code> into a repository</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/tag?repo=myrepo&amp;force=0&amp;tag=v42 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>repo</strong> – The repository to tag in</li> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>tag</strong> - The new tag name</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-an-image\">Remove an image</h3> <p><code>DELETE /images/(name)</code></p> <p>Remove the image <code>name</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /images/test HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-type: application/json\n\n[\n {\"Untagged\": \"3e2f21a89f\"},\n {\"Deleted\": \"3e2f21a89f\"},\n {\"Deleted\": \"53b4f83ac9\"}\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>noprune</strong> – 1/True/true or 0/False/false, default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"search-images\">Search images</h3> <p><code>GET /images/search</code></p> <p>Search for an image on <a href=\"https://hub.docker.com\">Docker Hub</a>. This API returns both <code>is_trusted</code> and <code>is_automated</code> images. Currently, they are considered identical. In the future, the <code>is_trusted</code> property will be deprecated and replaced by the <code>is_automated</code> property.</p> <blockquote> <p><strong>Note</strong>: The response keys have changed from API v1.6 to reflect the JSON sent by the registry server to the docker daemon’s request.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>GET /images/search?term=sshd HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n    [\n            {\n                \"star_count\": 12,\n                \"is_official\": false,\n                \"name\": \"wma55/u1210sshd\",\n                \"is_trusted\": false,\n                \"is_automated\": false,\n                \"description\": \"\",\n            },\n            {\n                \"star_count\": 10,\n                \"is_official\": false,\n                \"name\": \"jdswinbank/sshd\",\n                \"is_trusted\": false,\n                \"is_automated\": false,\n                \"description\": \"\",\n            },\n            {\n                \"star_count\": 18,\n                \"is_official\": false,\n                \"name\": \"vgauthier/sshd\",\n                \"is_trusted\": false,\n                \"is_automated\": false,\n                \"description\": \"\",\n            }\n    ...\n    ]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>term</strong> – term to search</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-3-misc\">2.3 Misc</h2> <h3 id=\"check-auth-configuration\">Check auth configuration</h3> <p><code>POST /auth</code></p> <p>Get the default username and email</p> <p><strong>Example request</strong>:</p> <pre>POST /auth HTTP/1.1\nContent-Type: application/json\n\n{\n     \"username\":\" hannibal\",\n     \"password: \"xxxx\",\n     \"email\": \"hannibal@a-team.com\",\n     \"serveraddress\": \"https://index.docker.io/v1/\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"display-system-wide-information\">Display system-wide information</h3> <p><code>GET /info</code></p> <p>Display system-wide information</p> <p><strong>Example request</strong>:</p> <pre>GET /info HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"Containers\": 11,\n    \"CpuCfsPeriod\": true,\n    \"CpuCfsQuota\": true,\n    \"Debug\": false,\n    \"DockerRootDir\": \"/var/lib/docker\",\n    \"Driver\": \"btrfs\",\n    \"DriverStatus\": [[\"\"]],\n    \"ExecutionDriver\": \"native-0.1\",\n    \"ExperimentalBuild\": false,\n    \"HttpProxy\": \"http://test:test@localhost:8080\",\n    \"HttpsProxy\": \"https://test:test@localhost:8080\",\n    \"ID\": \"7TRN:IPZB:QYBB:VPBQ:UMPP:KARE:6ZNR:XE6T:7EWV:PKF4:ZOJD:TPYS\",\n    \"IPv4Forwarding\": true,\n    \"Images\": 16,\n    \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n    \"InitPath\": \"/usr/bin/docker\",\n    \"InitSha1\": \"\",\n    \"KernelVersion\": \"3.12.0-1-amd64\",\n    \"Labels\": [\n        \"storage=ssd\"\n    ],\n    \"MemTotal\": 2099236864,\n    \"MemoryLimit\": true,\n    \"NCPU\": 1,\n    \"NEventsListener\": 0,\n    \"NFd\": 11,\n    \"NGoroutines\": 21,\n    \"Name\": \"prod-server-42\",\n    \"NoProxy\": \"9.81.1.160\",\n    \"OomKillDisable\": true,\n    \"OperatingSystem\": \"Boot2Docker\",\n    \"RegistryConfig\": {\n        \"IndexConfigs\": {\n            \"docker.io\": {\n                \"Mirrors\": null,\n                \"Name\": \"docker.io\",\n                \"Official\": true,\n                \"Secure\": true\n            }\n        },\n        \"InsecureRegistryCIDRs\": [\n            \"127.0.0.0/8\"\n        ]\n    },\n    \"SwapLimit\": false,\n    \"SystemTime\": \"2015-03-10T11:11:23.730591467-07:00\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"show-the-docker-version-information\">Show the docker version information</h3> <p><code>GET /version</code></p> <p>Show the docker version information</p> <p><strong>Example request</strong>:</p> <pre>GET /version HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n     \"Version\": \"1.5.0\",\n     \"Os\": \"linux\",\n     \"KernelVersion\": \"3.18.5-tinycore64\",\n     \"GoVersion\": \"go1.4.1\",\n     \"GitCommit\": \"a8a31ef\",\n     \"Arch\": \"amd64\",\n     \"ApiVersion\": \"1.19\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"ping-the-docker-server\">Ping the docker server</h3> <p><code>GET /_ping</code></p> <p>Ping the docker server</p> <p><strong>Example request</strong>:</p> <pre>GET /_ping HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: text/plain\n\nOK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-new-image-from-a-container-s-changes\">Create a new image from a container’s changes</h3> <p><code>POST /commit</code></p> <p>Create a new image from a container’s changes</p> <p><strong>Example request</strong>:</p> <pre>POST /commit?container=44c004db4b17&amp;comment=message&amp;repo=myrepo HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Hostname\": \"\",\n     \"Domainname\": \"\",\n     \"User\": \"\",\n     \"AttachStdin\": false,\n     \"AttachStdout\": true,\n     \"AttachStderr\": true,\n     \"PortSpecs\": null,\n     \"Tty\": false,\n     \"OpenStdin\": false,\n     \"StdinOnce\": false,\n     \"Env\": null,\n     \"Cmd\": [\n             \"date\"\n     ],\n     \"Volumes\": {\n             \"/tmp\": {}\n     },\n     \"Labels\": {\n             \"key1\": \"value1\",\n             \"key2\": \"value2\"\n      },\n     \"WorkingDir\": \"\",\n     \"NetworkDisabled\": false,\n     \"ExposedPorts\": {\n             \"22/tcp\": {}\n     }\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\"Id\": \"596069db4bf5\"}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>config</strong> - the container’s configuration</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>container</strong> – source container</li> <li>\n<strong>repo</strong> – repository</li> <li>\n<strong>tag</strong> – tag</li> <li>\n<strong>comment</strong> – commit message</li> <li>\n<strong>author</strong> – author (e.g., “John Hannibal Smith &lt;<a href=\"mailto:hannibal%40a-team.com\">hannibal@a-team.com</a>&gt;“)</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"monitor-docker-s-events\">Monitor Docker’s events</h3> <p><code>GET /events</code></p> <p>Get container events from docker, either in real time via streaming, or via polling (using since).</p> <p>Docker containers report the following events:</p> <pre>attach, commit, copy, create, destroy, die, exec_create, exec_start, export, kill, oom, pause, rename, resize, restart, start, stop, top, unpause\n</pre> <p>and Docker images report:</p> <pre>untag, delete\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /events?since=1374067924\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"create\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067924}\n{\"status\": \"start\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067924}\n{\"status\": \"stop\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067966}\n{\"status\": \"destroy\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067970}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>since</strong> – Timestamp used for polling</li> <li>\n<strong>until</strong> – Timestamp used for polling</li> <li>\n<strong>filters</strong> – A json encoded value of the filters (a map[string][]string) to process on the event list. Available filters: <ul> <li>\n<code>event=&lt;string&gt;</code>; -- event to filter</li> <li>\n<code>image=&lt;string&gt;</code>; -- image to filter</li> <li>\n<code>container=&lt;string&gt;</code>; -- container to filter</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images-in-a-repository\">Get a tarball containing all images in a repository</h3> <p><code>GET /images/(name)/get</code></p> <p>Get a tarball containing all images and metadata for the repository specified by <code>name</code>.</p> <p>If <code>name</code> is a specific name and tag (e.g. ubuntu:latest), then only that image (and its parents) are returned. If <code>name</code> is an image ID, similarly only that image (and its parents) are returned, but with the exclusion of the ‘repositories’ file in the tarball, as there were no image names referenced.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/ubuntu/get\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images\">Get a tarball containing all images.</h3> <p><code>GET /images/get</code></p> <p>Get a tarball containing all images and metadata for one or more repositories.</p> <p>For each value of the <code>names</code> parameter: if it is a specific name and tag (e.g. <code>ubuntu:latest</code>), then only that image (and its parents) are returned; if it is an image ID, similarly only that image (and its parents) are returned and there would be no names referenced in the ‘repositories’ file for this image ID.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/get?names=myname%2Fmyapp%3Alatest&amp;names=busybox\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"load-a-tarball-with-a-set-of-images-and-tags-into-docker\">Load a tarball with a set of images and tags into docker</h3> <p><code>POST /images/load</code></p> <p>Load a set of images and tags into a Docker repository. See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>POST /images/load\n\nTarball in body\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"image-tarball-format\">Image tarball format</h3> <p>An image tarball contains one directory per image layer (named using its long ID), each containing these files:</p> <ul> <li>\n<code>VERSION</code>: currently <code>1.0</code> - the file format version</li> <li>\n<code>json</code>: detailed layer information, similar to <code>docker inspect layer_id</code>\n</li> <li>\n<code>layer.tar</code>: A tarfile containing the filesystem changes in this layer</li> </ul> <p>The <code>layer.tar</code> file contains <code>aufs</code> style <code>.wh..wh.aufs</code> files and directories for storing attribute changes and deletions.</p> <p>If the tarball defines a repository, the tarball should also include a <code>repositories</code> file at the root that contains a list of repository and tag names mapped to layer IDs.</p> <pre>{\"hello-world\":\n    {\"latest\": \"565a9d68a73f6706862bfe8409a7f659776d4d60a8d096eb4a3cbce6999cc2a1\"}\n}\n</pre> <h3 id=\"exec-create\">Exec Create</h3> <p><code>POST /containers/(id or name)/exec</code></p> <p>Sets up an exec instance in a running container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/exec HTTP/1.1\nContent-Type: application/json\n\n  {\n   \"AttachStdin\": false,\n   \"AttachStdout\": true,\n   \"AttachStderr\": true,\n   \"Tty\": false,\n   \"Cmd\": [\n                 \"date\"\n         ],\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n     \"Id\": \"f90e34656806\",\n     \"Warnings\":[]\n}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code> of the <code>exec</code> command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> </ul> <h3 id=\"exec-start\">Exec Start</h3> <p><code>POST /exec/(id)/start</code></p> <p>Starts a previously set up <code>exec</code> instance <code>id</code>. If <code>detach</code> is true, this API returns after starting the <code>exec</code> command. Otherwise, this API sets up an interactive session with the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/start HTTP/1.1\nContent-Type: application/json\n\n{\n \"Detach\": false,\n \"Tty\": false,\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/vnd.docker.raw-stream\n\n{{ STREAM }}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Detach</strong> - Detach from the <code>exec</code> command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<p><strong>404</strong> – no such exec instance</p> <p><strong>Stream details</strong>: Similar to the stream behavior of <code>POST /containers/(id or name)/attach</code> API</p>\n</li> </ul> <h3 id=\"exec-resize\">Exec Resize</h3> <p><code>POST /exec/(id)/resize</code></p> <p>Resizes the <code>tty</code> session used by the <code>exec</code> command <code>id</code>. This API is valid only if <code>tty</code> was specified as part of creating and starting the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/resize HTTP/1.1\nContent-Type: text/plain\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: text/plain\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of <code>tty</code> session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> </ul> <h3 id=\"exec-inspect\">Exec Inspect</h3> <p><code>GET /exec/(id)/json</code></p> <p>Return low-level information about the <code>exec</code> command <code>id</code>.</p> <p><strong>Example request</strong>:</p> <pre>GET /exec/11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: plain/text\n\n{\n  \"ID\" : \"11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39\",\n  \"Running\" : false,\n  \"ExitCode\" : 2,\n  \"ProcessConfig\" : {\n    \"privileged\" : false,\n    \"user\" : \"\",\n    \"tty\" : false,\n    \"entrypoint\" : \"sh\",\n    \"arguments\" : [\n      \"-c\",\n      \"exit 2\"\n    ]\n  },\n  \"OpenStdin\" : false,\n  \"OpenStderr\" : false,\n  \"OpenStdout\" : false,\n  \"Container\" : {\n    \"State\" : {\n      \"Running\" : true,\n      \"Paused\" : false,\n      \"Restarting\" : false,\n      \"OOMKilled\" : false,\n      \"Pid\" : 3650,\n      \"ExitCode\" : 0,\n      \"Error\" : \"\",\n      \"StartedAt\" : \"2014-11-17T22:26:03.717657531Z\",\n      \"FinishedAt\" : \"0001-01-01T00:00:00Z\"\n    },\n    \"ID\" : \"8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c\",\n    \"Created\" : \"2014-11-17T22:26:03.626304998Z\",\n    \"Path\" : \"date\",\n    \"Args\" : [],\n    \"Config\" : {\n      \"Hostname\" : \"8f177a186b97\",\n      \"Domainname\" : \"\",\n      \"User\" : \"\",\n      \"AttachStdin\" : false,\n      \"AttachStdout\" : false,\n      \"AttachStderr\" : false,\n      \"PortSpecs\": null,\n      \"ExposedPorts\" : null,\n      \"Tty\" : false,\n      \"OpenStdin\" : false,\n      \"StdinOnce\" : false,\n      \"Env\" : [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ],\n      \"Cmd\" : [\n        \"date\"\n      ],\n      \"Image\" : \"ubuntu\",\n      \"Volumes\" : null,\n      \"WorkingDir\" : \"\",\n      \"Entrypoint\" : null,\n      \"NetworkDisabled\" : false,\n      \"MacAddress\" : \"\",\n      \"OnBuild\" : null,\n      \"SecurityOpt\" : null\n    },\n    \"Image\" : \"5506de2b643be1e6febbf3b8a240760c6843244c41e12aa2f60ccbb7153d17f5\",\n    \"NetworkSettings\" : {\n      \"IPAddress\" : \"172.17.0.2\",\n      \"IPPrefixLen\" : 16,\n      \"MacAddress\" : \"02:42:ac:11:00:02\",\n      \"Gateway\" : \"172.17.42.1\",\n      \"Bridge\" : \"docker0\",\n      \"PortMapping\" : null,\n      \"Ports\" : {}\n    },\n    \"ResolvConfPath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/resolv.conf\",\n    \"HostnamePath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/hostname\",\n    \"HostsPath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Name\" : \"/test\",\n    \"Driver\" : \"aufs\",\n    \"ExecDriver\" : \"native-0.2\",\n    \"MountLabel\" : \"\",\n    \"ProcessLabel\" : \"\",\n    \"AppArmorProfile\" : \"\",\n    \"RestartCount\" : 0,\n    \"Volumes\" : {},\n    \"VolumesRW\" : {}\n  }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<strong>500</strong> - server error</li> </ul> <h1 id=\"3-going-further\">3. Going further</h1> <h2 id=\"3-1-inside-docker-run\">3.1 Inside <code>docker run</code>\n</h2> <p>As an example, the <code>docker run</code> command line makes the following API calls:</p> <ul> <li><p>Create the container</p></li> <li>\n<p>If the status code is 404, it means the image doesn’t exist:</p> <ul> <li>Try to pull it.</li> <li>Then, retry to create the container.</li> </ul>\n</li> <li><p>Start the container.</p></li> <li><p>If you are not in detached mode:</p></li> <li><p>Attach to the container, using <code>logs=1</code> (to have <code>stdout</code> and <code>stderr</code> from the container’s start) and <code>stream=1</code></p></li> <li><p>If in detached mode or only <code>stdin</code> is attached, display the container’s id.</p></li> </ul> <h2 id=\"3-2-hijacking\">3.2 Hijacking</h2> <p>In this version of the API, <code>/attach</code>, uses hijacking to transport <code>stdin</code>, <code>stdout</code>, and <code>stderr</code> on the same socket.</p> <p>To hint potential proxies about connection hijacking, Docker client sends connection upgrade headers similarly to websocket.</p> <pre>Upgrade: tcp\nConnection: Upgrade\n</pre> <p>When Docker daemon detects the <code>Upgrade</code> header, it switches its status code from <strong>200 OK</strong> to <strong>101 UPGRADED</strong> and resends the same headers.</p> <h2 id=\"3-3-cors-requests\">3.3 CORS Requests</h2> <p>To set cross origin requests to the remote api please give values to <code>--api-cors-header</code> when running Docker in daemon mode. Set * (asterisk) allows all, default or blank means CORS disabled</p> <pre>$ docker -d -H=\"192.168.1.9:2375\" --api-cors-header=\"http://foo.bar\"\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.19/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.19/</a>\n  </p>\n</div>\n","swarm/install-w-machine/index":"<h1 id=\"evaluate-swarm-in-a-sandbox\">Evaluate Swarm in a sandbox</h1> <p>This getting started example shows you how to create a Docker Swarm, the native clustering tool for Docker.</p> <p>You’ll use Docker Toolbox to install Docker Machine and some other tools on your computer. Then you’ll use Docker Machine to provision a set of Docker Engine hosts. Lastly, you’ll use Docker client to connect to the hosts, where you’ll create a discovery token, create a cluster of one Swarm manager and nodes, and manage the cluster.</p> <p>When you finish, you’ll have a Docker Swarm up and running in VirtualBox on your local Mac or Windows computer. You can use this Swarm as personal development sandbox.</p> <p>To use Docker Swarm on Linux, see <a href=\"../install-manual/index\">Build a Swarm cluster for production</a>.</p> <h2 id=\"install-docker-toolbox\">Install Docker Toolbox</h2> <p>Download and install <a href=\"https://www.docker.com/docker-toolbox\">Docker Toolbox</a>.</p> <p>The toolbox installs a handful of tools on your local Windows or Mac OS X computer. In this exercise, you use three of those tools:</p> <ul> <li>Docker Machine: To deploy virtual machines that run Docker Engine.</li> <li>VirtualBox: To host the virtual machines deployed from Docker Machine.</li> <li>Docker Client: To connect from your local computer to the Docker Engines on the VMs and issue docker commands to create the Swarm.</li> </ul> <p>The following sections provide more information on each of these tools. The rest of the document uses the abbreviation, VM, for virtual machine.</p> <h2 id=\"create-three-vms-running-docker-engine\">Create three VMs running Docker Engine</h2> <p>Here, you use Docker Machine to provision three VMs running Docker Engine.</p> <ol> <li>\n<p>Open a terminal on your computer. Use Docker Machine to list any VMs in VirtualBox.</p> <pre>$ docker-machine ls\nNAME         ACTIVE   DRIVER       STATE     URL                         SWARM\ndefault    *        virtualbox   Running   tcp://192.168.99.100:2376\n</pre>\n</li> <li>\n<p>Optional: To conserve system resources, stop any virtual machines you are not using. For example, to stop the VM named <code>default</code>, enter:</p> <pre>$ docker-machine stop default\n</pre>\n</li> <li>\n<p>Create and run a VM named <code>manager</code>.</p> <pre>$ docker-machine create -d virtualbox manager\n</pre>\n</li> <li>\n<p>Create and run a VM named <code>agent1</code>.</p> <pre>$ docker-machine create -d virtualbox agent1\n</pre>\n</li> <li>\n<p>Create and run a VM named <code>agent2</code>.</p> <pre>$ docker-machine create -d virtualbox agent2\n</pre>\n</li> </ol> <p>Each create command checks for a local copy of the <em>latest</em> VM image, called boot2docker.iso. If it isn’t available, Docker Machine downloads the image from Docker Hub. Then, Docker Machine uses boot2docker.iso to create a VM that automatically runs Docker Engine.</p> <blockquote> <p>Troubleshooting: If your computer or hosts cannot reach Docker Hub, the <code>docker-machine</code> or <code>docker run</code> commands that pull images may fail. In that case, check the <a href=\"http://status.docker.com/\">Docker Hub status page</a> for service availability. Then, check whether your computer is connected to the Internet. Finally, check whether VirtualBox’s network settings allow your hosts to connect to the Internet.</p> </blockquote> <h2 id=\"create-a-swarm-discovery-token\">Create a Swarm discovery token</h2> <p>Here you use the discovery backend hosted on Docker Hub to create a unique discovery token for your cluster. This discovery backend is only for low-volume development and testing purposes, not for production. Later on, when you run the Swarm manager and nodes, they register with the discovery backend as members of the cluster that’s associated with the unique token. The discovery backend maintains an up-to-date list of cluster members and shares that list with the Swarm manager. The Swarm manager uses this list to assign tasks to the nodes.</p> <ol> <li>\n<p>Connect the Docker Client on your computer to the Docker Engine running on <code>manager</code>.</p> <pre>$ eval $(docker-machine env manager)\n</pre> <p>The client will send the <code>docker</code> commands in the following steps to the Docker Engine on <code>manager</code>.</p>\n</li> <li>\n<p>Create a unique id for the Swarm cluster.</p> <pre>$ docker run --rm swarm create\n.\n.\n.\nStatus: Downloaded newer image for swarm:latest\n0ac50ef75c9739f5bfeeaf00503d4e6e\n</pre> <p>The <code>docker run</code> command gets the latest <code>swarm</code> image and runs it as a container. The <code>create</code> argument makes the Swarm container connect to the Docker Hub discovery service and get a unique Swarm ID, also known as a “discovery token”. The token appears in the output, it is not saved to a file on the host. The <code>--rm</code> option automatically cleans up the container and removes the file system when the container exits.</p> <p>The discovery service keeps unused tokens for approximately one week.</p>\n</li> <li><p>Copy the discovery token from the last line of the previous output to a safe place.</p></li> </ol> <h2 id=\"create-the-swarm-manager-and-nodes\">Create the Swarm manager and nodes</h2> <p>Here, you connect to each of the hosts and create a Swarm manager or node.</p> <ol> <li>\n<p>List the VMs to check that they’re set up and running. For example:</p> <pre>$ docker-machine ls\nNAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS\nagent1    -        virtualbox   Running   tcp://192.168.99.102:2376           v1.9.1\nagent2    -        virtualbox   Running   tcp://192.168.99.103:2376           v1.9.1\nmanager   *        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1\n</pre>\n</li> <li>\n<p>Your client should still be pointing to Docker Engine on <code>manager</code>. Use the following syntax to run a Swarm container that functions as the primary manager on <code>manager</code>.</p> <pre>$ docker run -d -p &lt;your_selected_port&gt;:3376 -t -v /var/lib/boot2docker:/certs:ro swarm manage -H 0.0.0.0:3376 --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/server.pem --tlskey=/certs/server-key.pem token://&lt;cluster_id&gt;\n</pre> <p>For example:</p> <pre>$ docker run -d -p 3376:3376 -t -v /var/lib/boot2docker:/certs:ro swarm manage -H 0.0.0.0:3376 --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/server.pem --tlskey=/certs/server-key.pem token://0ac50ef75c9739f5bfeeaf00503d4e6e\n</pre> <p>The <code>-p</code> option maps a port 3376 on the container to port 3376 on the host. The <code>-v</code> option mounts the directory containing TLS certificates (<code>/var/lib/boot2docker</code> for the <code>manager</code> VM) into the container running Swarm manager in read-only mode.</p>\n</li> <li>\n<p>Connect Docker Client to <code>agent1</code>.</p> <pre>$ eval $(docker-machine env agent1)\n</pre>\n</li> <li>\n<p>Use the following syntax to run a Swarm container that functions as an agent on <code>agent1</code>. Replace <code>&lt;node_ip&gt;</code> with the IP address of the VM from above, or use the <code>docker-machine ip</code> command.</p> <pre>$ docker run -d swarm join --addr=&lt;node_ip&gt;:&lt;node_port&gt; token://&lt;cluster_id&gt;\n</pre> <p>For example:</p> <pre>$ docker run -d swarm join --addr=$(docker-machine ip agent1):2376 token://0ac50ef75c9739f5bfeeaf00503d4e6e\n</pre>\n</li> <li>\n<p>Connect Docker Client to <code>agent2</code>.</p> <pre>$ eval $(docker-machine env agent2)\n</pre>\n</li> <li>\n<p>Run a Swarm container as an agent on <code>agent2</code>. For example:</p> <pre>$ docker run -d swarm join --addr=$(docker-machine ip agent2):2376 token://0ac50ef75c9739f5bfeeaf00503d4e6e\n</pre>\n</li> </ol> <h2 id=\"manage-your-swarm\">Manage your Swarm</h2> <p>Here, you connect to the cluster and review information about the Swarm manager and nodes. You tell the Swarm to run a container and check which node did the work.</p> <ol> <li>\n<p>Connect the Docker Client to the Swarm by updating the <code>DOCKER_HOST</code> environment variable.</p> <pre>$ DOCKER_HOST=&lt;manager_ip&gt;:&lt;your_selected_port&gt;\n</pre> <p>We use the <code>docker-machine ip</code> command, and we selected port 3376 for the Swarm manager.</p> <pre>$ DOCKER_HOST=$(docker-machine ip manager):3376\n</pre> <p>Because Docker Swarm uses the standard Docker API, you can connect to it using Docker Client and other tools such as Docker Compose, Dokku, Jenkins, and Krane, among others.</p>\n</li> <li>\n<p>Get information about the Swarm.</p> <pre>$ docker info\n</pre> <p>As you can see, the output displays information about the two agent nodes and the one manager node in the Swarm.</p>\n</li> <li>\n<p>Check the images currently running on your Swarm.</p> <pre>$ docker ps\n</pre>\n</li> <li>\n<p>Run a container on the Swarm.</p> <pre>$ docker run hello-world\nHello from Docker.\n.\n.\n.\n</pre>\n</li> <li>\n<p>Use the <code>docker ps</code> command to find out which node the container ran on. For example:</p> <pre>$ docker ps -a\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\n0b0628349187        hello-world         \"/hello\"                 20 minutes ago      Exited (0) 11 seconds ago                       agent1\n.\n.\n.\n</pre> <p>In this case, the Swarm ran <code>hello-world</code> on <code>agent1</code>.</p> <p>By default, Docker Swarm uses the “spread” strategy to choose which node runs a container. When you run multiple containers, the spread strategy assigns each container to the node with the fewest containers.</p>\n</li> </ol> <h2 id=\"where-to-go-next\">Where to go next</h2> <p>At this point, you’ve done the following:</p> <ul> <li>Created a Swarm discovery token.</li> <li>Created Swarm nodes using Docker Machine.</li> <li>Managed a Swarm cluster and run containers on it.</li> <li>Learned Swarm-related concepts and terminology.</li> </ul> <p>However, Docker Swarm has many other aspects and capabilities. For more information, visit <a href=\"https://www.docker.com/docker-swarm\">the Swarm landing page</a> or read the <a href=\"https://docs.docker.com/swarm/\">Swarm documentation</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/install-w-machine/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/install-w-machine/</a>\n  </p>\n</div>\n","engine/reference/api/docker_remote_api_v1.18/index":"<h1 id=\"docker-remote-api-v1-18\">Docker Remote API v1.18</h1> <h2 id=\"1-brief-introduction\">1. Brief introduction</h2> <ul> <li>The Remote API has replaced <code>rcli</code>.</li> <li>The daemon listens on <code>unix:///var/run/docker.sock</code> but you can <a href=\"../../../quickstart/index#bind-docker-to-another-host-port-or-a-unix-socket\">Bind Docker to another host/port or a Unix socket</a>.</li> <li>The API tends to be REST, but for some complex commands, like <code>attach</code> or <code>pull</code>, the HTTP connection is hijacked to transport <code>STDOUT</code>, <code>STDIN</code> and <code>STDERR</code>.</li> </ul> <h1 id=\"2-endpoints\">2. Endpoints</h1> <h2 id=\"2-1-containers\">2.1 Containers</h2> <h3 id=\"list-containers\">List containers</h3> <p><code>GET /containers/json</code></p> <p>List containers</p> <p><strong>Example request</strong>:</p> <pre>    GET /containers/json?all=1&amp;before=8dfafdbc3a40&amp;size=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    [\n         {\n                 \"Id\": \"8dfafdbc3a40\",\n                 \"Names\":[\"/boring_feynman\"],\n                 \"Image\": \"ubuntu:latest\",\n                 \"Command\": \"echo 1\",\n                 \"Created\": 1367854155,\n                 \"Status\": \"Exit 0\",\n                 \"Ports\": [{\"PrivatePort\": 2222, \"PublicPort\": 3333, \"Type\": \"tcp\"}],\n                 \"Labels\": {\n                         \"com.example.vendor\": \"Acme\",\n                         \"com.example.license\": \"GPL\",\n                         \"com.example.version\": \"1.0\"\n                 },\n                 \"SizeRw\": 12288,\n                 \"SizeRootFs\": 0\n         },\n         {\n                 \"Id\": \"9cd87474be90\",\n                 \"Names\":[\"/coolName\"],\n                 \"Image\": \"ubuntu:latest\",\n                 \"Command\": \"echo 222222\",\n                 \"Created\": 1367854155,\n                 \"Status\": \"Exit 0\",\n                 \"Ports\": [],\n                 \"Labels\": {},\n                 \"SizeRw\": 12288,\n                 \"SizeRootFs\": 0\n         },\n         {\n                 \"Id\": \"3176a2479c92\",\n                 \"Names\":[\"/sleepy_dog\"],\n                 \"Image\": \"ubuntu:latest\",\n                 \"Command\": \"echo 3333333333333333\",\n                 \"Created\": 1367854154,\n                 \"Status\": \"Exit 0\",\n                 \"Ports\":[],\n                 \"Labels\": {},\n                 \"SizeRw\":12288,\n                 \"SizeRootFs\":0\n         },\n         {\n                 \"Id\": \"4cb07b47f9fb\",\n                 \"Names\":[\"/running_cat\"],\n                 \"Image\": \"ubuntu:latest\",\n                 \"Command\": \"echo 444444444444444444444444444444444\",\n                 \"Created\": 1367854152,\n                 \"Status\": \"Exit 0\",\n                 \"Ports\": [],\n                 \"Labels\": {},\n                 \"SizeRw\": 12288,\n                 \"SizeRootFs\": 0\n         }\n    ]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, Show all containers. Only running containers are shown by default (i.e., this defaults to false)</li> <li>\n<strong>limit</strong> – Show <code>limit</code> last created containers, include non-running ones.</li> <li>\n<strong>since</strong> – Show only containers created since Id, include non-running ones.</li> <li>\n<strong>before</strong> – Show only containers created before Id, include non-running ones.</li> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, Show the containers sizes</li> <li>\n<strong>filters</strong> - a json encoded value of the filters (a map[string][]string) to process on the containers list. Available filters: <ul> <li>exited=&lt;int&gt; -- containers with exit code of &lt;int&gt;</li> <li>status=(restarting|running|paused|exited)</li> <li>label=<code>key</code> or <code>label=\"key=value\"</code> of a container label</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-a-container\">Create a container</h3> <p><code>POST /containers/create</code></p> <p>Create a container</p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/create HTTP/1.1\n    Content-Type: application/json\n\n    {\n         \"Hostname\": \"\",\n         \"Domainname\": \"\",\n         \"User\": \"\",\n         \"AttachStdin\": false,\n         \"AttachStdout\": true,\n         \"AttachStderr\": true,\n         \"Tty\": false,\n         \"OpenStdin\": false,\n         \"StdinOnce\": false,\n         \"Env\": [\n                 \"FOO=bar\",\n                 \"BAZ=quux\"\n         ],\n         \"Cmd\": [\n                 \"date\"\n         ],\n         \"Entrypoint\": \"\",\n         \"Image\": \"ubuntu\",\n         \"Labels\": {\n                 \"com.example.vendor\": \"Acme\",\n                 \"com.example.license\": \"GPL\",\n                 \"com.example.version\": \"1.0\"\n         },\n         \"Volumes\": {\n                 \"/tmp\": {}\n         },\n         \"WorkingDir\": \"\",\n         \"NetworkDisabled\": false,\n         \"MacAddress\": \"12:34:56:78:9a:bc\",\n         \"ExposedPorts\": {\n                 \"22/tcp\": {}\n         },\n         \"HostConfig\": {\n           \"Binds\": [\"/tmp:/tmp\"],\n           \"Links\": [\"redis3:redis\"],\n           \"LxcConf\": {\"lxc.utsname\":\"docker\"},\n           \"Memory\": 0,\n           \"MemorySwap\": 0,\n           \"CpuShares\": 512,\n           \"CpusetCpus\": \"0,1\",\n           \"PortBindings\": { \"22/tcp\": [{ \"HostPort\": \"11022\" }] },\n           \"PublishAllPorts\": false,\n           \"Privileged\": false,\n           \"ReadonlyRootfs\": false,\n           \"Dns\": [\"8.8.8.8\"],\n           \"DnsSearch\": [\"\"],\n           \"ExtraHosts\": null,\n           \"VolumesFrom\": [\"parent\", \"other:ro\"],\n           \"CapAdd\": [\"NET_ADMIN\"],\n           \"CapDrop\": [\"MKNOD\"],\n           \"RestartPolicy\": { \"Name\": \"\", \"MaximumRetryCount\": 0 },\n           \"NetworkMode\": \"bridge\",\n           \"Devices\": [],\n           \"Ulimits\": [{}],\n           \"LogConfig\": { \"Type\": \"json-file\", Config: {} },\n           \"SecurityOpt\": [],\n           \"CgroupParent\": \"\"\n        }\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 201 Created\n    Content-Type: application/json\n\n    {\n         \"Id\":\"e90e34656806\",\n         \"Warnings\":[]\n    }\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Hostname</strong> - A string value containing the desired hostname to use for the container.</li> <li>\n<strong>Domainname</strong> - A string value containing the desired domain name to use for the container.</li> <li>\n<strong>User</strong> - A string value containing the user to use inside the container.</li> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to stdin.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to stdout.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to stderr.</li> <li>\n<strong>Tty</strong> - Boolean value, Attach standard streams to a tty, including stdin if it is not closed.</li> <li>\n<strong>OpenStdin</strong> - Boolean value, opens stdin,</li> <li>\n<strong>StdinOnce</strong> - Boolean value, close stdin after the 1 attached client disconnects.</li> <li>\n<strong>Env</strong> - A list of environment variables in the form of <code>[\"VAR=value\"[,\"VAR2=value2\"]]</code>\n</li> <li>\n<strong>Labels</strong> - Adds a map of labels that to a container. To specify a map: <code>{\"key\":\"value\"[,\"key2\":\"value2\"]}</code>\n</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> <li>\n<strong>Entrypoint</strong> - Set the entrypoint for the container a string or an array of strings</li> <li>\n<strong>Image</strong> - String value containing the image name to use for the container</li> <li>\n<strong>Volumes</strong> – An object mapping mountpoint paths (strings) inside the container to empty objects.</li> <li>\n<strong>WorkingDir</strong> - A string value containing the working dir for commands to run in.</li> <li>\n<strong>NetworkDisabled</strong> - Boolean value, when true disables networking for the container</li> <li>\n<strong>ExposedPorts</strong> - An object mapping ports to an empty object in the form of: <code>\"ExposedPorts\": { \"&lt;port&gt;/&lt;tcp|udp&gt;: {}\" }</code>\n</li> <li>\n<strong>HostConfig</strong> <ul> <li>\n<strong>Binds</strong> – A list of volume bindings for this container. Each volume binding is a string of the form <code>container_path</code> (to create a new volume for the container), <code>host_path:container_path</code> (to bind-mount a host path into the container), or <code>host_path:container_path:ro</code> (to make the bind-mount read-only inside the container).</li> <li>\n<strong>Links</strong> - A list of links for the container. Each link entry should be in the form of <code>container_name:alias</code>.</li> <li>\n<strong>LxcConf</strong> - LXC specific configurations. These configurations will only work when using the <code>lxc</code> execution driver.</li> <li>\n<strong>Memory</strong> - Memory limit in bytes.</li> <li>\n<strong>MemorySwap</strong> - Total memory limit (memory + swap); set <code>-1</code> to enable unlimited swap. You must use this with <code>memory</code> and make the swap value larger than <code>memory</code>.</li> <li>\n<strong>CpuShares</strong> - An integer value containing the CPU Shares for container (ie. the relative weight vs other containers).</li> <li>\n<strong>CpusetCpus</strong> - String value containing the cgroups CpusetCpus to use.</li> <li>\n<strong>PortBindings</strong> - A map of exposed container ports and the host port they should map to. It should be specified in the form <code>{ &lt;port&gt;/&lt;protocol&gt;: [{ \"HostPort\": \"&lt;port&gt;\" }] }</code> Take note that <code>port</code> is specified as a string and not an integer value.</li> <li>\n<strong>PublishAllPorts</strong> - Allocates a random host port for all of a container’s exposed ports. Specified as a boolean value.</li> <li>\n<strong>Privileged</strong> - Gives the container full access to the host. Specified as a boolean value.</li> <li>\n<strong>ReadonlyRootfs</strong> - Mount the container’s root filesystem as read only. Specified as a boolean value.</li> <li>\n<strong>Dns</strong> - A list of dns servers for the container to use.</li> <li>\n<strong>DnsSearch</strong> - A list of DNS search domains</li> <li>\n<strong>ExtraHosts</strong> - A list of hostnames/IP mappings to be added to the container’s <code>/etc/hosts</code> file. Specified in the form <code>[\"hostname:IP\"]</code>.</li> <li>\n<strong>VolumesFrom</strong> - A list of volumes to inherit from another container. Specified in the form <code>&lt;container name&gt;[:&lt;ro|rw&gt;]</code>\n</li> <li>\n<strong>CapAdd</strong> - A list of kernel capabilities to add to the container.</li> <li>\n<strong>Capdrop</strong> - A list of kernel capabilities to drop from the container.</li> <li>\n<strong>RestartPolicy</strong> – The behavior to apply when the container exits. The value is an object with a <code>Name</code> property of either <code>\"always\"</code> to always restart or <code>\"on-failure\"</code> to restart only when the container exit code is non-zero. If <code>on-failure</code> is used, <code>MaximumRetryCount</code> controls the number of times to retry before giving up. The default is not to restart. (optional) An ever increasing delay (double the previous delay, starting at 100mS) is added before each restart to prevent flooding the server.</li> <li>\n<strong>NetworkMode</strong> - Sets the networking mode for the container. Supported values are: <code>bridge</code>, <code>host</code>, <code>none</code>, and <code>container:&lt;name|id&gt;</code>\n</li> <li>\n<strong>Devices</strong> - A list of devices to add to the container specified in the form <code>{ \"PathOnHost\": \"/dev/deviceName\", \"PathInContainer\": \"/dev/deviceName\", \"CgroupPermissions\": \"mrw\"}</code>\n</li> <li>\n<strong>Ulimits</strong> - A list of ulimits to be set in the container, specified as <code>{ \"Name\": &lt;name&gt;, \"Soft\": &lt;soft limit&gt;, \"Hard\": &lt;hard limit&gt; }</code>, for example: <code>Ulimits: { \"Name\": \"nofile\", \"Soft\": 1024, \"Hard\": 2048 }</code>\n</li> <li>\n<strong>SecurityOpt</strong>: A list of string values to customize labels for MLS systems, such as SELinux.</li> <li>\n<strong>LogConfig</strong> - Log configuration for the container, specified as <code>{ \"Type\": \"&lt;driver_name&gt;\", \"Config\": {\"key1\": \"val1\"}}</code>. Available types: <code>json-file</code>, <code>syslog</code>, <code>journald</code>, <code>none</code>. <code>json-file</code> logging driver.</li> <li>\n<strong>CgroupParent</strong> - Path to cgroups under which the cgroup for the container will be created. If the path is not absolute, the path is considered to be relative to the cgroups path of the init process. Cgroups will be created if they do not already exist.</li> </ul>\n</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – Assign the specified name to the container. Must match <code>/?[a-zA-Z0-9_-]+</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>406</strong> – impossible to attach (container not running)</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-a-container\">Inspect a container</h3> <p><code>GET /containers/(id or name)/json</code></p> <p>Return low-level information on the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    GET /containers/4fa6e0f0c678/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n{\n    \"AppArmorProfile\": \"\",\n    \"Args\": [\n        \"-c\",\n        \"exit 9\"\n    ],\n    \"Config\": {\n        \"AttachStderr\": true,\n        \"AttachStdin\": false,\n        \"AttachStdout\": true,\n        \"Cmd\": [\n            \"/bin/sh\",\n            \"-c\",\n            \"exit 9\"\n        ],\n        \"Domainname\": \"\",\n        \"Entrypoint\": null,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": null,\n        \"Hostname\": \"ba033ac44011\",\n        \"Image\": \"ubuntu\",\n        \"Labels\": {\n            \"com.example.vendor\": \"Acme\",\n            \"com.example.license\": \"GPL\",\n            \"com.example.version\": \"1.0\"\n        },\n        \"MacAddress\": \"\",\n        \"NetworkDisabled\": false,\n        \"OnBuild\": null,\n        \"OpenStdin\": false,\n        \"PortSpecs\": null,\n        \"StdinOnce\": false,\n        \"Tty\": false,\n        \"User\": \"\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\"\n    },\n    \"Created\": \"2015-01-06T15:47:31.485331387Z\",\n    \"Driver\": \"devicemapper\",\n    \"ExecDriver\": \"native-0.2\",\n    \"ExecIDs\": null,\n    \"HostConfig\": {\n        \"Binds\": null,\n        \"CapAdd\": null,\n        \"CapDrop\": null,\n        \"ContainerIDFile\": \"\",\n        \"CpusetCpus\": \"\",\n        \"CpuShares\": 0,\n        \"Devices\": [],\n        \"Dns\": null,\n        \"DnsSearch\": null,\n        \"ExtraHosts\": null,\n        \"IpcMode\": \"\",\n        \"Links\": null,\n        \"LxcConf\": [],\n        \"Memory\": 0,\n        \"MemorySwap\": 0,\n        \"NetworkMode\": \"bridge\",\n        \"PortBindings\": {},\n        \"Privileged\": false,\n        \"ReadonlyRootfs\": false,\n        \"PublishAllPorts\": false,\n        \"RestartPolicy\": {\n            \"MaximumRetryCount\": 2,\n            \"Name\": \"on-failure\"\n        },\n        \"LogConfig\": {\n            \"Config\": null,\n            \"Type\": \"json-file\"\n        },\n        \"SecurityOpt\": null,\n        \"VolumesFrom\": null,\n        \"Ulimits\": [{}]\n    },\n    \"HostnamePath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hostname\",\n    \"HostsPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Id\": \"ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39\",\n    \"Image\": \"04c5d3b7b0656168630d3ba35d8889bd0e9caafcaeb3004d2bfbc47e7c5d35d2\",\n    \"MountLabel\": \"\",\n    \"Name\": \"/boring_euclid\",\n    \"NetworkSettings\": {\n        \"Bridge\": \"\",\n        \"Gateway\": \"\",\n        \"IPAddress\": \"\",\n        \"IPPrefixLen\": 0,\n        \"MacAddress\": \"\",\n        \"PortMapping\": null,\n        \"Ports\": null\n    },\n    \"Path\": \"/bin/sh\",\n    \"ProcessLabel\": \"\",\n    \"ResolvConfPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/resolv.conf\",\n    \"RestartCount\": 1,\n    \"State\": {\n        \"Error\": \"\",\n        \"ExitCode\": 9,\n        \"FinishedAt\": \"2015-01-06T15:47:32.080254511Z\",\n        \"OOMKilled\": false,\n        \"Paused\": false,\n        \"Pid\": 0,\n        \"Restarting\": false,\n        \"Running\": false,\n        \"StartedAt\": \"2015-01-06T15:47:32.072697474Z\"\n    },\n    \"Volumes\": {},\n    \"VolumesRW\": {}\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"list-processes-running-inside-a-container\">List processes running inside a container</h3> <p><code>GET /containers/(id or name)/top</code></p> <p>List processes running inside the container <code>id</code>. On Unix systems this is done by running the <code>ps</code> command. This endpoint is not supported on Windows.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Titles\" : [\n     \"UID\", \"PID\", \"PPID\", \"C\", \"STIME\", \"TTY\", \"TIME\", \"CMD\"\n   ],\n   \"Processes\" : [\n     [\n       \"root\", \"13642\", \"882\", \"0\", \"17:03\", \"pts/0\", \"00:00:00\", \"/bin/bash\"\n     ],\n     [\n       \"root\", \"13735\", \"13642\", \"0\", \"17:06\", \"pts/0\", \"00:00:00\", \"sleep 10\"\n     ]\n   ]\n}\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top?ps_args=aux HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Titles\" : [\n    \"USER\",\"PID\",\"%CPU\",\"%MEM\",\"VSZ\",\"RSS\",\"TTY\",\"STAT\",\"START\",\"TIME\",\"COMMAND\"\n  ]\n  \"Processes\" : [\n    [\n      \"root\",\"13642\",\"0.0\",\"0.1\",\"18172\",\"3184\",\"pts/0\",\"Ss\",\"17:03\",\"0:00\",\"/bin/bash\"\n    ],\n    [\n      \"root\",\"13895\",\"0.0\",\"0.0\",\"4348\",\"692\",\"pts/0\",\"S+\",\"17:15\",\"0:00\",\"sleep 10\"\n    ]\n  ],\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>ps_args</strong> – <code>ps</code> arguments to use (e.g., <code>aux</code>), defaults to <code>-ef</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-logs\">Get container logs</h3> <p><code>GET /containers/(id or name)/logs</code></p> <p>Get stdout and stderr logs from the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: This endpoint works only for containers with the <code>json-file</code> or <code>journald</code> logging drivers.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>   GET /containers/4fa6e0f0c678/logs?stderr=1&amp;stdout=1&amp;timestamps=1&amp;follow=1&amp;tail=10 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>   HTTP/1.1 101 UPGRADED\n   Content-Type: application/vnd.docker.raw-stream\n   Connection: Upgrade\n   Upgrade: tcp\n\n   {{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>follow</strong> – 1/True/true or 0/False/false, return stream. Default false</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, show stdout log. Default false</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, show stderr log. Default false</li> <li>\n<strong>timestamps</strong> – 1/True/true or 0/False/false, print timestamps for every log line. Default false</li> <li>\n<strong>tail</strong> – Output specified number of lines at the end of logs: <code>all</code> or <code>&lt;number&gt;</code>. Default all</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-changes-on-a-container-s-filesystem\">Inspect changes on a container’s filesystem</h3> <p><code>GET /containers/(id or name)/changes</code></p> <p>Inspect changes on container <code>id</code>’s filesystem</p> <p><strong>Example request</strong>:</p> <pre>    GET /containers/4fa6e0f0c678/changes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    [\n         {\n                 \"Path\": \"/dev\",\n                 \"Kind\": 0\n         },\n         {\n                 \"Path\": \"/dev/kmsg\",\n                 \"Kind\": 1\n         },\n         {\n                 \"Path\": \"/test\",\n                 \"Kind\": 1\n         }\n    ]\n</pre> <p>Values for <code>Kind</code>:</p> <ul> <li>\n<code>0</code>: Modify</li> <li>\n<code>1</code>: Add</li> <li>\n<code>2</code>: Delete</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"export-a-container\">Export a container</h3> <p><code>GET /containers/(id or name)/export</code></p> <p>Export the contents of container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    GET /containers/4fa6e0f0c678/export HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/octet-stream\n\n    {{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-stats-based-on-resource-usage\">Get container stats based on resource usage</h3> <p><code>GET /containers/(id or name)/stats</code></p> <p>This endpoint returns a live stream of a container’s resource usage statistics.</p> <p><strong>Example request</strong>:</p> <pre>    GET /containers/redis1/stats HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\n       \"read\" : \"2015-01-08T22:57:31.547920715Z\",\n       \"network\" : {\n          \"rx_dropped\" : 0,\n          \"rx_bytes\" : 648,\n          \"rx_errors\" : 0,\n          \"tx_packets\" : 8,\n          \"tx_dropped\" : 0,\n          \"rx_packets\" : 8,\n          \"tx_errors\" : 0,\n          \"tx_bytes\" : 648\n       },\n       \"memory_stats\" : {\n          \"stats\" : {\n             \"total_pgmajfault\" : 0,\n             \"cache\" : 0,\n             \"mapped_file\" : 0,\n             \"total_inactive_file\" : 0,\n             \"pgpgout\" : 414,\n             \"rss\" : 6537216,\n             \"total_mapped_file\" : 0,\n             \"writeback\" : 0,\n             \"unevictable\" : 0,\n             \"pgpgin\" : 477,\n             \"total_unevictable\" : 0,\n             \"pgmajfault\" : 0,\n             \"total_rss\" : 6537216,\n             \"total_rss_huge\" : 6291456,\n             \"total_writeback\" : 0,\n             \"total_inactive_anon\" : 0,\n             \"rss_huge\" : 6291456,\n             \"hierarchical_memory_limit\" : 67108864,\n             \"total_pgfault\" : 964,\n             \"total_active_file\" : 0,\n             \"active_anon\" : 6537216,\n             \"total_active_anon\" : 6537216,\n             \"total_pgpgout\" : 414,\n             \"total_cache\" : 0,\n             \"inactive_anon\" : 0,\n             \"active_file\" : 0,\n             \"pgfault\" : 964,\n             \"inactive_file\" : 0,\n             \"total_pgpgin\" : 477\n          },\n          \"max_usage\" : 6651904,\n          \"usage\" : 6537216,\n          \"failcnt\" : 0,\n          \"limit\" : 67108864\n       },\n       \"blkio_stats\" : {},\n       \"cpu_stats\" : {\n          \"cpu_usage\" : {\n             \"percpu_usage\" : [\n                16970827,\n                1839451,\n                7107380,\n                10571290\n             ],\n             \"usage_in_usermode\" : 10000000,\n             \"total_usage\" : 36488948,\n             \"usage_in_kernelmode\" : 20000000\n          },\n          \"system_cpu_usage\" : 20091722000000000,\n          \"throttling_data\" : {}\n       }\n    }\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"resize-a-container-tty\">Resize a container TTY</h3> <p><code>POST /containers/(id or name)/resize?h=&lt;height&gt;&amp;w=&lt;width&gt;</code></p> <p>Resize the TTY for container with <code>id</code>. The container must be restarted for the resize to take effect.</p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/4fa6e0f0c678/resize?h=40&amp;w=80 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Length: 0\n    Content-Type: text/plain; charset=utf-8\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – No such container</li> <li>\n<strong>500</strong> – Cannot resize container</li> </ul> <h3 id=\"start-a-container\">Start a container</h3> <p><code>POST /containers/(id or name)/start</code></p> <p>Start the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: For backwards compatibility, this endpoint accepts a <code>HostConfig</code> as JSON-encoded request body. See <a href=\"#create-a-container\">create a container</a> for details.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>    POST /containers/e90e34656806/start HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already started</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"stop-a-container\">Stop a container</h3> <p><code>POST /containers/(id or name)/stop</code></p> <p>Stop the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/e90e34656806/stop?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already stopped</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"restart-a-container\">Restart a container</h3> <p><code>POST /containers/(id or name)/restart</code></p> <p>Restart the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/e90e34656806/restart?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"kill-a-container\">Kill a container</h3> <p><code>POST /containers/(id or name)/kill</code></p> <p>Kill the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/e90e34656806/kill HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 No Content\n</pre> <p>Query Parameters</p> <ul> <li>\n<strong>signal</strong> - Signal to send to the container: integer or string like “SIGINT”. When not set, SIGKILL is assumed and the call will waits for the container to exit.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"rename-a-container\">Rename a container</h3> <p><code>POST /containers/(id or name)/rename</code></p> <p>Rename the container <code>id</code> to a <code>new_name</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/e90e34656806/rename?name=new_name HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – new name for the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - conflict name already assigned</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"pause-a-container\">Pause a container</h3> <p><code>POST /containers/(id or name)/pause</code></p> <p>Pause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/e90e34656806/pause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"unpause-a-container\">Unpause a container</h3> <p><code>POST /containers/(id or name)/unpause</code></p> <p>Unpause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/e90e34656806/unpause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"attach-to-a-container\">Attach to a container</h3> <p><code>POST /containers/(id or name)/attach</code></p> <p>Attach to the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/16253994b7c4/attach?logs=1&amp;stream=0&amp;stdout=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 101 UPGRADED\n    Content-Type: application/vnd.docker.raw-stream\n    Connection: Upgrade\n    Upgrade: tcp\n\n    {{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default false</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default false</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if stream=true, attach to stdin. Default false</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if logs=true, return stdout log, if stream=true, attach to stdout. Default false</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if logs=true, return stderr log, if stream=true, attach to stderr. Default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<p><strong>500</strong> – server error</p> <p><strong>Stream details</strong>:</p> <p>When using the TTY setting is enabled in <a href=\"#create-a-container\"><code>POST /containers/create</code> </a>, the stream is the raw data from the process PTY and client’s stdin. When the TTY is disabled, then the stream is multiplexed to separate stdout and stderr.</p> <p>The format is a <strong>Header</strong> and a <strong>Payload</strong> (frame).</p> <p><strong>HEADER</strong></p> <p>The header will contain the information on which stream write the stream (stdout or stderr). It also contain the size of the associated frame encoded on the last 4 bytes (uint32).</p> <p>It is encoded on the first 8 bytes like this:</p> <pre>header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}\n</pre> <p><code>STREAM_TYPE</code> can be:</p>\n</li> <li><p>0: stdin (will be written on stdout)</p></li> <li><p>1: stdout</p></li> <li>\n<p>2: stderr</p> <p><code>SIZE1, SIZE2, SIZE3, SIZE4</code> are the 4 bytes of the uint32 size encoded as big endian.</p> <p><strong>PAYLOAD</strong></p> <p>The payload is the raw stream.</p> <p><strong>IMPLEMENTATION</strong></p> <p>The simplest way to implement the Attach protocol is the following:</p> <ol> <li>Read 8 bytes</li> <li>chose stdout or stderr depending on the first byte</li> <li>Extract the frame size from the last 4 bytes</li> <li>Read the extracted size and output it on the correct output</li> <li>Goto 1</li> </ol>\n</li> </ul> <h3 id=\"attach-to-a-container-websocket\">Attach to a container (websocket)</h3> <p><code>GET /containers/(id or name)/attach/ws</code></p> <p>Attach to the container <code>id</code> via websocket</p> <p>Implements websocket protocol handshake according to <a href=\"http://tools.ietf.org/html/rfc6455\">RFC 6455</a></p> <p><strong>Example request</strong></p> <pre>    GET /containers/e90e34656806/attach/ws?logs=0&amp;stream=1&amp;stdin=1&amp;stdout=1&amp;stderr=1 HTTP/1.1\n</pre> <p><strong>Example response</strong></p> <pre>    {{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default false</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default false</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if stream=true, attach to stdin. Default false</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if logs=true, return stdout log, if stream=true, attach to stdout. Default false</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if logs=true, return stderr log, if stream=true, attach to stderr. Default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"wait-a-container\">Wait a container</h3> <p><code>POST /containers/(id or name)/wait</code></p> <p>Block until container <code>id</code> stops, then returns the exit code</p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/16253994b7c4/wait HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\"StatusCode\": 0}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-a-container\">Remove a container</h3> <p><code>DELETE /containers/(id or name)</code></p> <p>Remove the container <code>id</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>    DELETE /containers/16253994b7c4?v=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>v</strong> – 1/True/true or 0/False/false, Remove the volumes associated to the container. Default false</li> <li>\n<strong>force</strong> - 1/True/true or 0/False/false, Kill then remove the container. Default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"copy-files-or-folders-from-a-container\">Copy files or folders from a container</h3> <p><code>POST /containers/(id or name)/copy</code></p> <p>Copy files or folders of container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/4fa6e0f0c678/copy HTTP/1.1\n    Content-Type: application/json\n\n    {\n         \"Resource\": \"test.txt\"\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/x-tar\n\n    {{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-2-images\">2.2 Images</h2> <h3 id=\"list-images\">List Images</h3> <p><code>GET /images/json</code></p> <p><strong>Example request</strong>:</p> <pre>    GET /images/json?all=0 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    [\n      {\n         \"RepoTags\": [\n           \"ubuntu:12.04\",\n           \"ubuntu:precise\",\n           \"ubuntu:latest\"\n         ],\n         \"Id\": \"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\n         \"Created\": 1365714795,\n         \"Size\": 131506275,\n         \"VirtualSize\": 131506275\n      },\n      {\n         \"RepoTags\": [\n           \"ubuntu:12.10\",\n           \"ubuntu:quantal\"\n         ],\n         \"ParentId\": \"27cf784147099545\",\n         \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n         \"Created\": 1364102658,\n         \"Size\": 24653,\n         \"VirtualSize\": 180116135\n      }\n    ]\n</pre> <p><strong>Example request, with digest information</strong>:</p> <pre>    GET /images/json?digests=1 HTTP/1.1\n</pre> <p><strong>Example response, with digest information</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    [\n      {\n        \"Created\": 1420064636,\n        \"Id\": \"4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125\",\n        \"ParentId\": \"ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2\",\n        \"RepoDigests\": [\n          \"localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n        ],\n        \"RepoTags\": [\n          \"localhost:5000/test/busybox:latest\",\n          \"playdate:latest\"\n        ],\n        \"Size\": 0,\n        \"VirtualSize\": 2429728\n      }\n    ]\n</pre> <p>The response shows a single image <code>Id</code> associated with two repositories (<code>RepoTags</code>): <code>localhost:5000/test/busybox</code>: and <code>playdate</code>. A caller can use either of the <code>RepoTags</code> values <code>localhost:5000/test/busybox:latest</code> or <code>playdate:latest</code> to reference the image.</p> <p>You can also use <code>RepoDigests</code> values to reference an image. In this response, the array has only one reference and that is to the <code>localhost:5000/test/busybox</code> repository; the <code>playdate</code> repository has no digest. You can reference this digest using the value: <code>localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d...</code></p> <p>See the <code>docker run</code> and <code>docker build</code> commands for examples of digest and tag references on the command line.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>filters</strong> – a json encoded value of the filters (a map[string][]string) to process on the images list. Available filters: <ul> <li>dangling=true</li> <li>label=<code>key</code> or <code>label=\"key=value\"</code> of an image label</li> </ul>\n</li> <li>\n<strong>filter</strong> - only return images with the specified name</li> </ul> <h3 id=\"build-image-from-a-dockerfile\">Build image from a Dockerfile</h3> <p><code>POST /build</code></p> <p>Build an image from a Dockerfile</p> <p><strong>Example request</strong>:</p> <pre>    POST /build HTTP/1.1\n\n    {{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\"stream\": \"Step 1...\"}\n    {\"stream\": \"...\"}\n    {\"error\": \"Error...\", \"errorDetail\": {\"code\": 123, \"message\": \"Error...\"}}\n</pre> <p>The input stream must be a tar archive compressed with one of the following algorithms: identity (no compression), gzip, bzip2, xz.</p> <p>The archive must include a build instructions file, typically called <code>Dockerfile</code> at the root of the archive. The <code>dockerfile</code> parameter may be used to specify a different build instructions file by having its value be the path to the alternate build instructions file to use.</p> <p>The archive may include any number of other files, which will be accessible in the build context (See the <a href=\"../../builder/index#dockerbuilder\"><em>ADD build command</em></a>).</p> <p>The build will also be canceled if the client drops the connection by quitting or being killed.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>dockerfile</strong> - path within the build context to the Dockerfile. This is ignored if <code>remote</code> is specified and points to an individual filename.</li> <li>\n<strong>t</strong> – repository name (and optionally a tag) to be applied to the resulting image in case of success</li> <li>\n<strong>remote</strong> – A Git repository URI or HTTP/HTTPS URI build source. If the URI specifies a filename, the file’s contents are placed into a file called <code>Dockerfile</code>.</li> <li>\n<strong>q</strong> – suppress verbose build output</li> <li>\n<strong>nocache</strong> – do not use the cache when building the image</li> <li>\n<strong>pull</strong> - attempt to pull the image even if an older image exists locally</li> <li>\n<strong>rm</strong> - remove intermediate containers after a successful build (default behavior)</li> <li>\n<strong>forcerm</strong> - always remove intermediate containers (includes rm)</li> <li>\n<strong>memory</strong> - set memory limit for build</li> <li>\n<strong>memswap</strong> - Total memory (memory + swap), <code>-1</code> to enable unlimited swap.</li> <li>\n<strong>cpushares</strong> - CPU shares (relative weight)</li> <li>\n<p><strong>cpusetcpus</strong> - CPUs in which to allow execution, e.g., <code>0-3</code>, <code>0,1</code></p> <p>Request Headers:</p>\n</li> <li><p><strong>Content-type</strong> – should be set to <code>\"application/tar\"</code>.</p></li> <li><p><strong>X-Registry-Config</strong> – base64-encoded ConfigFile object</p></li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-an-image\">Create an image</h3> <p><code>POST /images/create</code></p> <p>Create an image, either by pulling it from the registry or by importing it</p> <p><strong>Example request</strong>:</p> <pre>    POST /images/create?fromImage=ubuntu HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\"status\": \"Pulling...\"}\n    {\"status\": \"Pulling\", \"progress\": \"1 B/ 100 B\", \"progressDetail\": {\"current\": 1, \"total\": 100}}\n    {\"error\": \"Invalid...\"}\n    ...\n\nWhen using this endpoint to pull an image from the registry, the\n`X-Registry-Auth` header can be used to include\na base64-encoded AuthConfig object.\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>fromImage</strong> – name of the image to pull</li> <li>\n<strong>fromSrc</strong> – source to import. The value may be a URL from which the image can be retrieved or <code>-</code> to read the image from the request body.</li> <li>\n<strong>repo</strong> – repository</li> <li>\n<p><strong>tag</strong> – tag</p> <p>Request Headers:</p>\n</li> <li><p><strong>X-Registry-Auth</strong> – base64-encoded AuthConfig object</p></li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-an-image\">Inspect an image</h3> <p><code>GET /images/(name)/json</code></p> <p>Return low-level information on the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>    GET /images/ubuntu/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\n         \"Created\": \"2013-03-23T22:24:18.818426-07:00\",\n         \"Container\": \"3d67245a8d72ecf13f33dffac9f79dcdf70f75acb84d308770391510e0c23ad0\",\n         \"ContainerConfig\":\n                 {\n                         \"Hostname\": \"\",\n                         \"User\": \"\",\n                         \"AttachStdin\": false,\n                         \"AttachStdout\": false,\n                         \"AttachStderr\": false,\n                         \"PortSpecs\": null,\n                         \"Tty\": true,\n                         \"OpenStdin\": true,\n                         \"StdinOnce\": false,\n                         \"Env\": null,\n                         \"Cmd\": [\"/bin/bash\"],\n                         \"Dns\": null,\n                         \"Image\": \"ubuntu\",\n                         \"Labels\": {\n                             \"com.example.vendor\": \"Acme\",\n                             \"com.example.license\": \"GPL\",\n                             \"com.example.version\": \"1.0\"\n                         },\n                         \"Volumes\": null,\n                         \"VolumesFrom\": \"\",\n                         \"WorkingDir\": \"\"\n                 },\n         \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n         \"Parent\": \"27cf784147099545\",\n         \"Size\": 6824592\n    }\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-the-history-of-an-image\">Get the history of an image</h3> <p><code>GET /images/(name)/history</code></p> <p>Return the history of the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>    GET /images/ubuntu/history HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    [\n         {\n                 \"Id\": \"b750fe79269d\",\n                 \"Created\": 1364102658,\n                 \"CreatedBy\": \"/bin/bash\"\n         },\n         {\n                 \"Id\": \"27cf78414709\",\n                 \"Created\": 1364068391,\n                 \"CreatedBy\": \"\"\n         }\n    ]\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"push-an-image-on-the-registry\">Push an image on the registry</h3> <p><code>POST /images/(name)/push</code></p> <p>Push the image <code>name</code> on the registry</p> <p><strong>Example request</strong>:</p> <pre>    POST /images/test/push HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\"status\": \"Pushing...\"}\n    {\"status\": \"Pushing\", \"progress\": \"1/? (n/a)\", \"progressDetail\": {\"current\": 1}}}\n    {\"error\": \"Invalid...\"}\n    ...\n\nIf you wish to push an image on to a private registry, that image must already have been tagged\ninto a repository which references that registry host name and port.  This repository name should\nthen be used in the URL. This mirrors the flow of the CLI.\n</pre> <p><strong>Example request</strong>:</p> <pre>    POST /images/registry.acme.com:5000/test/push HTTP/1.1\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>tag</strong> – the tag to associate with the image on the registry, optional</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>X-Registry-Auth</strong> – include a base64-encoded AuthConfig object.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"tag-an-image-into-a-repository\">Tag an image into a repository</h3> <p><code>POST /images/(name)/tag</code></p> <p>Tag the image <code>name</code> into a repository</p> <p><strong>Example request</strong>:</p> <pre>    POST /images/test/tag?repo=myrepo&amp;force=0&amp;tag=v42 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 201 Created\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>repo</strong> – The repository to tag in</li> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>tag</strong> - The new tag name</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-an-image\">Remove an image</h3> <p><code>DELETE /images/(name)</code></p> <p>Remove the image <code>name</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>    DELETE /images/test HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-type: application/json\n\n    [\n     {\"Untagged\": \"3e2f21a89f\"},\n     {\"Deleted\": \"3e2f21a89f\"},\n     {\"Deleted\": \"53b4f83ac9\"}\n    ]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>noprune</strong> – 1/True/true or 0/False/false, default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"search-images\">Search images</h3> <p><code>GET /images/search</code></p> <p>Search for an image on <a href=\"https://hub.docker.com\">Docker Hub</a>.</p> <blockquote> <p><strong>Note</strong>: The response keys have changed from API v1.6 to reflect the JSON sent by the registry server to the docker daemon’s request.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>    GET /images/search?term=sshd HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    [\n            {\n                \"description\": \"\",\n                \"is_official\": false,\n                \"is_automated\": false,\n                \"name\": \"wma55/u1210sshd\",\n                \"star_count\": 0\n            },\n            {\n                \"description\": \"\",\n                \"is_official\": false,\n                \"is_automated\": false,\n                \"name\": \"jdswinbank/sshd\",\n                \"star_count\": 0\n            },\n            {\n                \"description\": \"\",\n                \"is_official\": false,\n                \"is_automated\": false,\n                \"name\": \"vgauthier/sshd\",\n                \"star_count\": 0\n            }\n    ...\n    ]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>term</strong> – term to search</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-3-misc\">2.3 Misc</h2> <h3 id=\"check-auth-configuration\">Check auth configuration</h3> <p><code>POST /auth</code></p> <p>Get the default username and email</p> <p><strong>Example request</strong>:</p> <pre>    POST /auth HTTP/1.1\n    Content-Type: application/json\n\n    {\n         \"username\":\" hannibal\",\n         \"password: \"xxxx\",\n         \"email\": \"hannibal@a-team.com\",\n         \"serveraddress\": \"https://index.docker.io/v1/\"\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"display-system-wide-information\">Display system-wide information</h3> <p><code>GET /info</code></p> <p>Display system-wide information</p> <p><strong>Example request</strong>:</p> <pre>    GET /info HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\n        \"Containers\": 11,\n        \"Debug\": 0,\n        \"DockerRootDir\": \"/var/lib/docker\",\n        \"Driver\": \"btrfs\",\n        \"DriverStatus\": [[\"\"]],\n        \"ExecutionDriver\": \"native-0.1\",\n        \"HttpProxy\": \"http://test:test@localhost:8080\",\n        \"HttpsProxy\": \"https://test:test@localhost:8080\",\n        \"ID\": \"7TRN:IPZB:QYBB:VPBQ:UMPP:KARE:6ZNR:XE6T:7EWV:PKF4:ZOJD:TPYS\",\n        \"IPv4Forwarding\": 1,\n        \"Images\": 16,\n        \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n        \"InitPath\": \"/usr/bin/docker\",\n        \"InitSha1\": \"\",\n        \"KernelVersion\": \"3.12.0-1-amd64\",\n        \"Labels\": [\n            \"storage=ssd\"\n        ],\n        \"MemTotal\": 2099236864,\n        \"MemoryLimit\": 1,\n        \"NCPU\": 1,\n        \"NEventsListener\": 0,\n        \"NFd\": 11,\n        \"NGoroutines\": 21,\n        \"Name\": \"prod-server-42\",\n        \"NoProxy\": \"9.81.1.160\",\n        \"OperatingSystem\": \"Boot2Docker\",\n        \"RegistryConfig\": {\n            \"IndexConfigs\": {\n                \"docker.io\": {\n                    \"Mirrors\": null,\n                    \"Name\": \"docker.io\",\n                    \"Official\": true,\n                    \"Secure\": true\n                }\n            },\n            \"InsecureRegistryCIDRs\": [\n                \"127.0.0.0/8\"\n            ]\n        },\n        \"SwapLimit\": 0,\n        \"SystemTime\": \"2015-03-10T11:11:23.730591467-07:00\"\n    }\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"show-the-docker-version-information\">Show the docker version information</h3> <p><code>GET /version</code></p> <p>Show the docker version information</p> <p><strong>Example request</strong>:</p> <pre>    GET /version HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\n         \"Version\": \"1.5.0\",\n         \"Os\": \"linux\",\n         \"KernelVersion\": \"3.18.5-tinycore64\",\n         \"GoVersion\": \"go1.4.1\",\n         \"GitCommit\": \"a8a31ef\",\n         \"Arch\": \"amd64\",\n         \"ApiVersion\": \"1.18\"\n    }\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"ping-the-docker-server\">Ping the docker server</h3> <p><code>GET /_ping</code></p> <p>Ping the docker server</p> <p><strong>Example request</strong>:</p> <pre>    GET /_ping HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: text/plain\n\n    OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-new-image-from-a-container-s-changes\">Create a new image from a container’s changes</h3> <p><code>POST /commit</code></p> <p>Create a new image from a container’s changes</p> <p><strong>Example request</strong>:</p> <pre>    POST /commit?container=44c004db4b17&amp;comment=message&amp;repo=myrepo HTTP/1.1\n    Content-Type: application/json\n\n    {\n         \"Hostname\": \"\",\n         \"Domainname\": \"\",\n         \"User\": \"\",\n         \"AttachStdin\": false,\n         \"AttachStdout\": true,\n         \"AttachStderr\": true,\n         \"PortSpecs\": null,\n         \"Tty\": false,\n         \"OpenStdin\": false,\n         \"StdinOnce\": false,\n         \"Env\": null,\n         \"Cmd\": [\n                 \"date\"\n         ],\n         \"Volumes\": {\n                 \"/tmp\": {}\n         },\n         \"WorkingDir\": \"\",\n         \"NetworkDisabled\": false,\n         \"ExposedPorts\": {\n                 \"22/tcp\": {}\n         }\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 201 Created\n    Content-Type: application/json\n\n    {\"Id\": \"596069db4bf5\"}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>config</strong> - the container’s configuration</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>container</strong> – source container</li> <li>\n<strong>repo</strong> – repository</li> <li>\n<strong>tag</strong> – tag</li> <li>\n<strong>comment</strong> – commit message</li> <li>\n<strong>author</strong> – author (e.g., “John Hannibal Smith &lt;<a href=\"mailto:hannibal%40a-team.com\">hannibal@a-team.com</a>&gt;“)</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"monitor-docker-s-events\">Monitor Docker’s events</h3> <p><code>GET /events</code></p> <p>Get container events from docker, either in real time via streaming, or via polling (using since).</p> <p>Docker containers will report the following events:</p> <pre>create, destroy, die, exec_create, exec_start, export, kill, oom, pause, restart, start, stop, unpause\n</pre> <p>and Docker images will report:</p> <pre>untag, delete\n</pre> <p><strong>Example request</strong>:</p> <pre>    GET /events?since=1374067924\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/json\n\n    {\"status\": \"create\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067924}\n    {\"status\": \"start\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067924}\n    {\"status\": \"stop\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067966}\n    {\"status\": \"destroy\", \"id\": \"dfdf82bd3881\",\"from\": \"ubuntu:latest\", \"time\":1374067970}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>since</strong> – timestamp used for polling</li> <li>\n<strong>until</strong> – timestamp used for polling</li> <li>\n<strong>filters</strong> – a json encoded value of the filters (a map[string][]string) to process on the event list. Available filters: <ul> <li>event=&lt;string&gt; -- event to filter</li> <li>image=&lt;string&gt; -- image to filter</li> <li>container=&lt;string&gt; -- container to filter</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images-in-a-repository\">Get a tarball containing all images in a repository</h3> <p><code>GET /images/(name)/get</code></p> <p>Get a tarball containing all images and metadata for the repository specified by <code>name</code>.</p> <p>If <code>name</code> is a specific name and tag (e.g. ubuntu:latest), then only that image (and its parents) are returned. If <code>name</code> is an image ID, similarly only that image (and its parents) are returned, but with the exclusion of the ‘repositories’ file in the tarball, as there were no image names referenced.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>    GET /images/ubuntu/get\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/x-tar\n\n    Binary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images\">Get a tarball containing all images.</h3> <p><code>GET /images/get</code></p> <p>Get a tarball containing all images and metadata for one or more repositories.</p> <p>For each value of the <code>names</code> parameter: if it is a specific name and tag (e.g. ubuntu:latest), then only that image (and its parents) are returned; if it is an image ID, similarly only that image (and its parents) are returned and there would be no names referenced in the ‘repositories’ file for this image ID.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>    GET /images/get?names=myname%2Fmyapp%3Alatest&amp;names=busybox\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/x-tar\n\n    Binary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"load-a-tarball-with-a-set-of-images-and-tags-into-docker\">Load a tarball with a set of images and tags into docker</h3> <p><code>POST /images/load</code></p> <p>Load a set of images and tags into the docker repository. See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>    POST /images/load\n\n    Tarball in body\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"image-tarball-format\">Image tarball format</h3> <p>An image tarball contains one directory per image layer (named using its long ID), each containing three files:</p> <ol> <li>\n<code>VERSION</code>: currently <code>1.0</code> - the file format version</li> <li>\n<code>json</code>: detailed layer information, similar to <code>docker inspect layer_id</code>\n</li> <li>\n<code>layer.tar</code>: A tarfile containing the filesystem changes in this layer</li> </ol> <p>The <code>layer.tar</code> file will contain <code>aufs</code> style <code>.wh..wh.aufs</code> files and directories for storing attribute changes and deletions.</p> <p>If the tarball defines a repository, there will also be a <code>repositories</code> file at the root that contains a list of repository and tag names mapped to layer IDs.</p> <pre>{\"hello-world\":\n    {\"latest\": \"565a9d68a73f6706862bfe8409a7f659776d4d60a8d096eb4a3cbce6999cc2a1\"}\n}\n</pre> <h3 id=\"exec-create\">Exec Create</h3> <p><code>POST /containers/(id or name)/exec</code></p> <p>Sets up an exec instance in a running container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>    POST /containers/e90e34656806/exec HTTP/1.1\n    Content-Type: application/json\n\n    {\n     \"AttachStdin\": false,\n     \"AttachStdout\": true,\n     \"AttachStderr\": true,\n     \"Tty\": false,\n     \"Cmd\": [\n                 \"date\"\n         ],\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 201 Created\n    Content-Type: application/json\n\n    {\n         \"Id\": \"f90e34656806\",\n         \"Warnings\":[]\n    }\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to stdin of the exec command.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to stdout of the exec command.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to stderr of the exec command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> </ul> <h3 id=\"exec-start\">Exec Start</h3> <p><code>POST /exec/(id)/start</code></p> <p>Starts a previously set up exec instance <code>id</code>. If <code>detach</code> is true, this API returns after starting the <code>exec</code> command. Otherwise, this API sets up an interactive session with the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>    POST /exec/e90e34656806/start HTTP/1.1\n    Content-Type: application/json\n\n    {\n     \"Detach\": false,\n     \"Tty\": false,\n    }\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/vnd.docker.raw-stream\n\n    {{ STREAM }}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Detach</strong> - Detach from the exec command</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<p><strong>404</strong> – no such exec instance</p> <p><strong>Stream details</strong>: Similar to the stream behavior of <code>POST /containers/(id or name)/attach</code> API</p>\n</li> </ul> <h3 id=\"exec-resize\">Exec Resize</h3> <p><code>POST /exec/(id)/resize</code></p> <p>Resizes the tty session used by the exec command <code>id</code>. This API is valid only if <code>tty</code> was specified as part of creating and starting the exec command.</p> <p><strong>Example request</strong>:</p> <pre>    POST /exec/e90e34656806/resize HTTP/1.1\n    Content-Type: text/plain\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 201 Created\n    Content-Type: text/plain\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of tty session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> </ul> <h3 id=\"exec-inspect\">Exec Inspect</h3> <p><code>GET /exec/(id)/json</code></p> <p>Return low-level information about the exec command <code>id</code>.</p> <p><strong>Example request</strong>:</p> <pre>    GET /exec/11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: plain/text\n\n    {\n      \"ID\" : \"11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39\",\n      \"Running\" : false,\n      \"ExitCode\" : 2,\n      \"ProcessConfig\" : {\n        \"privileged\" : false,\n        \"user\" : \"\",\n        \"tty\" : false,\n        \"entrypoint\" : \"sh\",\n        \"arguments\" : [\n          \"-c\",\n          \"exit 2\"\n        ]\n      },\n      \"OpenStdin\" : false,\n      \"OpenStderr\" : false,\n      \"OpenStdout\" : false,\n      \"Container\" : {\n        \"State\" : {\n          \"Running\" : true,\n          \"Paused\" : false,\n          \"Restarting\" : false,\n          \"OOMKilled\" : false,\n          \"Pid\" : 3650,\n          \"ExitCode\" : 0,\n          \"Error\" : \"\",\n          \"StartedAt\" : \"2014-11-17T22:26:03.717657531Z\",\n          \"FinishedAt\" : \"0001-01-01T00:00:00Z\"\n        },\n        \"ID\" : \"8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c\",\n        \"Created\" : \"2014-11-17T22:26:03.626304998Z\",\n        \"Path\" : \"date\",\n        \"Args\" : [],\n        \"Config\" : {\n          \"Hostname\" : \"8f177a186b97\",\n          \"Domainname\" : \"\",\n          \"User\" : \"\",\n          \"AttachStdin\" : false,\n          \"AttachStdout\" : false,\n          \"AttachStderr\" : false,\n          \"PortSpecs\" : null,\n          \"ExposedPorts\" : null,\n          \"Tty\" : false,\n          \"OpenStdin\" : false,\n          \"StdinOnce\" : false,\n          \"Env\" : [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ],\n          \"Cmd\" : [\n            \"date\"\n          ],\n          \"Image\" : \"ubuntu\",\n          \"Volumes\" : null,\n          \"WorkingDir\" : \"\",\n          \"Entrypoint\" : null,\n          \"NetworkDisabled\" : false,\n          \"MacAddress\" : \"\",\n          \"OnBuild\" : null,\n          \"SecurityOpt\" : null\n        },\n        \"Image\" : \"5506de2b643be1e6febbf3b8a240760c6843244c41e12aa2f60ccbb7153d17f5\",\n        \"NetworkSettings\" : {\n          \"IPAddress\" : \"172.17.0.2\",\n          \"IPPrefixLen\" : 16,\n          \"MacAddress\" : \"02:42:ac:11:00:02\",\n          \"Gateway\" : \"172.17.42.1\",\n          \"Bridge\" : \"docker0\",\n          \"PortMapping\" : null,\n          \"Ports\" : {}\n        },\n        \"ResolvConfPath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/resolv.conf\",\n        \"HostnamePath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/hostname\",\n        \"HostsPath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/hosts\",\n        \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n        \"Name\" : \"/test\",\n        \"Driver\" : \"aufs\",\n        \"ExecDriver\" : \"native-0.2\",\n        \"MountLabel\" : \"\",\n        \"ProcessLabel\" : \"\",\n        \"AppArmorProfile\" : \"\",\n        \"RestartCount\" : 0,\n        \"Volumes\" : {},\n        \"VolumesRW\" : {}\n      }\n    }\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<strong>500</strong> - server error</li> </ul> <h1 id=\"3-going-further\">3. Going further</h1> <h2 id=\"3-1-inside-docker-run\">3.1 Inside <code>docker run</code>\n</h2> <p>As an example, the <code>docker run</code> command line makes the following API calls:</p> <ul> <li><p>Create the container</p></li> <li>\n<p>If the status code is 404, it means the image doesn’t exist:</p> <ul> <li>Try to pull it</li> <li>Then retry to create the container</li> </ul>\n</li> <li><p>Start the container</p></li> <li><p>If you are not in detached mode:</p></li> <li><p>Attach to the container, using logs=1 (to have stdout and stderr from the container’s start) and stream=1</p></li> <li><p>If in detached mode or only stdin is attached:</p></li> <li><p>Display the container’s id</p></li> </ul> <h2 id=\"3-2-hijacking\">3.2 Hijacking</h2> <p>In this version of the API, /attach, uses hijacking to transport stdin, stdout and stderr on the same socket.</p> <p>To hint potential proxies about connection hijacking, Docker client sends connection upgrade headers similarly to websocket.</p> <pre>Upgrade: tcp\nConnection: Upgrade\n</pre> <p>When Docker daemon detects the <code>Upgrade</code> header, it will switch its status code from <strong>200 OK</strong> to <strong>101 UPGRADED</strong> and resend the same headers.</p> <p>This might change in the future.</p> <h2 id=\"3-3-cors-requests\">3.3 CORS Requests</h2> <p>To set cross origin requests to the remote api please give values to “--api-cors-header” when running docker in daemon mode. Set * will allow all, default or blank means CORS disabled</p> <pre>$ docker -d -H=\"192.168.1.9:2375\" --api-cors-header=\"http://foo.bar\"\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.18/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.18/</a>\n  </p>\n</div>\n","swarm/plan-for-production/index":"<h1 id=\"plan-for-swarm-in-production\">Plan for Swarm in production</h1> <p>This article provides guidance to help you plan, deploy, and manage Docker Swarm clusters in business critical production environments. The following high level topics are covered:</p> <ul> <li><a href=\"#security\">Security</a></li> <li><a href=\"#high-availability\">High Availability</a></li> <li><a href=\"#performance\">Performance</a></li> <li><a href=\"#cluster-ownership\">Cluster ownership</a></li> </ul> <h2 id=\"security\">Security</h2> <p>There are many aspects to securing a Docker Swarm cluster. This section covers:</p> <ul> <li>Authentication using TLS</li> <li>Network access control</li> </ul> <p>These topics are not exhaustive. They form part of a wider security architecture that includes: security patching, strong password policies, role based access control, technologies such as SELinux and AppArmor, strict auditing, and more.</p> <h3 id=\"configure-swarm-for-tls\">Configure Swarm for TLS</h3> <p>All nodes in a Swarm cluster must bind their Docker Engine daemons to a network port. This brings with it all of the usual network related security implications such as man-in-the-middle attacks. These risks are compounded when the network in question is untrusted such as the internet. To mitigate these risks, Swarm and the Engine support Transport Layer Security(TLS) for authentication.</p> <p>The Engine daemons, including the Swarm manager, that are configured to use TLS will only accept commands from Docker Engine clients that sign their communications. The Engine and Swarm support external 3rd party Certificate Authorities (CA) as well as internal corporate CAs.</p> <p>The default Engine and Swarm ports for TLS are:</p> <ul> <li>Engine daemon: 2376/tcp</li> <li>Swarm manager: 3376/tcp</li> </ul> <p>For more information on configuring Swarm for TLS, see the <a href=\"../secure-swarm-tls/index\">Overview Docker Swarm with TLS</a> page.</p> <h3 id=\"network-access-control\">Network access control</h3> <p>Production networks are complex, and usually locked down so that only allowed traffic can flow on the network. The list below shows the network ports that the different components of a Swam cluster listen on. You should use these to configure your firewalls and other network access control lists.</p> <ul> <li>\n<strong>Swarm manager.</strong> <ul> <li>\n<strong>Inbound 80/tcp (HTTP)</strong>. This allows <code>docker pull</code> commands to work. If you plan to pull images from Docker Hub, you must allow Internet connections through port 80.</li> <li>\n<strong>Inbound 2375/tcp</strong>. This allows Docker Engine CLI commands direct to the Engine daemon.</li> <li>\n<strong>Inbound 3375/tcp</strong>. This allows Engine CLI commands to the Swarm manager.</li> <li>\n<strong>Inbound 22/tcp</strong>. This allows remote management via SSH</li> </ul>\n</li> <li>\n<strong>Service Discovery</strong>: <ul> <li>\n<strong>Inbound 80/tcp (HTTP)</strong>. This allows <code>docker pull</code> commands to work. If you plan to pull images from Docker Hub, you must allow Internet connections through port 80.</li> <li>\n<strong>Inbound *Discovery service port</strong>*. This needs setting to the port that the backend discovery service listens on (consul, etcd, or zookeeper).</li> <li>\n<strong>Inbound 22/tcp</strong>. This allows remote management via SSH</li> </ul>\n</li> <li>\n<strong>Swarm nodes</strong>: <ul> <li>\n<strong>Inbound 80/tcp (HTTP)</strong>. This allows <code>docker pull</code> commands to work. If you plan to pull images from Docker Hub, you must allow Internet connections through port 80.</li> <li>\n<strong>Inbound 2375/tcp</strong>. This allows Engine CLI commands direct to the Docker daemon.</li> <li>\n<strong>Inbound 22/tcp</strong>. This allows remote management via SSH.</li> </ul>\n</li> <li>\n<strong>Custom, cross-host container networks</strong>: <ul> <li>\n<strong>Inbound 7946/tcp</strong> Allows for discovering other container networks.</li> <li>\n<strong>Inbound 7946/udp</strong> Allows for discovering other container networks.</li> <li>\n<strong>Inbound <store-port>/tcp</store-port></strong> Network key-value store service port.</li> <li>\n<strong>4789/udp</strong> For the container overlay network.</li> </ul>\n</li> </ul> <p>If your firewalls and other network devices are connection state aware, they will allow responses to established TCP connections. If your devices are not state aware, you will need to open up ephemeral ports from 32768-65535. For added security you can configure the ephemeral port rules to only allow connections from interfaces on known Swarm devices.</p> <p>If your Swarm cluster is configured for TLS, replace <code>2375</code> with <code>2376</code>, and <code>3375</code> with <code>3376</code>.</p> <p>The ports listed above are just for Swarm cluster operations such as; cluster creation, cluster management, and scheduling of containers against the cluster. You may need to open additional network ports for application-related communications.</p> <p>It is possible for different components of a Swarm cluster to exist on separate networks. For example, many organizations operate separate management and production networks. Some Docker Engine clients may exist on a management network, while Swarm managers, discovery service instances, and nodes might exist on one or more production networks. To offset against network failures, you can deploy Swarm managers, discovery services, and nodes across multiple production networks. In all of these cases you can use the list of ports above to assist the work of your network infrastructure teams to efficiently and securely configure your network.</p> <h2 id=\"high-availability-ha\">High Availability (HA)</h2> <p>All production environments should be highly available, meaning they are continuously operational over long periods of time. To achieve high availability, an environment must the survive failures of its individual component parts.</p> <p>The following sections discuss some technologies and best practices that can enable you to build resilient, highly-available Swarm clusters. You can then use these cluster to run your most demanding production applications and workloads.</p> <h3 id=\"swarm-manager-ha\">Swarm manager HA</h3> <p>The Swarm manager is responsible for accepting all commands coming in to a Swarm cluster, and scheduling resources against the cluster. If the Swarm manager becomes unavailable, some cluster operations cannot be performed until the Swarm manager becomes available again. This is unacceptable in large-scale business critical scenarios.</p> <p>Swarm provides HA features to mitigate against possible failures of the Swarm manager. You can use Swarm’s HA feature to configure multiple Swarm managers for a single cluster. These Swarm managers operate in an active/passive formation with a single Swarm manager being the <em>primary</em>, and all others being <em>secondaries</em>.</p> <p>Swarm secondary managers operate as <em>warm standby’s</em>, meaning they run in the background of the primary Swarm manager. The secondary Swarm managers are online and accept commands issued to the cluster, just as the primary Swarm manager. However, any commands received by the secondaries are forwarded to the primary where they are executed. Should the primary Swarm manager fail, a new primary is elected from the surviving secondaries.</p> <p>When creating HA Swarm managers, you should take care to distribute them over as many <em>failure domains</em> as possible. A failure domain is a network section that can be negatively affected if a critical device or service experiences problems. For example, if your cluster is running in the Ireland Region of Amazon Web Services (eu-west-1) and you configure three Swarm managers (1 x primary, 2 x secondary), you should place one in each availability zone as shown below.</p> <p><img src=\"http://farm2.staticflickr.com/1657/24581727611_0a076b79de_b.jpg\" alt=\"\"></p> <p>In this configuration, the Swarm cluster can survive the loss of any two availability zones. For your applications to survive such failures, they must be architected across as many failure domains as well.</p> <p>For Swarm clusters serving high-demand, line-of-business applications, you should have 3 or more Swarm managers. This configuration allows you to take one manager down for maintenance, suffer an unexpected failure, and still continue to manage and operate the cluster.</p> <h3 id=\"discovery-service-ha\">Discovery service HA</h3> <p>The discovery service is a key component of a Swarm cluster. If the discovery service becomes unavailable, this can prevent certain cluster operations. For example, without a working discovery service, operations such as adding new nodes to the cluster and making queries against the cluster configuration fail. This is not acceptable in business critical production environments.</p> <p>Swarm supports four backend discovery services:</p> <ul> <li>Hosted (not for production use)</li> <li>Consul</li> <li>etcd</li> <li>Zookeeper</li> </ul> <p>Consul, etcd, and Zookeeper are all suitable for production, and should be configured for high availability. You should use each service’s existing tools and best practices to configure these for HA.</p> <p>For Swarm clusters serving high-demand, line-of-business applications, it is recommended to have 5 or more discovery service instances. This due to the replication/HA technologies they use (such as Paxos/Raft) requiring a strong quorum. Having 5 instances allows you to take one down for maintenance, suffer an unexpected failure, and still be able to achieve a strong quorum.</p> <p>When creating a highly available Swarm discovery service, you should take care to distribute each discovery service instance over as many failure domains as possible. For example, if your cluster is running in the Ireland Region of Amazon Web Services (eu-west-1) and you configure three discovery service instances, you should place one in each availability zone.</p> <p>The diagram below shows a Swarm cluster configured for HA. It has three Swarm managers and three discovery service instances spread over three failure domains (availability zones). It also has Swarm nodes balanced across all three failure domains. The loss of two availability zones in the configuration shown below does not cause the Swarm cluster to go down.</p> <p><img src=\"http://farm2.staticflickr.com/1675/24380252320_999687d2bb_b.jpg\" alt=\"\"></p> <p>It is possible to share the same Consul, etcd, or Zookeeper containers between the Swarm discovery and Engine container networks. However, for best performance and availability you should deploy dedicated instances – a discovery instance for Swarm and another for your container networks.</p> <h3 id=\"multiple-clouds\">Multiple clouds</h3> <p>You can architect and build Swarm clusters that stretch across multiple cloud providers, and even across public cloud and on premises infrastructures. The diagram below shows an example Swarm cluster stretched across AWS and Azure.</p> <p><img src=\"http://farm2.staticflickr.com/1493/24676269945_d19daf856c_b.jpg\" alt=\"\"></p> <p>While such architectures may appear to provide the ultimate in availability, there are several factors to consider. Network latency can be problematic, as can partitioning. As such, you should seriously consider technologies that provide reliable, high speed, low latency connections into these cloud platforms – technologies such as AWS Direct Connect and Azure ExpressRoute.</p> <p>If you are considering a production deployment across multiple infrastructures like this, make sure you have good test coverage over your entire system.</p> <h3 id=\"isolated-production-environments\">Isolated production environments</h3> <p>It is possible to run multiple environments, such as development, staging, and production, on a single Swarm cluster. You accomplish this by tagging Swarm nodes and using constraints to filter containers onto nodes tagged as <code>production</code> or <code>staging</code> etc. However, this is not recommended. The recommended approach is to air-gap production environments, especially high performance business critical production environments.</p> <p>For example, many companies not only deploy dedicated isolated infrastructures for production – such as networks, storage, compute and other systems. They also deploy separate management systems and policies. This results in things like users having separate accounts for logging on to production systems etc. In these types of environments, it is mandatory to deploy dedicated production Swarm clusters that operate on the production hardware infrastructure and follow thorough production management, monitoring, audit and other policies.</p> <h3 id=\"operating-system-selection\">Operating system selection</h3> <p>You should give careful consideration to the operating system that your Swarm infrastructure relies on. This consideration is vital for production environments.</p> <p>It is not unusual for a company to use one operating system in development environments, and a different one in production. A common example of this is to use CentOS in development environments, but then to use Red Hat Enterprise Linux (RHEL) in production. This decision is often a balance between cost and support. CentOS Linux can be downloaded and used for free, but commercial support options are few and far between. Whereas RHEL has an associated support and license cost, but comes with world class commercial support from Red Hat.</p> <p>When choosing the production operating system to use with your Swarm clusters, choose one that closely matches what you have used in development and staging environments. Although containers abstract much of the underlying OS, some features have configuration requirements. For example, to use Docker container networking with Docker Engine 1.10 or higher, your host must have a Linux kernel that is version 3.10 or higher. Refer to the change logs to understand the requirements for a particular version of Docker Engine or Swarm.</p> <p>You should also consider procedures and channels for deploying and potentially patching your production operating systems.</p> <h2 id=\"performance\">Performance</h2> <p>Performance is critical in environments that support business critical line of business applications. The following sections discuss some technologies and best practices that can help you build high performance Swarm clusters.</p> <h3 id=\"container-networks\">Container networks</h3> <p>Docker Engine container networks are overlay networks and can be created across multiple Engine hosts. For this reason, a container network requires a key-value (KV) store to maintain network configuration and state. This KV store can be shared in common with the one used by the Swarm cluster discovery service. However, for best performance and fault isolation, you should deploy individual KV store instances for container networks and Swarm discovery. This is especially so in demanding business critical production environments.</p> <p>Beginning with Docker Engine 1.9, Docker container networks require specific Linux kernel versions. Higher kernel versions are usually preferred, but carry an increased risk of instability because of the newness of the kernel. Where possible, use a kernel version that is already approved for use in your production environment. If you can not use a 3.10 or higher Linux kernel version for production, you should begin the process of approving a newer kernel as early as possible.</p> <h3 id=\"scheduling-strategies\">Scheduling strategies</h3>  <p>Scheduling strategies are how Swarm decides which nodes on a cluster to start containers on. Swarm supports the following strategies:</p> <ul> <li>spread</li> <li>binpack</li> <li>random (not for production use)</li> </ul> <p>You can also write your own.</p> <p><strong>Spread</strong> is the default strategy. It attempts to balance the number of containers evenly across all nodes in the cluster. This is a good choice for high performance clusters, as it spreads container workload across all resources in the cluster. These resources include CPU, RAM, storage, and network bandwidth.</p> <p>If your Swarm nodes are balanced across multiple failure domains, the spread strategy evenly balance containers across those failure domains. However, spread on its own is not aware of the roles of any of those containers, so has no intelligence to spread multiple instances of the same service across failure domains. To achieve this you should use tags and constraints.</p> <p>The <strong>binpack</strong> strategy runs as many containers as possible on a node, effectively filling it up, before scheduling containers on the next node.</p> <p>This means that binpack does not use all cluster resources until the cluster fills up. As a result, applications running on Swarm clusters that operate the binpack strategy might not perform as well as those that operate the spread strategy. However, binpack is a good choice for minimizing infrastructure requirements and cost. For example, imagine you have a 10-node cluster where each node has 16 CPUs and 128GB of RAM. However, your container workload across the entire cluster is only using the equivalent of 6 CPUs and 64GB RAM. The spread strategy would balance containers across all nodes in the cluster. However, the binpack strategy would fit all containers on a single node, potentially allowing you turn off the additional nodes and save on cost.</p> <h2 id=\"ownership-of-swarm-clusters\">Ownership of Swarm clusters</h2> <p>The question of ownership is vital in production environments. It is therefore vital that you consider and agree on all of the following when planning, documenting, and deploying your production Swarm clusters.</p> <ul> <li>Whose budget does the production Swarm infrastructure come out of?</li> <li>Who owns the accounts that can administer and manage the production Swarm cluster?</li> <li>Who is responsible for monitoring the production Swarm infrastructure?</li> <li>Who is responsible for patching and upgrading the production Swarm infrastructure?</li> <li>On-call responsibilities and escalation procedures?</li> </ul> <p>The above is not a complete list, and the answers to the questions will vary depending on how your organization’s and team’s are structured. Some companies are along way down the DevOps route, while others are not. Whatever situation your company is in, it is important that you factor all of the above into the planning, deployment, and ongoing management of your production Swarm clusters.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../swarm_at_scale/index\">Try Swarm at scale</a></li> <li><a href=\"../networking/index\">Swarm and container networks</a></li> <li><a href=\"../multi-manager-setup/index\">High availability in Docker Swarm</a></li> <li><a href=\"https://www.docker.com/products/docker-universal-control-plane\">Universal Control plane</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/plan-for-production/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/plan-for-production/</a>\n  </p>\n</div>\n","swarm/install-manual/index":"<h1 id=\"build-a-swarm-cluster-for-production\">Build a Swarm cluster for production</h1> <p>This page teaches you to deploy a high-availability Docker Swarm cluster. Although the example installation uses the Amazon Web Services (AWS) platform, you can deploy an equivalent Docker Swarm cluster on many other platforms. In this example, you do the following:</p> <ul> <li><a href=\"#prerequisites\">Verify you have the prequisites</a></li> <li><a href=\"#step-1-add-network-security-rules\">Establish basic network security</a></li> <li><a href=\"#step-2-create-your-instances\">Create your nodes</a></li> <li><a href=\"#step-3-install-engine-on-each-node\">Install Engine on each node</a></li> <li><a href=\"#step-4-set-up-a-discovery-backend\">Configure a discovery backend</a></li> <li><a href=\"#step-5-create-swarm-cluster\">Create Swarm cluster</a></li> <li><a href=\"#step-6-communicate-with-the-swarm\">Communicate with the Swarm</a></li> <li><a href=\"#step-7-test-swarm-failover\">Test the high-availability Swarm managers</a></li> <li><a href=\"#additional-resources\">Additional Resources</a></li> </ul> <p>For a gentler introduction to Swarm, try the <a href=\"../install-w-machine/index\">Evaluate Swarm in a sandbox</a> page.</p> <h2 id=\"prerequisites\">Prerequisites</h2> <ul> <li>An Amazon Web Services (AWS) account</li> <li>Familiarity with AWS features and tools, such as: <ul> <li>Elastic Cloud (EC2) Dashboard</li> <li>Virtual Private Cloud (VPC) Dashboard</li> <li>VPC Security groups</li> <li>Connecting to an EC2 instance using SSH</li> </ul>\n</li> </ul> <h2 id=\"step-1-add-network-security-rules\">Step 1. Add network security rules</h2> <p>AWS uses a “security group” to allow specific types of network traffic on your VPC network. The <strong>default</strong> security group’s initial set of rules deny all inbound traffic, allow all outbound traffic, and allow all traffic between instances.</p> <p>You’re going to add a couple of rules to allow inbound SSH connections and inbound container images. This set of rules somewhat protects the Engine, Swarm, and Consul ports. For a production environment, you would apply more restrictive security measures. Do not leave Docker Engine ports unprotected.</p> <p>From your AWS home console, do the following:</p> <ol> <li>\n<p>Click <strong>VPC - Isolated Cloud Resources</strong>.</p> <p>The VPC Dashboard opens.</p>\n</li> <li><p>Navigate to <strong>Security Groups</strong>.</p></li> <li><p>Select the <strong>default</strong> security group that’s associated with your default VPC.</p></li> <li>\n<p>Add the following two rules.</p> <table> <tr> <th>Type</th> <th>Protocol</th> <th>Port Range</th> <th>Source</th> </tr> <tr> <td>SSH</td> <td>TCP</td> <td>22</td> <td>0.0.0.0/0</td> </tr> <tr> <td>HTTP</td> <td>TCP</td> <td>80</td> <td>0.0.0.0/0</td> </tr> </table>\n</li> </ol> <p>The SSH connection allows you to connect to the host while the HTTP is for container images.</p> <h2 id=\"step-2-create-your-instances\">Step 2. Create your instances</h2> <p>In this step, you create five Linux hosts that are part of your default security group. When complete, the example deployment contains three types of nodes:</p> <table> <thead> <tr> <th>Node Description</th> <th>Name</th> </tr> </thead> <tbody> <tr> <td>Swarm primary and secondary managers</td> <td>\n<code>manager0</code>, <code>manager1</code>\n</td> </tr> <tr> <td>Swarm node</td> <td>\n<code>node0</code>, <code>node1</code>\n</td> </tr> <tr> <td>Discovery backend</td> <td><code>consul0</code></td> </tr> </tbody> </table> <p>To create the instances do the following:</p> <ol> <li>\n<p>Open the EC2 Dashboard and launch four EC2 instances, one at a time.</p> <ul> <li><p>During <strong>Step 1: Choose an Amazon Machine Image (AMI)</strong>, pick the <em>Amazon Linux AMI</em>.</p></li> <li>\n<p>During <strong>Step 5: Tag Instance</strong>, under <strong>Value</strong>, give each instance one of these names:</p> <ul> <li><code>manager0</code></li> <li><code>manager1</code></li> <li><code>consul0</code></li> <li><code>node0</code></li> <li><code>node1</code></li> </ul>\n</li> <li><p>During <strong>Step 6: Configure Security Group</strong>, choose <strong>Select an existing security group</strong> and pick the “default” security group.</p></li> </ul>\n</li> <li><p>Review and launch your instances.</p></li> </ol> <h2 id=\"step-3-install-engine-on-each-node\">Step 3. Install Engine on each node</h2> <p>In this step, you install Docker Engine on each node. By installing Engine, you enable the Swarm manager to address the nodes via the Engine CLI and API.</p> <p>SSH to each node in turn and do the following.</p> <ol> <li>\n<p>Update the yum packages.</p> <p>Keep an eye out for the “y/n/abort” prompt:</p> <pre>$ sudo yum update\n</pre>\n</li> <li>\n<p>Run the installation script.</p> <pre>$ curl -sSL https://get.docker.com/ | sh\n</pre>\n</li> <li>\n<p>Configure and start Engine so it listens for Swarm nodes on port <code>2375</code>.</p> <pre>$ sudo docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\n</pre>\n</li> <li>\n<p>Verify that Docker Engine is installed correctly:</p> <pre>$ sudo docker run hello-world\n</pre> <p>The output should display a “Hello World” message and other text without any error messages.</p>\n</li> <li>\n<p>Give the <code>ec2-user</code> root privileges:</p> <pre>$ sudo usermod -aG docker ec2-user\n</pre>\n</li> <li><p>Enter <code>logout</code>.</p></li> </ol> <h4 id=\"troubleshooting\">Troubleshooting</h4> <ul> <li><p>If entering a <code>docker</code> command produces a message asking whether docker is available on this host, it may be because the user doesn’t have root privileges. If so, use <code>sudo</code> or give the user root privileges.</p></li> <li><p>For this example, don’t create an AMI image from one of your instances running Docker Engine and then re-use it to create the other instances. Doing so will produce errors.</p></li> <li><p>If your host cannot reach Docker Hub, the <code>docker run</code> commands that pull container images may fail. In that case, check that your VPC is associated with a security group with a rule that allows inbound traffic (e.g., HTTP/TCP/80/0.0.0.0/0). Also Check the <a href=\"http://status.docker.com/\">Docker Hub status page</a> for service availability.</p></li> </ul> <h2 id=\"step-4-set-up-a-discovery-backend\">Step 4. Set up a discovery backend</h2> <p>Here, you’re going to create a minimalist discovery backend. The Swarm managers and nodes use this backend to authenticate themselves as members of the cluster. The Swarm managers also use this information to identify which nodes are available to run containers.</p> <p>To keep things simple, you are going to run a single consul daemon on the same host as one of the Swarm managers.</p> <ol> <li>\n<p>To start, copy the following launch command to a text file.</p> <pre>$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap\n</pre>\n</li> <li>\n<p>Use SSH to connect to the <code>manager0</code> and <code>consul0</code> instance.</p> <pre>$ ifconfig\n</pre>\n</li> <li><p>From the output, copy the <code>eth0</code> IP address from <code>inet addr</code>.</p></li> <li><p>Using SSH, connect to the <code>manager0</code> and <code>consul0</code> instance.</p></li> <li>\n<p>Paste the launch command you created in step 1. into the command line.</p> <pre>$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap\n</pre>\n</li> </ol> <p>Your Consul node is up and running, providing your cluster with a discovery backend. To increase its reliability, you can create a high-availability cluster using a trio of consul nodes using the link mentioned at the end of this page. (Before creating a cluster of consul nodes, update the VPC security group with rules to allow inbound traffic on the required port numbers.)</p> <h2 id=\"step-5-create-swarm-cluster\">Step 5. Create Swarm cluster</h2> <p>After creating the discovery backend, you can create the Swarm managers. In this step, you are going to create two Swarm managers in a high-availability configuration. The first manager you run becomes the Swarm’s <em>primary manager</em>. Some documentation still refers to a primary manager as a “master”, but that term has been superseded. The second manager you run serves as a <em>replica</em>. If the primary manager becomes unavailable, the cluster elects the replica as the primary manager.</p> <ol> <li>\n<p>To create the primary manager in a high-availability Swarm cluster, use the following syntax:</p> <pre>$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;manager0_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500\n</pre> <p>Because this is particular manager is on the same <code>manager0</code> and <code>consul0</code> instance as the consul node, replace both <code>&lt;manager0_ip&gt;</code> and <code>&lt;consul_ip&gt;</code> with the same IP address. For example:</p> <pre>$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise 172.30.0.161:4000 consul://172.30.0.161:8500\n</pre>\n</li> <li>\n<p>Enter <code>docker ps</code>.</p> <p>From the output, verify that both a Swarm cluster and a consul container are running. Then, disconnect from the <code>manager0</code> and <code>consul0</code> instance.</p>\n</li> <li>\n<p>Connect to the <code>manager1</code> node and use <code>ifconfig</code> to get its IP address.</p> <pre>$ ifconfig\n</pre>\n</li> <li>\n<p>Start the secondary Swarm manager using following command.</p> <p>Replacing <code>&lt;manager1_ip&gt;</code> with the IP address from the previous command, for example:</p> <pre>$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;manager1_ip&gt;:4000 consul://172.30.0.161:8500\n</pre>\n</li> <li><p>Enter <code>docker ps</code>to verify that a Swarm container is running.</p></li> <li>\n<p>Connect to <code>node0</code> and <code>node1</code> in turn and join them to the cluster.</p> <p>a. Get the node IP addresses with the <code>ifconfig</code> command.</p> <p>b. Start a Swarm container each using the following syntax:</p> <pre>docker run -d swarm join --advertise=&lt;node_ip&gt;:2375 consul://&lt;consul_ip&gt;:8500\n</pre> <p>For example:</p> <pre>$ docker run -d swarm join --advertise=172.30.0.69:2375 consul://172.30.0.161:8500\n</pre>\n</li> </ol> <p>Your small Swarm cluster is up and running on multiple hosts, providing you with a high-availability virtual Docker Engine. To increase its reliability and capacity, you can add more Swarm managers, nodes, and a high-availability discovery backend.</p> <h2 id=\"step-6-communicate-with-the-swarm\">Step 6. Communicate with the Swarm</h2> <p>You can communicate with the Swarm to get information about the managers and nodes using the Swarm API, which is nearly the same as the standard Docker API. In this example, you use SSL to connect to <code>manager0</code> and <code>consul0</code> host again. Then, you address commands to the Swarm manager.</p> <ol> <li>\n<p>Get information about the manager and nodes in the cluster:</p> <pre>$ docker -H :4000 info\n</pre> <p>The output gives the manager’s role as primary (<code>Role: primary</code>) and information about each of the nodes.</p>\n</li> <li>\n<p>Run an application on the Swarm:</p> <pre>$ docker -H :4000 run hello-world\n</pre>\n</li> <li>\n<p>Check which Swarm node ran the application:</p> <pre>$ docker -H :4000 ps\n</pre>\n</li> </ol> <h2 id=\"step-7-test-swarm-failover\">Step 7. Test Swarm failover</h2> <p>To see the replica instance take over, you’re going to shut down the primary manager. Doing so kicks off an election, and the replica becomes the primary manager. When you start the manager you shut down earlier, it becomes the replica.</p> <ol> <li><p>SSH connection to the <code>manager0</code> instance.</p></li> <li>\n<p>Get the container id or name of the <code>swarm</code> container:</p> <pre>$ docker ps\n</pre>\n</li> <li>\n<p>Shut down the primary manager, replacing <code>&lt;id_name&gt;</code> with the container’s id or name (for example, “8862717fe6d3” or “trusting_lamarr”).</p> <pre>docker rm -f &lt;id_name&gt;\n</pre>\n</li> <li>\n<p>Start the Swarm manager. For example:</p> <pre>$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise 172.30.0.161:4000 consul://172.30.0.161:8500\n</pre>\n</li> <li>\n<p>Review the Engine’s daemon logs the logs, replacing <code>&lt;id_name&gt;</code> with the new container’s id or name:</p> <pre>$ sudo docker logs &lt;id_name&gt;\n</pre> <p>The output shows will show two entries like these ones:</p> <pre>time=\"2016-02-02T02:12:32Z\" level=info msg=\"Leader Election: Cluster leadership lost\"\ntime=\"2016-02-02T02:12:32Z\" level=info msg=\"New leader elected: 172.30.0.160:4000\"\n</pre>\n</li> <li>\n<p>To get information about the manager and nodes in the cluster, enter:</p> <pre>$ docker -H :4000 info\n</pre>\n</li> </ol> <p>You can connect to the <code>manager1</code> node and run the <code>info</code> and <code>logs</code> commands. They will display corresponding entries for the change in leadership.</p> <h2 id=\"additional-resources\">Additional Resources</h2> <ul> <li><a href=\"http://docs.docker.com/engine/installation/cloud/cloud-ex-aws/\">Installing Docker Engine on a cloud provider</a></li> <li><a href=\"../multi-manager-setup/index\">High availability in Docker Swarm</a></li> <li><a href=\"../discovery/index\">Discovery</a></li> <li><a href=\"https://hub.docker.com/r/progrium/consul/\">High-availability cluster using a trio of consul nodes</a></li> <li><a href=\"../networking/index\">Networking</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/install-manual/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/install-manual/</a>\n  </p>\n</div>\n","engine/reference/api/docker_remote_api_v1.21/index":"<h1 id=\"docker-remote-api-v1-21\">Docker Remote API v1.21</h1> <h2 id=\"1-brief-introduction\">1. Brief introduction</h2> <ul> <li>The Remote API has replaced <code>rcli</code>.</li> <li>The daemon listens on <code>unix:///var/run/docker.sock</code> but you can <a href=\"../../../quickstart/index#bind-docker-to-another-host-port-or-a-unix-socket\">Bind Docker to another host/port or a Unix socket</a>.</li> <li>The API tends to be REST. However, for some complex commands, like <code>attach</code> or <code>pull</code>, the HTTP connection is hijacked to transport <code>stdout</code>, <code>stdin</code> and <code>stderr</code>.</li> <li>When the client API version is newer than the daemon’s, these calls return an HTTP <code>400 Bad Request</code> error message.</li> </ul> <h1 id=\"2-endpoints\">2. Endpoints</h1> <h2 id=\"2-1-containers\">2.1 Containers</h2> <h3 id=\"list-containers\">List containers</h3> <p><code>GET /containers/json</code></p> <p>List containers</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/json?all=1&amp;before=8dfafdbc3a40&amp;size=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Id\": \"8dfafdbc3a40\",\n             \"Names\":[\"/boring_feynman\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 1\",\n             \"Created\": 1367854155,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [{\"PrivatePort\": 2222, \"PublicPort\": 3333, \"Type\": \"tcp\"}],\n             \"Labels\": {\n                     \"com.example.vendor\": \"Acme\",\n                     \"com.example.license\": \"GPL\",\n                     \"com.example.version\": \"1.0\"\n             },\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     },\n     {\n             \"Id\": \"9cd87474be90\",\n             \"Names\":[\"/coolName\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 222222\",\n             \"Created\": 1367854155,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     },\n     {\n             \"Id\": \"3176a2479c92\",\n             \"Names\":[\"/sleepy_dog\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 3333333333333333\",\n             \"Created\": 1367854154,\n             \"Status\": \"Exit 0\",\n             \"Ports\":[],\n             \"Labels\": {},\n             \"SizeRw\":12288,\n             \"SizeRootFs\":0\n     },\n     {\n             \"Id\": \"4cb07b47f9fb\",\n             \"Names\":[\"/running_cat\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 444444444444444444444444444444444\",\n             \"Created\": 1367854152,\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0\n     }\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, Show all containers. Only running containers are shown by default (i.e., this defaults to false)</li> <li>\n<strong>limit</strong> – Show <code>limit</code> last created containers, include non-running ones.</li> <li>\n<strong>since</strong> – Show only containers created since Id, include non-running ones.</li> <li>\n<strong>before</strong> – Show only containers created before Id, include non-running ones.</li> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, Show the containers sizes</li> <li>\n<strong>filters</strong> - a JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the containers list. Available filters: <ul> <li>\n<code>exited=&lt;int&gt;</code>; -- containers with exit code of <code>&lt;int&gt;</code> ;</li> <li>\n<code>status=</code>(<code>created</code>|<code>restarting</code>|<code>running</code>|<code>paused</code>|<code>exited</code>)</li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of a container label</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-a-container\">Create a container</h3> <p><code>POST /containers/create</code></p> <p>Create a container</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/create HTTP/1.1\nContent-Type: application/json\n\n{\n       \"Hostname\": \"\",\n       \"Domainname\": \"\",\n       \"User\": \"\",\n       \"AttachStdin\": false,\n       \"AttachStdout\": true,\n       \"AttachStderr\": true,\n       \"Tty\": false,\n       \"OpenStdin\": false,\n       \"StdinOnce\": false,\n       \"Env\": [\n               \"FOO=bar\",\n               \"BAZ=quux\"\n       ],\n       \"Cmd\": [\n               \"date\"\n       ],\n       \"Entrypoint\": \"\",\n       \"Image\": \"ubuntu\",\n       \"Labels\": {\n               \"com.example.vendor\": \"Acme\",\n               \"com.example.license\": \"GPL\",\n               \"com.example.version\": \"1.0\"\n       },\n       \"Volumes\": {\n         \"/volumes/data\": {}\n       },\n       \"WorkingDir\": \"\",\n       \"NetworkDisabled\": false,\n       \"MacAddress\": \"12:34:56:78:9a:bc\",\n       \"ExposedPorts\": {\n               \"22/tcp\": {}\n       },\n       \"StopSignal\": \"SIGTERM\",\n       \"HostConfig\": {\n         \"Binds\": [\"/tmp:/tmp\"],\n         \"Links\": [\"redis3:redis\"],\n         \"LxcConf\": {\"lxc.utsname\":\"docker\"},\n         \"Memory\": 0,\n         \"MemorySwap\": 0,\n         \"MemoryReservation\": 0,\n         \"KernelMemory\": 0,\n         \"CpuShares\": 512,\n         \"CpuPeriod\": 100000,\n         \"CpuQuota\": 50000,\n         \"CpusetCpus\": \"0,1\",\n         \"CpusetMems\": \"0,1\",\n         \"BlkioWeight\": 300,\n         \"MemorySwappiness\": 60,\n         \"OomKillDisable\": false,\n         \"PortBindings\": { \"22/tcp\": [{ \"HostPort\": \"11022\" }] },\n         \"PublishAllPorts\": false,\n         \"Privileged\": false,\n         \"ReadonlyRootfs\": false,\n         \"Dns\": [\"8.8.8.8\"],\n         \"DnsOptions\": [\"\"],\n         \"DnsSearch\": [\"\"],\n         \"ExtraHosts\": null,\n         \"VolumesFrom\": [\"parent\", \"other:ro\"],\n         \"CapAdd\": [\"NET_ADMIN\"],\n         \"CapDrop\": [\"MKNOD\"],\n         \"GroupAdd\": [\"newgroup\"],\n         \"RestartPolicy\": { \"Name\": \"\", \"MaximumRetryCount\": 0 },\n         \"NetworkMode\": \"bridge\",\n         \"Devices\": [],\n         \"Ulimits\": [{}],\n         \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} },\n         \"SecurityOpt\": [],\n         \"CgroupParent\": \"\",\n         \"VolumeDriver\": \"\"\n      }\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 201 Created\n  Content-Type: application/json\n\n  {\n       \"Id\":\"e90e34656806\",\n       \"Warnings\":[]\n  }\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Hostname</strong> - A string value containing the hostname to use for the container.</li> <li>\n<strong>Domainname</strong> - A string value containing the domain name to use for the container.</li> <li>\n<strong>User</strong> - A string value specifying the user inside the container.</li> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code>.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code>.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code>.</li> <li>\n<strong>Tty</strong> - Boolean value, Attach standard streams to a <code>tty</code>, including <code>stdin</code> if it is not closed.</li> <li>\n<strong>OpenStdin</strong> - Boolean value, opens stdin,</li> <li>\n<strong>StdinOnce</strong> - Boolean value, close <code>stdin</code> after the 1 attached client disconnects.</li> <li>\n<strong>Env</strong> - A list of environment variables in the form of <code>[\"VAR=value\"[,\"VAR2=value2\"]]</code>\n</li> <li>\n<strong>Labels</strong> - Adds a map of labels to a container. To specify a map: <code>{\"key\":\"value\"[,\"key2\":\"value2\"]}</code>\n</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> <li>\n<strong>Entrypoint</strong> - Set the entry point for the container as a string or an array of strings.</li> <li>\n<strong>Image</strong> - A string specifying the image name to use for the container.</li> <li>\n<strong>Volumes</strong> - An object mapping mount point paths (strings) inside the container to empty objects.</li> <li>\n<strong>WorkingDir</strong> - A string specifying the working directory for commands to run in.</li> <li>\n<strong>NetworkDisabled</strong> - Boolean value, when true disables networking for the container</li> <li>\n<strong>ExposedPorts</strong> - An object mapping ports to an empty object in the form of: <code>\"ExposedPorts\": { \"&lt;port&gt;/&lt;tcp|udp&gt;: {}\" }</code>\n</li> <li>\n<strong>StopSignal</strong> - Signal to stop a container as a string or unsigned integer. <code>SIGTERM</code> by default.</li> <li>\n<strong>HostConfig</strong> <ul> <li>\n<strong>Binds</strong> – A list of volume bindings for this container. Each volume binding is a string in one of these forms: <ul> <li>\n<code>container_path</code> to create a new volume for the container</li> <li>\n<code>host_path:container_path</code> to bind-mount a host path into the container</li> <li>\n<code>host_path:container_path:ro</code> to make the bind-mount read-only inside the container.</li> <li>\n<code>volume_name:container_path</code> to bind-mount a volume managed by a volume plugin into the container.</li> <li>\n<code>volume_name:container_path:ro</code> to make the bind mount read-only inside the container.</li> </ul>\n</li> <li>\n<strong>Links</strong> - A list of links for the container. Each link entry should be in the form of <code>container_name:alias</code>.</li> <li>\n<strong>LxcConf</strong> - LXC specific configurations. These configurations only work when using the <code>lxc</code> execution driver.</li> <li>\n<strong>Memory</strong> - Memory limit in bytes.</li> <li>\n<strong>MemorySwap</strong> - Total memory limit (memory + swap); set <code>-1</code> to enable unlimited swap. You must use this with <code>memory</code> and make the swap value larger than <code>memory</code>.</li> <li>\n<strong>MemoryReservation</strong> - Memory soft limit in bytes.</li> <li>\n<strong>KernelMemory</strong> - Kernel memory limit in bytes.</li> <li>\n<strong>CpuShares</strong> - An integer value containing the container’s CPU Shares (ie. the relative weight vs other containers).</li> <li>\n<strong>CpuPeriod</strong> - The length of a CPU period in microseconds.</li> <li>\n<strong>CpuQuota</strong> - Microseconds of CPU time that the container can get in a CPU period.</li> <li>\n<strong>CpusetCpus</strong> - String value containing the <code>cgroups CpusetCpus</code> to use.</li> <li>\n<strong>CpusetMems</strong> - Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems.</li> <li>\n<strong>BlkioWeight</strong> - Block IO weight (relative weight) accepts a weight value between 10 and 1000.</li> <li>\n<strong>MemorySwappiness</strong> - Tune a container’s memory swappiness behavior. Accepts an integer between 0 and 100.</li> <li>\n<strong>OomKillDisable</strong> - Boolean value, whether to disable OOM Killer for the container or not.</li> <li>\n<strong>PortBindings</strong> - A map of exposed container ports and the host port they should map to. A JSON object in the form <code>{ &lt;port&gt;/&lt;protocol&gt;: [{ \"HostPort\": \"&lt;port&gt;\" }] }</code> Take note that <code>port</code> is specified as a string and not an integer value.</li> <li>\n<strong>PublishAllPorts</strong> - Allocates a random host port for all of a container’s exposed ports. Specified as a boolean value.</li> <li>\n<strong>Privileged</strong> - Gives the container full access to the host. Specified as a boolean value.</li> <li>\n<strong>ReadonlyRootfs</strong> - Mount the container’s root filesystem as read only. Specified as a boolean value.</li> <li>\n<strong>Dns</strong> - A list of DNS servers for the container to use.</li> <li>\n<strong>DnsOptions</strong> - A list of DNS options</li> <li>\n<strong>DnsSearch</strong> - A list of DNS search domains</li> <li>\n<strong>ExtraHosts</strong> - A list of hostnames/IP mappings to add to the container’s <code>/etc/hosts</code> file. Specified in the form <code>[\"hostname:IP\"]</code>.</li> <li>\n<strong>VolumesFrom</strong> - A list of volumes to inherit from another container. Specified in the form <code>&lt;container name&gt;[:&lt;ro|rw&gt;]</code>\n</li> <li>\n<strong>CapAdd</strong> - A list of kernel capabilities to add to the container.</li> <li>\n<strong>Capdrop</strong> - A list of kernel capabilities to drop from the container.</li> <li>\n<strong>GroupAdd</strong> - A list of additional groups that the container process will run as</li> <li>\n<strong>RestartPolicy</strong> – The behavior to apply when the container exits. The value is an object with a <code>Name</code> property of either <code>\"always\"</code> to always restart, <code>\"unless-stopped\"</code> to restart always except when user has manually stopped the container or <code>\"on-failure\"</code> to restart only when the container exit code is non-zero. If <code>on-failure</code> is used, <code>MaximumRetryCount</code> controls the number of times to retry before giving up. The default is not to restart. (optional) An ever increasing delay (double the previous delay, starting at 100mS) is added before each restart to prevent flooding the server.</li> <li>\n<strong>NetworkMode</strong> - Sets the networking mode for the container. Supported standard values are: <code>bridge</code>, <code>host</code>, <code>none</code>, and <code>container:&lt;name|id&gt;</code>. Any other value is taken as a custom network’s name to which this container should connect to.</li> <li>\n<strong>Devices</strong> - A list of devices to add to the container specified as a JSON object in the form <code>{ \"PathOnHost\": \"/dev/deviceName\", \"PathInContainer\": \"/dev/deviceName\", \"CgroupPermissions\": \"mrw\"}</code>\n</li> <li>\n<strong>Ulimits</strong> - A list of ulimits to set in the container, specified as <code>{ \"Name\": &lt;name&gt;, \"Soft\": &lt;soft limit&gt;, \"Hard\": &lt;hard limit&gt; }</code>, for example: <code>Ulimits: { \"Name\": \"nofile\", \"Soft\": 1024, \"Hard\": 2048 }</code>\n</li> <li>\n<strong>SecurityOpt</strong>: A list of string values to customize labels for MLS systems, such as SELinux.</li> <li>\n<strong>LogConfig</strong> - Log configuration for the container, specified as a JSON object in the form <code>{ \"Type\": \"&lt;driver_name&gt;\", \"Config\": {\"key1\": \"val1\"}}</code>. Available types: <code>json-file</code>, <code>syslog</code>, <code>journald</code>, <code>gelf</code>, <code>awslogs</code>, <code>none</code>. <code>json-file</code> logging driver.</li> <li>\n<strong>CgroupParent</strong> - Path to <code>cgroups</code> under which the container’s <code>cgroup</code> is created. If the path is not absolute, the path is considered to be relative to the <code>cgroups</code> path of the init process. Cgroups are created if they do not already exist.</li> <li>\n<strong>VolumeDriver</strong> - Driver that this container users to mount volumes.</li> </ul>\n</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – Assign the specified name to the container. Must match <code>/?[a-zA-Z0-9_-]+</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>406</strong> – impossible to attach (container not running)</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-a-container\">Inspect a container</h3> <p><code>GET /containers/(id or name)/json</code></p> <p>Return low-level information on the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>  GET /containers/4fa6e0f0c678/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"AppArmorProfile\": \"\",\n    \"Args\": [\n        \"-c\",\n        \"exit 9\"\n    ],\n    \"Config\": {\n        \"AttachStderr\": true,\n        \"AttachStdin\": false,\n        \"AttachStdout\": true,\n        \"Cmd\": [\n            \"/bin/sh\",\n            \"-c\",\n            \"exit 9\"\n        ],\n        \"Domainname\": \"\",\n        \"Entrypoint\": null,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": null,\n        \"Hostname\": \"ba033ac44011\",\n        \"Image\": \"ubuntu\",\n        \"Labels\": {\n            \"com.example.vendor\": \"Acme\",\n            \"com.example.license\": \"GPL\",\n            \"com.example.version\": \"1.0\"\n        },\n        \"MacAddress\": \"\",\n        \"NetworkDisabled\": false,\n        \"OnBuild\": null,\n        \"OpenStdin\": false,\n        \"StdinOnce\": false,\n        \"Tty\": false,\n        \"User\": \"\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\",\n        \"StopSignal\": \"SIGTERM\"\n    },\n    \"Created\": \"2015-01-06T15:47:31.485331387Z\",\n    \"Driver\": \"devicemapper\",\n    \"ExecDriver\": \"native-0.2\",\n    \"ExecIDs\": null,\n    \"HostConfig\": {\n        \"Binds\": null,\n        \"BlkioWeight\": 0,\n        \"CapAdd\": null,\n        \"CapDrop\": null,\n        \"ContainerIDFile\": \"\",\n        \"CpusetCpus\": \"\",\n        \"CpusetMems\": \"\",\n        \"CpuShares\": 0,\n        \"CpuPeriod\": 100000,\n        \"Devices\": [],\n        \"Dns\": null,\n        \"DnsOptions\": null,\n        \"DnsSearch\": null,\n        \"ExtraHosts\": null,\n        \"IpcMode\": \"\",\n        \"Links\": null,\n        \"LxcConf\": [],\n        \"Memory\": 0,\n        \"MemorySwap\": 0,\n        \"MemoryReservation\": 0,\n        \"KernelMemory\": 0,\n        \"OomKillDisable\": false,\n        \"NetworkMode\": \"bridge\",\n        \"PortBindings\": {},\n        \"Privileged\": false,\n        \"ReadonlyRootfs\": false,\n        \"PublishAllPorts\": false,\n        \"RestartPolicy\": {\n            \"MaximumRetryCount\": 2,\n            \"Name\": \"on-failure\"\n        },\n        \"LogConfig\": {\n            \"Config\": null,\n            \"Type\": \"json-file\"\n        },\n        \"SecurityOpt\": null,\n        \"VolumesFrom\": null,\n        \"Ulimits\": [{}],\n        \"VolumeDriver\": \"\"\n    },\n    \"HostnamePath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hostname\",\n    \"HostsPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Id\": \"ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39\",\n    \"Image\": \"04c5d3b7b0656168630d3ba35d8889bd0e9caafcaeb3004d2bfbc47e7c5d35d2\",\n    \"MountLabel\": \"\",\n    \"Name\": \"/boring_euclid\",\n    \"NetworkSettings\": {\n        \"Bridge\": \"\",\n        \"SandboxID\": \"\",\n        \"HairpinMode\": false,\n        \"LinkLocalIPv6Address\": \"\",\n        \"LinkLocalIPv6PrefixLen\": 0,\n        \"Ports\": null,\n        \"SandboxKey\": \"\",\n        \"SecondaryIPAddresses\": null,\n        \"SecondaryIPv6Addresses\": null,\n        \"EndpointID\": \"\",\n        \"Gateway\": \"\",\n        \"GlobalIPv6Address\": \"\",\n        \"GlobalIPv6PrefixLen\": 0,\n        \"IPAddress\": \"\",\n        \"IPPrefixLen\": 0,\n        \"IPv6Gateway\": \"\",\n        \"MacAddress\": \"\",\n        \"Networks\": {\n            \"bridge\": {\n                \"EndpointID\": \"\",\n                \"Gateway\": \"\",\n                \"IPAddress\": \"\",\n                \"IPPrefixLen\": 0,\n                \"IPv6Gateway\": \"\",\n                \"GlobalIPv6Address\": \"\",\n                \"GlobalIPv6PrefixLen\": 0,\n                \"MacAddress\": \"\"\n            }\n        }\n    },\n    \"Path\": \"/bin/sh\",\n    \"ProcessLabel\": \"\",\n    \"ResolvConfPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/resolv.conf\",\n    \"RestartCount\": 1,\n    \"State\": {\n        \"Error\": \"\",\n        \"ExitCode\": 9,\n        \"FinishedAt\": \"2015-01-06T15:47:32.080254511Z\",\n        \"OOMKilled\": false,\n        \"Paused\": false,\n        \"Pid\": 0,\n        \"Restarting\": false,\n        \"Running\": true,\n        \"StartedAt\": \"2015-01-06T15:47:32.072697474Z\",\n        \"Status\": \"running\"\n    },\n    \"Mounts\": [\n        {\n            \"Source\": \"/data\",\n            \"Destination\": \"/data\",\n            \"Mode\": \"ro,Z\",\n            \"RW\": false\n        }\n    ]\n}\n</pre> <p><strong>Example request, with size information</strong>:</p> <pre>GET /containers/4fa6e0f0c678/json?size=1 HTTP/1.1\n</pre> <p><strong>Example response, with size information</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n....\n\"SizeRw\": 0,\n\"SizeRootFs\": 972,\n....\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>size</strong> – 1/True/true or 0/False/false, return container size information. Default is <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"list-processes-running-inside-a-container\">List processes running inside a container</h3> <p><code>GET /containers/(id or name)/top</code></p> <p>List processes running inside the container <code>id</code>. On Unix systems this is done by running the <code>ps</code> command. This endpoint is not supported on Windows.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Titles\" : [\n     \"UID\", \"PID\", \"PPID\", \"C\", \"STIME\", \"TTY\", \"TIME\", \"CMD\"\n   ],\n   \"Processes\" : [\n     [\n       \"root\", \"13642\", \"882\", \"0\", \"17:03\", \"pts/0\", \"00:00:00\", \"/bin/bash\"\n     ],\n     [\n       \"root\", \"13735\", \"13642\", \"0\", \"17:06\", \"pts/0\", \"00:00:00\", \"sleep 10\"\n     ]\n   ]\n}\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/top?ps_args=aux HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Titles\" : [\n    \"USER\",\"PID\",\"%CPU\",\"%MEM\",\"VSZ\",\"RSS\",\"TTY\",\"STAT\",\"START\",\"TIME\",\"COMMAND\"\n  ]\n  \"Processes\" : [\n    [\n      \"root\",\"13642\",\"0.0\",\"0.1\",\"18172\",\"3184\",\"pts/0\",\"Ss\",\"17:03\",\"0:00\",\"/bin/bash\"\n    ],\n    [\n      \"root\",\"13895\",\"0.0\",\"0.0\",\"4348\",\"692\",\"pts/0\",\"S+\",\"17:15\",\"0:00\",\"sleep 10\"\n    ]\n  ],\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>ps_args</strong> – <code>ps</code> arguments to use (e.g., <code>aux</code>), defaults to <code>-ef</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-logs\">Get container logs</h3> <p><code>GET /containers/(id or name)/logs</code></p> <p>Get <code>stdout</code> and <code>stderr</code> logs from the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: This endpoint works only for containers with the <code>json-file</code> or <code>journald</code> logging drivers.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre> GET /containers/4fa6e0f0c678/logs?stderr=1&amp;stdout=1&amp;timestamps=1&amp;follow=1&amp;tail=10&amp;since=1428990821 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre> HTTP/1.1 101 UPGRADED\n Content-Type: application/vnd.docker.raw-stream\n Connection: Upgrade\n Upgrade: tcp\n\n {{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>follow</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, show <code>stdout</code> log. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, show <code>stderr</code> log. Default <code>false</code>.</li> <li>\n<strong>since</strong> – UNIX timestamp (integer) to filter logs. Specifying a timestamp will only output log-entries since that timestamp. Default: 0 (unfiltered)</li> <li>\n<strong>timestamps</strong> – 1/True/true or 0/False/false, print timestamps for every log line. Default <code>false</code>.</li> <li>\n<strong>tail</strong> – Output specified number of lines at the end of logs: <code>all</code> or <code>&lt;number&gt;</code>. Default all.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-changes-on-a-container-s-filesystem\">Inspect changes on a container’s filesystem</h3> <p><code>GET /containers/(id or name)/changes</code></p> <p>Inspect changes on container <code>id</code>’s filesystem</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/changes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Path\": \"/dev\",\n             \"Kind\": 0\n     },\n     {\n             \"Path\": \"/dev/kmsg\",\n             \"Kind\": 1\n     },\n     {\n             \"Path\": \"/test\",\n             \"Kind\": 1\n     }\n]\n</pre> <p>Values for <code>Kind</code>:</p> <ul> <li>\n<code>0</code>: Modify</li> <li>\n<code>1</code>: Add</li> <li>\n<code>2</code>: Delete</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"export-a-container\">Export a container</h3> <p><code>GET /containers/(id or name)/export</code></p> <p>Export the contents of container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>GET /containers/4fa6e0f0c678/export HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/octet-stream\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-container-stats-based-on-resource-usage\">Get container stats based on resource usage</h3> <p><code>GET /containers/(id or name)/stats</code></p> <p>This endpoint returns a live stream of a container’s resource usage statistics.</p> <p><strong>Example request</strong>:</p> <pre>GET /containers/redis1/stats HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Type: application/json\n\n  {\n     \"read\" : \"2015-01-08T22:57:31.547920715Z\",\n     \"networks\": {\n             \"eth0\": {\n                 \"rx_bytes\": 5338,\n                 \"rx_dropped\": 0,\n                 \"rx_errors\": 0,\n                 \"rx_packets\": 36,\n                 \"tx_bytes\": 648,\n                 \"tx_dropped\": 0,\n                 \"tx_errors\": 0,\n                 \"tx_packets\": 8\n             },\n             \"eth5\": {\n                 \"rx_bytes\": 4641,\n                 \"rx_dropped\": 0,\n                 \"rx_errors\": 0,\n                 \"rx_packets\": 26,\n                 \"tx_bytes\": 690,\n                 \"tx_dropped\": 0,\n                 \"tx_errors\": 0,\n                 \"tx_packets\": 9\n             }\n     },\n     \"memory_stats\" : {\n        \"stats\" : {\n           \"total_pgmajfault\" : 0,\n           \"cache\" : 0,\n           \"mapped_file\" : 0,\n           \"total_inactive_file\" : 0,\n           \"pgpgout\" : 414,\n           \"rss\" : 6537216,\n           \"total_mapped_file\" : 0,\n           \"writeback\" : 0,\n           \"unevictable\" : 0,\n           \"pgpgin\" : 477,\n           \"total_unevictable\" : 0,\n           \"pgmajfault\" : 0,\n           \"total_rss\" : 6537216,\n           \"total_rss_huge\" : 6291456,\n           \"total_writeback\" : 0,\n           \"total_inactive_anon\" : 0,\n           \"rss_huge\" : 6291456,\n           \"hierarchical_memory_limit\" : 67108864,\n           \"total_pgfault\" : 964,\n           \"total_active_file\" : 0,\n           \"active_anon\" : 6537216,\n           \"total_active_anon\" : 6537216,\n           \"total_pgpgout\" : 414,\n           \"total_cache\" : 0,\n           \"inactive_anon\" : 0,\n           \"active_file\" : 0,\n           \"pgfault\" : 964,\n           \"inactive_file\" : 0,\n           \"total_pgpgin\" : 477\n        },\n        \"max_usage\" : 6651904,\n        \"usage\" : 6537216,\n        \"failcnt\" : 0,\n        \"limit\" : 67108864\n     },\n     \"blkio_stats\" : {},\n     \"cpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24472255,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100215355,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 739306590000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     },\n     \"precpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24350896,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100093996,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 9492140000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     }\n  }\n</pre> <p>The precpu_stats is the cpu statistic of last read, which is used for calculating the cpu usage percent. It is not the exact copy of the “cpu_stats” field.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, pull stats once then disconnect. Default <code>true</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"resize-a-container-tty\">Resize a container TTY</h3> <p><code>POST /containers/(id or name)/resize</code></p> <p>Resize the TTY for container with <code>id</code>. The unit is number of characters. You must restart the container for the resize to take effect.</p> <p><strong>Example request</strong>:</p> <pre>  POST /containers/4fa6e0f0c678/resize?h=40&amp;w=80 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>  HTTP/1.1 200 OK\n  Content-Length: 0\n  Content-Type: text/plain; charset=utf-8\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of <code>tty</code> session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – No such container</li> <li>\n<strong>500</strong> – Cannot resize container</li> </ul> <h3 id=\"start-a-container\">Start a container</h3> <p><code>POST /containers/(id or name)/start</code></p> <p>Start the container <code>id</code></p> <blockquote> <p><strong>Note</strong>: For backwards compatibility, this endpoint accepts a <code>HostConfig</code> as JSON-encoded request body. See <a href=\"#create-a-container\">create a container</a> for details.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/start HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already started</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"stop-a-container\">Stop a container</h3> <p><code>POST /containers/(id or name)/stop</code></p> <p>Stop the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/stop?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>304</strong> – container already stopped</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"restart-a-container\">Restart a container</h3> <p><code>POST /containers/(id or name)/restart</code></p> <p>Restart the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/restart?t=5 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>t</strong> – number of seconds to wait before killing the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"kill-a-container\">Kill a container</h3> <p><code>POST /containers/(id or name)/kill</code></p> <p>Kill the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/kill HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters</p> <ul> <li>\n<strong>signal</strong> - Signal to send to the container: integer or string like <code>SIGINT</code>. When not set, <code>SIGKILL</code> is assumed and the call waits for the container to exit.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"rename-a-container\">Rename a container</h3> <p><code>POST /containers/(id or name)/rename</code></p> <p>Rename the container <code>id</code> to a <code>new_name</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/rename?name=new_name HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>name</strong> – new name for the container</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - conflict name already assigned</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"pause-a-container\">Pause a container</h3> <p><code>POST /containers/(id or name)/pause</code></p> <p>Pause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/pause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"unpause-a-container\">Unpause a container</h3> <p><code>POST /containers/(id or name)/unpause</code></p> <p>Unpause the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/unpause HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"attach-to-a-container\">Attach to a container</h3> <p><code>POST /containers/(id or name)/attach</code></p> <p>Attach to the container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/attach?logs=1&amp;stream=0&amp;stdout=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 101 UPGRADED\nContent-Type: application/vnd.docker.raw-stream\nConnection: Upgrade\nUpgrade: tcp\n\n{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>101</strong> – no error, hints proxy about hijacking</li> <li>\n<strong>200</strong> – no error, no upgrade header found</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<p><strong>500</strong> – server error</p> <p><strong>Stream details</strong>:</p> <p>When using the TTY setting is enabled in <a href=\"#create-a-container\"><code>POST /containers/create</code> </a>, the stream is the raw data from the process PTY and client’s <code>stdin</code>. When the TTY is disabled, then the stream is multiplexed to separate <code>stdout</code> and <code>stderr</code>.</p> <p>The format is a <strong>Header</strong> and a <strong>Payload</strong> (frame).</p> <p><strong>HEADER</strong></p> <p>The header contains the information which the stream writes (<code>stdout</code> or <code>stderr</code>). It also contains the size of the associated frame encoded in the last four bytes (<code>uint32</code>).</p> <p>It is encoded on the first eight bytes like this:</p> <pre>header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}\n</pre> <p><code>STREAM_TYPE</code> can be:</p>\n</li> <li><p>0: <code>stdin</code> (is written on <code>stdout</code>)</p></li> <li><p>1: <code>stdout</code></p></li> <li>\n<p>2: <code>stderr</code></p> <p><code>SIZE1, SIZE2, SIZE3, SIZE4</code> are the four bytes of the <code>uint32</code> size encoded as big endian.</p> <p><strong>PAYLOAD</strong></p> <p>The payload is the raw stream.</p> <p><strong>IMPLEMENTATION</strong></p> <p>The simplest way to implement the Attach protocol is the following:</p> <ol> <li>Read eight bytes.</li> <li>Choose <code>stdout</code> or <code>stderr</code> depending on the first byte.</li> <li>Extract the frame size from the last four bytes.</li> <li>Read the extracted size and output it on the correct output.</li> <li>Goto 1.</li> </ol>\n</li> </ul> <h3 id=\"attach-to-a-container-websocket\">Attach to a container (websocket)</h3> <p><code>GET /containers/(id or name)/attach/ws</code></p> <p>Attach to the container <code>id</code> via websocket</p> <p>Implements websocket protocol handshake according to <a href=\"http://tools.ietf.org/html/rfc6455\">RFC 6455</a></p> <p><strong>Example request</strong></p> <pre>GET /containers/e90e34656806/attach/ws?logs=0&amp;stream=1&amp;stdin=1&amp;stdout=1&amp;stderr=1 HTTP/1.1\n</pre> <p><strong>Example response</strong></p> <pre>{{ STREAM }}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>logs</strong> – 1/True/true or 0/False/false, return logs. Default <code>false</code>.</li> <li>\n<strong>stream</strong> – 1/True/true or 0/False/false, return stream. Default <code>false</code>.</li> <li>\n<strong>stdin</strong> – 1/True/true or 0/False/false, if <code>stream=true</code>, attach to <code>stdin</code>. Default <code>false</code>.</li> <li>\n<strong>stdout</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stdout</code> log, if <code>stream=true</code>, attach to <code>stdout</code>. Default <code>false</code>.</li> <li>\n<strong>stderr</strong> – 1/True/true or 0/False/false, if <code>logs=true</code>, return <code>stderr</code> log, if <code>stream=true</code>, attach to <code>stderr</code>. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"wait-a-container\">Wait a container</h3> <p><code>POST /containers/(id or name)/wait</code></p> <p>Block until container <code>id</code> stops, then returns the exit code</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/16253994b7c4/wait HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"StatusCode\": 0}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-a-container\">Remove a container</h3> <p><code>DELETE /containers/(id or name)</code></p> <p>Remove the container <code>id</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /containers/16253994b7c4?v=1 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>v</strong> – 1/True/true or 0/False/false, Remove the volumes associated to the container. Default <code>false</code>.</li> <li>\n<strong>force</strong> - 1/True/true or 0/False/false, Kill then remove the container. Default <code>false</code>.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"copy-files-or-folders-from-a-container\">Copy files or folders from a container</h3> <p><code>POST /containers/(id or name)/copy</code></p> <p>Copy files or folders of container <code>id</code></p> <p><strong>Deprecated</strong> in favor of the <code>archive</code> endpoint below.</p> <p><strong>Example request</strong>:</p> <pre>POST /containers/4fa6e0f0c678/copy HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Resource\": \"test.txt\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\n{{ TAR STREAM }}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"retrieving-information-about-files-and-folders-in-a-container\">Retrieving information about files and folders in a container</h3> <p><code>HEAD /containers/(id or name)/archive</code></p> <p>See the description of the <code>X-Docker-Container-Path-Stat</code> header in the following section.</p> <h3 id=\"get-an-archive-of-a-filesystem-resource-in-a-container\">Get an archive of a filesystem resource in a container</h3> <p><code>GET /containers/(id or name)/archive</code></p> <p>Get a tar archive of a resource in the filesystem of container <code>id</code>.</p> <p>Query Parameters:</p> <ul> <li>\n<p><strong>path</strong> - resource in the container’s filesystem to archive. Required.</p> <p>If not an absolute path, it is relative to the container’s root directory. The resource specified by <strong>path</strong> must exist. To assert that the resource is expected to be a directory, <strong>path</strong> should end in <code>/</code> or <code>/.</code> (assuming a path separator of <code>/</code>). If <strong>path</strong> ends in <code>/.</code> then this indicates that only the contents of the <strong>path</strong> directory should be copied. A symlink is always resolved to its target.</p> <p><strong>Note</strong>: It is not possible to copy certain system files such as resources under <code>/proc</code>, <code>/sys</code>, <code>/dev</code>, and mounts created by the user in the container.</p>\n</li> </ul> <p><strong>Example request</strong>:</p> <pre>    GET /containers/8cce319429b2/archive?path=/root HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>    HTTP/1.1 200 OK\n    Content-Type: application/x-tar\n    X-Docker-Container-Path-Stat: eyJuYW1lIjoicm9vdCIsInNpemUiOjQwOTYsIm1vZGUiOjIxNDc0ODQwOTYsIm10aW1lIjoiMjAxNC0wMi0yN1QyMDo1MToyM1oiLCJsaW5rVGFyZ2V0IjoiIn0=\n\n    {{ TAR STREAM }}\n</pre> <p>On success, a response header <code>X-Docker-Container-Path-Stat</code> will be set to a base64-encoded JSON object containing some filesystem header information about the archived resource. The above example value would decode to the following JSON object (whitespace added for readability):</p> <pre>    {\n        \"name\": \"root\",\n        \"size\": 4096,\n        \"mode\": 2147484096,\n        \"mtime\": \"2014-02-27T20:51:23Z\",\n        \"linkTarget\": \"\"\n    }\n</pre> <p>A <code>HEAD</code> request can also be made to this endpoint if only this information is desired.</p> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - success, returns archive of copied resource</li> <li>\n<strong>400</strong> - client error, bad parameter, details in JSON response body, one of: <ul> <li>must specify path parameter (<strong>path</strong> cannot be empty)</li> <li>not a directory (<strong>path</strong> was asserted to be a directory but exists as a file)</li> </ul>\n</li> <li>\n<strong>404</strong> - client error, resource not found, one of: – no such container (container <code>id</code> does not exist) <ul> <li>no such file or directory (<strong>path</strong> does not exist)</li> </ul>\n</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"extract-an-archive-of-files-or-folders-to-a-directory-in-a-container\">Extract an archive of files or folders to a directory in a container</h3> <p><code>PUT /containers/(id or name)/archive</code></p> <p>Upload a tar archive to be extracted to a path in the filesystem of container <code>id</code>.</p> <p>Query Parameters:</p> <ul> <li>\n<p><strong>path</strong> - path to a directory in the container to extract the archive’s contents into. Required.</p> <p>If not an absolute path, it is relative to the container’s root directory. The <strong>path</strong> resource must exist.</p>\n</li> <li><p><strong>noOverwriteDirNonDir</strong> - If “1”, “true”, or “True” then it will be an error if unpacking the given content would cause an existing directory to be replaced with a non-directory and vice versa.</p></li> </ul> <p><strong>Example request</strong>:</p> <pre>PUT /containers/8cce319429b2/archive?path=/vol1 HTTP/1.1\nContent-Type: application/x-tar\n\n{{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – the content was extracted successfully</li> <li>\n<strong>400</strong> - client error, bad parameter, details in JSON response body, one of: <ul> <li>must specify path parameter (<strong>path</strong> cannot be empty)</li> <li>not a directory (<strong>path</strong> should be a directory but exists as a file)</li> <li>unable to overwrite existing directory with non-directory (if <strong>noOverwriteDirNonDir</strong>)</li> <li>unable to overwrite existing non-directory with directory (if <strong>noOverwriteDirNonDir</strong>)</li> </ul>\n</li> <li>\n<strong>403</strong> - client error, permission denied, the volume or container rootfs is marked as read-only.</li> <li>\n<strong>404</strong> - client error, resource not found, one of: – no such container (container <code>id</code> does not exist) <ul> <li>no such file or directory (<strong>path</strong> resource does not exist)</li> </ul>\n</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-2-images\">2.2 Images</h2> <h3 id=\"list-images\">List Images</h3> <p><code>GET /images/json</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/json?all=0 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.04\",\n       \"ubuntu:precise\",\n       \"ubuntu:latest\"\n     ],\n     \"Id\": \"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\n     \"Created\": 1365714795,\n     \"Size\": 131506275,\n     \"VirtualSize\": 131506275,\n     \"Labels\": {}\n  },\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.10\",\n       \"ubuntu:quantal\"\n     ],\n     \"ParentId\": \"27cf784147099545\",\n     \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n     \"Created\": 1364102658,\n     \"Size\": 24653,\n     \"VirtualSize\": 180116135,\n     \"Labels\": {\n        \"com.example.version\": \"v1\"\n     }\n  }\n]\n</pre> <p><strong>Example request, with digest information</strong>:</p> <pre>GET /images/json?digests=1 HTTP/1.1\n</pre> <p><strong>Example response, with digest information</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Created\": 1420064636,\n    \"Id\": \"4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125\",\n    \"ParentId\": \"ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2\",\n    \"RepoDigests\": [\n      \"localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n    ],\n    \"RepoTags\": [\n      \"localhost:5000/test/busybox:latest\",\n      \"playdate:latest\"\n    ],\n    \"Size\": 0,\n    \"VirtualSize\": 2429728,\n    \"Labels\": {}\n  }\n]\n</pre> <p>The response shows a single image <code>Id</code> associated with two repositories (<code>RepoTags</code>): <code>localhost:5000/test/busybox</code>: and <code>playdate</code>. A caller can use either of the <code>RepoTags</code> values <code>localhost:5000/test/busybox:latest</code> or <code>playdate:latest</code> to reference the image.</p> <p>You can also use <code>RepoDigests</code> values to reference an image. In this response, the array has only one reference and that is to the <code>localhost:5000/test/busybox</code> repository; the <code>playdate</code> repository has no digest. You can reference this digest using the value: <code>localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d...</code></p> <p>See the <code>docker run</code> and <code>docker build</code> commands for examples of digest and tag references on the command line.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>all</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>filters</strong> – a JSON encoded value of the filters (a map[string][]string) to process on the images list. Available filters: <ul> <li><code>dangling=true</code></li> <li>\n<code>label=key</code> or <code>label=\"key=value\"</code> of an image label</li> </ul>\n</li> <li>\n<strong>filter</strong> - only return images with the specified name</li> </ul> <h3 id=\"build-image-from-a-dockerfile\">Build image from a Dockerfile</h3> <p><code>POST /build</code></p> <p>Build an image from a Dockerfile</p> <p><strong>Example request</strong>:</p> <pre>POST /build HTTP/1.1\n\n{{ TAR STREAM }}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"stream\": \"Step 1...\"}\n{\"stream\": \"...\"}\n{\"error\": \"Error...\", \"errorDetail\": {\"code\": 123, \"message\": \"Error...\"}}\n</pre> <p>The input stream must be a <code>tar</code> archive compressed with one of the following algorithms: <code>identity</code> (no compression), <code>gzip</code>, <code>bzip2</code>, <code>xz</code>.</p> <p>The archive must include a build instructions file, typically called <code>Dockerfile</code> at the archive’s root. The <code>dockerfile</code> parameter may be used to specify a different build instructions file. To do this, its value must be the path to the alternate build instructions file to use.</p> <p>The archive may include any number of other files, which are accessible in the build context (See the <a href=\"../../builder/index#dockerbuilder\"><em>ADD build command</em></a>).</p> <p>The build is canceled if the client drops the connection by quitting or being killed.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>dockerfile</strong> - Path within the build context to the Dockerfile. This is ignored if <code>remote</code> is specified and points to an individual filename.</li> <li>\n<strong>t</strong> – A name and optional tag to apply to the image in the <code>name:tag</code> format. If you omit the <code>tag</code> the default <code>latest</code> value is assumed. You can provide one or more <code>t</code> parameters.</li> <li>\n<strong>remote</strong> – A Git repository URI or HTTP/HTTPS URI build source. If the URI specifies a filename, the file’s contents are placed into a file called <code>Dockerfile</code>.</li> <li>\n<strong>q</strong> – Suppress verbose build output.</li> <li>\n<strong>nocache</strong> – Do not use the cache when building the image.</li> <li>\n<strong>pull</strong> - Attempt to pull the image even if an older image exists locally.</li> <li>\n<strong>rm</strong> - Remove intermediate containers after a successful build (default behavior).</li> <li>\n<strong>forcerm</strong> - Always remove intermediate containers (includes <code>rm</code>).</li> <li>\n<strong>memory</strong> - Set memory limit for build.</li> <li>\n<strong>memswap</strong> - Total memory (memory + swap), <code>-1</code> to enable unlimited swap.</li> <li>\n<strong>cpushares</strong> - CPU shares (relative weight).</li> <li>\n<strong>cpusetcpus</strong> - CPUs in which to allow execution (e.g., <code>0-3</code>, <code>0,1</code>).</li> <li>\n<strong>cpuperiod</strong> - The length of a CPU period in microseconds.</li> <li>\n<strong>cpuquota</strong> - Microseconds of CPU time that the container can get in a CPU period.</li> <li>\n<p><strong>buildargs</strong> – JSON map of string pairs for build-time variables. Users pass these values at build-time. Docker uses the <code>buildargs</code> as the environment context for command(s) run via the Dockerfile’s <code>RUN</code> instruction or for variable expansion in other Dockerfile instructions. This is not meant for passing secret values. <a href=\"../../builder/index#arg\">Read more about the buildargs instruction</a></p> <p>Request Headers:</p>\n</li> <li><p><strong>Content-type</strong> – Set to <code>\"application/tar\"</code>.</p></li> <li>\n<p><strong>X-Registry-Config</strong> – A base64-url-safe-encoded Registry Auth Config JSON object with the following structure:</p> <pre>    {\n        \"docker.example.com\": {\n            \"username\": \"janedoe\",\n            \"password\": \"hunter2\"\n        },\n        \"https://index.docker.io/v1/\": {\n            \"username\": \"mobydock\",\n            \"password\": \"conta1n3rize14\"\n        }\n    }\n</pre> <p>This object maps the hostname of a registry to an object containing the “username” and “password” for that registry. Multiple registries may be specified as the build may be based on an image requiring authentication to pull from any arbitrary registry. Only the registry domain name (and port if not the default “443”) are required. However (for legacy reasons) the “official” Docker, Inc. hosted registry must be specified with both a “https://” prefix and a “/v1/” suffix even though Docker will prefer to use the v2 registry API.</p>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"create-an-image\">Create an image</h3> <p><code>POST /images/create</code></p> <p>Create an image either by pulling it from the registry or by importing it</p> <p><strong>Example request</strong>:</p> <pre>POST /images/create?fromImage=ubuntu HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pulling...\"}\n{\"status\": \"Pulling\", \"progress\": \"1 B/ 100 B\", \"progressDetail\": {\"current\": 1, \"total\": 100}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>When using this endpoint to pull an image from the registry, the <code>X-Registry-Auth</code> header can be used to include a base64-encoded AuthConfig object.</p> <p>Query Parameters:</p> <ul> <li>\n<strong>fromImage</strong> – Name of the image to pull. The name may include a tag or digest. This parameter may only be used when pulling an image.</li> <li>\n<strong>fromSrc</strong> – Source to import. The value may be a URL from which the image can be retrieved or <code>-</code> to read the image from the request body. This parameter may only be used when importing an image.</li> <li>\n<strong>repo</strong> – Repository name given to an image when it is imported. The repo may include a tag. This parameter may only be used when importing an image.</li> <li>\n<p><strong>tag</strong> – Tag or digest.</p> <p>Request Headers:</p>\n</li> <li><p><strong>X-Registry-Auth</strong> – base64-encoded AuthConfig object</p></li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"inspect-an-image\">Inspect an image</h3> <p><code>GET /images/(name)/json</code></p> <p>Return low-level information on the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/example/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Id\" : \"85f05633ddc1c50679be2b16a0479ab6f7637f8884e0cfe0f4d20e1ebb3d6e7c\",\n   \"Container\" : \"cb91e48a60d01f1e27028b4fc6819f4f290b3cf12496c8176ec714d0d390984a\",\n   \"Comment\" : \"\",\n   \"Os\" : \"linux\",\n   \"Architecture\" : \"amd64\",\n   \"Parent\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n   \"ContainerConfig\" : {\n      \"Tty\" : false,\n      \"Hostname\" : \"e611e15f9c9d\",\n      \"Volumes\" : null,\n      \"Domainname\" : \"\",\n      \"AttachStdout\" : false,\n      \"PublishService\" : \"\",\n      \"AttachStdin\" : false,\n      \"OpenStdin\" : false,\n      \"StdinOnce\" : false,\n      \"NetworkDisabled\" : false,\n      \"OnBuild\" : [],\n      \"Image\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n      \"User\" : \"\",\n      \"WorkingDir\" : \"\",\n      \"Entrypoint\" : null,\n      \"MacAddress\" : \"\",\n      \"AttachStderr\" : false,\n      \"Labels\" : {\n         \"com.example.license\" : \"GPL\",\n         \"com.example.version\" : \"1.0\",\n         \"com.example.vendor\" : \"Acme\"\n      },\n      \"Env\" : [\n         \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n      ],\n      \"ExposedPorts\" : null,\n      \"Cmd\" : [\n         \"/bin/sh\",\n         \"-c\",\n         \"#(nop) LABEL com.example.vendor=Acme com.example.license=GPL com.example.version=1.0\"\n      ]\n   },\n   \"DockerVersion\" : \"1.9.0-dev\",\n   \"VirtualSize\" : 188359297,\n   \"Size\" : 0,\n   \"Author\" : \"\",\n   \"Created\" : \"2015-09-10T08:30:53.26995814Z\",\n   \"GraphDriver\" : {\n      \"Name\" : \"aufs\",\n      \"Data\" : null\n   },\n   \"RepoDigests\" : [\n      \"localhost:5000/test/busybox/example@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n   ],\n   \"RepoTags\" : [\n      \"example:1.0\",\n      \"example:latest\",\n      \"example:stable\"\n   ],\n   \"Config\" : {\n      \"Image\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n      \"NetworkDisabled\" : false,\n      \"OnBuild\" : [],\n      \"StdinOnce\" : false,\n      \"PublishService\" : \"\",\n      \"AttachStdin\" : false,\n      \"OpenStdin\" : false,\n      \"Domainname\" : \"\",\n      \"AttachStdout\" : false,\n      \"Tty\" : false,\n      \"Hostname\" : \"e611e15f9c9d\",\n      \"Volumes\" : null,\n      \"Cmd\" : [\n         \"/bin/bash\"\n      ],\n      \"ExposedPorts\" : null,\n      \"Env\" : [\n         \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n      ],\n      \"Labels\" : {\n         \"com.example.vendor\" : \"Acme\",\n         \"com.example.version\" : \"1.0\",\n         \"com.example.license\" : \"GPL\"\n      },\n      \"Entrypoint\" : null,\n      \"MacAddress\" : \"\",\n      \"AttachStderr\" : false,\n      \"WorkingDir\" : \"\",\n      \"User\" : \"\"\n   }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-the-history-of-an-image\">Get the history of an image</h3> <p><code>GET /images/(name)/history</code></p> <p>Return the history of the image <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /images/ubuntu/history HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n    {\n        \"Id\": \"3db9c44f45209632d6050b35958829c3a2aa256d81b9a7be45b362ff85c54710\",\n        \"Created\": 1398108230,\n        \"CreatedBy\": \"/bin/sh -c #(nop) ADD file:eb15dbd63394e063b805a3c32ca7bf0266ef64676d5a6fab4801f2e81e2a5148 in /\",\n        \"Tags\": [\n            \"ubuntu:lucid\",\n            \"ubuntu:10.04\"\n        ],\n        \"Size\": 182964289,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"6cfa4d1f33fb861d4d114f43b25abd0ac737509268065cdfd69d544a59c85ab8\",\n        \"Created\": 1398108222,\n        \"CreatedBy\": \"/bin/sh -c #(nop) MAINTAINER Tianon Gravi &lt;admwiggin@gmail.com&gt; - mkimage-debootstrap.sh -i iproute,iputils-ping,ubuntu-minimal -t lucid.tar.xz lucid http://archive.ubuntu.com/ubuntu/\",\n        \"Tags\": null,\n        \"Size\": 0,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\n        \"Created\": 1371157430,\n        \"CreatedBy\": \"\",\n        \"Tags\": [\n            \"scratch12:latest\",\n            \"scratch:latest\"\n        ],\n        \"Size\": 0,\n        \"Comment\": \"Imported from -\"\n    }\n]\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"push-an-image-on-the-registry\">Push an image on the registry</h3> <p><code>POST /images/(name)/push</code></p> <p>Push the image <code>name</code> on the registry</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/push HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pushing...\"}\n{\"status\": \"Pushing\", \"progress\": \"1/? (n/a)\", \"progressDetail\": {\"current\": 1}}}\n{\"error\": \"Invalid...\"}\n...\n</pre> <p>If you wish to push an image on to a private registry, that image must already have a tag into a repository which references that registry <code>hostname</code> and <code>port</code>. This repository name should then be used in the URL. This duplicates the command line’s flow.</p> <p><strong>Example request</strong>:</p> <pre>POST /images/registry.acme.com:5000/test/push HTTP/1.1\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>tag</strong> – The tag to associate with the image on the registry. This is optional.</li> </ul> <p>Request Headers:</p> <ul> <li>\n<strong>X-Registry-Auth</strong> – Include a base64-encoded AuthConfig. object.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"tag-an-image-into-a-repository\">Tag an image into a repository</h3> <p><code>POST /images/(name)/tag</code></p> <p>Tag the image <code>name</code> into a repository</p> <p><strong>Example request</strong>:</p> <pre>POST /images/test/tag?repo=myrepo&amp;force=0&amp;tag=v42 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>repo</strong> – The repository to tag in</li> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>tag</strong> - The new tag name</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>400</strong> – bad parameter</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"remove-an-image\">Remove an image</h3> <p><code>DELETE /images/(name)</code></p> <p>Remove the image <code>name</code> from the filesystem</p> <p><strong>Example request</strong>:</p> <pre>DELETE /images/test HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-type: application/json\n\n[\n {\"Untagged\": \"3e2f21a89f\"},\n {\"Deleted\": \"3e2f21a89f\"},\n {\"Deleted\": \"53b4f83ac9\"}\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>force</strong> – 1/True/true or 0/False/false, default false</li> <li>\n<strong>noprune</strong> – 1/True/true or 0/False/false, default false</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such image</li> <li>\n<strong>409</strong> – conflict</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"search-images\">Search images</h3> <p><code>GET /images/search</code></p> <p>Search for an image on <a href=\"https://hub.docker.com\">Docker Hub</a>.</p> <blockquote> <p><strong>Note</strong>: The response keys have changed from API v1.6 to reflect the JSON sent by the registry server to the docker daemon’s request.</p> </blockquote> <p><strong>Example request</strong>:</p> <pre>GET /images/search?term=sshd HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"wma55/u1210sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"jdswinbank/sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"vgauthier/sshd\",\n            \"star_count\": 0\n        }\n...\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>term</strong> – term to search</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h2 id=\"2-3-misc\">2.3 Misc</h2> <h3 id=\"check-auth-configuration\">Check auth configuration</h3> <p><code>POST /auth</code></p> <p>Get the default username and email</p> <p><strong>Example request</strong>:</p> <pre>POST /auth HTTP/1.1\nContent-Type: application/json\n\n{\n     \"username\":\" hannibal\",\n     \"password: \"xxxx\",\n     \"email\": \"hannibal@a-team.com\",\n     \"serveraddress\": \"https://index.docker.io/v1/\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>204</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"display-system-wide-information\">Display system-wide information</h3> <p><code>GET /info</code></p> <p>Display system-wide information</p> <p><strong>Example request</strong>:</p> <pre>GET /info HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"ClusterStore\": \"etcd://localhost:2379\",\n    \"Containers\": 11,\n    \"CpuCfsPeriod\": true,\n    \"CpuCfsQuota\": true,\n    \"Debug\": false,\n    \"DockerRootDir\": \"/var/lib/docker\",\n    \"Driver\": \"btrfs\",\n    \"DriverStatus\": [[\"\"]],\n    \"ExecutionDriver\": \"native-0.1\",\n    \"ExperimentalBuild\": false,\n    \"HttpProxy\": \"http://test:test@localhost:8080\",\n    \"HttpsProxy\": \"https://test:test@localhost:8080\",\n    \"ID\": \"7TRN:IPZB:QYBB:VPBQ:UMPP:KARE:6ZNR:XE6T:7EWV:PKF4:ZOJD:TPYS\",\n    \"IPv4Forwarding\": true,\n    \"Images\": 16,\n    \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n    \"InitPath\": \"/usr/bin/docker\",\n    \"InitSha1\": \"\",\n    \"KernelVersion\": \"3.12.0-1-amd64\",\n    \"Labels\": [\n        \"storage=ssd\"\n    ],\n    \"MemTotal\": 2099236864,\n    \"MemoryLimit\": true,\n    \"NCPU\": 1,\n    \"NEventsListener\": 0,\n    \"NFd\": 11,\n    \"NGoroutines\": 21,\n    \"Name\": \"prod-server-42\",\n    \"NoProxy\": \"9.81.1.160\",\n    \"OomKillDisable\": true,\n    \"OperatingSystem\": \"Boot2Docker\",\n    \"RegistryConfig\": {\n        \"IndexConfigs\": {\n            \"docker.io\": {\n                \"Mirrors\": null,\n                \"Name\": \"docker.io\",\n                \"Official\": true,\n                \"Secure\": true\n            }\n        },\n        \"InsecureRegistryCIDRs\": [\n            \"127.0.0.0/8\"\n        ]\n    },\n    \"ServerVersion\": \"1.9.0\",\n    \"SwapLimit\": false,\n    \"SystemTime\": \"2015-03-10T11:11:23.730591467-07:00\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"show-the-docker-version-information\">Show the docker version information</h3> <p><code>GET /version</code></p> <p>Show the docker version information</p> <p><strong>Example request</strong>:</p> <pre>GET /version HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n     \"Version\": \"1.5.0\",\n     \"Os\": \"linux\",\n     \"KernelVersion\": \"3.18.5-tinycore64\",\n     \"GoVersion\": \"go1.4.1\",\n     \"GitCommit\": \"a8a31ef\",\n     \"Arch\": \"amd64\",\n     \"ApiVersion\": \"1.20\",\n     \"Experimental\": false\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"ping-the-docker-server\">Ping the docker server</h3> <p><code>GET /_ping</code></p> <p>Ping the docker server</p> <p><strong>Example request</strong>:</p> <pre>GET /_ping HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: text/plain\n\nOK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-new-image-from-a-container-s-changes\">Create a new image from a container’s changes</h3> <p><code>POST /commit</code></p> <p>Create a new image from a container’s changes</p> <p><strong>Example request</strong>:</p> <pre>POST /commit?container=44c004db4b17&amp;comment=message&amp;repo=myrepo HTTP/1.1\nContent-Type: application/json\n\n{\n     \"Hostname\": \"\",\n     \"Domainname\": \"\",\n     \"User\": \"\",\n     \"AttachStdin\": false,\n     \"AttachStdout\": true,\n     \"AttachStderr\": true,\n     \"Tty\": false,\n     \"OpenStdin\": false,\n     \"StdinOnce\": false,\n     \"Env\": null,\n     \"Cmd\": [\n             \"date\"\n     ],\n     \"Mounts\": [\n       {\n         \"Source\": \"/data\",\n         \"Destination\": \"/data\",\n         \"Mode\": \"ro,Z\",\n         \"RW\": false\n       }\n     ],\n     \"Labels\": {\n             \"key1\": \"value1\",\n             \"key2\": \"value2\"\n      },\n     \"WorkingDir\": \"\",\n     \"NetworkDisabled\": false,\n     \"ExposedPorts\": {\n             \"22/tcp\": {}\n     }\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\"Id\": \"596069db4bf5\"}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>config</strong> - the container’s configuration</li> </ul> <p>Query Parameters:</p> <ul> <li>\n<strong>container</strong> – source container</li> <li>\n<strong>repo</strong> – repository</li> <li>\n<strong>tag</strong> – tag</li> <li>\n<strong>comment</strong> – commit message</li> <li>\n<strong>author</strong> – author (e.g., “John Hannibal Smith &lt;<a href=\"mailto:hannibal%40a-team.com\">hannibal@a-team.com</a>&gt;“)</li> <li>\n<strong>pause</strong> – 1/True/true or 0/False/false, whether to pause the container before committing</li> <li>\n<strong>changes</strong> – Dockerfile instructions to apply while committing</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"monitor-docker-s-events\">Monitor Docker’s events</h3> <p><code>GET /events</code></p> <p>Get container events from docker, either in real time via streaming, or via polling (using since).</p> <p>Docker containers report the following events:</p> <pre>attach, commit, copy, create, destroy, die, exec_create, exec_start, export, kill, oom, pause, rename, resize, restart, start, stop, top, unpause\n</pre> <p>and Docker images report:</p> <pre>delete, import, pull, push, tag, untag\n</pre> <p><strong>Example request</strong>:</p> <pre>GET /events?since=1374067924\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\":\"pull\",\"id\":\"busybox:latest\",\"time\":1442421700,\"timeNano\":1442421700598988358}\n{\"status\":\"create\",\"id\":\"5745704abe9caa5\",\"from\":\"busybox\",\"time\":1442421716,\"timeNano\":1442421716853979870}\n{\"status\":\"attach\",\"id\":\"5745704abe9caa5\",\"from\":\"busybox\",\"time\":1442421716,\"timeNano\":1442421716894759198}\n{\"status\":\"start\",\"id\":\"5745704abe9caa5\",\"from\":\"busybox\",\"time\":1442421716,\"timeNano\":1442421716983607193}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>since</strong> – Timestamp used for polling</li> <li>\n<strong>until</strong> – Timestamp used for polling</li> <li>\n<strong>filters</strong> – A json encoded value of the filters (a map[string][]string) to process on the event list. Available filters: <ul> <li>\n<code>container=&lt;string&gt;</code>; -- container to filter</li> <li>\n<code>event=&lt;string&gt;</code>; -- event to filter</li> <li>\n<code>image=&lt;string&gt;</code>; -- image to filter</li> <li>\n<code>label=&lt;string&gt;</code>; -- image and container label to filter</li> </ul>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images-in-a-repository\">Get a tarball containing all images in a repository</h3> <p><code>GET /images/(name)/get</code></p> <p>Get a tarball containing all images and metadata for the repository specified by <code>name</code>.</p> <p>If <code>name</code> is a specific name and tag (e.g. ubuntu:latest), then only that image (and its parents) are returned. If <code>name</code> is an image ID, similarly only that image (and its parents) are returned, but with the exclusion of the ‘repositories’ file in the tarball, as there were no image names referenced.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/ubuntu/get\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"get-a-tarball-containing-all-images\">Get a tarball containing all images.</h3> <p><code>GET /images/get</code></p> <p>Get a tarball containing all images and metadata for one or more repositories.</p> <p>For each value of the <code>names</code> parameter: if it is a specific name and tag (e.g. <code>ubuntu:latest</code>), then only that image (and its parents) are returned; if it is an image ID, similarly only that image (and its parents) are returned and there would be no names referenced in the ‘repositories’ file for this image ID.</p> <p>See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>GET /images/get?names=myname%2Fmyapp%3Alatest&amp;names=busybox\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"load-a-tarball-with-a-set-of-images-and-tags-into-docker\">Load a tarball with a set of images and tags into docker</h3> <p><code>POST /images/load</code></p> <p>Load a set of images and tags into a Docker repository. See the <a href=\"#image-tarball-format\">image tarball format</a> for more details.</p> <p><strong>Example request</strong></p> <pre>POST /images/load\n\nTarball in body\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>500</strong> – server error</li> </ul> <h3 id=\"image-tarball-format\">Image tarball format</h3> <p>An image tarball contains one directory per image layer (named using its long ID), each containing these files:</p> <ul> <li>\n<code>VERSION</code>: currently <code>1.0</code> - the file format version</li> <li>\n<code>json</code>: detailed layer information, similar to <code>docker inspect layer_id</code>\n</li> <li>\n<code>layer.tar</code>: A tarfile containing the filesystem changes in this layer</li> </ul> <p>The <code>layer.tar</code> file contains <code>aufs</code> style <code>.wh..wh.aufs</code> files and directories for storing attribute changes and deletions.</p> <p>If the tarball defines a repository, the tarball should also include a <code>repositories</code> file at the root that contains a list of repository and tag names mapped to layer IDs.</p> <pre>{\"hello-world\":\n    {\"latest\": \"565a9d68a73f6706862bfe8409a7f659776d4d60a8d096eb4a3cbce6999cc2a1\"}\n}\n</pre> <h3 id=\"exec-create\">Exec Create</h3> <p><code>POST /containers/(id or name)/exec</code></p> <p>Sets up an exec instance in a running container <code>id</code></p> <p><strong>Example request</strong>:</p> <pre>POST /containers/e90e34656806/exec HTTP/1.1\nContent-Type: application/json\n\n  {\n   \"AttachStdin\": false,\n   \"AttachStdout\": true,\n   \"AttachStderr\": true,\n   \"Tty\": false,\n   \"Cmd\": [\n                 \"date\"\n         ]\n  }\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n     \"Id\": \"f90e34656806\",\n     \"Warnings\":[]\n}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>AttachStdin</strong> - Boolean value, attaches to <code>stdin</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStdout</strong> - Boolean value, attaches to <code>stdout</code> of the <code>exec</code> command.</li> <li>\n<strong>AttachStderr</strong> - Boolean value, attaches to <code>stderr</code> of the <code>exec</code> command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> <li>\n<strong>Cmd</strong> - Command to run specified as a string or an array of strings.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such container</li> <li>\n<strong>409</strong> - container is paused</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"exec-start\">Exec Start</h3> <p><code>POST /exec/(id)/start</code></p> <p>Starts a previously set up <code>exec</code> instance <code>id</code>. If <code>detach</code> is true, this API returns after starting the <code>exec</code> command. Otherwise, this API sets up an interactive session with the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/start HTTP/1.1\nContent-Type: application/json\n\n{\n \"Detach\": false,\n \"Tty\": false\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/vnd.docker.raw-stream\n\n{{ STREAM }}\n</pre> <p>Json Parameters:</p> <ul> <li>\n<strong>Detach</strong> - Detach from the <code>exec</code> command.</li> <li>\n<strong>Tty</strong> - Boolean value to allocate a pseudo-TTY.</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<p><strong>409</strong> - container is paused</p> <p><strong>Stream details</strong>: Similar to the stream behavior of <code>POST /containers/(id or name)/attach</code> API</p>\n</li> </ul> <h3 id=\"exec-resize\">Exec Resize</h3> <p><code>POST /exec/(id)/resize</code></p> <p>Resizes the <code>tty</code> session used by the <code>exec</code> command <code>id</code>. The unit is number of characters. This API is valid only if <code>tty</code> was specified as part of creating and starting the <code>exec</code> command.</p> <p><strong>Example request</strong>:</p> <pre>POST /exec/e90e34656806/resize?h=40&amp;w=80 HTTP/1.1\nContent-Type: text/plain\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: text/plain\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>h</strong> – height of <code>tty</code> session</li> <li>\n<strong>w</strong> – width</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> </ul> <h3 id=\"exec-inspect\">Exec Inspect</h3> <p><code>GET /exec/(id)/json</code></p> <p>Return low-level information about the <code>exec</code> command <code>id</code>.</p> <p><strong>Example request</strong>:</p> <pre>GET /exec/11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39/json HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: plain/text\n\n{\n  \"ID\" : \"11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39\",\n  \"Running\" : false,\n  \"ExitCode\" : 2,\n  \"ProcessConfig\" : {\n    \"privileged\" : false,\n    \"user\" : \"\",\n    \"tty\" : false,\n    \"entrypoint\" : \"sh\",\n    \"arguments\" : [\n      \"-c\",\n      \"exit 2\"\n    ]\n  },\n  \"OpenStdin\" : false,\n  \"OpenStderr\" : false,\n  \"OpenStdout\" : false,\n  \"Container\" : {\n    \"State\" : {\n      \"Status\" : \"running\",\n      \"Running\" : true,\n      \"Paused\" : false,\n      \"Restarting\" : false,\n      \"OOMKilled\" : false,\n      \"Pid\" : 3650,\n      \"ExitCode\" : 0,\n      \"Error\" : \"\",\n      \"StartedAt\" : \"2014-11-17T22:26:03.717657531Z\",\n      \"FinishedAt\" : \"0001-01-01T00:00:00Z\"\n    },\n    \"ID\" : \"8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c\",\n    \"Created\" : \"2014-11-17T22:26:03.626304998Z\",\n    \"Path\" : \"date\",\n    \"Args\" : [],\n    \"Config\" : {\n      \"Hostname\" : \"8f177a186b97\",\n      \"Domainname\" : \"\",\n      \"User\" : \"\",\n      \"AttachStdin\" : false,\n      \"AttachStdout\" : false,\n      \"AttachStderr\" : false,\n      \"ExposedPorts\" : null,\n      \"Tty\" : false,\n      \"OpenStdin\" : false,\n      \"StdinOnce\" : false,\n      \"Env\" : [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ],\n      \"Cmd\" : [\n        \"date\"\n      ],\n      \"Image\" : \"ubuntu\",\n      \"Volumes\" : null,\n      \"WorkingDir\" : \"\",\n      \"Entrypoint\" : null,\n      \"NetworkDisabled\" : false,\n      \"MacAddress\" : \"\",\n      \"OnBuild\" : null,\n      \"SecurityOpt\" : null\n    },\n    \"Image\" : \"5506de2b643be1e6febbf3b8a240760c6843244c41e12aa2f60ccbb7153d17f5\",\n    \"NetworkSettings\": {\n        \"Bridge\": \"\",\n        \"SandboxID\": \"\",\n        \"HairpinMode\": false,\n        \"LinkLocalIPv6Address\": \"\",\n        \"LinkLocalIPv6PrefixLen\": 0,\n        \"Ports\": null,\n        \"SandboxKey\": \"\",\n        \"SecondaryIPAddresses\": null,\n        \"SecondaryIPv6Addresses\": null,\n        \"EndpointID\": \"\",\n        \"Gateway\": \"\",\n        \"GlobalIPv6Address\": \"\",\n        \"GlobalIPv6PrefixLen\": 0,\n        \"IPAddress\": \"\",\n        \"IPPrefixLen\": 0,\n        \"IPv6Gateway\": \"\",\n        \"MacAddress\": \"\",\n        \"Networks\": {\n            \"bridge\": {\n                \"EndpointID\": \"\",\n                \"Gateway\": \"\",\n                \"IPAddress\": \"\",\n                \"IPPrefixLen\": 0,\n                \"IPv6Gateway\": \"\",\n                \"GlobalIPv6Address\": \"\",\n                \"GlobalIPv6PrefixLen\": 0,\n                \"MacAddress\": \"\"\n            }\n        }\n    },\n    \"ResolvConfPath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/resolv.conf\",\n    \"HostnamePath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/hostname\",\n    \"HostsPath\" : \"/var/lib/docker/containers/8f177a186b977fb451136e0fdf182abff5599a08b3c7f6ef0d36a55aaf89634c/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n    \"Name\" : \"/test\",\n    \"Driver\" : \"aufs\",\n    \"ExecDriver\" : \"native-0.2\",\n    \"MountLabel\" : \"\",\n    \"ProcessLabel\" : \"\",\n    \"AppArmorProfile\" : \"\",\n    \"RestartCount\" : 0,\n    \"Mounts\" : []\n  }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> – no error</li> <li>\n<strong>404</strong> – no such exec instance</li> <li>\n<strong>500</strong> - server error</li> </ul> <h2 id=\"2-4-volumes\">2.4 Volumes</h2> <h3 id=\"list-volumes\">List volumes</h3> <p><code>GET /volumes</code></p> <p><strong>Example request</strong>:</p> <pre>GET /volumes HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Volumes\": [\n    {\n      \"Name\": \"tardis\",\n      \"Driver\": \"local\",\n      \"Mountpoint\": \"/var/lib/docker/volumes/tardis\"\n    }\n  ]\n}\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>filters</strong> - JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the volumes list. There is one available filter: <code>dangling=true</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"create-a-volume\">Create a volume</h3> <p><code>POST /volumes/create</code></p> <p>Create a volume</p> <p><strong>Example request</strong>:</p> <pre>POST /volumes/create HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\",\n  \"Driver\": \"local\",\n  \"Mountpoint\": \"/var/lib/docker/volumes/tardis\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Name</strong> - The new volume’s name. If not specified, Docker generates a name.</li> <li>\n<strong>Driver</strong> - Name of the volume driver to use. Defaults to <code>local</code> for the name.</li> <li>\n<strong>DriverOpts</strong> - A mapping of driver options and values. These options are passed directly to the driver and are driver specific.</li> </ul> <h3 id=\"inspect-a-volume\">Inspect a volume</h3> <p><code>GET /volumes/(name)</code></p> <p>Return low-level information on the volume <code>name</code></p> <p><strong>Example request</strong>:</p> <pre>GET /volumes/tardis\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\",\n  \"Driver\": \"local\",\n  \"Mountpoint\": \"/var/lib/docker/volumes/tardis\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - no such volume</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"remove-a-volume\">Remove a volume</h3> <p><code>DELETE /volumes/(name)</code></p> <p>Instruct the driver to remove the volume (<code>name</code>).</p> <p><strong>Example request</strong>:</p> <pre>DELETE /volumes/tardis HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 204 No Content\n</pre> <p>Status Codes</p> <ul> <li>\n<strong>204</strong> - no error</li> <li>\n<strong>404</strong> - no such volume or volume driver</li> <li>\n<strong>409</strong> - volume is in use and cannot be removed</li> <li>\n<strong>500</strong> - server error</li> </ul> <h2 id=\"2-5-networks\">2.5 Networks</h2> <h3 id=\"list-networks\">List networks</h3> <p><code>GET /networks</code></p> <p><strong>Example request</strong>:</p> <pre>GET /networks HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Name\": \"bridge\",\n    \"Id\": \"f2de39df4171b0dc801e8002d1d999b77256983dfc63041c0f34030aa3977566\",\n    \"Scope\": \"local\",\n    \"Driver\": \"bridge\",\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": [\n        {\n          \"Subnet\": \"172.17.0.0/16\"\n        }\n      ]\n    },\n    \"Containers\": {\n      \"39b69226f9d79f5634485fb236a23b2fe4e96a0a94128390a7fbbcc167065867\": {\n        \"EndpointID\": \"ed2419a97c1d9954d05b46e462e7002ea552f216e9b136b80a7db8d98b442eda\",\n        \"MacAddress\": \"02:42:ac:11:00:02\",\n        \"IPv4Address\": \"172.17.0.2/16\",\n        \"IPv6Address\": \"\"\n      }\n    },\n    \"Options\": {\n      \"com.docker.network.bridge.default_bridge\": \"true\",\n      \"com.docker.network.bridge.enable_icc\": \"true\",\n      \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n      \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n      \"com.docker.network.bridge.name\": \"docker0\",\n      \"com.docker.network.driver.mtu\": \"1500\"\n    }\n  },\n  {\n    \"Name\": \"none\",\n    \"Id\": \"e086a3893b05ab69242d3c44e49483a3bbbd3a26b46baa8f61ab797c1088d794\",\n    \"Scope\": \"local\",\n    \"Driver\": \"null\",\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": []\n    },\n    \"Containers\": {},\n    \"Options\": {}\n  },\n  {\n    \"Name\": \"host\",\n    \"Id\": \"13e871235c677f196c4e1ecebb9dc733b9b2d2ab589e30c539efeda84a24215e\",\n    \"Scope\": \"local\",\n    \"Driver\": \"host\",\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Config\": []\n    },\n    \"Containers\": {},\n    \"Options\": {}\n  }\n]\n</pre> <p>Query Parameters:</p> <ul> <li>\n<strong>filters</strong> - JSON encoded value of the filters (a <code>map[string][]string</code>) to process on the networks list. Available filters: <code>name=[network-names]</code> , <code>id=[network-ids]</code>\n</li> </ul> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>500</strong> - server error</li> </ul> <h3 id=\"inspect-network\">Inspect network</h3> <p><code>GET /networks/&lt;network-id&gt;</code></p> <p><strong>Example request</strong>:</p> <pre>GET /networks/f2de39df4171b0dc801e8002d1d999b77256983dfc63041c0f34030aa3977566 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Name\": \"bridge\",\n  \"Id\": \"f2de39df4171b0dc801e8002d1d999b77256983dfc63041c0f34030aa3977566\",\n  \"Scope\": \"local\",\n  \"Driver\": \"bridge\",\n  \"IPAM\": {\n    \"Driver\": \"default\",\n    \"Config\": [\n      {\n        \"Subnet\": \"172.17.0.0/16\"\n      }\n    ]\n  },\n  \"Containers\": {\n    \"39b69226f9d79f5634485fb236a23b2fe4e96a0a94128390a7fbbcc167065867\": {\n      \"EndpointID\": \"ed2419a97c1d9954d05b46e462e7002ea552f216e9b136b80a7db8d98b442eda\",\n      \"MacAddress\": \"02:42:ac:11:00:02\",\n      \"IPv4Address\": \"172.17.0.2/16\",\n      \"IPv6Address\": \"\"\n    }\n  },\n  \"Options\": {\n    \"com.docker.network.bridge.default_bridge\": \"true\",\n    \"com.docker.network.bridge.enable_icc\": \"true\",\n    \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n    \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n    \"com.docker.network.bridge.name\": \"docker0\",\n    \"com.docker.network.driver.mtu\": \"1500\"\n  }\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network not found</li> </ul> <h3 id=\"create-a-network\">Create a network</h3> <p><code>POST /networks/create</code></p> <p>Create a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/create HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Name\":\"isolated_nw\",\n  \"Driver\":\"bridge\"\n  \"IPAM\":{\n    \"Config\":[{\n      \"Subnet\":\"172.20.0.0/16\",\n      \"IPRange\":\"172.20.10.0/24\",\n      \"Gateway\":\"172.20.10.11\"\n    }]\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n  \"Id\": \"22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30\",\n  \"Warning\": \"\"\n}\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>201</strong> - no error</li> <li>\n<strong>404</strong> - plugin not found</li> <li>\n<strong>500</strong> - server error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Name</strong> - The new network’s name. this is a mandatory field</li> <li>\n<strong>Driver</strong> - Name of the network driver plugin to use. Defaults to <code>bridge</code> driver</li> <li>\n<strong>IPAM</strong> - Optional custom IP scheme for the network</li> <li>\n<strong>Options</strong> - Network specific options to be used by the drivers</li> <li>\n<strong>CheckDuplicate</strong> - Requests daemon to check for networks with same name</li> </ul> <h3 id=\"connect-a-container-to-a-network\">Connect a container to a network</h3> <p><code>POST /networks/(id)/connect</code></p> <p>Connect a container to a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30/connect HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Container\":\"3613f73ba0e4\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network or container is not found</li> <li>\n<strong>500</strong> - Internal Server Error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>container</strong> - container-id/name to be connected to the network</li> </ul> <h3 id=\"disconnect-a-container-from-a-network\">Disconnect a container from a network</h3> <p><code>POST /networks/(id)/disconnect</code></p> <p>Disconnect a container from a network</p> <p><strong>Example request</strong>:</p> <pre>POST /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30/disconnect HTTP/1.1\nContent-Type: application/json\n\n{\n  \"Container\":\"3613f73ba0e4\"\n}\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes:</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - network or container not found</li> <li>\n<strong>500</strong> - Internal Server Error</li> </ul> <p>JSON Parameters:</p> <ul> <li>\n<strong>Container</strong> - container-id/name to be disconnected from a network</li> </ul> <h3 id=\"remove-a-network\">Remove a network</h3> <p><code>DELETE /networks/(id)</code></p> <p>Instruct the driver to remove the network (<code>id</code>).</p> <p><strong>Example request</strong>:</p> <pre>DELETE /networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30 HTTP/1.1\n</pre> <p><strong>Example response</strong>:</p> <pre>HTTP/1.1 200 OK\n</pre> <p>Status Codes</p> <ul> <li>\n<strong>200</strong> - no error</li> <li>\n<strong>404</strong> - no such network</li> <li>\n<strong>500</strong> - server error</li> </ul> <h1 id=\"3-going-further\">3. Going further</h1> <h2 id=\"3-1-inside-docker-run\">3.1 Inside <code>docker run</code>\n</h2> <p>As an example, the <code>docker run</code> command line makes the following API calls:</p> <ul> <li><p>Create the container</p></li> <li>\n<p>If the status code is 404, it means the image doesn’t exist:</p> <ul> <li>Try to pull it.</li> <li>Then, retry to create the container.</li> </ul>\n</li> <li><p>Start the container.</p></li> <li><p>If you are not in detached mode:</p></li> <li><p>Attach to the container, using <code>logs=1</code> (to have <code>stdout</code> and <code>stderr</code> from the container’s start) and <code>stream=1</code></p></li> <li><p>If in detached mode or only <code>stdin</code> is attached, display the container’s id.</p></li> </ul> <h2 id=\"3-2-hijacking\">3.2 Hijacking</h2> <p>In this version of the API, <code>/attach</code>, uses hijacking to transport <code>stdin</code>, <code>stdout</code>, and <code>stderr</code> on the same socket.</p> <p>To hint potential proxies about connection hijacking, Docker client sends connection upgrade headers similarly to websocket.</p> <pre>Upgrade: tcp\nConnection: Upgrade\n</pre> <p>When Docker daemon detects the <code>Upgrade</code> header, it switches its status code from <strong>200 OK</strong> to <strong>101 UPGRADED</strong> and resends the same headers.</p> <h2 id=\"3-3-cors-requests\">3.3 CORS Requests</h2> <p>To set cross origin requests to the remote api please give values to <code>--api-cors-header</code> when running Docker in daemon mode. Set * (asterisk) allows all, default or blank means CORS disabled</p> <pre>$ docker daemon -H=\"192.168.1.9:2375\" --api-cors-header=\"http://foo.bar\"\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.21/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/api/docker_remote_api_v1.21/</a>\n  </p>\n</div>\n","swarm/swarm_at_scale/about/index":"<h1 id=\"learn-the-application-architecture\">Learn the application architecture</h1> <p>On this page, you learn about the Swarm at scale example. Make sure you have read through <a href=\"../index\">the introduction</a> to get an idea of the skills and time required first.</p> <h2 id=\"learn-the-example-back-story\">Learn the example back story</h2> <p>Your company is a pet food company that has bought a commercial during the Superbowl. The commercial drives viewers to a web survey that asks users to vote – cats or dogs. You are developing the web survey.</p> <p>Your survey must ensure that millions of people can vote concurrently without your website becoming unavailable. You don’t need real-time results, a company press release announces the results. However, you do need confidence that every vote is counted.</p> <h2 id=\"understand-the-application-architecture\">Understand the application architecture</h2> <p>The voting application is composed of several microservices. It uses a parallel web frontend that sends jobs to asynchronous background workers. The application’s design can accommodate arbitrarily large scale. The diagram below shows the appliation’s high level architecture:</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/app-architecture.png\" alt=\"\"></p> <p>All the servers are running Docker Engine. The entire application is fully “Dockerized” in that all services are running inside of containers.</p> <p>The frontend consists of a load balancer with <em>N</em> frontend instances. Each frontend consists of a web server and a Redis queue. The load balancer can handle an arbitrary number of web containers behind it (<code>frontend01</code>- <code>frontendN</code>). The web containers run a simple Python application that takes a vote between two options. It queues the votes to a Redis container running on the datastore.</p> <p>Behind the frontend is a worker tier which runs on separate nodes. This tier:</p> <ul> <li>scans the Redis containers</li> <li>dequeues votes</li> <li>deduplicates votes to prevent double voting</li> <li>commits the results to a Postgres database</li> </ul> <p>Just like the frontend, the worker tier can also scale arbitrarily. The worker count and frontend count are independent from each other.</p> <p>The application’s Dockerized microservices are deployed to a container network. Container networks are a feature of Docker Engine that allows communication between multiple containers across multiple Docker hosts.</p> <h2 id=\"swarm-cluster-architecture\">Swarm Cluster Architecture</h2> <p>To support the application, the design calls for a Swarm cluster with a single Swarm manager and four nodes as shown below.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/swarm-cluster-arch.png\" alt=\"\"></p> <p>All four nodes in the cluster are running the Docker daemon, as is the Swarm manager and the load balancer. The Swarm manager is part of the cluster and is considered out of band for the application. A single host running the Consul server acts as a keystore for both Swarm discovery and for the container network. The load balancer could be placed inside of the cluster, but for this demonstration it is not.</p> <p>After completing the example and deploying your application, this is what your environment should look like.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/final-result.png\" alt=\"\"></p> <p>As the previous diagram shows, each node in the cluster runs the following containers:</p> <ul> <li>\n<code>frontend01</code>: <ul> <li>Container: voting-app</li> <li>Container: Swarm agent</li> </ul>\n</li> <li>\n<code>frontend02</code>: <ul> <li>Container: voting-app</li> <li>Container: Swarm agent</li> </ul>\n</li> <li>\n<code>worker01</code>: <ul> <li>Container: voting-app-worker</li> <li>Container: Swarm agent</li> </ul>\n</li> <li>\n<code>dbstore</code>: <ul> <li>Container: voting-app-result-app</li> <li>Container: db (Postgres 9.4)</li> <li>Container: redis</li> <li>Container: Swarm agent</li> </ul>\n</li> </ul> <p>After deploying the application, you’ll configure your local system so that you can test the application from your local browser. In production, of course, this step wouldn’t be needed.</p> <h2 id=\"next-step\">Next step</h2> <p>Now that you understand the application architecture, you need to deploy a network configuration that can support it. In the next step, you <a href=\"../deploy-infra/index\">deploy network infrastructure</a> for use in this sample.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/swarm_at_scale/about/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/swarm_at_scale/about/</a>\n  </p>\n</div>\n","swarm/swarm_at_scale/deploy-infra/index":"<h1 id=\"deploy-your-infrastructure\">Deploy your infrastructure</h1> <p>In this step, you create several Docker hosts to run your application stack on. Before you continue, make sure you have taken the time to <a href=\"../about/index\">learn the application architecture</a>.</p> <h2 id=\"about-these-instructions\">About these instructions</h2> <p>This example assumes you are running on a Mac or Windows system and enabling Docker Engine <code>docker</code> commands by provisioning local VirtualBox virtual machines thru Docker Machine. For this evaluation installation, you’ll need 6 (six) VirtualBox VMs.</p> <p>While this example uses Docker Machine, this is only one example of an infrastructure you can use. You can create the environment design on whatever infrastructure you wish. For example, you could place the application on another public cloud platform such as Azure or DigitalOcean, on premises in your data center, or even in in a test environment on your laptop.</p> <p>Finally, these instructions use some common <code>bash</code> command substituion techniques to resolve some values, for example:</p> <pre>$ eval $(docker-machine env keystore)\n</pre> <p>In a Windows environment, these substituation fail. If you are running in Windows, replace the substitution <code>$(docker-machine env keystore)</code> with the actual value.</p> <h2 id=\"task-1-create-the-keystore-server\">Task 1. Create the keystore server</h2> <p>To enable a Docker container network and Swarm discovery, you must supply deploy a key-value store. As a discovery backend, the keystore maintains an up-to-date list of cluster members and shares that list with the Swarm manager. The Swarm manager uses this list to assign tasks to the nodes.</p> <p>An overlay network requires a key-value store. The key-value store holds information about the network state which includes discovery, networks, endpoints, IP addresses, and more.</p> <p>Several different backends are supported. This example uses <a href=\"https://www.consul.io/\" target=\"blank\">Consul</a> container.</p> <ol> <li>\n<p>Create a “machine” named <code>keystore</code>.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory \"2000\" \\\n--engine-opt=\"label=com.function=consul\"  keystore\n</pre> <p>You can set options for the Engine daemon with the <code>--engine-opt</code> flag. You’ll use it to label this Engine instance.</p>\n</li> <li>\n<p>Set your local shell to the <code>keystore</code> Docker host.</p> <pre>$ eval $(docker-machine env keystore)\n</pre>\n</li> <li>\n<p>Run <a href=\"https://hub.docker.com/r/progrium/consul/\" target=\"_blank\">the <code>consul</code> container</a>.</p> <pre>$ docker run --restart=unless-stopped -d -p 8500:8500 -h consul progrium/consul -server -bootstrap\n</pre> <p>The <code>-p</code> flag publishes port 8500 on the container which is where the Consul server listens. The server also has several other ports exposed which you can see by running <code>docker ps</code>.</p> <pre>$ docker ps\nCONTAINER ID        IMAGE               ...       PORTS                                                                            NAMES\n372ffcbc96ed        progrium/consul     ...       53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500-&gt;8500/tcp   dreamy_ptolemy\n</pre>\n</li> <li>\n<p>Use a <code>curl</code> command test the server by listing the nodes.</p> <pre>$ curl $(docker-machine ip keystore):8500/v1/catalog/nodes\n[{\"Node\":\"consul\",\"Address\":\"172.17.0.2\"}]\n</pre>\n</li> </ol> <h2 id=\"task-2-create-the-swarm-manager\">Task 2. Create the Swarm manager</h2> <p>In this step, you create the Swarm manager and connect it to the <code>keystore</code> instance. The Swarm manager container is the heart of your Swarm cluster. It is responsible for receiving all Docker commands sent to the cluster, and for scheduling resources against the cluster. In a real-world production deployment, you should configure additional replica Swarm managers as secondaries for high availability (HA).</p> <p>You’ll use the <code>--eng-opt</code> flag to set the <code>cluster-store</code> and <code>cluster-advertise</code> options to refer to the <code>keystore</code> server. These options support the container network you’ll create later.</p> <ol> <li>\n<p>Create the <code>manager</code> host.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory \"2000\" \\\n--engine-opt=\"label=com.function=manager\" \\\n--engine-opt=\"cluster-store=consul://$(docker-machine ip keystore):8500\" \\\n--engine-opt=\"cluster-advertise=eth1:2376\" manager\n</pre> <p>You also give the daemon a <code>manager</code> label.</p>\n</li> <li>\n<p>Set your local shell to the <code>manager</code> Docker host.</p> <pre>$ eval $(docker-machine env manager)\n</pre>\n</li> <li>\n<p>Start the Swarm manager process.</p> <pre>$ docker run --restart=unless-stopped -d -p 3376:2375 \\\n-v /var/lib/boot2docker:/certs:ro \\\nswarm manage --tlsverify \\\n--tlscacert=/certs/ca.pem \\\n--tlscert=/certs/server.pem \\\n--tlskey=/certs/server-key.pem \\\nconsul://$(docker-machine ip keystore):8500\n</pre> <p>This command uses the TLS certificates created for the <code>boot2docker.iso</code> or the manager. This is key for the manager when it connects to other machines in the cluster.</p>\n</li> <li>\n<p>Test your work by using displaying the Docker daemon logs from the host.</p> <pre>$ docker-machine ssh manager\n&lt;-- output snipped --&gt;\ndocker@manager:~$ tail /var/lib/boot2docker/docker.log\ntime=\"2016-04-06T23:11:56.481947896Z\" level=debug msg=\"Calling GET /v1.15/version\"\ntime=\"2016-04-06T23:11:56.481984742Z\" level=debug msg=\"GET /v1.15/version\"\ntime=\"2016-04-06T23:12:13.070231761Z\" level=debug msg=\"Watch triggered with 1 nodes\" discovery=consul\ntime=\"2016-04-06T23:12:33.069387215Z\" level=debug msg=\"Watch triggered with 1 nodes\" discovery=consul\ntime=\"2016-04-06T23:12:53.069471308Z\" level=debug msg=\"Watch triggered with 1 nodes\" discovery=consul\ntime=\"2016-04-06T23:13:13.069512320Z\" level=debug msg=\"Watch triggered with 1 nodes\" discovery=consul\ntime=\"2016-04-06T23:13:33.070021418Z\" level=debug msg=\"Watch triggered with 1 nodes\" discovery=consul\ntime=\"2016-04-06T23:13:53.069395005Z\" level=debug msg=\"Watch triggered with 1 nodes\" discovery=consul\ntime=\"2016-04-06T23:14:13.071417551Z\" level=debug msg=\"Watch triggered with 1 nodes\" discovery=consul\ntime=\"2016-04-06T23:14:33.069843647Z\" level=debug msg=\"Watch triggered with 1 nodes\" discovery=consul\n</pre> <p>The output indicates that the <code>consul</code> and the <code>manager</code> are communicating correctly.</p>\n</li> <li>\n<p>Exit the Docker host.</p> <pre>docker@manager:~$ exit\n</pre>\n</li> </ol> <h2 id=\"task-3-add-the-load-balancer\">Task 3. Add the load balancer</h2> <p>The application uses <a href=\"https://github.com/ehazlett/interlock\">Interlock</a> and Nginx as a loadbalancer. Before you build the load balancer host, you’ll create the configuration you’ll use for Nginx.</p> <ol> <li>On your local host, create a <code>config</code> directory.</li> <li>\n<p>Change directories to the <code>config</code> directory.</p> <pre>$ cd config\n</pre>\n</li> <li>\n<p>Get the IP address of the Swarm manager host.</p> <p>For example:</p> <pre>$ docker-machine ip manager\n192.168.99.101\n</pre>\n</li> <li>\n<p>Use your favorite editor to create a <code>config.toml</code> file and add this content to the file:</p> <pre>ListenAddr = \":8080\"\nDockerURL = \"tcp://SWARM_MANAGER_IP:3376\"\nTLSCACert = \"/var/lib/boot2docker/ca.pem\"\nTLSCert = \"/var/lib/boot2docker/server.pem\"\nTLSKey = \"/var/lib/boot2docker/server-key.pem\"\n\n[[Extensions]]\nName = \"nginx\"\nConfigPath = \"/etc/conf/nginx.conf\"\nPidPath = \"/etc/conf/nginx.pid\"\nMaxConn = 1024\nPort = 80\n</pre>\n</li> <li>\n<p>In the configuration, replace the <code>SWARM_MANAGER_IP</code> with the <code>manager</code> IP you got in Step 4.</p> <p>You use this value because the load balancer listens on the manager’s event stream.</p>\n</li> <li><p>Save and close the <code>config.toml</code> file.</p></li> <li>\n<p>Create a machine for the load balancer.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory \"2000\" \\\n--engine-opt=\"label=com.function=interlock\" loadbalancer\n</pre>\n</li> <li>\n<p>Switch the environment to the <code>loadbalancer</code>.</p> <pre>$ eval $(docker-machine env loadbalancer)\n</pre>\n</li> <li>\n<p>Start an <code>interlock</code> container running.</p> <pre>$ docker run \\\n    -P \\\n    -d \\\n    -ti \\\n    -v nginx:/etc/conf \\\n    -v /var/lib/boot2docker:/var/lib/boot2docker:ro \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v $(pwd)/config.toml:/etc/config.toml \\\n    --name interlock \\\n    ehazlett/interlock:1.0.1 \\\n    -D run -c /etc/config.toml\n</pre> <p>This command relies on the <code>config.toml</code> file being in the current directory. After running the command, confirm the image is runing:</p> <pre>$ docker ps\nCONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                     NAMES\nd846b801a978        ehazlett/interlock:1.0.1   \"/bin/interlock -D ru\"   2 minutes ago       Up 2 minutes        0.0.0.0:32770-&gt;8080/tcp   interlock\n</pre> <p>If you don’t see the image runing, use <code>docker ps -a</code> to list all images to make sure the system attempted to start the image. Then, get the logs to see why the container failed to start.</p> <pre>$ docker logs interlock\nINFO[0000] interlock 1.0.1 (000291d)\nDEBU[0000] loading config from: /etc/config.toml\nFATA[0000] read /etc/config.toml: is a directory\n</pre> <p>This error usually means you weren’t starting the <code>docker run</code> from the same <code>config</code> directory where the <code>config.toml</code> file is. If you run the command and get a Conflict error such as:</p> <pre>docker: Error response from daemon: Conflict. The name \"/interlock\" is already in use by container d846b801a978c76979d46a839bb05c26d2ab949ff9f4f740b06b5e2564bae958. You have to remove (or rename) that container to be able to reuse that name.\n</pre> <p>Remove the interlock container with the <code>docker rm interlock</code> and try again.</p>\n</li> <li>\n<p>Start an <code>nginx</code> container on the load balancer.</p> <pre>$ docker run -ti -d \\\n  -p 80:80 \\\n  --label interlock.ext.name=nginx \\\n  --link=interlock:interlock \\\n  -v nginx:/etc/conf \\\n  --name nginx \\\n  nginx nginx -g \"daemon off;\" -c /etc/conf/nginx.conf\n</pre>\n</li> </ol> <h2 id=\"task-4-create-the-other-swarm-nodes\">Task 4. Create the other Swarm nodes</h2> <p>A host in a Swarm cluster is called a <em>node</em>. You’ve already created the manager node. Here, the task is to create each virtual host for each node. There are three commands required:</p> <ul> <li>create the host with Docker Machine</li> <li>point the local environment to the new host</li> <li>join the host to the Swarm cluster</li> </ul> <p>If you were building this in a non-Mac/Windows environment, you’d only need to run the <code>join</code> command to add a node to the Swarm cluster and register it with the Consul discovery service. When you create a node, you also give it a label, for example:</p> <pre>--engine-opt=\"label=com.function=frontend01\"\n</pre> <p>You’ll use these labels later when starting application containers. In the commands below, notice the label you are applying to each node.</p> <ol> <li>\n<p>Create the <code>frontend01</code> host and add it to the Swarm cluster.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory \"2000\" \\\n--engine-opt=\"label=com.function=frontend01\" \\\n--engine-opt=\"cluster-store=consul://$(docker-machine ip keystore):8500\" \\\n--engine-opt=\"cluster-advertise=eth1:2376\" frontend01\n$ eval $(docker-machine env frontend01)\n$ docker run -d swarm join --addr=$(docker-machine ip frontend01):2376 consul://$(docker-machine ip keystore):8500\n</pre>\n</li> <li>\n<p>Create the <code>frontend02</code> VM.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory \"2000\" \\\n--engine-opt=\"label=com.function=frontend02\" \\\n--engine-opt=\"cluster-store=consul://$(docker-machine ip keystore):8500\" \\\n--engine-opt=\"cluster-advertise=eth1:2376\" frontend02\n$ eval $(docker-machine env frontend02)\n$ docker run -d swarm join --addr=$(docker-machine ip frontend02):2376 consul://$(docker-machine ip keystore):8500\n</pre>\n</li> <li>\n<p>Create the <code>worker01</code> VM.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory \"2000\" \\\n--engine-opt=\"label=com.function=worker01\" \\\n--engine-opt=\"cluster-store=consul://$(docker-machine ip keystore):8500\" \\\n--engine-opt=\"cluster-advertise=eth1:2376\" worker01\n$ eval $(docker-machine env worker01)\n$ docker run -d swarm join --addr=$(docker-machine ip worker01):2376 consul://$(docker-machine ip keystore):8500\n</pre>\n</li> <li>\n<p>Create the <code>dbstore</code> VM.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory \"2000\" \\\n--engine-opt=\"label=com.function=dbstore\" \\\n--engine-opt=\"cluster-store=consul://$(docker-machine ip keystore):8500\" \\\n--engine-opt=\"cluster-advertise=eth1:2376\" dbstore\n$ eval $(docker-machine env dbstore)\n$ docker run -d swarm join --addr=$(docker-machine ip dbstore):2376 consul://$(docker-machine ip keystore):8500\n</pre>\n</li> <li>\n<p>Check your work.</p> <p>At this point, you have deployed on the infrastructure you need to run the application. Test this now by listing the running machines:</p> <pre>$ docker-machine ls\nNAME           ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS\ndbstore        -        virtualbox   Running   tcp://192.168.99.111:2376           v1.10.3\nfrontend01     -        virtualbox   Running   tcp://192.168.99.108:2376           v1.10.3\nfrontend02     -        virtualbox   Running   tcp://192.168.99.109:2376           v1.10.3\nkeystore       -        virtualbox   Running   tcp://192.168.99.100:2376           v1.10.3\nloadbalancer   -        virtualbox   Running   tcp://192.168.99.107:2376           v1.10.3\nmanager        -        virtualbox   Running   tcp://192.168.99.101:2376           v1.10.3\nworker01       *        virtualbox   Running   tcp://192.168.99.110:2376           v1.10.3\n</pre>\n</li> <li>\n<p>Make sure the Swarm manager sees all your nodes.</p> <pre>$ docker -H $(docker-machine ip manager):3376 info\nContainers: 4\n Running: 4\n Paused: 0\n Stopped: 0\nImages: 3\nServer Version: swarm/1.1.3\nRole: primary\nStrategy: spread\nFilters: health, port, dependency, affinity, constraint\nNodes: 4\n dbstore: 192.168.99.111:2376\n  └ Status: Healthy\n  └ Containers: 1\n  └ Reserved CPUs: 0 / 1\n  └ Reserved Memory: 0 B / 2.004 GiB\n  └ Labels: com.function=dbstore, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs\n  └ Error: (none)\n  └ UpdatedAt: 2016-04-07T18:25:37Z\n frontend01: 192.168.99.108:2376\n  └ Status: Healthy\n  └ Containers: 1\n  └ Reserved CPUs: 0 / 1\n  └ Reserved Memory: 0 B / 2.004 GiB\n  └ Labels: com.function=frontend01, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs\n  └ Error: (none)\n  └ UpdatedAt: 2016-04-07T18:26:10Z\n frontend02: 192.168.99.109:2376\n  └ Status: Healthy\n  └ Containers: 1\n  └ Reserved CPUs: 0 / 1\n  └ Reserved Memory: 0 B / 2.004 GiB\n  └ Labels: com.function=frontend02, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs\n  └ Error: (none)\n  └ UpdatedAt: 2016-04-07T18:25:43Z\n worker01: 192.168.99.110:2376\n  └ Status: Healthy\n  └ Containers: 1\n  └ Reserved CPUs: 0 / 1\n  └ Reserved Memory: 0 B / 2.004 GiB\n  └ Labels: com.function=worker01, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs\n  └ Error: (none)\n  └ UpdatedAt: 2016-04-07T18:25:56Z\nPlugins:\n Volume:\n Network:\nKernel Version: 4.1.19-boot2docker\nOperating System: linux\nArchitecture: amd64\nCPUs: 4\nTotal Memory: 8.017 GiB\nName: bb13b7cf80e8\n</pre> <p>The command is acting on the Swarm port, so it returns information about the entire cluster. You have a manager and no nodes.</p>\n</li> </ol> <h2 id=\"next-step\">Next Step</h2> <p>Your key-store, load balancer, and Swarm cluster infrastructure is up. You are ready to <a href=\"../deploy-app/index\">build and run the voting application</a> on it.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/swarm_at_scale/deploy-infra/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/swarm_at_scale/deploy-infra/</a>\n  </p>\n</div>\n","swarm/swarm_at_scale/deploy-app/index":"<h1 id=\"deploy-the-application\">Deploy the application</h1> <p>You’ve <a href=\"../deploy-infra/index\">deployed the load balancer, the discovery backend, and a Swarm cluster</a> so now you can build and deploy the voting application itself. You do this by starting a number of “Dockerized applications” running in containers.</p> <p>The diagram below shows the final application configuration including the overlay container network, <code>voteapp</code>.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/final-result.png\" alt=\"\"></p> <p>In this procedure you will connect containers to this network. The <code>voteapp</code> network is available to all Docker hosts using the Consul discovery backend. Notice that the <code>interlock</code>, <code>nginx</code>, <code>consul</code>, and <code>swarm manager</code> containers on are not part of the <code>voteapp</code> overlay container network.</p> <h2 id=\"task-1-set-up-volume-and-network\">Task 1. Set up volume and network</h2> <p>This application relies on both an overlay container network and a container volume. The Docker Engine provides these two features. You’ll create them both on the Swarm <code>manager</code> instance.</p> <ol> <li>\n<p>Direct your local environment to the Swarm manager host.</p> <pre>$ eval $(docker-machine env manager)\n</pre> <p>You can create the network on a cluster node at the network is visible on them all.</p>\n</li> <li>\n<p>Create the <code>voteapp</code> container network.</p> <pre>$ docker network create -d overlay voteapp\n</pre>\n</li> <li>\n<p>Switch to the db store.</p> <pre>$ eval $(docker-machine env dbstore)\n</pre>\n</li> <li>\n<p>Verify you can see the new network from the dbstore node.</p> <pre>$ docker network ls\nNETWORK ID          NAME                DRIVER\ne952814f610a        voteapp             overlay\n1f12c5e7bcc4        bridge              bridge\n3ca38e887cd8        none                null\n3da57c44586b        host                host\n</pre>\n</li> <li>\n<p>Create a container volume called <code>db-data</code>.</p> <pre>$ docker volume create --name db-data\n</pre>\n</li> </ol> <h2 id=\"task-2-start-the-containerized-microservices\">Task 2. Start the containerized microservices</h2> <p>At this point, you are ready to start the component microservices that make up the application. Some of the application’s containers are launched from existing images pulled directly from Docker Hub. Other containers are launched from custom images you must build. The list below shows which containers use custom images and which do not:</p> <ul> <li>Load balancer container: stock image (<code>ehazlett/interlock</code>)</li> <li>Redis containers: stock image (official <code>redis</code> image)</li> <li>Postgres (PostgreSQL) containers: stock image (official <code>postgres</code> image)</li> <li>Web containers: custom built image</li> <li>Worker containers: custom built image</li> <li>Results containers: custom built image</li> </ul> <p>You can launch these containers from any host in the cluster using the commands in this section. Each command includs a <code>-H</code>flag so that they execute against the Swarm manager.</p> <p>The commands also all use the <code>-e</code> flag which is a Swarm constraint. The constraint tells the manager to look for a node with a matching function label. You set established the labels when you created the nodes. As you run each command below, look for the value constraint.</p> <ol> <li>\n<p>Start a Postgres database container.</p> <pre>$ docker -H $(docker-machine ip manager):3376 run -t -d \\\n-v db-data:/var/lib/postgresql/data \\\n-e constraint:com.function==dbstore \\\n--net=\"voteapp\" \\\n--name db postgres:9.4\n</pre>\n</li> <li>\n<p>Start the Redis container.</p> <pre>$ docker -H $(docker-machine ip manager):3376 run -t -d \\\n-p 6379:6379 \\\n-e constraint:com.function==dbstore \\\n--net=\"voteapp\" \\\n--name redis redis\n</pre> <p>The <code>redis</code> name is important so don’t change it.</p>\n</li> <li>\n<p>Start the worker application</p> <pre>$ docker -H $(docker-machine ip manager):3376 run -t -d \\\n-e constraint:com.function==worker01 \\\n--net=\"voteapp\" \\\n--net-alias=workers \\\n--name worker01 docker/example-voting-app-worker\n</pre>\n</li> <li>\n<p>Start the results application.</p> <pre>$ docker -H $(docker-machine ip manager):3376 run -t -d \\\n-p 80:80 \\\n--label=interlock.hostname=results \\\n--label=interlock.domain=myenterprise.com \\\n-e constraint:com.function==dbstore \\\n--net=\"voteapp\" \\\n--name results-app docker/example-voting-app-result-app\n</pre>\n</li> <li>\n<p>Start the voting application twice; once on each frontend node.</p> <pre>$ docker -H $(docker-machine ip manager):3376 run -t -d \\\n-p 80:80 \\\n--label=interlock.hostname=vote \\\n--label=interlock.domain=myenterprise.com \\\n-e constraint:com.function==frontend01 \\\n--net=\"voteapp\" \\\n--name voting-app01 docker/example-voting-app-voting-app\n</pre> <p>And again on the other frontend node.</p> <pre>$ docker -H $(docker-machine ip manager):3376 run -t -d \\\n-p 80:80 \\\n--label=interlock.hostname=vote \\\n--label=interlock.domain=myenterprise.com \\\n-e constraint:com.function==frontend02 \\\n--net=\"voteapp\" \\\n--name voting-app02 docker/example-voting-app-voting-app\n</pre>\n</li> </ol> <h2 id=\"task-3-check-your-work-and-update-etc-hosts\">Task 3. Check your work and update /etc/hosts</h2> <p>In this step, you check your work to make sure the Nginx configuration recorded the containers correctly. You’ll update your local systems <code>/etc/hosts</code> file to allow you to take advantage of the loadbalancer.</p> <ol> <li>\n<p>Change to the <code>loadbalancer</code> node.</p> <pre>$ eval $(docker-machine env loadbalancer)\n</pre>\n</li> <li>\n<p>Check your work by reviewing the configuration of nginx.</p> <pre>$ docker exec interlock cat /etc/conf/nginx.conf\n... output snipped ...\n\nupstream results.myenterprise.com {\n    zone results.myenterprise.com_backend 64k;\n\n    server 192.168.99.111:80;\n\n}\nserver {\n    listen 80;\n\n    server_name results.myenterprise.com;\n\n    location / {\n        proxy_pass http://results.myenterprise.com;\n    }\n}\nupstream vote.myenterprise.com {\n    zone vote.myenterprise.com_backend 64k;\n\n    server 192.168.99.109:80;\n    server 192.168.99.108:80;\n\n}\nserver {\n    listen 80;\n\n    server_name vote.myenterprise.com;\n\n    location / {\n        proxy_pass http://vote.myenterprise.com;\n    }\n}\n\ninclude /etc/conf/conf.d/*.conf;\n}\n</pre>\n</li> </ol> <p>The <code>http://vote.myenterprise.com</code> site configuration should point to either frontend node. Requests to <code>http://results.myenterprise.com</code> go just to the single <code>dbstore</code> node where the <code>example-voting-app-result-app</code> is running.</p> <ol> <li><p>On your local host, edit <code>/etc/hosts</code> file add the resolution for both these sites.</p></li> <li><p>Save and close the <code>/etc/hosts</code> file.</p></li> <li>\n<p>Restart the <code>nginx</code> container.</p> <p>Manual restart is required because the current Interlock server is not forcing an Nginx configuration reload.</p> <pre>$ docker restart nginx\n</pre>\n</li> </ol> <h2 id=\"task-4-test-the-application\">Task 4. Test the application</h2> <p>Now, you can test your application.</p> <ol> <li>\n<p>Open a browser and navigate to the <code>http://vote.myenterprise.com</code> site.</p> <p>You should see something similar to the following:</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/vote-app-test.png\" alt=\"\"></p>\n</li> <li><p>Click on one of the two voting options.</p></li> <li><p>Navigate to the <code>http://results.myenterprise.com</code> site to see the results.</p></li> <li>\n<p>Try changing your vote.</p> <p>You’ll see both sides change as you switch your vote.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/votes.gif\" alt=\"\"></p>\n</li> </ol> <h2 id=\"extra-credit-deployment-with-docker-compose\">Extra Credit: Deployment with Docker Compose</h2> <p>Up to this point, you’ve deployed each application container individually. This can be cumbersome espeically because their are several different containers and starting them is order dependent. For example, that database should be running before the worker.</p> <p>Docker Compose let’s you define your microservice containers and their dependencies in a Compose file. Then, you can use the Compose file to start all the containers at once. This extra credit</p> <ol> <li>\n<p>Before you begin, stop all the containers you started.</p> <p>a. Set the host to the manager.</p> <pre>$ DOCKER_HOST=$(docker-machine ip manager):3376\n</pre> <p>b. List all the application continers on the Swarm.</p> <p>c. Stop and remove each container.</p>\n</li> <li>\n<p>Try to create Compose file on your own by reviewing the tasks in this tutorial.</p> <p><a href=\"../../../compose/compose-file/index\">The version 2 Compose file format</a> is the best to use. Translate each <code>docker run</code> command into a service in the <code>docker-compose.yml</code> file. For example, this command:</p> <pre>$ docker -H $(docker-machine ip manager):3376 run -t -d \\\n-e constraint:com.function==worker01 \\\n--net=\"voteapp\" \\\n--net-alias=workers \\\n--name worker01 docker/example-voting-app-worker\n</pre> <p>Becomes this in a Compose file.</p> <pre>worker:\n  image: docker/example-voting-app-worker\n  networks:\n    voteapp:\n      aliases:\n      - workers\n</pre> <p>In general, Compose starts services in reverse order they appear in the file. So, if you want a service to start before all the others, make it the last service in the file file. This applciation relies on a volume and a network, declare those at the bottom of the file.</p>\n</li> <li><p>Check your work against <a href=\"../docker-compose.yml/index\" target=\"_blank\">this result file</a></p></li> <li><p>When you are satisifed, save the <code>docker-compose.yml</code> file to your system.</p></li> <li>\n<p>Set <code>DOCKER_HOST</code> to the Swarm manager.</p> <pre>$ DOCKER_HOST=$(docker-machine ip manager):3376\n</pre>\n</li> <li>\n<p>In the same directory as your <code>docker-compose.yml</code> file, start the services.</p> <pre>$ docker-compose up -d\nCreating network \"scale_voteapp\" with the default driver\nCreating volume \"scale_db-data\" with default driver\nPulling db (postgres:9.4)...\nworker01: Pulling postgres:9.4... : downloaded\ndbstore: Pulling postgres:9.4... : downloaded\nfrontend01: Pulling postgres:9.4... : downloaded\nfrontend02: Pulling postgres:9.4... : downloaded\nCreating db\nPulling redis (redis:latest)...\ndbstore: Pulling redis:latest... : downloaded\nfrontend01: Pulling redis:latest... : downloaded\nfrontend02: Pulling redis:latest... : downloaded\nworker01: Pulling redis:latest... : downloaded\nCreating redis\nPulling worker (docker/example-voting-app-worker:latest)...\ndbstore: Pulling docker/example-voting-app-worker:latest... : downloaded\nfrontend01: Pulling docker/example-voting-app-worker:latest... : downloaded\nfrontend02: Pulling docker/example-voting-app-worker:latest... : downloaded\nworker01: Pulling docker/example-voting-app-worker:latest... : downloaded\nCreating scale_worker_1\nPulling voting-app (docker/example-voting-app-voting-app:latest)...\ndbstore: Pulling docker/example-voting-app-voting-app:latest... : downloaded\nfrontend01: Pulling docker/example-voting-app-voting-app:latest... : downloaded\nfrontend02: Pulling docker/example-voting-app-voting-app:latest... : downloaded\nworker01: Pulling docker/example-voting-app-voting-app:latest... : downloaded\nCreating scale_voting-app_1\nPulling result-app (docker/example-voting-app-result-app:latest)...\ndbstore: Pulling docker/example-voting-app-result-app:latest... : downloaded\nfrontend01: Pulling docker/example-voting-app-result-app:latest... : downloaded\nfrontend02: Pulling docker/example-voting-app-result-app:latest... : downloaded\nworker01: Pulling docker/example-voting-app-result-app:latest... : downloaded\nCreating scale_result-app_1\n</pre>\n</li> <li>\n<p>Use the <code>docker ps</code> command to see the containers on the Swarm cluster.</p> <pre>$ docker -H $(docker-machine ip manager):3376 ps\nCONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                            NAMES\nb71555033caa        docker/example-voting-app-result-app   \"node server.js\"         6 seconds ago       Up 4 seconds        192.168.99.104:32774-&gt;80/tcp     frontend01/scale_result-app_1\ncf29ea21475d        docker/example-voting-app-worker       \"/usr/lib/jvm/java-7-\"   6 seconds ago       Up 4 seconds                                         worker01/scale_worker_1\n98414cd40ab9        redis                                  \"/entrypoint.sh redis\"   7 seconds ago       Up 5 seconds        192.168.99.105:32774-&gt;6379/tcp   frontend02/redis\n1f214acb77ae        postgres:9.4                           \"/docker-entrypoint.s\"   7 seconds ago       Up 5 seconds        5432/tcp                         frontend01/db\n1a4b8f7ce4a9        docker/example-voting-app-voting-app   \"python app.py\"          7 seconds ago       Up 5 seconds        192.168.99.107:32772-&gt;80/tcp     dbstore/scale_voting-app_1\n</pre> <p>When you started the services manually, you had a <code>voting-app</code> instances running on two frontend servers. How many do you have now?</p>\n</li> <li>\n<p>Scale your application up by adding some <code>voting-app</code> instances.</p> <pre>$ docker-compose scale voting-app=3\nCreating and starting 2 ... done\nCreating and starting 3 ... done\n</pre> <p>After you scale up, list the containers on the cluster again.</p>\n</li> <li>\n<p>Change to the <code>loadbalancer</code> node.</p> <pre>$ eval $(docker-machine env loadbalancer)\n</pre>\n</li> <li>\n<p>Restart the Nginx server.</p> <pre>$ docker restart nginx\n</pre>\n</li> <li><p>Check your work again by visiting the <code>http://vote.myenterprise.com</code> and <code>http://results.myenterprise.com</code> again.</p></li> <li><p>You can view the logs on an indvidual container.</p></li> </ol> <pre>  $ docker logs scale_voting-app_1\n   * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)\n   * Restarting with stat\n   * Debugger is active!\n   * Debugger pin code: 285-809-660\n  192.168.99.103 - - [11/Apr/2016 17:15:44] \"GET / HTTP/1.0\" 200 -\n  192.168.99.103 - - [11/Apr/2016 17:15:44] \"GET /static/stylesheets/style.css HTTP/1.0\" 304 -\n  192.168.99.103 - - [11/Apr/2016 17:15:45] \"GET /favicon.ico HTTP/1.0\" 404 -\n  192.168.99.103 - - [11/Apr/2016 17:22:24] \"POST / HTTP/1.0\" 200 -\n  192.168.99.103 - - [11/Apr/2016 17:23:37] \"POST / HTTP/1.0\" 200 -\n  192.168.99.103 - - [11/Apr/2016 17:23:39] \"POST / HTTP/1.0\" 200 -\n  192.168.99.103 - - [11/Apr/2016 17:23:40] \"POST / HTTP/1.0\" 200 -\n  192.168.99.103 - - [11/Apr/2016 17:23:41] \"POST / HTTP/1.0\" 200 -\n  192.168.99.103 - - [11/Apr/2016 17:23:43] \"POST / HTTP/1.0\" 200 -\n  192.168.99.103 - - [11/Apr/2016 17:23:44] \"POST / HTTP/1.0\" 200 -\n  192.168.99.103 - - [11/Apr/2016 17:23:46] \"POST / HTTP/1.0\" 200 -\n</pre> <p>This log shows the activity on one of the active voting application containers.</p> <h2 id=\"next-steps\">Next steps</h2> <p>Congratulations. You have successfully walked through manually deploying a microservice-based application to a Swarm cluster. Of course, not every deployment goes smoothly. Now that you’ve learned how to successfully deploy an application at scale, you should learn <a href=\"../troubleshoot/index\">what to consider when troubleshooting large applications running on a Swarm cluster</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/swarm_at_scale/deploy-app/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/swarm_at_scale/deploy-app/</a>\n  </p>\n</div>\n","swarm/swarm_at_scale/troubleshoot/index":"<h1 id=\"troubleshoot-the-application\">Troubleshoot the application</h1> <p>It’s a fact of life that things fail. With this in mind, it’s important to understand what happens when failures occur and how to mitigate them. The following sections cover different failure scenarios:</p> <ul> <li><a href=\"#swarm-manager-failures\">Swarm manager failures</a></li> <li><a href=\"#consul-discovery-backend-failures\">Consul (discovery backend) failures</a></li> <li><a href=\"#interlock-load-balancer-failures\">Interlock load balancer failures</a></li> <li><a href=\"#web-voting-app-failures\">Web (voting-app) failures</a></li> <li><a href=\"#redis-failures\">Redis failures</a></li> <li><a href=\"#worker-vote-worker-failures\">Worker (vote-worker) failures</a></li> <li><a href=\"#postgres-failures\">Postgres failures</a></li> <li><a href=\"#results-app-failures\">Results-app failures</a></li> <li><a href=\"#infrastructure-failures\">Infrastructure failures</a></li> </ul> <h2 id=\"swarm-manager-failures\">Swarm manager failures</h2> <p>In it’s current configuration, the Swarm cluster only has single manager container running on a single node. If the container exits or the node fails, you will not be able to administer the cluster until you either; fix it, or replace it.</p> <p>If the failure is the Swarm manager container unexpectedly exiting, Docker will automatically attempt to restart it. This is because the container was started with the <code>--restart=unless-stopped</code> switch.</p> <p>While the Swarm manager is unavailable, the application will continue to work in its current configuration. However, you will not be able to provision more nodes or containers until you have a working Swarm manager.</p> <p>Docker Swarm supports high availability for Swarm managers. This allows a single Swarm cluster to have two or more managers. One manager is elected as the primary manager and all others operate as secondaries. In the event that the primary manager fails, one of the secondaries is elected as the new primary, and cluster operations continue gracefully. If you are deploying multiple Swarm managers for high availability, you should consider spreading them across multiple failure domains within your infrastructure.</p> <h2 id=\"consul-discovery-backend-failures\">Consul (discovery backend) failures</h2> <p>The Swarm cluster that you have deployed has a single Consul container on a single node performing the cluster discovery service. In this setup, if the Consul container exits or the node fails, the application will continue to operate in its current configuration. However, certain cluster management operations will fail. These include registering new containers in the cluster and making lookups against the cluster configuration.</p> <p>If the failure is the <code>consul</code> container unexpectedly exiting, Docker will automatically attempt to restart it. This is because the container was started with the <code>--restart=unless-stopped</code> switch.</p> <p>The <code>Consul</code>, <code>etcd</code>, and <code>Zookeeper</code> discovery service backends support various options for high availability. These include Paxos/Raft quorums. You should follow existing best practices for deploying HA configurations of your chosen discover service backend. If you are deploying multiple discovery service instances for high availability, you should consider spreading them across multiple failure domains within your infrastructure.</p> <p>If you operate your Swarm cluster with a single discovery backend service and this service fails and is unrecoverable, you can start a new empty instance of the discovery backend and the Swarm agents on each node in the cluster will repopulate it.</p> <h3 id=\"handling-failures\">Handling failures</h3> <p>There are many reasons why containers can fail. However, Swarm does not attempt to restart failed containers.</p> <p>One way to automatically restart failed containers is to explicitly start them with the <code>--restart=unless-stopped</code> flag. This will tell the local Docker daemon to attempt to restart the container if it unexpectedly exits. This will only work in situations where the node hosting the container and it’s Docker daemon are still up. This cannot restart a container if the node hosting it has failed, or if the Docker daemon itself has failed.</p> <p>Another way is to have an external tool (external to the cluster) monitor the state of your application, and make sure that certain service levels are maintained. These service levels can include things like “have at least 10 web server containers running”. In this scenario, if the number of web containers drops below 10, the tool will attempt to start more.</p> <p>In our simple voting-app example, the front-end is scalable and serviced by a load balancer. In the event that on the of the two web containers fails (or the node that is hosting it), the load balancer will stop routing requests to it and send all requests the surviving web container. This solution is highly scalable meaning you can have up to <em>n</em> web containers behind the load balancer.</p> <h2 id=\"interlock-load-balancer-failures\">Interlock load balancer failures</h2> <p>The environment that you have provisioned has a single <a href=\"https://github.com/ehazlett/interlock\">interlock</a> load balancer container running on a single node. In this setup, if the container exits or node fails, the application will no longer be able to service incoming requests and the application will be unavailable.</p> <p>If the failure is the <code>interlock</code> container unexpectedly exiting, Docker will automatically attempt to restart it. This is because the container was started with the <code>--restart=unless-stopped</code> switch.</p> <p>It is possible to build an HA Interlock load balancer configuration. One such way is to have multiple Interlock containers on multiple nodes. You can then use DNS round robin, or other technologies, to load balance across each Interlock container. That way, if one Interlock container or node goes down, the others will continue to service requests.</p> <p>If you deploy multiple interlock load balancers, you should consider spreading them across multiple failure domains within your infrastructure.</p> <h2 id=\"web-voting-app-failures\">Web (voting-app) failures</h2> <p>The environment that you have configured has two voting-app containers running on two separate nodes. They operate behind an Interlock load balancer that distributes incoming connections across both.</p> <p>In the event that one of the web containers or nodes fails, the load balancer will start directing all incoming requests to surviving instance. Once the failed instance is back up, or a replacement is added, the load balancer will add it to the configuration and start sending a portion of the incoming requests to it.</p> <p>For highest availability you should deploy the two frontend web services (<code>frontend01</code> and <code>frontend02</code>) in different failure zones within your infrastructure. You should also consider deploying more.</p> <h2 id=\"redis-failures\">Redis failures</h2> <p>If the a <code>redis</code> container fails, it’s partnered <code>voting-app</code> container will not function correctly. The best solution in this instance might be to configure health monitoring that verifies the ability to write to each Redis instance. If an unhealthy <code>redis</code> instance is encountered, remove the <code>voting-app</code> and <code>redis</code> combination and attempt remedial actions.</p> <h2 id=\"worker-vote-worker-failures\">Worker (vote-worker) failures</h2> <p>If the worker container exits, or the node that is hosting it fails, the redis containers will queue votes until the worker container comes back up. This situation can prevail indefinitely, though a worker needs to come back at some point and process the votes.</p> <p>If the failure is the <code>worker01</code> container unexpectedly exiting, Docker will automatically attempt to restart it. This is because the container was started with the <code>--restart=unless-stopped</code> switch.</p> <h2 id=\"postgres-failures\">Postgres failures</h2> <p>This application does not implement any for of HA or replication for Postgres. Therefore losing the Postgres container would cause the application to fail and potential lose or corrupt data. A better solution would be to implement some form of Postgres HA or replication.</p> <h2 id=\"results-app-failures\">Results-app failures</h2> <p>If the results-app container exits, you will not be able to browse to the results of the poll until the container is back up and running. Results will continue to be collected and counted, you will just not be able to view results until the container is back up and running.</p> <p>The results-app container was started with the <code>--restart=unless-stopped</code> flag meaning that the Docker daemon will automatically attempt to restart it unless it was administratively stopped.</p> <h2 id=\"infrastructure-failures\">Infrastructure failures</h2> <p>There are many ways in which the infrastructure underpinning your applications can fail. However, there are a few best practices that can be followed to help mitigate and offset these failures.</p> <p>One of these is to deploy infrastructure components over as many failure domains as possible. On a service such as AWS, this often translates into balancing infrastructure and services across multiple AWS Availability Zones (AZ) within a Region.</p> <p>To increase the availability of our Swarm cluster you could:</p> <ul> <li>Configure the Swarm manager for HA and deploy HA nodes in different AZs</li> <li>Configure the Consul discovery service for HA and deploy HA nodes in different AZs</li> <li>Deploy all scalable components of the application across multiple AZs</li> </ul> <p>This configuration is shown in the diagram below.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/infrastructure-failures.jpg\" alt=\"\"></p> <p>This will allow us to lose an entire AZ and still have our cluster and application operate.</p> <p>But it doesn’t have to stop there. Some applications can be balanced across AWS Regions. It’s even becoming possible to deploy services across cloud providers, or have balance services across public cloud providers and your on premises date centers!</p> <p>The diagram below shows parts of the application and infrastructure deployed across AWS and Microsoft Azure. But you could just as easily replace one of those cloud providers with your own on premises data center. In these scenarios, network latency and reliability is key to a smooth and workable solution.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/deployed-across.jpg\" alt=\"\"></p> <h2 id=\"related-information\">Related information</h2> <p>The application in this example could be deployed on Docker Universal Control Plane (UCP) which is currently in Beta release. To try the application on UCP in your environment, <a href=\"https://www.docker.com/products/docker-universal-control-plane\">request access to the UCP Beta release</a>. Other useful documentation:</p> <ul> <li><a href=\"../../plan-for-production/index\">Plan for Swarm in production</a></li> <li><a href=\"../../networking/index\">Swarm and container networks</a></li> <li><a href=\"../../multi-manager-setup/index\">High availability in Docker Swarm</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/swarm_at_scale/troubleshoot/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/swarm_at_scale/troubleshoot/</a>\n  </p>\n</div>\n","swarm/multi-manager-setup/index":"<h1 id=\"high-availability-in-docker-swarm\">High availability in Docker Swarm</h1> <p>In Docker Swarm, the <strong>Swarm manager</strong> is responsible for the entire cluster and manages the resources of multiple <em>Docker hosts</em> at scale. If the Swarm manager dies, you must create a new one and deal with an interruption of service.</p> <p>The <em>High Availability</em> feature allows a Docker Swarm to gracefully handle the failover of a manager instance. Using this feature, you can create a single <strong>primary manager</strong> instance and multiple <strong>replica</strong> instances.</p> <p>A primary manager is the main point of contact with the Docker Swarm cluster. You can also create and talk to replica instances that will act as backups. Requests issued on a replica are automatically proxied to the primary manager. If the primary manager fails, a replica takes away the lead. In this way, you always keep a point of contact with the cluster.</p> <h2 id=\"setup-primary-and-replicas\">Setup primary and replicas</h2> <p>This section explains how to set up Docker Swarm using multiple <strong>managers</strong>.</p> <h3 id=\"assumptions\">Assumptions</h3> <p>You need either a <code>Consul</code>, <code>etcd</code>, or <code>Zookeeper</code> cluster. This procedure is written assuming a <code>Consul</code> server running on address <code>192.168.42.10:8500</code>. All hosts will have a Docker Engine configured to listen on port 2375. We will be configuring the Managers to operate on port 4000. The sample Swarm configuration has three machines:</p> <ul> <li>\n<code>manager-1</code> on <code>192.168.42.200</code>\n</li> <li>\n<code>manager-2</code> on <code>192.168.42.201</code>\n</li> <li>\n<code>manager-3</code> on <code>192.168.42.202</code>\n</li> </ul> <h3 id=\"create-the-primary-manager\">Create the primary manager</h3> <p>You use the <code>swarm manage</code> command with the <code>--replication</code> and <code>--advertise</code> flags to create a primary manager.</p> <pre>  user@manager-1 $ swarm manage -H :4000 &lt;tls-config-flags&gt; --replication --advertise 192.168.42.200:4000 consul://192.168.42.10:8500/nodes\n  INFO[0000] Listening for HTTP addr=:4000 proto=tcp\n  INFO[0000] Cluster leadership acquired\n  INFO[0000] New leader elected: 192.168.42.200:4000\n  [...]\n</pre> <p>The <code>--replication</code> flag tells Swarm that the manager is part of a multi-manager configuration and that this primary manager competes with other manager instances for the primary role. The primary manager has the authority to manage cluster, replicate logs, and replicate events happening inside the cluster.</p> <p>The <code>--advertise</code> option specifies the primary manager address. Swarm uses this address to advertise to the cluster when the node is elected as the primary. As you see in the command’s output, the address you provided now appears to be the one of the elected Primary manager.</p> <h3 id=\"create-two-replicas\">Create two replicas</h3> <p>Now that you have a primary manager, you can create replicas.</p> <pre>user@manager-2 $ swarm manage -H :4000 &lt;tls-config-flags&gt; --replication --advertise 192.168.42.201:4000 consul://192.168.42.10:8500/nodes\nINFO[0000] Listening for HTTP                            addr=:4000 proto=tcp\nINFO[0000] Cluster leadership lost\nINFO[0000] New leader elected: 192.168.42.200:4000\n[...]\n</pre> <p>This command creates a replica manager on <code>192.168.42.201:4000</code> which is looking at <code>192.168.42.200:4000</code> as the primary manager.</p> <p>Create an additional, third <em>manager</em> instance:</p> <pre>user@manager-3 $ swarm manage -H :4000 &lt;tls-config-flags&gt; --replication --advertise 192.168.42.202:4000 consul://192.168.42.10:8500/nodes\nINFO[0000] Listening for HTTP                            addr=:4000 proto=tcp\nINFO[0000] Cluster leadership lost\nINFO[0000] New leader elected: 192.168.42.200:4000\n[...]\n</pre> <p>Once you have established your primary manager and the replicas, create <strong>Swarm agents</strong> as you normally would.</p> <h3 id=\"list-machines-in-the-cluster\">List machines in the cluster</h3> <p>Typing <code>docker info</code> should give you an output similar to the following:</p> <pre>user@my-machine $ export DOCKER_HOST=192.168.42.200:4000 # Points to manager-1\nuser@my-machine $ docker info\nContainers: 0\nImages: 25\nStorage Driver:\nRole: Primary  &lt;--------- manager-1 is the Primary manager\nPrimary: 192.168.42.200\nStrategy: spread\nFilters: affinity, health, constraint, port, dependency\nNodes: 3\n swarm-agent-0: 192.168.42.100:2375\n  └ Containers: 0\n  └ Reserved CPUs: 0 / 1\n  └ Reserved Memory: 0 B / 2.053 GiB\n  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs\n swarm-agent-1: 192.168.42.101:2375\n  └ Containers: 0\n  └ Reserved CPUs: 0 / 1\n  └ Reserved Memory: 0 B / 2.053 GiB\n  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs\n swarm-agent-2: 192.168.42.102:2375\n  └ Containers: 0\n  └ Reserved CPUs: 0 / 1\n  └ Reserved Memory: 0 B / 2.053 GiB\n  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs\nExecution Driver:\nKernel Version:\nOperating System:\nCPUs: 3\nTotal Memory: 6.158 GiB\nName:\nID:\nHttp Proxy:\nHttps Proxy:\nNo Proxy:\n</pre> <p>This information shows that <code>manager-1</code> is the current primary and supplies the address to use to contact this primary.</p> <h2 id=\"test-the-failover-mechanism\">Test the failover mechanism</h2> <p>To test the failover mechanism, you shut down the designated primary manager. Issue a <code>Ctrl-C</code> or <code>kill</code> the current primary manager (<code>manager-1</code>) to shut it down.</p> <h3 id=\"wait-for-automated-failover\">Wait for automated failover</h3> <p>After a short time, the other instances detect the failure and a replica takes the <em>lead</em> to become the primary manager.</p> <p>For example, look at <code>manager-2</code>’s logs:</p> <pre>user@manager-2 $ swarm manage -H :4000 &lt;tls-config-flags&gt; --replication --advertise 192.168.42.201:4000 consul://192.168.42.10:8500/nodes\nINFO[0000] Listening for HTTP                            addr=:4000 proto=tcp\nINFO[0000] Cluster leadership lost\nINFO[0000] New leader elected: 192.168.42.200:4000\nINFO[0038] New leader elected: 192.168.42.201:4000\nINFO[0038] Cluster leadership acquired               &lt;--- We have been elected as the new Primary Manager\n[...]\n</pre> <p>Because the primary manager, <code>manager-1</code>, failed right after it was elected, the replica with the address <code>192.168.42.201:4000</code>, <code>manager-2</code>, recognized the failure and attempted to take away the lead. Because <code>manager-2</code> was fast enough, the process was effectively elected as the primary manager. As a result, <code>manager-2</code> became the primary manager of the cluster.</p> <p>If we take a look at <code>manager-3</code> we should see those <code>logs</code>:</p> <pre>user@manager-3 $ swarm manage -H :4000 &lt;tls-config-flags&gt; --replication --advertise 192.168.42.202:4000 consul://192.168.42.10:8500/nodes\nINFO[0000] Listening for HTTP                            addr=:4000 proto=tcp\nINFO[0000] Cluster leadership lost\nINFO[0000] New leader elected: 192.168.42.200:4000\nINFO[0036] New leader elected: 192.168.42.201:4000   &lt;--- manager-2 sees the new Primary Manager\n[...]\n</pre> <p>At this point, we need to export the new <code>DOCKER_HOST</code> value.</p> <h3 id=\"switch-the-primary\">Switch the primary</h3> <p>To switch the <code>DOCKER_HOST</code> to use <code>manager-2</code> as the primary, you do the following:</p> <pre>user@my-machine $ export DOCKER_HOST=192.168.42.201:4000 # Points to manager-2\nuser@my-machine $ docker info\nContainers: 0\nImages: 25\nStorage Driver:\nRole: Replica  &lt;--------- manager-2 is a Replica\nPrimary: 192.168.42.200\nStrategy: spread\nFilters: affinity, health, constraint, port, dependency\nNodes: 3\n</pre> <p>You can use the <code>docker</code> command on any Docker Swarm primary manager or any replica.</p> <p>If you like, you can use custom mechanisms to always point <code>DOCKER_HOST</code> to the current primary manager. Then, you never lose contact with your Docker Swarm in the event of a failover.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/multi-manager-setup/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/multi-manager-setup/</a>\n  </p>\n</div>\n","swarm/discovery/index":"<h1 id=\"docker-swarm-discovery\">Docker Swarm Discovery</h1> <p>Docker Swarm comes with multiple discovery backends. You use a hosted discovery service with Docker Swarm. The service maintains a list of IPs in your cluster. This page describes the different types of hosted discovery available to you. These are:</p> <h2 id=\"using-a-distributed-key-value-store\">Using a distributed key/value store</h2> <p>The recommended way to do node discovery in Swarm is Docker’s libkv project. The libkv project is an abstraction layer over existing distributed key/value stores. As of this writing, the project supports:</p> <ul> <li>Consul 0.5.1 or higher</li> <li>Etcd 2.0 or higher</li> <li>ZooKeeper 3.4.5 or higher</li> </ul> <p>For details about libkv and a detailed technical overview of the supported backends, refer to the <a href=\"https://github.com/docker/libkv\">libkv project</a>.</p> <h3 id=\"using-a-hosted-discovery-key-store\">Using a hosted discovery key store</h3> <ol> <li>\n<p>On each node, start the Swarm agent.</p> <p>The node IP address doesn’t have to be public as long as the Swarm manager can access it. In a large cluster, the nodes joining swarm may trigger request spikes to discovery. For example, a large number of nodes are added by a script, or recovered from a network partition. This may result in discovery failure. You can use <code>--delay</code> option to specify a delay limit. Swarm join will add a random delay less than this limit to reduce pressure to discovery.</p> <p><strong>Etcd</strong>:</p> <pre>swarm join --advertise=&lt;node_ip:2375&gt; etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt;\n</pre> <p><strong>Consul</strong>:</p> <pre>swarm join --advertise=&lt;node_ip:2375&gt; consul://&lt;consul_addr&gt;/&lt;optional path prefix&gt;\n</pre> <p><strong>ZooKeeper</strong>:</p> <pre>swarm join --advertise=&lt;node_ip:2375&gt; zk://&lt;zookeeper_addr1&gt;,&lt;zookeeper_addr2&gt;/&lt;optional path prefix&gt;\n</pre>\n</li> <li>\n<p>Start the Swarm manager on any machine or your laptop.</p> <p><strong>Etcd</strong>:</p> <pre>swarm manage -H tcp://&lt;swarm_ip:swarm_port&gt; etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt;\n</pre> <p><strong>Consul</strong>:</p> <pre>swarm manage -H tcp://&lt;swarm_ip:swarm_port&gt; consul://&lt;consul_addr&gt;/&lt;optional path prefix&gt;\n</pre> <p><strong>ZooKeeper</strong>:</p> <pre>swarm manage -H tcp://&lt;swarm_ip:swarm_port&gt; zk://&lt;zookeeper_addr1&gt;,&lt;zookeeper_addr2&gt;/&lt;optional path prefix&gt;\n</pre>\n</li> <li>\n<p>Use the regular Docker commands.</p> <pre>docker -H tcp://&lt;swarm_ip:swarm_port&gt; info\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; run ...\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; ps\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; logs ...\n...\n</pre>\n</li> <li>\n<p>Try listing the nodes in your cluster.</p> <p><strong>Etcd</strong>:</p> <pre>swarm list etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt;\n&lt;node_ip:2375&gt;\n</pre> <p><strong>Consul</strong>:</p> <pre>swarm list consul://&lt;consul_addr&gt;/&lt;optional path prefix&gt;\n&lt;node_ip:2375&gt;\n</pre> <p><strong>ZooKeeper</strong>:</p> <pre>swarm list zk://&lt;zookeeper_addr1&gt;,&lt;zookeeper_addr2&gt;/&lt;optional path prefix&gt;\n&lt;node_ip:2375&gt;\n</pre>\n</li> </ol> <h3 id=\"use-tls-with-distributed-key-value-discovery\">Use TLS with distributed key/value discovery</h3> <p>You can securely talk to the distributed k/v store using TLS. To connect securely to the store, you must generate the certificates for a node when you <code>join</code> it to the swarm. You can only use with Consul and Etcd. The following example illustrates this with Consul:</p> <pre>swarm join \\\n    --advertise=&lt;node_ip:2375&gt; \\\n    --discovery-opt kv.cacertfile=/path/to/mycacert.pem \\\n    --discovery-opt kv.certfile=/path/to/mycert.pem \\\n    --discovery-opt kv.keyfile=/path/to/mykey.pem \\\n    consul://&lt;consul_addr&gt;/&lt;optional path prefix&gt;\n</pre> <p>This works the same way for the Swarm <code>manage</code> and <code>list</code> commands.</p> <h2 id=\"a-static-file-or-list-of-nodes\">A static file or list of nodes</h2> <blockquote> <p><strong>Note</strong>*: This discovery method is incompatible with replicating Swarm managers. If you require replication, you should use a hosted discovery key store.</p> </blockquote> <p>You can use a static file or list of nodes for your discovery backend. The file must be stored on a host that is accessible from the Swarm manager. You can also pass a node list as an option when you start Swarm.</p> <p>Both the static file and the <code>nodes</code> option support an IP address ranges. To specify a range supply a pattern, for example, <code>10.0.0.[10:200]</code> refers to nodes starting from <code>10.0.0.10</code> to <code>10.0.0.200</code>. For example for the <code>file</code> discovery method.</p> <pre>    $ echo \"10.0.0.[11:100]:2375\"   &gt;&gt; /tmp/my_cluster\n    $ echo \"10.0.1.[15:20]:2375\"    &gt;&gt; /tmp/my_cluster\n    $ echo \"192.168.1.2:[2:20]375\"  &gt;&gt; /tmp/my_cluster\n</pre> <p>Or with node discovery:</p> <pre>    swarm manage -H &lt;swarm_ip:swarm_port&gt; \"nodes://10.0.0.[10:200]:2375,10.0.1.[2:250]:2375\"\n</pre> <h3 id=\"to-create-a-file\">To create a file</h3> <ol> <li>\n<p>Edit the file and add line for each of your nodes.</p> <pre>echo &lt;node_ip1:2375&gt; &gt;&gt; /opt/my_cluster\necho &lt;node_ip2:2375&gt; &gt;&gt; /opt/my_cluster\necho &lt;node_ip3:2375&gt; &gt;&gt; /opt/my_cluster\n</pre> <p>This example creates a file named <code>/tmp/my_cluster</code>. You can use any name you like.</p>\n</li> <li>\n<p>Start the Swarm manager on any machine.</p> <pre>swarm manage -H tcp://&lt;swarm_ip:swarm_port&gt; file:///tmp/my_cluster\n</pre>\n</li> <li>\n<p>Use the regular Docker commands.</p> <pre>docker -H tcp://&lt;swarm_ip:swarm_port&gt; info\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; run ...\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; ps\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; logs ...\n...\n</pre>\n</li> <li>\n<p>List the nodes in your cluster.</p> <pre>$ swarm list file:///tmp/my_cluster\n&lt;node_ip1:2375&gt;\n&lt;node_ip2:2375&gt;\n&lt;node_ip3:2375&gt;\n</pre>\n</li> </ol> <h3 id=\"to-use-a-node-list\">To use a node list</h3> <ol> <li>\n<p>Start the manager on any machine or your laptop.</p> <pre>swarm manage -H &lt;swarm_ip:swarm_port&gt; nodes://&lt;node_ip1:2375&gt;,&lt;node_ip2:2375&gt;\n</pre> <p>or</p> <pre>swarm manage -H &lt;swarm_ip:swarm_port&gt; &lt;node_ip1:2375&gt;,&lt;node_ip2:2375&gt;\n</pre>\n</li> <li>\n<p>Use the regular Docker commands.</p> <pre>docker -H &lt;swarm_ip:swarm_port&gt; info\ndocker -H &lt;swarm_ip:swarm_port&gt; run ...\ndocker -H &lt;swarm_ip:swarm_port&gt; ps\ndocker -H &lt;swarm_ip:swarm_port&gt; logs ...\n</pre>\n</li> <li>\n<p>List the nodes in your cluster.</p> <pre>$ swarm list file:///tmp/my_cluster\n&lt;node_ip1:2375&gt;\n&lt;node_ip2:2375&gt;\n&lt;node_ip3:2375&gt;\n</pre>\n</li> </ol> <h2 id=\"docker-hub-as-a-hosted-discovery-service\">Docker Hub as a hosted discovery service</h2> <blockquote> <p><strong>Warning</strong>: The Docker Hub Hosted Discovery Service <strong>is not recommended</strong> for production use. It’s intended to be used for testing/development. See the discovery backends for production use.</p> </blockquote> <p>This example uses the hosted discovery service on Docker Hub. Using Docker Hub’s hosted discovery service requires that each node in the swarm is connected to the public internet. To create your cluster:</p> <ol> <li>\n<p>Create a cluster.</p> <pre>$ swarm create\n6856663cdefdec325839a4b7e1de38e8 # &lt;- this is your unique &lt;cluster_id&gt;\n</pre>\n</li> <li>\n<p>Create each node and join them to the cluster.</p> <p>On each of your nodes, start the swarm agent. The <node_ip> doesn’t have to be public (eg. 192.168.0.X) but the the Swarm manager must be able to access it.</node_ip></p> <pre>$ swarm join --advertise=&lt;node_ip:2375&gt; token://&lt;cluster_id&gt;\n</pre>\n</li> <li>\n<p>Start the Swarm manager.</p> <p>This can be on any machine or even your laptop.</p> <pre>$ swarm manage -H tcp://&lt;swarm_ip:swarm_port&gt; token://&lt;cluster_id&gt;\n</pre>\n</li> <li>\n<p>Use regular Docker commands to interact with your cluster.</p> <pre>docker -H tcp://&lt;swarm_ip:swarm_port&gt; info\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; run ...\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; ps\ndocker -H tcp://&lt;swarm_ip:swarm_port&gt; logs ...\n...\n</pre>\n</li> <li>\n<p>List the nodes in your cluster.</p> <pre>swarm list token://&lt;cluster_id&gt;\n&lt;node_ip:2375&gt;\n</pre>\n</li> </ol> <h2 id=\"contribute-a-new-discovery-backend\">Contribute a new discovery backend</h2> <p>You can contribute a new discovery backend to Swarm. For information on how to do this, see <a href=\"https://github.com/docker/docker/tree/master/pkg/discovery\"> github.com/docker/docker/pkg/discovery</a>.</p> <h2 id=\"docker-swarm-documentation-index\">Docker Swarm documentation index</h2> <ul> <li><a href=\"../index\">Docker Swarm overview</a></li> <li><a href=\"../scheduler/strategy/index\">Scheduler strategies</a></li> <li><a href=\"../scheduler/filter/index\">Scheduler filters</a></li> <li><a href=\"../swarm-api/index\">Swarm API</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/discovery/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/discovery/</a>\n  </p>\n</div>\n","swarm/networking/index":"<h1 id=\"swarm-and-container-networks\">Swarm and container networks</h1> <p>Docker Swarm is fully compatible with Docker’s networking features. This includes the multi-host networking feature which allows creation of custom container networks that span multiple Docker hosts.</p> <p>Before using Swarm with a custom network, read through the conceptual information in <a href=\"https://docs.docker.com/engine/userguide/networking/dockernetworks/\">Docker container networking</a>. You should also have walked through the <a href=\"https://docs.docker.com/engine/userguide/networking/get-started-overlay/\">Get started with multi-host networking</a> example.</p> <h2 id=\"create-a-custom-network-in-a-swarm-cluster\">Create a custom network in a Swarm cluster</h2> <p>Multi-host networks require a key-value store. The key-value store holds information about the network state which includes discovery, networks, endpoints, IP addresses, and more. Through the Docker’s libkv project, Docker supports Consul, Etcd, and ZooKeeper key-value store backends. For details about the supported backends, refer to the <a href=\"https://github.com/docker/libkv\">libkv project</a>.</p> <p>To create a custom network, you must choose a key-value store backend and implement it on your network. Then, you configure the Docker Engine daemon to use this store. Two required parameters, <code>--cluster-store</code> and <code>--cluster-advertise</code>, refer to your key-value store server.</p> <p>Once you’ve configured and restarted the daemon on each Swarm node, you are ready to create a network.</p> <h2 id=\"list-networks\">List networks</h2> <p>This example assumes there are two nodes <code>node-0</code> and <code>node-1</code> in the cluster. From a Swarm node, list the networks:</p> <pre>$ docker network ls\nNETWORK ID          NAME                   DRIVER\n3dd50db9706d        node-0/host            host\n09138343e80e        node-0/bridge          bridge\n8834dbd552e5        node-0/none            null\n45782acfe427        node-1/host            host\n8926accb25fd        node-1/bridge          bridge\n6382abccd23d        node-1/none            null\n</pre> <p>As you can see, each network name is prefixed by the node name.</p> <h2 id=\"create-a-network\">Create a network</h2> <p>By default, Swarm is using the <code>overlay</code> network driver, a global-scope network driver. A global-scope network driver creates a network across an entire Swarm cluster. When you create an <code>overlay</code> network under Swarm, you can omit the <code>-d</code> option:</p> <pre>$ docker network create swarm_network\n42131321acab3233ba342443Ba4312\n$ docker network ls\nNETWORK ID          NAME                   DRIVER\n3dd50db9706d        node-0/host            host\n09138343e80e        node-0/bridge          bridge\n8834dbd552e5        node-0/none            null\n42131321acab        node-0/swarm_network   overlay\n45782acfe427        node-1/host            host\n8926accb25fd        node-1/bridge          bridge\n6382abccd23d        node-1/none            null\n42131321acab        node-1/swarm_network   overlay\n</pre> <p>As you can see here, both the <code>node-0/swarm_network</code> and the <code>node-1/swarm_network</code> have the same ID. This is because when you create a network on the cluster, it is accessible from all the nodes.</p> <p>To create a local scope network (for example with the <code>bridge</code> network driver) you should use <code>&lt;node&gt;/&lt;name&gt;</code> otherwise your network is created on a random node.</p> <pre>$ docker network create node-0/bridge2 -b bridge\n921817fefea521673217123abab223\n$ docker network create node-1/bridge2 -b bridge\n5262bbfe5616fef6627771289aacc2\n$ docker network ls\nNETWORK ID          NAME                   DRIVER\n3dd50db9706d        node-0/host            host\n09138343e80e        node-0/bridge          bridge\n8834dbd552e5        node-0/none            null\n42131321acab        node-0/swarm_network   overlay\n921817fefea5        node-0/bridge2         bridge\n45782acfe427        node-1/host            host\n8926accb25fd        node-1/bridge          bridge\n6382abccd23d        node-1/none            null\n42131321acab        node-1/swarm_network   overlay\n5262bbfe5616        node-1/bridge2         bridge\n</pre> <h2 id=\"remove-a-network\">Remove a network</h2> <p>To remove a network you can use its ID or its name. If two different networks have the same name, include the <code>&lt;node&gt;</code> value:</p> <pre>$ docker network rm swarm_network\n42131321acab3233ba342443Ba4312\n$ docker network rm node-0/bridge2\n921817fefea521673217123abab223\n$ docker network ls\nNETWORK ID          NAME                   DRIVER\n3dd50db9706d        node-0/host            host\n09138343e80e        node-0/bridge          bridge\n8834dbd552e5        node-0/none            null\n45782acfe427        node-1/host            host\n8926accb25fd        node-1/bridge          bridge\n6382abccd23d        node-1/none            null\n5262bbfe5616        node-1/bridge2         bridge\n</pre> <p>The <code>swarm_network</code> was removed from every node. The <code>bridge2</code> was removed only from <code>node-0</code>.</p> <h2 id=\"docker-swarm-documentation-index\">Docker Swarm documentation index</h2> <ul> <li><a href=\"../index\">Docker Swarm overview</a></li> <li><a href=\"../scheduler/strategy/index\">Scheduler strategies</a></li> <li><a href=\"../scheduler/filter/index\">Scheduler filters</a></li> <li><a href=\"../swarm-api/index\">Swarm API</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/networking/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/networking/</a>\n  </p>\n</div>\n","swarm/provision-with-machine/index":"<h1 id=\"provision-a-swarm-cluster-with-docker-machine\">Provision a Swarm cluster with Docker Machine</h1> <p>You can use Docker Machine to provision a Docker Swarm cluster. Machine is the Docker provisioning tool. Machine provisions the hosts, installs Docker Engine on them, and then configures the Docker CLI client. With Machine’s Swarm options, you can also quickly configure a Swarm cluster as part of this provisioning.</p> <p>This page explains the commands you need to provision a basic Swarm cluster on a local Mac or Windows computer using Machine. Once you understand the process, you can use it to setup a Swarm cluster on a cloud provider, or inside your company’s data center.</p> <p>If this is the first time you are creating a Swarm cluster, you should first learn about Swarm and its requirements by <a href=\"../install-w-machine/index\">installing a Swarm for evaluation</a> or <a href=\"../install-manual/index\">installing a Swarm for production</a>. If this is the first time you have used Machine, you should take some time to <a href=\"https://docs.docker.com/machine\">understand Machine before continuing</a>.</p> <h2 id=\"what-you-need\">What you need</h2> <p>If you are using Mac OS X or Windows and have installed with Docker Toolbox, you should already have Machine installed. If you need to install, see the instructions for <a href=\"https://docs.docker.com/engine/installation/mac/\">Mac OS X</a> or <a href=\"https://docs.docker.com/engine/installation/mac/\">Windows</a>.</p> <p>Machine supports installing on AWS, Digital Ocean, Google Cloud Platform, IBM Softlayer, Microsoft Azure and Hyper-V, OpenStack, Rackspace, VirtualBox, VMware Fusion®, vCloud® Air<sup>TM</sup> and vSphere®. In this example, you’ll use VirtualBox to run several VMs based on the <code>boot2docker.iso</code> image. This image is a small-footprint Linux distribution for running Engine.</p> <p>The Toolbox installation gives you VirtualBox and the <code>boot2docker.iso</code> image you need. It also gives you the ability provision on all the systems Machine supports.</p> <p><strong>Note</strong>:These examples assume you are using Mac OS X or Windows, if you like you can also <a href=\"http://docs.docker.com/machine/install-machine\">install Docker Machine directly on a Linux system</a>.</p> <h2 id=\"provision-a-host-to-generate-a-swarm-token\">Provision a host to generate a Swarm token</h2> <p>Before you can configure a Swarm, you start by provisioning a host with Engine. Open a terminal on the host where you installed Machine. Then, to provision a host called <code>local</code>, do the following:</p> <pre>docker-machine create -d virtualbox local\n</pre> <p>This examples uses VirtualBox but it could easily be DigitalOcean or a host on your data center. The <code>local</code> value is the host name. Once you create it, configure your terminal’s shell environment to interact with the <code>local</code> host.</p> <pre>eval \"$(docker-machine env local)\"\n</pre> <p>Each Swarm host has a token installed into its Engine configuration. The token allows the Swarm discovery backend to recognize a node as belonging to a particular Swarm cluster. Create the token for your cluster by running the <code>swarm</code> image:</p> <pre>docker run swarm create\nUnable to find image 'swarm' locally\n1.1.0-rc2: Pulling from library/swarm\n892cb307750a: Pull complete\nfe3c9860e6d5: Pull complete\ncc01ef3f1fbc: Pull complete\nb7e14a9c9c72: Pull complete\n3ec746117013: Pull complete\n703cb7acfce6: Pull complete\nd4f6bb678158: Pull complete\n2ad500e1bf96: Pull complete\nDigest: sha256:f02993cd1afd86b399f35dc7ca0240969e971c92b0232a8839cf17a37d6e7009\nStatus: Downloaded newer image for swarm\n0de84fa62a1d9e9cc2156111f63ac31f\n</pre> <p>The output of the <code>swarm create</code> command is a cluster token. Copy the token to a safe place you will remember. Once you have the token, you can provision the Swarm nodes and join them to the cluster_id. The rest of this documentation, refers to this token as the <code>SWARM_CLUSTER_TOKEN</code>.</p> <h2 id=\"provision-swarm-nodes\">Provision Swarm nodes</h2> <p>All Swarm nodes in a cluster must have Engine installed. With Machine and the <code>SWARM_CLUSTER_TOKEN</code> you can provision a host with Engine and configure it as a Swarm node with one Machine command. To create a Swarm manager node on a new VM called <code>swarm-manager</code>, you do the following:</p> <pre>docker-machine create \\\n    -d virtualbox \\\n    --swarm \\\n    --swarm-master \\\n    --swarm-discovery token://SWARM_CLUSTER_TOKEN \\\n    swarm-manager\n</pre> <p>Then, provision an additional node. You must supply the <code>SWARM_CLUSTER_TOKEN</code> and a unique name for each host node, <code>HOST_NODE_NAME</code>.</p> <pre>docker-machine create \\\n    -d virtualbox \\\n    --swarm \\\n    --swarm-discovery token://SWARM_CLUSTER_TOKEN \\\n    HOST_NODE_NAME\n</pre> <p>For example, you might use <code>node-01</code> as the <code>HOST_NODE_NAME</code> in the previous example.</p> <blockquote> <p><strong>Note</strong>: These command rely on Docker Swarm’s hosted discovery service, Docker Hub. If Docker Hub or your network is having issues, these commands may fail. Check the <a href=\"http://status.docker.com/\">Docker Hub status page</a> for service availability. If the problem Docker Hub, you can wait for it to recover or configure other types of discovery backends.</p> </blockquote> <h2 id=\"connect-node-environments-with-machine\">Connect node environments with Machine</h2> <p>If you are connecting to typical host environment with Machine, you use the <code>env</code> subcommand, like this:</p> <pre>eval \"$(docker-machine env local)\"\n</pre> <p>Docker Machine provides a special <code>--swarm</code> flag with its <code>env</code> command to connect to Swarm nodes.</p> <pre>docker-machine env --swarm HOST_NODE_NAME\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://192.168.99.101:3376\"\nexport DOCKER_CERT_PATH=\"/Users/mary/.docker/machine/machines/swarm-manager\"\nexport DOCKER_MACHINE_NAME=\"swarm-manager\"\n# Run this command to configure your shell:\n# eval $(docker-machine env --swarm HOST_NODE_NAME)\n</pre> <p>To set your SHELL connect to a Swarm node called <code>swarm-manager</code>, you would do this:</p> <pre>eval \"$(docker-machine env --swarm swarm-manager)\"\n</pre> <p>Now, you can use the Docker CLI to query and interact with your cluster.</p> <pre>docker info\nContainers: 2\nImages: 1\nRole: primary\nStrategy: spread\nFilters: health, port, dependency, affinity, constraint\nNodes: 1\n swarm-manager: 192.168.99.101:2376\n  └ Status: Healthy\n  └ Containers: 2\n  └ Reserved CPUs: 0 / 1\n  └ Reserved Memory: 0 B / 1.021 GiB\n  └ Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs\nCPUs: 1\nTotal Memory: 1.021 GiB\nName: swarm-manager\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../install-w-machine/index\">Evaluate Swarm in a sandbox</a></li> <li><a href=\"../install-manual/index\">Build a Swarm cluster for production</a></li> <li><a href=\"../discovery/index\">Swarm Discovery</a></li> <li>\n<a href=\"https://docs.docker.com/machine\">Docker Machine</a> documentation</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/provision-with-machine/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/provision-with-machine/</a>\n  </p>\n</div>\n","swarm/scheduler/rescheduling/index":"<h1 id=\"swarm-rescheduling\">Swarm Rescheduling</h1> <p>You can set recheduling policies with Docker Swarm. A rescheduling policy determines what the Swarm scheduler does for containers when the nodes they are running on fail.</p> <h2 id=\"rescheduling-policies\">Rescheduling policies</h2> <p>You set the reschedule policy when you start a container. You can do this with the <code>reschedule</code> environment variable or the <code>com.docker.swarm.reschedule-policies</code> label. If you don’t specify a policy, the default rescheduling policy is <code>off</code> which means that Swarm does not restart a container when a node fails.</p> <p>To set the <code>on-node-failure</code> policy with a <code>reschedule</code> environment variable:</p> <pre>$ docker run -d -e reschedule:on-node-failure redis\n</pre> <p>To set the same policy with a <code>com.docker.swarm.reschedule-policies</code> label:</p> <pre>$ docker run -d -l 'com.docker.swarm.reschedule-policies=[\"on-node-failure\"]' redis\n</pre> <h2 id=\"review-reschedule-logs\">Review reschedule logs</h2> <p>You can use the <code>docker logs</code> command to review the rescheduled container actions. To do this, use the following command syntax:</p> <pre>docker logs SWARM_MANAGER_CONTAINER_ID\n</pre> <p>When a container is successfully rescheduled, it generates a message similar to the following:</p> <pre>Rescheduled container 2536adb23 from node-1 to node-2 as 2362901cb213da321\nContainer 2536adb23 was running, starting container 2362901cb213da321\n</pre> <p>If for some reason, the new container fails to start on the new node, the log contains:</p> <pre>Failed to start rescheduled container 2362901cb213da321\n</pre> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"https://docs.docker.com/engine/userguide/labels-custom-metadata/\">Apply custom metadata</a></li> <li><a href=\"https://docs.docker.com/engine/reference/run/#env-environment-variables\">Environment variables with run</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/scheduler/rescheduling/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/scheduler/rescheduling/</a>\n  </p>\n</div>\n","swarm/scheduler/strategy/index":"<h1 id=\"docker-swarm-strategies\">Docker Swarm strategies</h1> <p>The Docker Swarm scheduler features multiple strategies for ranking nodes. The strategy you choose determines how Swarm computes ranking. When you run a new container, Swarm chooses to place it on the node with the highest computed ranking for your chosen strategy.</p> <p>To choose a ranking strategy, pass the <code>--strategy</code> flag and a strategy value to the <code>swarm manage</code> command. Swarm currently supports these values:</p> <ul> <li><code>spread</code></li> <li><code>binpack</code></li> <li><code>random</code></li> </ul> <p>The <code>spread</code> and <code>binpack</code> strategies compute rank according to a node’s available CPU, its RAM, and the number of containers it has. The <code>random</code> strategy uses no computation. It selects a node at random and is primarily intended for debugging.</p> <p>Your goal in choosing a strategy is to best optimize your cluster according to your company’s needs.</p> <p>Under the <code>spread</code> strategy, Swarm optimizes for the node with the least number of containers. The <code>binpack</code> strategy causes Swarm to optimize for the node which is most packed. Note that a container occupies resource during its life cycle, including <code>exited</code> state. Users should be aware of this condition to schedule containers. For example, <code>spread</code> strategy only checks number of containers disregarding their states. A node with no active containers but high number of stopped containers may not be selected, defeating the purpose of load sharing. User could either remove stopped containers, or start stopped containers to achieve load spreading. The <code>random</code> strategy, like it sounds, chooses nodes at random regardless of their available CPU or RAM.</p> <p>Using the <code>spread</code> strategy results in containers spread thinly over many machines. The advantage of this strategy is that if a node goes down you only lose a few containers.</p> <p>The <code>binpack</code> strategy avoids fragmentation because it leaves room for bigger containers on unused machines. The strategic advantage of <code>binpack</code> is that you use fewer machines as Swarm tries to pack as many containers as it can on a node.</p> <p>If you do not specify a <code>--strategy</code> Swarm uses <code>spread</code> by default.</p> <h2 id=\"spread-strategy-example\">Spread strategy example</h2> <p>In this example, your cluster is using the <code>spread</code> strategy which optimizes for nodes that have the fewest containers. In this cluster, both <code>node-1</code> and <code>node-2</code> have 2G of RAM, 2 CPUs, and neither node is running a container. Under this strategy <code>node-1</code> and <code>node-2</code> have the same ranking.</p> <p>When you run a new container, the system chooses <code>node-1</code> at random from the Swarm cluster of two equally ranked nodes:</p> <pre>  $ docker tcp://&lt;manager_ip:manager_port&gt; run -d -P -m 1G --name db mysql\n  f8b693db9cd6\n\n  $ docker tcp://&lt;manager_ip:manager_port&gt; ps\n  CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n  f8b693db9cd6        mysql:latest        \"mysqld\"            Less than a second ago   running             192.168.0.42:49178-&gt;3306/tcp    node-1/db\n</pre> <p>Now, we start another container and ask for 1G of RAM again.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -P -m 1G --name frontend nginx\n963841b138d8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n963841b138d8        nginx:latest        \"nginx\"             Less than a second ago   running             192.168.0.42:49177-&gt;80/tcp      node-2/frontend\nf8b693db9cd6        mysql:latest        \"mysqld\"            Up About a minute        running             192.168.0.42:49178-&gt;3306/tcp    node-1/db\n</pre> <p>The container <code>frontend</code> was started on <code>node-2</code> because it was the node the least loaded already. If two nodes have the same amount of available RAM and CPUs, the <code>spread</code> strategy prefers the node with least containers.</p> <h2 id=\"binpack-strategy-example\">BinPack strategy example</h2> <p>In this example, let’s says that both <code>node-1</code> and <code>node-2</code> have 2G of RAM and neither is running a container. Again, the nodes are equal. When you run a new container, the system chooses <code>node-1</code> at random from the cluster:</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -P -m 1G --name db mysql\nf8b693db9cd6\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\nf8b693db9cd6        mysql:latest        \"mysqld\"            Less than a second ago   running             192.168.0.42:49178-&gt;3306/tcp    node-1/db\n</pre> <p>Now, you start another container, asking for 1G of RAM again.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -P -m 1G --name frontend nginx\n963841b138d8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n963841b138d8        nginx:latest        \"nginx\"             Less than a second ago   running             192.168.0.42:49177-&gt;80/tcp      node-1/frontend\nf8b693db9cd6        mysql:latest        \"mysqld\"            Up About a minute        running             192.168.0.42:49178-&gt;3306/tcp    node-1/db\n</pre> <p>The system starts the new <code>frontend</code> container on <code>node-1</code> because it was the node the most packed already. This allows us to start a container requiring 2G of RAM on <code>node-2</code>.</p> <p>If two nodes have the same amount of available RAM and CPUs, the <code>binpack</code> strategy prefers the node with most containers.</p> <h2 id=\"docker-swarm-documentation-index\">Docker Swarm documentation index</h2> <ul> <li><a href=\"../../index\">Docker Swarm overview</a></li> <li><a href=\"../../discovery/index\">Discovery options</a></li> <li><a href=\"../filter/index\">Scheduler filters</a></li> <li><a href=\"../../swarm-api/index\">Swarm API</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/scheduler/strategy/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/scheduler/strategy/</a>\n  </p>\n</div>\n","swarm/scheduler/filter/index":"<h1 id=\"swarm-filters\">Swarm filters</h1> <p>Filters tell Docker Swarm scheduler which nodes to use when creating and running a container.</p> <h2 id=\"configure-the-available-filters\">Configure the available filters</h2> <p>Filters are divided into two categories, node filters and container configuration filters. Node filters operate on characteristics of the Docker host or on the configuration of the Docker daemon. Container configuration filters operate on characteristics of containers, or on the availability of images on a host.</p> <p>Each filter has a name that identifies it. The node filters are:</p> <ul> <li><code>constraint</code></li> <li><code>health</code></li> <li><code>containerslots</code></li> </ul> <p>The container configuration filters are:</p> <ul> <li><code>affinity</code></li> <li><code>dependency</code></li> <li><code>port</code></li> </ul> <p>When you start a Swarm manager with the <code>swarm manage</code> command, all the filters are enabled. If you want to limit the filters available to your Swarm, specify a subset of filters by passing the <code>--filter</code> flag and the name:</p> <pre>$ swarm manage --filter=health --filter=dependency\n</pre> <blockquote> <p><strong>Note</strong>: Container configuration filters match all containers, including stopped containers, when applying the filter. To release a node used by a container, you must remove the container from the node.</p> </blockquote> <h2 id=\"node-filters\">Node filters</h2> <p>When creating a container or building an image, you use a <code>constraint</code> or <code>health</code> filter to select a subset of nodes to consider for scheduling. If a node in Swarm cluster has a label with key <code>containerslots</code> and a number-value, Swarm will not launch more containers than the given number.</p> <h3 id=\"use-a-constraint-filter\">Use a constraint filter</h3> <p>Node constraints can refer to Docker’s default tags or to custom labels. Default tags are sourced from <code>docker info</code>. Often, they relate to properties of the Docker host. Currently, the default tags include:</p> <ul> <li>\n<code>node</code> to refer to the node by ID or name</li> <li><code>storagedriver</code></li> <li><code>executiondriver</code></li> <li><code>kernelversion</code></li> <li><code>operatingsystem</code></li> </ul> <p>Custom node labels you apply when you start the <code>docker daemon</code>, for example:</p> <pre>$ docker daemon --label com.example.environment=\"production\" --label\ncom.example.storage=\"ssd\"\n</pre> <p>Then, when you start a container on the cluster, you can set constraints using these default tags or custom labels. The Swarm scheduler looks for matching node on the cluster and starts the container there. This approach has several practical applications:</p> <ul> <li>Schedule based on specific host properties, for example,<code>storage=ssd</code> schedules containers on specific hardware.</li> <li>Force containers to run in a given location, for example region=us-east`.</li> <li>Create logical cluster partitions by splitting a cluster into sub-clusters with different properties, for example <code>environment=production</code>.</li> </ul> <h4 id=\"example-node-constraints\">Example node constraints</h4> <p>To specify custom label for a node, pass a list of <code>--label</code> options at <code>docker</code> startup time. For instance, to start <code>node-1</code> with the <code>storage=ssd</code> label:</p> <pre>$ docker daemon --label storage=ssd\n$ swarm join --advertise=192.168.0.42:2375 token://XXXXXXXXXXXXXXXXXX\n</pre> <p>You might start a different <code>node-2</code> with <code>storage=disk</code>:</p> <pre>$ docker daemon --label storage=disk\n$ swarm join --advertise=192.168.0.43:2375 token://XXXXXXXXXXXXXXXXXX\n</pre> <p>Once the nodes are joined to a cluster, the Swarm manager pulls their respective tags. Moving forward, the manager takes the tags into account when scheduling new containers.</p> <p>Continuing the previous example, assuming your cluster with <code>node-1</code> and <code>node-2</code>, you can run a MySQL server container on the cluster. When you run the container, you can use a <code>constraint</code> to ensure the database gets good I/O performance. You do this by filtering for nodes with flash drives:</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt;  run -d -P -e constraint:storage==ssd --name db mysql\nf8b693db9cd6\n\n$ docker tcp://&lt;manager_ip:manager_port&gt;  ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\nf8b693db9cd6        mysql:latest        \"mysqld\"            Less than a second ago   running             192.168.0.42:49178-&gt;3306/tcp    node-1/db\n</pre> <p>In this example, the manager selected all nodes that met the <code>storage=ssd</code> constraint and applied resource management on top of them. Only <code>node-1</code> was selected because it’s the only host running flash.</p> <p>Suppose you want to run an Nginx frontend in a cluster. In this case, you wouldn’t want flash drives because the frontend mostly writes logs to disk.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -P -e constraint:storage==disk --name frontend nginx\n963841b138d8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n963841b138d8        nginx:latest        \"nginx\"             Less than a second ago   running             192.168.0.43:49177-&gt;80/tcp      node-2/frontend\nf8b693db9cd6        mysql:latest        \"mysqld\"            Up About a minute        running             192.168.0.42:49178-&gt;3306/tcp    node-1/db\n</pre> <p>The scheduler selected <code>node-2</code> since it was started with the <code>storage=disk</code> label.</p> <p>Finally, build args can be used to apply node constraints to a <code>docker build</code>. Again, you’ll avoid flash drives.</p> <pre>$ mkdir sinatra\n$ cd sinatra\n$ echo \"FROM ubuntu:14.04\" &gt; Dockerfile\n$ echo \"MAINTAINER Kate Smith &lt;ksmith@example.com&gt;\" &gt;&gt; Dockerfile\n$ echo \"RUN apt-get update &amp;&amp; apt-get install -y ruby ruby-dev\" &gt;&gt; Dockerfile\n$ echo \"RUN gem install sinatra\" &gt;&gt; Dockerfile\n$ docker build --build-arg=constraint:storage==disk -t ouruser/sinatra:v2 .\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM ubuntu:14.04\n ---&gt; a5a467fddcb8\nStep 2 : MAINTAINER Kate Smith &lt;ksmith@example.com&gt;\n ---&gt; Running in 49e97019dcb8\n ---&gt; de8670dcf80e\nRemoving intermediate container 49e97019dcb8\nStep 3 : RUN apt-get update &amp;&amp; apt-get install -y ruby ruby-dev\n ---&gt; Running in 26c9fbc55aeb\n ---&gt; 30681ef95fff\nRemoving intermediate container 26c9fbc55aeb\nStep 4 : RUN gem install sinatra\n ---&gt; Running in 68671d4a17b0\n ---&gt; cd70495a1514\nRemoving intermediate container 68671d4a17b0\nSuccessfully built cd70495a1514\n\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\ndockerswarm/swarm   manager             8c2c56438951        2 days ago          795.7 MB\nouruser/sinatra     v2                  cd70495a1514        35 seconds ago      318.7 MB\nubuntu              14.04               a5a467fddcb8        11 days ago         187.9 MB\n</pre> <h3 id=\"use-the-health-filter\">Use the health filter</h3> <p>The node <code>health</code> filter prevents the scheduler form running containers on unhealthy nodes. A node is considered unhealthy if the node is down or it can’t communicate with the cluster store.</p> <h3 id=\"use-the-containerslots-filter\">Use the containerslots filter</h3> <p>You may give your Docker nodes the containerslots label</p> <pre>$ docker daemon --label containerslots=3\n</pre> <p>Swarm will run up to 3 containers at this node, if all nodes are “full”, an error is thrown indicating no suitable node can be found. If the value is not castable to an integer number or is not present, there will be no limit on container number.</p> <h2 id=\"container-filters\">Container filters</h2> <p>When creating a container, you can use three types of container filters:</p> <ul> <li><a href=\"#use-an-affinity-filter\"><code>affinity</code></a></li> <li><a href=\"#use-a-depedency-filter\"><code>dependency</code></a></li> <li><a href=\"#use-a-port-filter\"><code>port</code></a></li> </ul> <h3 id=\"use-an-affinity-filter\">Use an affinity filter</h3> <p>Use an <code>affinity</code> filter to create “attractions” between containers. For example, you can run a container and instruct Swarm to schedule it next to another container based on these affinities:</p> <ul> <li>container name or id</li> <li>an image on the host</li> <li>a custom label applied to the container</li> </ul> <p>These affinities ensure that containers run on the same network node — without you having to know what each node is running.</p> <h4 id=\"example-name-affinity\">Example name affinity</h4> <p>You can schedule a new container to run next to another based on a container name or ID. For example, you can start a container called <code>frontend</code> running <code>nginx</code>:</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt;  run -d -p 80:80 --name frontend nginx\n 87c4376856a8\n\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n87c4376856a8        nginx:latest        \"nginx\"             Less than a second ago   running             192.168.0.42:80-&gt;80/tcp         node-1/frontend\n</pre> <p>Then, using <code>-e affinity:container==frontend</code> value to schedule a second container to locate and run next to the container named <code>frontend</code>.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name logger -e affinity:container==frontend logger\n 87c4376856a8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n87c4376856a8        nginx:latest        \"nginx\"             Less than a second ago   running             192.168.0.42:80-&gt;80/tcp         node-1/frontend\n963841b138d8        logger:latest       \"logger\"            Less than a second ago   running                                             node-1/logger\n</pre> <p>Because of <code>name</code> affinity, the <code>logger</code> container ends up on <code>node-1</code> along with the <code>frontend</code> container. Instead of the <code>frontend</code> name you could have supplied its ID as follows:</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name logger -e affinity:container==87c4376856a8\n</pre> <h4 id=\"example-image-affinity\">Example image affinity</h4> <p>You can schedule a container to run only on nodes where a specific image is already pulled. For example, suppose you pull a <code>redis</code> image to two hosts and a <code>mysql</code> image to a third.</p> <pre>$ docker -H node-1:2375 pull redis\n$ docker -H node-2:2375 pull mysql\n$ docker -H node-3:2375 pull redis\n</pre> <p>Only <code>node-1</code> and <code>node-3</code> have the <code>redis</code> image. Specify a <code>-e\naffinity:image==redis</code> filter to schedule several additional containers to run on these nodes.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis1 -e affinity:image==redis redis\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis2 -e affinity:image==redis redis\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis3 -e affinity:image==redis redis\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis4 -e affinity:image==redis redis\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis5 -e affinity:image==redis redis\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis6 -e affinity:image==redis redis\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis7 -e affinity:image==redis redis\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis8 -e affinity:image==redis redis\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n87c4376856a8        redis:latest        \"redis\"             Less than a second ago   running                                             node-1/redis1\n1212386856a8        redis:latest        \"redis\"             Less than a second ago   running                                             node-1/redis2\n87c4376639a8        redis:latest        \"redis\"             Less than a second ago   running                                             node-3/redis3\n1234376856a8        redis:latest        \"redis\"             Less than a second ago   running                                             node-1/redis4\n86c2136253a8        redis:latest        \"redis\"             Less than a second ago   running                                             node-3/redis5\n87c3236856a8        redis:latest        \"redis\"             Less than a second ago   running                                             node-3/redis6\n87c4376856a8        redis:latest        \"redis\"             Less than a second ago   running                                             node-3/redis7\n963841b138d8        redis:latest        \"redis\"             Less than a second ago   running                                             node-1/redis8\n</pre> <p>As you can see here, the containers were only scheduled on nodes that had the <code>redis</code> image. Instead of the image name, you could have specified the image ID.</p> <pre>$ docker images\nREPOSITORY                         TAG                       IMAGE ID            CREATED             VIRTUAL SIZE\nredis                              latest                    06a1f75304ba        2 days ago          111.1 MB\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis1 -e affinity:image==06a1f75304ba redis\n</pre> <h4 id=\"example-label-affinity\">Example label affinity</h4> <p>A label affinity allows you to filter based on a custom container label. For example, you can run a <code>nginx</code> container and apply the <code>com.example.type=frontend</code> custom label.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -p 80:80 --label com.example.type=frontend nginx\n 87c4376856a8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps  --filter \"label=com.example.type=frontend\"\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n87c4376856a8        nginx:latest        \"nginx\"             Less than a second ago   running             192.168.0.42:80-&gt;80/tcp         node-1/trusting_yonath\n</pre> <p>Then, use <code>-e affinity:com.example.type==frontend</code> to schedule a container next to the container with the <code>com.example.type==frontend</code> label.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -e affinity:com.example.type==frontend logger\n 87c4376856a8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES\n87c4376856a8        nginx:latest        \"nginx\"             Less than a second ago   running             192.168.0.42:80-&gt;80/tcp         node-1/trusting_yonath\n963841b138d8        logger:latest       \"logger\"            Less than a second ago   running                                             node-1/happy_hawking\n</pre> <p>The <code>logger</code> container ends up on <code>node-1</code> because its affinity with the <code>com.example.type==frontend</code> label.</p> <h3 id=\"use-a-dependency-filter\">Use a dependency filter</h3> <p>A container dependency filter co-schedules dependent containers on the same node. Currently, dependencies are declared as follows:</p> <ul> <li>\n<code>--volumes-from=dependency</code> (shared volumes)</li> <li>\n<code>--link=dependency:alias</code> (links)</li> <li>\n<code>--net=container:dependency</code> (shared network stacks)</li> </ul> <p>Swarm attempts to co-locate the dependent container on the same node. If it cannot be done (because the dependent container doesn’t exist, or because the node doesn’t have enough resources), it will prevent the container creation.</p> <p>The combination of multiple dependencies are honored if possible. For instance, if you specify <code>--volumes-from=A --net=container:B</code>, the scheduler attempts to co-locate the container on the same node as <code>A</code> and <code>B</code>. If those containers are running on different nodes, Swarm does not schedule the container.</p> <h3 id=\"use-a-port-filter\">Use a port filter</h3> <p>When the <code>port</code> filter is enabled, a container’s port configuration is used as a unique constraint. Docker Swarm selects a node where a particular port is available and unoccupied by another container or process. Required ports may be specified by mapping a host port, or using the host networking and exposing a port using the container configuration.</p> <h4 id=\"example-in-bridge-mode\">Example in bridge mode</h4> <p>By default, containers run on Docker’s bridge network. To use the <code>port</code> filter with the bridge network, you run a container as follows.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -p 80:80 nginx\n87c4376856a8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID    IMAGE               COMMAND         PORTS                       NAMES\n87c4376856a8    nginx:latest        \"nginx\"         192.168.0.42:80-&gt;80/tcp     node-1/prickly_engelbart\n</pre> <p>Docker Swarm selects a node where port <code>80</code> is available and unoccupied by another container or process, in this case <code>node-1</code>. Attempting to run another container that uses the host port <code>80</code> results in Swarm selecting a different node, because port <code>80</code> is already occupied on <code>node-1</code>:</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -p 80:80 nginx\n963841b138d8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE          COMMAND        PORTS                           NAMES\n963841b138d8        nginx:latest   \"nginx\"        192.168.0.43:80-&gt;80/tcp         node-2/dreamy_turing\n87c4376856a8        nginx:latest   \"nginx\"        192.168.0.42:80-&gt;80/tcp         node-1/prickly_engelbart\n</pre> <p>Again, repeating the same command will result in the selection of <code>node-3</code>, since port <code>80</code> is neither available on <code>node-1</code> nor <code>node-2</code>:</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -p 80:80 nginx\n963841b138d8\n\n$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID   IMAGE               COMMAND        PORTS                           NAMES\nf8b693db9cd6   nginx:latest        \"nginx\"        192.168.0.44:80-&gt;80/tcp         node-3/stoic_albattani\n963841b138d8   nginx:latest        \"nginx\"        192.168.0.43:80-&gt;80/tcp         node-2/dreamy_turing\n87c4376856a8   nginx:latest        \"nginx\"        192.168.0.42:80-&gt;80/tcp         node-1/prickly_engelbart\n</pre> <p>Finally, Docker Swarm will refuse to run another container that requires port <code>80</code>, because it is not available on any node in the cluster:</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d -p 80:80 nginx\n2014/10/29 00:33:20 Error response from daemon: no resources available to schedule container\n</pre> <p>Each container occupies port <code>80</code> on its residing node when the container is created and releases the port when the container is deleted. A container in <code>exited</code> state still owns the port. If <code>prickly_engelbart</code> on <code>node-1</code> is stopped but not deleted, trying to start another container on <code>node-1</code> that requires port <code>80</code> would fail because port <code>80</code> is associated with <code>prickly_engelbart</code>. To increase running instances of nginx, you can either restart <code>prickly_engelbart</code>, or start another container after deleting <code>prickly_englbart</code>.</p> <h4 id=\"node-port-filter-with-host-networking\">Node port filter with host networking</h4> <p>A container running with <code>--net=host</code> differs from the default <code>bridge</code> mode as the <code>host</code> mode does not perform any port binding. Instead, host mode requires that you explicitly expose one or more port numbers. You expose a port using <code>EXPOSE</code> in the <code>Dockerfile</code> or <code>--expose</code> on the command line. Swarm makes use of this information in conjunction with the <code>host</code> mode to choose an available node for a new container.</p> <p>For example, the following commands start <code>nginx</code> on 3-node cluster.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --expose=80 --net=host nginx\n640297cb29a7\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --expose=80 --net=host nginx\n7ecf562b1b3f\n$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --expose=80 --net=host nginx\n09a92f582bc2\n</pre> <p>Port binding information is not available through the <code>docker ps</code> command because all the nodes were started with the <code>host</code> network.</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND                CREATED                  STATUS              PORTS               NAMES\n640297cb29a7        nginx:1             \"nginx -g 'daemon of   Less than a second ago   Up 30 seconds                           box3/furious_heisenberg\n7ecf562b1b3f        nginx:1             \"nginx -g 'daemon of   Less than a second ago   Up 28 seconds                           box2/ecstatic_meitner\n09a92f582bc2        nginx:1             \"nginx -g 'daemon of   46 seconds ago           Up 27 seconds                           box1/mad_goldstine\n</pre> <p>Swarm refuses the operation when trying to instantiate the 4th container.</p> <pre>$  docker tcp://&lt;manager_ip:manager_port&gt; run -d --expose=80 --net=host nginx\nFATA[0000] Error response from daemon: unable to find a node with port 80/tcp available in the Host mode\n</pre> <p>However, port binding to the different value, for example <code>81</code>, is still allowed.</p> <pre>$  docker tcp://&lt;manager_ip:manager_port&gt; run -d -p 81:80 nginx:latest\n832f42819adc\n$  docker tcp://&lt;manager_ip:manager_port&gt; ps\nCONTAINER ID        IMAGE               COMMAND                CREATED                  STATUS                  PORTS                                 NAMES\n832f42819adc        nginx:1             \"nginx -g 'daemon of   Less than a second ago   Up Less than a second   443/tcp, 192.168.136.136:81-&gt;80/tcp   box3/thirsty_hawking\n640297cb29a7        nginx:1             \"nginx -g 'daemon of   8 seconds ago            Up About a minute                                             box3/furious_heisenberg\n7ecf562b1b3f        nginx:1             \"nginx -g 'daemon of   13 seconds ago           Up About a minute                                             box2/ecstatic_meitner\n09a92f582bc2        nginx:1             \"nginx -g 'daemon of   About a minute ago       Up About a minute                                             box1/mad_goldstine\n</pre> <h2 id=\"how-to-write-filter-expressions\">How to write filter expressions</h2> <p>To apply a node <code>constraint</code> or container <code>affinity</code> filters you must set environment variables on the container using filter expressions, for example:</p> <pre>$ docker tcp://&lt;manager_ip:manager_port&gt; run -d --name redis1 -e affinity:image==~redis redis\n</pre> <p>Each expression must be in the form:</p> <pre>&lt;filter-type&gt;:&lt;key&gt;&lt;operator&gt;&lt;value&gt;\n</pre> <p>The <code>&lt;filter-type&gt;</code> is either the <code>affinity</code> or the <code>constraint</code> keyword. It identifies the type filter you intend to use.</p> <p>The <code>&lt;key&gt;</code> is an alpha-numeric and must start with a letter or underscore. The <code>&lt;key&gt;</code> corresponds to one of the following:</p> <ul> <li>the <code>container</code> keyword</li> <li>the <code>node</code> keyword</li> <li>a default tag (node constraints)</li> <li>a custom metadata label (nodes or containers).</li> </ul> <p>The <code>&lt;operator&gt;</code>is either <code>==</code> or <code>!=</code>. By default, expression operators are hard enforced. If an expression is not met exactly , the manager does not schedule the container. You can use a <code>~</code>(tilde) to create a “soft” expression. The scheduler tries to match a soft expression. If the expression is not met, the scheduler discards the filter and schedules the container according to the scheduler’s strategy.</p> <p>The <code>&lt;value&gt;</code> is an alpha-numeric string, dots, hyphens, and underscores making up one of the following:</p> <ul> <li>A globbing pattern, for example, <code>abc*</code>.</li> <li>A regular expression in the form of <code>/regexp/</code>. See <a href=\"https://github.com/google/re2/wiki/Syntax\">re2 syntax</a> for the supported regex syntax.</li> </ul> <p>The following examples illustrate some possible expressions:</p> <ul> <li>\n<code>constraint:node==node1</code> matches node <code>node1</code>.</li> <li>\n<code>constraint:node!=node1</code> matches all nodes, except <code>node1</code>.</li> <li>\n<code>constraint:region!=us*</code> matches all nodes outside with a <code>region</code> tag prefixed with <code>us</code>.</li> <li>\n<code>constraint:node==/node[12]/</code> matches nodes <code>node1</code> and <code>node2</code>.</li> <li>\n<code>constraint:node==/node\\d/</code> matches all nodes with <code>node</code> + 1 digit.</li> <li>\n<code>constraint:node!=/node-[01]/</code> matches all nodes, except <code>node-0</code> and <code>node-1</code>.</li> <li>\n<code>constraint:node!=/foo\\[bar\\]/</code> matches all nodes, except <code>foo[bar]</code>. You can see the use of escape characters here.</li> <li>\n<code>constraint:node==/(?i)node1/</code> matches node <code>node1</code> case-insensitive. So <code>NoDe1</code> or <code>NODE1</code> also match.</li> <li>\n<code>affinity:image==~redis</code> tries to match for nodes running container with a <code>redis</code> image</li> <li>\n<code>constraint:region==~us*</code> searches for nodes in the cluster belonging to the <code>us</code> region</li> <li>\n<code>affinity:container!=~redis*</code> schedule a new <code>redis5</code> container to a node without a container that satisfies <code>redis*</code>\n</li> </ul> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../../index\">Docker Swarm overview</a></li> <li><a href=\"../../discovery/index\">Discovery options</a></li> <li><a href=\"../strategy/index\">Scheduler strategies</a></li> <li><a href=\"../../swarm-api/index\">Swarm API</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/scheduler/filter/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/scheduler/filter/</a>\n  </p>\n</div>\n","swarm/configure-tls/index":"<h1 id=\"configure-docker-swarm-for-tls\">Configure Docker Swarm for TLS</h1> <p>In this procedure you create a two-node Swarm cluster, a Docker Engine CLI, a Swarm Manager, and a Certificate Authority as shown below. All the Docker Engine hosts (<code>client</code>, <code>swarm</code>, <code>node1</code>, and <code>node2</code>) have a copy of the CA’s certificate as well as their own key-pair signed by the CA.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/tls-1.jpg\" alt=\"\"></p> <p>You will complete the following steps in this procedure:</p> <ul> <li><a href=\"#step-1-set-up-the-prerequisites\">Step 1: Set up the prerequisites</a></li> <li><a href=\"#step-2-create-a-certificate-authority-ca-server\">Step 2: Create a Certificate Authority (CA) server</a></li> <li><a href=\"#step-3-create-and-sign-keys\">Step 3: Create and sign keys</a></li> <li><a href=\"#step-4-install-the-keys\">Step 4: Install the keys</a></li> <li><a href=\"#step-5-configure-the-engine-daemon-for-tls\">Step 5: Configure the Engine daemon for TLS</a></li> <li><a href=\"#step-6-create-a-swarm-cluster\">Step 6: Create a Swarm cluster</a></li> <li><a href=\"#step-7-create-the-swarm-manager-using-tls\">Step 7: Create the Swarm Manager using TLS</a></li> <li><a href=\"#step-8-test-the-swarm-manager-configuration\">Step 8: Test the Swarm manager configuration</a></li> <li><a href=\"#step-9-configure-the-engine-cli-to-use-tls\">Step 9: Configure the Engine CLI to use TLS</a></li> </ul> <h3 id=\"before-you-begin\">Before you begin</h3> <p>The article includes steps to create your own CA using OpenSSL. This is similar to operating your own internal corporate CA and PKI. However, this <code>must not</code> be used as a guide to building a production-worthy internal CA and PKI. These steps are included for demonstration purposes only - so that readers without access to an existing CA and set of certificates can follow along and configure Docker Swarm to use TLS.</p> <h2 id=\"step-1-set-up-the-prerequisites\">Step 1: Set up the prerequisites</h2> <p>To complete this procedure you must stand up 5 (five) Linux servers. These servers can be any mix of physical and virtual servers; they may be on premises or in the public cloud. The following table lists each server name and its purpose.</p> <table> <thead> <tr> <th>Server name</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>ca</code></td> <td>Acts as the Certificate Authority (CA) server.</td> </tr> <tr> <td><code>swarm</code></td> <td>Acts as the Swarm Manager.</td> </tr> <tr> <td><code>node1</code></td> <td>Act as a Swarm node.</td> </tr> <tr> <td><code>node2</code></td> <td>Act as a Swarm node.</td> </tr> <tr> <td><code>client</code></td> <td>Acts as a remote Docker Engine client</td> </tr> </tbody> </table> <p>Make sure that you have SSH access to all 5 servers and that they can communicate with each other using DNS name resolution. In particular:</p> <ul> <li>Open TCP port 2376 between the Swarm Manager and Swarm nodes</li> <li>Open TCP port 3376 between the Docker Engine client and the Swarm Manager</li> </ul> <p>You can choose different ports if these are already in use. This example assumes you use these ports though.</p> <p>Each server must run an operating system compatible with Docker Engine. For simplicity, the steps that follow assume all servers are running Ubuntu 14.04 LTS.</p> <h2 id=\"step-2-create-a-certificate-authority-ca-server\">Step 2: Create a Certificate Authority (CA) server</h2> <blockquote> <p><strong>Note</strong>:If you already have access to a CA and certificates, and are comfortable working with them, you should skip this step and go to the next.</p> </blockquote> <p>In this step, you configure a Linux server as a CA. You use this CA to create and sign keys. This step included so that readers without access to an existing CA (external or corporate) and certificates can follow along and complete the later steps that require installing and using certificates. It is <code>not</code> intended as a model for how to deploy production-worthy CA.</p> <ol> <li>\n<p>Logon to the terminal of your CA server and elevate to root.</p> <pre>$ sudo su\n</pre>\n</li> <li>\n<p>Create a private key called <code>ca-priv-key.pem</code> for the CA:</p> <pre># openssl genrsa -out ca-priv-key.pem 2048\nGenerating RSA private key, 2048 bit long modulus\n...........................................................+++\n.....+++\ne is 65537 (0x10001)\n</pre>\n</li> <li>\n<p>Create a public key called <code>ca.pem</code> for the CA.</p> <p>The public key is based on the private key created in the previous step.</p> <pre># openssl req -config /usr/lib/ssl/openssl.cnf -new -key ca-priv-key.pem -x509 -days 1825 -out ca.pem\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:US\n&lt;output truncated&gt;\n</pre>\n</li> </ol> <p>You have now configured a CA server with a public and private keypair. You can inspect the contents of each key. To inspect the private key:</p> <pre># openssl rsa -in ca-priv-key.pem -noout -text\n</pre> <p>To inspect the public key (cert): `</p> <pre># openssl x509 -in ca.pem -noout -text`\n</pre> <p>The following command shows the partial contents of the CA’s public key.</p> <pre># openssl x509 -in ca.pem -noout -text\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number: 17432010264024107661 (0xf1eaf0f9f41eca8d)\n    Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc\n        Validity\n            Not Before: Jan 16 18:28:12 2016 GMT\n            Not After : Jan 13 18:28:12 2026 GMT\n        Subject: C=US, ST=CA, L=San Francisco, O=Docker Inc\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                Public-Key: (2048 bit)\n                Modulus:\n                    00:d1:fe:6e:55:d4:93:fc:c9:8a:04:07:2d:ba:f0:\n                    55:97:c5:2c:f5:d7:1d:6a:9b:f0:f0:55:6c:5d:90:\n&lt;output truncated&gt;\n</pre> <p>Later, you’ll use this to certificate to sign keys for other servers in the infrastructure.</p> <h2 id=\"step-3-create-and-sign-keys\">Step 3: Create and sign keys</h2> <p>Now that you have a working CA, you need to create key pairs for the Swarm Manager, Swarm nodes, and remote Docker Engine client. The commands and process to create key pairs is identical for all servers. You’ll create the following keys:</p> <table> <tr> <th></th> <th></th> </tr> <tr> <td><code>ca-priv-key.pem</code></td> <td>The CA's private key and must be kept secure. It is used later to sign new keys for the other nodes in the environment. Together with the <code>ca.pem</code> file, this makes up the CA's key pair.</td> </tr> <tr> <td><code>ca.pem</code></td> <td>The CA's public key (also called certificate). This is installed on all nodes in the environment so that all nodes trust certificates signed by the CA. Together with the <code>ca-priv-key.pem</code> file, this makes up the CA's key pair.</td> </tr> <tr> <td><code><i>node</i>.csr</code></td> <td>A certificate signing request (CSR). A CSR is effectively an application to the CA to create a new key pair for a particular node. The CA takes the information provided in the CSR and generates the public and private key pair for that node.</td> </tr> <tr> <td><code><i>node</i>-priv.key</code></td> <td>A private key signed by the CA. The node uses this key to authenticate itself with remote Docker Engines. Together with the <code><i>node</i>-cert.pem</code> file, this makes up a node's key pair.</td> </tr> <tr> <td><code><i>node</i>-cert.pem</code></td> <td>A certificate signed by the CA. This is not used in this example. Together with the <code><i>node</i>-priv.key</code> file, this makes up a node's key pair</td> </tr> </table> <p>The commands below show how to create keys for all of your nodes. You perform this procedure in a working directory located on your CA server.</p> <ol> <li>\n<p>Logon to the terminal of your CA server and elevate to root.</p> <pre>$ sudo su\n</pre>\n</li> <li>\n<p>Create a private key <code>swarm-priv-key.pem</code> for your Swarm Manager</p> <pre># openssl genrsa -out swarm-priv-key.pem 2048\nGenerating RSA private key, 2048 bit long modulus\n............................................................+++\n........+++\ne is 65537 (0x10001)\n</pre>\n</li> <li>\n<p>Generate a certificate signing request (CSR) <code>swarm.csr</code> using the private key you create in the previous step.</p> <pre># openssl req -subj \"/CN=swarm\" -new -key swarm-priv-key.pem -out swarm.csr\n</pre> <p>Remember, this is only for demonstration purposes. The process to create a CSR will be slightly different in real-world production environments.</p>\n</li> <li>\n<p>Create the certificate <code>swarm-cert.pem</code> based on the CSR created in the previous step.</p> <pre># openssl x509 -req -days 1825 -in swarm.csr -CA ca.pem -CAkey ca-priv-key.pem -CAcreateserial -out swarm-cert.pem -extensions v3_req -extfile /usr/lib/ssl/openssl.cnf\n&lt;snip&gt;\n# openssl rsa -in swarm-priv-key.pem -out swarm-priv-key.pem\n</pre>\n</li> </ol> <p>You now have a keypair for the Swarm Manager.</p> <ol> <li>\n<p>Repeat the steps above for the remaining nodes in your infrastructure (<code>node1</code>, <code>node2</code>, and <code>client</code>).</p> <p>Remember to replace the <code>swarm</code> specific values with the values relevant to the node you are creating the key pair for.</p> <table> <tr> <th>Server name</th> <th>Private key</th> <th>CSR</th> <th>Certificate</th> </tr> <tr> <td><code>node1 </code></td> <td><code>node1-priv-key.pem</code></td> <td><code>node1.csr</code></td> <td><code>node1-cert.pem</code></td> </tr> <tr> <td><code>node2</code></td> <td><code>node2-priv-key.pem</code></td> <td><code>node2.csr</code></td> <td><code>node2-cert.pem</code></td> </tr> <tr> <td><code>client</code></td> <td><code>client-priv-key.pem</code></td> <td><code>client.csr</code></td> <td><code>client-cert.pem</code></td> </tr> </table>\n</li> <li>\n<p>Verify that your working directory contains the following files:</p> <pre># ls -l\ntotal 64\n-rw-r--r-- 1 root   root   1679 Jan 16 18:27 ca-priv-key.pem\n-rw-r--r-- 1 root   root   1229 Jan 16 18:28 ca.pem\n-rw-r--r-- 1 root   root     17 Jan 18 09:56 ca.srl\n-rw-r--r-- 1 root   root   1086 Jan 18 09:56 client-cert.pem\n-rw-r--r-- 1 root   root    887 Jan 18 09:55 client.csr\n-rw-r--r-- 1 root   root   1679 Jan 18 09:56 client-priv-key.pem\n-rw-r--r-- 1 root   root   1082 Jan 18 09:44 node1-cert.pem\n-rw-r--r-- 1 root   root    887 Jan 18 09:43 node1.csr\n-rw-r--r-- 1 root   root   1675 Jan 18 09:44 node1-priv-key.pem\n-rw-r--r-- 1 root   root   1082 Jan 18 09:49 node2-cert.pem\n-rw-r--r-- 1 root   root    887 Jan 18 09:49 node2.csr\n-rw-r--r-- 1 root   root   1675 Jan 18 09:49 node2-priv-key.pem\n-rw-r--r-- 1 root   root   1082 Jan 18 09:42 swarm-cert.pem\n-rw-r--r-- 1 root   root    887 Jan 18 09:41 swarm.csr\n-rw-r--r-- 1 root   root   1679 Jan 18 09:42 swarm-priv-key.pem\n</pre>\n</li> </ol> <p>You can inspect the contents of each of the keys. To inspect a private key:</p> <pre>openssl rsa -in &lt;key-name&gt; -noout -text\n</pre> <p>To inspect a public key (cert):</p> <pre>openssl x509 -in &lt;key-name&gt; -noout -text\n</pre> <p>The following commands shows the partial contents of the Swarm Manager’s public <code>swarm-cert.pem</code> key.</p> <pre># openssl x509 -in ca.pem -noout -text\nCertificate:\nData:\n    Version: 3 (0x2)\n    Serial Number: 9590646456311914051 (0x8518d2237ad49e43)\nSignature Algorithm: sha256WithRSAEncryption\n    Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc\n    Validity\n        Not Before: Jan 18 09:42:16 2016 GMT\n        Not After : Jan 15 09:42:16 2026 GMT\n    Subject: CN=swarm\n\n&lt;output truncated&gt;\n</pre> <h2 id=\"step-4-install-the-keys\">Step 4: Install the keys</h2> <p>In this step, you install the keys on the relevant servers in the infrastructure. Each server needs three files:</p> <ul> <li>A copy of the Certificate Authority’s public key (<code>ca.pem</code>)</li> <li>It’s own private key</li> <li>It’s own public key (cert)</li> </ul> <p>The procedure below shows you how to copy these files from the CA server to each server using <code>scp</code>. As part of the copy procedure, you’ll rename each file as follows on each node:</p> <table> <thead> <tr> <th>Original name</th> <th>Copied name</th> </tr> </thead> <tbody> <tr> <td><code>ca.pem</code></td> <td><code>ca.pem</code></td> </tr> <tr> <td><code>&lt;server&gt;-cert.pem</code></td> <td><code>cert.pem</code></td> </tr> <tr> <td><code>&lt;server&gt;-priv-key.pem</code></td> <td><code>key.pem</code></td> </tr> </tbody> </table> <ol> <li>\n<p>Logon to the terminal of your CA server and elevate to root.</p> <pre>$ sudo su\n</pre>\n</li> <li>\n<p>Create a<code>~/.certs</code> directory on the Swarm manager. Here we assume user account is ubuntu.</p> <pre>$ ssh ubuntu@swarm 'mkdir -p /home/ubuntu/.certs'\n</pre>\n</li> <li>\n<p>Copy the keys from the CA to the Swarm Manager server.</p> <pre>$ scp ./ca.pem ubuntu@swarm:/home/ubuntu/.certs/ca.pem\n$ scp ./swarm-cert.pem ubuntu@swarm:/home/ubuntu/.certs/cert.pem\n$ scp ./swarm-priv-key.pem ubuntu@swarm:/home/ubuntu/.certs/key.pem\n</pre> <blockquote> <p><strong>Note</strong>: You may need to provide authentication for the <code>scp</code> commands to work. For example, AWS EC2 instances use certificate-based authentication. To copy the files to an EC2 instance associated with a public key called <code>nigel.pem</code>, modify the <code>scp</code> command as follows: <code>scp -i /path/to/nigel.pem ./ca.pem ubuntu@swarm:/home/ubuntu/.certs/ca.pem</code>.</p> </blockquote>\n</li> <li>\n<p>Repeat step 2 for each remaining server in the infrastructure.</p> <ul> <li><code>node1</code></li> <li><code>node2</code></li> <li><code>client</code></li> </ul>\n</li> <li>\n<p>Verify your work.</p> <p>When the copying is complete, each machine should have the following keys.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/tls-2.jpeg\" alt=\"\"></p> <p>Each node in your infrastructure should have the following files in the <code>/home/ubuntu/.certs/</code> directory:</p> <pre># ls -l /home/ubuntu/.certs/\ntotal 16\n-rw-r--r-- 1 ubuntu ubuntu 1229 Jan 18 10:03 ca.pem\n-rw-r--r-- 1 ubuntu ubuntu 1082 Jan 18 10:06 cert.pem\n-rw-r--r-- 1 ubuntu ubuntu 1679 Jan 18 10:06 key.pem\n</pre>\n</li> </ol> <h2 id=\"step-5-configure-the-engine-daemon-for-tls\">Step 5: Configure the Engine daemon for TLS</h2> <p>In the last step, you created and installed the necessary keys on each of your Swarm nodes. In this step, you configure them to listen on the network and only accept connections using TLS. Once you complete this step, your Swarm nodes will listen on TCP port 2376, and only accept connections using TLS.</p> <p>On <code>node1</code> and <code>node2</code> (your Swarm nodes), do the following:</p> <ol> <li>\n<p>Open a terminal on <code>node1</code> and elevate to root.</p> <pre>$ sudo su\n</pre>\n</li> <li>\n<p>Edit Docker Engine configuration file.</p> <p>If you are following along with these instructions and using Ubuntu 14.04 LTS, the configuration file is <code>/etc/default/docker</code>. The Docker Engine configuration file may be different depending on the Linux distribution you are using.</p>\n</li> <li>\n<p>Add the following options to the <code>DOCKER_OPTS</code> line.</p> <pre> -H tcp://0.0.0.0:2376 --tlsverify --tlscacert=/home/ubuntu/.certs/ca.pem --tlscert=/home/ubuntu/.certs/cert.pem --tlskey=/home/ubuntu/.certs/key.pem\n</pre>\n</li> <li>\n<p>Restart the Docker Engine daemon.</p> <pre> $ service docker restart\n</pre>\n</li> <li><p>Repeat the procedure on <code>node2</code> as well.</p></li> </ol> <h2 id=\"step-6-create-a-swarm-cluster\">Step 6: Create a Swarm cluster</h2> <p>Next create a Swarm cluster. In this procedure you create a two-node Swarm cluster using the default <em>hosted discovery</em> backend. The default hosted discovery backend uses Docker Hub and is not recommended for production use.</p> <ol> <li><p>Logon to the terminal of your Swarm manager node.</p></li> <li>\n<p>Create the cluster and export it’s unique ID to the <code>TOKEN</code> environment variable.</p> <pre>$ sudo export TOKEN=$(docker run --rm swarm create)\nUnable to find image 'swarm:latest' locally\nlatest: Pulling from library/swarm\nd681c900c6e3: Pulling fs layer\n&lt;snip&gt;\n986340ab62f0: Pull complete\na9975e2cc0a3: Pull complete\nDigest: sha256:c21fd414b0488637b1f05f13a59b032a3f9da5d818d31da1a4ca98a84c0c781b\nStatus: Downloaded newer image for swarm:latest\n</pre>\n</li> <li>\n<p>Join <code>node1</code> to the cluster.</p> <p>Be sure to specify TCP port <code>2376</code> and not <code>2375</code>.</p> <pre>$ sudo docker run -d swarm join --addr=node1:2376 token://$TOKEN\n7bacc98536ed6b4200825ff6f4004940eb2cec891e1df71c6bbf20157c5f9761\n</pre>\n</li> <li>\n<p>Join <code>node2</code> to the cluster.</p> <pre>$ sudo docker run -d swarm join --addr=node2:2376 token://$TOKEN\ndb3f49d397bad957202e91f0679ff84f526e74d6c5bf1b6734d834f5edcbca6c\n</pre>\n</li> </ol> <h2 id=\"step-7-start-the-swarm-manager-using-tls\">Step 7: Start the Swarm Manager using TLS</h2> <ol> <li>\n<p>Launch a new container with TLS enables</p> <pre>$ docker run -d -p 3376:3376 -v /home/ubuntu/.certs:/certs:ro swarm manage --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/cert.pem --tlskey=/certs/key.pem --host=0.0.0.0:3376 token://$TOKEN\n</pre> <p>The command above launches a new container based on the <code>swarm</code> image and it maps port <code>3376</code> on the server to port <code>3376</code> inside the container. This mapping ensures that Docker Engine commands sent to the host on port <code>3376</code> are passed on to port <code>3376</code> inside the container. The container runs the Swarm <code>manage</code> process with the <code>--tlsverify</code>, <code>--tlscacert</code>, <code>--tlscert</code> and <code>--tlskey</code> options specified. These options force TLS verification and specify the location of the Swarm manager’s TLS keys.</p>\n</li> <li>\n<p>Run a <code>docker ps</code> command to verify that your Swarm manager container is up and running.</p> <pre>$ docker ps\nCONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                              NAMES\n035dbf57b26e   swarm               \"/swarm manage --tlsv\"   7 seconds ago    Up 7 seconds    2375/tcp, 0.0.0.0:3376-&gt;3376/tcp   compassionate_lovelace\n</pre>\n</li> </ol> <p>Your Swarm cluster is now configured to use TLS.</p> <h2 id=\"step-8-test-the-swarm-manager-configuration\">Step 8: Test the Swarm manager configuration</h2> <p>Now that you have a Swarm cluster built and configured to use TLS, you’ll test that it works with a Docker Engine CLI.</p> <ol> <li><p>Open a terminal onto your <code>client</code> server.</p></li> <li>\n<p>Issue the <code>docker version</code> command.</p> <p>When issuing the command, you must pass it the location of the clients certifications.</p> <pre>$ sudo docker --tlsverify --tlscacert=/home/ubuntu/.certs/ca.pem --tlscert=/home/ubuntu/.certs/cert.pem --tlskey=/home/ubuntu/.certs/key.pem -H swarm:3376 version\nClient:\n Version:      1.9.1\n API version:  1.21\n Go version:   go1.4.2\n Git commit:   a34a1d5\n Built:        Fri Nov 20 13:12:04 UTC 2015\n OS/Arch:      linux/amd64\n\nServer:\n Version:      swarm/1.0.1\n API version:  1.21\n Go version:   go1.5.2\n Git commit:   744e3a3\n Built:\n OS/Arch:      linux/amd64\n</pre> <p>The output above shows the <code>Server</code> version as “swarm/1.0.1”. This means that the command was successfully issued against the Swarm manager.</p>\n</li> <li>\n<p>Verify that the same command does not work without TLS.</p> <p>This time, do not pass your certs to the Swarm manager.</p> <pre>$ sudo docker -H swarm:3376 version\n:\n Version:      1.9.1\n API version:  1.21\n Go version:   go1.4.2\n Git commit:   a34a1d5\n Built:        Fri Nov 20 13:12:04 UTC 2015\n OS/Arch:      linux/amd64\nGet http://swarm:3376/v1.21/version: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\".\n* Are you trying to connect to a TLS-enabled daemon without TLS?\n</pre> <p>The output above shows that the command was rejected by the server. This is because the server (Swarm manager) is configured to only accept connections from authenticated clients using TLS.</p>\n</li> </ol> <h2 id=\"step-9-configure-the-engine-cli-to-use-tls\">Step 9: Configure the Engine CLI to use TLS</h2> <p>You can configure the Engine so that you don’t have to pass the TLS options when you issue a command. To do this, you’ll configure the <code>Docker Engine host</code> and <code>TLS</code> settings as defaults on your Docker Engine client.</p> <p>To do this, you place the client’s keys in your <code>~/.docker</code> configuration folder. If you have other users on your system using the Engine command line, you’ll need to configure their account’s <code>~/.docker</code> as well. The procedure below shows how to do this for the <code>ubuntu</code> user on your Docker Engine client.</p> <ol> <li><p>Open a terminal onto your <code>client</code> server.</p></li> <li>\n<p>If it doesn’t exist, create a <code>.docker</code> directory in the <code>ubuntu</code> user’s home directory.</p> <pre>$ mkdir /home/ubuntu/.docker\n</pre>\n</li> <li>\n<p>Copy the Docker Engine client’s keys from <code>/home/ubuntu/.certs</code> to <code>/home/ubuntu/.docker</code></p> <pre>$ cp /home/ubuntu/.certs/{ca,cert,key}.pem /home/ubuntu/.docker\n</pre>\n</li> <li><p>Edit the account’s <code>~/.bash_profile</code>.</p></li> <li>\n<p>Set the following variables:</p> <table> <tr> <th>Variable</th> <th>Description</th> </tr> <tr> <td><code>DOCKER_HOST</code></td> <td>Sets the Docker host and TCP port to send all Engine commands to.</td> </tr> <tr> <td><code>DOCKER_TLS_VERIFY</code></td> <td>Tell's Engine to use TLS.</td> </tr> <tr> <td><code>DOCKER_CERT_PATH</code></td> <td>Specifies the location of TLS keys.</td> </tr> </table> <p>For example:</p> <pre>    export DOCKER_HOST=tcp://swarm:3376\n    export DOCKER_TLS_VERIFY=1\n    export DOCKER_CERT_PATH=/home/ubuntu/.docker/\n</pre>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>Source the file to pick up the new variables.</p> <pre>    $ source ~/.bash_profile\n</pre>\n</li> <li>\n<p>Verify that the procedure worked by issuing a <code>docker version</code> command</p> <pre>$ docker version\nClient:\n Version:      1.9.1\n API version:  1.21\n Go version:   go1.4.2\n Git commit:   a34a1d5\n Built:        Fri Nov 20 13:12:04 UTC 2015\n OS/Arch:      linux/amd64\n\nServer:\n Version:      swarm/1.0.1\n API version:  1.21\n Go version:   go1.5.2\n Git commit:   744e3a3\n Built:\n OS/Arch:      linux/amd64\n</pre> <p>The server portion of the output above command shows that your Docker client is issuing commands to the Swarm Manager and using TLS.</p>\n</li> </ol> <p>Congratulations! You have configured a Docker Swarm cluster to use TLS.</p> <h2 id=\"related-information\">Related Information</h2> <ul> <li><a href=\"../secure-swarm-tls/index\">Secure Docker Swarm with TLS</a></li> <li><a href=\"https://docs.docker.com/engine/security/security/\">Docker security</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/configure-tls/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/configure-tls/</a>\n  </p>\n</div>\n","swarm/reference/create/index":"<h1 id=\"create-create-a-discovery-token\">create — Create a discovery token</h1> <p>The <code>create</code> command uses Docker Hub’s hosted discovery backend to create a unique <em>discovery token</em> for your cluster. For example:</p> <pre>$ docker run --rm  swarm create\n86222732d62b6868d441d430aee4f055\n</pre> <p>Later, when you use <a href=\"../manage/index\"><code>manage</code></a> or <a href=\"../join/index\"><code>join</code></a> to create Swarm managers and nodes, you use the discovery token in the <code>&lt;discovery&gt;</code> argument (e.g., <code>token://86222732d62b6868d441d430aee4f055</code>). The discovery backend registers each new Swarm manager and node that uses the token as a member of your cluster.</p> <p>Some documentation also refers to the discovery token as a <em>cluster_id</em>.</p> <blockquote> <p>Warning: Docker Hub’s hosted discovery backend is not recommended for production use. It’s intended only for testing/development.</p> </blockquote> <p>For more information and examples about this and other discovery backends, see the <a href=\"../../discovery/index\">Docker Swarm Discovery</a> topic.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/reference/create/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/reference/create/</a>\n  </p>\n</div>\n","swarm/reference/join/index":"<h1 id=\"join-create-a-swarm-node\">join — Create a Swarm node</h1> <p>Prerequisite: Before using <code>join</code>, establish a discovery backend as described in <a href=\"../../discovery/index\">this discovery topic</a>.</p> <p>The <code>join</code> command creates a Swarm node whose purpose is to run containers on behalf of the cluster. A typical cluster has multiple Swarm nodes.</p> <p>To create a Swarm node, use the following syntax:</p> <pre>$ docker run swarm join [OPTIONS] &lt;discovery&gt;\n</pre> <p>For example, to create a Swarm node in a high-availability cluster with other managers, enter:</p> <pre>$ docker run -d swarm join --advertise=172.30.0.69:2375 consul://172.30.0.161:8500\n</pre> <p>Or, for example, to create a Swarm node that uses Transport Layer Security (TLS) to authenticate the Docker Swarm nodes, enter:</p> <pre>$ sudo docker run -d swarm join --addr=node1:2376 token://86222732d62b6868d441d430aee4f055\n</pre> <h2 id=\"arguments\">Arguments</h2> <p>The <code>join</code> command has only one argument:</p> <h3 id=\"discovery-discovery-backend\">\n<code>&lt;discovery&gt;</code> — Discovery backend</h3> <p>Before you create a Swarm node, <a href=\"../create/index\">create a discovery token</a> or <a href=\"../../discovery/index\">set up a discovery backend</a> for your cluster.</p> <p>When you create the Swarm node, use the <code>&lt;discovery&gt;</code> argument to specify one of the following discovery backends:</p> <ul> <li><code>token://&lt;token&gt;</code></li> <li><code>consul://&lt;ip1&gt;/&lt;path&gt;</code></li> <li><code>etcd://&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip2&gt;/&lt;path&gt;</code></li> <li><code>file://&lt;path/to/file&gt;</code></li> <li><code>zk://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;</code></li> <li><code>[nodes://]&lt;iprange&gt;,&lt;iprange&gt;</code></li> </ul> <p>Where:</p> <ul> <li><p><code>&lt;token&gt;</code> is a discovery token generated by Docker Hub’s hosted discovery service. To generate this discovery token, use the <a href=\"../create/index\"><code>create</code></a> command. &gt; Warning: Docker Hub’s hosted discovery backend is not recommended for production use. It’s intended only for testing/development.</p></li> <li><p><code>ip1</code>, <code>ip2</code>, <code>ip3</code> are each the IP address and port numbers of a discovery backend node.</p></li> <li><p><code>path</code> (optional) is a path to a key-value store on the discovery backend. When you use a single backend to service multiple clusters, you use paths to maintain separate key-value stores for each cluster.</p></li> <li><p><code>path/to/file</code> is the path to a file that contains a static list of the Swarm managers and nodes that are members the cluster. </p></li> <li><p><code>iprange</code> is an IP address or a range of IP addresses followed by a port number.</p></li> </ul> <p>For example: * A discovery token: <code>token://0ac50ef75c9739f5bfeeaf00503d4e6e</code> * A Consul node: <code>consul://172.30.0.165:8500</code></p> <p>The environment variable for <code>&lt;discovery&gt;</code> is <code>$SWARM_DISCOVERY</code>.</p> <p>For more information and examples, see the <a href=\"../../discovery/index\">Docker Swarm Discovery</a> topic.</p> <h2 id=\"options\">Options</h2> <p>The <code>join</code> command has the following options:</p> <h3 id=\"advertise-or-addr-advertise-the-docker-engine-s-ip-and-port-number\">\n<code>--advertise</code> or <code>--addr</code> — Advertise the Docker Engine’s IP and port number</h3> <p>Use <code>--advertise &lt;ip&gt;:&lt;port&gt;</code> or <code>--addr &lt;ip&gt;:&lt;port&gt;</code> to advertise the IP address and port number of the Docker Engine. For example, <code>--advertise 172.30.0.161:4000</code>. Swarm managers MUST be able to reach this Swarm node at this address.</p> <p>The environment variable for <code>--advertise</code> is <code>$SWARM_ADVERTISE</code>.</p> <h3 id=\"heartbeat-period-between-each-heartbeat\">\n<code>--heartbeat</code> — Period between each heartbeat</h3> <p>Use <code>--heartbeat \"&lt;interval&gt;s\"</code> to specify the interval, in seconds, between heartbeats the node sends to the primary manager. These heartbeats indicate that the node is healthy and reachable. By default, the interval is 60 seconds.</p> <h3 id=\"ttl-sets-the-expiration-of-an-ephemeral-node\">\n<code>--ttl</code> — Sets the expiration of an ephemeral node</h3> <p>Use <code>--ttl \"&lt;interval&gt;s\"</code> to specify the time-to-live (TTL) interval, in seconds, of an ephemeral node. The default interval is <code>180s</code>. </p> <h3 id=\"delay-add-a-random-delay-in-0s-delay-to-avoid-synchronized-registration\">\n<code>--delay</code> — Add a random delay in [0s,delay] to avoid synchronized registration</h3> <p>Use <code>--delay \"&lt;interval&gt;s\"</code> to specify the maximum interval for a random delay, in seconds, before the node registers with the discovery backend. If you deploy a large number of nodes simultaneously, the random delay spreads registrations out over the interval and avoids saturating the discovery backend.</p> <h3 id=\"discovery-opt-discovery-options\">\n<code>--discovery-opt</code> — Discovery options</h3> <p>Use <code>--discovery-opt &lt;value&gt;</code> to discovery options, such as paths to the TLS files; the CA’s public key certificate, the certificate, and the private key of the distributed K/V store on a Consul or etcd discovery backend. You can enter multiple discovery options. For example:</p> <pre>--discovery-opt kv.cacertfile=/path/to/mycacert.pem \\\n--discovery-opt kv.certfile=/path/to/mycert.pem \\\n--discovery-opt kv.keyfile=/path/to/mykey.pem \\\n</pre> <p>For more information, see <a href=\"../../discovery/index\">Use TLS with distributed key/value discovery</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/reference/join/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/reference/join/</a>\n  </p>\n</div>\n","swarm/secure-swarm-tls/index":"<h1 id=\"overview-swarm-with-tls\">Overview Swarm with TLS</h1> <p>All nodes in a Swarm cluster must bind their Docker daemons to a network port. This has obvious security implications. These implications are compounded when the network in question is untrusted such as the internet. To mitigate these risks, Docker Swarm and the Docker Engine daemon support Transport Layer Security (TLS).</p> <blockquote> <p><strong>Note</strong>: TLS is the successor to SSL (Secure Sockets Layer) and the two terms are often used interchangeably. Docker uses TLS, this term is used throughout this article.</p> </blockquote> <h2 id=\"learn-the-tls-concepts\">Learn the TLS concepts</h2> <p>Before going further, it is important to understand the basic concepts of TLS and public key infrastructure (PKI).</p> <p>Public key infrastructure is a combination of security related technologies, policies, and procedures, that are used to create and manage digital certificates. These certificates and infrastructure secure digital communication using mechanisms such as authentication and encryption.</p> <p>The following analogy may be useful. It is common practice that passports are used to verify an individual’s identity. Passports usually contain a photograph and biometric information that identify the owner. A passport also lists the country that issued it, as well as <em>valid from</em> and <em>valid to</em> dates. Digital certificates are very similar. The text below is an extract from a digital certificate:</p> <pre>Certificate:\nData:\n    Version: 3 (0x2)\n    Serial Number: 9590646456311914051 (0x8518d2237ad49e43)\nSignature Algorithm: sha256WithRSAEncryption\n    Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc\n    Validity\n        Not Before: Jan 18 09:42:16 2016 GMT\n        Not After : Jan 15 09:42:16 2026 GMT\n    Subject: CN=swarm\n</pre> <p>This certificate identifies a computer called <strong>swarm</strong>. The certificate is valid between January 2016 and January 2026 and was issued by Docker Inc. based in the state of California in the US.</p> <p>Just as passports authenticate individuals as they board flights and clear customs, digital certificates authenticate computers on a network.</p> <p>Public key infrastructure (PKI) is the combination of technologies, policies, and procedures that work behind the scenes to enable digital certificates. Some of the technologies, policies and procedures provided by PKI include:</p> <ul> <li>Services to securely request certificates</li> <li>Procedures to authenticate the entity requesting the certificate</li> <li>Procedures to determine the entity’s eligibility for the certificate</li> <li>Technologies and processes to issue certificates</li> <li>Technologies and processes to revoke certificates</li> </ul> <h2 id=\"how-does-docker-engine-authenticate-using-tls\">How does Docker Engine authenticate using TLS</h2> <p>In this section, you’ll learn how Docker Engine and Swarm use PKI and certificates to increase security.</p>  <p>You can configure both the Docker Engine CLI and the Docker Engine daemon to require TLS for authentication. Configuring TLS means that all communications between the Docker Engine CLI and the Docker Engine daemon must be accompanied with, and signed by a trusted digital certificate. The Docker Engine CLI must provide its digital certificate before the Docker Engine daemon will accept incoming commands from it.</p> <p>The Docker Engine daemon must also trust the certificate that the Docker Engine CLI uses. This trust is usually established by way of a trusted third party. The Docker Engine CLI and Docker Engine daemon in the diagram below are configured to require TLS authentication.</p> <p><img src=\"https://docs.docker.com/v1.11/swarm/images/trust-diagram.jpg\" alt=\"\"></p> <p>The trusted third party in this diagram is the the Certificate Authority (CA) server. Like the country in the passport example, a CA creates, signs, issues, revokes certificates. Trust is established by installing the CA’s root certificate on the host running the Docker Engine daemon. The Docker Engine CLI then requests its own certificate from the CA server, which the CA server signs and issues to the client.</p> <p>The Docker Engine CLI sends its certificate to the Docker Engine daemon before issuing commands. The Docker Engine daemon inspects the certificate, and because the Docker Engine daemon trusts the CA, the Docker Engine daemon automatically trusts any certificates signed by the CA. Assuming the certificate is in order (the certificate has not expired or been revoked etc.) the Docker Engine daemon accepts commands from this trusted Docker Engine CLI.</p> <p>The Docker Engine CLI is simply a client that uses the Docker Engine Remote API to communicate with the Docker Engine daemon. Any client that uses this Docker Engine Remote API can use TLS. For example, Dcoker Engine clients such as ‘Docker Universal Control Plane’ (UCP) have TLS support built-in. Other, third party products, that use Docker Engine Remote API, can also be configured this way.</p> <h2 id=\"tls-modes-with-docker-and-swarm\">TLS modes with Docker and Swarm</h2> <p>Now that you know how certificates are used by the Docker Engine daemon for authentication, it’s important to be aware of the three TLS configurations possible with Docker Engine daemon and its clients:</p> <ul> <li>External 3rd party CA</li> <li>Internal corporate CA</li> <li>Self-signed certificates</li> </ul> <p>These configurations are differentiated by the type of entity acting as the Certificate Authority (CA).</p> <h3 id=\"external-3rd-party-ca\">External 3rd party CA</h3> <p>An external CA is a trusted 3rd party company that provides a means of creating, issuing, revoking, and otherwise managing certificates. They are <em>trusted</em> in the sense that they have to fulfill specific conditions and maintain high levels of security and business practices to win your business. You also have to install the external CA’s root certificates for you computers and services to <em>trust</em> them.</p> <p>When you use an external 3rd party CA, they create, sign, issue, revoke and otherwise manage your certificates. They normally charge a fee for these services, but are considered an enterprise-class scalable solution that provides a high degree of trust.</p> <h3 id=\"internal-corporate-ca\">Internal corporate CA</h3> <p>Many organizations choose to implement their own Certificate Authorities and PKI. Common examples are using OpenSSL and Microsoft Active Directory. In this case, your company is its own Certificate Authority with all the work it entails. The benefit is, as your own CA, you have more control over your PKI.</p> <p>Running your own CA and PKI requires you to provide all of the services offered by external 3rd party CAs. These include creating, issuing, revoking, and otherwise managing certificates. Doing all of this yourself has its own costs and overheads. However, for a large corporation, it still may reduce costs in comparison to using an external 3rd party service.</p> <p>Assuming you operate and manage your own internal CAs and PKI properly, an internal, corporate CA can be a highly scalable and highly secure option.</p> <h3 id=\"self-signed-certificates\">Self-signed certificates</h3> <p>As the name suggests, self-signed certificates are certificates that are signed with their own private key rather than a trusted CA. This is a low cost and simple to use option. If you implement and manage self-signed certificates correctly, they can be better than using no certificates.</p> <p>Because self-signed certificates lack of a full-blown PKI, they do not scale well and lack many of the advantages offered by the other options. One of their disadvantages is that you cannot revoke self-signed certificates. Due to this, and other limitations, self-signed certificates are considered the least secure of the three options. Self-signed certificates are not recommended for public facing production workloads exposed to untrusted networks.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../configure-tls/index\">Configure Docker Swarm for TLS</a></li> <li><a href=\"https://docs.docker.com/engine/security/security/\">Docker security</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/secure-swarm-tls/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/secure-swarm-tls/</a>\n  </p>\n</div>\n","swarm/reference/help/index":"<h1 id=\"help-display-information-about-a-command\">help - Display information about a command</h1> <p>The <code>help</code> command displays information about how to use a command.</p> <p>For example, to see a list of Swarm options and commands, enter:</p> <pre>$ docker run swarm --help\n</pre> <p>To see a list of arguments and options for a specific Swarm command, enter:</p> <pre>$ docker run swarm &lt;command&gt; --help\n</pre> <p>For example:</p> <pre>$ docker run swarm list --help\nUsage: swarm list [OPTIONS] &lt;discovery&gt;\n\nList nodes in a cluster\n\nArguments:\n   &lt;discovery&gt;    discovery service to use [$SWARM_DISCOVERY]\n                   * token://&lt;token&gt;\n                   * consul://&lt;ip&gt;/&lt;path&gt;\n                   * etcd://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;\n                   * file://path/to/file\n                   * zk://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;\n                   * [nodes://]&lt;ip1&gt;,&lt;ip2&gt;\n\nOptions:\n   --timeout \"10s\"                          timeout period\n   --discovery-opt [--discovery-opt option --discovery-opt option]  discovery options\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/reference/help/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/reference/help/</a>\n  </p>\n</div>\n","swarm/reference/list/index":"<h1 id=\"list-list-the-nodes-in-a-cluster\">list — List the nodes in a cluster</h1> <p>Use <code>list</code> to display a list of the nodes in a cluster.</p> <p>To list the nodes in a cluster, use the following syntax:</p> <pre>docker run swarm list [OPTIONS] &lt;discovery&gt;\n</pre> <p>The following examples show a few different syntaxes for the <code>&lt;discovery&gt;</code> argument:</p> <p>etcd:</p> <pre>swarm list etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt; &lt;node_ip:port&gt;\n</pre> <p>Consul:</p> <pre>swarm list consul://&lt;consul_addr&gt;/&lt;optional path prefix&gt; &lt;node_ip:port&gt;\n</pre> <p>ZooKeeper:</p> <pre>swarm list zk://&lt;zookeeper_addr1&gt;,&lt;zookeeper_addr2&gt;/&lt;optional path prefix&gt; &lt;node_ip:port&gt;\n</pre>  <h2 id=\"arguments\">Arguments</h2> <p>The <code>list</code> command has only one argument:</p> <h3 id=\"discovery-discovery-backend\">\n<code>&lt;discovery&gt;</code> — Discovery backend</h3> <p>When you use the <code>list</code> command, use the <code>&lt;discovery&gt;</code> argument to specify one of the following discovery backends:</p> <ul> <li><code>token://&lt;token&gt;</code></li> <li><code>consul://&lt;ip1&gt;/&lt;path&gt;</code></li> <li><code>etcd://&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip2&gt;/&lt;path&gt;</code></li> <li><code>file://&lt;path/to/file&gt;</code></li> <li><code>zk://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;</code></li> <li><code>[nodes://]&lt;iprange&gt;,&lt;iprange&gt;</code></li> </ul> <p>Where:</p> <ul> <li><p><code>&lt;token&gt;</code> is a discovery token generated by Docker Hub’s hosted discovery service. To generate this discovery token, use the <a href=\"../create/index\"><code>create</code></a> command. &gt; Warning: Docker Hub’s hosted discovery backend is not recommended for production use. It’s intended only for testing/development.</p></li> <li><p><code>ip1</code>, <code>ip2</code>, <code>ip3</code> are each the IP address and port numbers of a discovery backend node.</p></li> <li><p><code>path</code> (optional) is a path to a key-value store on the discovery backend. When you use a single backend to service multiple clusters, you use paths to maintain separate key-value stores for each cluster.</p></li> <li><p><code>path/to/file</code> is the path to a file that contains a static list of the Swarm managers and nodes that are members the cluster. </p></li> <li><p><code>iprange</code> is an IP address or a range of IP addresses followed by a port number.</p></li> </ul> <p>For example:</p> <ul> <li>A discovery token: <code>token://0ac50ef75c9739f5bfeeaf00503d4e6e</code>\n</li> <li>A Consul node: <code>consul://172.30.0.165:8500</code>\n</li> </ul> <p>The environment variable for <code>&lt;discovery&gt;</code> is <code>$SWARM_DISCOVERY</code>.</p> <p>For more information and examples, see the <a href=\"../../discovery/index\">Docker Swarm Discovery</a> topic.</p> <h2 id=\"options\">Options</h2> <p>The <code>list</code> command has the following options:</p> <h3 id=\"timeout-timeout-period\">\n<code>--timeout</code> — Timeout period</h3> <p>Use <code>--timeout \"&lt;interval&gt;s\"</code> to specify the timeout period, in seconds, to wait for the discovery backend to return the list. The default interval is <code>10s</code>.</p> <h3 id=\"discovery-opt-discovery-options\">\n<code>--discovery-opt</code> — Discovery options</h3> <p>Use <code>--discovery-opt &lt;value&gt;</code> to discovery options, such as paths to the TLS files; the CA’s public key certificate, the certificate, and the private key of the distributed K/V store on a Consul or etcd discovery backend. You can enter multiple discovery options. For example:</p> <pre>--discovery-opt kv.cacertfile=/path/to/mycacert.pem \\\n--discovery-opt kv.certfile=/path/to/mycert.pem \\\n--discovery-opt kv.keyfile=/path/to/mykey.pem \\\n</pre> <p>For more information, see <a href=\"../../discovery/index\">Use TLS with distributed key/value discovery</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/reference/list/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/reference/list/</a>\n  </p>\n</div>\n","swarm/reference/swarm/index":"<h1 id=\"swarm-a-docker-native-clustering-system\">Swarm — A Docker-native clustering system</h1> <p>The <code>swarm</code> command runs a Swarm container on a Docker Engine host and performs the task specified by the required subcommand, <code>COMMAND</code>.</p> <p>Use <code>swarm</code> with the following syntax:</p> <pre>$ docker run swarm [OPTIONS] COMMAND [arg...]\n</pre> <p>For example, you use <code>swarm</code> with the <code>manage</code> subcommand to create a Swarm manager in a high-availability cluster with other managers:</p> <pre>$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise 172.30.0.161:4000 consul://172.30.0.165:8500\n</pre> <h2 id=\"options\">Options</h2> <p>The <code>swarm</code> command has the following options:</p> <ul> <li>\n<code>--debug</code> — Enable debug mode. Display messages that you can use to debug a Swarm node. For example: time=“2016-02-17T17:57:40Z” level=fatal msg=“discovery required to join a cluster. See ‘swarm join --help’.” The environment variable for this option is <code>[$DEBUG]</code>.</li> <li>\n<code>--log-level \"&lt;value&gt;\"</code> or <code>-l \"&lt;value&gt;\"</code> — Set the log level. Where <code>&lt;value&gt;</code> is: <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>, <code>fatal</code>, or <code>panic</code>. The default value is <code>info</code>.</li> <li>\n<code>--experimental</code> — Enable experimental features.</li> <li>\n<code>--help</code> or <code>-h</code> — Display help.</li> <li>\n<code>--version</code> or <code>-v</code> — Display the version. For example: $ docker run swarm --version swarm version 1.1.0 (a0fd82b)</li> </ul> <h2 id=\"commands\">Commands</h2> <p>The <code>swarm</code> command has the following subcommands:</p> <ul> <li>\n<a href=\"../create/index\">create, c</a> - Create a discovery token</li> <li>\n<a href=\"../list/index\">list, l</a> - List the nodes in a Docker cluster</li> <li>\n<a href=\"../manage/index\">manage, m</a> - Create a Swarm manager</li> <li>\n<a href=\"../join/index\">join, j</a> - Create a Swarm node</li> <li>\n<a href=\"../help/index\">help</a> - Display a list of Swarm commands, or help for one command</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/reference/swarm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/reference/swarm/</a>\n  </p>\n</div>\n","swarm/reference/manage/index":"<h1 id=\"manage-create-a-swarm-manager\">manage — Create a Swarm manager</h1> <p>Prerequisite: Before using <code>manage</code> to create a Swarm manager, establish a discovery backend as described in <a href=\"../../discovery/index\">this discovery topic</a>.</p> <p>The <code>manage</code> command creates a Swarm manager whose purpose is to receive commands on behalf of the cluster and assign containers to Swarm nodes. You can create multiple Swarm managers as part of a high-availability cluster.</p> <p>To create a Swarm manager, use the following syntax:</p> <pre>$ docker run swarm manage [OPTIONS] &lt;discovery&gt;\n</pre> <p>For example, you can use <code>manage</code> to create a Swarm manager in a high-availability cluster with other managers:</p> <pre>$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise 172.30.0.161:4000 consul://172.30.0.165:8500\n</pre> <p>Or, for example, you can use it to create a Swarm manager that uses Transport Layer Security (TLS) to authenticate the Docker Client and Swarm nodes:</p> <pre>$ docker run -d -p 3376:3376 -v /home/ubuntu/.certs:/certs:ro swarm manage --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/cert.pem --tlskey=/certs/key.pem --host=0.0.0.0:3376 token://$TOKEN\n</pre> <h2 id=\"argument\">Argument</h2> <p>The <code>manage</code> command has only one argument:</p> <h3 id=\"discovery-discovery-backend\">\n<code>&lt;discovery&gt;</code> — Discovery backend</h3> <p>Before you create a Swarm manager, <a href=\"../create/index\">create a discovery token</a> or <a href=\"../../discovery/index\">set up a discovery backend</a> for your cluster.</p> <p>When you create the swarm node, use the <code>&lt;discovery&gt;</code> argument to specify one of the following discovery backends:</p> <ul> <li><code>token://&lt;token&gt;</code></li> <li><code>consul://&lt;ip1&gt;/&lt;path&gt;</code></li> <li><code>etcd://&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip2&gt;/&lt;path&gt;</code></li> <li><code>file://&lt;path/to/file&gt;</code></li> <li><code>zk://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;</code></li> <li><code>[nodes://]&lt;iprange&gt;,&lt;iprange&gt;</code></li> </ul> <p>Where:</p> <ul> <li><p><code>&lt;token&gt;</code> is a discovery token generated by Docker Hub’s hosted discovery service. To generate this discovery token, use the <a href=\"../create/index\"><code>create</code></a> command. &gt; Warning: Docker Hub’s hosted discovery backend is not recommended for production use. It’s intended only for testing/development.</p></li> <li><p><code>ip1</code>, <code>ip2</code>, <code>ip3</code> are each the IP address and port numbers of a discovery backend node.</p></li> <li><p><code>path</code> (optional) is a path to a key-value store on the discovery backend. When you use a single backend to service multiple clusters, you use paths to maintain separate key-value stores for each cluster.</p></li> <li><p><code>path/to/file</code> is the path to a file that contains a static list of the Swarm managers and nodes that are members the cluster. </p></li> <li><p><code>iprange</code> is an IP address or a range of IP addresses followed by a port number.</p></li> </ul> <p>Here are a pair of <code>&lt;discovery&gt;</code> argument examples:</p> <ul> <li>A discovery token: <code>token://0ac50ef75c9739f5bfeeaf00503d4e6e</code>\n</li> <li>A Consul node: <code>consul://172.30.0.165:8500</code>\n</li> </ul> <p>The environment variable for <code>&lt;discovery&gt;</code> is <code>$SWARM_DISCOVERY</code>.</p> <p>For more information and examples, see the <a href=\"../../discovery/index\">Docker Swarm Discovery</a> topic.</p> <h2 id=\"options\">Options</h2> <p>The <code>manage</code> command has the following options:</p> <h3 id=\"strategy-scheduler-placement-strategy\">\n<code>--strategy</code> — Scheduler placement strategy</h3> <p>Use <code>--strategy \"&lt;value&gt;\"</code> to tell the Docker Swarm scheduler which placement strategy to use.</p> <p>Where <code>&lt;value&gt;</code> is:</p> <ul> <li>\n<code>spread</code> — Assign each container to the Swarm node with the most available resources.</li> <li>\n<code>binpack</code> - Assign containers to one Swarm node until it is full before assigning them to another one.</li> <li>\n<code>random</code> - Assign each container to a random Swarm node.</li> </ul> <p>By default, the scheduler applies the <code>spread</code> strategy.</p> <p>For more information and examples, see <a href=\"../../scheduler/strategy/index\">Docker Swarm strategies</a>.</p> <h3 id=\"filter-f-scheduler-filter\">\n<code>--filter</code>, <code>-f</code> — Scheduler filter</h3> <p>Use <code>--filter &lt;value&gt;</code> or <code>-f &lt;value&gt;</code> to tell the Docker Swarm scheduler which nodes to use when creating and running a container.</p> <p>Where <code>&lt;value&gt;</code> is:</p> <ul> <li>\n<code>health</code> — Use nodes that are running and communicating with the discovery backend.</li> <li>\n<code>port</code> — For containers that have a static port mapping, use nodes whose corresponding port number is available (i.e., not occupied by another container or process).</li> <li>\n<code>dependency</code> — For containers that have a declared dependency, use nodes that already have a container with the same dependency.</li> <li>\n<code>affinity</code> — For containers that have a declared affinity, use nodes that already have a container with the same affinity.</li> <li>\n<code>constraint</code> — For containers that have a declared constraint, use nodes that already have a container with the same constraint.</li> </ul> <p>You can use multiple scheduler filters, like this:</p> <p><code>--filter &lt;value&gt; --filter &lt;value&gt;</code></p> <p>For more information and examples, see <a href=\"../../scheduler/filter/index\">Swarm filters</a>.</p> <h3 id=\"host-h-listen-to-ip-port\">\n<code>--host</code>, <code>-H</code> — Listen to IP/port</h3> <p>Use <code>--host &lt;ip&gt;:&lt;port&gt;</code> or <code>-H &lt;ip&gt;:&lt;port&gt;</code> to specify the IP address and port number to which the manager listens for incoming messages. If you replace <code>&lt;ip&gt;</code> with zeros or omit it altogether, the manager uses the default host IP. For example, <code>--host=0.0.0.0:3376</code> or <code>-H :4000</code>.</p> <p>The environment variable for <code>--host</code> is <code>$SWARM_HOST</code>.  </p> <h3 id=\"replication-enable-swarm-manager-replication\">\n<code>--replication</code> — Enable Swarm manager replication</h3> <p>Enable Swarm manager replication between the <em>primary</em> and <em>secondary</em> managers in a high-availability cluster. Replication mirrors cluster information from the primary to the secondary managers so that, if the primary manager fails, a secondary can become the primary manager.</p> <h3 id=\"replication-ttl-leader-lock-release-time-on-failure\">\n<code>--replication-ttl</code> — Leader lock release time on failure</h3> <p>Use <code>--replication-ttl \"&lt;delay&gt;s\"</code> to specify the delay, in seconds, before notifying secondary managers that the primary manager is down or unreachable. This notification triggers an election in which one of the secondary managers becomes the primary manager. By default, the delay is 15 seconds.</p> <h3 id=\"advertise-addr-advertise-docker-engine-s-ip-and-port-number\">\n<code>--advertise</code>, <code>--addr</code> — Advertise Docker Engine’s IP and port number</h3> <p>Use <code>--advertise &lt;ip&gt;:&lt;port&gt;</code> or <code>--addr &lt;ip&gt;:&lt;port&gt;</code> to advertise the IP address and port number of the Docker Engine. For example, <code>--advertise 172.30.0.161:4000</code>. Other Swarm managers MUST be able to reach this Swarm manager at this address.</p> <p>The environment variable for <code>--advertise</code> is <code>$SWARM_ADVERTISE</code>.</p> <h3 id=\"tls-enable-transport-layer-security-tls\">\n<code>--tls</code> — Enable transport layer security (TLS)</h3> <p>Use <code>--tls</code> to enable transport layer security (TLS). If you use <code>--tlsverify</code>, you do not need to use <code>--tls</code>.</p> <h3 id=\"tlscacert-path-to-a-ca-s-public-key-file\">\n<code>--tlscacert</code> — Path to a CA’s public key file</h3> <p>Use <code>--tlscacert=&lt;path/file&gt;</code> to specify the path and filename of the public key (certificate) from a Certificate Authority (CA). For example, <code>--tlscacert=/certs/ca.pem</code>. When specified, the manager trusts only remotes that provide a certificate signed by the same CA.</p> <h3 id=\"tlscert-path-to-the-node-s-tls-certificate-file\">\n<code>--tlscert</code> — Path to the node’s TLS certificate file</h3> <p>Use <code>--tlscert</code> to specify the path and filename of the manager’s certificate (signed by the CA). For example, <code>--tlscert=/certs/cert.pem</code>.</p> <h3 id=\"tlskey-path-to-the-node-s-tls-key-file\">\n<code>--tlskey</code> — Path to the node’s TLS key file</h3> <p>Use <code>--tlskey</code> to specify the path and filename of the manager’s private key (signed by the CA). For example, <code>--tlskey=/certs/key.pem</code>.</p> <h3 id=\"tlsverify-use-tls-and-verify-the-remote\">\n<code>--tlsverify</code> — Use TLS and verify the remote</h3> <p>Use <code>--tlsverify</code> to enable transport layer security (TLS) and accept connections from only those managers, nodes, and clients that have a certificate signed by the same CA. If you use <code>--tlsverify</code>, you do not need to use <code>--tls</code>.</p> <h3 id=\"engine-refresh-min-interval-set-engine-refresh-minimum-interval\">\n<code>--engine-refresh-min-interval</code> — Set engine refresh minimum interval</h3> <p>Use <code>--engine-refresh-min-interval \"&lt;interval&gt;s\"</code> to specify the minimum interval, in seconds, between Engine refreshes. By default, the interval is 30 seconds.</p> <blockquote> <p>When the primary manager in performs an <em>Engine refresh</em>, it gets updated information about an Engine in the cluster. The manager uses this information to, among other things, determine whether the Engine is <em>healthy</em>. If there is a connection failure, the manager determines that the node is <em>unhealthy</em>. The manager <em>retries</em> an Engine refresh a specified number of times. If the Engine responds to one of the retries, the manager determines that the Engine is healthy again. Otherwise, the manager stops retrying and ignores the Engine. </p> </blockquote> <h3 id=\"engine-refresh-max-interval-set-engine-refresh-maximum-interval\">\n<code>--engine-refresh-max-interval</code> — Set engine refresh maximum interval</h3> <p>Use <code>--engine-refresh-max-interval \"&lt;interval&gt;s\"</code> to specify the minimum interval, in seconds, between Engine refresh. By default, the interval is 60 seconds.</p> <h3 id=\"engine-failure-retry-set-engine-failure-retry-count\">\n<code>--engine-failure-retry</code> — Set engine failure retry count</h3> <p>Use <code>--engine-failure-retry \"&lt;number&gt;\"</code> to specify the number of retries to attempt if the engine fails. By default, the number is 3 retries.</p> <h3 id=\"engine-refresh-retry-deprecated\">\n<code>--engine-refresh-retry</code> — Deprecated</h3> <p>Deprecated; Use <code>--engine-failure-retry</code> instead of <code>--engine-refresh-retry \"&lt;number&gt;\"</code>. The default number is 3 retries.</p> <h3 id=\"heartbeat-period-between-each-heartbeat\">\n<code>--heartbeat</code> — Period between each heartbeat</h3> <p>Use <code>--heartbeat \"&lt;interval&gt;s\"</code> to specify the interval, in seconds, between heartbeats the manager sends to the primary manager. These heartbeats indicate that the manager is healthy and reachable. By default, the interval is 60 seconds.</p> <h3 id=\"api-enable-cors-cors-enable-cors-headers-in-the-remote-api\">\n<code>--api-enable-cors</code>, <code>--cors</code> — Enable CORS headers in the remote API</h3> <p>Use <code>--api-enable-cors</code> or <code>--cors</code> to enable cross-origin resource sharing (CORS) headers in the remote API.</p> <h3 id=\"cluster-driver-c-cluster-driver-to-use\">\n<code>--cluster-driver</code>, <code>-c</code> — Cluster driver to use</h3> <p>Use <code>--cluster-driver \"&lt;driver&gt;\"</code>, <code>-c \"&lt;driver&gt;\"</code> to specify a cluster driver to use. Where <code>&lt;driver&gt;</code> is one of the following:</p> <ul> <li>\n<code>swarm</code> is the Docker Swarm driver.</li> <li>\n<code>mesos-experimental</code> is the Mesos cluster driver.</li> </ul> <p>By default, the driver is <code>swarm</code>.</p> <p>For more information about using Mesos driver, see <a href=\"https://github.com/docker/swarm/blob/master/cluster/mesos/README.md\">Using Docker Swarm and Mesos</a>.</p> <h3 id=\"cluster-opt-cluster-driver-options\">\n<code>--cluster-opt</code> — Cluster driver options</h3> <p>You can enter multiple cluster driver options, like this:</p> <p><code>--cluster-opt &lt;value&gt; --cluster-opt &lt;value&gt;</code></p> <p>Where <code>&lt;value&gt;</code> is one of the following:</p> <ul> <li>\n<code>swarm.overcommit=0.05</code> — Set the fractional percentage by which to overcommit resources. The default value is <code>0.05</code>, or 5 percent.</li> <li>\n<code>swarm.createretry=0</code> — Specify the number of retries to attempt when creating a container fails. The default value is <code>0</code> retries.</li> <li>\n<code>mesos.address=</code> — Specify the Mesos address to bind on. The environment variable for this option is <code>$SWARM_MESOS_ADDRESS</code>.</li> <li>\n<code>mesos.checkpointfailover=false</code> — Enable Mesos checkpointing, which allows a restarted slave to reconnect with old executors and recover status updates, at the cost of disk I/O. The environment variable for this option is <code>$SWARM_MESOS_CHECKPOINT_FAILOVER</code>. The default value is <code>false</code> (disabled).</li> <li>\n<code>mesos.port=</code> — Specify the Mesos port to bind on. The environment variable for this option is <code>$SWARM_MESOS_PORT</code>.</li> <li>\n<code>mesos.offertimeout=30s</code> — Specify the Mesos timeout for offers, in seconds. The environment variable for this option is <code>$SWARM_MESOS_OFFER_TIMEOUT</code>. The default value is <code>30s</code>.</li> <li>\n<code>mesos.offerrefusetimeout=5s</code> — Specify timeout for Mesos to consider unused resources refused, in seconds. The environment variable for this option is <code>$SWARM_MESOS_OFFER_REFUSE_TIMEOUT</code>. The default value is <code>5s</code>.</li> <li>\n<code>mesos.tasktimeout=5s</code> — Specify the timeout for Mesos task creation, in seconds. The environment variable for this option is <code>$SWARM_MESOS_TASK_TIMEOUT</code>. The default value is <code>5s</code>.</li> <li>\n<code>mesos.user=</code> — Specify the Mesos framework user name. The environment variable for this option is <code>$SWARM_MESOS_USER</code>.</li> </ul> <h3 id=\"discovery-opt-discovery-options\">\n<code>--discovery-opt</code> — Discovery options</h3> <p>Use <code>--discovery-opt &lt;value&gt;</code> to discovery options, such as paths to the TLS files; the CA’s public key certificate, the certificate, and the private key of the distributed K/V store on a Consul or etcd discovery backend. You can enter multiple discovery options. For example:</p> <pre>--discovery-opt kv.cacertfile=/path/to/mycacert.pem \\\n--discovery-opt kv.certfile=/path/to/mycert.pem \\\n--discovery-opt kv.keyfile=/path/to/mykey.pem \\\n</pre> <p>For more information, see <a href=\"../../discovery/index\">Use TLS with distributed key/value discovery</a></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/reference/manage/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/reference/manage/</a>\n  </p>\n</div>\n","swarm/swarm-api/index":"<h1 id=\"docker-swarm-api\">Docker Swarm API</h1> <p>The Docker Swarm API is mostly compatible with the <a href=\"https://docs.docker.com/engine/reference/api/docker_remote_api/\">Docker Remote API</a>. This document is an overview of the differences between the Swarm API and the Docker Remote API.</p> <h2 id=\"missing-endpoints\">Missing endpoints</h2> <p>Some endpoints have not yet been implemented and will return a 404 error.</p> <pre>POST \"/images/create\" : \"docker import\" flow not implement\n</pre> <h2 id=\"endpoints-which-behave-differently\">Endpoints which behave differently</h2> <table> <tr> <th>Endpoint</th> <th>Differences</th> </tr> <tr> <td> <code>GET \"/containers/{name:.*}/json\"</code> </td> <td> New field <code>Node</code> added:<br> <pre>\n\"Node\": {\n    \"Id\": \"ODAI:IC6Q:MSBL:TPB5:HIEE:6IKC:VCAM:QRNH:PRGX:ERZT:OK46:PMFX\",\n    \"Ip\": \"0.0.0.0\",\n    \"Addr\": \"http://0.0.0.0:4243\",\n    \"Name\": \"vagrant-ubuntu-saucy-64\"\n}\n</pre> </td> </tr> <tr> <td> <code>GET \"/containers/{name:.*}/json\"</code> </td> <td> <code>HostIP</code> replaced by the the actual Node's IP if <code>HostIP</code> is <code>0.0.0.0</code> </td> </tr> <tr> <td> <code>GET \"/containers/json\"</code> </td> <td> Node's name prepended to the container name. </td> </tr> <tr> <td> <code>GET \"/containers/json\"</code> </td> <td> <code>HostIP</code> replaced by the the actual Node's IP if <code>HostIP</code> is <code>0.0.0.0</code> </td> </tr> <tr> <td> <code>GET \"/containers/json\"</code> </td> <td> Containers started from the <code>swarm</code> official image are hidden by default, use <code>all=1</code> to display them. </td> </tr> <tr> <td> <code>GET \"/images/json\"</code> </td> <td> Use <code>--filter node=&lt;Node name&gt;</code> to show images of the specific node. </td> </tr> <tr> <td> <code>POST \"/containers/create\"</code> </td> <td> <code>CpuShares</code> in <code>HostConfig</code> sets the number of CPU cores allocated to the container. </td> </tr> </table> <h2 id=\"registry-authentication\">Registry Authentication</h2> <p>During container create calls, the Swarm API will optionally accept an <code>X-Registry-Auth</code> header. If provided, this header is passed down to the engine if the image must be pulled to complete the create operation.</p> <p>The following two examples demonstrate how to utilize this using the existing Docker CLI</p> <h3 id=\"authenticate-using-registry-tokens\">Authenticate using registry tokens</h3> <blockquote> <p><strong>Note:</strong> This example requires Docker Engine 1.10 with auth token support. For older Engine versions, refer to <a href=\"#authenticate-using-username-and-password\">authenticate using username and password</a></p> </blockquote> <p>This example uses the <a href=\"https://stedolan.github.io/jq/\"><code>jq</code> command-line utility</a>. To run this example, install <code>jq</code> using your package manager (<code>apt-get install jq</code> or <code>yum install jq</code>).</p> <pre>REPO=yourrepo/yourimage\nREPO_USER=yourusername\nread -s PASSWORD\nAUTH_URL=https://auth.docker.io/token\n\n# obtain a JSON token, and extract the \"token\" value using 'jq'\nTOKEN=$(curl -s -u \"${REPO_USER}:${PASSWORD}\" \"${AUTH_URL}?scope=repository:${REPO}:pull&amp;service=registry.docker.io\" | jq -r \".token\")\nHEADER=$(echo \"{\\\"registrytoken\\\":\\\"${TOKEN}\\\"}\"|base64 -w 0 )\necho HEADER=$HEADER\n</pre> <p>Add the header you’ve calculated to your <code>~/.docker/config.json</code>:</p> <pre>\"HttpHeaders\": {\n    \"X-Registry-Auth\": \"&lt;HEADER string from above&gt;\"\n}\n</pre> <p>You can now authenticate to the registry, and run private images on Swarm:</p> <pre>$ docker run --rm -it yourprivateimage:latest\n</pre> <p>Be aware that tokens are short-lived and will expire quickly.</p> <h3 id=\"authenticate-using-username-and-password\">Authenticate using username and password</h3> <blockquote> <p><strong>Note:</strong> this authentication method stores your credentials unencrypted on the filesystem. Refer to <a href=\"#authenticate-using-registry-tokens\">Authenticate using registry tokens</a> for a more secure approach.</p> </blockquote> <p>First, calculate the header</p> <pre>REPO_USER=yourusername\nread -s PASSWORD\nHEADER=$(echo \"{\\\"username\\\":\\\"${REPO_USER}\\\",\\\"password\\\":\\\"${PASSWORD}\\\"}\" | base64 -w 0 )\nunset PASSWORD\necho HEADER=$HEADER\n</pre> <p>Add the header you’ve calculated to your <code>~/.docker/config.json</code>:</p> <pre>\"HttpHeaders\": {\n    \"X-Registry-Auth\": \"&lt;HEADER string from above&gt;\"\n}\n</pre> <p>You can now authenticate to the registry, and run private images on Swarm:</p> <pre>$ docker run --rm -it yourprivateimage:latest\n</pre> <h2 id=\"docker-swarm-documentation-index\">Docker Swarm documentation index</h2> <ul> <li><a href=\"https://docs.docker.com/swarm/\">Docker Swarm overview</a></li> <li><a href=\"https://docs.docker.com/swarm/discovery/\">Discovery options</a></li> <li><a href=\"https://docs.docker.com/swarm/scheduler/strategy/\">Scheduler strategies</a></li> <li><a href=\"https://docs.docker.com/swarm/scheduler/filter/\">Scheduler filters</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/swarm-api/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/swarm-api/</a>\n  </p>\n</div>\n","swarm/status-code-comparison-to-docker/index":"<h1 id=\"understand-the-swarm-vs-engine-response-codes\">Understand the Swarm vs. Engine response codes</h1> <p>Docker Engine provides a REST API for making calls to the Engine daemon. Docker Swarm allows a caller to make the same calls to a cluster of Engine daemons. While the API calls are the same, the API response status codes do differ. This document explains the differences.</p> <p>Four methods are included, and they are GET, POST, PUT and DELETE.</p> <p>The comparison is based on api v1.22, and all Docker Status Codes in api v1.22 are referenced from <a href=\"https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.22.md\">docker-remote-api-v1.22</a>.</p> <h2 id=\"get\">GET</h2> <ul> <li>Route: <code>/_ping</code>\n</li> <li>Handler: <code>ping</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/events</code><br>\n</li> <li>Handler: <code>getEvents</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/info</code><br>\n</li> <li>Handler: <code>getInfo</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/version</code><br>\n</li> <li>Handler: <code>getVersion</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/json</code><br>\n</li> <li>Handler: <code>getImagesJSON</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/viz</code><br>\n</li> <li>Handler: <code>notImplementedHandler</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">501</td> <td align=\"center\">no this api</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/search</code><br>\n</li> <li>Handler: <code>proxyRandom</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/get</code><br>\n</li> <li>Handler: <code>getImages</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/{name:.*}/get</code><br>\n</li> <li>Handler: <code>proxyImageGet</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/{name:.*}/history</code><br>\n</li> <li>Handler: <code>proxyImage</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/{name:.*}/json</code><br>\n</li> <li>Handler: <code>proxyImage</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/ps</code><br>\n</li> <li>Handler: <code>getContainersJSON</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">no this api</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">no this api</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">no this api</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/json</code><br>\n</li> <li>Handler: <code>getContainersJSON</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">400</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/archive</code><br>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\">400</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/export</code><br>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/changes</code><br>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/json</code><br>\n</li> <li>Handler: <code>getContainerJSON</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/top</code><br>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/logs</code><br>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">101</td> <td align=\"center\">101</td> </tr> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/stats</code><br>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/attach/ws</code>\n</li> <li>Handler: <code>proxyHijack</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\">400</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/exec/{execid:.*}/json</code><br>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/networks</code><br>\n</li> <li>Handler: <code>getNetworks</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/networks/{networkid:.*}</code><br>\n</li> <li>Handler: <code>getNetwork</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> </tbody> </table> <ul> <li>Route: <code>/volumes</code><br>\n</li> <li>Handler: <code>getVolumes</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/volumes/{volumename:.*}</code><br>\n</li> <li>Handler: <code>getVolume</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">500</td> </tr> </tbody> </table> <h2 id=\"post\">POST</h2> <ul> <li>Route: <code>/auth</code>\n</li> <li>Handler: <code>proxyRandom</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/commit</code>\n</li> <li>Handler: <code>postCommit</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">201</td> <td align=\"center\">201</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/build</code>\n</li> <li>Handler: <code>postBuild</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/create</code>\n</li> <li>Handler: <code>postImagesCreate</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/load</code>\n</li> <li>Handler: <code>postImagesLoad</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\"></td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">201</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/{name:.*}/push</code>\n</li> <li>Handler: <code>proxyImagePush</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/{name:.*}/tag</code>\n</li> <li>Handler: <code>postTagImage</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">201</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">400</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">409</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/create</code>\n</li> <li>Handler: <code>postContainersCreate</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">201</td> <td align=\"center\">201</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">406</td> </tr> <tr> <td align=\"center\">409</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/kill</code>\n</li> <li>Handler: <code>proxyContainerAndForceRefresh</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/pause</code>\n</li> <li>Handler: <code>proxyContainerAndForceRefresh</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/unpause</code>\n</li> <li>Handler: <code>proxyContainerAndForceRefresh</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/rename</code>\n</li> <li>Handler: <code>postRenameContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">409</td> <td align=\"center\">409</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/restart</code>\n</li> <li>Handler: <code>proxyContainerAndForceRefresh</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/start</code>\n</li> <li>Handler: <code>postContainersStart</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">304</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/stop</code>\n</li> <li>Handler: <code>proxyContainerAndForceRefresh</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">304</td> <td align=\"center\">304</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/update</code>\n</li> <li>Handler: <code>proxyContainerAndForceRefresh</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\">400</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/wait</code>\n</li> <li>Handler: <code>proxyContainerAndForceRefresh</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/resize</code>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/attach</code>\n</li> <li>Handler: <code>proxyHijack</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">101</td> <td align=\"center\">101</td> </tr> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\">400</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/copy</code>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/containers/{name:.*}/exec</code>\n</li> <li>Handler: <code>postContainersExec</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">201</td> <td align=\"center\">201</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">409</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/exec/{execid:.*}/start</code>\n</li> <li>Handler: <code>postExecStart</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">409</td> <td align=\"center\">409</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\"></td> </tr> </tbody> </table> <ul> <li>Route: <code>/exec/{execid:.*}/resize</code>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">201</td> <td align=\"center\">201</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\"></td> </tr> </tbody> </table> <ul> <li>Route: <code>/networks/create</code>\n</li> <li>Handler: <code>postNetworksCreate</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">201</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/networks/{networkid:.*}/connect</code>\n</li> <li>Handler: <code>proxyNetworkConnect</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/networks/{networkid:.*}/disconnect</code>\n</li> <li>Handler: <code>proxyNetworkDisconnect</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/volumes/create</code>\n</li> <li>Handler: <code>postVolumesCreate</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">201</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <h2 id=\"put\">PUT</h2> <ul> <li>Route: <code>/containers/{name:.*}/archive\"</code>\n</li> <li>Handler: <code>proxyContainer</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">400</td> <td align=\"center\">400</td> </tr> <tr> <td align=\"center\">403</td> <td align=\"center\">403</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <h2 id=\"delete\">DELETE</h2> <ul> <li>Route: <code>/containers/{name:.*}</code>\n</li> <li>Handler: <code>deleteContainers</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">400</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/images/{name:.*}</code>\n</li> <li>Handler: <code>deleteImages</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">200</td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">409</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/networks/{networkid:.*}</code>\n</li> <li>Handler: <code>deleteNetworks</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\"></td> <td align=\"center\">200</td> </tr> <tr> <td align=\"center\">204</td> <td align=\"center\"></td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table> <ul> <li>Route: <code>/volumes/{name:.*}\"</code>\n</li> <li>Handler: <code>deleteVolumes</code>\n</li> </ul> <table> <thead> <tr> <th align=\"center\">Swarm Status Code</th> <th align=\"center\">Docker Status Code</th> </tr> </thead> <tbody> <tr> <td align=\"center\">204</td> <td align=\"center\">204</td> </tr> <tr> <td align=\"center\">404</td> <td align=\"center\">404</td> </tr> <tr> <td align=\"center\"></td> <td align=\"center\">409</td> </tr> <tr> <td align=\"center\">500</td> <td align=\"center\">500</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/status-code-comparison-to-docker/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/status-code-comparison-to-docker/</a>\n  </p>\n</div>\n","compose/overview/index":"<h1 id=\"overview-of-docker-compose\">Overview of Docker Compose</h1> <p>Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a Compose file to configure your application’s services. Then, using a single command, you create and start all the services from your configuration. To learn more about all the features of Compose see <a href=\"#features\">the list of features</a>.</p> <p>Compose is great for development, testing, and staging environments, as well as CI workflows. You can learn more about each case in <a href=\"#common-use-cases\">Common Use Cases</a>.</p> <p>Using Compose is basically a three-step process.</p> <ol> <li><p>Define your app’s environment with a <code>Dockerfile</code> so it can be reproduced anywhere.</p></li> <li><p>Define the services that make up your app in <code>docker-compose.yml</code> so they can be run together in an isolated environment.</p></li> <li><p>Lastly, run <code>docker-compose up</code> and Compose will start and run your entire app.</p></li> </ol> <p>A <code>docker-compose.yml</code> looks like this:</p> <pre>version: '2'\nservices:\n  web:\n    build: .\n    ports:\n    - \"5000:5000\"\n    volumes:\n    - .:/code\n    - logvolume01:/var/log\n    links:\n    - redis\n  redis:\n    image: redis\nvolumes:\n  logvolume01: {}\n</pre> <p>For more information about the Compose file, see the <a href=\"../compose-file/index\">Compose file reference</a></p> <p>Compose has commands for managing the whole lifecycle of your application:</p> <ul> <li>Start, stop and rebuild services</li> <li>View the status of running services</li> <li>Stream the log output of running services</li> <li>Run a one-off command on a service</li> </ul> <h2 id=\"compose-documentation\">Compose documentation</h2> <ul> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../gettingstarted/index\">Getting Started</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../faq/index\">Frequently asked questions</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul> <h2 id=\"features\">Features</h2> <p>The features of Compose that make it effective are:</p> <ul> <li><a href=\"#Multiple-isolated-environments-on-a-single-host\">Multiple isolated environments on a single host</a></li> <li><a href=\"#preserve-volume-data-when-containers-are-created\">Preserve volume data when containers are created</a></li> <li><a href=\"#only-recreate-containers-that-have-changed\">Only recreate containers that have changed</a></li> <li><a href=\"#variables-and-moving-a-composition-between-environments\">Variables and moving a composition between environments</a></li> </ul> <h3 id=\"multiple-isolated-environments-on-a-single-host\">Multiple isolated environments on a single host</h3> <p>Compose uses a project name to isolate environments from each other. You can make use of this project name in several different contexts:</p> <ul> <li>on a dev host, to create multiple copies of a single environment (e.g., you want to run a stable copy for each feature branch of a project)</li> <li>on a CI server, to keep builds from interfering with each other, you can set the project name to a unique build number</li> <li>on a shared host or dev host, to prevent different projects, which may use the same service names, from interfering with each other</li> </ul> <p>The default project name is the basename of the project directory. You can set a custom project name by using the <a href=\"../reference/overview/index\"><code>-p</code> command line option</a> or the <a href=\"../reference/envvars/index#compose-project-name\"><code>COMPOSE_PROJECT_NAME</code> environment variable</a>.</p> <h3 id=\"preserve-volume-data-when-containers-are-created\">Preserve volume data when containers are created</h3> <p>Compose preserves all volumes used by your services. When <code>docker-compose up</code> runs, if it finds any containers from previous runs, it copies the volumes from the old container to the new container. This process ensures that any data you’ve created in volumes isn’t lost.</p> <h3 id=\"only-recreate-containers-that-have-changed\">Only recreate containers that have changed</h3> <p>Compose caches the configuration used to create a container. When you restart a service that has not changed, Compose re-uses the existing containers. Re-using containers means that you can make changes to your environment very quickly.</p> <h3 id=\"variables-and-moving-a-composition-between-environments\">Variables and moving a composition between environments</h3> <p>Compose supports variables in the Compose file. You can use these variables to customize your composition for different environments, or different users. See <a href=\"../compose-file/index#variable-substitution\">Variable substitution</a> for more details.</p> <p>You can extend a Compose file using the <code>extends</code> field or by creating multiple Compose files. See <a href=\"../extends/index\">extends</a> for more details.</p> <h2 id=\"common-use-cases\">Common Use Cases</h2> <p>Compose can be used in many different ways. Some common use cases are outlined below.</p> <h3 id=\"development-environments\">Development environments</h3> <p>When you’re developing software, the ability to run an application in an isolated environment and interact with it is crucial. The Compose command line tool can be used to create the environment and interact with it.</p> <p>The <a href=\"../compose-file/index\">Compose file</a> provides a way to document and configure all of the application’s service dependencies (databases, queues, caches, web service APIs, etc). Using the Compose command line tool you can create and start one or more containers for each dependency with a single command (<code>docker-compose up</code>).</p> <p>Together, these features provide a convenient way for developers to get started on a project. Compose can reduce a multi-page “developer getting started guide” to a single machine readable Compose file and a few commands.</p> <h3 id=\"automated-testing-environments\">Automated testing environments</h3> <p>An important part of any Continuous Deployment or Continuous Integration process is the automated test suite. Automated end-to-end testing requires an environment in which to run tests. Compose provides a convenient way to create and destroy isolated testing environments for your test suite. By defining the full environment in a <a href=\"../compose-file/index\">Compose file</a> you can create and destroy these environments in just a few commands:</p> <pre>$ docker-compose up -d\n$ ./run_tests\n$ docker-compose down\n</pre> <h3 id=\"single-host-deployments\">Single host deployments</h3> <p>Compose has traditionally been focused on development and testing workflows, but with each release we’re making progress on more production-oriented features. You can use Compose to deploy to a remote Docker Engine. The Docker Engine may be a single instance provisioned with <a href=\"../../machine/overview/index\">Docker Machine</a> or an entire <a href=\"../../swarm/overview/index\">Docker Swarm</a> cluster.</p> <p>For details on using production-oriented features, see <a href=\"../production/index\">compose in production</a> in this documentation.</p> <h2 id=\"release-notes\">Release Notes</h2> <p>To see a detailed list of changes for past and current releases of Docker Compose, please refer to the <a href=\"https://github.com/docker/compose/blob/master/CHANGELOG.md\">CHANGELOG</a>.</p> <h2 id=\"getting-help\">Getting help</h2> <p>Docker Compose is under active development. If you need help, would like to contribute, or simply want to talk about the project with like-minded individuals, we have a number of open channels for communication.</p> <ul> <li><p>To report bugs or file feature requests: please use the <a href=\"https://github.com/docker/compose/issues\">issue tracker on Github</a>.</p></li> <li><p>To talk about the project with people in real time: please join the <code>#docker-compose</code> channel on freenode IRC.</p></li> <li><p>To contribute code or documentation changes: please submit a <a href=\"https://github.com/docker/compose/pulls\">pull request on Github</a>.</p></li> </ul> <p>For more information and resources, please visit the <a href=\"https://docs.docker.com/opensource/get-help/\">Getting Help project page</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/overview/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/overview/</a>\n  </p>\n</div>\n","compose/install/index":"<h1 id=\"install-docker-compose\">Install Docker Compose</h1> <p>You can run Compose on OS X, Windows and 64-bit Linux. To install it, you’ll need to install Docker first.</p> <p>To install Compose, do the following:</p> <ol> <li>\n<p>Install Docker Engine:</p> <ul> <li><p><a href=\"../../engine/installation/mac/index\" target=\"_blank\">Mac OS X installation</a></p></li> <li><p><a href=\"../../engine/installation/windows/index\" target=\"_blank\">Windows installation</a></p></li> <li><p><a href=\"../../engine/installation/linux/ubuntulinux/index\" target=\"_blank\">Ubuntu installation</a></p></li> <li><p><a href=\"../../engine/installation/index\" target=\"_blank\">other system installations</a></p></li> </ul>\n</li> <li><p>The Docker Toolbox installation includes both Engine and Compose, so Mac and Windows users are done installing. Others should continue to the next step.</p></li> <li><p>Go to the <a href=\"https://github.com/docker/compose/releases\" target=\"_blank\">Compose repository release page on GitHub</a>.</p></li> <li>\n<p>Follow the instructions from the release page and run the <code>curl</code> command, which the release page specifies, in your terminal.</p> <blockquote> <p>Note: If you get a “Permission denied” error, your <code>/usr/local/bin</code> directory probably isn’t writable and you’ll need to install Compose as the superuser. Run <code>sudo -i</code>, then the two commands below, then <code>exit</code>.</p> </blockquote> <p>The following is an example command illustrating the format:</p> <pre>curl -L https://github.com/docker/compose/releases/download/1.7.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n</pre> <p>If you have problems installing with <code>curl</code>, see <a href=\"#alternative-install-options\">Alternative Install Options</a>.</p>\n</li> <li>\n<p>Apply executable permissions to the binary:</p> <pre>$ chmod +x /usr/local/bin/docker-compose\n</pre>\n</li> <li><p>Optionally, install <a href=\"../completion/index\">command completion</a> for the <code>bash</code> and <code>zsh</code> shell.</p></li> <li>\n<p>Test the installation.</p> <pre>$ docker-compose --version\ndocker-compose version: 1.7.1\n</pre>\n</li> </ol> <h2 id=\"alternative-install-options\">Alternative install options</h2> <h3 id=\"install-using-pip\">Install using pip</h3> <p>Compose can be installed from <a href=\"https://pypi.python.org/pypi/docker-compose\">pypi</a> using <code>pip</code>. If you install using <code>pip</code> it is highly recommended that you use a <a href=\"https://virtualenv.pypa.io/en/latest/\">virtualenv</a> because many operating systems have python system packages that conflict with docker-compose dependencies. See the <a href=\"http://docs.python-guide.org/en/latest/dev/virtualenvs/\">virtualenv tutorial</a> to get started.</p> <pre>$ pip install docker-compose\n</pre> <blockquote> <p><strong>Note:</strong> pip version 6.0 or greater is required</p> </blockquote> <h3 id=\"install-as-a-container\">Install as a container</h3> <p>Compose can also be run inside a container, from a small bash script wrapper. To install compose as a container run:</p> <pre>$ curl -L https://github.com/docker/compose/releases/download/1.7.1/run.sh &gt; /usr/local/bin/docker-compose\n$ chmod +x /usr/local/bin/docker-compose\n</pre> <h2 id=\"master-builds\">Master builds</h2> <p>If you’re interested in trying out a pre-release build you can download a binary from <a href=\"https://dl.bintray.com/docker-compose/master/\">https://dl.bintray.com/docker-compose/master/</a>. Pre-release builds allow you to try out new features before they are released, but may be less stable.</p> <h2 id=\"upgrading\">Upgrading</h2> <p>If you’re upgrading from Compose 1.2 or earlier, you’ll need to remove or migrate your existing containers after upgrading Compose. This is because, as of version 1.3, Compose uses Docker labels to keep track of containers, and so they need to be recreated with labels added.</p> <p>If Compose detects containers that were created without labels, it will refuse to run so that you don’t end up with two sets of them. If you want to keep using your existing containers (for example, because they have data volumes you want to preserve) you can use compose 1.5.x to migrate them with the following command:</p> <pre>$ docker-compose migrate-to-labels\n</pre> <p>Alternatively, if you’re not worried about keeping them, you can remove them. Compose will just create new ones.</p> <pre>$ docker rm -f -v myapp_web_1 myapp_db_1 ...\n</pre> <h2 id=\"uninstallation\">Uninstallation</h2> <p>To uninstall Docker Compose if you installed using <code>curl</code>:</p> <pre>$ rm /usr/local/bin/docker-compose\n</pre> <p>To uninstall Docker Compose if you installed using <code>pip</code>:</p> <pre>$ pip uninstall docker-compose\n</pre> <blockquote> <p><strong>Note</strong>: If you get a “Permission denied” error using either of the above methods, you probably do not have the proper permissions to remove <code>docker-compose</code>. To force the removal, prepend <code>sudo</code> to either of the above commands and run again.</p> </blockquote> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../gettingstarted/index\">Getting Started</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/install/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/install/</a>\n  </p>\n</div>\n","compose/gettingstarted/index":"<h1 id=\"getting-started\">Getting Started</h1> <p>On this page you build a simple Python web application running on Docker Compose. The application uses the Flask framework and increments a value in Redis. While the sample uses Python, the concepts demonstrated here should be understandable even if you’re not familiar with it.</p> <h2 id=\"prerequisites\">Prerequisites</h2> <p>Make sure you have already <a href=\"../install/index\">installed both Docker Engine and Docker Compose</a>. You don’t need to install Python, it is provided by a Docker image.</p> <h2 id=\"step-1-setup\">Step 1: Setup</h2> <ol> <li>\n<p>Create a directory for the project:</p> <pre>$ mkdir composetest\n$ cd composetest\n</pre>\n</li> <li>\n<p>With your favorite text editor create a file called <code>app.py</code> in your project directory.</p> <pre>from flask import Flask\nfrom redis import Redis\n\napp = Flask(__name__)\nredis = Redis(host='redis', port=6379)\n\n@app.route('/')\ndef hello():\n    redis.incr('hits')\n    return 'Hello World! I have been seen %s times.' % redis.get('hits')\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", debug=True)\n</pre>\n</li> <li>\n<p>Create another file called <code>requirements.txt</code> in your project directory and add the following:</p> <pre>flask\nredis\n</pre>\n</li> </ol> <p>These define the applications dependencies.</p> <h2 id=\"step-2-create-a-docker-image\">Step 2: Create a Docker image</h2> <p>In this step, you build a new Docker image. The image contains all the dependencies the Python application requires, including Python itself.</p> <ol> <li>\n<p>In your project directory create a file named <code>Dockerfile</code> and add the following:</p> <pre>FROM python:2.7\nADD . /code\nWORKDIR /code\nRUN pip install -r requirements.txt\nCMD python app.py\n</pre>\n</li> </ol> <p>This tells Docker to:</p> <ul> <li>Build an image starting with the Python 2.7 image.</li> <li>Add the current directory <code>.</code> into the path <code>/code</code> in the image.</li> <li>Set the working directory to <code>/code</code>.</li> <li>Install the Python dependencies.</li> <li>Set the default command for the container to <code>python app.py</code>\n</li> </ul> <p>For more information on how to write Dockerfiles, see the <a href=\"../../engine/userguide/containers/dockerimages/index#building-an-image-from-a-dockerfile\">Docker user guide</a> and the <a href=\"../../engine/reference/builder/index\">Dockerfile reference</a>.</p> <ol> <li>\n<p>Build the image.</p> <pre>$ docker build -t web .\n</pre>\n</li> </ol> <p>This command builds an image named <code>web</code> from the contents of the current directory. The command automatically locates the <code>Dockerfile</code>, <code>app.py</code>, and <code>requirements.txt</code> files.</p> <h2 id=\"step-3-define-services\">Step 3: Define services</h2> <p>Define a set of services using <code>docker-compose.yml</code>:</p> <ol> <li>\n<p>Create a file called docker-compose.yml in your project directory and add the following:</p> <pre>version: '2'\nservices:\n  web:\n    build: .\n    ports:\n     - \"5000:5000\"\n    volumes:\n     - .:/code\n    depends_on:\n     - redis\n  redis:\n    image: redis\n</pre>\n</li> </ol> <p>This Compose file defines two services, <code>web</code> and <code>redis</code>. The web service:</p> <ul> <li>Builds from the <code>Dockerfile</code> in the current directory.</li> <li>Forwards the exposed port 5000 on the container to port 5000 on the host machine.</li> <li>Mounts the project directory on the host to <code>/code</code> inside the container allowing you to modify the code without having to rebuild the image.</li> <li>Links the web service to the Redis service.</li> </ul> <p>The <code>redis</code> service uses the latest public <a href=\"https://registry.hub.docker.com/_/redis/\">Redis</a> image pulled from the Docker Hub registry.</p> <h2 id=\"step-4-build-and-run-your-app-with-compose\">Step 4: Build and run your app with Compose</h2> <ol> <li>\n<p>From your project directory, start up your application.</p> <pre>$ docker-compose up\nPulling image redis...\nBuilding web...\nStarting composetest_redis_1...\nStarting composetest_web_1...\nredis_1 | [8] 02 Jan 18:43:35.576 # Server started, Redis version 2.8.3\nweb_1   |  * Running on http://0.0.0.0:5000/\nweb_1   |  * Restarting with stat\n</pre>\n</li> </ol> <p>Compose pulls a Redis image, builds an image for your code, and start the services you defined.</p> <ol> <li>Enter <code>http://0.0.0.0:5000/</code> in a browser to see the application running.</li> </ol> <p>If you’re using Docker on Linux natively, then the web app should now be listening on port 5000 on your Docker daemon host. If <code>http://0.0.0.0:5000</code> doesn’t resolve, you can also try <code>http://localhost:5000</code>.</p> <p>If you’re using Docker Machine on a Mac, use <code>docker-machine ip MACHINE_VM</code> to get the IP address of your Docker host. Then, <code>open http://MACHINE_VM_IP:5000</code> in a browser.</p> <p>You should see a message in your browser saying:</p> <p><code>Hello World! I have been seen 1 times.</code></p> <ol> <li>Refresh the page.</li> </ol> <p>The number should increment.</p> <h2 id=\"step-5-experiment-with-some-other-commands\">Step 5: Experiment with some other commands</h2> <p>If you want to run your services in the background, you can pass the <code>-d</code> flag (for “detached” mode) to <code>docker-compose up</code> and use <code>docker-compose ps</code> to see what is currently running:</p> <pre>    $ docker-compose up -d\n    Starting composetest_redis_1...\n    Starting composetest_web_1...\n    $ docker-compose ps\n    Name                 Command            State       Ports\n    -------------------------------------------------------------------\n    composetest_redis_1   /usr/local/bin/run         Up\n    composetest_web_1     /bin/sh -c python app.py   Up      5000-&gt;5000/tcp\n</pre> <p>The <code>docker-compose run</code> command allows you to run one-off commands for your services. For example, to see what environment variables are available to the <code>web</code> service:</p> <pre>    $ docker-compose run web env\n</pre> <p>See <code>docker-compose --help</code> to see other available commands. You can also install <a href=\"../completion/index\">command completion</a> for the bash and zsh shell, which will also show you available commands.</p> <p>If you started Compose with <code>docker-compose up -d</code>, you’ll probably want to stop your services once you’ve finished with them:</p> <pre>    $ docker-compose stop\n</pre> <p>At this point, you have seen the basics of how Compose works.</p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li>Next, try the quick start guide for <a href=\"../django/index\">Django</a>, <a href=\"../rails/index\">Rails</a>, or <a href=\"../wordpress/index\">WordPress</a>.</li> <li><a href=\"../reference/index\">Explore the full list of Compose commands</a></li> <li><a href=\"../compose-file/index\">Compose configuration file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/gettingstarted/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/gettingstarted/</a>\n  </p>\n</div>\n","compose/swarm/index":"<h1 id=\"using-compose-with-swarm\">Using Compose with Swarm</h1> <p>Docker Compose and <a href=\"../../swarm/overview/index\">Docker Swarm</a> aim to have full integration, meaning you can point a Compose app at a Swarm cluster and have it all just work as if you were using a single Docker host.</p> <p>The actual extent of integration depends on which version of the <a href=\"../compose-file/index#versioning\">Compose file format</a> you are using:</p> <ol> <li><p>If you’re using version 1 along with <code>links</code>, your app will work, but Swarm will schedule all containers on one host, because links between containers do not work across hosts with the old networking system.</p></li> <li>\n<p>If you’re using version 2, your app should work with no changes:</p> <ul> <li><p>subject to the <a href=\"#limitations\">limitations</a> described below,</p></li> <li><p>as long as the Swarm cluster is configured to use the <a href=\"https://docs.docker.com/engine/userguide/networking/dockernetworks/#an-overlay-network\">overlay driver</a>, or a custom driver which supports multi-host networking.</p></li> </ul>\n</li> </ol> <p>Read <a href=\"https://docs.docker.com/engine/userguide/networking/get-started-overlay/\">Get started with multi-host networking</a> to see how to set up a Swarm cluster with <a href=\"../../machine/overview/index\">Docker Machine</a> and the overlay driver. Once you’ve got it running, deploying your app to it should be as simple as:</p> <pre>$ eval \"$(docker-machine env --swarm &lt;name of swarm master machine&gt;)\"\n$ docker-compose up\n</pre> <h2 id=\"limitations\">Limitations</h2> <h3 id=\"building-images\">Building images</h3> <p>Swarm can build an image from a Dockerfile just like a single-host Docker instance can, but the resulting image will only live on a single node and won’t be distributed to other nodes.</p> <p>If you want to use Compose to scale the service in question to multiple nodes, you’ll have to build it yourself, push it to a registry (e.g. the Docker Hub) and reference it from <code>docker-compose.yml</code>:</p> <pre>$ docker build -t myusername/web .\n$ docker push myusername/web\n\n$ cat docker-compose.yml\nweb:\n  image: myusername/web\n\n$ docker-compose up -d\n$ docker-compose scale web=3\n</pre> <h3 id=\"multiple-dependencies\">Multiple dependencies</h3> <p>If a service has multiple dependencies of the type which force co-scheduling (see <a href=\"#automatic-scheduling\">Automatic scheduling</a> below), it’s possible that Swarm will schedule the dependencies on different nodes, making the dependent service impossible to schedule. For example, here <code>foo</code> needs to be co-scheduled with <code>bar</code> and <code>baz</code>:</p> <pre>version: \"2\"\nservices:\n  foo:\n    image: foo\n    volumes_from: [\"bar\"]\n    network_mode: \"service:baz\"\n  bar:\n    image: bar\n  baz:\n    image: baz\n</pre> <p>The problem is that Swarm might first schedule <code>bar</code> and <code>baz</code> on different nodes (since they’re not dependent on one another), making it impossible to pick an appropriate node for <code>foo</code>.</p> <p>To work around this, use <a href=\"#manual-scheduling\">manual scheduling</a> to ensure that all three services end up on the same node:</p> <pre>version: \"2\"\nservices:\n  foo:\n    image: foo\n    volumes_from: [\"bar\"]\n    network_mode: \"service:baz\"\n    environment:\n      - \"constraint:node==node-1\"\n  bar:\n    image: bar\n    environment:\n      - \"constraint:node==node-1\"\n  baz:\n    image: baz\n    environment:\n      - \"constraint:node==node-1\"\n</pre> <h3 id=\"host-ports-and-recreating-containers\">Host ports and recreating containers</h3> <p>If a service maps a port from the host, e.g. <code>80:8000</code>, then you may get an error like this when running <code>docker-compose up</code> on it after the first time:</p> <pre>docker: Error response from daemon: unable to find a node that satisfies\ncontainer==6ab2dfe36615ae786ef3fc35d641a260e3ea9663d6e69c5b70ce0ca6cb373c02.\n</pre> <p>The usual cause of this error is that the container has a volume (defined either in its image or in the Compose file) without an explicit mapping, and so in order to preserve its data, Compose has directed Swarm to schedule the new container on the same node as the old container. This results in a port clash.</p> <p>There are two viable workarounds for this problem:</p> <ul> <li>\n<p>Specify a named volume, and use a volume driver which is capable of mounting the volume into the container regardless of what node it’s scheduled on.</p> <p>Compose does not give Swarm any specific scheduling instructions if a service uses only named volumes.</p> <pre>version: \"2\"\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"80:8000\"\n    volumes:\n      - web-logs:/var/log/web\n\nvolumes:\n  web-logs:\n    driver: custom-volume-driver\n</pre>\n</li> <li>\n<p>Remove the old container before creating the new one. You will lose any data in the volume.</p> <pre>$ docker-compose stop web\n$ docker-compose rm -f web\n$ docker-compose up web\n</pre>\n</li> </ul> <h2 id=\"scheduling-containers\">Scheduling containers</h2> <h3 id=\"automatic-scheduling\">Automatic scheduling</h3> <p>Some configuration options will result in containers being automatically scheduled on the same Swarm node to ensure that they work correctly. These are:</p> <ul> <li><p><code>network_mode: \"service:...\"</code> and <code>network_mode: \"container:...\"</code> (and <code>net: \"container:...\"</code> in the version 1 file format).</p></li> <li><p><code>volumes_from</code></p></li> <li><p><code>links</code></p></li> </ul> <h3 id=\"manual-scheduling\">Manual scheduling</h3> <p>Swarm offers a rich set of scheduling and affinity hints, enabling you to control where containers are located. They are specified via container environment variables, so you can use Compose’s <code>environment</code> option to set them.</p> <pre># Schedule containers on a specific node\nenvironment:\n  - \"constraint:node==node-1\"\n\n# Schedule containers on a node that has the 'storage' label set to 'ssd'\nenvironment:\n  - \"constraint:storage==ssd\"\n\n# Schedule containers where the 'redis' image is already pulled\nenvironment:\n  - \"affinity:image==redis\"\n</pre> <p>For the full set of available filters and expressions, see the <a href=\"../../swarm/scheduler/filter/index\">Swarm documentation</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/swarm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/swarm/</a>\n  </p>\n</div>\n","compose/django/index":"<h1 id=\"quickstart-docker-compose-and-django\">Quickstart: Docker Compose and Django</h1> <p>This quick-start guide demonstrates how to use Docker Compose to set up and run a simple Django/PostgreSQL app. Before starting, you’ll need to have <a href=\"../install/index\">Compose installed</a>.</p> <h3 id=\"define-the-project-components\">Define the project components</h3> <p>For this project, you need to create a Dockerfile, a Python dependencies file, and a <code>docker-compose.yml</code> file.</p> <ol> <li>\n<p>Create an empty project directory.</p> <p>You can name the directory something easy for you to remember. This directory is the context for your application image. The directory should only contain resources to build that image.</p>\n</li> <li>\n<p>Create a new file called <code>Dockerfile</code> in your project directory.</p> <p>The Dockerfile defines an application’s image content via one or more build commands that configure that image. Once built, you can run the image in a container. For more information on <code>Dockerfiles</code>, see the <a href=\"../../engine/userguide/containers/dockerimages/index#building-an-image-from-a-dockerfile\">Docker user guide</a> and the <a href=\"../../engine/reference/builder/index\">Dockerfile reference</a>.</p>\n</li> <li>\n<p>Add the following content to the <code>Dockerfile</code>.</p> <pre>FROM python:2.7\nENV PYTHONUNBUFFERED 1\nRUN mkdir /code\nWORKDIR /code\nADD requirements.txt /code/\nRUN pip install -r requirements.txt\nADD . /code/\n</pre> <p>This <code>Dockerfile</code> starts with a Python 2.7 base image. The base image is modified by adding a new <code>code</code> directory. The base image is further modified by installing the Python requirements defined in the <code>requirements.txt</code> file.</p>\n</li> <li><p>Save and close the <code>Dockerfile</code>.</p></li> <li>\n<p>Create a <code>requirements.txt</code> in your project directory.</p> <p>This file is used by the <code>RUN pip install -r requirements.txt</code> command in your <code>Dockerfile</code>.</p>\n</li> <li>\n<p>Add the required software in the file.</p> <pre>Django\npsycopg2\n</pre>\n</li> <li><p>Save and close the <code>requirements.txt</code> file.</p></li> <li>\n<p>Create a file called <code>docker-compose.yml</code> in your project directory.</p> <p>The <code>docker-compose.yml</code> file describes the services that make your app. In this example those services are a web server and database. The compose file also describes which Docker images these services use, how they link together, any volumes they might need mounted inside the containers. Finally, the <code>docker-compose.yml</code> file describes which ports these services expose. See the <a href=\"../compose-file/index\"><code>docker-compose.yml</code> reference</a> for more information on how this file works.</p>\n</li> <li>\n<p>Add the following configuration to the file.</p> <pre>version: '2'\nservices:\n  db:\n    image: postgres\n  web:\n    build: .\n    command: python manage.py runserver 0.0.0.0:8000\n    volumes:\n      - .:/code\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - db\n</pre> <p>This file defines two services: The <code>db</code> service and the <code>web</code> service.</p>\n</li> <li><p>Save and close the <code>docker-compose.yml</code> file.</p></li> </ol> <h3 id=\"create-a-django-project\">Create a Django project</h3> <p>In this step, you create a Django started project by building the image from the build context defined in the previous procedure.</p> <ol> <li><p>Change to the root of your project directory.</p></li> <li>\n<p>Create the Django project using the <code>docker-compose</code> command.</p> <pre>$ docker-compose run web django-admin.py startproject composeexample .\n</pre> <p>This instructs Compose to run <code>django-admin.py startproject composeeexample</code> in a container, using the <code>web</code> service’s image and configuration. Because the <code>web</code> image doesn’t exist yet, Compose builds it from the current directory, as specified by the <code>build: .</code> line in <code>docker-compose.yml</code>.</p> <p>Once the <code>web</code> service image is built, Compose runs it and executes the <code>django-admin.py startproject</code> command in the container. This command instructs Django to create a set of files and directories representing a Django project.</p>\n</li> <li>\n<p>After the <code>docker-compose</code> command completes, list the contents of your project.</p> <pre>$ ls -l\ndrwxr-xr-x 2 root   root   composeexample\n-rw-rw-r-- 1 user   user   docker-compose.yml\n-rw-rw-r-- 1 user   user   Dockerfile\n-rwxr-xr-x 1 root   root   manage.py\n-rw-rw-r-- 1 user   user   requirements.txt\n</pre>\n</li> </ol> <p>If you are running Docker on Linux, the files <code>django-admin</code> created are owned by root. This happens because the container runs as the root user. Change the ownership of the the new files.</p> <pre>      sudo chown -R $USER:$USER .\n</pre> <p>If you are running Docker on Mac or Windows, you should already have ownership of all files, including those generated by <code>django-admin</code>. List the files just verify this.</p> <pre>    $ ls -l\n    total 32\n    -rw-r--r--  1 user  staff  145 Feb 13 23:00 Dockerfile\n    drwxr-xr-x  6 user  staff  204 Feb 13 23:07 composeexample\n    -rw-r--r--  1 user  staff  159 Feb 13 23:02 docker-compose.yml\n    -rwxr-xr-x  1 user  staff  257 Feb 13 23:07 manage.py\n    -rw-r--r--  1 user  staff   16 Feb 13 23:01 requirements.txt\n</pre> <h3 id=\"connect-the-database\">Connect the database</h3> <p>In this section, you set up the database connection for Django.</p> <ol> <li><p>In your project directory, edit the <code>composeexample/settings.py</code> file.</p></li> <li>\n<p>Replace the <code>DATABASES = ...</code> with the following:</p> <pre>DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'postgres',\n        'USER': 'postgres',\n        'HOST': 'db',\n        'PORT': 5432,\n    }\n}\n</pre> <p>These settings are determined by the <a href=\"https://hub.docker.com/_/postgres/\">postgres</a> Docker image specified in <code>docker-compose.yml</code>.</p>\n</li> <li><p>Save and close the file.</p></li> <li>\n<p>Run the <code>docker-compose up</code> command.</p> <pre>$ docker-compose up\nStarting composepractice_db_1...\nStarting composepractice_web_1...\nAttaching to composepractice_db_1, composepractice_web_1\n...\ndb_1  | PostgreSQL init process complete; ready for start up.\n...\ndb_1  | LOG:  database system is ready to accept connections\ndb_1  | LOG:  autovacuum launcher started\n..\nweb_1 | Django version 1.8.4, using settings 'composeexample.settings'\nweb_1 | Starting development server at http://0.0.0.0:8000/\nweb_1 | Quit the server with CONTROL-C.\n</pre> <p>At this point, your Django app should be running at port <code>8000</code> on your Docker host. If you are using a Docker Machine VM, you can use the <code>docker-machine ip MACHINE_NAME</code> to get the IP address.</p> <p><img src=\"https://docs.docker.com/v1.11/compose/images/django-it-worked.png\" alt=\"Django example\"></p>\n</li> </ol> <h2 id=\"more-compose-documentation\">More Compose documentation</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../gettingstarted/index\">Getting Started</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/django/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/django/</a>\n  </p>\n</div>\n","compose/rails/index":"<h1 id=\"quickstart-docker-compose-and-rails\">Quickstart: Docker Compose and Rails</h1> <p>This Quickstart guide will show you how to use Docker Compose to set up and run a Rails/PostgreSQL app. Before starting, you’ll need to have <a href=\"../install/index\">Compose installed</a>.</p> <h3 id=\"define-the-project\">Define the project</h3> <p>Start by setting up the three files you’ll need to build the app. First, since your app is going to run inside a Docker container containing all of its dependencies, you’ll need to define exactly what needs to be included in the container. This is done using a file called <code>Dockerfile</code>. To begin with, the Dockerfile consists of:</p> <pre>FROM ruby:2.2.0\nRUN apt-get update -qq &amp;&amp; apt-get install -y build-essential libpq-dev nodejs\nRUN mkdir /myapp\nWORKDIR /myapp\nADD Gemfile /myapp/Gemfile\nADD Gemfile.lock /myapp/Gemfile.lock\nRUN bundle install\nADD . /myapp\n</pre> <p>That’ll put your application code inside an image that will build a container with Ruby, Bundler and all your dependencies inside it. For more information on how to write Dockerfiles, see the <a href=\"../../engine/userguide/containers/dockerimages/index#building-an-image-from-a-dockerfile\">Docker user guide</a> and the <a href=\"../../engine/reference/builder/index\">Dockerfile reference</a>.</p> <p>Next, create a bootstrap <code>Gemfile</code> which just loads Rails. It’ll be overwritten in a moment by <code>rails new</code>.</p> <pre>source 'https://rubygems.org'\ngem 'rails', '4.2.0'\n</pre> <p>You’ll need an empty <code>Gemfile.lock</code> in order to build our <code>Dockerfile</code>.</p> <pre>$ touch Gemfile.lock\n</pre> <p>Finally, <code>docker-compose.yml</code> is where the magic happens. This file describes the services that comprise your app (a database and a web app), how to get each one’s Docker image (the database just runs on a pre-made PostgreSQL image, and the web app is built from the current directory), and the configuration needed to link them together and expose the web app’s port.</p> <pre>version: '2'\nservices:\n  db:\n    image: postgres\n  web:\n    build: .\n    command: bundle exec rails s -p 3000 -b '0.0.0.0'\n    volumes:\n      - .:/myapp\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - db\n</pre> <h3 id=\"build-the-project\">Build the project</h3> <p>With those three files in place, you can now generate the Rails skeleton app using <code>docker-compose run</code>:</p> <pre>$ docker-compose run web rails new . --force --database=postgresql --skip-bundle\n</pre> <p>First, Compose will build the image for the <code>web</code> service using the <code>Dockerfile</code>. Then it’ll run <code>rails new</code> inside a new container, using that image. Once it’s done, you should have generated a fresh app:</p> <pre>  $ ls -l\n  total 56\n  -rw-r--r--   1 user  staff   215 Feb 13 23:33 Dockerfile\n  -rw-r--r--   1 user  staff  1480 Feb 13 23:43 Gemfile\n  -rw-r--r--   1 user  staff  2535 Feb 13 23:43 Gemfile.lock\n  -rw-r--r--   1 root  root   478 Feb 13 23:43 README.rdoc\n  -rw-r--r--   1 root  root   249 Feb 13 23:43 Rakefile\n  drwxr-xr-x   8 root  root   272 Feb 13 23:43 app\n  drwxr-xr-x   6 root  root   204 Feb 13 23:43 bin\n  drwxr-xr-x  11 root  root   374 Feb 13 23:43 config\n  -rw-r--r--   1 root  root   153 Feb 13 23:43 config.ru\n  drwxr-xr-x   3 root  root   102 Feb 13 23:43 db\n  -rw-r--r--   1 user  staff   161 Feb 13 23:35 docker-compose.yml\n  drwxr-xr-x   4 root  root   136 Feb 13 23:43 lib\n  drwxr-xr-x   3 root  root   102 Feb 13 23:43 log\n  drwxr-xr-x   7 root  root   238 Feb 13 23:43 public\n  drwxr-xr-x   9 root  root   306 Feb 13 23:43 test\n  drwxr-xr-x   3 root  root   102 Feb 13 23:43 tmp\n  drwxr-xr-x   3 root  root   102 Feb 13 23:43 vendor\n</pre> <p>If you are running Docker on Linux, the files <code>rails new</code> created are owned by root. This happens because the container runs as the root user. Change the ownership of the the new files.</p> <pre>  sudo chown -R $USER:$USER .\n</pre> <p>If you are running Docker on Mac or Windows, you should already have ownership of all files, including those generated by <code>rails new</code>. List the files just to verify this.</p> <p>Uncomment the line in your new <code>Gemfile</code> which loads <code>therubyracer</code>, so you’ve got a Javascript runtime:</p> <pre>gem 'therubyracer', platforms: :ruby\n</pre> <p>Now that you’ve got a new <code>Gemfile</code>, you need to build the image again. (This, and changes to the Dockerfile itself, should be the only times you’ll need to rebuild.)</p> <pre>$ docker-compose build\n</pre> <h3 id=\"connect-the-database\">Connect the database</h3> <p>The app is now bootable, but you’re not quite there yet. By default, Rails expects a database to be running on <code>localhost</code> - so you need to point it at the <code>db</code> container instead. You also need to change the database and username to align with the defaults set by the <code>postgres</code> image.</p> <p>Replace the contents of <code>config/database.yml</code> with the following:</p> <pre>development: &amp;default\n  adapter: postgresql\n  encoding: unicode\n  database: postgres\n  pool: 5\n  username: postgres\n  password:\n  host: db\n\ntest:\n  &lt;&lt;: *default\n  database: myapp_test\n</pre> <p>You can now boot the app with:</p> <pre>$ docker-compose up\n</pre> <p>If all’s well, you should see some PostgreSQL output, and then—after a few seconds—the familiar refrain:</p> <pre>myapp_web_1 | [2014-01-17 17:16:29] INFO  WEBrick 1.3.1\nmyapp_web_1 | [2014-01-17 17:16:29] INFO  ruby 2.2.0 (2014-12-25) [x86_64-linux-gnu]\nmyapp_web_1 | [2014-01-17 17:16:29] INFO  WEBrick::HTTPServer#start: pid=1 port=3000\n</pre> <p>Finally, you need to create the database. In another terminal, run:</p> <pre>$ docker-compose run web rake db:create\n</pre> <p>That’s it. Your app should now be running on port 3000 on your Docker daemon. If you’re using <a href=\"../../machine/overview/index\">Docker Machine</a>, then <code>docker-machine ip MACHINE_VM</code> returns the Docker host IP address.</p> <p><img src=\"https://docs.docker.com/v1.11/compose/images/rails-welcome.png\" alt=\"Rails example\"></p> <blockquote> <p><strong>Note</strong>: If you stop the example application and attempt to restart it, you might get the following error: <code>web_1 | A server is already running. Check\n/myapp/tmp/pids/server.pid.</code> One way to resolve this is to delete the file <code>tmp/pids/server.pid</code>, and then re-start the application with <code>docker-compose\nup</code>.</p> </blockquote> <h2 id=\"more-compose-documentation\">More Compose documentation</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../gettingstarted/index\">Getting Started</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/rails/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/rails/</a>\n  </p>\n</div>\n","compose/env-file/index":"<h1 id=\"environment-file\">Environment file</h1> <p>Compose supports declaring default environment variables in an environment file named <code>.env</code> placed in the folder <code>docker-compose</code> command is executed from <em>(current working directory)</em>.</p> <p>Compose expects each line in an env file to be in <code>VAR=VAL</code> format. Lines beginning with <code>#</code> (i.e. comments) are ignored, as are blank lines.</p> <blockquote> <p>Note: Values present in the environment at runtime will always override those defined inside the <code>.env</code> file. Similarly, values passed via command-line arguments take precedence as well.</p> </blockquote> <p>Those environment variables will be used for <a href=\"../compose-file/index#variable-substitution\">variable substitution</a> in your Compose file, but can also be used to define the following <a href=\"../reference/envvars/index\">CLI variables</a>:</p> <ul> <li><code>COMPOSE_API_VERSION</code></li> <li><code>COMPOSE_FILE</code></li> <li><code>COMPOSE_HTTP_TIMEOUT</code></li> <li><code>COMPOSE_PROJECT_NAME</code></li> <li><code>DOCKER_CERT_PATH</code></li> <li><code>DOCKER_HOST</code></li> <li><code>DOCKER_TLS_VERIFY</code></li> </ul> <h2 id=\"more-compose-documentation\">More Compose documentation</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/env-file/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/env-file/</a>\n  </p>\n</div>\n","compose/wordpress/index":"<h1 id=\"quickstart-docker-compose-and-wordpress\">Quickstart: Docker Compose and WordPress</h1> <p>You can use Docker Compose to easily run WordPress in an isolated environment built with Docker containers. This quick-start guide demonstrates how to use Compose to set up and run WordPress. Before starting, you’ll need to have <a href=\"../install/index\">Compose installed</a>.</p> <h3 id=\"define-the-project\">Define the project</h3> <ol> <li>\n<p>Create an empty project directory.</p> <p>You can name the directory something easy for you to remember. This directory is the context for your application image. The directory should only contain resources to build that image.</p> <p>This project directory will contain a <code>docker-compose.yaml</code> file which will be complete in itself for a good starter wordpress project.</p>\n</li> <li>\n<p>Change directories into your project directory.</p> <p>For example, if you named your directory <code>my_wordpress</code>:</p> <pre>$ cd my-wordpress/\n</pre>\n</li> <li>\n<p>Create a <code>docker-compose.yml</code> file that will start your <code>Wordpress</code> blog and a separate <code>MySQL</code> instance with a volume mount for data persistence:</p> <pre>version: '2'\nservices:\n  db:\n    image: mysql:5.7\n    volumes:\n      - \"./.data/db:/var/lib/mysql\"\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: wordpress\n      MYSQL_DATABASE: wordpress\n      MYSQL_USER: wordpress\n      MYSQL_PASSWORD: wordpress\n\n  wordpress:\n    depends_on:\n      - db\n    image: wordpress:latest\n    links:\n      - db\n    ports:\n      - \"8000:80\"\n    restart: always\n    environment:\n      WORDPRESS_DB_HOST: db:3306\n      WORDPRESS_DB_PASSWORD: wordpress\n</pre> <p><strong>NOTE</strong>: The folder <code>./.data/db</code> will be automatically created in the project directory alongside the <code>docker-compose.yml</code> which will persist any updates made by wordpress to the database.</p>\n</li> </ol> <h3 id=\"build-the-project\">Build the project</h3> <p>Now, run <code>docker-compose up -d</code> from your project directory.</p> <p>This pulls the needed images, and starts the wordpress and database containers, as shown in the example below.</p> <pre>$ docker-compose up -d\nCreating network \"my_wordpress_default\" with the default driver\nPulling db (mysql:5.7)...\n5.7: Pulling from library/mysql\nefd26ecc9548: Pull complete\na3ed95caeb02: Pull complete\n...\nDigest: sha256:34a0aca88e85f2efa5edff1cea77cf5d3147ad93545dbec99cfe705b03c520de\nStatus: Downloaded newer image for mysql:5.7\nPulling wordpress (wordpress:latest)...\nlatest: Pulling from library/wordpress\nefd26ecc9548: Already exists\na3ed95caeb02: Pull complete\n589a9d9a7c64: Pull complete\n...\nDigest: sha256:ed28506ae44d5def89075fd5c01456610cd6c64006addfe5210b8c675881aff6\nStatus: Downloaded newer image for wordpress:latest\nCreating my_wordpress_db_1\nCreating my_wordpress_wordpress_1\n</pre> <h3 id=\"bring-up-wordpress-in-a-web-browser\">Bring up WordPress in a web browser</h3> <p>If you’re using <a href=\"https://docs.docker.com/machine/\">Docker Machine</a>, then <code>docker-machine ip MACHINE_VM</code> gives you the machine address and you can open <code>http://MACHINE_VM_IP:8000</code> in a browser.</p> <p>At this point, WordPress should be running on port <code>8000</code> of your Docker Host, and you can complete the “famous five-minute installation” as a WordPress administrator.</p> <p><strong>NOTE</strong>: The Wordpress site will not be immediately available on port <code>8000</code> because the containers are still being initialized and may take a couple of minutes before the first load.</p> <p><img src=\"https://docs.docker.com/v1.11/compose/images/wordpress-lang.png\" alt=\"Choose language for WordPress install\"></p> <p><img src=\"https://docs.docker.com/v1.11/compose/images/wordpress-welcome.png\" alt=\"WordPress Welcome\"></p> <h2 id=\"more-compose-documentation\">More Compose documentation</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../gettingstarted/index\">Getting Started</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/wordpress/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/wordpress/</a>\n  </p>\n</div>\n","compose/extends/index":"<h1 id=\"extending-services-and-compose-files\">Extending services and Compose files</h1> <p>Compose supports two methods of sharing common configuration:</p> <ol> <li>Extending an entire Compose file by <a href=\"#multiple-compose-files\">using multiple Compose files</a>\n</li> <li>Extending individual services with <a href=\"#extending-services\">the <code>extends</code> field</a>\n</li> </ol> <h2 id=\"multiple-compose-files\">Multiple Compose files</h2> <p>Using multiple Compose files enables you to customize a Compose application for different environments or different workflows.</p> <h3 id=\"understanding-multiple-compose-files\">Understanding multiple Compose files</h3> <p>By default, Compose reads two files, a <code>docker-compose.yml</code> and an optional <code>docker-compose.override.yml</code> file. By convention, the <code>docker-compose.yml</code> contains your base configuration. The override file, as its name implies, can contain configuration overrides for existing services or entirely new services.</p> <p>If a service is defined in both files Compose merges the configurations using the rules described in <a href=\"#adding-and-overriding-configuration\">Adding and overriding configuration</a>.</p> <p>To use multiple override files, or an override file with a different name, you can use the <code>-f</code> option to specify the list of files. Compose merges files in the order they’re specified on the command line. See the <a href=\"../reference/overview/index\"><code>docker-compose</code> command reference</a> for more information about using <code>-f</code>.</p> <p>When you use multiple configuration files, you must make sure all paths in the files are relative to the base Compose file (the first Compose file specified with <code>-f</code>). This is required because override files need not be valid Compose files. Override files can contain small fragments of configuration. Tracking which fragment of a service is relative to which path is difficult and confusing, so to keep paths easier to understand, all paths must be defined relative to the base file.</p> <h3 id=\"example-use-case\">Example use case</h3> <p>In this section are two common use cases for multiple compose files: changing a Compose app for different environments, and running administrative tasks against a Compose app.</p> <h4 id=\"different-environments\">Different environments</h4> <p>A common use case for multiple files is changing a development Compose app for a production-like environment (which may be production, staging or CI). To support these differences, you can split your Compose configuration into a few different files:</p> <p>Start with a base file that defines the canonical configuration for the services.</p> <p><strong>docker-compose.yml</strong></p> <pre>web:\n  image: example/my_web_app:latest\n  links:\n    - db\n    - cache\n\ndb:\n  image: postgres:latest\n\ncache:\n  image: redis:latest\n</pre> <p>In this example the development configuration exposes some ports to the host, mounts our code as a volume, and builds the web image.</p> <p><strong>docker-compose.override.yml</strong></p> <pre>web:\n  build: .\n  volumes:\n    - '.:/code'\n  ports:\n    - 8883:80\n  environment:\n    DEBUG: 'true'\n\ndb:\n  command: '-d'\n  ports:\n    - 5432:5432\n\ncache:\n  ports:\n    - 6379:6379\n</pre> <p>When you run <code>docker-compose up</code> it reads the overrides automatically.</p> <p>Now, it would be nice to use this Compose app in a production environment. So, create another override file (which might be stored in a different git repo or managed by a different team).</p> <p><strong>docker-compose.prod.yml</strong></p> <pre>web:\n  ports:\n    - 80:80\n  environment:\n    PRODUCTION: 'true'\n\ncache:\n  environment:\n    TTL: '500'\n</pre> <p>To deploy with this production Compose file you can run</p> <pre>docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n</pre> <p>This deploys all three services using the configuration in <code>docker-compose.yml</code> and <code>docker-compose.prod.yml</code> (but not the dev configuration in <code>docker-compose.override.yml</code>).</p> <p>See <a href=\"../production/index\">production</a> for more information about Compose in production.</p> <h4 id=\"administrative-tasks\">Administrative tasks</h4> <p>Another common use case is running adhoc or administrative tasks against one or more services in a Compose app. This example demonstrates running a database backup.</p> <p>Start with a <strong>docker-compose.yml</strong>.</p> <pre>web:\n  image: example/my_web_app:latest\n  links:\n    - db\n\ndb:\n  image: postgres:latest\n</pre> <p>In a <strong>docker-compose.admin.yml</strong> add a new service to run the database export or backup.</p> <pre>dbadmin:\n  build: database_admin/\n  links:\n    - db\n</pre> <p>To start a normal environment run <code>docker-compose up -d</code>. To run a database backup, include the <code>docker-compose.admin.yml</code> as well.</p> <pre>docker-compose -f docker-compose.yml -f docker-compose.admin.yml \\\n    run dbadmin db-backup\n</pre> <h2 id=\"extending-services\">Extending services</h2> <p>Docker Compose’s <code>extends</code> keyword enables sharing of common configurations among different files, or even different projects entirely. Extending services is useful if you have several services that reuse a common set of configuration options. Using <code>extends</code> you can define a common set of service options in one place and refer to it from anywhere.</p> <blockquote> <p><strong>Note:</strong> <code>links</code>, <code>volumes_from</code>, and <code>depends_on</code> are never shared between services using &gt;<code>extends</code>. These exceptions exist to avoid implicit dependencies—you always define <code>links</code> and <code>volumes_from</code> locally. This ensures dependencies between services are clearly visible when reading the current file. Defining these locally also ensures changes to the referenced file don’t result in breakage.</p> </blockquote> <h3 id=\"understand-the-extends-configuration\">Understand the extends configuration</h3> <p>When defining any service in <code>docker-compose.yml</code>, you can declare that you are extending another service like this:</p> <pre>web:\n  extends:\n    file: common-services.yml\n    service: webapp\n</pre> <p>This instructs Compose to re-use the configuration for the <code>webapp</code> service defined in the <code>common-services.yml</code> file. Suppose that <code>common-services.yml</code> looks like this:</p> <pre>webapp:\n  build: .\n  ports:\n    - \"8000:8000\"\n  volumes:\n    - \"/data\"\n</pre> <p>In this case, you’ll get exactly the same result as if you wrote <code>docker-compose.yml</code> with the same <code>build</code>, <code>ports</code> and <code>volumes</code> configuration values defined directly under <code>web</code>.</p> <p>You can go further and define (or re-define) configuration locally in <code>docker-compose.yml</code>:</p> <pre>web:\n  extends:\n    file: common-services.yml\n    service: webapp\n  environment:\n    - DEBUG=1\n  cpu_shares: 5\n\nimportant_web:\n  extends: web\n  cpu_shares: 10\n</pre> <p>You can also write other services and link your <code>web</code> service to them:</p> <pre>web:\n  extends:\n    file: common-services.yml\n    service: webapp\n  environment:\n    - DEBUG=1\n  cpu_shares: 5\n  links:\n    - db\ndb:\n  image: postgres\n</pre> <h3 id=\"example-use-case-1\">Example use case</h3> <p>Extending an individual service is useful when you have multiple services that have a common configuration. The example below is a Compose app with two services: a web application and a queue worker. Both services use the same codebase and share many configuration options.</p> <p>In a <strong>common.yml</strong> we define the common configuration:</p> <pre>app:\n  build: .\n  environment:\n    CONFIG_FILE_PATH: /code/config\n    API_KEY: xxxyyy\n  cpu_shares: 5\n</pre> <p>In a <strong>docker-compose.yml</strong> we define the concrete services which use the common configuration:</p> <pre>webapp:\n  extends:\n    file: common.yml\n    service: app\n  command: /code/run_web_app\n  ports:\n    - 8080:8080\n  links:\n    - queue\n    - db\n\nqueue_worker:\n  extends:\n    file: common.yml\n    service: app\n  command: /code/run_worker\n  links:\n    - queue\n</pre> <h2 id=\"adding-and-overriding-configuration\">Adding and overriding configuration</h2> <p>Compose copies configurations from the original service over to the local one. If a configuration option is defined in both the original service the local service, the local value <em>replaces</em> or <em>extends</em> the original value.</p> <p>For single-value options like <code>image</code>, <code>command</code> or <code>mem_limit</code>, the new value replaces the old value.</p> <pre># original service\ncommand: python app.py\n\n# local service\ncommand: python otherapp.py\n\n# result\ncommand: python otherapp.py\n</pre> <blockquote> <p><strong>Note:</strong> In the case of <code>build</code> and <code>image</code>, when using <a href=\"../compose-file/index#version-1\">version 1 of the Compose file format</a>, using one option in the local service causes Compose to discard the other option if it was defined in the original service.</p> <p>For example, if the original service defines <code>image: webapp</code> and the local service defines <code>build: .</code> then the resulting service will have <code>build: .</code> and no <code>image</code> option.</p> <p>This is because <code>build</code> and <code>image</code> cannot be used together in a version 1 file.</p> </blockquote> <p>For the <strong>multi-value options</strong> <code>ports</code>, <code>expose</code>, <code>external_links</code>, <code>dns</code>, <code>dns_search</code>, and <code>tmpfs</code>, Compose concatenates both sets of values:</p> <pre># original service\nexpose:\n  - \"3000\"\n\n# local service\nexpose:\n  - \"4000\"\n  - \"5000\"\n\n# result\nexpose:\n  - \"3000\"\n  - \"4000\"\n  - \"5000\"\n</pre> <p>In the case of <code>environment</code>, <code>labels</code>, <code>volumes</code> and <code>devices</code>, Compose “merges” entries together with locally-defined values taking precedence:</p> <pre># original service\nenvironment:\n  - FOO=original\n  - BAR=original\n\n# local service\nenvironment:\n  - BAR=local\n  - BAZ=local\n\n# result\nenvironment:\n  - FOO=original\n  - BAR=local\n  - BAZ=local\n</pre> <h2 id=\"compose-documentation\">Compose documentation</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../gettingstarted/index\">Getting Started</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/extends/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/extends/</a>\n  </p>\n</div>\n","compose/production/index":"<h1 id=\"using-compose-in-production\">Using Compose in production</h1> <p>When you define your app with Compose in development, you can use this definition to run your application in different environments such as CI, staging, and production.</p> <p>The easiest way to deploy an application is to run it on a single server, similar to how you would run your development environment. If you want to scale up your application, you can run Compose apps on a Swarm cluster.</p> <h3 id=\"modify-your-compose-file-for-production\">Modify your Compose file for production</h3> <p>You’ll almost certainly want to make changes to your app configuration that are more appropriate to a live environment. These changes may include:</p> <ul> <li>Removing any volume bindings for application code, so that code stays inside the container and can’t be changed from outside</li> <li>Binding to different ports on the host</li> <li>Setting environment variables differently (e.g., to decrease the verbosity of logging, or to enable email sending)</li> <li>Specifying a restart policy (e.g., <code>restart: always</code>) to avoid downtime</li> <li>Adding extra services (e.g., a log aggregator)</li> </ul> <p>For this reason, you’ll probably want to define an additional Compose file, say <code>production.yml</code>, which specifies production-appropriate configuration. This configuration file only needs to include the changes you’d like to make from the original Compose file. The additional Compose file can be applied over the original <code>docker-compose.yml</code> to create a new configuration.</p> <p>Once you’ve got a second configuration file, tell Compose to use it with the <code>-f</code> option:</p> <pre>$ docker-compose -f docker-compose.yml -f production.yml up -d\n</pre> <p>See <a href=\"../extends/index#different-environments\">Using multiple compose files</a> for a more complete example.</p> <h3 id=\"deploying-changes\">Deploying changes</h3> <p>When you make changes to your app code, you’ll need to rebuild your image and recreate your app’s containers. To redeploy a service called <code>web</code>, you would use:</p> <pre>$ docker-compose build web\n$ docker-compose up --no-deps -d web\n</pre> <p>This will first rebuild the image for <code>web</code> and then stop, destroy, and recreate <em>just</em> the <code>web</code> service. The <code>--no-deps</code> flag prevents Compose from also recreating any services which <code>web</code> depends on.</p> <h3 id=\"running-compose-on-a-single-server\">Running Compose on a single server</h3> <p>You can use Compose to deploy an app to a remote Docker host by setting the <code>DOCKER_HOST</code>, <code>DOCKER_TLS_VERIFY</code>, and <code>DOCKER_CERT_PATH</code> environment variables appropriately. For tasks like this, <a href=\"../../machine/overview/index\">Docker Machine</a> makes managing local and remote Docker hosts very easy, and is recommended even if you’re not deploying remotely.</p> <p>Once you’ve set up your environment variables, all the normal <code>docker-compose</code> commands will work with no further configuration.</p> <h3 id=\"running-compose-on-a-swarm-cluster\">Running Compose on a Swarm cluster</h3> <p><a href=\"../../swarm/overview/index\">Docker Swarm</a>, a Docker-native clustering system, exposes the same API as a single Docker host, which means you can use Compose against a Swarm instance and run your apps across multiple hosts.</p> <p>Read more about the Compose/Swarm integration in the <a href=\"../swarm/index\">integration guide</a>.</p> <h2 id=\"compose-documentation\">Compose documentation</h2> <ul> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/production/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/production/</a>\n  </p>\n</div>\n","compose/reference/overview/index":"<h1 id=\"overview-of-docker-compose-cli\">Overview of docker-compose CLI</h1> <p>This page provides the usage information for the <code>docker-compose</code> Command. You can also see this information by running <code>docker-compose --help</code> from the command line.</p> <pre>Define and run multi-container applications with Docker.\n\nUsage:\n  docker-compose [-f=&lt;arg&gt;...] [options] [COMMAND] [ARGS...]\n  docker-compose -h|--help\n\nOptions:\n  -f, --file FILE             Specify an alternate compose file (default: docker-compose.yml)\n  -p, --project-name NAME     Specify an alternate project name (default: directory name)\n  --verbose                   Show more output\n  -v, --version               Print version and exit\n  -H, --host HOST             Daemon socket to connect to\n\n  --tls                       Use TLS; implied by --tlsverify\n  --tlscacert CA_PATH         Trust certs signed only by this CA\n  --tlscert CLIENT_CERT_PATH  Path to TLS certificate file\n  --tlskey TLS_KEY_PATH       Path to TLS key file\n  --tlsverify                 Use TLS and verify the remote\n  --skip-hostname-check       Don't check the daemon's hostname against the name specified\n                              in the client certificate (for example if your docker host\n                              is an IP address)\n\nCommands:\n  build              Build or rebuild services\n  config             Validate and view the compose file\n  create             Create services\n  down               Stop and remove containers, networks, images, and volumes\n  events             Receive real time events from containers\n  help               Get help on a command\n  kill               Kill containers\n  logs               View output from containers\n  pause              Pause services\n  port               Print the public port for a port binding\n  ps                 List containers\n  pull               Pulls service images\n  restart            Restart services\n  rm                 Remove stopped containers\n  run                Run a one-off command\n  scale              Set number of containers for a service\n  start              Start services\n  stop               Stop services\n  unpause            Unpause services\n  up                 Create and start containers\n  version            Show the Docker-Compose version information\n\n</pre> <p>The Docker Compose binary. You use this command to build and manage multiple services in Docker containers.</p> <p>Use the <code>-f</code> flag to specify the location of a Compose configuration file. You can supply multiple <code>-f</code> configuration files. When you supply multiple files, Compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their successors.</p> <p>For example, consider this command line:</p> <pre>$ docker-compose -f docker-compose.yml -f docker-compose.admin.yml run backup_db`\n</pre> <p>The <code>docker-compose.yml</code> file might specify a <code>webapp</code> service.</p> <pre>webapp:\n  image: examples/web\n  ports:\n    - \"8000:8000\"\n  volumes:\n    - \"/data\"\n</pre> <p>If the <code>docker-compose.admin.yml</code> also specifies this same service, any matching fields will override the previous file. New values, add to the <code>webapp</code> service configuration.</p> <pre>webapp:\n  build: .\n  environment:\n    - DEBUG=1\n</pre> <p>Use a <code>-f</code> with <code>-</code> (dash) as the filename to read the configuration from stdin. When stdin is used all paths in the configuration are relative to the current working directory.</p> <p>The <code>-f</code> flag is optional. If you don’t provide this flag on the command line, Compose traverses the working directory and its parent directories looking for a <code>docker-compose.yml</code> and a <code>docker-compose.override.yml</code> file. You must supply at least the <code>docker-compose.yml</code> file. If both files are present on the same directory level, Compose combines the two files into a single configuration. The configuration in the <code>docker-compose.override.yml</code> file is applied over and in addition to the values in the <code>docker-compose.yml</code> file.</p> <p>See also the <code>COMPOSE_FILE</code> <a href=\"../envvars/index#compose-file\">environment variable</a>.</p> <p>Each configuration has a project name. If you supply a <code>-p</code> flag, you can specify a project name. If you don’t specify the flag, Compose uses the current directory name. See also the <code>COMPOSE_PROJECT_NAME</code> <a href=\"../envvars/index#compose-project-name\">environment variable</a></p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><a href=\"../envvars/index\">CLI environment variables</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/overview/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/overview/</a>\n  </p>\n</div>\n","compose/reference/build/index":"<h1 id=\"build\">build</h1> <pre>Usage: build [options] [SERVICE...]\n\nOptions:\n--force-rm  Always remove intermediate containers.\n--no-cache  Do not use cache when building the image.\n--pull      Always attempt to pull a newer version of the image.\n</pre> <p>Services are built once and then tagged as <code>project_service</code>, e.g., <code>composetest_db</code>. If you change a service’s Dockerfile or the contents of its build directory, run <code>docker-compose build</code> to rebuild it.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/build/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/build/</a>\n  </p>\n</div>\n","compose/networking/index":"<h1 id=\"networking-in-compose\">Networking in Compose</h1> <blockquote> <p><strong>Note:</strong> This document only applies if you’re using <a href=\"../compose-file/index#versioning\">version 2 of the Compose file format</a>. Networking features are not supported for version 1 (legacy) Compose files.</p> </blockquote> <p>By default Compose sets up a single <a href=\"https://docs.docker.com/engine/reference/commandline/network_create/\">network</a> for your app. Each container for a service joins the default network and is both <em>reachable</em> by other containers on that network, and <em>discoverable</em> by them at a hostname identical to the container name.</p> <blockquote> <p><strong>Note:</strong> Your app’s network is given a name based on the “project name”, which is based on the name of the directory it lives in. You can override the project name with either the <a href=\"../reference/overview/index\"><code>--project-name</code> flag</a> or the <a href=\"../reference/envvars/index#compose-project-name\"><code>COMPOSE_PROJECT_NAME</code> environment variable</a>.</p> </blockquote> <p>For example, suppose your app is in a directory called <code>myapp</code>, and your <code>docker-compose.yml</code> looks like this:</p> <pre>version: '2'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8000:8000\"\n  db:\n    image: postgres\n</pre> <p>When you run <code>docker-compose up</code>, the following happens:</p> <ol> <li>A network called <code>myapp_default</code> is created.</li> <li>A container is created using <code>web</code>’s configuration. It joins the network <code>myapp_default</code> under the name <code>web</code>.</li> <li>A container is created using <code>db</code>’s configuration. It joins the network <code>myapp_default</code> under the name <code>db</code>.</li> </ol> <p>Each container can now look up the hostname <code>web</code> or <code>db</code> and get back the appropriate container’s IP address. For example, <code>web</code>’s application code could connect to the URL <code>postgres://db:5432</code> and start using the Postgres database.</p> <p>Because <code>web</code> explicitly maps a port, it’s also accessible from the outside world via port 8000 on your Docker host’s network interface.</p> <h2 id=\"updating-containers\">Updating containers</h2> <p>If you make a configuration change to a service and run <code>docker-compose up</code> to update it, the old container will be removed and the new one will join the network under a different IP address but the same name. Running containers will be able to look up that name and connect to the new address, but the old address will stop working.</p> <p>If any containers have connections open to the old container, they will be closed. It is a container’s responsibility to detect this condition, look up the name again and reconnect.</p> <h2 id=\"links\">Links</h2> <p>Links allow you to define extra aliases by which a service is reachable from another service. They are not required to enable services to communicate - by default, any service can reach any other service at that service’s name. In the following example, <code>db</code> is reachable from <code>web</code> at the hostnames <code>db</code> and <code>database</code>:</p> <pre>version: '2'\nservices:\n  web:\n    build: .\n    links:\n      - \"db:database\"\n  db:\n    image: postgres\n</pre> <p>See the <a href=\"../compose-file/index#links\">links reference</a> for more information.</p> <h2 id=\"multi-host-networking\">Multi-host networking</h2> <p>When <a href=\"../swarm/index\">deploying a Compose application to a Swarm cluster</a>, you can make use of the built-in <code>overlay</code> driver to enable multi-host communication between containers with no changes to your Compose file or application code.</p> <p>Consult the <a href=\"https://docs.docker.com/engine/userguide/networking/get-started-overlay/\">Getting started with multi-host networking</a> to see how to set up a Swarm cluster. The cluster will use the <code>overlay</code> driver by default, but you can specify it explicitly if you prefer - see below for how to do this.</p> <h2 id=\"specifying-custom-networks\">Specifying custom networks</h2> <p>Instead of just using the default app network, you can specify your own networks with the top-level <code>networks</code> key. This lets you create more complex topologies and specify <a href=\"https://docs.docker.com/engine/extend/plugins_network/\">custom network drivers</a> and options. You can also use it to connect services to externally-created networks which aren’t managed by Compose.</p> <p>Each service can specify what networks to connect to with the <em>service-level</em> <code>networks</code> key, which is a list of names referencing entries under the <em>top-level</em> <code>networks</code> key.</p> <p>Here’s an example Compose file defining two custom networks. The <code>proxy</code> service is isolated from the <code>db</code> service, because they do not share a network in common - only <code>app</code> can talk to both.</p> <pre>version: '2'\n\nservices:\n  proxy:\n    build: ./proxy\n    networks:\n      - front\n  app:\n    build: ./app\n    networks:\n      - front\n      - back\n  db:\n    image: postgres\n    networks:\n      - back\n\nnetworks:\n  front:\n    # Use a custom driver\n    driver: custom-driver-1\n  back:\n    # Use a custom driver which takes special options\n    driver: custom-driver-2\n    driver_opts:\n      foo: \"1\"\n      bar: \"2\"\n</pre> <p>Networks can be configured with static IP addresses by setting the <a href=\"../compose-file/index#ipv4-address-ipv6-address\">ipv4_address and/or ipv6_address</a> for each attached network.</p> <p>For full details of the network configuration options available, see the following references:</p> <ul> <li><a href=\"../compose-file/index#network-configuration-reference\">Top-level <code>networks</code> key</a></li> <li><a href=\"../compose-file/index#networks\">Service-level <code>networks</code> key</a></li> </ul> <h2 id=\"configuring-the-default-network\">Configuring the default network</h2> <p>Instead of (or as well as) specifying your own networks, you can also change the settings of the app-wide default network by defining an entry under <code>networks</code> named <code>default</code>:</p> <pre>version: '2'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8000:8000\"\n  db:\n    image: postgres\n\nnetworks:\n  default:\n    # Use a custom driver\n    driver: custom-driver-1\n</pre> <h2 id=\"using-a-pre-existing-network\">Using a pre-existing network</h2> <p>If you want your containers to join a pre-existing network, use the <a href=\"../compose-file/index#network-configuration-reference\"><code>external</code> option</a>:</p> <pre>networks:\n  default:\n    external:\n      name: my-pre-existing-network\n</pre> <p>Instead of attemping to create a network called <code>[projectname]_default</code>, Compose will look for a network called <code>my-pre-existing-network</code> and connect your app’s containers to it.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/networking/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/networking/</a>\n  </p>\n</div>\n","compose/reference/envvars/index":"<h1 id=\"cli-environment-variables\">CLI Environment Variables</h1> <p>Several environment variables are available for you to configure the Docker Compose command-line behaviour.</p> <p>Variables starting with <code>DOCKER_</code> are the same as those used to configure the Docker command-line client. If you’re using <code>docker-machine</code>, then the <code>eval \"$(docker-machine env my-docker-vm)\"</code> command should set them to their correct values. (In this example, <code>my-docker-vm</code> is the name of a machine you created.)</p> <blockquote> <p>Note: Some of these variables can also be provided using an <a href=\"../../env-file/index\">environment file</a></p> </blockquote> <h2 id=\"compose-project-name\">COMPOSE_PROJECT_NAME</h2> <p>Sets the project name. This value is prepended along with the service name to the container container on start up. For example, if you project name is <code>myapp</code> and it includes two services <code>db</code> and <code>web</code> then compose starts containers named <code>myapp_db_1</code> and <code>myapp_web_1</code> respectively.</p> <p>Setting this is optional. If you do not set this, the <code>COMPOSE_PROJECT_NAME</code> defaults to the <code>basename</code> of the project directory. See also the <code>-p</code> <a href=\"../overview/index\">command-line option</a>.</p> <h2 id=\"compose-file\">COMPOSE_FILE</h2> <p>Specify the path to a Compose file. If not provided, Compose looks for a file named <code>docker-compose.yml</code> in the current directory and then each parent directory in succession until a file by that name is found.</p> <p>This variable supports multiple compose files separate by a path separator (on Linux and OSX the path separator is <code>:</code>, on Windows it is <code>;</code>). For example: <code>COMPOSE_FILE=docker-compose.yml:docker-compose.prod.yml</code></p> <p>See also the <code>-f</code> <a href=\"../overview/index\">command-line option</a>.</p> <h2 id=\"compose-api-version\">COMPOSE_API_VERSION</h2> <p>The Docker API only supports requests from clients which report a specific version. If you receive a <code>client and server don't have same version error</code> using <code>docker-compose</code>, you can workaround this error by setting this environment variable. Set the version value to match the server version.</p> <p>Setting this variable is intended as a workaround for situations where you need to run temporarily with a mismatch between the client and server version. For example, if you can upgrade the client but need to wait to upgrade the server.</p> <p>Running with this variable set and a known mismatch does prevent some Docker features from working properly. The exact features that fail would depend on the Docker client and server versions. For this reason, running with this variable set is only intended as a workaround and it is not officially supported.</p> <p>If you run into problems running with this set, resolve the mismatch through upgrade and remove this setting to see if your problems resolve before notifying support.</p> <h2 id=\"docker-host\">DOCKER_HOST</h2> <p>Sets the URL of the <code>docker</code> daemon. As with the Docker client, defaults to <code>unix:///var/run/docker.sock</code>.</p> <h2 id=\"docker-tls-verify\">DOCKER_TLS_VERIFY</h2> <p>When set to anything other than an empty string, enables TLS communication with the <code>docker</code> daemon.</p> <h2 id=\"docker-cert-path\">DOCKER_CERT_PATH</h2> <p>Configures the path to the <code>ca.pem</code>, <code>cert.pem</code>, and <code>key.pem</code> files used for TLS verification. Defaults to <code>~/.docker</code>.</p> <h2 id=\"compose-http-timeout\">COMPOSE_HTTP_TIMEOUT</h2> <p>Configures the time (in seconds) a request to the Docker daemon is allowed to hang before Compose considers it failed. Defaults to 60 seconds.</p> <h2 id=\"related-information\">Related Information</h2> <ul> <li><a href=\"../../index\">User guide</a></li> <li><a href=\"../../install/index\">Installing Compose</a></li> <li><a href=\"../../compose-file/index\">Compose file reference</a></li> <li><a href=\"../../env-file/index\">Environment file</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/envvars/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/envvars/</a>\n  </p>\n</div>\n","compose/reference/config/index":"<h1 id=\"config\">config</h1> <pre>Usage: config [options]\n\nOptions:\n-q, --quiet     Only validate the configuration, don't print\n                anything.\n--services      Print the service names, one per line.\n</pre> <p>Validate and view the compose file.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/config/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/config/</a>\n  </p>\n</div>\n","compose/compose-file/index":"<h1 id=\"compose-file-reference\">Compose file reference</h1> <p>The Compose file is a <a href=\"http://yaml.org/\">YAML</a> file defining <a href=\"#service-configuration-reference\">services</a>, <a href=\"#network-configuration-reference\">networks</a> and <a href=\"#volume-configuration-reference\">volumes</a>. The default path for a Compose file is <code>./docker-compose.yml</code>.</p> <p>A service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to <code>docker run</code>. Likewise, network and volume definitions are analogous to <code>docker network create</code> and <code>docker volume create</code>.</p> <p>As with <code>docker run</code>, options specified in the Dockerfile (e.g., <code>CMD</code>, <code>EXPOSE</code>, <code>VOLUME</code>, <code>ENV</code>) are respected by default - you don’t need to specify them again in <code>docker-compose.yml</code>.</p> <p>You can use environment variables in configuration values with a Bash-like <code>${VARIABLE}</code> syntax - see <a href=\"#variable-substitution\">variable substitution</a> for full details.</p> <h2 id=\"service-configuration-reference\">Service configuration reference</h2> <blockquote> <p><strong>Note:</strong> There are two versions of the Compose file format – version 1 (the legacy format, which does not support volumes or networks) and version 2 (the most up-to-date). For more information, see the <a href=\"#versioning\">Versioning</a> section.</p> </blockquote> <p>This section contains a list of all configuration options supported by a service definition.</p> <h3 id=\"build\">build</h3> <p>Configuration options that are applied at build time.</p> <p><code>build</code> can be specified either as a string containing a path to the build context, or an object with the path specified under <a href=\"#context\">context</a> and optionally <a href=\"#dockerfile\">dockerfile</a> and <a href=\"#args\">args</a>.</p> <pre>build: ./dir\n\nbuild:\n  context: ./dir\n  dockerfile: Dockerfile-alternate\n  args:\n    buildno: 1\n</pre> <p>If you specify <code>image</code> as well as <code>build</code>, then Compose names the built image with the <code>webapp</code> and optional <code>tag</code> specified in <code>image</code>:</p> <pre>build: ./dir\nimage: webapp:tag\n</pre> <p>This will result in an image named <code>webapp</code> and tagged <code>tag</code>, built from <code>./dir</code>.</p> <blockquote> <p><strong>Note</strong>: In the <a href=\"#version-1\">version 1 file format</a>, <code>build</code> is different in two ways:</p> <ul> <li>Only the string form (<code>build: .</code>) is allowed - not the object form.</li> <li>Using <code>build</code> together with <code>image</code> is not allowed. Attempting to do so results in an error.</li> </ul> </blockquote> <h4 id=\"context\">context</h4> <blockquote> <p><a href=\"#version-2\">Version 2 file format</a> only. In version 1, just use <a href=\"#build\">build</a>.</p> </blockquote> <p>Either a path to a directory containing a Dockerfile, or a url to a git repository.</p> <p>When the value supplied is a relative path, it is interpreted as relative to the location of the Compose file. This directory is also the build context that is sent to the Docker daemon.</p> <p>Compose will build and tag it with a generated name, and use that image thereafter.</p> <pre>build:\n  context: ./dir\n</pre> <h4 id=\"dockerfile\">dockerfile</h4> <p>Alternate Dockerfile.</p> <p>Compose will use an alternate file to build with. A build path must also be specified.</p> <pre>build:\n  context: .\n  dockerfile: Dockerfile-alternate\n</pre> <blockquote> <p><strong>Note</strong>: In the <a href=\"#version-1\">version 1 file format</a>, <code>dockerfile</code> is different in two ways:</p> </blockquote> <ul> <li>\n<p>It appears alongside <code>build</code>, not as a sub-option:</p> <pre>build: .\ndockerfile: Dockerfile-alternate\n</pre>\n</li> <li><p>Using <code>dockerfile</code> together with <code>image</code> is not allowed. Attempting to do so results in an error.</p></li> </ul> <h4 id=\"args\">args</h4> <blockquote> <p><a href=\"#version-2\">Version 2 file format</a> only.</p> </blockquote> <p>Add build arguments. You can use either an array or a dictionary. Any boolean values; true, false, yes, no, need to be enclosed in quotes to ensure they are not converted to True or False by the YML parser.</p> <p>Build arguments with only a key are resolved to their environment value on the machine Compose is running on.</p> <pre>build:\n  args:\n    buildno: 1\n    user: someuser\n\nbuild:\n  args:\n    - buildno=1\n    - user=someuser\n</pre> <h3 id=\"cap-add-cap-drop\">cap_add, cap_drop</h3> <p>Add or drop container capabilities. See <code>man 7 capabilities</code> for a full list.</p> <pre>cap_add:\n  - ALL\n\ncap_drop:\n  - NET_ADMIN\n  - SYS_ADMIN\n</pre> <h3 id=\"command\">command</h3> <p>Override the default command.</p> <pre>command: bundle exec thin -p 3000\n</pre> <p>The command can also be a list, in a manner similar to <a href=\"https://docs.docker.com/engine/reference/builder/#cmd\">dockerfile</a>:</p> <pre>command: [bundle, exec, thin, -p, 3000]\n</pre> <h3 id=\"cgroup-parent\">cgroup_parent</h3> <p>Specify an optional parent cgroup for the container.</p> <pre>cgroup_parent: m-executor-abcd\n</pre> <h3 id=\"container-name\">container_name</h3> <p>Specify a custom container name, rather than a generated default name.</p> <pre>container_name: my-web-container\n</pre> <p>Because Docker container names must be unique, you cannot scale a service beyond 1 container if you have specified a custom name. Attempting to do so results in an error.</p> <h3 id=\"devices\">devices</h3> <p>List of device mappings. Uses the same format as the <code>--device</code> docker client create option.</p> <pre>devices:\n  - \"/dev/ttyUSB0:/dev/ttyUSB0\"\n</pre> <h3 id=\"depends-on\">depends_on</h3> <p>Express dependency between services, which has two effects:</p> <ul> <li><p><code>docker-compose up</code> will start services in dependency order. In the following example, <code>db</code> and <code>redis</code> will be started before <code>web</code>.</p></li> <li><p><code>docker-compose up SERVICE</code> will automatically include <code>SERVICE</code>’s dependencies. In the following example, <code>docker-compose up web</code> will also create and start <code>db</code> and <code>redis</code>.</p></li> </ul> <p>Simple example:</p> <pre>version: '2'\nservices:\n  web:\n    build: .\n    depends_on:\n      - db\n      - redis\n  redis:\n    image: redis\n  db:\n    image: postgres\n</pre> <blockquote> <p><strong>Note:</strong> <code>depends_on</code> will not wait for <code>db</code> and <code>redis</code> to be “ready” before starting <code>web</code> - only until they have been started. If you need to wait for a service to be ready, see <a href=\"../startup-order/index\">Controlling startup order</a> for more on this problem and strategies for solving it.</p> </blockquote> <h3 id=\"dns\">dns</h3> <p>Custom DNS servers. Can be a single value or a list.</p> <pre>dns: 8.8.8.8\ndns:\n  - 8.8.8.8\n  - 9.9.9.9\n</pre> <h3 id=\"dns-search\">dns_search</h3> <p>Custom DNS search domains. Can be a single value or a list.</p> <pre>dns_search: example.com\ndns_search:\n  - dc1.example.com\n  - dc2.example.com\n</pre> <h3 id=\"tmpfs\">tmpfs</h3> <p>Mount a temporary file system inside the container. Can be a single value or a list.</p> <pre>tmpfs: /run\ntmpfs:\n  - /run\n  - /tmp\n</pre> <h3 id=\"entrypoint\">entrypoint</h3> <p>Override the default entrypoint.</p> <pre>entrypoint: /code/entrypoint.sh\n</pre> <p>The entrypoint can also be a list, in a manner similar to <a href=\"https://docs.docker.com/engine/reference/builder/#entrypoint\">dockerfile</a>:</p> <pre>entrypoint:\n    - php\n    - -d\n    - zend_extension=/usr/local/lib/php/extensions/no-debug-non-zts-20100525/xdebug.so\n    - -d\n    - memory_limit=-1\n    - vendor/bin/phpunit\n</pre> <h3 id=\"env-file\">env_file</h3> <p>Add environment variables from a file. Can be a single value or a list.</p> <p>If you have specified a Compose file with <code>docker-compose -f FILE</code>, paths in <code>env_file</code> are relative to the directory that file is in.</p> <p>Environment variables specified in <code>environment</code> override these values.</p> <pre>env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n</pre> <p>Compose expects each line in an env file to be in <code>VAR=VAL</code> format. Lines beginning with <code>#</code> (i.e. comments) are ignored, as are blank lines.</p> <pre># Set Rails/Rack environment\nRACK_ENV=development\n</pre> <h3 id=\"environment\">environment</h3> <p>Add environment variables. You can use either an array or a dictionary. Any boolean values; true, false, yes no, need to be enclosed in quotes to ensure they are not converted to True or False by the YML parser.</p> <p>Environment variables with only a key are resolved to their values on the machine Compose is running on, which can be helpful for secret or host-specific values.</p> <pre>environment:\n  RACK_ENV: development\n  SHOW: 'true'\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SHOW=true\n  - SESSION_SECRET\n</pre> <h3 id=\"expose\">expose</h3> <p>Expose ports without publishing them to the host machine - they’ll only be accessible to linked services. Only the internal port can be specified.</p> <pre>expose:\n - \"3000\"\n - \"8000\"\n</pre> <h3 id=\"extends\">extends</h3> <p>Extend another service, in the current file or another, optionally overriding configuration.</p> <p>You can use <code>extends</code> on any service together with other configuration keys. The <code>extends</code> value must be a dictionary defined with a required <code>service</code> and an optional <code>file</code> key.</p> <pre>extends:\n  file: common.yml\n  service: webapp\n</pre> <p>The <code>service</code> the name of the service being extended, for example <code>web</code> or <code>database</code>. The <code>file</code> is the location of a Compose configuration file defining that service.</p> <p>If you omit the <code>file</code> Compose looks for the service configuration in the current file. The <code>file</code> value can be an absolute or relative path. If you specify a relative path, Compose treats it as relative to the location of the current file.</p> <p>You can extend a service that itself extends another. You can extend indefinitely. Compose does not support circular references and <code>docker-compose</code> returns an error if it encounters one.</p> <p>For more on <code>extends</code>, see the <a href=\"../extends/index#extending-services\">the extends documentation</a>.</p> <h3 id=\"external-links\">external_links</h3> <p>Link to containers started outside this <code>docker-compose.yml</code> or even outside of Compose, especially for containers that provide shared or common services. <code>external_links</code> follow semantics similar to <code>links</code> when specifying both the container name and the link alias (<code>CONTAINER:ALIAS</code>).</p> <pre>external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n</pre> <blockquote> <p><strong>Note:</strong> If you’re using the <a href=\"#version-2\">version 2 file format</a>, the externally-created containers must be connected to at least one of the same networks as the service which is linking to them.</p> </blockquote> <h3 id=\"extra-hosts\">extra_hosts</h3> <p>Add hostname mappings. Use the same values as the docker client <code>--add-host</code> parameter.</p> <pre>extra_hosts:\n - \"somehost:162.242.195.82\"\n - \"otherhost:50.31.209.229\"\n</pre> <p>An entry with the ip address and hostname will be created in <code>/etc/hosts</code> inside containers for this service, e.g:</p> <pre>162.242.195.82  somehost\n50.31.209.229   otherhost\n</pre> <h3 id=\"image\">image</h3> <p>Specify the image to start the container from. Can either be a repository/tag or a partial image ID.</p> <pre>image: redis\nimage: ubuntu:14.04\nimage: tutum/influxdb\nimage: example-registry.com:4000/postgresql\nimage: a4bc65fd\n</pre> <p>If the image does not exist, Compose attempts to pull it, unless you have also specified <a href=\"#build\">build</a>, in which case it builds it using the specified options and tags it with the specified tag.</p> <blockquote> <p><strong>Note</strong>: In the <a href=\"#version-1\">version 1 file format</a>, using <code>build</code> together with <code>image</code> is not allowed. Attempting to do so results in an error.</p> </blockquote> <h3 id=\"labels\">labels</h3> <p>Add metadata to containers using <a href=\"https://docs.docker.com/engine/userguide/labels-custom-metadata/\">Docker labels</a>. You can use either an array or a dictionary.</p> <p>It’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.</p> <pre>labels:\n  com.example.description: \"Accounting webapp\"\n  com.example.department: \"Finance\"\n  com.example.label-with-empty-value: \"\"\n\nlabels:\n  - \"com.example.description=Accounting webapp\"\n  - \"com.example.department=Finance\"\n  - \"com.example.label-with-empty-value\"\n</pre> <h3 id=\"links\">links</h3> <p>Link to containers in another service. Either specify both the service name and a link alias (<code>SERVICE:ALIAS</code>), or just the service name.</p> <pre>web:\n  links:\n   - db\n   - db:database\n   - redis\n</pre> <p>Containers for the linked service will be reachable at a hostname identical to the alias, or the service name if no alias was specified.</p> <p>Links also express dependency between services in the same way as <a href=\"#depends-on\">depends_on</a>, so they determine the order of service startup.</p> <blockquote> <p><strong>Note:</strong> If you define both links and <a href=\"#networks\">networks</a>, services with links between them must share at least one network in common in order to communicate.</p> </blockquote> <h3 id=\"logging\">logging</h3> <blockquote> <p><a href=\"#version-2\">Version 2 file format</a> only. In version 1, use <a href=\"#log_driver\">log_driver</a> and <a href=\"#log_opt\">log_opt</a>.</p> </blockquote> <p>Logging configuration for the service.</p> <pre>logging:\n  driver: syslog\n  options:\n    syslog-address: \"tcp://192.168.0.42:123\"\n</pre> <p>The <code>driver</code> name specifies a logging driver for the service’s containers, as with the <code>--log-driver</code> option for docker run (<a href=\"https://docs.docker.com/engine/reference/logging/overview/\">documented here</a>).</p> <p>The default value is json-file.</p> <pre>driver: \"json-file\"\ndriver: \"syslog\"\ndriver: \"none\"\n</pre> <blockquote> <p><strong>Note:</strong> Only the <code>json-file</code> driver makes the logs available directly from <code>docker-compose up</code> and <code>docker-compose logs</code>. Using any other driver will not print any logs.</p> </blockquote> <p>Specify logging options for the logging driver with the <code>options</code> key, as with the <code>--log-opt</code> option for <code>docker run</code>.</p> <p>Logging options are key-value pairs. An example of <code>syslog</code> options:</p> <pre>driver: \"syslog\"\noptions:\n  syslog-address: \"tcp://192.168.0.42:123\"\n</pre> <h3 id=\"log-driver\">log_driver</h3> <blockquote> <p><a href=\"#version-1\">Version 1 file format</a> only. In version 2, use <a href=\"#logging\">logging</a>.</p> </blockquote> <p>Specify a log driver. The default is <code>json-file</code>.</p> <pre>log_driver: syslog\n</pre> <h3 id=\"log-opt\">log_opt</h3> <blockquote> <p><a href=\"#version-1\">Version 1 file format</a> only. In version 2, use <a href=\"#logging\">logging</a>.</p> </blockquote> <p>Specify logging options as key-value pairs. An example of <code>syslog</code> options:</p> <pre>log_opt:\n  syslog-address: \"tcp://192.168.0.42:123\"\n</pre> <h3 id=\"net\">net</h3> <blockquote> <p><a href=\"#version-1\">Version 1 file format</a> only. In version 2, use <a href=\"#network_mode\">network_mode</a>.</p> </blockquote> <p>Network mode. Use the same values as the docker client <code>--net</code> parameter. The <code>container:...</code> form can take a service name instead of a container name or id.</p> <pre>net: \"bridge\"\nnet: \"host\"\nnet: \"none\"\nnet: \"container:[service name or container name/id]\"\n</pre> <h3 id=\"network-mode\">network_mode</h3> <blockquote> <p><a href=\"#version-2\">Version 2 file format</a> only. In version 1, use <a href=\"#net\">net</a>.</p> </blockquote> <p>Network mode. Use the same values as the docker client <code>--net</code> parameter, plus the special form <code>service:[service name]</code>.</p> <pre>network_mode: \"bridge\"\nnetwork_mode: \"host\"\nnetwork_mode: \"none\"\nnetwork_mode: \"service:[service name]\"\nnetwork_mode: \"container:[container name/id]\"\n</pre> <h3 id=\"networks\">networks</h3> <blockquote> <p><a href=\"#version-2\">Version 2 file format</a> only. In version 1, use <a href=\"#net\">net</a>.</p> </blockquote> <p>Networks to join, referencing entries under the <a href=\"#network-configuration-reference\">top-level <code>networks</code> key</a>.</p> <pre>services:\n  some-service:\n    networks:\n     - some-network\n     - other-network\n</pre> <h4 id=\"aliases\">aliases</h4> <p>Aliases (alternative hostnames) for this service on the network. Other containers on the same network can use either the service name or this alias to connect to one of the service’s containers.</p> <p>Since <code>aliases</code> is network-scoped, the same service can have different aliases on different networks.</p> <blockquote> <p><strong>Note</strong>: A network-wide alias can be shared by multiple containers, and even by multiple services. If it is, then exactly which container the name will resolve to is not guaranteed.</p> </blockquote> <p>The general format is shown here.</p> <pre>services:\n  some-service:\n    networks:\n      some-network:\n        aliases:\n         - alias1\n         - alias3\n      other-network:\n        aliases:\n         - alias2\n</pre> <p>In the example below, three services are provided (<code>web</code>, <code>worker</code>, and <code>db</code>), along with two networks (<code>new</code> and <code>legacy</code>). The <code>db</code> service is reachable at the hostname <code>db</code> or <code>database</code> on the <code>new</code> network, and at <code>db</code> or <code>mysql</code> on the <code>legacy</code> network.</p> <pre>version: '2'\n\nservices:\n  web:\n    build: ./web\n    networks:\n      - new\n\n  worker:\n    build: ./worker\n    networks:\n    - legacy\n\n  db:\n    image: mysql\n    networks:\n      new:\n        aliases:\n          - database\n      legacy:\n        aliases:\n          - mysql\n\nnetworks:\n  new:\n  legacy:\n</pre> <h4 id=\"ipv4-address-ipv6-address\">ipv4_address, ipv6_address</h4> <p>Specify a static IP address for containers for this service when joining the network.</p> <p>The corresponding network configuration in the <a href=\"#network-configuration-reference\">top-level networks section</a> must have an <code>ipam</code> block with subnet and gateway configurations covering each static address. If IPv6 addressing is desired, the <code>com.docker.network.enable_ipv6</code> driver option must be set to <code>true</code>.</p> <p>An example:</p> <pre>version: '2'\n\nservices:\n  app:\n    image: busybox\n    command: ifconfig\n    networks:\n      app_net:\n        ipv4_address: 172.16.238.10\n        ipv6_address: 2001:3984:3989::10\n\nnetworks:\n  app_net:\n    driver: bridge\n    driver_opts:\n      com.docker.network.enable_ipv6: \"true\"\n    ipam:\n      driver: default\n      config:\n      - subnet: 172.16.238.0/24\n        gateway: 172.16.238.1\n      - subnet: 2001:3984:3989::/64\n        gateway: 2001:3984:3989::1\n</pre> <h3 id=\"pid\">pid</h3> <pre>pid: \"host\"\n</pre> <p>Sets the PID mode to the host PID mode. This turns on sharing between container and the host operating system the PID address space. Containers launched with this flag will be able to access and manipulate other containers in the bare-metal machine’s namespace and vise-versa.</p> <h3 id=\"ports\">ports</h3> <p>Expose ports. Either specify both ports (<code>HOST:CONTAINER</code>), or just the container port (a random host port will be chosen).</p> <blockquote> <p><strong>Note:</strong> When mapping ports in the <code>HOST:CONTAINER</code> format, you may experience erroneous results when using a container port lower than 60, because YAML will parse numbers in the format <code>xx:yy</code> as sexagesimal (base 60). For this reason, we recommend always explicitly specifying your port mappings as strings.</p> </blockquote> <pre>ports:\n - \"3000\"\n - \"3000-3005\"\n - \"8000:8000\"\n - \"9090-9091:8080-8081\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n - \"127.0.0.1:5000-5010:5000-5010\"\n</pre> <h3 id=\"security-opt\">security_opt</h3> <p>Override the default labeling scheme for each container.</p> <pre>security_opt:\n  - label:user:USER\n  - label:role:ROLE\n</pre> <h3 id=\"stop-signal\">stop_signal</h3> <p>Sets an alternative signal to stop the container. By default <code>stop</code> uses SIGTERM. Setting an alternative signal using <code>stop_signal</code> will cause <code>stop</code> to send that signal instead.</p> <pre>stop_signal: SIGUSR1\n</pre> <h3 id=\"ulimits\">ulimits</h3> <p>Override the default ulimits for a container. You can either specify a single limit as an integer or soft/hard limits as a mapping.</p> <pre>ulimits:\n  nproc: 65535\n  nofile:\n    soft: 20000\n    hard: 40000\n</pre> <h3 id=\"volumes-volume-driver\">volumes, volume_driver</h3> <p>Mount paths or named volumes, optionally specifying a path on the host machine (<code>HOST:CONTAINER</code>), or an access mode (<code>HOST:CONTAINER:ro</code>). For <a href=\"#version-2\">version 2 files</a>, named volumes need to be specified with the <a href=\"#volume-configuration-reference\">top-level <code>volumes</code> key</a>. When using <a href=\"#version-1\">version 1</a>, the Docker Engine will create the named volume automatically if it doesn’t exist.</p> <p>You can mount a relative path on the host, which will expand relative to the directory of the Compose configuration file being used. Relative paths should always begin with <code>.</code> or <code>..</code>.</p> <pre>volumes:\n  # Just specify a path and let the Engine create a volume\n  - /var/lib/mysql\n\n  # Specify an absolute path mapping\n  - /opt/data:/var/lib/mysql\n\n  # Path on the host, relative to the Compose file\n  - ./cache:/tmp/cache\n\n  # User-relative path\n  - ~/configs:/etc/configs/:ro\n\n  # Named volume\n  - datavolume:/var/lib/mysql\n</pre> <p>If you do not use a host path, you may specify a <code>volume_driver</code>.</p> <pre>volume_driver: mydriver\n</pre> <p>Note that for <a href=\"#version-2\">version 2 files</a>, this driver will not apply to named volumes (you should use the <code>driver</code> option when <a href=\"#volume-configuration-reference\">declaring the volume</a> instead). For <a href=\"#version-1\">version 1</a>, both named volumes and container volumes will use the specified driver.</p> <blockquote> <p>Note: No path expansion will be done if you have also specified a <code>volume_driver</code>.</p> </blockquote> <p>See <a href=\"https://docs.docker.com/engine/userguide/dockervolumes/\">Docker Volumes</a> and <a href=\"https://docs.docker.com/engine/extend/plugins_volume/\">Volume Plugins</a> for more information.</p> <h3 id=\"volumes-from\">volumes_from</h3> <p>Mount all of the volumes from another service or container, optionally specifying read-only access (<code>ro</code>) or read-write (<code>rw</code>). If no access level is specified, then read-write will be used.</p> <pre>volumes_from:\n - service_name\n - service_name:ro\n - container:container_name\n - container:container_name:rw\n</pre> <blockquote> <p><strong>Note:</strong> The <code>container:...</code> formats are only supported in the <a href=\"#version-2\">version 2 file format</a>. In <a href=\"#version-1\">version 1</a>, you can use container names without marking them as such:</p> <pre>- service_name\n- service_name:ro\n- container_name\n- container_name:rw\n</pre> </blockquote> <h3 id=\"cpu-shares-cpu-quota-cpuset-domainname-hostname-ipc-mac-address-mem-limit-memswap-limit-privileged-read-only-restart-shm-size-stdin-open-tty-user-working-dir\">cpu_shares, cpu_quota, cpuset, domainname, hostname, ipc, mac_address, mem_limit, memswap_limit, privileged, read_only, restart, shm_size, stdin_open, tty, user, working_dir</h3> <p>Each of these is a single value, analogous to its <a href=\"https://docs.docker.com/engine/reference/run/\">docker run</a> counterpart.</p> <pre>cpu_shares: 73\ncpu_quota: 50000\ncpuset: 0,1\n\nuser: postgresql\nworking_dir: /code\n\ndomainname: foo.com\nhostname: foo\nipc: host\nmac_address: 02:42:ac:11:65:43\n\nmem_limit: 1000000000\nmemswap_limit: 2000000000\nprivileged: true\n\nrestart: always\n\nread_only: true\nshm_size: 64M\nstdin_open: true\ntty: true\n</pre> <h2 id=\"volume-configuration-reference\">Volume configuration reference</h2> <p>While it is possible to declare volumes on the fly as part of the service declaration, this section allows you to create named volumes that can be reused across multiple services (without relying on <code>volumes_from</code>), and are easily retrieved and inspected using the docker command line or API. See the <a href=\"https://docs.docker.com/engine/reference/commandline/volume_create/\">docker volume</a> subcommand documentation for more information.</p> <h3 id=\"driver\">driver</h3> <p>Specify which volume driver should be used for this volume. Defaults to <code>local</code>. The Docker Engine will return an error if the driver is not available.</p> <pre> driver: foobar\n</pre> <h3 id=\"driver-opts\">driver_opts</h3> <p>Specify a list of options as key-value pairs to pass to the driver for this volume. Those options are driver-dependent - consult the driver’s documentation for more information. Optional.</p> <pre> driver_opts:\n   foo: \"bar\"\n   baz: 1\n</pre> <h3 id=\"external\">external</h3> <p>If set to <code>true</code>, specifies that this volume has been created outside of Compose. <code>docker-compose up</code> will not attempt to create it, and will raise an error if it doesn’t exist.</p> <p><code>external</code> cannot be used in conjunction with other volume configuration keys (<code>driver</code>, <code>driver_opts</code>).</p> <p>In the example below, instead of attemping to create a volume called <code>[projectname]_data</code>, Compose will look for an existing volume simply called <code>data</code> and mount it into the <code>db</code> service’s containers.</p> <pre>version: '2'\n\nservices:\n  db:\n    image: postgres\n    volumes:\n      - data:/var/lib/postgres/data\n\nvolumes:\n  data:\n    external: true\n</pre> <p>You can also specify the name of the volume separately from the name used to refer to it within the Compose file:</p> <pre>volumes\n  data:\n    external:\n      name: actual-name-of-volume\n</pre> <h2 id=\"network-configuration-reference\">Network configuration reference</h2> <p>The top-level <code>networks</code> key lets you specify networks to be created. For a full explanation of Compose’s use of Docker networking features, see the <a href=\"../networking/index\">Networking guide</a>.</p> <h3 id=\"driver-1\">driver</h3> <p>Specify which driver should be used for this network.</p> <p>The default driver depends on how the Docker Engine you’re using is configured, but in most instances it will be <code>bridge</code> on a single host and <code>overlay</code> on a Swarm.</p> <p>The Docker Engine will return an error if the driver is not available.</p> <pre>driver: overlay\n</pre> <h3 id=\"driver-opts-1\">driver_opts</h3> <p>Specify a list of options as key-value pairs to pass to the driver for this network. Those options are driver-dependent - consult the driver’s documentation for more information. Optional.</p> <pre>  driver_opts:\n    foo: \"bar\"\n    baz: 1\n</pre> <h3 id=\"ipam\">ipam</h3> <p>Specify custom IPAM config. This is an object with several properties, each of which is optional:</p> <ul> <li>\n<code>driver</code>: Custom IPAM driver, instead of the default.</li> <li>\n<code>config</code>: A list with zero or more config blocks, each containing any of the following keys: <ul> <li>\n<code>subnet</code>: Subnet in CIDR format that represents a network segment</li> <li>\n<code>ip_range</code>: Range of IPs from which to allocate container IPs</li> <li>\n<code>gateway</code>: IPv4 or IPv6 gateway for the master subnet</li> <li>\n<code>aux_addresses</code>: Auxiliary IPv4 or IPv6 addresses used by Network driver, as a mapping from hostname to IP</li> </ul>\n</li> </ul> <p>A full example:</p> <pre>ipam:\n  driver: default\n  config:\n    - subnet: 172.28.0.0/16\n      ip_range: 172.28.5.0/24\n      gateway: 172.28.5.254\n      aux_addresses:\n        host1: 172.28.1.5\n        host2: 172.28.1.6\n        host3: 172.28.1.7\n</pre> <h3 id=\"external-1\">external</h3> <p>If set to <code>true</code>, specifies that this network has been created outside of Compose. <code>docker-compose up</code> will not attempt to create it, and will raise an error if it doesn’t exist.</p> <p><code>external</code> cannot be used in conjunction with other network configuration keys (<code>driver</code>, <code>driver_opts</code>, <code>ipam</code>).</p> <p>In the example below, <code>proxy</code> is the gateway to the outside world. Instead of attemping to create a network called <code>[projectname]_outside</code>, Compose will look for an existing network simply called <code>outside</code> and connect the <code>proxy</code> service’s containers to it.</p> <pre>version: '2'\n\nservices:\n  proxy:\n    build: ./proxy\n    networks:\n      - outside\n      - default\n  app:\n    build: ./app\n    networks:\n      - default\n\nnetworks:\n  outside:\n    external: true\n</pre> <p>You can also specify the name of the network separately from the name used to refer to it within the Compose file:</p> <pre>networks:\n  outside:\n    external:\n      name: actual-name-of-network\n</pre> <h2 id=\"versioning\">Versioning</h2> <p>There are two versions of the Compose file format:</p> <ul> <li>Version 1, the legacy format. This is specified by omitting a <code>version</code> key at the root of the YAML.</li> <li>Version 2, the recommended format. This is specified with a <code>version: '2'</code> entry at the root of the YAML.</li> </ul> <p>To move your project from version 1 to 2, see the <a href=\"#upgrading\">Upgrading</a> section.</p> <blockquote> <p><strong>Note:</strong> If you’re using <a href=\"../extends/index#different-environments\">multiple Compose files</a> or <a href=\"../extends/index#extending-services\">extending services</a>, each file must be of the same version - you cannot mix version 1 and 2 in a single project.</p> </blockquote> <p>Several things differ depending on which version you use:</p> <ul> <li>The structure and permitted configuration keys</li> <li>The minimum Docker Engine version you must be running</li> <li>Compose’s behaviour with regards to networking</li> </ul> <p>These differences are explained below.</p> <h3 id=\"version-1\">Version 1</h3> <p>Compose files that do not declare a version are considered “version 1”. In those files, all the <a href=\"#service-configuration-reference\">services</a> are declared at the root of the document.</p> <p>Version 1 is supported by <strong>Compose up to 1.6.x</strong>. It will be deprecated in a future Compose release.</p> <p>Version 1 files cannot declare named <a href=\"#volume-configuration-reference\">volumes</a>, <a href=\"../networking/index\">networks</a> or <a href=\"#args\">build arguments</a>.</p> <p>Example:</p> <pre>web:\n  build: .\n  ports:\n   - \"5000:5000\"\n  volumes:\n   - .:/code\n  links:\n   - redis\nredis:\n  image: redis\n</pre> <h3 id=\"version-2\">Version 2</h3> <p>Compose files using the version 2 syntax must indicate the version number at the root of the document. All <a href=\"#service-configuration-reference\">services</a> must be declared under the <code>services</code> key.</p> <p>Version 2 files are supported by <strong>Compose 1.6.0+</strong> and require a Docker Engine of version <strong>1.10.0+</strong>.</p> <p>Named <a href=\"#volume-configuration-reference\">volumes</a> can be declared under the <code>volumes</code> key, and <a href=\"#network-configuration-reference\">networks</a> can be declared under the <code>networks</code> key.</p> <p>Simple example:</p> <pre>version: '2'\nservices:\n  web:\n    build: .\n    ports:\n     - \"5000:5000\"\n    volumes:\n     - .:/code\n  redis:\n    image: redis\n</pre> <p>A more extended example, defining volumes and networks:</p> <pre>version: '2'\nservices:\n  web:\n    build: .\n    ports:\n     - \"5000:5000\"\n    volumes:\n     - .:/code\n    networks:\n      - front-tier\n      - back-tier\n  redis:\n    image: redis\n    volumes:\n      - redis-data:/var/lib/redis\n    networks:\n      - back-tier\nvolumes:\n  redis-data:\n    driver: local\nnetworks:\n  front-tier:\n    driver: bridge\n  back-tier:\n    driver: bridge\n</pre> <h3 id=\"upgrading\">Upgrading</h3> <p>In the majority of cases, moving from version 1 to 2 is a very simple process:</p> <ol> <li>Indent the whole file by one level and put a <code>services:</code> key at the top.</li> <li>Add a <code>version: '2'</code> line at the top of the file.</li> </ol> <p>It’s more complicated if you’re using particular configuration features:</p> <ul> <li>\n<p><code>dockerfile</code>: This now lives under the <code>build</code> key:</p> <pre>build:\n  context: .\n  dockerfile: Dockerfile-alternate\n</pre>\n</li> <li>\n<p><code>log_driver</code>, <code>log_opt</code>: These now live under the <code>logging</code> key:</p> <pre>logging:\n  driver: syslog\n  options:\n    syslog-address: \"tcp://192.168.0.42:123\"\n</pre>\n</li> <li>\n<p><code>links</code> with environment variables: As documented in the <a href=\"../link-env-deprecated/index\">environment variables reference</a>, environment variables created by links have been deprecated for some time. In the new Docker network system, they have been removed. You should either connect directly to the appropriate hostname or set the relevant environment variable yourself, using the link hostname:</p> <pre>web:\n  links:\n    - db\n  environment:\n    - DB_PORT=tcp://db:5432\n</pre>\n</li> <li>\n<p><code>external_links</code>: Compose uses Docker networks when running version 2 projects, so links behave slightly differently. In particular, two containers must be connected to at least one network in common in order to communicate, even if explicitly linked together.</p> <p>Either connect the external container to your app’s <a href=\"../networking/index\">default network</a>, or connect both the external container and your service’s containers to an <a href=\"../networking/index#using-a-pre-existing-network\">external network</a>.</p>\n</li> <li>\n<p><code>net</code>: This is now replaced by <a href=\"#network_mode\">network_mode</a>:</p> <pre>net: host    -&gt;  network_mode: host\nnet: bridge  -&gt;  network_mode: bridge\nnet: none    -&gt;  network_mode: none\n</pre> <p>If you’re using <code>net: \"container:[service name]\"</code>, you must now use <code>network_mode: \"service:[service name]\"</code> instead.</p> <pre>net: \"container:web\"  -&gt;  network_mode: \"service:web\"\n</pre> <p>If you’re using <code>net: \"container:[container name/id]\"</code>, the value does not need to change.</p> <pre>net: \"container:cont-name\"  -&gt;  network_mode: \"container:cont-name\"\nnet: \"container:abc12345\"   -&gt;  network_mode: \"container:abc12345\"\n</pre>\n</li> <li>\n<p><code>volumes</code> with named volumes: these must now be explicitly declared in a top-level <code>volumes</code> section of your Compose file. If a service mounts a named volume called <code>data</code>, you must declare a <code>data</code> volume in your top-level <code>volumes</code> section. The whole file might look like this:</p> <pre>version: '2'\nservices:\n  db:\n    image: postgres\n    volumes:\n      - data:/var/lib/postgresql/data\nvolumes:\n  data: {}\n</pre> <p>By default, Compose creates a volume whose name is prefixed with your project name. If you want it to just be called <code>data</code>, declare it as external:</p> <pre>volumes:\n  data:\n    external: true\n</pre>\n</li> </ul> <h2 id=\"variable-substitution\">Variable substitution</h2> <p>Your configuration options can contain environment variables. Compose uses the variable values from the shell environment in which <code>docker-compose</code> is run. For example, suppose the shell contains <code>EXTERNAL_PORT=8000</code> and you supply this configuration:</p> <pre>web:\n  build: .\n  ports:\n    - \"${EXTERNAL_PORT}:5000\"\n</pre> <p>When you run <code>docker-compose up</code> with this configuration, Compose looks for the <code>EXTERNAL_PORT</code> environment variable in the shell and substitutes its value in. In this example, Compose resolves the port mapping to <code>\"8000:5000\"</code> before creating the <code>web</code> container.</p> <p>If an environment variable is not set, Compose substitutes with an empty string. In the example above, if <code>EXTERNAL_PORT</code> is not set, the value for the port mapping is <code>:5000</code> (which is of course an invalid port mapping, and will result in an error when attempting to create the container).</p> <p>Both <code>$VARIABLE</code> and <code>${VARIABLE}</code> syntax are supported. Extended shell-style features, such as <code>${VARIABLE-default}</code> and <code>${VARIABLE/foo/bar}</code>, are not supported.</p> <p>You can use a <code>$$</code> (double-dollar sign) when your configuration needs a literal dollar sign. This also prevents Compose from interpolating a value, so a <code>$$</code> allows you to refer to environment variables that you don’t want processed by Compose.</p> <pre>web:\n  build: .\n  command: \"$$VAR_NOT_INTERPOLATED_BY_COMPOSE\"\n</pre> <p>If you forget and use a single dollar sign (<code>$</code>), Compose interprets the value as an environment variable and will warn you:</p> <p>The VAR_NOT_INTERPOLATED_BY_COMPOSE is not set. Substituting an empty string.</p> <h2 id=\"compose-documentation\">Compose documentation</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/compose-file/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/compose-file/</a>\n  </p>\n</div>\n","compose/reference/events/index":"<h1 id=\"events\">events</h1> <pre>Usage: events [options] [SERVICE...]\n\nOptions:\n    --json      Output events as a stream of json objects\n</pre> <p>Stream container events for every container in the project.</p> <p>With the <code>--json</code> flag, a json object will be printed one per line with the format:</p> <pre>{\n    \"service\": \"web\",\n    \"event\": \"create\",\n    \"container\": \"213cf75fc39a\",\n    \"image\": \"alpine:edge\",\n    \"time\": \"2015-11-20T18:01:03.615550\",\n}\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/events/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/events/</a>\n  </p>\n</div>\n","compose/reference/create/index":"<h1 id=\"create\">create</h1> <pre>Creates containers for a service.\n\nUsage: create [options] [SERVICE...]\n\nOptions:\n    --force-recreate       Recreate containers even if their configuration and\n                           image haven't changed. Incompatible with --no-recreate.\n    --no-recreate          If containers already exist, don't recreate them.\n                           Incompatible with --force-recreate.\n    --no-build             Don't build an image, even if it's missing.\n    --build                Build images before creating containers.\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/create/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/create/</a>\n  </p>\n</div>\n","compose/reference/down/index":"<h1 id=\"down\">down</h1> <pre>Usage: down [options]\n\nOptions:\n    --rmi type          Remove images. Type must be one of:\n                        'all': Remove all images used by any service.\n                        'local': Remove only images that don't have a custom tag\n                        set by the `image` field.\n    -v, --volumes       Remove named volumes declared in the `volumes` section\n                        of the Compose file and anonymous volumes\n                        attached to containers.\n    --remove-orphans    Remove containers for services not defined in the\n                        Compose file\n</pre> <p>Stops containers and removes containers, networks, volumes, and images created by <code>up</code>.</p> <p>By default, the only things removed are:</p> <ul> <li>Containers for services defined in the Compose file</li> <li>Networks defined in the <code>networks</code> section of the Compose file</li> <li>The default network, if one is used</li> </ul> <p>Networks and volumes defined as <code>external</code> are never removed.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/down/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/down/</a>\n  </p>\n</div>\n","compose/reference/exec/index":"<h1 id=\"exec\">exec</h1> <pre>Usage: exec [options] SERVICE COMMAND [ARGS...]\n\nOptions:\n-d                Detached mode: Run command in the background.\n--privileged      Give extended privileges to the process.\n--user USER       Run the command as this user.\n-T                Disable pseudo-tty allocation. By default `docker-compose exec`\n                  allocates a TTY.\n--index=index     index of the container if there are multiple\n                  instances of a service [default: 1]\n</pre> <p>This is equivalent of <code>docker exec</code>. With this subcommand you can run arbitrary commands in your services. Commands are by default allocating a TTY, so you can do e.g. <code>docker-compose exec web sh</code> to get an interactive prompt.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/exec/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/exec/</a>\n  </p>\n</div>\n","compose/reference/help/index":"<h1 id=\"help\">help</h1> <pre>Usage: help COMMAND\n</pre> <p>Displays help and usage instructions for a command.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/help/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/help/</a>\n  </p>\n</div>\n","compose/reference/kill/index":"<h1 id=\"kill\">kill</h1> <pre>Usage: kill [options] [SERVICE...]\n\nOptions:\n-s SIGNAL         SIGNAL to send to the container. Default signal is SIGKILL.\n</pre> <p>Forces running containers to stop by sending a <code>SIGKILL</code> signal. Optionally the signal can be passed, for example:</p> <pre>$ docker-compose kill -s SIGINT\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/kill/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/kill/</a>\n  </p>\n</div>\n","compose/reference/pause/index":"<h1 id=\"pause\">pause</h1> <pre>Usage: pause [SERVICE...]\n</pre> <p>Pauses running containers of a service. They can be unpaused with <code>docker-compose unpause</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/pause/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/pause/</a>\n  </p>\n</div>\n","compose/reference/logs/index":"<h1 id=\"logs\">logs</h1> <pre>Usage: logs [options] [SERVICE...]\n\nOptions:\n--no-color          Produce monochrome output.\n-f, --follow        Follow log output\n-t, --timestamps    Show timestamps\n--tail              Number of lines to show from the end of the logs\n                    for each container.\n</pre> <p>Displays log output from services.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/logs/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/logs/</a>\n  </p>\n</div>\n","compose/reference/port/index":"<h1 id=\"port\">port</h1> <pre>Usage: port [options] SERVICE PRIVATE_PORT\n\nOptions:\n--protocol=proto  tcp or udp [default: tcp]\n--index=index     index of the container if there are multiple\n                  instances of a service [default: 1]\n</pre> <p>Prints the public port for a port binding.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/port/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/port/</a>\n  </p>\n</div>\n","compose/reference/ps/index":"<h1 id=\"ps\">ps</h1> <pre>Usage: ps [options] [SERVICE...]\n\nOptions:\n-q    Only display IDs\n</pre> <p>Lists containers.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/ps/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/ps/</a>\n  </p>\n</div>\n","compose/reference/pull/index":"<h1 id=\"pull\">pull</h1> <pre>Usage: pull [options] [SERVICE...]\n\nOptions:\n--ignore-pull-failures  Pull what it can and ignores images with pull failures.\n</pre> <p>Pulls service images.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/pull/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/pull/</a>\n  </p>\n</div>\n","compose/reference/restart/index":"<h1 id=\"restart\">restart</h1> <pre>Usage: restart [options] [SERVICE...]\n\nOptions:\n-t, --timeout TIMEOUT      Specify a shutdown timeout in seconds. (default: 10)\n</pre> <p>Restarts services.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/restart/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/restart/</a>\n  </p>\n</div>\n","compose/reference/rm/index":"<h1 id=\"rm\">rm</h1> <pre>Usage: rm [options] [SERVICE...]\n\nOptions:\n    -f, --force   Don't ask to confirm removal\n    -v            Remove any anonymous volumes attached to containers\n    -a, --all     Also remove one-off containers created by\n                  docker-compose run\n</pre> <p>Removes stopped service containers.</p> <p>By default, anonymous volumes attached to containers will not be removed. You can override this with <code>-v</code>. To list all volumes, use <code>docker volume ls</code>.</p> <p>Any data which is not in a volume will be lost.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/rm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/rm/</a>\n  </p>\n</div>\n","compose/reference/start/index":"<h1 id=\"start\">start</h1> <pre>Usage: start [SERVICE...]\n</pre> <p>Starts existing containers for a service.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/start/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/start/</a>\n  </p>\n</div>\n","compose/reference/scale/index":"<h1 id=\"scale\">scale</h1> <pre>Usage: scale [SERVICE=NUM...]\n</pre> <p>Sets the number of containers to run for a service.</p> <p>Numbers are specified as arguments in the form <code>service=num</code>. For example:</p> <pre>$ docker-compose scale web=2 worker=3\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/scale/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/scale/</a>\n  </p>\n</div>\n","compose/reference/run/index":"<h1 id=\"run\">run</h1> <pre>Usage: run [options] [-e KEY=VAL...] SERVICE [COMMAND] [ARGS...]\n\nOptions:\n-d                    Detached mode: Run container in the background, print\n                          new container name.\n--name NAME           Assign a name to the container\n--entrypoint CMD      Override the entrypoint of the image.\n-e KEY=VAL            Set an environment variable (can be used multiple times)\n-u, --user=\"\"         Run as specified username or uid\n--no-deps             Don't start linked services.\n--rm                  Remove container after run. Ignored in detached mode.\n-p, --publish=[]      Publish a container's port(s) to the host\n--service-ports       Run command with the service's ports enabled and mapped to the host.\n-T                    Disable pseudo-tty allocation. By default `docker-compose run` allocates a TTY.\n-w, --workdir=\"\"      Working directory inside the container\n</pre> <p>Runs a one-time command against a service. For example, the following command starts the <code>web</code> service and runs <code>bash</code> as its command.</p> <pre>$ docker-compose run web bash\n</pre> <p>Commands you use with <code>run</code> start in new containers with the same configuration as defined by the service’ configuration. This means the container has the same volumes, links, as defined in the configuration file. There two differences though.</p> <p>First, the command passed by <code>run</code> overrides the command defined in the service configuration. For example, if the <code>web</code> service configuration is started with <code>bash</code>, then <code>docker-compose run web python app.py</code> overrides it with <code>python app.py</code>.</p> <p>The second difference is the <code>docker-compose run</code> command does not create any of the ports specified in the service configuration. This prevents the port collisions with already open ports. If you <em>do want</em> the service’s ports created and mapped to the host, specify the <code>--service-ports</code> flag:</p> <pre>$ docker-compose run --service-ports web python manage.py shell\n</pre> <p>Alternatively manual port mapping can be specified. Same as when running Docker’s <code>run</code> command - using <code>--publish</code> or <code>-p</code> options:</p> <pre>$ docker-compose run --publish 8080:80 -p 2022:22 -p 127.0.0.1:2021:21 web python manage.py shell\n</pre> <p>If you start a service configured with links, the <code>run</code> command first checks to see if the linked service is running and starts the service if it is stopped. Once all the linked services are running, the <code>run</code> executes the command you passed it. So, for example, you could run:</p> <pre>$ docker-compose run db psql -h db -U docker\n</pre> <p>This would open up an interactive PostgreSQL shell for the linked <code>db</code> container.</p> <p>If you do not want the <code>run</code> command to start linked containers, specify the <code>--no-deps</code> flag:</p> <pre>$ docker-compose run --no-deps web python manage.py shell\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/run/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/run/</a>\n  </p>\n</div>\n","compose/reference/stop/index":"<h1 id=\"stop\">stop</h1> <pre>Usage: stop [options] [SERVICE...]\n\nOptions:\n-t, --timeout TIMEOUT      Specify a shutdown timeout in seconds (default: 10).\n</pre> <p>Stops running containers without removing them. They can be started again with <code>docker-compose start</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/stop/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/stop/</a>\n  </p>\n</div>\n","compose/reference/unpause/index":"<h1 id=\"unpause\">unpause</h1> <pre>Usage: unpause [SERVICE...]\n</pre> <p>Unpauses paused containers of a service.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/unpause/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/unpause/</a>\n  </p>\n</div>\n","compose/reference/up/index":"<h1 id=\"up\">up</h1> <pre>Usage: up [options] [SERVICE...]\n\nOptions:\n    -d                         Detached mode: Run containers in the background,\n                               print new container names.\n                               Incompatible with --abort-on-container-exit.\n    --no-color                 Produce monochrome output.\n    --no-deps                  Don't start linked services.\n    --force-recreate           Recreate containers even if their configuration\n                               and image haven't changed.\n                               Incompatible with --no-recreate.\n    --no-recreate              If containers already exist, don't recreate them.\n                               Incompatible with --force-recreate.\n    --no-build                 Don't build an image, even if it's missing.\n    --build                    Build images before starting containers.\n    --abort-on-container-exit  Stops all containers if any container was stopped.\n                               Incompatible with -d.\n    -t, --timeout TIMEOUT      Use this timeout in seconds for container shutdown\n                               when attached or when containers are already\n                               running. (default: 10)\n    --remove-orphans           Remove containers for services not defined in\n                               the Compose file\n\n</pre> <p>Builds, (re)creates, starts, and attaches to containers for a service.</p> <p>Unless they are already running, this command also starts any linked services.</p> <p>The <code>docker-compose up</code> command aggregates the output of each container. When the command exits, all containers are stopped. Running <code>docker-compose up -d</code> starts the containers in the background and leaves them running.</p> <p>If there are existing containers for a service, and the service’s configuration or image was changed after the container’s creation, <code>docker-compose up</code> picks up the changes by stopping and recreating the containers (preserving mounted volumes). To prevent Compose from picking up changes, use the <code>--no-recreate</code> flag.</p> <p>If you want to force Compose to stop and recreate all containers, use the <code>--force-recreate</code> flag.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/up/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/up/</a>\n  </p>\n</div>\n","compose/link-env-deprecated/index":"<h1 id=\"link-environment-variables-reference\">Link environment variables reference</h1> <blockquote> <p><strong>Note:</strong> Environment variables are no longer the recommended method for connecting to linked services. Instead, you should use the link name (by default, the name of the linked service) as the hostname to connect to. See the <a href=\"../compose-file/index#links\">docker-compose.yml documentation</a> for details.</p> <p>Environment variables will only be populated if you’re using the <a href=\"../compose-file/index#versioning\">legacy version 1 Compose file format</a>.</p> </blockquote> <p>Compose uses <a href=\"../../engine/userguide/networking/default_network/dockerlinks/index\">Docker links</a> to expose services’ containers to one another. Each linked container injects a set of environment variables, each of which begins with the uppercase name of the container.</p> <p>To see what environment variables are available to a service, run <code>docker-compose run SERVICE env</code>.</p> <p><b><i>name</i>_PORT</b><br> Full URL, e.g. <code>DB_PORT=tcp://172.17.0.5:5432</code></p> <p><b><i>name</i>_PORT_<i>num</i>_<i>protocol</i></b><br> Full URL, e.g. <code>DB_PORT_5432_TCP=tcp://172.17.0.5:5432</code></p> <p><b><i>name</i>_PORT_<i>num</i>_<i>protocol</i>_ADDR</b><br> Container’s IP address, e.g. <code>DB_PORT_5432_TCP_ADDR=172.17.0.5</code></p> <p><b><i>name</i>_PORT_<i>num</i>_<i>protocol</i>_PORT</b><br> Exposed port number, e.g. <code>DB_PORT_5432_TCP_PORT=5432</code></p> <p><b><i>name</i>_PORT_<i>num</i>_<i>protocol</i>_PROTO</b><br> Protocol (tcp or udp), e.g. <code>DB_PORT_5432_TCP_PROTO=tcp</code></p> <p><b><i>name</i>_NAME</b><br> Fully qualified container name, e.g. <code>DB_1_NAME=/myapp_web_1/myapp_db_1</code></p> <h2 id=\"related-information\">Related Information</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/link-env-deprecated/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/link-env-deprecated/</a>\n  </p>\n</div>\n","compose/completion/index":"<h1 id=\"command-line-completion\">Command-line Completion</h1> <p>Compose comes with <a href=\"http://en.wikipedia.org/wiki/Command-line_completion\">command completion</a> for the bash and zsh shell.</p> <h2 id=\"installing-command-completion\">Installing Command Completion</h2> <h3 id=\"bash\">Bash</h3> <p>Make sure bash completion is installed. If you use a current Linux in a non-minimal installation, bash completion should be available. On a Mac, install with <code>brew install bash-completion</code></p> <p>Place the completion script in <code>/etc/bash_completion.d/</code> (<code>/usr/local/etc/bash_completion.d/</code> on a Mac), using e.g.</p> <pre> curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose version --short)/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose\n</pre> <p>Completion will be available upon next login.</p> <h3 id=\"zsh\">Zsh</h3> <p>Place the completion script in your <code>/path/to/zsh/completion</code>, using e.g. <code>~/.zsh/completion/</code></p> <pre>mkdir -p ~/.zsh/completion\ncurl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose version --short)/contrib/completion/zsh/_docker-compose &gt; ~/.zsh/completion/_docker-compose\n</pre> <p>Include the directory in your <code>$fpath</code>, e.g. by adding in <code>~/.zshrc</code></p> <pre>fpath=(~/.zsh/completion $fpath)\n</pre> <p>Make sure <code>compinit</code> is loaded or do it by adding in <code>~/.zshrc</code></p> <pre>autoload -Uz compinit &amp;&amp; compinit -i\n</pre> <p>Then reload your shell</p> <pre>exec $SHELL -l\n</pre> <h2 id=\"available-completions\">Available completions</h2> <p>Depending on what you typed on the command line so far, it will complete</p> <ul> <li>available docker-compose commands</li> <li>options that are available for a particular command</li> <li>service names that make sense in a given context (e.g. services with running or stopped instances or services based on images vs. services based on Dockerfiles). For <code>docker-compose scale</code>, completed service names will automatically have “=” appended.</li> <li>arguments for selected options, e.g. <code>docker-compose kill -s</code> will complete some signals like SIGHUP and SIGUSR1.</li> </ul> <p>Enjoy working with Compose faster and with less typos!</p> <h2 id=\"compose-documentation\">Compose documentation</h2> <ul> <li><a href=\"../index\">User guide</a></li> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/completion/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/completion/</a>\n  </p>\n</div>\n","compose/startup-order/index":"<h1 id=\"controlling-startup-order-in-compose\">Controlling startup order in Compose</h1> <p>You can control the order of service startup with the <a href=\"../compose-file/index#depends-on\">depends_on</a> option. Compose always starts containers in dependency order, where dependencies are determined by <code>depends_on</code>, <code>links</code>, <code>volumes_from</code> and <code>network_mode: \"service:...\"</code>.</p> <p>However, Compose will not wait until a container is “ready” (whatever that means for your particular application) - only until it’s running. There’s a good reason for this.</p> <p>The problem of waiting for a database (for example) to be ready is really just a subset of a much larger problem of distributed systems. In production, your database could become unavailable or move hosts at any time. Your application needs to be resilient to these types of failures.</p> <p>To handle this, your application should attempt to re-establish a connection to the database after a failure. If the application retries the connection, it should eventually be able to connect to the database.</p> <p>The best solution is to perform this check in your application code, both at startup and whenever a connection is lost for any reason. However, if you don’t need this level of resilience, you can work around the problem with a wrapper script:</p> <ul> <li>\n<p>Use a tool such as <a href=\"https://github.com/vishnubob/wait-for-it\">wait-for-it</a> or <a href=\"https://github.com/jwilder/dockerize\">dockerize</a>. These are small wrapper scripts which you can include in your application’s image and will poll a given host and port until it’s accepting TCP connections.</p> <p>Supposing your application’s image has a <code>CMD</code> set in its Dockerfile, you can wrap it by setting the entrypoint in <code>docker-compose.yml</code>:</p> <pre>version: \"2\"\nservices:\n  web:\n    build: .\n    ports:\n      - \"80:8000\"\n    depends_on:\n      - \"db\"\n    entrypoint: ./wait-for-it.sh db:5432\n  db:\n    image: postgres\n</pre>\n</li> <li>\n<p>Write your own wrapper script to perform a more application-specific health check. For example, you might want to wait until Postgres is definitely ready to accept commands:</p> <pre>#!/bin/bash\n\nset -e\n\nhost=\"$1\"\nshift\ncmd=\"$@\"\n\nuntil psql -h \"$host\" -U \"postgres\" -c '\\l'; do\n  &gt;&amp;2 echo \"Postgres is unavailable - sleeping\"\n  sleep 1\ndone\n\n&gt;&amp;2 echo \"Postgres is up - executing command\"\nexec $cmd\n</pre> <p>You can use this as a wrapper script as in the previous example, by setting <code>entrypoint: ./wait-for-postgres.sh db</code>.</p>\n</li> </ul> <h2 id=\"compose-documentation\">Compose documentation</h2> <ul> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/startup-order/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/startup-order/</a>\n  </p>\n</div>\n","compose/faq/index":"<h1 id=\"frequently-asked-questions\">Frequently asked questions</h1> <p>If you don’t see your question here, feel free to drop by <code>#docker-compose</code> on freenode IRC and ask the community.</p> <h2 id=\"can-i-control-service-startup-order\">Can I control service startup order?</h2> <p>Yes - see <a href=\"../startup-order/index\">Controlling startup order</a>.</p> <h2 id=\"why-do-my-services-take-10-seconds-to-recreate-or-stop\">Why do my services take 10 seconds to recreate or stop?</h2> <p>Compose stop attempts to stop a container by sending a <code>SIGTERM</code>. It then waits for a <a href=\"../reference/stop/index\">default timeout of 10 seconds</a>. After the timeout, a <code>SIGKILL</code> is sent to the container to forcefully kill it. If you are waiting for this timeout, it means that your containers aren’t shutting down when they receive the <code>SIGTERM</code> signal.</p> <p>There has already been a lot written about this problem of <a href=\"https://medium.com/@gchudnov/trapping-signals-in-docker-containers-7a57fdda7d86\">processes handling signals</a> in containers.</p> <p>To fix this problem, try the following:</p> <ul> <li>Make sure you’re using the JSON form of <code>CMD</code> and <code>ENTRYPOINT</code> in your Dockerfile.</li> </ul> <p>For example use <code>[\"program\", \"arg1\", \"arg2\"]</code> not <code>\"program arg1 arg2\"</code>. Using the string form causes Docker to run your process using <code>bash</code> which doesn’t handle signals properly. Compose always uses the JSON form, so don’t worry if you override the command or entrypoint in your Compose file.</p> <ul> <li><p>If you are able, modify the application that you’re running to add an explicit signal handler for <code>SIGTERM</code>.</p></li> <li>\n<p>Set the <code>stop_signal</code> to a signal which the application knows how to handle:</p> <p>web: build: . stop_signal: SIGINT</p>\n</li> <li><p>If you can’t modify the application, wrap the application in a lightweight init system (like <a href=\"http://skarnet.org/software/s6/\">s6</a>) or a signal proxy (like <a href=\"https://github.com/Yelp/dumb-init\">dumb-init</a> or <a href=\"https://github.com/krallin/tini\">tini</a>). Either of these wrappers take care of handling <code>SIGTERM</code> properly.</p></li> </ul> <h2 id=\"how-do-i-run-multiple-copies-of-a-compose-file-on-the-same-host\">How do I run multiple copies of a Compose file on the same host?</h2> <p>Compose uses the project name to create unique identifiers for all of a project’s containers and other resources. To run multiple copies of a project, set a custom project name using the <a href=\"../reference/overview/index\"><code>-p</code> command line option</a> or the <a href=\"../reference/envvars/index#compose-project-name\"><code>COMPOSE_PROJECT_NAME</code> environment variable</a>.</p> <h2 id=\"what-s-the-difference-between-up-run-and-start\">What’s the difference between <code>up</code>, <code>run</code>, and <code>start</code>?</h2> <p>Typically, you want <code>docker-compose up</code>. Use <code>up</code> to start or restart all the services defined in a <code>docker-compose.yml</code>. In the default “attached” mode, you’ll see all the logs from all the containers. In “detached” mode (<code>-d</code>), Compose exits after starting the containers, but the containers continue to run in the background.</p> <p>The <code>docker-compose run</code> command is for running “one-off” or “adhoc” tasks. It requires the service name you want to run and only starts containers for services that the running service depends on. Use <code>run</code> to run tests or perform an administrative task such as removing or adding data to a data volume container. The <code>run</code> command acts like <code>docker run -ti</code> in that it opens an interactive terminal to the container and returns an exit status matching the exit status of the process in the container.</p> <p>The <code>docker-compose start</code> command is useful only to restart containers that were previously created, but were stopped. It never creates new containers.</p> <h2 id=\"can-i-use-json-instead-of-yaml-for-my-compose-file\">Can I use json instead of yaml for my Compose file?</h2> <p>Yes. <a href=\"http://stackoverflow.com/a/1729545/444646\">Yaml is a superset of json</a> so any JSON file should be valid Yaml. To use a JSON file with Compose, specify the filename to use, for example:</p> <pre>docker-compose -f docker-compose.json up\n</pre> <h2 id=\"should-i-include-my-code-with-copy-add-or-a-volume\">Should I include my code with <code>COPY</code>/<code>ADD</code> or a volume?</h2> <p>You can add your code to the image using <code>COPY</code> or <code>ADD</code> directive in a <code>Dockerfile</code>. This is useful if you need to relocate your code along with the Docker image, for example when you’re sending code to another environment (production, CI, etc).</p> <p>You should use a <code>volume</code> if you want to make changes to your code and see them reflected immediately, for example when you’re developing code and your server supports hot code reloading or live-reload.</p> <p>There may be cases where you’ll want to use both. You can have the image include the code using a <code>COPY</code>, and use a <code>volume</code> in your Compose file to include the code from the host during development. The volume overrides the directory contents of the image.</p> <h2 id=\"where-can-i-find-example-compose-files\">Where can I find example compose files?</h2> <p>There are <a href=\"https://github.com/search?q=in%3Apath+docker-compose.yml+extension%3Ayml&amp;type=Code\">many examples of Compose files on github</a>.</p> <h2 id=\"compose-documentation\">Compose documentation</h2> <ul> <li><a href=\"../install/index\">Installing Compose</a></li> <li><a href=\"../django/index\">Get started with Django</a></li> <li><a href=\"../rails/index\">Get started with Rails</a></li> <li><a href=\"../wordpress/index\">Get started with WordPress</a></li> <li><a href=\"../reference/index\">Command line reference</a></li> <li><a href=\"../compose-file/index\">Compose file reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/faq/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/faq/</a>\n  </p>\n</div>\n","machine/install-machine/index":"<h1 id=\"install-docker-machine\">Install Docker Machine</h1> <p>On OS X and Windows, Machine is installed along with other Docker products when you install the Docker Toolbox. For details on installing Docker Toolbox, see the <a href=\"https://docs.docker.com/installation/mac/\" target=\"_blank\">Mac OS X installation</a> instructions or <a href=\"https://docs.docker.com/installation/windows\" target=\"_blank\">Windows installation</a> instructions.</p> <p>If you want only Docker Machine, you can install the Machine binaries directly by following the instructions in the next section. You can find the latest versions of the binaries are on the <a href=\"https://github.com/docker/machine/releases/\" target=\"_blank\"> docker/machine release page</a> on GitHub.</p> <h2 id=\"installing-machine-directly\">Installing Machine Directly</h2> <ol> <li><p>Install <a href=\"https://docs.docker.com/installation/\" target=\"_blank\">the Docker binary</a>.</p></li> <li>\n<p>Download the Docker Machine binary and extract it to your PATH.</p> <p>If you are running OS X or Linux:</p> <pre>$ curl -L https://github.com/docker/machine/releases/download/v0.7.0/docker-machine-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-machine &amp;&amp; \\\nchmod +x /usr/local/bin/docker-machine\n</pre> <p>If you are running Windows with git bash</p> <pre>$ if [[ ! -d \"$HOME/bin\" ]]; then mkdir -p \"$HOME/bin\"; fi &amp;&amp; \\\ncurl -L https://github.com/docker/machine/releases/download/v0.7.0/docker-machine-Windows-x86_64.exe &gt; \"$HOME/bin/docker-machine.exe\" &amp;&amp; \\\nchmod +x \"$HOME/bin/docker-machine.exe\"\n</pre> <p>Otherwise, download one of the releases from the <a href=\"https://github.com/docker/machine/releases/\" target=\"_blank\"> docker/machine release page</a> directly.</p>\n</li> <li>\n<p>Check the installation by displaying the Machine version:</p> <pre>$ docker-machine version\ndocker-machine version 0.7.0, build 61388e9\n</pre>\n</li> </ol> <h2 id=\"installing-bash-completion-scripts\">Installing bash completion scripts</h2> <p>The Machine repository supplies several <code>bash</code> scripts that add features such as:</p> <ul> <li>command completion</li> <li>a function that displays the active machine in your shell prompt</li> <li>a function wrapper that adds a <code>docker-machine use</code> subcommand to switch the active machine</li> </ul> <p>To install the scripts, copy or link them into your <code>/etc/bash_completion.d</code> or <code>/usr/local/etc/bash_completion.d</code> directory. To enable the <code>docker-machine</code> shell prompt, add <code>$(__docker_machine_ps1)</code> to your <code>PS1</code> setting in <code>~/.bashrc</code>.</p> <pre>PS1='[\\u@\\h \\W$(__docker_machine_ps1)]\\$ '\n</pre> <p>You can find additional documentation in the comments at the <a href=\"https://github.com/docker/machine/tree/master/contrib/completion/bash\" target=\"_blank\">top of each script</a>.</p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><a href=\"../overview/index\">Docker Machine overview</a></li> <li>Create and run a Docker host on your <a href=\"../get-started/index\">local system using VirtualBox</a>\n</li> <li>Provision multiple Docker hosts <a href=\"../get-started-cloud/index\">on your cloud provider</a>\n</li> <li><a href=\"https://docs.docker.com/machine/drivers/\" target=\"_blank\">Docker Machine driver reference</a></li> <li><a href=\"https://docs.docker.com/machine/reference/\" target=\"_blank\">Docker Machine subcommand reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/install-machine/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/install-machine/</a>\n  </p>\n</div>\n","machine/overview/index":"<h1 id=\"docker-machine-overview\">Docker Machine Overview</h1> <p>You can use Docker Machine to:</p> <ul> <li>Install and run Docker on Mac or Windows</li> <li>Provision and manage multiple remote Docker hosts</li> <li>Provision Swarm clusters</li> </ul> <h2 id=\"what-is-docker-machine\">What is Docker Machine?</h2> <p>Docker Machine is a tool that lets you install Docker Engine on virtual hosts, and manage the hosts with <code>docker-machine</code> commands. You can use Machine to create Docker hosts on your local Mac or Windows box, on your company network, in your data center, or on cloud providers like AWS or Digital Ocean.</p> <p>Using <code>docker-machine</code> commands, you can start, inspect, stop, and restart a managed host, upgrade the Docker client and daemon, and configure a Docker client to talk to your host.</p> <p>Point the Machine CLI at a running, managed host, and you can run <code>docker</code> commands directly on that host. For example, run <code>docker-machine env default</code> to point to a host called <code>default</code>, follow on-screen instructions to complete <code>env</code> setup, and run <code>docker ps</code>, <code>docker run hello-world</code>, and so forth.</p> <h2 id=\"why-should-i-use-it\">Why should I use it?</h2> <p>Machine is currently the only way to run Docker on Mac or Windows, and the best way to provision multiple remote Docker hosts on various flavors of Linux.</p> <p>Docker Machine has these two broad use cases.</p> <ul> <li><strong>I want to run Docker on Mac or Windows</strong></li> </ul> <p><img src=\"https://docs.docker.com/v1.11/machine/img/machine-mac-win.png\" alt=\"Docker Machine on Mac and Windows\"></p> <p>If you work primarily on a Mac or Windows laptop or desktop, you need Docker Machine in order to “run Docker” (that is, Docker Engine) locally. Installing Docker Machine on a Mac or Windows box provisions a local virtual machine with Docker Engine, gives you the ability to connect it, and run <code>docker</code> commands.</p> <ul> <li><strong>I want to provision Docker hosts on remote systems</strong></li> </ul> <p><img src=\"https://docs.docker.com/v1.11/machine/img/provision-use-case.png\" alt=\"Docker Machine for provisioning multiple systems\"></p> <p>Docker Engine runs natively on Linux systems. If you have a Linux box as your primary system, and want to run <code>docker</code> commands, all you need to do is download and install Docker Engine. However, if you want an efficient way to provision multiple Docker hosts on a network, in the cloud or even locally, you need Docker Machine.</p> <p>Whether your primary system is Mac, Windows, or Linux, you can install Docker Machine on it and use <code>docker-machine</code> commands to provision and manage large numbers of Docker hosts. It automatically creates hosts, installs Docker Engine on them, then configures the <code>docker</code> clients. Each managed host (”<strong><em>machine</em></strong>”) is the combination of a Docker host and a configured client.</p> <h2 id=\"what-s-the-difference-between-docker-engine-and-docker-machine\">What’s the difference between Docker Engine and Docker Machine?</h2> <p>When people say “Docker” they typically mean <strong>Docker Engine</strong>, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts <code>docker</code> commands from the CLI, such as <code>docker run &lt;image&gt;</code>, <code>docker ps</code> to list running containers, <code>docker images</code> to list images, and so on.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/engine.png\" alt=\"Docker Engine\"></p> <p><strong>Docker Machine</strong> is a tool for provisioning and managing your Dockerized hosts (hosts with Docker Engine on them). Typically, you install Docker Machine on your local system. Docker Machine has its own command line client <code>docker-machine</code> and the Docker Engine client, <code>docker</code>. You can use Machine to install Docker Engine on one or more virtual systems. These virtual systems can be local (as when you use Machine to install and run Docker Engine in VirtualBox on Mac or Windows) or remote (as when you use Machine to provision Dockerized hosts on cloud providers). The Dockerized hosts themselves can be thought of, and are sometimes referred to as, managed “<strong><em>machines</em></strong>”.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/machine.png\" alt=\"Docker Machine\"></p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><a href=\"../install-machine/index\">Install Docker Machine</a></li> <li>Create and run a Docker host on your <a href=\"../get-started/index\">local system using VirtualBox</a>\n</li> <li>Provision multiple Docker hosts <a href=\"../get-started-cloud/index\">on your cloud provider</a>\n</li> <li><a href=\"../../swarm/provision-with-machine/index\">Provision a Docker Swarm cluster with Docker Machine</a></li> <li><a href=\"../concepts/index\">Understand Machine concepts</a></li> <li><a href=\"../drivers/index\">Docker Machine driver reference</a></li> <li><a href=\"../reference/index\">Docker Machine subcommand reference</a></li> <li><a href=\"../migrate-to-machine/index\">Migrate from Boot2Docker to Docker Machine</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/overview/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/overview/</a>\n  </p>\n</div>\n","machine/get-started/index":"<h1 id=\"get-started-with-docker-machine-and-a-local-vm\">Get started with Docker Machine and a local VM</h1> <p>Let’s take a look at using <code>docker-machine</code> for creating, using, and managing a Docker host inside of <a href=\"https://www.virtualbox.org/\" target=\"_blank\">VirtualBox</a>.</p> <h2 id=\"prerequisites\">Prerequisites</h2> <ul> <li><p>Make sure you have <a href=\"https://www.virtualbox.org/wiki/Downloads\" target=\"_blank\">the latest VirtualBox</a> correctly installed on your system. If you used <a href=\"https://www.docker.com/products/docker-toolbox\" target=\"_blank\">Toolbox</a> for <a href=\"https://docs.docker.com/engine/installation/mac/\" target=\"_blank\">Mac</a> or <a href=\"https://docs.docker.com/engine/installation/windows/\" target=\"_blank\">Windows</a> to install Docker Machine, VirtualBox is automatically installed.</p></li> <li><p>If you used the Quickstart Terminal to launch your first machine and set your terminal environment to point to it, a default machine was automatically created. If this is the case, you can still follow along with these steps, but create another machine and name it something other than “default” (e.g., staging or sandbox).</p></li> </ul> <h2 id=\"use-machine-to-run-docker-containers\">Use Machine to run Docker containers</h2> <p>To run a Docker container, you:</p> <ul> <li>create a new (or start an existing) Docker virtual machine</li> <li>switch your environment to your new VM</li> <li>use the docker client to create, load, and manage containers</li> </ul> <p>Once you create a machine, you can reuse it as often as you like. Like any VirtualBox VM, it maintains its configuration between uses.</p> <p>The examples here show how to create and start a machine, run Docker commands, and work with containers.</p> <h2 id=\"create-a-machine\">Create a machine</h2> <ol> <li>\n<p>Open a command shell or terminal window.</p> <p>These command examples shows a Bash shell. For a different shell, such as C Shell, the same commands are the same except where noted.</p>\n</li> <li>\n<p>Use <code>docker-machine ls</code> to list available machines.</p> <p>In this example, no machines have been created yet.</p> <pre>$ docker-machine ls\nNAME   ACTIVE   DRIVER   STATE   URL   SWARM   DOCKER   ERRORS\n</pre>\n</li> <li>\n<p>Create a machine.</p> <p>Run the <code>docker-machine create</code> command, passing the string <code>virtualbox</code> to the <code>--driver</code> flag. The final argument is the name of the machine. If this is your first machine, name it <code>default</code>. If you already have a “default” machine, choose another name for this new machine.</p> <pre>$ docker-machine create --driver virtualbox default\nRunning pre-create checks...\nCreating machine...\n(staging) Copying /Users/ripley/.docker/machine/cache/boot2docker.iso to /Users/ripley/.docker/machine/machines/default/boot2docker.iso...\n(staging) Creating VirtualBox VM...\n(staging) Creating SSH key...\n(staging) Starting the VM...\n(staging) Waiting for an IP...\nWaiting for machine to be running, this may take a few minutes...\nMachine is running, waiting for SSH to be available...\nDetecting operating system of created instance...\nDetecting the provisioner...\nProvisioning with boot2docker...\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\nChecking connection to Docker...\nDocker is up and running!\nTo see how to connect Docker to this machine, run: docker-machine env default\n</pre> <p>This command downloads a lightweight Linux distribution (<a href=\"https://github.com/boot2docker/boot2docker\" target=\"_blank\">boot2docker</a>) with the Docker daemon installed, and creates and starts a VirtualBox VM with Docker running.</p>\n</li> <li>\n<p>List available machines again to see your newly minted machine.</p> <pre>$ docker-machine ls\nNAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS\ndefault   *        virtualbox   Running   tcp://192.168.99.187:2376           v1.9.1\n</pre>\n</li> <li>\n<p>Get the environment commands for your new VM.</p> <p>As noted in the output of the <code>docker-machine create</code> command, you need to tell Docker to talk to the new machine. You can do this with the <code>docker-machine env</code> command.</p> <pre>$ docker-machine env default\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://172.16.62.130:2376\"\nexport DOCKER_CERT_PATH=\"/Users/&lt;yourusername&gt;/.docker/machine/machines/default\"\nexport DOCKER_MACHINE_NAME=\"default\"\n# Run this command to configure your shell:\n# eval \"$(docker-machine env default)\"\n</pre>\n</li> <li>\n<p>Connect your shell to the new machine.</p> <pre>$ eval \"$(docker-machine env default)\"\n</pre> <p><strong>Note</strong>: If you are using <code>fish</code>, or a Windows shell such as Powershell/<code>cmd.exe</code> the above method will not work as described. Instead, see <a href=\"https://docs.docker.com/machine/reference/env/\" target=\"_blank\">the <code>env</code> command’s documentation</a> to learn how to set the environment variables for your shell.</p> <p>This sets environment variables for the current shell that the Docker client will read which specify the TLS settings. You need to do this each time you open a new shell or restart your machine.</p> <p>You can now run Docker commands on this host.</p>\n</li> </ol> <h2 id=\"run-containers-and-experiment-with-machine-commands\">Run containers and experiment with Machine commands</h2> <p>Run a container with <code>docker run</code> to verify your set up.</p> <ol> <li>\n<p>Use <code>docker run</code> to download and run <code>busybox</code> with a simple ‘echo’ command.</p> <pre>$ docker run busybox echo hello world\nUnable to find image 'busybox' locally\nPulling repository busybox\ne72ac664f4f0: Download complete\n511136ea3c5a: Download complete\ndf7546f9f060: Download complete\ne433a6c5b276: Download complete\nhello world\n</pre>\n</li> <li>\n<p>Get the host IP address.</p> <p>Any exposed ports are available on the Docker host’s IP address, which you can get using the <code>docker-machine ip</code> command:</p> <pre>$ docker-machine ip default\n192.168.99.100\n</pre>\n</li> <li>\n<p>Run a webserver (<a href=\"https://www.nginx.com/\" target=\"_blank\">nginx</a>) in a container with the following command:</p> <pre>$ docker run -d -p 8000:80 nginx\n</pre> <p>When the image is finished pulling, you can hit the server at port 8000 on the IP address given to you by <code>docker-machine ip</code>. For instance:</p> <pre>    $ curl $(docker-machine ip default):8000\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n    &lt;title&gt;Welcome to nginx!&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            width: 35em;\n            margin: 0 auto;\n            font-family: Tahoma, Verdana, Arial, sans-serif;\n        }\n    &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n    &lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n    &lt;p&gt;If you see this page, the nginx web server is successfully installed and\n    working. Further configuration is required.&lt;/p&gt;\n\n    &lt;p&gt;For online documentation and support please refer to\n    &lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\n    Commercial support is available at\n    &lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n    &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n</pre>\n</li> </ol> <p>You can create and manage as many local VMs running Docker as you please; just run <code>docker-machine create</code> again. All created machines will appear in the output of <code>docker-machine ls</code>.</p> <h2 id=\"start-and-stop-machines\">Start and stop machines</h2> <p>If you are finished using a host for the time being, you can stop it with <code>docker-machine stop</code> and later start it again with <code>docker-machine start</code>.</p> <pre>    $ docker-machine stop default\n    $ docker-machine start default\n</pre> <h2 id=\"operate-on-machines-without-specifying-the-name\">Operate on machines without specifying the name</h2> <p>Some <code>docker-machine</code> commands will assume that the given operation should be run on a machine named <code>default</code> (if it exists) if no machine name is specified. Because using a local VM named <code>default</code> is such a common pattern, this allows you to save some typing on the most frequently used Machine commands.</p> <p>For example:</p> <pre>      $ docker-machine stop\n      Stopping \"default\"....\n      Machine \"default\" was stopped.\n\n      $ docker-machine start\n      Starting \"default\"...\n      (default) Waiting for an IP...\n      Machine \"default\" was started.\n      Started machines may have new IP addresses.  You may need to re-run the `docker-machine env` command.\n\n      $ eval $(docker-machine env)\n\n      $ docker-machine ip\n        192.168.99.100\n</pre> <p>Commands that follow this style are:</p> <pre>    - `docker-machine config`\n    - `docker-machine env`\n    - `docker-machine inspect`\n    - `docker-machine ip`\n    - `docker-machine kill`\n    - `docker-machine provision`\n    - `docker-machine regenerate-certs`\n    - `docker-machine restart`\n    - `docker-machine ssh`\n    - `docker-machine start`\n    - `docker-machine status`\n    - `docker-machine stop`\n    - `docker-machine upgrade`\n    - `docker-machine url`\n</pre> <p>For machines other than <code>default</code>, and commands other than those listed above, you must always specify the name explicitly as an argument.</p> <h2 id=\"start-local-machines-on-startup\">Start local machines on startup</h2> <p>In order to ensure that the Docker client is automatically configured at the start of each shell session, some users like to embed <code>eval $(docker-machine env default)</code> in their shell profiles (e.g., the <code>~/.bash_profile</code> file). However, this fails if the <code>default</code> machine is not running. If desired, you can configure your system to start the <code>default</code> machine automatically.</p> <p>Here is an example of how to configure this on OS X.</p> <p>Create a file called <code>com.docker.machine.default.plist</code> under <code>~/Library/LaunchAgents</code> with the following content:</p> <pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n    &lt;dict&gt;\n        &lt;key&gt;EnvironmentVariables&lt;/key&gt;\n        &lt;dict&gt;\n            &lt;key&gt;PATH&lt;/key&gt;\n            &lt;string&gt;/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin&lt;/string&gt;\n        &lt;/dict&gt;\n        &lt;key&gt;Label&lt;/key&gt;\n        &lt;string&gt;com.docker.machine.default&lt;/string&gt;\n        &lt;key&gt;ProgramArguments&lt;/key&gt;\n        &lt;array&gt;\n            &lt;string&gt;/usr/local/bin/docker-machine&lt;/string&gt;\n            &lt;string&gt;start&lt;/string&gt;\n            &lt;string&gt;default&lt;/string&gt;\n        &lt;/array&gt;\n        &lt;key&gt;RunAtLoad&lt;/key&gt;\n        &lt;true/&gt;\n    &lt;/dict&gt;\n&lt;/plist&gt;\n</pre> <p>You can change the <code>default</code> string above to make this <code>LaunchAgent</code> start any machine(s) you desire.</p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li>Provision multiple Docker hosts <a href=\"../get-started-cloud/index\">on your cloud provider</a>\n</li> <li><a href=\"../concepts/index\">Understand Machine concepts</a></li> <li><a href=\"https://docs.docker.com/machine/drivers/\" target=\"_blank\">Docker Machine driver reference</a></li> <li><a href=\"https://docs.docker.com/machine/reference/\" target=\"_blank\">Docker Machine subcommand reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/get-started/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/get-started/</a>\n  </p>\n</div>\n","machine/get-started-cloud/index":"<h1 id=\"use-docker-machine-to-provision-hosts-on-cloud-providers\">Use Docker Machine to provision hosts on cloud providers</h1> <p>Docker Machine driver plugins are available for many cloud platforms, so you can use Machine to provision cloud hosts. When you use Docker Machine for provisioning, you create cloud hosts with Docker Engine installed on them.</p> <p>You’ll need to install and run Docker Machine, and create an account with the cloud provider.</p> <p>Then you provide account verification, security credentials, and configuration options for the providers as flags to <code>docker-machine create</code>. The flags are unique for each cloud-specific driver. For instance, to pass a Digital Ocean access token you use the <code>--digitalocean-access-token</code> flag. Take a look at the examples below for Digital Ocean and AWS.</p> <h2 id=\"examples\">Examples</h2> <h3 id=\"digital-ocean\">Digital Ocean</h3> <p>For Digital Ocean, this command creates a Droplet (cloud host) called “docker-sandbox”.</p> <pre>  $ docker-machine create --driver digitalocean --digitalocean-access-token xxxxx docker-sandbox\n</pre> <p>For a step-by-step guide on using Machine to create Docker hosts on Digital Ocean, see the <a href=\"../examples/ocean/index\">Digital Ocean Example</a>.</p> <h3 id=\"amazon-web-services-aws\">Amazon Web Services (AWS)</h3> <p>For AWS EC2, this command creates an instance called “aws-sandbox”:</p> <pre>  $ docker-machine create --driver amazonec2 --amazonec2-access-key AKI******* --amazonec2-secret-key 8T93C*******  aws-sandbox\n</pre> <p>For a step-by-step guide on using Machine to create Dockerized AWS instances, see the <a href=\"../examples/aws/index\">Amazon Web Services (AWS) example</a>.</p> <h2 id=\"the-docker-machine-create-command\">The docker-machine create command</h2> <p>The <code>docker-machine create</code> command typically requires that you specify, at a minimum:</p> <ul> <li><p><code>--driver</code> - to indicate the provider on which to create the machine (VirtualBox, DigitalOcean, AWS, and so on)</p></li> <li><p>Account verification and security credentials (for cloud providers), specific to the cloud service you are using</p></li> <li><p><code>&lt;machine&gt;</code> - name of the host you want to create</p></li> </ul> <p>For convenience, <code>docker-machine</code> will use sensible defaults for choosing settings such as the image that the server is based on, but you override the defaults using the respective flags (e.g. <code>--digitalocean-image</code>). This is useful if, for example, you want to create a cloud server with a lot of memory and CPUs (by default <code>docker-machine</code> creates a small server).</p> <p>For a full list of the flags/settings available and their defaults, see the output of <code>docker-machine create -h</code> at the command line, the <a href=\"../reference/create/index\" target=\"_blank\">create</a> command in the Machine <a href=\"../reference/index\" target=\"_blank\">command line reference</a>, and <a href=\"https://docs.docker.com/machine/drivers/os-base/\" target=\"_blank\">driver options and operating system defaults</a> in the Machine driver reference.</p> <h2 id=\"drivers-for-cloud-providers\">Drivers for cloud providers</h2> <p>When you install Docker Machine, you get a set of drivers for various cloud providers (like Amazon Web Services, Digital Ocean, or Microsoft Azure) and local providers (like Oracle VirtualBox, VMWare Fusion, or Microsoft Hyper-V).</p> <p>See <a href=\"../drivers/index\" target=\"_blank\">Docker Machine driver reference</a> for details on the drivers, including required flags and configuration options (which vary by provider).</p> <h2 id=\"3rd-party-driver-plugins\">3rd-party driver plugins</h2> <p>Several Docker Machine driver plugins for use with other cloud platforms are available from 3rd party contributors. These are use-at-your-own-risk plugins, not maintained by or formally associated with Docker.</p> <p>See <a href=\"https://github.com/docker/machine/blob/master/docs/AVAILABLE_DRIVER_PLUGINS.md\" target=\"_blank\">Available driver plugins</a> in the docker/machine repo on GitHub.</p> <h2 id=\"adding-a-host-without-a-driver\">Adding a host without a driver</h2> <p>You can add a host to Docker which only has a URL and no driver. Then you can use the machine name you provide here for an existing host so you don’t have to type out the URL every time you run a Docker command.</p> <pre>$ docker-machine create --url=tcp://50.134.234.20:2376 custombox\n$ docker-machine ls\nNAME        ACTIVE   DRIVER    STATE     URL\ncustombox   *        none      Running   tcp://50.134.234.20:2376\n</pre> <h2 id=\"using-machine-to-provision-docker-swarm-clusters\">Using Machine to provision Docker Swarm clusters</h2> <p>Docker Machine can also provision <a href=\"https://docs.docker.com/swarm/overview/\" target=\"_blank\">Docker Swarm</a> clusters. This can be used with any driver and will be secured with TLS.</p> <ul> <li><p>To get started with Swarm, see <a href=\"https://docs.docker.com/swarm/get-swarm/\" target=\"_blank\">How to get Docker Swarm</a>.</p></li> <li><p>To learn how to use Machine to provision a Swarm cluster, see <a href=\"https://docs.docker.com/swarm/provision-with-machine/\" target=\"_blank\">Provision a Swarm cluster with Docker Machine</a>.</p></li> </ul> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li>Example: Provision Dockerized <a href=\"../examples/ocean/index\">Digital Ocean Droplets</a>\n</li> <li>Example: Provision Dockerized <a href=\"../examples/aws/index\">AWS EC2 Instances</a>\n</li> <li><a href=\"../concepts/index\">Understand Machine concepts</a></li> <li><a href=\"../drivers/index\">Docker Machine driver reference</a></li> <li><a href=\"../reference/index\">Docker Machine subcommand reference</a></li> <li><a href=\"../../swarm/provision-with-machine/index\">Provision a Docker Swarm cluster with Docker Machine</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/get-started-cloud/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/get-started-cloud/</a>\n  </p>\n</div>\n","machine/examples/ocean/index":"<h1 id=\"digital-ocean-example\">Digital Ocean example</h1> <p>Follow along with this example to create a Dockerized <a href=\"https://digitalocean.com\" target=\"_blank\">Digital Ocean</a> Droplet (cloud host).</p> <h3 id=\"step-1-create-a-digital-ocean-account\">Step 1. Create a Digital Ocean account</h3> <p>If you have not done so already, go to <a href=\"https://digitalocean.com\" target=\"_blank\">Digital Ocean</a>, create an account, and log in.</p> <h3 id=\"step-2-generate-a-personal-access-token\">Step 2. Generate a personal access token</h3> <p>To generate your access token:</p> <ol> <li>\n<p>Go to the Digital Ocean administrator console and click <strong>API</strong> in the header.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/ocean_click_api.png\" alt=\"Click API in Digital Ocean console\"></p>\n</li> <li>\n<p>Click <strong>Generate New Token</strong> to get to the token generator.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/ocean_gen_token.png\" alt=\"Generate token\"></p>\n</li> <li>\n<p>Give the token a clever name (e.g. “machine”), make sure the <strong>Write (Optional)</strong> checkbox is checked, and click <strong>Generate Token</strong>.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/ocean_token_create.png\" alt=\"Name and generate token\"></p>\n</li> <li>\n<p>Grab (copy to clipboard) the generated big long hex string and store it somewhere safe.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/ocean_save_token.png\" alt=\"Copy and save personal access token\"></p> <p>This is the personal access token you’ll use in the next step to create your cloud server.</p>\n</li> </ol> <h3 id=\"step-3-use-machine-to-create-the-droplet\">Step 3. Use Machine to create the Droplet</h3> <ol> <li>\n<p>Run <code>docker-machine create</code> with the <code>digitalocean</code> driver and pass your key to the <code>--digitalocean-access-token</code> flag, along with a name for the new cloud server.</p> <p>For this example, we’ll call our new Droplet “docker-sandbox”.</p> <pre>$ docker-machine create --driver digitalocean --digitalocean-access-token xxxxx docker-sandbox\nRunning pre-create checks...\nCreating machine...\n(docker-sandbox) OUT | Creating SSH key...\n(docker-sandbox) OUT | Creating Digital Ocean droplet...\n(docker-sandbox) OUT | Waiting for IP address to be assigned to the Droplet...\nWaiting for machine to be running, this may take a few minutes...\nMachine is running, waiting for SSH to be available...\nDetecting operating system of created instance...\nDetecting the provisioner...\nProvisioning created instance...\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\nTo see how to connect Docker to this machine, run: docker-machine env docker-sandbox\n</pre> <p>When the Droplet is created, Docker generates a unique SSH key and stores it on your local system in <code>~/.docker/machines</code>. Initially, this is used to provision the host. Later, it’s used under the hood to access the Droplet directly with the <code>docker-machine ssh</code> command. Docker Engine is installed on the cloud server and the daemon is configured to accept remote connections over TCP using TLS for authentication.</p>\n</li> <li>\n<p>Go to the Digital Ocean console to view the new Droplet.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/ocean_droplet.png\" alt=\"Droplet in Digital Ocean created with Machine\"></p>\n</li> <li>\n<p>At the command terminal, run <code>docker-machine ls</code>.</p> <pre>$ docker-machine ls\nNAME             ACTIVE   DRIVER         STATE     URL                         SWARM\ndefault          -        virtualbox     Running   tcp://192.168.99.100:2376\ndocker-sandbox   *        digitalocean   Running   tcp://45.55.139.48:2376\n</pre> <p>The new <code>docker-sandbox</code> machine is running, and it is the active host as indicated by the asterisk (*). When you create a new machine, your command shell automatically connects to it. If for some reason your new machine is not the active host, you’ll need to run <code>docker-machine env docker-sandbox</code>, followed by <code>eval $(docker-machine env docker-sandbox)</code> to connect to it.</p>\n</li> </ol> <h3 id=\"step-4-run-docker-commands-on-the-droplet\">Step 4. Run Docker commands on the Droplet</h3> <ol> <li>\n<p>Run some <code>docker-machine</code> commands to inspect the remote host. For example, <code>docker-machine ip &lt;machine&gt;</code> gets the host IP adddress and <code>docker-machine inspect &lt;machine&gt;</code> lists all the details.</p> <pre>$ docker-machine ip docker-sandbox\n104.131.43.236\n\n$ docker-machine inspect docker-sandbox\n{\n    \"ConfigVersion\": 3,\n    \"Driver\": {\n    \"IPAddress\": \"104.131.43.236\",\n    \"MachineName\": \"docker-sandbox\",\n    \"SSHUser\": \"root\",\n    \"SSHPort\": 22,\n    \"SSHKeyPath\": \"/Users/samanthastevens/.docker/machine/machines/docker-sandbox/id_rsa\",\n    \"StorePath\": \"/Users/samanthastevens/.docker/machine\",\n    \"SwarmMaster\": false,\n    \"SwarmHost\": \"tcp://0.0.0.0:3376\",\n    \"SwarmDiscovery\": \"\",\n    ...\n</pre>\n</li> <li>\n<p>Verify Docker Engine is installed correctly by running <code>docker</code> commands.</p> <p>Start with something basic like <code>docker run hello-world</code>, or for a more interesting test, run a Dockerized webserver on your new remote machine.</p> <p>In this example, the <code>-p</code> option is used to expose port 80 from the <code>nginx</code> container and make it accessible on port <code>8000</code> of the <code>docker-sandbox</code> host.</p> <pre>$ docker run -d -p 8000:80 --name webserver kitematic/hello-world-nginx\nUnable to find image 'kitematic/hello-world-nginx:latest' locally\nlatest: Pulling from kitematic/hello-world-nginx\na285d7f063ea: Pull complete\n2d7baf27389b: Pull complete\n...\nDigest: sha256:ec0ca6dcb034916784c988b4f2432716e2e92b995ac606e080c7a54b52b87066\nStatus: Downloaded newer image for kitematic/hello-world-nginx:latest\n942dfb4a0eaae75bf26c9785ade4ff47ceb2ec2a152be82b9d7960e8b5777e65\n</pre> <p>In a web browser, go to <code>http://&lt;host_ip&gt;:8000</code> to bring up the webserver home page. You got the <code>&lt;host_ip&gt;</code> from the output of the <code>docker-machine ip &lt;machine&gt;</code> command you ran in a previous step. Use the port you exposed in the <code>docker run</code> command.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/nginx-webserver.png\" alt=\"nginx webserver\"></p>\n</li> </ol> <h3 id=\"step-5-use-machine-to-remove-the-droplet\">Step 5. Use Machine to remove the Droplet</h3> <p>To remove a host and all of its containers and images, first stop the machine, then use <code>docker-machine rm</code>:</p> <pre>$ docker-machine stop docker-sandbox\n$ docker-machine rm docker-sandbox\nDo you really want to remove \"docker-sandbox\"? (y/n): y\nSuccessfully removed docker-sandbox\n\n$ docker-machine ls\nNAME      ACTIVE   DRIVER       STATE     URL                         SWARM\ndefault   *        virtualbox   Running   tcp:////xxx.xxx.xx.xxx:xxxx\n</pre> <p>If you monitor the Digital Ocean console while you run these commands, you will see it update first to reflect that the Droplet was stopped, and then removed.</p> <p>If you create a host with Docker Machine, but remove it through the cloud provider console, Machine will lose track of the server status. So please use the <code>docker-machine rm</code> command for hosts you create with <code>docker-machine create</code>.</p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><a href=\"../../concepts/index\">Understand Machine concepts</a></li> <li><a href=\"../../drivers/index\">Docker Machine driver reference</a></li> <li><a href=\"../../reference/index\">Docker Machine subcommand reference</a></li> <li><a href=\"../../../swarm/provision-with-machine/index\">Provision a Docker Swarm cluster with Docker Machine</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/examples/ocean/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/examples/ocean/</a>\n  </p>\n</div>\n","machine/examples/aws/index":"<h1 id=\"amazon-web-services-aws-ec2-example\">Amazon Web Services (AWS) EC2 example</h1> <p>Follow along with this example to create a Dockerized <a href=\"https://aws.amazon.com/\" target=\"_blank\"> Amazon Web Services (AWS)</a> EC2 instance.</p> <h3 id=\"step-1-sign-up-for-aws-and-configure-credentials\">Step 1. Sign up for AWS and configure credentials</h3> <ol> <li>\n<p>If you are not already an AWS user, sign up for <a href=\"https://aws.amazon.com/\" target=\"_blank\"> AWS</a> to create an account and get root access to EC2 cloud computers.</p> <p>If you have an Amazon account, you can use it as your root user account.</p>\n</li> <li>\n<p>Create an IAM (Identity and Access Management) administrator user, an admin group, and a key pair associated with a region.</p> <p>From the AWS menus, select <strong>Services</strong> &gt; <strong>IAM</strong> to get started.</p> <p>To create machines on AWS, you must supply two parameters:</p> <ul> <li><p>an AWS Access Key ID</p></li> <li><p>an AWS Secret Access Key</p></li> </ul> <p>See the AWS documentation on <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html\" target=\"_blank\">Setting Up with Amazon EC2</a>. Follow the steps for “Create an IAM User” and “Create a Key Pair”.</p>\n</li> </ol> <h3 id=\"step-2-use-machine-to-create-the-instance\">Step 2. Use Machine to create the instance</h3> <ol> <li>\n<p>Optionally, create an AWS credential file.</p> <p>You can create an <code>~/.aws/credentials</code> file to hold your AWS keys so that you don’t have to type them every time you run the <code>docker-machine create</code> command. Here is an example of a credentials file.</p> <pre>    [default]\n    aws_access_key_id = AKID1234567890\n    aws_secret_access_key = MY-SECRET-KEY\n</pre>\n</li> <li>\n<p>Run <code>docker-machine create</code> with the <code>amazonec2</code> driver, your keys, and a name for the new instance.</p> <p><strong>Using a credentials file</strong></p> <p>If you specified your keys in a credentials file, this command looks like this to create an instance called <code>aws-sandbox</code>:</p> <pre>    docker-machine create --driver amazonec2 aws-sandbox\n</pre> <p><strong>Specifying keys at the command line</strong></p> <p>If you don’t have a credentials file, you can use the flags <code>--amazonec2-access-key</code> and <code>--amazonec2-secret-key</code> on the command line:</p> <pre>    $ docker-machine create --driver amazonec2 --amazonec2-access-key AKI******* --amazonec2-secret-key 8T93C*******  aws-sandbox\n</pre> <p><strong>Specifying a region</strong></p> <p>By default, the driver creates new instances in region us-east-1 (North Virginia). You can specify a different region by using the <code>--amazonec2-region</code> flag. For example, this command creates a machine called “aws-01” in us-west-1 (Northern California).</p> <pre>    $ docker-machine create --driver amazonec2 --amazonec2-region us-west-1 aws-01\n</pre>\n</li> <li>\n<p>Go to the AWS EC2 Dashboard to view the new instance.</p> <p>Log into AWS with your IAM credentials, and navigate to your EC2 Running Instances.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/aws-instance-east.png\" alt=\"instance on AWS EC2 Dashboard\"></p> <p><strong>Note</strong>: Make sure you set the region appropriately from the menu in the upper right; otherwise, you won’t see the new instance. If you did not specify a region as part of <code>docker-machine create</code> (with the optional <code>--amazonec2-region</code> flag), then the region will be US East, which is the default.</p>\n</li> <li>\n<p>At the command terminal, run <code>docker-machine ls</code>.</p> <pre>$ docker-machine ls\nNAME             ACTIVE   DRIVER         STATE     URL                         SWARM   DOCKER        ERRORS      \naws-sandbox      *        amazonec2      Running   tcp://52.90.113.128:2376            v1.10.0       \ndefault          -        virtualbox     Running   tcp://192.168.99.100:2376           v1.10.0-rc4   \naws-sandbox   -        digitalocean   Running   tcp://104.131.43.236:2376           v1.9.1        \n</pre> <p>The new <code>aws-sandbox</code> instance is running, and it is the active host as indicated by the asterisk (*). When you create a new machine, your command shell automatically connects to it. If for some reason your new machine is not the active host, you’ll need to run <code>docker-machine env aws-sandbox</code>, followed by <code>eval $(docker-machine env aws-sandbox)</code> to connect to it.</p>\n</li> </ol> <h3 id=\"step-3-run-docker-commands-on-the-instance\">Step 3. Run Docker commands on the instance</h3> <ol> <li>\n<p>Run some <code>docker-machine</code> commands to inspect the remote host. For example, <code>docker-machine ip &lt;machine&gt;</code> gets the host IP address and <code>docker-machine inspect &lt;machine&gt;</code> lists all the details.</p> <pre>  $ docker-machine ip\n  192.168.99.100\n\n  $ docker-machine inspect aws-sandbox\n  {\n      \"ConfigVersion\": 3,\n      \"Driver\": {\n          \"IPAddress\": \"52.90.113.128\",\n          \"MachineName\": \"aws-sandbox\",\n          \"SSHUser\": \"ubuntu\",\n          \"SSHPort\": 22,\n          ...\n</pre>\n</li> <li>\n<p>Verify Docker Engine is installed correctly by running <code>docker</code> commands.</p> <p>Start with something basic like <code>docker run hello-world</code>, or for a more interesting test, run a Dockerized webserver on your new remote machine.</p> <p>In this example, the <code>-p</code> option is used to expose port 80 from the <code>nginx</code> container and make it accessible on port <code>8000</code> of the <code>aws-sandbox</code> host.</p> <pre>$ docker run -d -p 8000:80 --name webserver kitematic/hello-world-nginx\nUnable to find image 'kitematic/hello-world-nginx:latest' locally\nlatest: Pulling from kitematic/hello-world-nginx\na285d7f063ea: Pull complete\n2d7baf27389b: Pull complete\n...\nDigest: sha256:ec0ca6dcb034916784c988b4f2432716e2e92b995ac606e080c7a54b52b87066\nStatus: Downloaded newer image for kitematic/hello-world-nginx:latest\n942dfb4a0eaae75bf26c9785ade4ff47ceb2ec2a152be82b9d7960e8b5777e65\n</pre> <p>In a web browser, go to <code>http://&lt;host_ip&gt;:8000</code> to bring up the webserver home page. You got the <code>&lt;host_ip&gt;</code> from the output of the <code>docker-machine ip &lt;machine&gt;</code> command you ran in a previous step. Use the port you exposed in the <code>docker run</code> command.</p> <p><img src=\"https://docs.docker.com/v1.11/machine/img/nginx-webserver.png\" alt=\"nginx webserver\"></p>\n</li> </ol> <h3 id=\"step-4-use-machine-to-remove-the-instance\">Step 4. Use Machine to remove the instance</h3> <p>To remove an instance and all of its containers and images, first stop the machine, then use <code>docker-machine rm</code>:</p> <pre>  $ docker-machine stop aws-sandbox\n  $ docker-machine rm aws-sandbox\n  Do you really want to remove \"aws-sandbox\"? (y/n): y\n  Successfully removed aws-sandbox\n</pre> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><a href=\"../../concepts/index\">Understand Machine concepts</a></li> <li><a href=\"../../drivers/index\">Docker Machine driver reference</a></li> <li><a href=\"../../reference/index\">Docker Machine subcommand reference</a></li> <li><a href=\"../../../swarm/provision-with-machine/index\">Provision a Docker Swarm cluster with Docker Machine</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/examples/aws/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/examples/aws/</a>\n  </p>\n</div>\n","machine/reference/active/index":"<h1 id=\"active\">active</h1> <p>See which machine is “active” (a machine is considered active if the <code>DOCKER_HOST</code> environment variable points to it).</p> <pre>$ docker-machine ls\nNAME      ACTIVE   DRIVER         STATE     URL\ndev       -        virtualbox     Running   tcp://192.168.99.103:2376\nstaging   *        digitalocean   Running   tcp://203.0.113.81:2376\n$ echo $DOCKER_HOST\ntcp://203.0.113.81:2376\n$ docker-machine active\nstaging\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/active/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/active/</a>\n  </p>\n</div>\n","machine/concepts/index":"<h1 id=\"understand-machine-concepts-and-get-help\">Understand Machine concepts and get help</h1> <p>Docker Machine allows you to provision Docker machines in a variety of environments, including virtual machines that reside on your local system, on cloud providers, or on bare metal servers (physical computers). Docker Machine creates a Docker host, and you use the Docker Engine client as needed to build images and create containers on the host.</p> <h2 id=\"drivers-for-creating-machines\">Drivers for creating machines</h2> <p>To create a virtual machine, you supply Docker Machine with the name of the driver you want use. The driver determines where the virtual machine is created. For example, on a local Mac or Windows system, the driver is typically Oracle VirtualBox. For provisioning physical machines, a generic driver is provided. For cloud providers, Docker Machine supports drivers such as AWS, Microsoft Azure, Digital Ocean, and many more. The Docker Machine reference includes a complete <a href=\"../drivers/index\">list of supported drivers</a>.</p> <h2 id=\"default-base-operating-systems-for-local-and-cloud-hosts\">Default base operating systems for local and cloud hosts</h2> <p>Since Docker runs on Linux, each VM that Docker Machine provisions relies on a base operating system. For convenience, there are default base operating systems. For the Oracle Virtual Box driver, this base operating system is <a href=\"https://github.com/boot2docker/boot2docker\" target=\"_blank\">boot2docker</a>. For drivers used to connect to cloud providers, the base operating system is Ubuntu 12.04+. You can change this default when you create a machine. The Docker Machine reference includes a complete <a href=\"../drivers/os-base/index\">list of supported operating systems</a>.</p> <h2 id=\"ip-addresses-for-docker-hosts\">IP addresses for Docker hosts</h2> <p>For each machine you create, the Docker host address is the IP address of the Linux VM. This address is assigned by the <code>docker-machine create</code> subcommand. You use the <code>docker-machine ls</code> command to list the machines you have created. The <code>docker-machine ip &lt;machine-name&gt;</code> command returns a specific host’s IP address.</p> <h2 id=\"configuring-cli-environment-variables-for-a-docker-host\">Configuring CLI environment variables for a Docker host</h2> <p>Before you can run a <code>docker</code> command on a machine, you need to configure your command-line to point to that machine. The <code>docker-machine env &lt;machine-name&gt;</code> subcommand outputs the configuration command you should use.</p> <p>For a complete list of <code>docker-machine</code> subcommands, see the <a href=\"../reference/index\">Docker Machine subcommand reference</a>.</p> <h2 id=\"crash-reporting\">Crash Reporting</h2> <p>Provisioning a host is a complex matter that can fail for a lot of reasons. Your workstation may have a wide variety of shell, network configuration, VPN, proxy or firewall issues. There are also reasons from the other end of the chain: your cloud provider or the network in between.</p> <p>To help <code>docker-machine</code> be as stable as possible, we added a monitoring of crashes whenever you try to <code>create</code> or <code>upgrade</code> a host. This will send, over HTTPS, to Bugsnag some information about your <code>docker-machine</code> version, build, OS, ARCH, the path to your current shell and, the history of the last command as you could see it with a <code>--debug</code> option. This data is sent to help us pinpoint recurring issues with <code>docker-machine</code> and will only be transmitted in the case of a crash of <code>docker-machine</code>.</p> <p>If you wish to opt out of error reporting, you can create a <code>no-error-report</code> file in your <code>$HOME/.docker/machine</code> directory, and Docker Machine will disable this behavior. e.g.:</p> <pre>$ mkdir -p ~/.docker/machine &amp;&amp; touch ~/.docker/machine/no-error-report\n</pre> <p>Leaving the file empty is fine -- Docker Machine just checks for its presence.</p> <h2 id=\"getting-help\">Getting help</h2> <p>Docker Machine is still in its infancy and under active development. If you need help, would like to contribute, or simply want to talk about the project with like-minded individuals, we have a number of open channels for communication.</p> <ul> <li>To report bugs or file feature requests: please use the <a href=\"https://github.com/docker/machine/issues\">issue tracker on Github</a>.</li> <li>To talk about the project with people in real time: please join the <code>#docker-machine</code> channel on IRC.</li> <li>To contribute code or documentation changes: please <a href=\"https://github.com/docker/machine/pulls\">submit a pull request on Github</a>.</li> </ul> <p>For more information and resources, please visit <a href=\"https://docs.docker.com/v1.11/opensource/get-help/\">our help page</a>.</p> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li>Create and run a Docker host on your <a href=\"../get-started/index\">local system using VirtualBox</a>\n</li> <li>Provision multiple Docker hosts <a href=\"../get-started-cloud/index\">on your cloud provider</a>\n</li> <li><a href=\"../drivers/index\" target=\"_blank\">Docker Machine driver reference</a></li> <li><a href=\"../reference/index\" target=\"_blank\">Docker Machine subcommand reference</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/concepts/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/concepts/</a>\n  </p>\n</div>\n","machine/reference/config/index":"<h1 id=\"config\">config</h1> <pre>Usage: docker-machine config [OPTIONS] [arg...]\n\nPrint the connection config for machine\n\nDescription:\n   Argument is a machine name.\n\nOptions:\n\n   --swarm      Display the Swarm config instead of the Docker daemon\n</pre> <p>For example:</p> <pre>$ docker-machine config dev\n--tlsverify\n--tlscacert=\"/Users/ehazlett/.docker/machines/dev/ca.pem\"\n--tlscert=\"/Users/ehazlett/.docker/machines/dev/cert.pem\"\n--tlskey=\"/Users/ehazlett/.docker/machines/dev/key.pem\"\n-H tcp://192.168.99.103:2376\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/config/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/config/</a>\n  </p>\n</div>\n","machine/migrate-to-machine/index":"<h1 id=\"migrate-from-boot2docker-to-docker-machine\">Migrate from Boot2Docker to Docker Machine</h1> <p>If you were using Boot2Docker previously, you have a pre-existing Docker <code>boot2docker-vm</code> VM on your local system. To allow Docker Machine to manage this older VM, you must migrate it.</p> <ol> <li><p>Open a terminal or the Docker CLI on your system.</p></li> <li>\n<p>Type the following command.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-import-boot2docker-vm boot2docker-vm docker-vm\n</pre>\n</li> <li><p>Use the <code>docker-machine</code> command to interact with the migrated VM.</p></li> </ol> <h2 id=\"subcommand-comparison\">Subcommand comparison</h2> <p>The <code>docker-machine</code> subcommands are slightly different than the <code>boot2docker</code> subcommands. The table below lists the equivalent <code>docker-machine</code> subcommand and what it does:</p> <table> <thead> <tr> <th><code>boot2docker</code></th> <th><code>docker-machine</code></th> <th>\n<code>docker-machine</code> description</th> </tr> </thead> <tbody> <tr> <td>init</td> <td>create</td> <td>Creates a new docker host.</td> </tr> <tr> <td>up</td> <td>start</td> <td>Starts a stopped machine.</td> </tr> <tr> <td>ssh</td> <td>ssh</td> <td>Runs a command or interactive ssh session on the machine.</td> </tr> <tr> <td>save</td> <td>-</td> <td>Not applicable.</td> </tr> <tr> <td>down</td> <td>stop</td> <td>Stops a running machine.</td> </tr> <tr> <td>poweroff</td> <td>stop</td> <td>Stops a running machine.</td> </tr> <tr> <td>reset</td> <td>restart</td> <td>Restarts a running machine.</td> </tr> <tr> <td>config</td> <td>inspect</td> <td>Prints machine configuration details.</td> </tr> <tr> <td>status</td> <td>ls</td> <td>Lists all machines and their status.</td> </tr> <tr> <td>info</td> <td>inspect</td> <td>Displays a machine’s details.</td> </tr> <tr> <td>ip</td> <td>ip</td> <td>Displays the machine’s ip address.</td> </tr> <tr> <td>shellinit</td> <td>env</td> <td>Displays shell commands needed to configure your shell to interact with a machine</td> </tr> <tr> <td>delete</td> <td>rm</td> <td>Removes a machine.</td> </tr> <tr> <td>download</td> <td>-</td> <td>Not applicable.</td> </tr> <tr> <td>upgrade</td> <td>upgrade</td> <td>Upgrades a machine’s Docker client to the latest stable release.</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/migrate-to-machine/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/migrate-to-machine/</a>\n  </p>\n</div>\n","machine/reference/create/index":"<h1 id=\"create\">create</h1> <p>Create a machine. Requires the <code>--driver</code> flag to indicate which provider (VirtualBox, DigitalOcean, AWS, etc.) the machine should be created on, and an argument to indicate the name of the created machine.</p> <pre>$ docker-machine create --driver virtualbox dev\nCreating CA: /home/username/.docker/machine/certs/ca.pem\nCreating client certificate: /home/username/.docker/machine/certs/cert.pem\nImage cache does not exist, creating it at /home/username/.docker/machine/cache...\nNo default boot2docker iso found locally, downloading the latest release...\nDownloading https://github.com/boot2docker/boot2docker/releases/download/v1.6.2/boot2docker.iso to /home/username/.docker/machine/cache/boot2docker.iso...\nCreating VirtualBox VM...\nCreating SSH key...\nStarting VirtualBox VM...\nStarting VM...\nTo see how to connect Docker to this machine, run: docker-machine env dev\n</pre> <h2 id=\"accessing-driver-specific-flags-in-the-help-text\">Accessing driver-specific flags in the help text</h2> <p>The <code>docker-machine create</code> command has some flags which are applicable to all drivers. These largely control aspects of Machine’s provisoning process (including the creation of Docker Swarm containers) that the user may wish to customize.</p> <pre>$ docker-machine create\nDocker Machine Version: 0.5.0 (45e3688)\nUsage: docker-machine create [OPTIONS] [arg...]\n\nCreate a machine.\n\nRun 'docker-machine create --driver name' to include the create flags for that driver in the help text.\n\nOptions:\n\n   --driver, -d \"none\"                                                                                  Driver to create machine with.\n   --engine-install-url \"https://get.docker.com\"                                                        Custom URL to use for engine installation [$MACHINE_DOCKER_INSTALL_URL]\n   --engine-opt [--engine-opt option --engine-opt option]                                               Specify arbitrary flags to include with the created engine in the form flag=value\n   --engine-insecure-registry [--engine-insecure-registry option --engine-insecure-registry option]     Specify insecure registries to allow with the created engine\n   --engine-registry-mirror [--engine-registry-mirror option --engine-registry-mirror option]           Specify registry mirrors to use [$ENGINE_REGISTRY_MIRROR]\n   --engine-label [--engine-label option --engine-label option]                                         Specify labels for the created engine\n   --engine-storage-driver                                                                              Specify a storage driver to use with the engine\n   --engine-env [--engine-env option --engine-env option]                                               Specify environment variables to set in the engine\n   --swarm                                                                                              Configure Machine with Swarm\n   --swarm-image \"swarm:latest\"                                                                         Specify Docker image to use for Swarm [$MACHINE_SWARM_IMAGE]\n   --swarm-master                                                                                       Configure Machine to be a Swarm master\n   --swarm-discovery                                                                                    Discovery service to use with Swarm\n   --swarm-strategy \"spread\"                                                                            Define a default scheduling strategy for Swarm\n   --swarm-opt [--swarm-opt option --swarm-opt option]                                                  Define arbitrary flags for swarm\n   --swarm-host \"tcp://0.0.0.0:3376\"                                                                    ip/socket to listen on for Swarm master\n   --swarm-addr                                                                                         addr to advertise for Swarm (default: detect and use the machine IP)\n   --swarm-experimental                                                                                 Enable Swarm experimental features\n</pre> <p>Additionally, drivers can specify flags that Machine can accept as part of their plugin code. These allow users to customize the provider-specific parameters of the created machine, such as size (<code>--amazonec2-instance-type m1.medium</code>), geographical region (<code>--amazonec2-region us-west-1</code>), and so on.</p> <p>To see the provider-specific flags, simply pass a value for <code>--driver</code> when invoking the <code>create</code> help text.</p> <pre>$ docker-machine create --driver virtualbox --help\nUsage: docker-machine create [OPTIONS] [arg...]\n\nCreate a machine.\n\nRun 'docker-machine create --driver name' to include the create flags for that driver in the help text.\n\nOptions:\n\n   --driver, -d \"none\"                                                                                  Driver to create machine with.\n   --engine-env [--engine-env option --engine-env option]                                               Specify environment variables to set in the engine\n   --engine-insecure-registry [--engine-insecure-registry option --engine-insecure-registry option]     Specify insecure registries to allow with the created engine\n   --engine-install-url \"https://get.docker.com\"                                                        Custom URL to use for engine installation [$MACHINE_DOCKER_INSTALL_URL]\n   --engine-label [--engine-label option --engine-label option]                                         Specify labels for the created engine\n   --engine-opt [--engine-opt option --engine-opt option]                                               Specify arbitrary flags to include with the created engine in the form flag=value\n   --engine-registry-mirror [--engine-registry-mirror option --engine-registry-mirror option]           Specify registry mirrors to use [$ENGINE_REGISTRY_MIRROR]\n   --engine-storage-driver                                                                              Specify a storage driver to use with the engine\n   --swarm                                                                                              Configure Machine with Swarm\n   --swarm-addr                                                                                         addr to advertise for Swarm (default: detect and use the machine IP)\n   --swarm-discovery                                                                                    Discovery service to use with Swarm\n   --swarm-experimental                                                                                 Enable Swarm experimental features\n   --swarm-host \"tcp://0.0.0.0:3376\"                                                                    ip/socket to listen on for Swarm master\n   --swarm-image \"swarm:latest\"                                                                         Specify Docker image to use for Swarm [$MACHINE_SWARM_IMAGE]\n   --swarm-master                                                                                       Configure Machine to be a Swarm master\n   --swarm-opt [--swarm-opt option --swarm-opt option]                                                  Define arbitrary flags for swarm\n   --swarm-strategy \"spread\"                                                                            Define a default scheduling strategy for Swarm\n   --virtualbox-boot2docker-url                                                                         The URL of the boot2docker image. Defaults to the latest available version [$VIRTUALBOX_BOOT2DOCKER_URL]\n   --virtualbox-cpu-count \"1\"                                                                           number of CPUs for the machine (-1 to use the number of CPUs available) [$VIRTUALBOX_CPU_COUNT]\n   --virtualbox-disk-size \"20000\"                                                                       Size of disk for host in MB [$VIRTUALBOX_DISK_SIZE]\n   --virtualbox-host-dns-resolver                                                                       Use the host DNS resolver [$VIRTUALBOX_HOST_DNS_RESOLVER]\n   --virtualbox-dns-proxy                                                                               Proxy all DNS requests to the host [$VIRTUALBOX_DNS_PROXY]\n   --virtualbox-hostonly-cidr \"192.168.99.1/24\"                                                         Specify the Host Only CIDR [$VIRTUALBOX_HOSTONLY_CIDR]\n   --virtualbox-hostonly-nicpromisc \"deny\"                                                              Specify the Host Only Network Adapter Promiscuous Mode [$VIRTUALBOX_HOSTONLY_NIC_PROMISC]\n   --virtualbox-hostonly-nictype \"82540EM\"                                                              Specify the Host Only Network Adapter Type [$VIRTUALBOX_HOSTONLY_NIC_TYPE]\n   --virtualbox-import-boot2docker-vm                                                                   The name of a Boot2Docker VM to import\n   --virtualbox-memory \"1024\"                                                                           Size of memory for host in MB [$VIRTUALBOX_MEMORY_SIZE]\n   --virtualbox-no-share                                                                                Disable the mount of your home directory\n</pre> <p>You may notice that some flags specify environment variables that they are associated with as well (located to the far left hand side of the row). If these environment variables are set when <code>docker-machine create</code> is invoked, Docker Machine will use them for the default value of the flag.</p> <h2 id=\"specifying-configuration-options-for-the-created-docker-engine\">Specifying configuration options for the created Docker engine</h2> <p>As part of the process of creation, Docker Machine installs Docker and configures it with some sensible defaults. For instance, it allows connection from the outside world over TCP with TLS-based encryption and defaults to AUFS as the <a href=\"../../../engine/reference/commandline/daemon/index#daemon-storage-driver-option\">storage driver</a> when available.</p> <p>There are several cases where the user might want to set options for the created Docker engine (also known as the Docker <em>daemon</em>) themselves. For example, they may want to allow connection to a <a href=\"https://docs.docker.com/v1.11/registry/\">registry</a> that they are running themselves using the <code>--insecure-registry</code> flag for the daemon. Docker Machine supports the configuration of such options for the created engines via the <code>create</code> command flags which begin with <code>--engine</code>.</p> <p>Note that Docker Machine simply sets the configured parameters on the daemon and does not set up any of the “dependencies” for you. For instance, if you specify that the created daemon should use <code>btrfs</code> as a storage driver, you still must ensure that the proper dependencies are installed, the BTRFS filesystem has been created, and so on.</p> <p>The following is an example usage:</p> <pre>$ docker-machine create -d virtualbox \\\n    --engine-label foo=bar \\\n    --engine-label spam=eggs \\\n    --engine-storage-driver overlay \\\n    --engine-insecure-registry registry.myco.com \\\n    foobarmachine\n</pre> <p>This will create a virtual machine running locally in Virtualbox which uses the <code>overlay</code> storage backend, has the key-value pairs <code>foo=bar</code> and <code>spam=eggs</code> as labels on the engine, and allows pushing / pulling from the insecure registry located at <code>registry.myco.com</code>. You can verify much of this by inspecting the output of <code>docker info</code>:</p> <pre>$ eval $(docker-machine env foobarmachine)\n$ docker info\nContainers: 0\nImages: 0\nStorage Driver: overlay\n...\nName: foobarmachine\n...\nLabels:\n foo=bar\n spam=eggs\n provider=virtualbox\n</pre> <p>The supported flags are as follows:</p> <ul> <li>\n<code>--engine-insecure-registry</code>: Specify <a href=\"../../../engine/reference/commandline/cli/index#insecure-registries\">insecure registries</a> to allow with the created engine</li> <li>\n<code>--engine-registry-mirror</code>: Specify <a href=\"https://docs.docker.com/v1.11/registry/recipes/mirror/\">registry mirrors</a> to use</li> <li>\n<code>--engine-label</code>: Specify <a href=\"../../../engine/userguide/labels-custom-metadata/index#daemon-labels\">labels</a> for the created engine</li> <li>\n<code>--engine-storage-driver</code>: Specify a <a href=\"../../../engine/reference/commandline/cli/index#daemon-storage-driver-option\">storage driver</a> to use with the engine</li> </ul> <p>If the engine supports specifying the flag multiple times (such as with <code>--label</code>), then so does Docker Machine.</p> <p>In addition to this subset of daemon flags which are directly supported, Docker Machine also supports an additional flag, <code>--engine-opt</code>, which can be used to specify arbitrary daemon options with the syntax <code>--engine-opt flagname=value</code>. For example, to specify that the daemon should use <code>8.8.8.8</code> as the DNS server for all containers, and always use the <code>syslog</code> <a href=\"../../../engine/reference/run/index#logging-drivers-log-driver\">log driver</a> you could run the following create command:</p> <pre>$ docker-machine create -d virtualbox \\\n    --engine-opt dns=8.8.8.8 \\\n    --engine-opt log-driver=syslog \\\n    gdns\n</pre> <p>Additionally, Docker Machine supports a flag, <code>--engine-env</code>, which can be used to specify arbitrary environment variables to be set within the engine with the syntax <code>--engine-env name=value</code>. For example, to specify that the engine should use <code>example.com</code> as the proxy server, you could run the following create command:</p> <pre>$ docker-machine create -d virtualbox \\\n    --engine-env HTTP_PROXY=http://example.com:8080 \\\n    --engine-env HTTPS_PROXY=https://example.com:8080 \\\n    --engine-env NO_PROXY=example2.com \\\n    proxbox\n</pre> <h2 id=\"specifying-docker-swarm-options-for-the-created-machine\">Specifying Docker Swarm options for the created machine</h2> <p>In addition to being able to configure Docker Engine options as listed above, you can use Machine to specify how the created Swarm master should be configured. There is a <code>--swarm-strategy</code> flag, which you can use to specify the <a href=\"../../../swarm/scheduler/strategy/index\">scheduling strategy</a> which Docker Swarm should use (Machine defaults to the <code>spread</code> strategy). There is also a general purpose <code>--swarm-opt</code> option which works similar to how the aforementioned <code>--engine-opt</code> option does, except that it specifies options for the <code>swarm manage</code> command (used to boot a master node) instead of the base command. You can use this to configure features that power users might be interested in, such as configuring the heartbeat interval or Swarm’s willingness to over-commit resources. There is also the <code>--swarm-experimental</code> flag, that allows you to access <a href=\"https://github.com/docker/swarm/tree/master/experimental\">experimental features</a> in Docker Swarm.</p> <p>If you’re not sure how to configure these options, it is best to not specify configuration at all. Docker Machine will choose sensible defaults for you and you won’t have to worry about it.</p> <p>Example create:</p> <pre>$ docker-machine create -d virtualbox \\\n    --swarm \\\n    --swarm-master \\\n    --swarm-discovery token://&lt;token&gt; \\\n    --swarm-strategy binpack \\\n    --swarm-opt heartbeat=5 \\\n    upbeat\n</pre> <p>This will set the swarm scheduling strategy to “binpack” (pack in containers as tightly as possible per host instead of spreading them out), and the “heartbeat” interval to 5 seconds.</p> <h2 id=\"pre-create-check\">Pre-create check</h2> <p>Since many drivers require a certain set of conditions to be in place before they can successfully perform a create (e.g. VirtualBox should be installed, or the provided API credentials should be valid), Docker Machine has a “pre-create check” which is specified at the driver level.</p> <p>If this pre-create check succeeds, Docker Machine will proceed with the creation as normal. If the pre-create check fails, the Docker Machine process will exit with status code 3 to indicate that the source of the non-zero exit was the pre-create check failing.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/create/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/create/</a>\n  </p>\n</div>\n","machine/reference/ip/index":"<h1 id=\"ip\">ip</h1> <p>Get the IP address of one or more machines.</p> <pre>$ docker-machine ip dev\n192.168.99.104\n$ docker-machine ip dev dev2\n192.168.99.104\n192.168.99.105\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/ip/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/ip/</a>\n  </p>\n</div>\n","machine/reference/kill/index":"<h1 id=\"kill\">kill</h1> <pre>Usage: docker-machine kill [arg...]\n\nKill (abruptly force stop) a machine\n\nDescription:\n   Argument(s) are one or more machine names.\n</pre> <p>For example:</p> <pre>$ docker-machine ls\nNAME   ACTIVE   DRIVER       STATE     URL\ndev    *        virtualbox   Running   tcp://192.168.99.104:2376\n$ docker-machine kill dev\n$ docker-machine ls\nNAME   ACTIVE   DRIVER       STATE     URL\ndev    *        virtualbox   Stopped\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/kill/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/kill/</a>\n  </p>\n</div>\n","machine/reference/env/index":"<h1 id=\"env\">env</h1> <p>Set environment variables to dictate that <code>docker</code> should run a command against a particular machine.</p> <pre>$ docker-machine env --help\n\nUsage: docker-machine env [OPTIONS] [arg...]\n\nDisplay the commands to set up the environment for the Docker client\n\nDescription:\n   Argument is a machine name.\n\nOptions:\n\n   --swarm  Display the Swarm config instead of the Docker daemon\n   --shell  Force environment to be configured for a specified shell: [fish, cmd, powershell], default is sh/bash\n   --unset, -u  Unset variables instead of setting them\n   --no-proxy   Add machine IP to NO_PROXY environment variable\n</pre> <p><code>docker-machine env machinename</code> will print out <code>export</code> commands which can be run in a subshell. Running <code>docker-machine env -u</code> will print <code>unset</code> commands which reverse this effect.</p> <pre>$ env | grep DOCKER\n$ eval \"$(docker-machine env dev)\"\n$ env | grep DOCKER\nDOCKER_HOST=tcp://192.168.99.101:2376\nDOCKER_CERT_PATH=/Users/nathanleclaire/.docker/machines/.client\nDOCKER_TLS_VERIFY=1\nDOCKER_MACHINE_NAME=dev\n$ # If you run a docker command, now it will run against that host.\n$ eval \"$(docker-machine env -u)\"\n$ env | grep DOCKER\n$ # The environment variables have been unset.\n</pre> <p>The output described above is intended for the shells <code>bash</code> and <code>zsh</code> (if you’re not sure which shell you’re using, there’s a very good possibility that it’s <code>bash</code>). However, these are not the only shells which Docker Machine supports. Depending of the environment you’re running your command into we will print them for the proper system. We support <code>bash</code>, <code>cmd</code>, <code>powershell</code> and <code>emacs</code>.</p> <p>If you are using <code>fish</code> and the <code>SHELL</code> environment variable is correctly set to the path where <code>fish</code> is located, <code>docker-machine env name</code> will print out the values in the format which <code>fish</code> expects:</p> <pre>set -x DOCKER_TLS_VERIFY 1;\nset -x DOCKER_CERT_PATH \"/Users/nathanleclaire/.docker/machine/machines/overlay\";\nset -x DOCKER_HOST tcp://192.168.99.102:2376;\nset -x DOCKER_MACHINE_NAME overlay\n# Run this command to configure your shell:\n# eval \"$(docker-machine env overlay)\"\n</pre> <p>If you are on Windows and using either Powershell or <code>cmd.exe</code>, <code>docker-machine env</code> Docker Machine should now detect your shell automatically. If the automagic detection does not work you can still override it using the <code>--shell</code> flag for <code>docker-machine env</code>.</p> <p>For Powershell:</p> <pre>$ docker-machine.exe env --shell powershell dev\n$Env:DOCKER_TLS_VERIFY = \"1\"\n$Env:DOCKER_HOST = \"tcp://192.168.99.101:2376\"\n$Env:DOCKER_CERT_PATH = \"C:\\Users\\captain\\.docker\\machine\\machines\\dev\"\n$Env:DOCKER_MACHINE_NAME = \"dev\"\n# Run this command to configure your shell:\n# docker-machine.exe env --shell=powershell dev | Invoke-Expression\n</pre> <p>For <code>cmd.exe</code>:</p> <pre>$ docker-machine.exe env --shell cmd dev\nset DOCKER_TLS_VERIFY=1\nset DOCKER_HOST=tcp://192.168.99.101:2376\nset DOCKER_CERT_PATH=C:\\Users\\captain\\.docker\\machine\\machines\\dev\nset DOCKER_MACHINE_NAME=dev\n# Run this command to configure your shell: copy and paste the above values into your command prompt\n</pre> <h2 id=\"excluding-the-created-machine-from-proxies\">Excluding the created machine from proxies</h2> <p>The env command supports a <code>--no-proxy</code> flag which will ensure that the created machine’s IP address is added to the <a href=\"https://wiki.archlinux.org/index.php/Proxy_settings\"><code>NO_PROXY</code>/<code>no_proxy</code> environment variable</a>.</p> <p>This is useful when using <code>docker-machine</code> with a local VM provider (e.g. <code>virtualbox</code> or <code>vmwarefusion</code>) in network environments where a HTTP proxy is required for internet access.</p> <pre>$ docker-machine env --no-proxy default\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://192.168.99.104:2376\"\nexport DOCKER_CERT_PATH=\"/Users/databus23/.docker/machine/certs\"\nexport DOCKER_MACHINE_NAME=\"default\"\nexport NO_PROXY=\"192.168.99.104\"\n# Run this command to configure your shell:\n# eval \"$(docker-machine env default)\"\n</pre> <p>You may also want to visit the <a href=\"../create/index#specifying-configuration-options-for-the-created-docker-engine\">documentation on setting <code>HTTP_PROXY</code> for the created daemon using the <code>--engine-env</code> flag for <code>docker-machine\ncreate</code></a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/env/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/env/</a>\n  </p>\n</div>\n","machine/reference/help/index":"<h1 id=\"help\">help</h1> <pre>Usage: docker-machine help [arg...]\n\nShows a list of commands or help for one command\n</pre> <p>Usage: docker-machine help <em>subcommand</em></p> <p>For example:</p> <pre>$ docker-machine help config\nUsage: docker-machine config [OPTIONS] [arg...]\n\nPrint the connection config for machine\n\nDescription:\n   Argument is a machine name.\n\nOptions:\n\n   --swarm      Display the Swarm config instead of the Docker daemon\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/help/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/help/</a>\n  </p>\n</div>\n","machine/reference/inspect/index":"<h1 id=\"inspect\">inspect</h1> <pre>Usage: docker-machine inspect [OPTIONS] [arg...]\n\nInspect information about a machine\n\nDescription:\n   Argument is a machine name.\n\nOptions:\n   --format, -f     Format the output using the given go template.\n</pre> <p>By default, this will render information about a machine as JSON. If a format is specified, the given template will be executed for each result.</p> <p>Go’s <a href=\"http://golang.org/pkg/text/template/\">text/template</a> package describes all the details of the format.</p> <p>In addition to the <code>text/template</code> syntax, there are some additional functions, <code>json</code> and <code>prettyjson</code>, which can be used to format the output as JSON (documented below).</p> <h2 id=\"examples\">Examples</h2> <p><strong>List all the details of a machine:</strong></p> <p>This is the default usage of <code>inspect</code>.</p> <pre>$ docker-machine inspect dev\n{\n    \"DriverName\": \"virtualbox\",\n    \"Driver\": {\n        \"MachineName\": \"docker-host-128be8d287b2028316c0ad5714b90bcfc11f998056f2f790f7c1f43f3d1e6eda\",\n        \"SSHPort\": 55834,\n        \"Memory\": 1024,\n        \"DiskSize\": 20000,\n        \"Boot2DockerURL\": \"\",\n        \"IPAddress\": \"192.168.5.99\"\n    },\n    ...\n}\n</pre> <p><strong>Get a machine’s IP address:</strong></p> <p>For the most part, you can pick out any field from the JSON in a fairly straightforward manner.</p> <pre>$ docker-machine inspect --format='{{.Driver.IPAddress}}' dev\n192.168.5.99\n</pre> <p><strong>Formatting details:</strong></p> <p>If you want a subset of information formatted as JSON, you can use the <code>json</code> function in the template.</p> <pre>$ docker-machine inspect --format='{{json .Driver}}' dev-fusion\n{\"Boot2DockerURL\":\"\",\"CPUS\":8,\"CPUs\":8,\"CaCertPath\":\"/Users/hairyhenderson/.docker/machine/certs/ca.pem\",\"DiskSize\":20000,\"IPAddress\":\"172.16.62.129\",\"ISO\":\"/Users/hairyhenderson/.docker/machine/machines/dev-fusion/boot2docker-1.5.0-GH747.iso\",\"MachineName\":\"dev-fusion\",\"Memory\":1024,\"PrivateKeyPath\":\"/Users/hairyhenderson/.docker/machine/certs/ca-key.pem\",\"SSHPort\":22,\"SSHUser\":\"docker\",\"SwarmDiscovery\":\"\",\"SwarmHost\":\"tcp://0.0.0.0:3376\",\"SwarmMaster\":false}\n</pre> <p>While this is usable, it’s not very human-readable. For this reason, there is <code>prettyjson</code>:</p> <pre>$ docker-machine inspect --format='{{prettyjson .Driver}}' dev-fusion\n{\n    \"Boot2DockerURL\": \"\",\n    \"CPUS\": 8,\n    \"CPUs\": 8,\n    \"CaCertPath\": \"/Users/hairyhenderson/.docker/machine/certs/ca.pem\",\n    \"DiskSize\": 20000,\n    \"IPAddress\": \"172.16.62.129\",\n    \"ISO\": \"/Users/hairyhenderson/.docker/machine/machines/dev-fusion/boot2docker-1.5.0-GH747.iso\",\n    \"MachineName\": \"dev-fusion\",\n    \"Memory\": 1024,\n    \"PrivateKeyPath\": \"/Users/hairyhenderson/.docker/machine/certs/ca-key.pem\",\n    \"SSHPort\": 22,\n    \"SSHUser\": \"docker\",\n    \"SwarmDiscovery\": \"\",\n    \"SwarmHost\": \"tcp://0.0.0.0:3376\",\n    \"SwarmMaster\": false\n}\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/inspect/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/inspect/</a>\n  </p>\n</div>\n","machine/reference/ls/index":"<h1 id=\"ls\">ls</h1> <pre>Usage: docker-machine ls [OPTIONS] [arg...]\n\nList machines\n\nOptions:\n\n   --quiet, -q                                  Enable quiet mode\n   --filter [--filter option --filter option]   Filter output based on conditions provided\n   --timeout, -t \"10\"                           Timeout in seconds, default to 10s\n   --format, -f                                 Pretty-print machines using a Go template\n</pre> <h2 id=\"timeout\">Timeout</h2> <p>The <code>ls</code> command tries to reach each host in parallel. If a given host does not answer in less than 10 seconds, the <code>ls</code> command will state that this host is in <code>Timeout</code> state. In some circumstances (poor connection, high load, or while troubleshooting), you may want to increase or decrease this value. You can use the -t flag for this purpose with a numerical value in seconds.</p> <h3 id=\"example\">Example</h3> <pre>$ docker-machine ls -t 12\nNAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS\ndefault   -        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1\n</pre> <h2 id=\"filtering\">Filtering</h2> <p>The filtering flag (<code>--filter</code>) format is a <code>key=value</code> pair. If there is more than one filter, then pass multiple flags (e.g. <code>--filter \"foo=bar\" --filter \"bif=baz\"</code>)</p> <p>The currently supported filters are:</p> <ul> <li>driver (driver name)</li> <li>swarm (swarm master’s name)</li> <li>state (<code>Running|Paused|Saved|Stopped|Stopping|Starting|Error</code>)</li> <li>name (Machine name returned by driver, supports <a href=\"https://github.com/google/re2/wiki/Syntax\">golang style</a> regular expressions)</li> <li>label (Machine created with <code>--engine-label</code> option, can be filtered with <code>label=&lt;key&gt;[=&lt;value&gt;]</code>)</li> </ul> <h3 id=\"examples\">Examples</h3> <pre>$ docker-machine ls\nNAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS\ndev    -        virtualbox   Stopped\nfoo0   -        virtualbox   Running   tcp://192.168.99.105:2376           v1.9.1\nfoo1   -        virtualbox   Running   tcp://192.168.99.106:2376           v1.9.1\nfoo2   *        virtualbox   Running   tcp://192.168.99.107:2376           v1.9.1\n\n$ docker-machine ls --filter name=foo0\nNAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS\nfoo0   -        virtualbox   Running   tcp://192.168.99.105:2376           v1.9.1\n\n$ docker-machine ls --filter driver=virtualbox --filter state=Stopped\nNAME   ACTIVE   DRIVER       STATE     URL   SWARM   DOCKER   ERRORS\ndev    -        virtualbox   Stopped                 v1.9.1\n\n$ docker-machine ls --filter label=com.class.app=foo1 --filter label=com.class.app=foo2\nNAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS\nfoo1   -        virtualbox   Running   tcp://192.168.99.105:2376           v1.9.1\nfoo2   *        virtualbox   Running   tcp://192.168.99.107:2376           v1.9.1\n</pre> <h2 id=\"formatting\">Formatting</h2> <p>The formatting option (<code>--format</code>) will pretty-print machines using a Go template.</p> <p>Valid placeholders for the Go template are listed below:</p> <table> <thead> <tr> <th>Placeholder</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>.Name</td> <td>Machine name</td> </tr> <tr> <td>.Active</td> <td>Is the machine active?</td> </tr> <tr> <td>.ActiveHost</td> <td>Is the machine an active non-swarm host?</td> </tr> <tr> <td>.ActiveSwarm</td> <td>Is the machine an active swarm master?</td> </tr> <tr> <td>.DriverName</td> <td>Driver name</td> </tr> <tr> <td>.State</td> <td>Machine state (running, stopped…)</td> </tr> <tr> <td>.URL</td> <td>Machine URL</td> </tr> <tr> <td>.Swarm</td> <td>Machine swarm name</td> </tr> <tr> <td>.Error</td> <td>Machine errors</td> </tr> <tr> <td>.DockerVersion</td> <td>Docker Daemon version</td> </tr> <tr> <td>.ResponseTime</td> <td>Time taken by the host to respond</td> </tr> </tbody> </table> <p>When using the <code>--format</code> option, the <code>ls</code> command will either output the data exactly as the template declares or, when using the table directive, will include column headers as well.</p> <p>The following example uses a template without headers and outputs the <code>Name</code> and <code>Driver</code> entries separated by a colon for all running machines:</p> <pre>$ docker-machine ls --format \"{{.Name}}: {{.DriverName}}\"\ndefault: virtualbox\nec2: amazonec2\n</pre> <p>To list all machine names with their driver in a table format you can use:</p> <pre>$ docker-machine ls --format \"table {{.Name}} {{.DriverName}}\"\nNAME     DRIVER\ndefault  virtualbox\nec2      amazonec2\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/ls/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/ls/</a>\n  </p>\n</div>\n","machine/reference/regenerate-certs/index":"<h1 id=\"regenerate-certs\">regenerate-certs</h1> <pre>Usage: docker-machine regenerate-certs [OPTIONS] [arg...]\n\nRegenerate TLS Certificates for a machine\n\nDescription:\n   Argument(s) are one or more machine names.\n\nOptions:\n\n   --force, -f  Force rebuild and do not prompt\n</pre> <p>Regenerate TLS certificates and update the machine with new certs.</p> <p>For example:</p> <pre>$ docker-machine regenerate-certs dev\nRegenerate TLS machine certs?  Warning: this is irreversible. (y/n): y\nRegenerating TLS certificates\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/regenerate-certs/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/regenerate-certs/</a>\n  </p>\n</div>\n","machine/reference/restart/index":"<h1 id=\"restart\">restart</h1> <pre>Usage: docker-machine restart [arg...]\n\nRestart a machine\n\nDescription:\n   Argument(s) are one or more machine names.\n</pre> <p>Restart a machine. Oftentimes this is equivalent to <code>docker-machine stop; docker-machine start</code>. But some cloud driver try to implement a clever restart which keeps the same ip address.</p> <pre>$ docker-machine restart dev\nWaiting for VM to start...\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/restart/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/restart/</a>\n  </p>\n</div>\n","machine/reference/provision/index":"<h1 id=\"provision\">provision</h1> <p>Re-run provisioning on a created machine.</p> <p>Sometimes it may be helpful to re-run Machine’s provisioning process on a created machine. Reasons for doing so may include a failure during the original provisioning process, or a drift from the desired system state (including the originally specified Swarm or Engine configuration).</p> <p>Usage is <code>docker-machine provision [name]</code>. Multiple names may be specified.</p> <pre>$ docker-machine provision foo bar\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\n</pre> <p>The Machine provisioning process will:</p> <ol> <li>Set the hostname on the instance to the name Machine addresses it by (e.g. <code>default</code>).</li> <li>Install Docker if it is not present already.</li> <li>Generate a set of certificates (usually with the default, self-signed CA) and configure the daemon to accept connections over TLS.</li> <li>Copy the generated certificates to the server and local config directory.</li> <li>Configure the Docker Engine according to the options specified at create time.</li> <li>Configure and activate Swarm if applicable.</li> </ol><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/provision/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/provision/</a>\n  </p>\n</div>\n","machine/reference/rm/index":"<h1 id=\"rm\">rm</h1> <p>Remove a machine. This will remove the local reference as well as delete it on the cloud provider or virtualization management platform.</p> <pre>$ docker-machine rm --help\n\nUsage: docker-machine rm [OPTIONS] [arg...]\n\nRemove a machine\n\nDescription:\n   Argument(s) are one or more machine names.\n\nOptions:\n\n   --force, -f  Remove local configuration even if machine cannot be removed, also implies an automatic yes (`-y`)\n   -y       Assumes automatic yes to proceed with remove, without prompting further user confirmation\n</pre> <h2 id=\"examples\">Examples</h2> <pre>$ docker-machine ls\nNAME   ACTIVE   URL          STATE     URL                         SWARM   DOCKER   ERRORS\nbar    -        virtualbox   Running   tcp://192.168.99.101:2376           v1.9.1\nbaz    -        virtualbox   Running   tcp://192.168.99.103:2376           v1.9.1\nfoo    -        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1\nqix    -        virtualbox   Running   tcp://192.168.99.102:2376           v1.9.1\n\n\n$ docker-machine rm baz\nAbout to remove baz\nAre you sure? (y/n): y\nSuccessfully removed baz\n\n\n$ docker-machine ls\nNAME   ACTIVE   URL          STATE     URL                         SWARM   DOCKER   ERRORS\nbar    -        virtualbox   Running   tcp://192.168.99.101:2376           v1.9.1\nfoo    -        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1\nqix    -        virtualbox   Running   tcp://192.168.99.102:2376           v1.9.1\n\n\n$ docker-machine rm bar qix\nAbout to remove bar, qix\nAre you sure? (y/n): y\nSuccessfully removed bar\nSuccessfully removed qix\n\n\n$ docker-machine ls\nNAME   ACTIVE   URL          STATE     URL                         SWARM   DOCKER   ERRORS\nfoo    -        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1\n\n$ docker-machine rm -y foo\nAbout to remove foo\nSuccessfully removed foo\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/rm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/rm/</a>\n  </p>\n</div>\n","machine/reference/scp/index":"<h1 id=\"scp\">scp</h1> <p>Copy files from your local host to a machine, from machine to machine, or from a machine to your local host using <code>scp</code>.</p> <p>The notation is <code>machinename:/path/to/files</code> for the arguments; in the host machine’s case, you don’t have to specify the name, just the path.</p> <p>Consider the following example:</p> <pre>$ cat foo.txt\ncat: foo.txt: No such file or directory\n$ docker-machine ssh dev pwd\n/home/docker\n$ docker-machine ssh dev 'echo A file created remotely! &gt;foo.txt'\n$ docker-machine scp dev:/home/docker/foo.txt .\nfoo.txt                                                           100%   28     0.0KB/s   00:00\n$ cat foo.txt\nA file created remotely!\n</pre> <p>Just like how <code>scp</code> has a <code>-r</code> flag for copying files recursively, <code>docker-machine</code> has a <code>-r</code> flag for this feature.</p> <p>In the case of transferring files from machine to machine, they go through the local host’s filesystem first (using <code>scp</code>’s <code>-3</code> flag).</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/scp/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/scp/</a>\n  </p>\n</div>\n","machine/reference/ssh/index":"<h1 id=\"ssh\">ssh</h1> <p>Log into or run a command on a machine using SSH.</p> <p>To login, just run <code>docker-machine ssh machinename</code>:</p> <pre>$ docker-machine ssh dev\n                        ##        .\n                  ## ## ##       ==\n               ## ## ## ##      ===\n           /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===\n      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~\n           \\______ o          __/\n             \\    \\        __/\n              \\____\\______/\n _                 _   ____     _            _\n| |__   ___   ___ | |_|___ \\ __| | ___   ___| | _____ _ __\n| '_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ '__|\n| |_) | (_) | (_) | |_ / __/ (_| | (_) | (__|   &lt;  __/ |\n|_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_|\nBoot2Docker version 1.4.0, build master : 69cf398 - Fri Dec 12 01:39:42 UTC 2014\ndocker@boot2docker:~$ ls /\nUsers/   dev/     home/    lib/     mnt/     proc/    run/     sys/     usr/\nbin/     etc/     init     linuxrc  opt/     root/    sbin/    tmp      var/\n</pre> <p>You can also specify commands to run remotely by appending them directly to the <code>docker-machine ssh</code> command, much like the regular <code>ssh</code> program works:</p> <pre>$ docker-machine ssh dev free\n             total         used         free       shared      buffers\nMem:       1023556       183136       840420            0        30920\n-/+ buffers:             152216       871340\nSwap:      1212036            0      1212036\n</pre> <p>Commands with flags will work as well:</p> <pre>$ docker-machine ssh dev df -h\nFilesystem                Size      Used Available Use% Mounted on\nrootfs                  899.6M     85.9M    813.7M  10% /\ntmpfs                   899.6M     85.9M    813.7M  10% /\ntmpfs                   499.8M         0    499.8M   0% /dev/shm\n/dev/sda1                18.2G     58.2M     17.2G   0% /mnt/sda1\ncgroup                  499.8M         0    499.8M   0% /sys/fs/cgroup\n/dev/sda1                18.2G     58.2M     17.2G   0%\n/mnt/sda1/var/lib/docker/aufs\n</pre> <p>If you are using the “external” SSH type as detailed in the next section, you can include additional arguments to pass through to the <code>ssh</code> binary in the generated command (unless they conflict with any of the default arguments for the command generated by Docker Machine). For instance, the following command will forward port 8080 from the <code>default</code> machine to <code>localhost</code> on your host computer:</p> <pre>$ docker-machine ssh default -L 8080:localhost:8080\n</pre> <h2 id=\"different-types-of-ssh\">Different types of SSH</h2> <p>When Docker Machine is invoked, it will check to see if you have the venerable <code>ssh</code> binary around locally and will attempt to use that for the SSH commands it needs to run, whether they are a part of an operation such as creation or have been requested by the user directly. If it does not find an external <code>ssh</code> binary locally, it will default to using a native Go implementation from <a href=\"https://godoc.org/golang.org/x/crypto/ssh\">crypto/ssh</a>. This is useful in situations where you may not have access to traditional UNIX tools, such as if you are using Docker Machine on Windows without having msysgit installed alongside of it.</p> <p>In most situations, you will not have to worry about this implementation detail and Docker Machine will act sensibly out of the box. However, if you deliberately want to use the Go native version, you can do so with a global command line flag / environment variable like so:</p> <pre>$ docker-machine --native-ssh ssh dev\n</pre> <p>There are some variations in behavior between the two methods, so please report any issues or inconsistencies if you come across them.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/ssh/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/ssh/</a>\n  </p>\n</div>\n","machine/reference/start/index":"<h1 id=\"start\">start</h1> <pre>Usage: docker-machine start [arg...]\n\nStart a machine\n\nDescription:\n   Argument(s) are one or more machine names.\n</pre> <p>For example:</p> <pre>$ docker-machine start dev\nStarting VM...\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/start/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/start/</a>\n  </p>\n</div>\n","machine/reference/status/index":"<h1 id=\"status\">status</h1> <pre>Usage: docker-machine status [arg...]\n\nGet the status of a machine\n\nDescription:\n   Argument is a machine name.\n</pre> <p>For example:</p> <pre>$ docker-machine status dev\nRunning\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/status/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/status/</a>\n  </p>\n</div>\n","machine/reference/stop/index":"<h1 id=\"stop\">stop</h1> <pre>Usage: docker-machine stop [arg...]\n\nGracefully Stop a machine\n\nDescription:\n   Argument(s) are one or more machine names.\n</pre> <p>For example:</p> <pre>$ docker-machine ls\nNAME   ACTIVE   DRIVER       STATE     URL\ndev    *        virtualbox   Running   tcp://192.168.99.104:2376\n$ docker-machine stop dev\n$ docker-machine ls\nNAME   ACTIVE   DRIVER       STATE     URL\ndev    *        virtualbox   Stopped\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/stop/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/stop/</a>\n  </p>\n</div>\n","machine/reference/upgrade/index":"<h1 id=\"upgrade\">upgrade</h1> <p>Upgrade a machine to the latest version of Docker. How this upgrade happens depends on the underlying distribution used on the created instance.</p> <p>For example, if the machine uses Ubuntu as the underlying operating system, it will run a command similar to <code>sudo apt-get upgrade docker-engine</code>, because Machine expects Ubuntu machines it manages to use this package. As another example, if the machine uses boot2docker for its OS, this command will download the latest boot2docker ISO and replace the machine’s existing ISO with the latest.</p> <pre>$ docker-machine upgrade default\nStopping machine to do the upgrade...\nUpgrading machine default...\nDownloading latest boot2docker release to /home/username/.docker/machine/cache/boot2docker.iso...\nStarting machine back up...\nWaiting for VM to start...\n</pre> <blockquote> <p><strong>Note</strong>: If you are using a custom boot2docker ISO specified using <code>--virtualbox-boot2docker-url</code> or an equivalent flag, running an upgrade on that machine will completely replace the specified ISO with the latest “vanilla” boot2docker ISO available.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/upgrade/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/upgrade/</a>\n  </p>\n</div>\n","machine/drivers/os-base/index":"<h1 id=\"driver-options-and-operating-system-defaults\">Driver options and operating system defaults</h1> <p>When Docker Machine provisions containers on local network provider or with a remote, cloud provider such as Amazon Web Services, you must define both the driver for your provider and a base operating system. There are over 10 supported drivers and a generic driver for adding machines for other providers.</p> <p>Each driver has a set of options specific to that provider. These options provide information to machine such as connection credentials, ports, and so forth. For example, to create an Azure machine:</p> <p>Grab your subscription ID from the portal, then run <code>docker-machine create</code> with these details:</p> <pre>$ docker-machine create -d azure --azure-subscription-id=\"SUB_ID\" --azure-subscription-cert=\"mycert.pem\" A-VERY-UNIQUE-NAME\n</pre> <p>To see a list of providers and review the options available to a provider, see the reference for that driver.</p> <p>In addition to the provider, you have the option of identifying a base operating system. It is an option because Docker Machine has defaults for both local and remote providers. For local providers such as VirtualBox, Fusion, Hyper-V, and so forth, the default base operating system is Boot2Docker. For cloud providers, the base operating system is the latest Ubuntu LTS the provider supports.</p> <table> <thead> <tr> <th>Operating System</th> <th>Version</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td>Boot2Docker</td> <td>1.5+</td> <td>default for local</td> </tr> <tr> <td>Ubuntu</td> <td>12.04+</td> <td>default for remote</td> </tr> <tr> <td>RancherOS</td> <td>0.3+</td> <td></td> </tr> <tr> <td>Debian</td> <td>8.0+</td> <td>experimental</td> </tr> <tr> <td>RedHat Enterprise Linux</td> <td>7.0+</td> <td>experimental</td> </tr> <tr> <td>CentOS</td> <td>7+</td> <td>experimental</td> </tr> <tr> <td>Fedora</td> <td>21+</td> <td>experimental</td> </tr> </tbody> </table> <p>To use a different base operating system on a remote provider, specify the provider’s image flag and one of its available images. For example, to select a <code>debian-8-x64</code> image on DigitalOcean you would supply the <code>--digitalocean-image=debian-8-x64</code> flag.</p> <p>If you change the base image for a provider, you may also need to change the SSH user. For example, the default Red Hat AMI on EC2 expects the SSH user to be <code>ec2-user</code>, so you would have to specify this with <code>--amazonec2-ssh-user ec2-user</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/os-base/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/os-base/</a>\n  </p>\n</div>\n","machine/reference/url/index":"<h1 id=\"url\">url</h1> <p>Get the URL of a host</p> <pre>$ docker-machine url dev\ntcp://192.168.99.109:2376\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/url/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/url/</a>\n  </p>\n</div>\n","machine/drivers/digital-ocean/index":"<h1 id=\"digital-ocean\">Digital Ocean</h1> <p>Create Docker machines on <a href=\"https://www.digitalocean.com/\">Digital Ocean</a>.</p> <p>You need to create a personal access token under “Apps &amp; API” in the Digital Ocean Control Panel and pass that to <code>docker-machine create</code> with the <code>--digitalocean-access-token</code> option.</p> <pre>$ docker-machine create --driver digitalocean --digitalocean-access-token=aa9399a2175a93b17b1c86c807e08d3fc4b79876545432a629602f61cf6ccd6b test-this\n</pre> <p>Options:</p> <ul> <li>\n<code>--digitalocean-access-token</code>: <strong>required</strong> Your personal access token for the Digital Ocean API.</li> <li>\n<code>--digitalocean-image</code>: The name of the Digital Ocean image to use.</li> <li>\n<code>--digitalocean-region</code>: The region to create the droplet in, see <a href=\"https://developers.digitalocean.com/documentation/v2/#regions\">Regions API</a> for how to get a list.</li> <li>\n<code>--digitalocean-size</code>: The size of the Digital Ocean droplet (larger than default options are of the form <code>2gb</code>).</li> <li>\n<code>--digitalocean-ipv6</code>: Enable IPv6 support for the droplet.</li> <li>\n<code>--digitalocean-private-networking</code>: Enable private networking support for the droplet.</li> <li>\n<code>--digitalocean-backups</code>: Enable Digital Oceans backups for the droplet.</li> <li>\n<code>--digitalocean-userdata</code>: Path to file containing User Data for the droplet.</li> <li>\n<code>--digitalocean-ssh-user</code>: SSH username.</li> <li>\n<code>--digitalocean-ssh-port</code>: SSH port.</li> <li>\n<code>--digitalocean-ssh-key-fingerprint</code>: Use an existing SSH key instead of creating a new one, see <a href=\"https://developers.digitalocean.com/documentation/v2/#ssh-keys\">SSH keys</a>.</li> </ul> <p>The DigitalOcean driver will use <code>ubuntu-15-10-x64</code> as the default image.</p> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><strong><code>--digitalocean-access-token</code></strong></td> <td><code>DIGITALOCEAN_ACCESS_TOKEN</code></td> <td>-</td> </tr> <tr> <td><code>--digitalocean-image</code></td> <td><code>DIGITALOCEAN_IMAGE</code></td> <td><code>ubuntu-15-10-x64</code></td> </tr> <tr> <td><code>--digitalocean-region</code></td> <td><code>DIGITALOCEAN_REGION</code></td> <td><code>nyc3</code></td> </tr> <tr> <td><code>--digitalocean-size</code></td> <td><code>DIGITALOCEAN_SIZE</code></td> <td><code>512mb</code></td> </tr> <tr> <td><code>--digitalocean-ipv6</code></td> <td><code>DIGITALOCEAN_IPV6</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--digitalocean-private-networking</code></td> <td><code>DIGITALOCEAN_PRIVATE_NETWORKING</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--digitalocean-backups</code></td> <td><code>DIGITALOCEAN_BACKUPS</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--digitalocean-userdata</code></td> <td><code>DIGITALOCEAN_USERDATA</code></td> <td>-</td> </tr> <tr> <td><code>--digitalocean-ssh-user</code></td> <td><code>DIGITALOCEAN_SSH_USER</code></td> <td><code>root</code></td> </tr> <tr> <td><code>--digitalocean-ssh-port</code></td> <td><code>DIGITALOCEAN_SSH_PORT</code></td> <td>22</td> </tr> <tr> <td><code>--digitalocean-ssh-key-fingerprint</code></td> <td><code>DIGITALOCEAN_SSH_KEY_FINGERPRINT</code></td> <td>-</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/digital-ocean/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/digital-ocean/</a>\n  </p>\n</div>\n","machine/drivers/generic/index":"<h1 id=\"generic\">Generic</h1> <p>Create machines using an existing VM/Host with SSH.</p> <p>This is useful if you are using a provider that Machine does not support directly or if you would like to import an existing host to allow Docker Machine to manage.</p> <p>The driver will perform a list of tasks on create:</p> <ul> <li>If docker is not running on the host, it will be installed automatically.</li> <li>It will update the host packages (<code>apt-get update</code>, <code>yum update</code>…).</li> <li>It will generate certificates to secure the docker daemon.</li> <li>The docker daemon will be restarted, thus all running containers will be stopped.</li> <li>The hostname will be changed to fit the machine name.</li> </ul> <h3 id=\"example\">Example</h3> <p>To create a machine instance, specify <code>--driver generic</code>, the IP address or DNS name of the host and the path to the SSH private key authorized to connect to the host.</p> <pre>$ docker-machine create \\\n  --driver generic \\\n  --generic-ip-address=203.0.113.81 \\\n  --generic-ssh-key ~/.ssh/id_rsa \\\n  vm\n</pre> <h3 id=\"sudo-privileges\">Sudo privileges</h3> <p>The user that is used to SSH into the host can be specified with <code>--generic-ssh-user</code> flag. This user has to have password-less sudo privileges. If it’s not the case, you need to edit the <code>sudoers</code> file and configure the user as a sudoer with <code>NOPASSWD</code>. See <a href=\"https://help.ubuntu.com/community/Sudoers\">https://help.ubuntu.com/community/Sudoers</a>.</p> <h3 id=\"options\">Options</h3> <ul> <li>\n<code>--generic-engine-port</code>: Port to use for Docker Daemon (Note: This flag will not work with boot2docker).</li> <li>\n<code>--generic-ip-address</code>: <strong>required</strong> IP Address of host.</li> <li>\n<code>--generic-ssh-key</code>: Path to the SSH user private key.</li> <li>\n<code>--generic-ssh-user</code>: SSH username used to connect.</li> <li>\n<code>--generic-ssh-port</code>: Port to use for SSH.</li> </ul> <blockquote> <p><strong>Note</strong>: You must use a base operating system supported by Machine.</p> </blockquote> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>--generic-engine-port</code></td> <td><code>GENERIC_ENGINE_PORT</code></td> <td><code>2376</code></td> </tr> <tr> <td><strong><code>--generic-ip-address</code></strong></td> <td><code>GENERIC_IP_ADDRESS</code></td> <td>-</td> </tr> <tr> <td><code>--generic-ssh-key</code></td> <td><code>GENERIC_SSH_KEY</code></td> <td>-</td> </tr> <tr> <td><code>--generic-ssh-user</code></td> <td><code>GENERIC_SSH_USER</code></td> <td><code>root</code></td> </tr> <tr> <td><code>--generic-ssh-port</code></td> <td><code>GENERIC_SSH_PORT</code></td> <td><code>22</code></td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/generic/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/generic/</a>\n  </p>\n</div>\n","machine/drivers/gce/index":"<h1 id=\"google-compute-engine\">Google Compute Engine</h1> <p>Create machines on <a href=\"https://cloud.google.com/compute/\">Google Compute Engine</a>. You will need a Google account and a project id. See <a href=\"https://cloud.google.com/compute/docs/projects\">https://cloud.google.com/compute/docs/projects</a> for details on projects.</p> <h3 id=\"credentials\">Credentials</h3> <p>The Google driver uses <a href=\"https://developers.google.com/identity/protocols/application-default-credentials\">Application Default Credentials</a> to get authorization credentials for use in calling Google APIs.</p> <p>So if <code>docker-machine</code> is used from a GCE host, authentication will happen automatically via the built-in service account. Otherwise, <a href=\"https://cloud.google.com/sdk/\">install gcloud</a> and get through the oauth2 process with <code>gcloud auth login</code>.</p> <h3 id=\"example\">Example</h3> <p>To create a machine instance, specify <code>--driver google</code>, the project id and the machine name.</p> <pre>$ gcloud auth login\n$ docker-machine create --driver google --google-project PROJECT_ID vm01\n$ docker-machine create --driver google \\\n  --google-project PROJECT_ID \\\n  --google-zone us-central1-a \\\n  --google-machine-type f1-micro \\\n  vm02\n</pre> <h3 id=\"options\">Options</h3> <pre>-   `--google-project`: **required** The id of your project to use when launching the instance.\n-   `--google-zone`: The zone to launch the instance.\n-   `--google-machine-type`: The type of instance.\n-   `--google-machine-image`: The absolute URL to a base VM image to instantiate.\n-   `--google-username`: The username to use for the instance.\n-   `--google-scopes`: The scopes for OAuth 2.0 to Access Google APIs. See [Google Compute Engine Doc](https://cloud.google.com/storage/docs/authentication).\n-   `--google-disk-size`: The disk size of instance.\n-   `--google-disk-type`: The disk type of instance.\n-   `--google-address`: Instance's static external IP (name or IP).\n-   `--google-preemptible`: Instance preemptibility.\n-   `--google-tags`: Instance tags (comma-separated).\n-   `--google-use-internal-ip`: When this option is used during create it will make docker-machine use internal rather than public NATed IPs. The flag is persistent in the sense that a machine created with it retains the IP. It's useful for managing docker machines from another machine on the same network e.g. while deploying swarm.\n-   `--google-use-internal-ip-only`: When this option is used during create, the new VM will not be assigned a public IP address. This is useful only when the host running `docker-machine` is located inside the Google Cloud infrastructure; otherwise, `docker-machine` can't reach the VM to provision the Docker daemon. The presence of this flag implies `--google-use-internal-ip`.\n-   `--google-use-existing`: Don't create a new VM, use an existing one. This is useful when you'd like to provision Docker on a VM you created yourself, maybe because it uses create options not supported by this driver. \n</pre> <p>The GCE driver will use the <code>ubuntu-1510-wily-v20151114</code> instance image unless otherwise specified. To obtain a list of image URLs run:</p> <pre>gcloud compute images list --uri\n</pre> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><strong><code>--google-project</code></strong></td> <td><code>GOOGLE_PROJECT</code></td> <td>-</td> </tr> <tr> <td><code>--google-zone</code></td> <td><code>GOOGLE_ZONE</code></td> <td><code>us-central1-a</code></td> </tr> <tr> <td><code>--google-machine-type</code></td> <td><code>GOOGLE_MACHINE_TYPE</code></td> <td><code>f1-standard-1</code></td> </tr> <tr> <td><code>--google-machine-image</code></td> <td><code>GOOGLE_MACHINE_IMAGE</code></td> <td><code>ubuntu-1510-wily-v20151114</code></td> </tr> <tr> <td><code>--google-username</code></td> <td><code>GOOGLE_USERNAME</code></td> <td><code>docker-user</code></td> </tr> <tr> <td><code>--google-scopes</code></td> <td><code>GOOGLE_SCOPES</code></td> <td><code>devstorage.read_only,logging.write</code></td> </tr> <tr> <td><code>--google-disk-size</code></td> <td><code>GOOGLE_DISK_SIZE</code></td> <td><code>10</code></td> </tr> <tr> <td><code>--google-disk-type</code></td> <td><code>GOOGLE_DISK_TYPE</code></td> <td><code>pd-standard</code></td> </tr> <tr> <td><code>--google-address</code></td> <td><code>GOOGLE_ADDRESS</code></td> <td>-</td> </tr> <tr> <td><code>--google-preemptible</code></td> <td><code>GOOGLE_PREEMPTIBLE</code></td> <td>-</td> </tr> <tr> <td><code>--google-tags</code></td> <td><code>GOOGLE_TAGS</code></td> <td>-</td> </tr> <tr> <td><code>--google-use-internal-ip</code></td> <td><code>GOOGLE_USE_INTERNAL_IP</code></td> <td>-</td> </tr> <tr> <td><code>--google-use-existing</code></td> <td><code>GOOGLE_USE_EXISTING</code></td> <td>-</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/gce/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/gce/</a>\n  </p>\n</div>\n","machine/drivers/azure/index":"<h1 id=\"microsoft-azure\">Microsoft Azure</h1> <p>You will need an Azure Subscription to use this Docker Machine driver. <a href=\"https://azure.microsoft.com/free/\">Sign up for a free trial.</a></p> <blockquote> <p><strong>NOTE:</strong> This documentation is for the new version of the Azure driver, which started shipping with v0.7.0. This driver is not backwards-compatible with the old Azure driver. If you want to continue managing your existing Azure machines, please download and use machine versions prior to v0.7.0.</p> </blockquote> <h2 id=\"authentication\">Authentication</h2> <p>The first time you try to create a machine, Azure driver will ask you to authenticate:</p> <pre>$ docker-machine create --driver azure --azure-subscription-id &lt;subs-id&gt; &lt;machine-name&gt;\nRunning pre-create checks...\nMicrosoft Azure: To sign in, use a web browser to open the page https://aka.ms/devicelogin.\nEnter the code [...] to authenticate.\n</pre> <p>After authenticating, the driver will remember your credentials up to two weeks.</p> <blockquote> <p><strong>KNOWN ISSUE:</strong> There is a known issue with Azure Active Directory causing stored credentials to expire within hours rather than 14 days when the user logs in with personal Microsoft Account (formerly <em>Live ID</em>) instead of an Active Directory account. Currently, there is no ETA for resolution, however in the meanwhile you can <a href=\"https://azure.microsoft.com/documentation/articles/virtual-machines-windows-create-aad-work-id/\">create an AAD account</a> and login with that as a workaround.</p> </blockquote> <h2 id=\"options\">Options</h2> <p>Azure driver only has a single required argument to make things easier. Please read the optional flags to configure machine details and placement further.</p> <p>Required:</p> <ul> <li>\n<code>--azure-subscription-id</code>: <strong>(required)</strong> Your Azure Subscription ID.</li> </ul> <p>Optional:</p> <ul> <li>\n<code>--azure-image</code>: Azure virtual machine image in the format of Publisher:Offer:Sku:Version [<a href=\"https://azure.microsoft.com/en-us/documentation/articles/resource-groups-vm-searching/\">?</a>]</li> <li>\n<code>--azure-location</code>: Azure region to create the virtual machine. [<a href=\"https://azure.microsoft.com/en-us/regions/\">?</a>]</li> <li>\n<code>--azure-resource-group</code>: Azure Resource Group name to create the resources in.</li> <li>\n<code>--azure-size</code>: Size for Azure Virtual Machine. [<a href=\"https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-size-specs/\">?</a>]</li> <li>\n<code>--azure-ssh-user</code>: Username for SSH login.</li> <li>\n<code>--azure-vnet</code>: Azure Virtual Network name to connect the virtual machine. [<a href=\"https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-overview/\">?</a>]</li> <li>\n<code>--azure-subnet</code>: Azure Subnet Name to be used within the Virtual Network.</li> <li>\n<code>--azure-subnet-prefix</code>: Private CIDR block. Used to create subnet if it does not exist. Must match in the case that the subnet does exist.</li> <li>\n<code>--azure-availability-set</code>: Azure Availability Set to place the virtual machine into. [<a href=\"https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-manage-availability/\">?</a>]</li> <li>\n<code>--azure-open-port</code>: Make additional port number(s) accessible from the Internet [<a href=\"https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-nsg/\">?</a>]</li> <li>\n<code>--azure-private-ip-address</code>: Specify a static private IP address for the machine.</li> <li>\n<code>--azure-use-private-ip</code>: Use private IP address of the machine to connect. It’s useful for managing Docker machines from another machine on the same network e.g. while deploying Swarm.</li> <li>\n<code>--azure-no-public-ip</code>: Do not create a public IP address for the machine (implies <code>--azure-use-private-ip</code>). Should be used only when creating machines from an Azure VM within the same subnet.</li> <li>\n<code>--azure-static-public-ip</code>: Assign a static public IP address to the machine.</li> <li>\n<code>--azure-docker-port</code>: Port number for Docker engine.</li> <li>\n<code>--azure-environment</code>: Azure environment (e.g. <code>AzurePublicCloud</code>, <code>AzureChinaCloud</code>).</li> </ul> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><strong><code>--azure-subscription-id</code></strong></td> <td><code>AZURE_SUBSCRIPTION_ID</code></td> <td>-</td> </tr> <tr> <td><code>--azure-environment</code></td> <td><code>AZURE_ENVIRONMENT</code></td> <td><code>AzurePublicCloud</code></td> </tr> <tr> <td><code>--azure-image</code></td> <td><code>AZURE_IMAGE</code></td> <td><code>canonical:UbuntuServer:15.10:latest</code></td> </tr> <tr> <td><code>--azure-location</code></td> <td><code>AZURE_LOCATION</code></td> <td><code>westus</code></td> </tr> <tr> <td><code>--azure-resource-group</code></td> <td><code>AZURE_RESOURCE_GROUP</code></td> <td><code>docker-machine</code></td> </tr> <tr> <td><code>--azure-size</code></td> <td><code>AZURE_SIZE</code></td> <td><code>Standard_A2</code></td> </tr> <tr> <td><code>--azure-ssh-user</code></td> <td><code>AZURE_SSH_USER</code></td> <td><code>docker-user</code></td> </tr> <tr> <td><code>--azure-vnet</code></td> <td><code>AZURE_VNET</code></td> <td><code>docker-machine</code></td> </tr> <tr> <td><code>--azure-subnet</code></td> <td><code>AZURE_SUBNET</code></td> <td><code>docker-machine</code></td> </tr> <tr> <td><code>--azure-subnet-prefix</code></td> <td><code>AZURE_SUBNET_PREFIX</code></td> <td><code>192.168.0.0/16</code></td> </tr> <tr> <td><code>--azure-availability-set</code></td> <td><code>AZURE_AVAILABILITY_SET</code></td> <td><code>docker-machine</code></td> </tr> <tr> <td><code>--azure-open-port</code></td> <td>-</td> <td>-</td> </tr> <tr> <td><code>--azure-private-ip-address</code></td> <td>-</td> <td>-</td> </tr> <tr> <td><code>--azure-use-private-ip</code></td> <td>-</td> <td>-</td> </tr> <tr> <td><code>--azure-no-public-ip</code></td> <td>-</td> <td>-</td> </tr> <tr> <td><code>--azure-static-public-ip</code></td> <td>-</td> <td>-</td> </tr> <tr> <td><code>--azure-docker-port</code></td> <td><code>AZURE_DOCKER_PORT</code></td> <td><code>2376</code></td> </tr> </tbody> </table> <h2 id=\"notes\">Notes</h2> <p>Azure runs fully on the new <a href=\"https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/\">Azure Resource Manager (ARM)</a> stack. Each machine created comes with a few more Azure resources associated with it:</p> <ul> <li>A <a href=\"https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-overview/\">Virtual Network</a> and a subnet under it is created to place your machines into. This establishes a local network between your docker machines.</li> <li>An <a href=\"https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-manage-availability/\">Availability Set</a> is created to maximize availability of your machines.</li> </ul> <p>These are created once when the first machine is created and reused afterwards. Although they are free resources, driver does a best effort to clean them up after the last machine using these resources is removed.</p> <p>Each machine is created with a public dynamic IP address for external connectivity. All its ports (except Docker and SSH) are closed by default. You can use <code>--azure-open-port</code> argument to specify multiple port numbers to be accessible from Internet.</p> <p>Once the machine is created, you can modify <a href=\"https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-nsg/\">Network Security Group</a> rules and open ports of the machine from the <a href=\"https://portal.azure.com/\">Azure Portal</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/azure/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/azure/</a>\n  </p>\n</div>\n","machine/drivers/aws/index":"<h1 id=\"amazon-web-services\">Amazon Web Services</h1> <p>Create machines on <a href=\"http://aws.amazon.com\">Amazon Web Services</a>.</p> <p>To create machines on <a href=\"http://aws.amazon.com\">Amazon Web Services</a>, you must supply two parameters: the AWS Access Key ID and the AWS Secret Access Key.</p> <h2 id=\"configuring-credentials\">Configuring credentials</h2> <p>Before using the amazonec2 driver, ensure that you’ve configured credentials.</p> <h3 id=\"aws-credential-file\">AWS credential file</h3> <p>One way to configure credentials is to use the standard credential file for Amazon AWS <code>~/.aws/credentials</code> file, which might look like:</p> <pre>[default]\naws_access_key_id = AKID1234567890\naws_secret_access_key = MY-SECRET-KEY\n</pre> <p>On Mac OS or various flavors of Linux you can install the <a href=\"http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration\">AWS Command Line Interface</a> (<code>aws cli</code>) in the terminal and use the <code>aws configure</code> command which guides you through the creation of the credentials file.</p> <p>This is the simplest method, you can then create a new machine with:</p> <pre>$ docker-machine create --driver amazonec2 aws01\n</pre> <h3 id=\"command-line-flags\">Command line flags</h3> <p>Alternatively, you can use the flags <code>--amazonec2-access-key</code> and <code>--amazonec2-secret-key</code> on the command line:</p> <pre>$ docker-machine create --driver amazonec2 --amazonec2-access-key AKI******* --amazonec2-secret-key 8T93C*******  aws01\n</pre> <h3 id=\"environment-variables\">Environment variables</h3> <p>You can use environment variables:</p> <pre>$ export AWS_ACCESS_KEY_ID=AKID1234567890\n$ export AWS_SECRET_ACCESS_KEY=MY-SECRET-KEY\n$ docker-machine create --driver amazonec2 aws01\n</pre> <h2 id=\"options\">Options</h2> <ul> <li>\n<code>--amazonec2-access-key</code>: Your access key id for the Amazon Web Services API.</li> <li>\n<code>--amazonec2-secret-key</code>: Your secret access key for the Amazon Web Services API.</li> <li>\n<code>--amazonec2-session-token</code>: Your session token for the Amazon Web Services API.</li> <li>\n<code>--amazonec2-ami</code>: The AMI ID of the instance to use.</li> <li>\n<code>--amazonec2-region</code>: The region to use when launching the instance.</li> <li>\n<code>--amazonec2-vpc-id</code>: Your VPC ID to launch the instance in.</li> <li>\n<code>--amazonec2-zone</code>: The AWS zone to launch the instance in (i.e. one of a,b,c,d,e).</li> <li>\n<code>--amazonec2-subnet-id</code>: AWS VPC subnet id.</li> <li>\n<code>--amazonec2-security-group</code>: AWS VPC security group name.</li> <li>\n<code>--amazonec2-tags</code>: AWS extra tag key-value pairs (comma-separated, e.g. key1,value1,key2,value2).</li> <li>\n<code>--amazonec2-instance-type</code>: The instance type to run.</li> <li>\n<code>--amazonec2-device-name</code>: The root device name of the instance.</li> <li>\n<code>--amazonec2-root-size</code>: The root disk size of the instance (in GB).</li> <li>\n<code>--amazonec2-volume-type</code>: The Amazon EBS volume type to be attached to the instance.</li> <li>\n<code>--amazonec2-iam-instance-profile</code>: The AWS IAM role name to be used as the instance profile.</li> <li>\n<code>--amazonec2-ssh-user</code>: The SSH Login username, which must match the default SSH user set in the ami used.</li> <li>\n<code>--amazonec2-request-spot-instance</code>: Use spot instances.</li> <li>\n<code>--amazonec2-spot-price</code>: Spot instance bid price (in dollars). Require the <code>--amazonec2-request-spot-instance</code> flag.</li> <li>\n<code>--amazonec2-use-private-address</code>: Use the private IP address for docker-machine, but still create a public IP address.</li> <li>\n<code>--amazonec2-private-address-only</code>: Use the private IP address only.</li> <li>\n<code>--amazonec2-monitoring</code>: Enable CloudWatch Monitoring.</li> <li>\n<code>--amazonec2-use-ebs-optimized-instance</code>: Create an EBS Optimized Instance, instance type must support it.</li> <li>\n<code>--amazonec2-ssh-keypath</code>: Path to Private Key file to use for instance. Matching public key with .pub extension should exist</li> <li>\n<code>--amazonec2-retries</code>: Set retry count for recoverable failures (use -1 to disable)</li> </ul> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>--amazonec2-access-key</code></td> <td><code>AWS_ACCESS_KEY_ID</code></td> <td>-</td> </tr> <tr> <td><code>--amazonec2-secret-key</code></td> <td><code>AWS_SECRET_ACCESS_KEY</code></td> <td>-</td> </tr> <tr> <td><code>--amazonec2-session-token</code></td> <td><code>AWS_SESSION_TOKEN</code></td> <td>-</td> </tr> <tr> <td><code>--amazonec2-ami</code></td> <td><code>AWS_AMI</code></td> <td><code>ami-5f709f34</code></td> </tr> <tr> <td><code>--amazonec2-region</code></td> <td><code>AWS_DEFAULT_REGION</code></td> <td><code>us-east-1</code></td> </tr> <tr> <td><code>--amazonec2-vpc-id</code></td> <td><code>AWS_VPC_ID</code></td> <td>-</td> </tr> <tr> <td><code>--amazonec2-zone</code></td> <td><code>AWS_ZONE</code></td> <td><code>a</code></td> </tr> <tr> <td><code>--amazonec2-subnet-id</code></td> <td><code>AWS_SUBNET_ID</code></td> <td>-</td> </tr> <tr> <td><code>--amazonec2-security-group</code></td> <td><code>AWS_SECURITY_GROUP</code></td> <td><code>docker-machine</code></td> </tr> <tr> <td><code>--amazonec2-tags</code></td> <td><code>AWS_TAGS</code></td> <td>-</td> </tr> <tr> <td><code>--amazonec2-instance-type</code></td> <td><code>AWS_INSTANCE_TYPE</code></td> <td><code>t2.micro</code></td> </tr> <tr> <td><code>--amazonec2-device-name</code></td> <td><code>AWS_DEVICE_NAME</code></td> <td><code>/dev/sda1</code></td> </tr> <tr> <td><code>--amazonec2-root-size</code></td> <td><code>AWS_ROOT_SIZE</code></td> <td><code>16</code></td> </tr> <tr> <td><code>--amazonec2-volume-type</code></td> <td><code>AWS_VOLUME_TYPE</code></td> <td><code>gp2</code></td> </tr> <tr> <td><code>--amazonec2-iam-instance-profile</code></td> <td><code>AWS_INSTANCE_PROFILE</code></td> <td>-</td> </tr> <tr> <td><code>--amazonec2-ssh-user</code></td> <td><code>AWS_SSH_USER</code></td> <td><code>ubuntu</code></td> </tr> <tr> <td><code>--amazonec2-request-spot-instance</code></td> <td>-</td> <td><code>false</code></td> </tr> <tr> <td><code>--amazonec2-spot-price</code></td> <td>-</td> <td><code>0.50</code></td> </tr> <tr> <td><code>--amazonec2-use-private-address</code></td> <td>-</td> <td><code>false</code></td> </tr> <tr> <td><code>--amazonec2-private-address-only</code></td> <td>-</td> <td><code>false</code></td> </tr> <tr> <td><code>--amazonec2-monitoring</code></td> <td>-</td> <td><code>false</code></td> </tr> <tr> <td><code>--amazonec2-use-ebs-optimized-instance</code></td> <td>-</td> <td><code>false</code></td> </tr> <tr> <td><code>--amazonec2-ssh-keypath</code></td> <td><code>AWS_SSH_KEYPATH</code></td> <td>-</td> </tr> <tr> <td><code>--amazonec2-retries</code></td> <td>-</td> <td><code>5</code></td> </tr> </tbody> </table> <h2 id=\"default-amis\">Default AMIs</h2> <p>By default, the Amazon EC2 driver will use a daily image of Ubuntu 15.10 LTS.</p> <table> <thead> <tr> <th>Region</th> <th>AMI ID</th> </tr> </thead> <tbody> <tr> <td>ap-northeast-1</td> <td>ami-b36d4edd</td> </tr> <tr> <td>ap-southeast-1</td> <td>ami-1069af73</td> </tr> <tr> <td>ap-southeast-2</td> <td>ami-1d336a7e</td> </tr> <tr> <td>cn-north-1</td> <td>ami-79eb2214</td> </tr> <tr> <td>eu-west-1</td> <td>ami-8aa67cf9</td> </tr> <tr> <td>eu-central-1</td> <td>ami-ab0210c7</td> </tr> <tr> <td>sa-east-1</td> <td>ami-185de774</td> </tr> <tr> <td>us-east-1</td> <td>ami-26d5af4c</td> </tr> <tr> <td>us-west-1</td> <td>ami-9cbcd2fc</td> </tr> <tr> <td>us-west-2</td> <td>ami-16b1a077</td> </tr> <tr> <td>us-gov-west-1</td> <td>ami-b0bad893</td> </tr> </tbody> </table> <h2 id=\"security-group\">Security Group</h2> <p>Note that a security group will be created and associated to the host. This security group will have the following ports opened inbound:</p> <ul> <li>ssh (22/tcp)</li> <li>docker (2376/tcp)</li> <li>swarm (3376/tcp), only if the node is a swarm master</li> </ul> <p>If you specify a security group yourself using the <code>--amazonec2-security-group</code> flag, the above ports will be checked and opened and the security group modified. If you want more ports to be opened, like application specific ports, use the aws console and modify the configuration manually.</p> <h2 id=\"vpc-id\">VPC ID</h2> <p>We determine your default vpc id at the start of a command. In some cases, either because your account does not have a default vpc, or you don’t want to use the default one, you can specify a vpc with the <code>--amazonec2-vpc-id</code> flag.</p> <p>To find the VPC ID:</p> <ol> <li>Login to the AWS console</li> <li>Go to <strong>Services -&gt; VPC -&gt; Your VPCs</strong>.</li> <li>Locate the VPC ID you want from the <em>VPC</em> column.</li> <li>\n<p>Go to <strong>Services -&gt; VPC -&gt; Subnets</strong>. Examine the <em>Availability Zone</em> column to verify that zone <code>a</code> exists and matches your VPC ID.</p> <p>For example, <code>us-east1-a</code> is in the <code>a</code> availability zone. If the <code>a</code> zone is not present, you can create a new subnet in that zone or specify a different zone when you create the machine.</p>\n</li> </ol> <p>To create a machine with a non-default vpc-id:</p> <pre>$ docker-machine create --driver amazonec2 --amazonec2-access-key AKI******* --amazonec2-secret-key 8T93C********* --amazonec2-vpc-id vpc-****** aws02\n</pre> <p>This example assumes the VPC ID was found in the <code>a</code> availability zone. Use the<code>--amazonec2-zone</code> flag to specify a zone other than the <code>a</code> zone. For example, <code>--amazonec2-zone c</code> signifies <code>us-east1-c</code>.</p> <h2 id=\"vpc-connectivity\">VPC Connectivity</h2> <p>Machine uses SSH to complete the set up of instances in EC2 and requires the ability to access the instance directly.</p> <p>If you use the flag <code>--amazonec2-private-address-only</code>, you will need to ensure that you have some method of accessing the new instance from within the internal network of the VPC (e.g. a corporate VPN to the VPC, a VPN instance inside the VPC or using Docker-machine from an instance within your VPC).</p> <p>Configuration of VPCs is beyond the scope of this guide, however the first step in troubleshooting is ensuring if you are using private subnets that you follow the design guidance in the <a href=\"http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\">AWS VPC User Guide</a> and have some form of NAT available so that the set up process can access the internet to complete set up.</p> <h2 id=\"custom-ami-and-ssh-username\">Custom AMI and SSH username</h2> <p>The default SSH username for the default AMIs is <code>ubuntu</code>.</p> <p>You need to change the SSH username only if the custom AMI you use has a different SSH username.</p> <p>You can change the SSH username with the <code>--amazonec2-ssh-user</code> according to the AMI you selected with the <code>--amazonec2-ami</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/aws/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/aws/</a>\n  </p>\n</div>\n","machine/drivers/soft-layer/index":"<h1 id=\"ibm-softlayer\">IBM Softlayer</h1> <p>Create machines on <a href=\"http://softlayer.com\">Softlayer</a>.</p> <p>You need to generate an API key in the softlayer control panel. <a href=\"http://knowledgelayer.softlayer.com/procedure/retrieve-your-api-key\">Retrieve your API key</a></p> <pre>$ docker-machine create --driver softlayer --softlayer-user=user --softlayer-api-key=KEY --softlayer-domain=domain vm\n</pre> <p>Options:</p> <ul> <li>\n<code>--softlayer-memory</code>: Memory for host in MB.</li> <li>\n<code>--softlayer-disk-size</code>: A value of <code>0</code> will set the SoftLayer default.</li> <li>\n<code>--softlayer-user</code>: <strong>required</strong> Username for your SoftLayer account, api key needs to match this user.</li> <li>\n<code>--softlayer-api-key</code>: <strong>required</strong> API key for your user account.</li> <li>\n<code>--softlayer-region</code>: SoftLayer region.</li> <li>\n<code>--softlayer-cpu</code>: Number of CPUs for the machine.</li> <li>\n<code>--softlayer-hostname</code>: Hostname for the machine.</li> <li>\n<code>--softlayer-domain</code>: <strong>required</strong> Domain name for the machine.</li> <li>\n<code>--softlayer-api-endpoint</code>: Change SoftLayer API endpoint.</li> <li>\n<code>--softlayer-hourly-billing</code>: Specifies that hourly billing should be used, otherwise monthly billing is used.</li> <li>\n<code>--softlayer-local-disk</code>: Use local machine disk instead of SoftLayer SAN.</li> <li>\n<code>--softlayer-private-net-only</code>: Disable public networking.</li> <li>\n<code>--softlayer-image</code>: OS Image to use.</li> <li>\n<code>--softlayer-public-vlan-id</code>: Your public VLAN ID.</li> <li>\n<code>--softlayer-private-vlan-id</code>: Your private VLAN ID.</li> </ul> <p>The SoftLayer driver will use <code>UBUNTU_LATEST</code> as the image type by default.</p> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>--softlayer-memory</code></td> <td><code>SOFTLAYER_MEMORY</code></td> <td><code>1024</code></td> </tr> <tr> <td><code>--softlayer-disk-size</code></td> <td><code>SOFTLAYER_DISK_SIZE</code></td> <td><code>0</code></td> </tr> <tr> <td><strong><code>--softlayer-user</code></strong></td> <td><code>SOFTLAYER_USER</code></td> <td>-</td> </tr> <tr> <td><strong><code>--softlayer-api-key</code></strong></td> <td><code>SOFTLAYER_API_KEY</code></td> <td>-</td> </tr> <tr> <td><code>--softlayer-region</code></td> <td><code>SOFTLAYER_REGION</code></td> <td><code>dal01</code></td> </tr> <tr> <td><code>--softlayer-cpu</code></td> <td><code>SOFTLAYER_CPU</code></td> <td><code>1</code></td> </tr> <tr> <td><code>--softlayer-hostname</code></td> <td><code>SOFTLAYER_HOSTNAME</code></td> <td><code>docker</code></td> </tr> <tr> <td><strong><code>--softlayer-domain</code></strong></td> <td><code>SOFTLAYER_DOMAIN</code></td> <td>-</td> </tr> <tr> <td><code>--softlayer-api-endpoint</code></td> <td><code>SOFTLAYER_API_ENDPOINT</code></td> <td><code>api.softlayer.com/rest/v3</code></td> </tr> <tr> <td><code>--softlayer-hourly-billing</code></td> <td><code>SOFTLAYER_HOURLY_BILLING</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--softlayer-local-disk</code></td> <td><code>SOFTLAYER_LOCAL_DISK</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--softlayer-private-net-only</code></td> <td><code>SOFTLAYER_PRIVATE_NET</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--softlayer-image</code></td> <td><code>SOFTLAYER_IMAGE</code></td> <td><code>UBUNTU_LATEST</code></td> </tr> <tr> <td><code>--softlayer-public-vlan-id</code></td> <td><code>SOFTLAYER_PUBLIC_VLAN_ID</code></td> <td><code>0</code></td> </tr> <tr> <td><code>--softlayer-private-vlan-id</code></td> <td><code>SOFTLAYER_PRIVATE_VLAN_ID</code></td> <td><code>0</code></td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/soft-layer/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/soft-layer/</a>\n  </p>\n</div>\n","machine/drivers/hyper-v/index":"<h1 id=\"microsoft-hyper-v\">Microsoft Hyper-V</h1> <p>Creates a Boot2Docker virtual machine locally on your Windows machine using Hyper-V. <a href=\"http://windows.microsoft.com/en-us/windows-8/hyper-v-run-virtual-machines\">See here</a> for instructions to enable Hyper-V. You will need to use an Administrator level account to create and manage Hyper-V machines.</p> <blockquote> <p><strong>Note</strong>: You will need an existing virtual switch to use the driver. Hyper-V can share an external network interface (aka bridging), see <a href=\"http://blogs.technet.com/b/canitpro/archive/2014/03/11/step-by-step-enabling-hyper-v-for-use-on-windows-8-1.aspx\">this blog</a>. If you would like to use NAT, create an internal network, and use <a href=\"http://www.packet6.com/allowing-windows-8-1-hyper-v-vm-to-work-with-wifi/\">Internet Connection Sharing</a>.</p> </blockquote> <pre>$ docker-machine create --driver hyperv vm\n</pre> <p>Options:</p> <ul> <li>\n<code>--hyperv-boot2docker-url</code>: The URL of the boot2docker ISO.</li> <li>\n<code>--hyperv-virtual-switch</code>: Name of the virtual switch to use.</li> <li>\n<code>--hyperv-disk-size</code>: Size of disk for the host in MB.</li> <li>\n<code>--hyperv-memory</code>: Size of memory for the host in MB.</li> <li>\n<code>--hyperv-cpu-count</code>: Number of CPUs for the host.</li> <li>\n<code>--hyperv-static-macaddress</code>: Hyper-V network adapter’s static MAC address.</li> <li>\n<code>--hyperv-vlan-id</code>: Hyper-V network adapter’s VLAN ID if any.</li> </ul> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>--hyperv-boot2docker-url</code></td> <td><code>HYPERV_BOOT2DOCKER_URL</code></td> <td><em>Latest boot2docker url</em></td> </tr> <tr> <td><code>--hyperv-virtual-switch</code></td> <td><code>HYPERV_VIRTUAL_SWITCH</code></td> <td><em>first found</em></td> </tr> <tr> <td><code>--hyperv-disk-size</code></td> <td><code>HYPERV_DISK_SIZE</code></td> <td><code>20000</code></td> </tr> <tr> <td><code>--hyperv-memory</code></td> <td><code>HYPERV_MEMORY</code></td> <td><code>1024</code></td> </tr> <tr> <td><code>--hyperv-cpu-count</code></td> <td><code>HYPERV_CPU_COUNT</code></td> <td><code>1</code></td> </tr> <tr> <td><code>--hyperv-static-macaddress</code></td> <td><code>HYPERV_STATIC_MACADDRESS</code></td> <td><em>undefined</em></td> </tr> <tr> <td><code>--hyperv-cpu-count</code></td> <td><code>HYPERV_VLAN_ID</code></td> <td><em>undefined</em></td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/hyper-v/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/hyper-v/</a>\n  </p>\n</div>\n","machine/drivers/openstack/index":"<h1 id=\"openstack\">OpenStack</h1> <p>Create machines on <a href=\"http://www.openstack.org/software/\">OpenStack</a></p> <p>Mandatory:</p> <ul> <li>\n<code>--openstack-auth-url</code>: Keystone service base URL.</li> <li>\n<code>--openstack-flavor-id</code> or <code>--openstack-flavor-name</code>: Identify the flavor that will be used for the machine.</li> <li>\n<p><code>--openstack-image-id</code> or <code>--openstack-image-name</code>: Identify the image that will be used for the machine.</p> <p>$ docker-machine create --driver openstack vm</p>\n</li> </ul> <p>Options:</p> <ul> <li>\n<code>--openstack-active-timeout</code>: The timeout in seconds until the OpenStack instance must be active.</li> <li>\n<code>--openstack-availability-zone</code>: The availability zone in which to launch the server.</li> <li>\n<code>--openstack-domain-name</code> or <code>--openstack-domain-id</code>: Domain to use for authentication (Keystone v3 only).</li> <li>\n<code>--openstack-endpoint-type</code>: Endpoint type can be <code>internalURL</code>, <code>adminURL</code> on <code>publicURL</code>. If is a helper for the driver to choose the right URL in the OpenStack service catalog. If not provided the default id <code>publicURL</code>\n</li> <li>\n<code>--openstack-floatingip-pool</code>: The IP pool that will be used to get a public IP can assign it to the machine. If there is an IP address already allocated but not assigned to any machine, this IP will be chosen and assigned to the machine. If there is no IP address already allocated a new IP will be allocated and assigned to the machine.</li> <li>\n<code>--openstack-keypair-name</code>: Specify the existing Nova keypair to use.</li> <li>\n<code>--openstack-insecure</code>: Explicitly allow openstack driver to perform “insecure” SSL (https) requests. The server’s certificate will not be verified against any certificate authorities. This option should be used with caution.</li> <li>\n<code>--openstack-ip-version</code>: If the instance has both IPv4 and IPv6 address, you can select IP version. If not provided <code>4</code> will be used.</li> <li>\n<code>--openstack-net-name</code> or <code>--openstack-net-id</code>: Identify the private network the machine will be connected on. If your OpenStack project project contains only one private network it will be use automatically.</li> <li>\n<code>--openstack-password</code>: User password. It can be omitted if the standard environment variable <code>OS_PASSWORD</code> is set.</li> <li>\n<code>--openstack-private-key-file</code>: Used with <code>--openstack-keypair-name</code>, associates the private key to the keypair.</li> <li>\n<code>--openstack-region</code>: The region to work on. Can be omitted if there is only one region on the OpenStack.</li> <li>\n<code>--openstack-sec-groups</code>: If security groups are available on your OpenStack you can specify a comma separated list to use for the machine (e.g. <code>secgrp001,secgrp002</code>).</li> <li>\n<code>--openstack-username</code>: User identifier to authenticate with.</li> <li>\n<code>--openstack-ssh-port</code>: Customize the SSH port if the SSH server on the machine does not listen on the default port.</li> <li>\n<code>--openstack-ssh-user</code>: The username to use for SSH into the machine. If not provided <code>root</code> will be used.</li> <li>\n<code>--openstack-tenant-name</code> or <code>--openstack-tenant-id</code>: Identify the tenant in which the machine will be created.</li> </ul> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>--openstack-active-timeout</code></td> <td><code>OS_ACTIVE_TIMEOUT</code></td> <td><code>200</code></td> </tr> <tr> <td><code>--openstack-auth-url</code></td> <td><code>OS_AUTH_URL</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-availability-zone</code></td> <td><code>OS_AVAILABILITY_ZONE</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-domain-id</code></td> <td><code>OS_DOMAIN_ID</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-domain-name</code></td> <td><code>OS_DOMAIN_NAME</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-endpoint-type</code></td> <td><code>OS_ENDPOINT_TYPE</code></td> <td><code>publicURL</code></td> </tr> <tr> <td><code>--openstack-flavor-id</code></td> <td><code>OS_FLAVOR_ID</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-flavor-name</code></td> <td><code>OS_FLAVOR_NAME</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-floatingip-pool</code></td> <td><code>OS_FLOATINGIP_POOL</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-image-id</code></td> <td><code>OS_IMAGE_ID</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-image-name</code></td> <td><code>OS_IMAGE_NAME</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-insecure</code></td> <td><code>OS_INSECURE</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--openstack-ip-version</code></td> <td><code>OS_IP_VERSION</code></td> <td><code>4</code></td> </tr> <tr> <td><code>--openstack-keypair-name</code></td> <td><code>OS_KEYPAIR_NAME</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-net-id</code></td> <td><code>OS_NETWORK_ID</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-net-name</code></td> <td><code>OS_NETWORK_NAME</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-password</code></td> <td><code>OS_PASSWORD</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-private-key-file</code></td> <td><code>OS_PRIVATE_KEY_FILE</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-region</code></td> <td><code>OS_REGION_NAME</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-sec-groups</code></td> <td><code>OS_SECURITY_GROUPS</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-ssh-port</code></td> <td><code>OS_SSH_PORT</code></td> <td><code>22</code></td> </tr> <tr> <td><code>--openstack-ssh-user</code></td> <td><code>OS_SSH_USER</code></td> <td><code>root</code></td> </tr> <tr> <td><code>--openstack-tenant-id</code></td> <td><code>OS_TENANT_ID</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-tenant-name</code></td> <td><code>OS_TENANT_NAME</code></td> <td>-</td> </tr> <tr> <td><code>--openstack-username</code></td> <td><code>OS_USERNAME</code></td> <td>-</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/openstack/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/openstack/</a>\n  </p>\n</div>\n","machine/drivers/virtualbox/index":"<h1 id=\"oracle-virtualbox\">Oracle VirtualBox</h1> <p>Create machines locally using <a href=\"https://www.virtualbox.org/\">VirtualBox</a>. This driver requires VirtualBox 5+ to be installed on your host. Using VirtualBox 4.3+ should work but will give you a warning. Older versions will refuse to work.</p> <pre>$ docker-machine create --driver=virtualbox vbox-test\n</pre> <p>You can create an entirely new machine or you can convert a Boot2Docker VM into a machine by importing the VM. To convert a Boot2Docker VM, you’d use the following command:</p> <pre>$ docker-machine create -d virtualbox --virtualbox-import-boot2docker-vm boot2docker-vm b2d\n</pre> <p>The size of the VM’s disk can be configured this way:</p> <pre>$ docker-machine create -d virtualbox --virtualbox-disk-size \"100000\" large\n</pre> <p>Options:</p> <ul> <li>\n<code>--virtualbox-memory</code>: Size of memory for the host in MB.</li> <li>\n<code>--virtualbox-cpu-count</code>: Number of CPUs to use to create the VM. Defaults to single CPU.</li> <li>\n<code>--virtualbox-disk-size</code>: Size of disk for the host in MB.</li> <li>\n<code>--virtualbox-host-dns-resolver</code>: Use the host DNS resolver. (Boolean value, defaults to false)</li> <li>\n<code>--virtualbox-boot2docker-url</code>: The URL of the boot2docker image. Defaults to the latest available version.</li> <li>\n<code>--virtualbox-import-boot2docker-vm</code>: The name of a Boot2Docker VM to import.</li> <li>\n<code>--virtualbox-hostonly-cidr</code>: The CIDR of the host only adapter.</li> <li>\n<code>--virtualbox-hostonly-nictype</code>: Host Only Network Adapter Type. Possible values are are ‘82540EM’ (Intel PRO/1000), ‘Am79C973’ (PCnet-FAST III) and ‘virtio’ Paravirtualized network adapter.</li> <li>\n<code>--virtualbox-hostonly-nicpromisc</code>: Host Only Network Adapter Promiscuous Mode. Possible options are deny , allow-vms, allow-all</li> <li>\n<code>--virtualbox-no-share</code>: Disable the mount of your home directory</li> <li>\n<code>--virtualbox-no-dns-proxy</code>: Disable proxying all DNS requests to the host (Boolean value, default to false)</li> <li>\n<code>--virtualbox-no-vtx-check</code>: Disable checking for the availability of hardware virtualization before the vm is started</li> </ul> <p>The <code>--virtualbox-boot2docker-url</code> flag takes a few different forms. By default, if no value is specified for this flag, Machine will check locally for a boot2docker ISO. If one is found, that will be used as the ISO for the created machine. If one is not found, the latest ISO release available on <a href=\"https://github.com/boot2docker/boot2docker\">boot2docker/boot2docker</a> will be downloaded and stored locally for future use. Note that this means you must run <code>docker-machine upgrade</code> deliberately on a machine if you wish to update the “cached” boot2docker ISO.</p> <p>This is the default behavior (when <code>--virtualbox-boot2docker-url=\"\"</code>), but the option also supports specifying ISOs by the <code>http://</code> and <code>file://</code> protocols. <code>file://</code> will look at the path specified locally to locate the ISO: for instance, you could specify <code>--virtualbox-boot2docker-url\nfile://$HOME/Downloads/rc.iso</code> to test out a release candidate ISO that you have downloaded already. You could also just get an ISO straight from the Internet using the <code>http://</code> form.</p> <p>To customize the host only adapter, you can use the <code>--virtualbox-hostonly-cidr</code> flag. This will specify the host IP and Machine will calculate the VirtualBox DHCP server address (a random IP on the subnet between <code>.1</code> and <code>.25</code>) so it does not clash with the specified host IP. Machine will also specify the DHCP lower bound to <code>.100</code> and the upper bound to <code>.254</code>. For example, a specified CIDR of <code>192.168.24.1/24</code> would have a DHCP server between <code>192.168.24.2-25</code>, a lower bound of <code>192.168.24.100</code> and upper bound of <code>192.168.24.254</code>.</p> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>--virtualbox-memory</code></td> <td><code>VIRTUALBOX_MEMORY_SIZE</code></td> <td><code>1024</code></td> </tr> <tr> <td><code>--virtualbox-cpu-count</code></td> <td><code>VIRTUALBOX_CPU_COUNT</code></td> <td><code>1</code></td> </tr> <tr> <td><code>--virtualbox-disk-size</code></td> <td><code>VIRTUALBOX_DISK_SIZE</code></td> <td><code>20000</code></td> </tr> <tr> <td><code>--virtualbox-host-dns-resolver</code></td> <td><code>VIRTUALBOX_HOST_DNS_RESOLVER</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--virtualbox-boot2docker-url</code></td> <td><code>VIRTUALBOX_BOOT2DOCKER_URL</code></td> <td><em>Latest boot2docker url</em></td> </tr> <tr> <td><code>--virtualbox-import-boot2docker-vm</code></td> <td><code>VIRTUALBOX_BOOT2DOCKER_IMPORT_VM</code></td> <td><code>boot2docker-vm</code></td> </tr> <tr> <td><code>--virtualbox-hostonly-cidr</code></td> <td><code>VIRTUALBOX_HOSTONLY_CIDR</code></td> <td><code>192.168.99.1/24</code></td> </tr> <tr> <td><code>--virtualbox-hostonly-nictype</code></td> <td><code>VIRTUALBOX_HOSTONLY_NIC_TYPE</code></td> <td><code>82540EM</code></td> </tr> <tr> <td><code>--virtualbox-hostonly-nicpromisc</code></td> <td><code>VIRTUALBOX_HOSTONLY_NIC_PROMISC</code></td> <td><code>deny</code></td> </tr> <tr> <td><code>--virtualbox-no-share</code></td> <td><code>VIRTUALBOX_NO_SHARE</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--virtualbox-no-dns-proxy</code></td> <td><code>VIRTUALBOX_NO_DNS_PROXY</code></td> <td><code>false</code></td> </tr> <tr> <td><code>--virtualbox-no-vtx-check</code></td> <td><code>VIRTUALBOX_NO_VTX_CHECK</code></td> <td><code>false</code></td> </tr> </tbody> </table> <h2 id=\"known-issues\">Known Issues</h2> <p>Vboxfs suffers from a <a href=\"https://www.virtualbox.org/ticket/9069\">longstanding bug</a> causing <a href=\"http://linux.die.net/man/2/sendfile\">sendfile(2)</a> to serve cached file contents.</p> <p>This will often cause problems when using a web server such as nginx to serve static files from a shared volume. For development environments, a good workaround is to disable sendfile in your server configuration.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/virtualbox/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/virtualbox/</a>\n  </p>\n</div>\n","machine/drivers/rackspace/index":"<h1 id=\"rackspace\">Rackspace</h1> <p>Create machines on <a href=\"http://www.rackspace.com/cloud\">Rackspace cloud</a></p> <pre>$ docker-machine create --driver rackspace --rackspace-username=user --rackspace-api-key=KEY --rackspace-region=region vm\n</pre> <p>Options:</p> <ul> <li>\n<code>--rackspace-username</code>: <strong>required</strong> Rackspace account username.</li> <li>\n<code>--rackspace-api-key</code>: <strong>required</strong> Rackspace API key.</li> <li>\n<code>--rackspace-region</code>: <strong>required</strong> Rackspace region name.</li> <li>\n<code>--rackspace-endpoint-type</code>: Rackspace endpoint type (<code>adminURL</code>, <code>internalURL</code> or the default <code>publicURL</code>).</li> <li>\n<code>--rackspace-image-id</code>: Rackspace image ID. Default: Ubuntu 15.10 (Wily Werewolf) (PVHVM).</li> <li>\n<code>--rackspace-flavor-id</code>: Rackspace flavor ID. Default: General Purpose 1GB.</li> <li>\n<code>--rackspace-ssh-user</code>: SSH user for the newly booted machine.</li> <li>\n<code>--rackspace-ssh-port</code>: SSH port for the newly booted machine.</li> <li>\n<code>--rackspace-docker-install</code>: Set if Docker has to be installed on the machine.</li> </ul> <p>The Rackspace driver will use <code>59a3fadd-93e7-4674-886a-64883e17115f</code> (Ubuntu 15.10) by default.</p> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><strong><code>--rackspace-username</code></strong></td> <td><code>OS_USERNAME</code></td> <td>-</td> </tr> <tr> <td><strong><code>--rackspace-api-key</code></strong></td> <td><code>OS_API_KEY</code></td> <td>-</td> </tr> <tr> <td><strong><code>--rackspace-region</code></strong></td> <td><code>OS_REGION_NAME</code></td> <td>-</td> </tr> <tr> <td><code>--rackspace-endpoint-type</code></td> <td><code>OS_ENDPOINT_TYPE</code></td> <td><code>publicURL</code></td> </tr> <tr> <td><code>--rackspace-image-id</code></td> <td>-</td> <td><code>59a3fadd-93e7-4674-886a-64883e17115f</code></td> </tr> <tr> <td><code>--rackspace-flavor-id</code></td> <td><code>OS_FLAVOR_ID</code></td> <td><code>general1-1</code></td> </tr> <tr> <td><code>--rackspace-ssh-user</code></td> <td>-</td> <td><code>root</code></td> </tr> <tr> <td><code>--rackspace-ssh-port</code></td> <td>-</td> <td><code>22</code></td> </tr> <tr> <td><code>--rackspace-docker-install</code></td> <td>-</td> <td><code>true</code></td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/rackspace/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/rackspace/</a>\n  </p>\n</div>\n","machine/drivers/vm-cloud/index":"<h1 id=\"vmware-vcloud-air\">VMware vCloud Air</h1> <p>Creates machines on <a href=\"http://vcloud.vmware.com\">vCloud Air</a> subscription service. You need an account within an existing subscription of vCloud Air VPC or Dedicated Cloud.</p> <pre>$ docker-machine create --driver vmwarevcloudair --vmwarevcloudair-username=user --vmwarevcloudair-password=SECRET vm\n</pre> <p>Options:</p> <ul> <li>\n<code>--vmwarevcloudair-username</code>: <strong>required</strong> vCloud Air Username.</li> <li>\n<code>--vmwarevcloudair-password</code>: <strong>required</strong> vCloud Air Password.</li> <li>\n<code>--vmwarevcloudair-computeid</code>: Compute ID (if using Dedicated Cloud).</li> <li>\n<code>--vmwarevcloudair-vdcid</code>: Virtual Data Center ID.</li> <li>\n<code>--vmwarevcloudair-orgvdcnetwork</code>: Organization VDC Network to attach.</li> <li>\n<code>--vmwarevcloudair-edgegateway</code>: Organization Edge Gateway.</li> <li>\n<code>--vmwarevcloudair-publicip</code>: Org Public IP to use.</li> <li>\n<code>--vmwarevcloudair-catalog</code>: Catalog.</li> <li>\n<code>--vmwarevcloudair-catalogitem</code>: Catalog Item.</li> <li>\n<code>--vmwarevcloudair-provision</code>: Install Docker binaries.</li> <li>\n<code>--vmwarevcloudair-cpu-count</code>: VM CPU Count.</li> <li>\n<code>--vmwarevcloudair-memory-size</code>: VM Memory Size in MB.</li> <li>\n<code>--vmwarevcloudair-ssh-port</code>: SSH port.</li> <li>\n<code>--vmwarevcloudair-docker-port</code>: Docker port.</li> </ul> <p>The VMware vCloud Air driver will use the <code>Ubuntu Server 12.04 LTS (amd64 20140927)</code> image by default.</p> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><strong><code>--vmwarevcloudair-username</code></strong></td> <td><code>VCLOUDAIR_USERNAME</code></td> <td>-</td> </tr> <tr> <td><strong><code>--vmwarevcloudair-password</code></strong></td> <td><code>VCLOUDAIR_PASSWORD</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevcloudair-computeid</code></td> <td><code>VCLOUDAIR_COMPUTEID</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevcloudair-vdcid</code></td> <td><code>VCLOUDAIR_VDCID</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevcloudair-orgvdcnetwork</code></td> <td><code>VCLOUDAIR_ORGVDCNETWORK</code></td> <td><code>&lt;vdcid&gt;-default-routed</code></td> </tr> <tr> <td><code>--vmwarevcloudair-edgegateway</code></td> <td><code>VCLOUDAIR_EDGEGATEWAY</code></td> <td><code>&lt;vdcid&gt;</code></td> </tr> <tr> <td><code>--vmwarevcloudair-publicip</code></td> <td><code>VCLOUDAIR_PUBLICIP</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevcloudair-catalog</code></td> <td><code>VCLOUDAIR_CATALOG</code></td> <td><code>Public Catalog</code></td> </tr> <tr> <td><code>--vmwarevcloudair-catalogitem</code></td> <td><code>VCLOUDAIR_CATALOGITEM</code></td> <td><code>Ubuntu Server 12.04 LTS (amd64 20140927)</code></td> </tr> <tr> <td><code>--vmwarevcloudair-provision</code></td> <td><code>VCLOUDAIR_PROVISION</code></td> <td><code>true</code></td> </tr> <tr> <td><code>--vmwarevcloudair-cpu-count</code></td> <td><code>VCLOUDAIR_CPU_COUNT</code></td> <td><code>1</code></td> </tr> <tr> <td><code>--vmwarevcloudair-memory-size</code></td> <td><code>VCLOUDAIR_MEMORY_SIZE</code></td> <td><code>2048</code></td> </tr> <tr> <td><code>--vmwarevcloudair-ssh-port</code></td> <td><code>VCLOUDAIR_SSH_PORT</code></td> <td><code>22</code></td> </tr> <tr> <td><code>--vmwarevcloudair-docker-port</code></td> <td><code>VCLOUDAIR_DOCKER_PORT</code></td> <td><code>2376</code></td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/vm-cloud/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/vm-cloud/</a>\n  </p>\n</div>\n","machine/drivers/vm-fusion/index":"<h1 id=\"vmware-fusion\">VMware Fusion</h1> <p>Creates machines locally on <a href=\"http://www.vmware.com/products/fusion\">VMware Fusion</a>. Requires VMware Fusion to be installed.</p> <pre>$ docker-machine create --driver vmwarefusion vm\n</pre> <p>Options:</p> <ul> <li>\n<code>--vmwarefusion-boot2docker-url</code>: URL for boot2docker image.</li> <li>\n<code>--vmwarefusion-cpu-count</code>: Number of CPUs for the machine (-1 to use the number of CPUs available)</li> <li>\n<code>--vmwarefusion-disk-size</code>: Size of disk for host VM (in MB).</li> <li>\n<code>--vmwarefusion-memory-size</code>: Size of memory for host VM (in MB).</li> <li>\n<code>--vmwarefusion-no-share</code>: Disable the mount of your home directory.</li> </ul> <p>The VMware Fusion driver uses the latest boot2docker image. See <a href=\"https://github.com/frapposelli/boot2docker/tree/vmware-64bit\">frapposelli/boot2docker</a></p> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>--vmwarefusion-boot2docker-url</code></td> <td><code>FUSION_BOOT2DOCKER_URL</code></td> <td><em>Latest boot2docker url</em></td> </tr> <tr> <td><code>--vmwarefusion-cpu-count</code></td> <td><code>FUSION_CPU_COUNT</code></td> <td><code>1</code></td> </tr> <tr> <td><code>--vmwarefusion-disk-size</code></td> <td><code>FUSION_DISK_SIZE</code></td> <td><code>20000</code></td> </tr> <tr> <td><code>--vmwarefusion-memory-size</code></td> <td><code>FUSION_MEMORY_SIZE</code></td> <td><code>1024</code></td> </tr> <tr> <td><code>--vmwarefusion-no-share</code></td> <td><code>FUSION_NO_SHARE</code></td> <td><code>false</code></td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/vm-fusion/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/vm-fusion/</a>\n  </p>\n</div>\n","machine/drivers/vsphere/index":"<h1 id=\"vmware-vsphere\">VMware vSphere</h1> <p>Creates machines on a <a href=\"http://www.vmware.com/products/vsphere\">VMware vSphere</a> Virtual Infrastructure. The machine must have a working vSphere ESXi installation. You can use a paid license or free 60 day trial license. Your installation may also include an optional VCenter server. $ docker-machine create --driver vmwarevsphere --vmwarevsphere-username=user --vmwarevsphere-password=SECRET vm</p> <p>Options:</p> <ul> <li>\n<code>--vmwarevsphere-username</code>: <strong>required</strong> vSphere Username.</li> <li>\n<code>--vmwarevsphere-password</code>: <strong>required</strong> vSphere Password.</li> <li>\n<code>--vmwarevsphere-cpu-count</code>: CPU number for Docker VM.</li> <li>\n<code>--vmwarevsphere-memory-size</code>: Size of memory for Docker VM (in MB).</li> <li>\n<code>--vmwarevsphere-disk-size</code>: Size of disk for Docker VM (in MB).</li> <li>\n<code>--vmwarevsphere-boot2docker-url</code>: URL for boot2docker image.</li> <li>\n<code>--vmwarevsphere-vcenter</code>: IP/hostname for vCenter (or ESXi if connecting directly to a single host).</li> <li>\n<code>--vmwarevsphere-vcenter-port</code>: vSphere Port for vCenter.</li> <li>\n<code>--vmwarevsphere-network</code>: Network where the Docker VM will be attached.</li> <li>\n<code>--vmwarevsphere-datastore</code>: Datastore for Docker VM.</li> <li>\n<code>--vmwarevsphere-datacenter</code>: Datacenter for Docker VM (must be set to <code>ha-datacenter</code> when connecting to a single host).</li> <li>\n<code>--vmwarevsphere-pool</code>: Resource pool for Docker VM.</li> <li>\n<code>--vmwarevsphere-hostsystem</code>: vSphere compute resource where the docker VM will be instantiated (use <cluster>/* or <cluster>/<host> if using a cluster).</host></cluster></cluster>\n</li> </ul> <p>The VMware vSphere driver uses the latest boot2docker image.</p> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><strong><code>--vmwarevsphere-username</code></strong></td> <td><code>VSPHERE_USERNAME</code></td> <td>-</td> </tr> <tr> <td><strong><code>--vmwarevsphere-password</code></strong></td> <td><code>VSPHERE_PASSWORD</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevsphere-cpu-count</code></td> <td><code>VSPHERE_CPU_COUNT</code></td> <td><code>2</code></td> </tr> <tr> <td><code>--vmwarevsphere-memory-size</code></td> <td><code>VSPHERE_MEMORY_SIZE</code></td> <td><code>2048</code></td> </tr> <tr> <td><code>--vmwarevsphere-boot2docker-url</code></td> <td><code>VSPHERE_BOOT2DOCKER_URL</code></td> <td><em>Latest boot2docker url</em></td> </tr> <tr> <td><code>--vmwarevsphere-vcenter</code></td> <td><code>VSPHERE_VCENTER</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevsphere-vcenter-port</code></td> <td><code>VSPHERE_VCENTER_PORT</code></td> <td>443</td> </tr> <tr> <td><code>--vmwarevsphere-disk-size</code></td> <td><code>VSPHERE_DISK_SIZE</code></td> <td><code>20000</code></td> </tr> <tr> <td><code>--vmwarevsphere-network</code></td> <td><code>VSPHERE_NETWORK</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevsphere-datastore</code></td> <td><code>VSPHERE_DATASTORE</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevsphere-datacenter</code></td> <td><code>VSPHERE_DATACENTER</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevsphere-pool</code></td> <td><code>VSPHERE_POOL</code></td> <td>-</td> </tr> <tr> <td><code>--vmwarevsphere-hostsystem</code></td> <td><code>VSPHERE_HOSTSYSTEM</code></td> <td>-</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/vsphere/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/vsphere/</a>\n  </p>\n</div>\n","machine/drivers/exoscale/index":"<h1 id=\"exoscale\">Exoscale</h1> <p>Create machines on <a href=\"https://www.exoscale.ch/\">exoscale</a>.</p> <p>Get your API key and API secret key from <a href=\"https://portal.exoscale.ch/account/api\">API details</a> and pass them to <code>machine create</code> with the <code>--exoscale-api-key</code> and <code>--exoscale-api-secret-key</code> options.</p> <pre>$ docker-machine create --driver exoscale --exoscale-api-key=API --exoscale-api-secret-key=SECRET vm\n</pre> <p>Options:</p> <ul> <li>\n<code>--exoscale-url</code>: Your API endpoint.</li> <li>\n<code>--exoscale-api-key</code>: <strong>required</strong> Your API key.</li> <li>\n<code>--exoscale-api-secret-key</code>: <strong>required</strong> Your API secret key.</li> <li>\n<code>--exoscale-instance-profile</code>: Instance profile.</li> <li>\n<code>--exoscale-disk-size</code>: Disk size for the host in GB (10, 50, 100, 200, 400).</li> <li>\n<code>--exoscale-image</code>: Image template (eg. ubuntu-14.04, ubuntu-15.10).</li> <li>\n<code>--exoscale-security-group</code>: Security group. It will be created if it doesn’t exist.</li> <li>\n<code>--exoscale-availability-zone</code>: Exoscale availability zone.</li> <li>\n<code>--exoscale-ssh-user</code>: SSH username, which must match the default SSH user for the used image.</li> <li>\n<code>--exoscale-userdata</code>: Path to file containing user data for cloud-init.</li> </ul> <p>If a custom security group is provided, you need to ensure that you allow TCP ports 22 and 2376 in an ingress rule. Moreover, if you want to use Swarm, also add TCP port 3376.</p> <p>Environment variables and default values:</p> <table> <thead> <tr> <th>CLI option</th> <th>Environment variable</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>--exoscale-url</code></td> <td><code>EXOSCALE_ENDPOINT</code></td> <td><code>https://api.exoscale.ch/compute</code></td> </tr> <tr> <td><strong><code>--exoscale-api-key</code></strong></td> <td><code>EXOSCALE_API_KEY</code></td> <td>-</td> </tr> <tr> <td><strong><code>--exoscale-api-secret-key</code></strong></td> <td><code>EXOSCALE_API_SECRET</code></td> <td>-</td> </tr> <tr> <td><code>--exoscale-instance-profile</code></td> <td><code>EXOSCALE_INSTANCE_PROFILE</code></td> <td><code>small</code></td> </tr> <tr> <td><code>--exoscale-disk-size</code></td> <td><code>EXOSCALE_DISK_SIZE</code></td> <td><code>50</code></td> </tr> <tr> <td><code>--exoscale-image</code></td> <td><code>EXOSCALE_IMAGE</code></td> <td><code>ubuntu-15.10</code></td> </tr> <tr> <td><code>--exoscale-security-group</code></td> <td><code>EXOSCALE_SECURITY_GROUP</code></td> <td><code>docker-machine</code></td> </tr> <tr> <td><code>--exoscale-availability-zone</code></td> <td><code>EXOSCALE_AVAILABILITY_ZONE</code></td> <td><code>ch-gva-2</code></td> </tr> <tr> <td><code>--exoscale-ssh-user</code></td> <td><code>EXOSCALE_SSH_USER</code></td> <td><code>ubuntu</code></td> </tr> <tr> <td><code>--exoscale-userdata</code></td> <td><code>EXOSCALE_USERDATA</code></td> <td>-</td> </tr> </tbody> </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/exoscale/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/exoscale/</a>\n  </p>\n</div>\n","machine/completion/index":"<h1 id=\"command-line-completion\">Command-line Completion</h1> <p>Docker Machine comes with <a href=\"http://en.wikipedia.org/wiki/Command-line_completion\">command completion</a> for the bash shell.</p> <h2 id=\"installing-command-completion\">Installing Command Completion</h2> <h3 id=\"bash\">Bash</h3> <p>Make sure bash completion is installed. If you use a current Linux in a non-minimal installation, bash completion should be available. On a Mac, install with <code>brew install bash-completion</code></p> <p>Place the completion scripts in <code>/etc/bash_completion.d/</code> (<code>`brew --prefix`/etc/bash_completion.d/</code> on a Mac), using e.g.</p> <pre>files=(docker-machine docker-machine-wrapper docker-machine-prompt)\nfor f in \"${files[@]}\"; do\n  curl -L https://raw.githubusercontent.com/docker/machine/v$(docker-machine --version | tr -ds ',' ' ' | awk 'NR==1{print $(3)}')/contrib/completion/bash/$f.bash &gt; `brew --prefix`/etc/bash_completion.d/$f\ndone\n</pre> <p>Completion will be available upon next login.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/completion/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/completion/</a>\n  </p>\n</div>\n","engine/reference/glossary/index":"<h1 id=\"glossary\">Glossary</h1> <p>A list of terms used around the Docker project.</p> <h2 id=\"aufs\">aufs</h2> <p>aufs (advanced multi layered unification filesystem) is a Linux <a href=\"#filesystem\">filesystem</a> that Docker supports as a storage backend. It implements the <a href=\"http://en.wikipedia.org/wiki/Union_mount\">union mount</a> for Linux file systems.</p> <h2 id=\"base-image\">Base image</h2> <p>An image that has no parent is a <strong>base image</strong>.</p> <h2 id=\"boot2docker\">boot2docker</h2> <p><a href=\"http://boot2docker.io/\">boot2docker</a> is a lightweight Linux distribution made specifically to run Docker containers. The boot2docker management tool for Mac and Windows was deprecated and replaced by <a href=\"#machine\"><code>docker-machine</code></a> which you can install with the Docker Toolbox.</p> <h2 id=\"btrfs\">btrfs</h2> <p>btrfs (B-tree file system) is a Linux <a href=\"#filesystem\">filesystem</a> that Docker supports as a storage backend. It is a <a href=\"http://en.wikipedia.org/wiki/Copy-on-write\">copy-on-write</a> filesystem.</p> <h2 id=\"build\">build</h2> <p>build is the process of building Docker images using a <a href=\"#dockerfile\">Dockerfile</a>. The build uses a Dockerfile and a “context”. The context is the set of files in the directory in which the image is built.</p> <h2 id=\"cgroups\">cgroups</h2> <p>cgroups is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes. Docker relies on cgroups to control and isolate resource limits.</p> <p><em>Also known as : control groups</em></p> <h2 id=\"compose\">Compose</h2> <p><a href=\"https://github.com/docker/compose\">Compose</a> is a tool for defining and running complex applications with Docker. With compose, you define a multi-container application in a single file, then spin your application up in a single command which does everything that needs to be done to get it running.</p> <p><em>Also known as : docker-compose, fig</em></p> <h2 id=\"container\">container</h2> <p>A container is a runtime instance of a <a href=\"#image\">docker image</a>.</p> <p>A Docker container consists of</p> <ul> <li>A Docker image</li> <li>Execution environment</li> <li>A standard set of instructions</li> </ul> <p>The concept is borrowed from Shipping Containers, which define a standard to ship goods globally. Docker defines a standard to ship software.</p> <h2 id=\"data-volume\">data volume</h2> <p>A data volume is a specially-designated directory within one or more containers that bypasses the Union File System. Data volumes are designed to persist data, independent of the container’s life cycle. Docker therefore never automatically delete volumes when you remove a container, nor will it “garbage collect” volumes that are no longer referenced by a container.</p> <h2 id=\"docker\">Docker</h2> <p>The term Docker can refer to</p> <ul> <li>The Docker project as a whole, which is a platform for developers and sysadmins to develop, ship, and run applications</li> <li>The docker daemon process running on the host which manages images and containers</li> </ul> <h2 id=\"docker-hub\">Docker Hub</h2> <p>The <a href=\"https://hub.docker.com/\">Docker Hub</a> is a centralized resource for working with Docker and its components. It provides the following services:</p> <ul> <li>Docker image hosting</li> <li>User authentication</li> <li>Automated image builds and work-flow tools such as build triggers and web hooks</li> <li>Integration with GitHub and Bitbucket</li> </ul> <h2 id=\"dockerfile\">Dockerfile</h2> <p>A Dockerfile is a text document that contains all the commands you would normally execute manually in order to build a Docker image. Docker can build images automatically by reading the instructions from a Dockerfile.</p> <h2 id=\"filesystem\">filesystem</h2> <p>A file system is the method an operating system uses to name files and assign them locations for efficient storage and retrieval.</p> <p>Examples :</p> <ul> <li>Linux : ext4, aufs, btrfs, zfs</li> <li>Windows : NTFS</li> <li>OS X : HFS+</li> </ul> <h2 id=\"image\">image</h2> <p>Docker images are the basis of <a href=\"#container\">containers</a>. An Image is an ordered collection of root filesystem changes and the corresponding execution parameters for use within a container runtime. An image typically contains a union of layered filesystems stacked on top of each other. An image does not have state and it never changes.</p> <h2 id=\"libcontainer\">libcontainer</h2> <p>libcontainer provides a native Go implementation for creating containers with namespaces, cgroups, capabilities, and filesystem access controls. It allows you to manage the lifecycle of the container performing additional operations after the container is created.</p> <h2 id=\"libnetwork\">libnetwork</h2> <p>libnetwork provides a native Go implementation for creating and managing container network namespaces and other network resources. It manage the networking lifecycle of the container performing additional operations after the container is created.</p> <h2 id=\"link\">link</h2> <p>links provide a legacy interface to connect Docker containers running on the same host to each other without exposing the hosts’ network ports. Use the Docker networks feature instead.</p> <h2 id=\"machine\">Machine</h2> <p><a href=\"https://github.com/docker/machine\">Machine</a> is a Docker tool which makes it really easy to create Docker hosts on your computer, on cloud providers and inside your own data center. It creates servers, installs Docker on them, then configures the Docker client to talk to them.</p> <p><em>Also known as : docker-machine</em></p> <h2 id=\"overlay-network-driver\">overlay network driver</h2> <p>Overlay network driver provides out of the box multi-host network connectivity for docker containers in a cluster.</p> <h2 id=\"overlay-storage-driver\">overlay storage driver</h2> <p>OverlayFS is a <a href=\"#filesystem\">filesystem</a> service for Linux which implements a <a href=\"http://en.wikipedia.org/wiki/Union_mount\">union mount</a> for other file systems. It is supported by the Docker daemon as a storage driver.</p> <h2 id=\"registry\">registry</h2> <p>A Registry is a hosted service containing <a href=\"#repository\">repositories</a> of <a href=\"#image\">images</a> which responds to the Registry API.</p> <p>The default registry can be accessed using a browser at <a href=\"#docker-hub\">Docker Hub</a> or using the <code>docker search</code> command.</p> <h2 id=\"repository\">repository</h2> <p>A repository is a set of Docker images. A repository can be shared by pushing it to a <a href=\"#registry\">registry</a> server. The different images in the repository can be labeled using <a href=\"#tag\">tags</a>.</p> <p>Here is an example of the shared <a href=\"https://hub.docker.com/_/nginx/\">nginx repository</a> and its <a href=\"https://hub.docker.com/r/library/nginx/tags/\">tags</a></p> <h2 id=\"swarm\">Swarm</h2> <p><a href=\"https://github.com/docker/swarm\">Swarm</a> is a native clustering tool for Docker. Swarm pools together several Docker hosts and exposes them as a single virtual Docker host. It serves the standard Docker API, so any tool that already works with Docker can now transparently scale up to multiple hosts.</p> <p><em>Also known as : docker-swarm</em></p> <h2 id=\"tag\">tag</h2> <p>A tag is a label applied to a Docker image in a <a href=\"#repository\">repository</a>. tags are how various images in a repository are distinguished from each other.</p> <p><em>Note : This label is not related to the key=value labels set for docker daemon</em></p> <h2 id=\"toolbox\">Toolbox</h2> <p>Docker Toolbox is the installer for Mac and Windows users.</p> <h2 id=\"union-file-system\">Union file system</h2> <p>Union file systems, or UnionFS, are file systems that operate by creating layers, making them very lightweight and fast. Docker uses union file systems to provide the building blocks for containers.</p> <h2 id=\"virtual-machine\">Virtual Machine</h2> <p>A Virtual Machine is a program that emulates a complete computer and imitates dedicated hardware. It shares physical hardware resources with other users but isolates the operating system. The end user has the same experience on a Virtual Machine as they would have on dedicated hardware.</p> <p>Compared to to containers, a Virtual Machine is heavier to run, provides more isolation, gets its own set of resources and does minimal sharing.</p> <p><em>Also known as : VM</em></p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/reference/glossary/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/reference/glossary/</a>\n  </p>\n</div>\n","engine/installation/index":"<h1 id=\"install-docker-engine\">Install Docker Engine</h1> <p>Docker Engine is supported on Linux, Cloud, Windows, and OS X. Installation instructions are available for the following:</p> <h2 id=\"on-linux\">On Linux</h2> <ul> <li><a href=\"linux/archlinux/index\">Arch Linux</a></li> <li><a href=\"linux/centos/index\">CentOS</a></li> <li><a href=\"linux/cruxlinux/index\">CRUX Linux</a></li> <li><a href=\"linux/debian/index\">Debian</a></li> <li><a href=\"linux/fedora/index\">Fedora</a></li> <li><a href=\"linux/frugalware/index\">FrugalWare</a></li> <li><a href=\"linux/gentoolinux/index\">Gentoo</a></li> <li><a href=\"linux/oracle/index\">Oracle Linux</a></li> <li><a href=\"linux/rhel/index\">Red Hat Enterprise Linux</a></li> <li><a href=\"linux/suse/index\">openSUSE and SUSE Linux Enterprise</a></li> <li><a href=\"linux/ubuntulinux/index\">Ubuntu</a></li> </ul> <p>If your linux distribution is not listed above, don’t give up yet. To try out Docker on a distribution that is not listed above, go here: <a href=\"binaries/index\">Installation from binaries</a>.</p> <h2 id=\"on-cloud\">On Cloud</h2> <ul> <li><a href=\"cloud/overview/index\">Choose how to Install</a></li> <li><a href=\"cloud/cloud-ex-aws/index\">Example: Manual install on a cloud provider</a></li> <li><a href=\"cloud/cloud-ex-machine-ocean/index\">Example: Use Docker Machine to provision cloud hosts</a></li> </ul> <h2 id=\"on-osx-and-windows\">On OSX and Windows</h2> <ul> <li><a href=\"mac/index\">Mac OS X</a></li> <li><a href=\"windows/index\">Windows</a></li> </ul> <h2 id=\"the-docker-archives\">The Docker Archives</h2> <p>Instructions for installing prior releases of Docker can be found in the following docker archives: <a href=\"http://docs.docker.com/v1.7/\">Docker v1.7</a>, <a href=\"http://docs.docker.com/v1.6/\">Docker v1.6</a>, <a href=\"http://docs.docker.com/v1.5/\">Docker v1.5</a>, and <a href=\"http://docs.docker.com/v1.4/\">Docker v1.4</a>.</p> <h2 id=\"where-to-go-after-installing\">Where to go after installing</h2> <ul> <li><a href=\"../index\">About Docker Engine</a></li> <li><a href=\"https://www.docker.com/support/\">Support</a></li> <li><a href=\"https://training.docker.com//\">Training</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/installation/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/installation/</a>\n  </p>\n</div>\n","engine/userguide/index":"<h1 id=\"docker-engine-user-guide\">Docker Engine user guide</h1> <p>This guide helps users learn how to use Docker Engine.</p> <ul> <li><a href=\"intro/index\">Introduction to Engine user guide</a></li> </ul> <h2 id=\"learn-by-example\">Learn by example</h2> <ul> <li><a href=\"containers/dockerizing/index\">Hello world in a container</a></li> <li><a href=\"containers/dockerimages/index\">Build your own images</a></li> <li><a href=\"containers/networkingcontainers/index\">Network containers</a></li> <li><a href=\"containers/usingdocker/index\">Run a simple application</a></li> <li><a href=\"containers/dockervolumes/index\">Manage data in containers</a></li> <li><a href=\"containers/dockerrepos/index\">Store images on Docker Hub</a></li> </ul> <h2 id=\"work-with-images\">Work with images</h2> <ul> <li><a href=\"eng-image/dockerfile_best-practices/index\">Best practices for writing Dockerfiles</a></li> <li><a href=\"eng-image/baseimages/index\">Create a base image</a></li> <li><a href=\"eng-image/image_management/index\">Image management</a></li> </ul> <h2 id=\"manage-storage-drivers\">Manage storage drivers</h2> <ul> <li><a href=\"storagedriver/imagesandcontainers/index\">Understand images, containers, and storage drivers</a></li> <li><a href=\"storagedriver/selectadriver/index\">Select a storage driver</a></li> <li><a href=\"storagedriver/aufs-driver/index\">AUFS storage in practice</a></li> <li><a href=\"storagedriver/btrfs-driver/index\">Btrfs storage in practice</a></li> <li><a href=\"storagedriver/device-mapper-driver/index\">Device Mapper storage in practice</a></li> <li><a href=\"storagedriver/overlayfs-driver/index\">OverlayFS storage in practice</a></li> <li><a href=\"storagedriver/zfs-driver/index\">ZFS storage in practice</a></li> </ul> <h2 id=\"configure-networks\">Configure networks</h2> <ul> <li><a href=\"networking/dockernetworks/index\">Understand Docker container networks</a></li> <li><a href=\"networking/configure-dns/index\">Embedded DNS server in user-defined networks</a></li> <li><a href=\"networking/get-started-overlay/index\">Get started with multi-host networking</a></li> <li><a href=\"networking/work-with-networks/index\">Work with network commands</a></li> </ul> <h3 id=\"work-with-the-default-network\">Work with the default network</h3> <ul> <li><a href=\"networking/default_network/container-communication/index\">Understand container communication</a></li> <li><a href=\"networking/default_network/dockerlinks/index\">Legacy container links</a></li> <li><a href=\"networking/default_network/binding/index\">Binding container ports to the host</a></li> <li><a href=\"networking/default_network/build-bridges/index\">Build your own bridge</a></li> <li><a href=\"networking/default_network/configure-dns/index\">Configure container DNS</a></li> <li>\n<a href=\"networking/default_network/custom-docker0/index\">Customize the docker0 bridge</a><br>\n</li> <li>\n<a href=\"networking/default_network/ipv6/index\">IPv6 with Docker</a><br>\n</li> </ul> <h2 id=\"misc\">Misc</h2> <ul> <li><a href=\"labels-custom-metadata/index\">Apply custom metadata</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/</a>\n  </p>\n</div>\n","engine/userguide/networking/default_network/dockerlinks/index":"<h1 id=\"legacy-container-links\">Legacy container links</h1> <p>The information in this section explains legacy container links within the Docker default bridge. This is a <code>bridge</code> network named <code>bridge</code> created automatically when you install Docker.</p> <p>Before the <a href=\"../../dockernetworks/index\">Docker networks feature</a>, you could use the Docker link feature to allow containers to discover each other and securely transfer information about one container to another container. With the introduction of the Docker networks feature, you can still create links but they behave differently between default <code>bridge</code> network and <a href=\"../../work-with-networks/index#linking-containers-in-user-defined-networks\">user defined networks</a></p> <p>This section briefly discusses connecting via a network port and then goes into detail on container linking in default <code>bridge</code> network.</p> <h2 id=\"connect-using-network-port-mapping\">Connect using network port mapping</h2> <p>In <a href=\"../../../containers/usingdocker/index\">the Using Docker section</a>, you created a container that ran a Python Flask application:</p> <pre>$ docker run -d -P training/webapp python app.py\n</pre> <blockquote> <p><strong>Note:</strong> Containers have an internal network and an IP address (as we saw when we used the <code>docker inspect</code> command to show the container’s IP address in the <a href=\"../../../containers/usingdocker/index\">Using Docker</a> section). Docker can have a variety of network configurations. You can see more information on Docker networking <a href=\"../../index\">here</a>.</p> </blockquote> <p>When that container was created, the <code>-P</code> flag was used to automatically map any network port inside it to a random high port within an <em>ephemeral port range</em> on your Docker host. Next, when <code>docker ps</code> was run, you saw that port 5000 in the container was bound to port 49155 on the host.</p> <pre>$ docker ps nostalgic_morse\nCONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES\nbc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse\n</pre> <p>You also saw how you can bind a container’s ports to a specific port using the <code>-p</code> flag. Here port 80 of the host is mapped to port 5000 of the container:</p> <pre>$ docker run -d -p 80:5000 training/webapp python app.py\n</pre> <p>And you saw why this isn’t such a great idea because it constrains you to only one container on that specific port.</p> <p>Instead, you may specify a range of host ports to bind a container port to that is different than the default <em>ephemeral port range</em>:</p> <pre>$ docker run -d -p 8000-9000:5000 training/webapp python app.py\n</pre> <p>This would bind port 5000 in the container to a randomly available port between 8000 and 9000 on the host.</p> <p>There are also a few other ways you can configure the <code>-p</code> flag. By default the <code>-p</code> flag will bind the specified port to all interfaces on the host machine. But you can also specify a binding to a specific interface, for example only to the <code>localhost</code>.</p> <pre>$ docker run -d -p 127.0.0.1:80:5000 training/webapp python app.py\n</pre> <p>This would bind port 5000 inside the container to port 80 on the <code>localhost</code> or <code>127.0.0.1</code> interface on the host machine.</p> <p>Or, to bind port 5000 of the container to a dynamic port but only on the <code>localhost</code>, you could use:</p> <pre>$ docker run -d -p 127.0.0.1::5000 training/webapp python app.py\n</pre> <p>You can also bind UDP ports by adding a trailing <code>/udp</code>. For example:</p> <pre>$ docker run -d -p 127.0.0.1:80:5000/udp training/webapp python app.py\n</pre> <p>You also learned about the useful <code>docker port</code> shortcut which showed us the current port bindings. This is also useful for showing you specific port configurations. For example, if you’ve bound the container port to the <code>localhost</code> on the host machine, then the <code>docker port</code> output will reflect that.</p> <pre>$ docker port nostalgic_morse 5000\n127.0.0.1:49155\n</pre> <blockquote> <p><strong>Note:</strong> The <code>-p</code> flag can be used multiple times to configure multiple ports.</p> </blockquote> <h2 id=\"connect-with-the-linking-system\">Connect with the linking system</h2> <blockquote> <p><strong>Note</strong>: This section covers the legacy link feature in the default <code>bridge</code> network. Please refer to <a href=\"../../work-with-networks/index#linking-containers-in-user-defined-networks\">linking containers in user-defined networks</a> for more information on links in user-defined networks.</p> </blockquote> <p>Network port mappings are not the only way Docker containers can connect to one another. Docker also has a linking system that allows you to link multiple containers together and send connection information from one to another. When containers are linked, information about a source container can be sent to a recipient container. This allows the recipient to see selected data describing aspects of the source container.</p> <h3 id=\"the-importance-of-naming\">The importance of naming</h3> <p>To establish links, Docker relies on the names of your containers. You’ve already seen that each container you create has an automatically created name; indeed you’ve become familiar with our old friend <code>nostalgic_morse</code> during this guide. You can also name containers yourself. This naming provides two useful functions:</p> <ol> <li><p>It can be useful to name containers that do specific functions in a way that makes it easier for you to remember them, for example naming a container containing a web application <code>web</code>.</p></li> <li><p>It provides Docker with a reference point that allows it to refer to other containers, for example, you can specify to link the container <code>web</code> to container <code>db</code>.</p></li> </ol> <p>You can name your container by using the <code>--name</code> flag, for example:</p> <pre>$ docker run -d -P --name web training/webapp python app.py\n</pre> <p>This launches a new container and uses the <code>--name</code> flag to name the container <code>web</code>. You can see the container’s name using the <code>docker ps</code> command.</p> <pre>$ docker ps -l\nCONTAINER ID  IMAGE                  COMMAND        CREATED       STATUS       PORTS                    NAMES\naed84ee21bde  training/webapp:latest python app.py  12 hours ago  Up 2 seconds 0.0.0.0:49154-&gt;5000/tcp  web\n</pre> <p>You can also use <code>docker inspect</code> to return the container’s name.</p> <blockquote> <p><strong>Note:</strong> Container names have to be unique. That means you can only call one container <code>web</code>. If you want to re-use a container name you must delete the old container (with <code>docker rm</code>) before you can create a new container with the same name. As an alternative you can use the <code>--rm</code> flag with the <code>docker run</code> command. This will delete the container immediately after it is stopped.</p> </blockquote> <h2 id=\"communication-across-links\">Communication across links</h2> <p>Links allow containers to discover each other and securely transfer information about one container to another container. When you set up a link, you create a conduit between a source container and a recipient container. The recipient can then access select data about the source. To create a link, you use the <code>--link</code> flag. First, create a new container, this time one containing a database.</p> <pre>$ docker run -d --name db training/postgres\n</pre> <p>This creates a new container called <code>db</code> from the <code>training/postgres</code> image, which contains a PostgreSQL database.</p> <p>Now, you need to delete the <code>web</code> container you created previously so you can replace it with a linked one:</p> <pre>$ docker rm -f web\n</pre> <p>Now, create a new <code>web</code> container and link it with your <code>db</code> container.</p> <pre>$ docker run -d -P --name web --link db:db training/webapp python app.py\n</pre> <p>This will link the new <code>web</code> container with the <code>db</code> container you created earlier. The <code>--link</code> flag takes the form:</p> <pre>--link &lt;name or id&gt;:alias\n</pre> <p>Where <code>name</code> is the name of the container we’re linking to and <code>alias</code> is an alias for the link name. You’ll see how that alias gets used shortly. The <code>--link</code> flag also takes the form:</p> <pre>--link &lt;name or id&gt;\n</pre> <p>In which case the alias will match the name. You could have written the previous example as:</p> <pre>$ docker run -d -P --name web --link db training/webapp python app.py\n</pre> <p>Next, inspect your linked containers with <code>docker inspect</code>:</p> <pre>$ docker inspect -f \"{{ .HostConfig.Links }}\" web\n[/db:/web/db]\n</pre> <p>You can see that the <code>web</code> container is now linked to the <code>db</code> container <code>web/db</code>. Which allows it to access information about the <code>db</code> container.</p> <p>So what does linking the containers actually do? You’ve learned that a link allows a source container to provide information about itself to a recipient container. In our example, the recipient, <code>web</code>, can access information about the source <code>db</code>. To do this, Docker creates a secure tunnel between the containers that doesn’t need to expose any ports externally on the container; you’ll note when we started the <code>db</code> container we did not use either the <code>-P</code> or <code>-p</code> flags. That’s a big benefit of linking: we don’t need to expose the source container, here the PostgreSQL database, to the network.</p> <p>Docker exposes connectivity information for the source container to the recipient container in two ways:</p> <ul> <li>Environment variables,</li> <li>Updating the <code>/etc/hosts</code> file.</li> </ul> <h3 id=\"environment-variables\">Environment variables</h3> <p>Docker creates several environment variables when you link containers. Docker automatically creates environment variables in the target container based on the <code>--link</code> parameters. It will also expose all environment variables originating from Docker from the source container. These include variables from:</p> <ul> <li>the <code>ENV</code> commands in the source container’s Dockerfile</li> <li>the <code>-e</code>, <code>--env</code> and <code>--env-file</code> options on the <code>docker run</code> command when the source container is started</li> </ul> <p>These environment variables enable programmatic discovery from within the target container of information related to the source container.</p> <blockquote> <p><strong>Warning</strong>: It is important to understand that <em>all</em> environment variables originating from Docker within a container are made available to <em>any</em> container that links to it. This could have serious security implications if sensitive data is stored in them.</p> </blockquote> <p>Docker sets an <code>&lt;alias&gt;_NAME</code> environment variable for each target container listed in the <code>--link</code> parameter. For example, if a new container called <code>web</code> is linked to a database container called <code>db</code> via <code>--link db:webdb</code>, then Docker creates a <code>WEBDB_NAME=/web/webdb</code> variable in the <code>web</code> container.</p> <p>Docker also defines a set of environment variables for each port exposed by the source container. Each variable has a unique prefix in the form:</p> <p><code>&lt;name&gt;_PORT_&lt;port&gt;_&lt;protocol&gt;</code></p> <p>The components in this prefix are:</p> <ul> <li>the alias <code>&lt;name&gt;</code> specified in the <code>--link</code> parameter (for example, <code>webdb</code>)</li> <li>the <code>&lt;port&gt;</code> number exposed</li> <li>a <code>&lt;protocol&gt;</code> which is either TCP or UDP</li> </ul> <p>Docker uses this prefix format to define three distinct environment variables:</p> <ul> <li>The <code>prefix_ADDR</code> variable contains the IP Address from the URL, for example <code>WEBDB_PORT_5432_TCP_ADDR=172.17.0.82</code>.</li> <li>The <code>prefix_PORT</code> variable contains just the port number from the URL for example <code>WEBDB_PORT_5432_TCP_PORT=5432</code>.</li> <li>The <code>prefix_PROTO</code> variable contains just the protocol from the URL for example <code>WEBDB_PORT_5432_TCP_PROTO=tcp</code>.</li> </ul> <p>If the container exposes multiple ports, an environment variable set is defined for each one. This means, for example, if a container exposes 4 ports that Docker creates 12 environment variables, 3 for each port.</p> <p>Additionally, Docker creates an environment variable called <code>&lt;alias&gt;_PORT</code>. This variable contains the URL of the source container’s first exposed port. The ‘first’ port is defined as the exposed port with the lowest number. For example, consider the <code>WEBDB_PORT=tcp://172.17.0.82:5432</code> variable. If that port is used for both tcp and udp, then the tcp one is specified.</p> <p>Finally, Docker also exposes each Docker originated environment variable from the source container as an environment variable in the target. For each variable Docker creates an <code>&lt;alias&gt;_ENV_&lt;name&gt;</code> variable in the target container. The variable’s value is set to the value Docker used when it started the source container.</p> <p>Returning back to our database example, you can run the <code>env</code> command to list the specified container’s environment variables.</p> <pre>    $ docker run --rm --name web2 --link db:db training/webapp env\n    . . .\n    DB_NAME=/web2/db\n    DB_PORT=tcp://172.17.0.5:5432\n    DB_PORT_5432_TCP=tcp://172.17.0.5:5432\n    DB_PORT_5432_TCP_PROTO=tcp\n    DB_PORT_5432_TCP_PORT=5432\n    DB_PORT_5432_TCP_ADDR=172.17.0.5\n    . . .\n</pre> <p>You can see that Docker has created a series of environment variables with useful information about the source <code>db</code> container. Each variable is prefixed with <code>DB_</code>, which is populated from the <code>alias</code> you specified above. If the <code>alias</code> were <code>db1</code>, the variables would be prefixed with <code>DB1_</code>. You can use these environment variables to configure your applications to connect to the database on the <code>db</code> container. The connection will be secure and private; only the linked <code>web</code> container will be able to talk to the <code>db</code> container.</p> <h3 id=\"important-notes-on-docker-environment-variables\">Important notes on Docker environment variables</h3> <p>Unlike host entries in the <a href=\"#updating-the-etchosts-file\"><code>/etc/hosts</code> file</a>, IP addresses stored in the environment variables are not automatically updated if the source container is restarted. We recommend using the host entries in <code>/etc/hosts</code> to resolve the IP address of linked containers.</p> <p>These environment variables are only set for the first process in the container. Some daemons, such as <code>sshd</code>, will scrub them when spawning shells for connection.</p> <h3 id=\"updating-the-etc-hosts-file\">Updating the <code>/etc/hosts</code> file</h3> <p>In addition to the environment variables, Docker adds a host entry for the source container to the <code>/etc/hosts</code> file. Here’s an entry for the <code>web</code> container:</p> <pre>$ docker run -t -i --rm --link db:webdb training/webapp /bin/bash\nroot@aed84ee21bde:/opt/webapp# cat /etc/hosts\n172.17.0.7  aed84ee21bde\n. . .\n172.17.0.5  webdb 6e5cdeb2d300 db\n</pre> <p>You can see two relevant host entries. The first is an entry for the <code>web</code> container that uses the Container ID as a host name. The second entry uses the link alias to reference the IP address of the <code>db</code> container. In addition to the alias you provide, the linked container’s name--if unique from the alias provided to the <code>--link</code> parameter--and the linked container’s hostname will also be added in <code>/etc/hosts</code> for the linked container’s IP address. You can ping that host now via any of these entries:</p> <pre>root@aed84ee21bde:/opt/webapp# apt-get install -yqq inetutils-ping\nroot@aed84ee21bde:/opt/webapp# ping webdb\nPING webdb (172.17.0.5): 48 data bytes\n56 bytes from 172.17.0.5: icmp_seq=0 ttl=64 time=0.267 ms\n56 bytes from 172.17.0.5: icmp_seq=1 ttl=64 time=0.250 ms\n56 bytes from 172.17.0.5: icmp_seq=2 ttl=64 time=0.256 ms\n</pre> <blockquote> <p><strong>Note:</strong> In the example, you’ll note you had to install <code>ping</code> because it was not included in the container initially.</p> </blockquote> <p>Here, you used the <code>ping</code> command to ping the <code>db</code> container using its host entry, which resolves to <code>172.17.0.5</code>. You can use this host entry to configure an application to make use of your <code>db</code> container.</p> <blockquote> <p><strong>Note:</strong> You can link multiple recipient containers to a single source. For example, you could have multiple (differently named) web containers attached to your <code>db</code> container.</p> </blockquote> <p>If you restart the source container, the linked containers <code>/etc/hosts</code> files will be automatically updated with the source container’s new IP address, allowing linked communication to continue.</p> <pre>$ docker restart db\ndb\n$ docker run -t -i --rm --link db:db training/webapp /bin/bash\nroot@aed84ee21bde:/opt/webapp# cat /etc/hosts\n172.17.0.7  aed84ee21bde\n. . .\n172.17.0.9  db\n</pre> <h1 id=\"related-information\">Related information</h1><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/dockerlinks/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/default_network/dockerlinks/</a>\n  </p>\n</div>\n","engine/security/trust/index":"<h1 id=\"use-trusted-images\">Use trusted images</h1> <p>The following topics are available:</p> <ul> <li><a href=\"content_trust/index\">Content trust in Docker</a></li> <li><a href=\"trust_key_mng/index\">Manage keys for content trust</a></li> <li><a href=\"trust_automation/index\">Automation with content trust</a></li> <li><a href=\"trust_delegation/index\">Delegations for content trust</a></li> <li><a href=\"trust_sandbox/index\">Play in a content trust sandbox</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/trust/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/trust/</a>\n  </p>\n</div>\n","engine/extend/index":"<h1 id=\"extending-docker-engine\">Extending Docker Engine</h1> <p>Currently, you can extend Docker Engine by adding a plugin. This section contains the following topics:</p> <ul> <li><a href=\"plugins/index\">Understand Docker plugins</a></li> <li><a href=\"plugins_volume/index\">Write a volume plugin</a></li> <li><a href=\"plugins_network/index\">Write a network plugin</a></li> <li><a href=\"plugins_authorization/index\">Write an authorization plugin</a></li> <li><a href=\"plugin_api/index\">Docker plugin API</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/extend/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/extend/</a>\n  </p>\n</div>\n","engine/userguide/networking/default_network/configure-dns/index":"<h1 id=\"configure-container-dns\">Configure container DNS</h1> <p>The information in this section explains configuring container DNS within the Docker default bridge. This is a <code>bridge</code> network named <code>bridge</code> created automatically when you install Docker.</p> <blockquote> <p><strong>Note</strong>: The <a href=\"../../dockernetworks/index\">Docker networks feature</a> allows you to create user-defined networks in addition to the default bridge network. Please refer to the <a href=\"../../configure-dns/index\">Docker Embedded DNS</a> section for more information on DNS configurations in user-defined networks.</p> </blockquote> <p>How can Docker supply each container with a hostname and DNS configuration, without having to build a custom image with the hostname written inside? Its trick is to overlay three crucial <code>/etc</code> files inside the container with virtual files where it can write fresh information. You can see this by running <code>mount</code> inside a container:</p> <pre>$$ mount\n...\n/dev/disk/by-uuid/1fec...ebdf on /etc/hostname type ext4 ...\n/dev/disk/by-uuid/1fec...ebdf on /etc/hosts type ext4 ...\n/dev/disk/by-uuid/1fec...ebdf on /etc/resolv.conf type ext4 ...\n...\n</pre> <p>This arrangement allows Docker to do clever things like keep <code>resolv.conf</code> up to date across all containers when the host machine receives new configuration over DHCP later. The exact details of how Docker maintains these files inside the container can change from one Docker version to the next, so you should leave the files themselves alone and use the following Docker options instead.</p> <p>Four different options affect container domain name services.</p> <table> <tr> <td> <p> <code>-h HOSTNAME</code> or <code>--hostname=HOSTNAME</code> </p> </td> <td> <p> Sets the hostname by which the container knows itself. This is written into <code>/etc/hostname</code>, into <code>/etc/hosts</code> as the name of the container's host-facing IP address, and is the name that <code>/bin/bash</code> inside the container will display inside its prompt. But the hostname is not easy to see from outside the container. It will not appear in <code>docker ps</code> nor in the <code>/etc/hosts</code> file of any other container. </p> </td> </tr> <tr> <td> <p> <code>--link=CONTAINER_NAME</code> or <code>ID:ALIAS</code> </p> </td> <td> <p> Using this option as you <code>run</code> a container gives the new container's <code>/etc/hosts</code> an extra entry named <code>ALIAS</code> that points to the IP address of the container identified by <code>CONTAINER_NAME_or_ID</code>. This lets processes inside the new container connect to the hostname <code>ALIAS</code> without having to know its IP. The <code>--link=</code> option is discussed in more detail below. Because Docker may assign a different IP address to the linked containers on restart, Docker updates the <code>ALIAS</code> entry in the <code>/etc/hosts</code> file of the recipient containers. </p> </td> </tr> <tr> <td><p> <code>--dns=IP_ADDRESS...</code> </p></td> <td><p> Sets the IP addresses added as <code>server</code> lines to the container's <code>/etc/resolv.conf</code> file. Processes in the container, when confronted with a hostname not in <code>/etc/hosts</code>, will connect to these IP addresses on port 53 looking for name resolution services. </p></td> </tr> <tr> <td><p> <code>--dns-search=DOMAIN...</code> </p></td> <td>\n<p> Sets the domain names that are searched when a bare unqualified hostname is used inside of the container, by writing <code>search</code> lines into the container's <code>/etc/resolv.conf</code>. When a container process attempts to access <code>host</code> and the search domain <code>example.com</code> is set, for instance, the DNS logic will not only look up <code>host</code> but also <code>host.example.com</code>. </p> <p> Use <code>--dns-search=.</code> if you don't wish to set the search domain. </p> </td> </tr> <tr> <td><p> <code>--dns-opt=OPTION...</code> </p></td> <td>\n<p> Sets the options used by DNS resolvers by writing an <code>options<code>\n      line into the container's <code>/etc/resolv.conf<code>.\n    </code></code></code></code></p> <p> See documentation for <code>resolv.conf<code> for a list of valid options\n    </code></code></p>\n</td> </tr> <tr> <td></td> <td></td> </tr> </table> <p>Regarding DNS settings, in the absence of the <code>--dns=IP_ADDRESS...</code>, <code>--dns-search=DOMAIN...</code>, or <code>--dns-opt=OPTION...</code> options, Docker makes each container’s <code>/etc/resolv.conf</code> look like the <code>/etc/resolv.conf</code> of the host machine (where the <code>docker</code> daemon runs). When creating the container’s <code>/etc/resolv.conf</code>, the daemon filters out all localhost IP address <code>nameserver</code> entries from the host’s original file.</p> <p>Filtering is necessary because all localhost addresses on the host are unreachable from the container’s network. After this filtering, if there are no more <code>nameserver</code> entries left in the container’s <code>/etc/resolv.conf</code> file, the daemon adds public Google DNS nameservers (8.8.8.8 and 8.8.4.4) to the container’s DNS configuration. If IPv6 is enabled on the daemon, the public IPv6 Google DNS nameservers will also be added (2001:4860:4860::8888 and 2001:4860:4860::8844).</p> <blockquote> <p><strong>Note</strong>: If you need access to a host’s localhost resolver, you must modify your DNS service on the host to listen on a non-localhost address that is reachable from within the container.</p> </blockquote> <p>You might wonder what happens when the host machine’s <code>/etc/resolv.conf</code> file changes. The <code>docker</code> daemon has a file change notifier active which will watch for changes to the host DNS configuration.</p> <blockquote> <p><strong>Note</strong>: The file change notifier relies on the Linux kernel’s inotify feature. Because this feature is currently incompatible with the overlay filesystem driver, a Docker daemon using “overlay” will not be able to take advantage of the <code>/etc/resolv.conf</code> auto-update feature.</p> </blockquote> <p>When the host file changes, all stopped containers which have a matching <code>resolv.conf</code> to the host will be updated immediately to this newest host configuration. Containers which are running when the host configuration changes will need to stop and start to pick up the host changes due to lack of a facility to ensure atomic writes of the <code>resolv.conf</code> file while the container is running. If the container’s <code>resolv.conf</code> has been edited since it was started with the default configuration, no replacement will be attempted as it would overwrite the changes performed by the container. If the options (<code>--dns</code>, <code>--dns-search</code>, or <code>--dns-opt</code>) have been used to modify the default host configuration, then the replacement with an updated host’s <code>/etc/resolv.conf</code> will not happen as well.</p> <blockquote> <p><strong>Note</strong>: For containers which were created prior to the implementation of the <code>/etc/resolv.conf</code> update feature in Docker 1.5.0: those containers will <strong>not</strong> receive updates when the host <code>resolv.conf</code> file changes. Only containers created with Docker 1.5.0 and above will utilize this auto-update feature.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/configure-dns/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/default_network/configure-dns/</a>\n  </p>\n</div>\n","engine/userguide/networking/default_network/binding/index":"<h1 id=\"bind-container-ports-to-the-host\">Bind container ports to the host</h1> <p>The information in this section explains binding container ports within the Docker default bridge. This is a <code>bridge</code> network named <code>bridge</code> created automatically when you install Docker.</p> <blockquote> <p><strong>Note</strong>: The <a href=\"../../dockernetworks/index\">Docker networks feature</a> allows you to create user-defined networks in addition to the default bridge network.</p> </blockquote> <p>By default Docker containers can make connections to the outside world, but the outside world cannot connect to containers. Each outgoing connection will appear to originate from one of the host machine’s own IP addresses thanks to an <code>iptables</code> masquerading rule on the host machine that the Docker server creates when it starts:</p> <pre>$ sudo iptables -t nat -L -n\n...\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nMASQUERADE  all  --  172.17.0.0/16       0.0.0.0/0\n...\n</pre> <p>The Docker server creates a masquerade rule that let containers connect to IP addresses in the outside world.</p> <p>If you want containers to accept incoming connections, you will need to provide special options when invoking <code>docker run</code>. There are two approaches.</p> <p>First, you can supply <code>-P</code> or <code>--publish-all=true|false</code> to <code>docker run</code> which is a blanket operation that identifies every port with an <code>EXPOSE</code> line in the image’s <code>Dockerfile</code> or <code>--expose &lt;port&gt;</code> commandline flag and maps it to a host port somewhere within an <em>ephemeral port range</em>. The <code>docker port</code> command then needs to be used to inspect created mapping. The <em>ephemeral port range</em> is configured by <code>/proc/sys/net/ipv4/ip_local_port_range</code> kernel parameter, typically ranging from 32768 to 61000.</p> <p>Mapping can be specified explicitly using <code>-p SPEC</code> or <code>--publish=SPEC</code> option. It allows you to particularize which port on docker server - which can be any port at all, not just one within the <em>ephemeral port range</em> -- you want mapped to which port in the container.</p> <p>Either way, you should be able to peek at what Docker has accomplished in your network stack by examining your NAT tables.</p> <pre># What your NAT rules might look like when Docker\n# is finished setting up a -P forward:\n\n$ iptables -t nat -L -n\n...\nChain DOCKER (2 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:49153 to:172.17.0.2:80\n\n# What your NAT rules might look like when Docker\n# is finished setting up a -p 80:80 forward:\n\nChain DOCKER (2 references)\ntarget     prot opt source               destination\nDNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:80 to:172.17.0.2:80\n</pre> <p>You can see that Docker has exposed these container ports on <code>0.0.0.0</code>, the wildcard IP address that will match any possible incoming port on the host machine. If you want to be more restrictive and only allow container services to be contacted through a specific external interface on the host machine, you have two choices. When you invoke <code>docker run</code> you can use either <code>-p\nIP:host_port:container_port</code> or <code>-p IP::port</code> to specify the external interface for one particular binding.</p> <p>Or if you always want Docker port forwards to bind to one specific IP address, you can edit your system-wide Docker server settings and add the option <code>--ip=IP_ADDRESS</code>. Remember to restart your Docker server after editing this setting.</p> <blockquote> <p><strong>Note</strong>: With hairpin NAT enabled (<code>--userland-proxy=false</code>), containers port exposure is achieved purely through iptables rules, and no attempt to bind the exposed port is ever made. This means that nothing prevents shadowing a previously listening service outside of Docker through exposing the same port for a container. In such conflicting situation, Docker created iptables rules will take precedence and route to the container.</p> </blockquote> <p>The <code>--userland-proxy</code> parameter, true by default, provides a userland implementation for inter-container and outside-to-container communication. When disabled, Docker uses both an additional <code>MASQUERADE</code> iptable rule and the <code>net.ipv4.route_localnet</code> kernel parameter which allow the host machine to connect to a local container exposed port through the commonly used loopback address: this alternative is preferred for performance reasons.</p> <h2 id=\"related-information\">Related information</h2> <ul> <li><a href=\"../../dockernetworks/index\">Understand Docker container networks</a></li> <li><a href=\"../../work-with-networks/index\">Work with network commands</a></li> <li><a href=\"../dockerlinks/index\">Legacy container links</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/binding/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/default_network/binding/</a>\n  </p>\n</div>\n","engine/userguide/networking/default_network/container-communication/index":"<h1 id=\"understand-container-communication\">Understand container communication</h1> <p>The information in this section explains container communication within the Docker default bridge. This is a <code>bridge</code> network named <code>bridge</code> created automatically when you install Docker.</p> <p><strong>Note</strong>: The <a href=\"../../dockernetworks/index\">Docker networks feature</a> allows you to create user-defined networks in addition to the default bridge network.</p> <h2 id=\"communicating-to-the-outside-world\">Communicating to the outside world</h2> <p>Whether a container can talk to the world is governed by two factors. The first factor is whether the host machine is forwarding its IP packets. The second is whether the host’s <code>iptables</code> allow this particular connection.</p> <p>IP packet forwarding is governed by the <code>ip_forward</code> system parameter. Packets can only pass between containers if this parameter is <code>1</code>. Usually you will simply leave the Docker server at its default setting <code>--ip-forward=true</code> and Docker will go set <code>ip_forward</code> to <code>1</code> for you when the server starts up. If you set <code>--ip-forward=false</code> and your system’s kernel has it enabled, the <code>--ip-forward=false</code> option has no effect. To check the setting on your kernel or to turn it on manually:</p> <pre>  $ sysctl net.ipv4.conf.all.forwarding\n  net.ipv4.conf.all.forwarding = 0\n  $ sysctl net.ipv4.conf.all.forwarding=1\n  $ sysctl net.ipv4.conf.all.forwarding\n  net.ipv4.conf.all.forwarding = 1\n</pre> <p>Many using Docker will want <code>ip_forward</code> to be on, to at least make communication <em>possible</em> between containers and the wider world. May also be needed for inter-container communication if you are in a multiple bridge setup.</p> <p>Docker will never make changes to your system <code>iptables</code> rules if you set <code>--iptables=false</code> when the daemon starts. Otherwise the Docker server will append forwarding rules to the <code>DOCKER</code> filter chain.</p> <p>Docker will not delete or modify any pre-existing rules from the <code>DOCKER</code> filter chain. This allows the user to create in advance any rules required to further restrict access to the containers.</p> <p>Docker’s forward rules permit all external source IPs by default. To allow only a specific IP or network to access the containers, insert a negated rule at the top of the <code>DOCKER</code> filter chain. For example, to restrict external access such that <em>only</em> source IP 8.8.8.8 can access the containers, the following rule could be added:</p> <pre>$ iptables -I DOCKER -i ext_if ! -s 8.8.8.8 -j DROP\n</pre> <p>where <em>ext_if</em> is the name of the interface providing external connectivity to the host.</p> <h2 id=\"communication-between-containers\">Communication between containers</h2> <p>Whether two containers can communicate is governed, at the operating system level, by two factors.</p> <ul> <li><p>Does the network topology even connect the containers’ network interfaces? By default Docker will attach all containers to a single <code>docker0</code> bridge, providing a path for packets to travel between them. See the later sections of this document for other possible topologies.</p></li> <li><p>Do your <code>iptables</code> allow this particular connection? Docker will never make changes to your system <code>iptables</code> rules if you set <code>--iptables=false</code> when the daemon starts. Otherwise the Docker server will add a default rule to the <code>FORWARD</code> chain with a blanket <code>ACCEPT</code> policy if you retain the default <code>--icc=true</code>, or else will set the policy to <code>DROP</code> if <code>--icc=false</code>.</p></li> </ul> <p>It is a strategic question whether to leave <code>--icc=true</code> or change it to <code>--icc=false</code> so that <code>iptables</code> will protect other containers -- and the main host -- from having arbitrary ports probed or accessed by a container that gets compromised.</p> <p>If you choose the most secure setting of <code>--icc=false</code>, then how can containers communicate in those cases where you <em>want</em> them to provide each other services? The answer is the <code>--link=CONTAINER_NAME_or_ID:ALIAS</code> option, which was mentioned in the previous section because of its effect upon name services. If the Docker daemon is running with both <code>--icc=false</code> and <code>--iptables=true</code> then, when it sees <code>docker run</code> invoked with the <code>--link=</code> option, the Docker server will insert a pair of <code>iptables</code> <code>ACCEPT</code> rules so that the new container can connect to the ports exposed by the other container -- the ports that it mentioned in the <code>EXPOSE</code> lines of its <code>Dockerfile</code>.</p> <blockquote> <p><strong>Note</strong>: The value <code>CONTAINER_NAME</code> in <code>--link=</code> must either be an auto-assigned Docker name like <code>stupefied_pare</code> or else the name you assigned with <code>--name=</code> when you ran <code>docker run</code>. It cannot be a hostname, which Docker will not recognize in the context of the <code>--link=</code> option.</p> </blockquote> <p>You can run the <code>iptables</code> command on your Docker host to see whether the <code>FORWARD</code> chain has a default policy of <code>ACCEPT</code> or <code>DROP</code>:</p> <pre># When --icc=false, you should see a DROP rule:\n\n$ sudo iptables -L -n\n...\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination\nDOCKER     all  --  0.0.0.0/0            0.0.0.0/0\nDROP       all  --  0.0.0.0/0            0.0.0.0/0\n...\n\n# When a --link= has been created under --icc=false,\n# you should see port-specific ACCEPT rules overriding\n# the subsequent DROP policy for all other packets:\n\n$ sudo iptables -L -n\n...\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination\nDOCKER     all  --  0.0.0.0/0            0.0.0.0/0\nDROP       all  --  0.0.0.0/0            0.0.0.0/0\n\nChain DOCKER (1 references)\ntarget     prot opt source               destination\nACCEPT     tcp  --  172.17.0.2           172.17.0.3           tcp spt:80\nACCEPT     tcp  --  172.17.0.3           172.17.0.2           tcp dpt:80\n</pre> <blockquote> <p><strong>Note</strong>: Docker is careful that its host-wide <code>iptables</code> rules fully expose containers to each other’s raw IP addresses, so connections from one container to another should always appear to be originating from the first container’s own IP address.</p> </blockquote><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/container-communication/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/default_network/container-communication/</a>\n  </p>\n</div>\n","engine/userguide/networking/default_network/build-bridges/index":"<h1 id=\"build-your-own-bridge\">Build your own bridge</h1> <p>This section explains how to build your own bridge to replace the Docker default bridge. This is a <code>bridge</code> network named <code>bridge</code> created automatically when you install Docker.</p> <blockquote> <p><strong>Note</strong>: The <a href=\"../../dockernetworks/index\">Docker networks feature</a> allows you to create user-defined networks in addition to the default bridge network.</p> </blockquote> <p>You can set up your own bridge before starting Docker and use <code>-b BRIDGE</code> or <code>--bridge=BRIDGE</code> to tell Docker to use your bridge instead. If you already have Docker up and running with its default <code>docker0</code> still configured, you can directly create your bridge and restart Docker with it or want to begin by stopping the service and removing the interface:</p> <pre># Stopping Docker and removing docker0\n\n$ sudo service docker stop\n$ sudo ip link set dev docker0 down\n$ sudo brctl delbr docker0\n$ sudo iptables -t nat -F POSTROUTING\n</pre> <p>Then, before starting the Docker service, create your own bridge and give it whatever configuration you want. Here we will create a simple enough bridge that we really could just have used the options in the previous section to customize <code>docker0</code>, but it will be enough to illustrate the technique.</p> <pre># Create our own bridge\n\n$ sudo brctl addbr bridge0\n$ sudo ip addr add 192.168.5.1/24 dev bridge0\n$ sudo ip link set dev bridge0 up\n\n# Confirming that our bridge is up and running\n\n$ ip addr show bridge0\n4: bridge0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state UP group default\n    link/ether 66:38:d0:0d:76:18 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.5.1/24 scope global bridge0\n       valid_lft forever preferred_lft forever\n\n# Tell Docker about it and restart (on Ubuntu)\n\n$ echo 'DOCKER_OPTS=\"-b=bridge0\"' &gt;&gt; /etc/default/docker\n$ sudo service docker start\n\n# Confirming new outgoing NAT masquerade is set up\n\n$ sudo iptables -t nat -L -n\n...\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nMASQUERADE  all  --  192.168.5.0/24      0.0.0.0/0\n</pre> <p>The result should be that the Docker server starts successfully and is now prepared to bind containers to the new bridge. After pausing to verify the bridge’s configuration, try creating a container -- you will see that its IP address is in your new IP address range, which Docker will have auto-detected.</p> <p>You can use the <code>brctl show</code> command to see Docker add and remove interfaces from the bridge as you start and stop containers, and can run <code>ip addr</code> and <code>ip\nroute</code> inside a container to see that it has been given an address in the bridge’s IP address range and has been told to use the Docker host’s IP address on the bridge as its default gateway to the rest of the Internet.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/build-bridges/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/default_network/build-bridges/</a>\n  </p>\n</div>\n","engine/userguide/networking/default_network/custom-docker0/index":"<h1 id=\"customize-the-docker0-bridge\">Customize the docker0 bridge</h1> <p>The information in this section explains how to customize the Docker default bridge. This is a <code>bridge</code> network named <code>bridge</code> created automatically when you install Docker.</p> <p><strong>Note</strong>: The <a href=\"../../dockernetworks/index\">Docker networks feature</a> allows you to create user-defined networks in addition to the default bridge network.</p> <p>By default, the Docker server creates and configures the host system’s <code>docker0</code> interface as an <em>Ethernet bridge</em> inside the Linux kernel that can pass packets back and forth between other physical or virtual network interfaces so that they behave as a single Ethernet network.</p> <p>Docker configures <code>docker0</code> with an IP address, netmask and IP allocation range. The host machine can both receive and send packets to containers connected to the bridge, and gives it an MTU -- the <em>maximum transmission unit</em> or largest packet length that the interface will allow -- of 1,500 bytes. These options are configurable at server startup:</p> <ul> <li><p><code>--bip=CIDR</code> -- supply a specific IP address and netmask for the <code>docker0</code> bridge, using standard CIDR notation like <code>192.168.1.5/24</code>.</p></li> <li><p><code>--fixed-cidr=CIDR</code> -- restrict the IP range from the <code>docker0</code> subnet, using the standard CIDR notation like <code>172.167.1.0/28</code>. This range must be an IPv4 range for fixed IPs (ex: 10.20.0.0/16) and must be a subset of the bridge IP range (<code>docker0</code> or set using <code>--bridge</code>). For example with <code>--fixed-cidr=192.168.1.0/25</code>, IPs for your containers will be chosen from the first half of <code>192.168.1.0/24</code> subnet.</p></li> <li><p><code>--mtu=BYTES</code> -- override the maximum packet length on <code>docker0</code>.</p></li> </ul> <p>Once you have one or more containers up and running, you can confirm that Docker has properly connected them to the <code>docker0</code> bridge by running the <code>brctl</code> command on the host machine and looking at the <code>interfaces</code> column of the output. Here is a host with two different containers connected:</p> <pre># Display bridge info\n\n$ sudo brctl show\nbridge name     bridge id               STP enabled     interfaces\ndocker0         8000.3a1d7362b4ee       no              veth65f9\n                                                        vethdda6\n</pre> <p>If the <code>brctl</code> command is not installed on your Docker host, then on Ubuntu you should be able to run <code>sudo apt-get install bridge-utils</code> to install it.</p> <p>Finally, the <code>docker0</code> Ethernet bridge settings are used every time you create a new container. Docker selects a free IP address from the range available on the bridge each time you <code>docker run</code> a new container, and configures the container’s <code>eth0</code> interface with that IP address and the bridge’s netmask. The Docker host’s own IP address on the bridge is used as the default gateway by which each container reaches the rest of the Internet.</p> <pre># The network, as seen from a container\n\n$ docker run -i -t --rm base /bin/bash\n\n$$ ip addr show eth0\n24: eth0: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 32:6f:e0:35:57:91 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.3/16 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::306f:e0ff:fe35:5791/64 scope link\n       valid_lft forever preferred_lft forever\n\n$$ ip route\ndefault via 172.17.42.1 dev eth0\n172.17.0.0/16 dev eth0  proto kernel  scope link  src 172.17.0.3\n\n$$ exit\n</pre> <p>Remember that the Docker host will not be willing to forward container packets out on to the Internet unless its <code>ip_forward</code> system setting is <code>1</code> -- see the section on <a href=\"../container-communication/index#communicating-to-the-outside-world\">Communicating to the outside world</a> for details.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/custom-docker0/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/default_network/custom-docker0/</a>\n  </p>\n</div>\n","engine/security/index":"<h1 id=\"secure-engine\">Secure Engine</h1> <p>This section discusses the security features you can configure and use within your Docker Engine installation.</p> <ul> <li><p>You can configure Docker’s trust features so that your users can push and pull trusted images. To learn how to do this, see <a href=\"trust/index\">Use trusted images</a> in this section.</p></li> <li><p>You can protect the Docker daemon socket and ensure only trusted Docker client connections. For more information, <a href=\"https/index\">Protect the Docker daemon socket</a></p></li> <li><p>You can use certificate-based client-server authentication to verify a Docker daemon has the rights to access images on a registry. For more information, see <a href=\"certificates/index\">Using certificates for repository client verification</a>.</p></li> <li><p>You can configure secure computing mode (Seccomp) policies to secure system calls in a container. For more information, see <a href=\"seccomp/index\">Seccomp security profiles for Docker</a>.</p></li> <li><p>An AppArmor profile for Docker is installed with the official <em>.deb</em> packages. For information about this profile and overriding it, see <a href=\"apparmor/index\">AppArmor security profiles for Docker</a>.</p></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/security/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/security/</a>\n  </p>\n</div>\n","engine/userguide/networking/index":"<h1 id=\"docker-networks-feature-overview\">Docker networks feature overview</h1> <p>This sections explains how to use the Docker networks feature. This feature allows users to define their own networks and connect containers to them. Using this feature you can create a network on a single host or a network that spans across multiple hosts.</p> <ul> <li><a href=\"dockernetworks/index\">Understand Docker container networks</a></li> <li><a href=\"work-with-networks/index\">Work with network commands</a></li> <li><a href=\"get-started-overlay/index\">Get started with multi-host networking</a></li> </ul> <p>If you are already familiar with Docker’s default bridge network, <code>docker0</code> that network continues to be supported. It is created automatically in every installation. The default bridge network is also named <code>bridge</code>. To see a list of topics related to that network, read the articles listed in the <a href=\"default_network/index\">Docker default bridge network</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/</a>\n  </p>\n</div>\n","engine/examples/index":"<h1 id=\"dockerize-an-application\">Dockerize an application</h1> <p>This section contains the following:</p> <ul> <li><a href=\"mongodb/index\">Dockerizing MongoDB</a></li> <li>\n<a href=\"postgresql_service/index\">Dockerizing PostgreSQL</a><br>\n</li> <li>\n<a href=\"couchdb_data_volumes/index\">Dockerizing a CouchDB service</a><br>\n</li> <li><a href=\"running_redis_service/index\">Dockerizing a Redis service</a></li> <li><a href=\"apt-cacher-ng/index\">Dockerizing an apt-cacher-ng service</a></li> <li><a href=\"../userguide/containers/dockerizing/index\">Dockerizing applications: A ‘Hello world’</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/examples/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/examples/</a>\n  </p>\n</div>\n","engine/userguide/networking/default_network/ipv6/index":"<h1 id=\"ipv6-with-docker\">IPv6 with Docker</h1> <p>The information in this section explains IPv6 with the Docker default bridge. This is a <code>bridge</code> network named <code>bridge</code> created automatically when you install Docker.</p> <p>As we are <a href=\"http://en.wikipedia.org/wiki/IPv4_address_exhaustion\">running out of IPv4 addresses</a> the IETF has standardized an IPv4 successor, <a href=\"http://en.wikipedia.org/wiki/IPv6\">Internet Protocol Version 6</a> , in <a href=\"https://www.ietf.org/rfc/rfc2460.txt\">RFC 2460</a>. Both protocols, IPv4 and IPv6, reside on layer 3 of the <a href=\"http://en.wikipedia.org/wiki/OSI_model\">OSI model</a>.</p> <h2 id=\"how-ipv6-works-on-docker\">How IPv6 works on Docker</h2> <p>By default, the Docker server configures the container network for IPv4 only. You can enable IPv4/IPv6 dualstack support by running the Docker daemon with the <code>--ipv6</code> flag. Docker will set up the bridge <code>docker0</code> with the IPv6 <a href=\"http://en.wikipedia.org/wiki/Link-local_address\">link-local address</a> <code>fe80::1</code>.</p> <p>By default, containers that are created will only get a link-local IPv6 address. To assign globally routable IPv6 addresses to your containers you have to specify an IPv6 subnet to pick the addresses from. Set the IPv6 subnet via the <code>--fixed-cidr-v6</code> parameter when starting Docker daemon:</p> <pre>docker daemon --ipv6 --fixed-cidr-v6=\"2001:db8:1::/64\"\n</pre> <p>The subnet for Docker containers should at least have a size of <code>/80</code>. This way an IPv6 address can end with the container’s MAC address and you prevent NDP neighbor cache invalidation issues in the Docker layer.</p> <p>With the <code>--fixed-cidr-v6</code> parameter set Docker will add a new route to the routing table. Further IPv6 routing will be enabled (you may prevent this by starting Docker daemon with <code>--ip-forward=false</code>):</p> <pre>$ ip -6 route add 2001:db8:1::/64 dev docker0\n$ sysctl net.ipv6.conf.default.forwarding=1\n$ sysctl net.ipv6.conf.all.forwarding=1\n</pre> <p>All traffic to the subnet <code>2001:db8:1::/64</code> will now be routed via the <code>docker0</code> interface.</p> <p>Be aware that IPv6 forwarding may interfere with your existing IPv6 configuration: If you are using Router Advertisements to get IPv6 settings for your host’s interfaces you should set <code>accept_ra</code> to <code>2</code>. Otherwise IPv6 enabled forwarding will result in rejecting Router Advertisements. E.g., if you want to configure <code>eth0</code> via Router Advertisements you should set:</p> <pre>$ sysctl net.ipv6.conf.eth0.accept_ra=2\n</pre> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/images/ipv6_basic_host_config.svg\" alt=\"\"></p> <p>Every new container will get an IPv6 address from the defined subnet. Further a default route will be added on <code>eth0</code> in the container via the address specified by the daemon option <code>--default-gateway-v6</code> if present, otherwise via <code>fe80::1</code>:</p> <pre>docker run -it ubuntu bash -c \"ip -6 addr show dev eth0; ip -6 route show\"\n\n15: eth0: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500\n   inet6 2001:db8:1:0:0:242:ac11:3/64 scope global\n      valid_lft forever preferred_lft forever\n   inet6 fe80::42:acff:fe11:3/64 scope link\n      valid_lft forever preferred_lft forever\n\n2001:db8:1::/64 dev eth0  proto kernel  metric 256\nfe80::/64 dev eth0  proto kernel  metric 256\ndefault via fe80::1 dev eth0  metric 1024\n</pre> <p>In this example the Docker container is assigned a link-local address with the network suffix <code>/64</code> (here: <code>fe80::42:acff:fe11:3/64</code>) and a globally routable IPv6 address (here: <code>2001:db8:1:0:0:242:ac11:3/64</code>). The container will create connections to addresses outside of the <code>2001:db8:1::/64</code> network via the link-local gateway at <code>fe80::1</code> on <code>eth0</code>.</p> <p>Often servers or virtual machines get a <code>/64</code> IPv6 subnet assigned (e.g. <code>2001:db8:23:42::/64</code>). In this case you can split it up further and provide Docker a <code>/80</code> subnet while using a separate <code>/80</code> subnet for other applications on the host:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/images/ipv6_slash64_subnet_config.svg\" alt=\"\"></p> <p>In this setup the subnet <code>2001:db8:23:42::/80</code> with a range from <code>2001:db8:23:42:0:0:0:0</code> to <code>2001:db8:23:42:0:ffff:ffff:ffff</code> is attached to <code>eth0</code>, with the host listening at <code>2001:db8:23:42::1</code>. The subnet <code>2001:db8:23:42:1::/80</code> with an address range from <code>2001:db8:23:42:1:0:0:0</code> to <code>2001:db8:23:42:1:ffff:ffff:ffff</code> is attached to <code>docker0</code> and will be used by containers.</p> <h3 id=\"using-ndp-proxying\">Using NDP proxying</h3> <p>If your Docker host is only part of an IPv6 subnet but has not got an IPv6 subnet assigned you can use NDP proxying to connect your containers via IPv6 to the internet. For example your host has the IPv6 address <code>2001:db8::c001</code>, is part of the subnet <code>2001:db8::/64</code> and your IaaS provider allows you to configure the IPv6 addresses <code>2001:db8::c000</code> to <code>2001:db8::c00f</code>:</p> <pre>$ ip -6 addr show\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qlen 1000\n    inet6 2001:db8::c001/64 scope global\n       valid_lft forever preferred_lft forever\n    inet6 fe80::601:3fff:fea1:9c01/64 scope link\n       valid_lft forever preferred_lft forever\n</pre> <p>Let’s split up the configurable address range into two subnets <code>2001:db8::c000/125</code> and <code>2001:db8::c008/125</code>. The first one can be used by the host itself, the latter by Docker:</p> <pre>docker daemon --ipv6 --fixed-cidr-v6 2001:db8::c008/125\n</pre> <p>You notice the Docker subnet is within the subnet managed by your router that is connected to <code>eth0</code>. This means all devices (containers) with the addresses from the Docker subnet are expected to be found within the router subnet. Therefore the router thinks it can talk to these containers directly.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/images/ipv6_ndp_proxying.svg\" alt=\"\"></p> <p>As soon as the router wants to send an IPv6 packet to the first container it will transmit a neighbor solicitation request, asking, who has <code>2001:db8::c009</code>? But it will get no answer because no one on this subnet has this address. The container with this address is hidden behind the Docker host. The Docker host has to listen to neighbor solicitation requests for the container address and send a response that itself is the device that is responsible for the address. This is done by a Kernel feature called <code>NDP Proxy</code>. You can enable it by executing</p> <pre>$ sysctl net.ipv6.conf.eth0.proxy_ndp=1\n</pre> <p>Now you can add the container’s IPv6 address to the NDP proxy table:</p> <pre>$ ip -6 neigh add proxy 2001:db8::c009 dev eth0\n</pre> <p>This command tells the Kernel to answer to incoming neighbor solicitation requests regarding the IPv6 address <code>2001:db8::c009</code> on the device <code>eth0</code>. As a consequence of this all traffic to this IPv6 address will go into the Docker host and it will forward it according to its routing table via the <code>docker0</code> device to the container network:</p> <pre>$ ip -6 route show\n2001:db8::c008/125 dev docker0  metric 1\n2001:db8::/64 dev eth0  proto kernel  metric 256\n</pre> <p>You have to execute the <code>ip -6 neigh add proxy ...</code> command for every IPv6 address in your Docker subnet. Unfortunately there is no functionality for adding a whole subnet by executing one command. An alternative approach would be to use an NDP proxy daemon such as <a href=\"https://github.com/DanielAdolfsson/ndppd\">ndppd</a>.</p> <h2 id=\"docker-ipv6-cluster\">Docker IPv6 cluster</h2> <h3 id=\"switched-network-environment\">Switched network environment</h3> <p>Using routable IPv6 addresses allows you to realize communication between containers on different hosts. Let’s have a look at a simple Docker IPv6 cluster example:</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/images/ipv6_switched_network_example.svg\" alt=\"\"></p> <p>The Docker hosts are in the <code>2001:db8:0::/64</code> subnet. Host1 is configured to provide addresses from the <code>2001:db8:1::/64</code> subnet to its containers. It has three routes configured:</p> <ul> <li>Route all traffic to <code>2001:db8:0::/64</code> via <code>eth0</code>\n</li> <li>Route all traffic to <code>2001:db8:1::/64</code> via <code>docker0</code>\n</li> <li>Route all traffic to <code>2001:db8:2::/64</code> via Host2 with IP <code>2001:db8::2</code>\n</li> </ul> <p>Host1 also acts as a router on OSI layer 3. When one of the network clients tries to contact a target that is specified in Host1’s routing table Host1 will forward the traffic accordingly. It acts as a router for all networks it knows: <code>2001:db8::/64</code>, <code>2001:db8:1::/64</code> and <code>2001:db8:2::/64</code>.</p> <p>On Host2 we have nearly the same configuration. Host2’s containers will get IPv6 addresses from <code>2001:db8:2::/64</code>. Host2 has three routes configured:</p> <ul> <li>Route all traffic to <code>2001:db8:0::/64</code> via <code>eth0</code>\n</li> <li>Route all traffic to <code>2001:db8:2::/64</code> via <code>docker0</code>\n</li> <li>Route all traffic to <code>2001:db8:1::/64</code> via Host1 with IP <code>2001:db8:0::1</code>\n</li> </ul> <p>The difference to Host1 is that the network <code>2001:db8:2::/64</code> is directly attached to the host via its <code>docker0</code> interface whereas it reaches <code>2001:db8:1::/64</code> via Host1’s IPv6 address <code>2001:db8::1</code>.</p> <p>This way every container is able to contact every other container. The containers <code>Container1-*</code> share the same subnet and contact each other directly. The traffic between <code>Container1-*</code> and <code>Container2-*</code> will be routed via Host1 and Host2 because those containers do not share the same subnet.</p> <p>In a switched environment every host has to know all routes to every subnet. You always have to update the hosts’ routing tables once you add or remove a host to the cluster.</p> <p>Every configuration in the diagram that is shown below the dashed line is handled by Docker: The <code>docker0</code> bridge IP address configuration, the route to the Docker subnet on the host, the container IP addresses and the routes on the containers. The configuration above the line is up to the user and can be adapted to the individual environment.</p> <h3 id=\"routed-network-environment\">Routed network environment</h3> <p>In a routed network environment you replace the layer 2 switch with a layer 3 router. Now the hosts just have to know their default gateway (the router) and the route to their own containers (managed by Docker). The router holds all routing information about the Docker subnets. When you add or remove a host to this environment you just have to update the routing table in the router - not on every host.</p> <p><img src=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/images/ipv6_routed_network_example.svg\" alt=\"\"></p> <p>In this scenario containers of the same host can communicate directly with each other. The traffic between containers on different hosts will be routed via their hosts and the router. For example packet from <code>Container1-1</code> to <code>Container2-1</code> will be routed through <code>Host1</code>, <code>Router</code> and <code>Host2</code> until it arrives at <code>Container2-1</code>.</p> <p>To keep the IPv6 addresses short in this example a <code>/48</code> network is assigned to every host. The hosts use a <code>/64</code> subnet of this for its own services and one for Docker. When adding a third host you would add a route for the subnet <code>2001:db8:3::/48</code> in the router and configure Docker on Host3 with <code>--fixed-cidr-v6=2001:db8:3:1::/64</code>.</p> <p>Remember the subnet for Docker containers should at least have a size of <code>/80</code>. This way an IPv6 address can end with the container’s MAC address and you prevent NDP neighbor cache invalidation issues in the Docker layer. So if you have a <code>/64</code> for your whole environment use <code>/78</code> subnets for the hosts and <code>/80</code> for the containers. This way you can use 4096 hosts with 16 <code>/80</code> subnets each.</p> <p>Every configuration in the diagram that is visualized below the dashed line is handled by Docker: The <code>docker0</code> bridge IP address configuration, the route to the Docker subnet on the host, the container IP addresses and the routes on the containers. The configuration above the line is up to the user and can be adapted to the individual environment.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/ipv6/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/userguide/networking/default_network/ipv6/</a>\n  </p>\n</div>\n","swarm/swarm_at_scale/index":"<h1 id=\"try-swarm-at-scale\">Try Swarm at scale</h1> <p>Using this example, you deploy a voting application on a Swarm cluster. This example illustrates a typical development process. After you establish an infrastructure, you create a Swarm cluster and deploy the application against the cluster.</p> <p>After building and manually deploying the voting application, you’ll construct a Docker Compose file. You (or others) can use the file to deploy and scale the application further. The article also provides a troubleshooting section you can use while developing or deploying the voting application.</p> <p>The sample is written for a novice network administrator. You should have basic skills on Linux systems and <code>ssh</code> experience. Some knowledge of Git is also useful but not strictly required. This example takes approximately an hour to complete and has the following steps:</p> <ul> <li><a href=\"about/index\">Learn the application architecture</a></li> <li><a href=\"deploy-infra/index\">Deploy your infrastructure</a></li> <li><a href=\"deploy-app/index\">Deploy the application</a></li> <li><a href=\"troubleshoot/index\">Troubleshoot the application</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/swarm_at_scale/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/swarm_at_scale/</a>\n  </p>\n</div>\n","swarm/index":"<h1 id=\"docker-swarm\">Docker Swarm</h1> <ul> <li><a href=\"overview/index\">Docker Swarm overview</a></li> <li><a href=\"get-swarm/index\">How to get Docker Swarm</a></li> <li><a href=\"install-w-machine/index\">Evaluate Swarm in a sandbox</a></li> <li><a href=\"plan-for-production/index\">Plan for Swarm in production</a></li> <li><a href=\"install-manual/index\">Build a Swarm cluster for production</a></li> <li><a href=\"swarm_at_scale/index\">Try Swarm at scale</a></li> <li><a href=\"secure-swarm-tls/index\">Overview Swarm with TLS</a></li> <li><a href=\"configure-tls/index\">Configure Docker Swarm for TLS</a></li> <li><a href=\"discovery/index\">Docker Swarm Discovery</a></li> <li><a href=\"multi-manager-setup/index\">High availability in Docker Swarm</a></li> <li><a href=\"networking/index\">Swarm and container networks</a></li> <li><a href=\"https://docs.docker.com/v1.11/swarm/scheduler/\">Advanced Scheduling</a></li> <li><a href=\"provision-with-machine/index\">Provision a Swarm cluster with Docker Machine</a></li> <li><a href=\"swarm-api/index\">Docker Swarm API</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/swarm/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/swarm/</a>\n  </p>\n</div>\n","compose/reference/index":"<h1 id=\"compose-command-line-reference\">Compose command-line reference</h1> <p>The following pages describe the usage information for the <a href=\"overview/index\">docker-compose</a> subcommands. You can also see this information by running <code>docker-compose [SUBCOMMAND] --help</code> from the command line.</p> <ul> <li><a href=\"overview/index\">docker-compose</a></li> <li><a href=\"build/index\">build</a></li> <li><a href=\"config/index\">config</a></li> <li><a href=\"create/index\">create</a></li> <li><a href=\"down/index\">down</a></li> <li><a href=\"events/index\">events</a></li> <li><a href=\"help/index\">help</a></li> <li><a href=\"kill/index\">kill</a></li> <li><a href=\"logs/index\">logs</a></li> <li><a href=\"pause/index\">pause</a></li> <li><a href=\"port/index\">port</a></li> <li><a href=\"ps/index\">ps</a></li> <li><a href=\"pull/index\">pull</a></li> <li><a href=\"restart/index\">restart</a></li> <li><a href=\"rm/index\">rm</a></li> <li><a href=\"run/index\">run</a></li> <li><a href=\"scale/index\">scale</a></li> <li><a href=\"start/index\">start</a></li> <li><a href=\"stop/index\">stop</a></li> <li><a href=\"unpause/index\">unpause</a></li> <li><a href=\"up/index\">up</a></li> </ul> <h2 id=\"where-to-go-next\">Where to go next</h2> <ul> <li><a href=\"envvars/index\">CLI environment variables</a></li> <li><a href=\"overview/index\">docker-compose Command</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/reference/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/reference/</a>\n  </p>\n</div>\n","compose/index":"<h1 id=\"docker-compose\">Docker Compose</h1> <p>Compose is a tool for defining and running multi-container Docker applications. To learn more about Compose refer to the following documentation:</p> <ul> <li><a href=\"overview/index\">Compose Overview</a></li> <li><a href=\"install/index\">Install Compose</a></li> <li><a href=\"gettingstarted/index\">Getting Started</a></li> <li><a href=\"django/index\">Get started with Django</a></li> <li><a href=\"rails/index\">Get started with Rails</a></li> <li><a href=\"wordpress/index\">Get started with WordPress</a></li> <li><a href=\"faq/index\">Frequently asked questions</a></li> <li><a href=\"reference/index\">Command line reference</a></li> <li><a href=\"compose-file/index\">Compose file reference</a></li> <li><a href=\"env-file/index\">Environment file</a></li> </ul> <p>To see a detailed list of changes for past and current releases of Docker Compose, please refer to the <a href=\"https://github.com/docker/compose/blob/master/CHANGELOG.md\">CHANGELOG</a>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/compose/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/compose/</a>\n  </p>\n</div>\n","machine/drivers/index":"<h1 id=\"supported-drivers\">Supported Drivers</h1> <ul> <li><a href=\"aws/index\">Amazon Web Services</a></li> <li><a href=\"azure/index\">Microsoft Azure</a></li> <li><a href=\"digital-ocean/index\">Digital Ocean</a></li> <li><a href=\"exoscale/index\">Exoscale</a></li> <li><a href=\"gce/index\">Google Compute Engine</a></li> <li><a href=\"generic/index\">Generic</a></li> <li><a href=\"hyper-v/index\">Microsoft Hyper-V</a></li> <li><a href=\"openstack/index\">OpenStack</a></li> <li><a href=\"rackspace/index\">Rackspace</a></li> <li><a href=\"soft-layer/index\">IBM Softlayer</a></li> <li><a href=\"virtualbox/index\">Oracle VirtualBox</a></li> <li><a href=\"vm-cloud/index\">VMware vCloud Air</a></li> <li><a href=\"vm-fusion/index\">VMware Fusion</a></li> <li><a href=\"vsphere/index\">VMware vSphere</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/drivers/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/drivers/</a>\n  </p>\n</div>\n","machine/reference/index":"<h1 id=\"docker-machine-command-line-reference\">Docker Machine command line reference</h1> <ul> <li><a href=\"active/index\">active</a></li> <li><a href=\"config/index\">config</a></li> <li><a href=\"create/index\">create</a></li> <li><a href=\"env/index\">env</a></li> <li><a href=\"help/index\">help</a></li> <li><a href=\"inspect/index\">inspect</a></li> <li><a href=\"ip/index\">ip</a></li> <li><a href=\"kill/index\">kill</a></li> <li><a href=\"ls/index\">ls</a></li> <li><a href=\"regenerate-certs/index\">regenerate-certs</a></li> <li><a href=\"restart/index\">restart</a></li> <li><a href=\"rm/index\">rm</a></li> <li><a href=\"scp/index\">scp</a></li> <li><a href=\"ssh/index\">ssh</a></li> <li><a href=\"start/index\">start</a></li> <li><a href=\"status/index\">status</a></li> <li><a href=\"stop/index\">stop</a></li> <li><a href=\"upgrade/index\">upgrade</a></li> <li><a href=\"url/index\">url</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/machine/reference/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/machine/reference/</a>\n  </p>\n</div>\n","engine/index":"<h1 id=\"about-docker-engine\">About Docker Engine</h1> <p><strong>Develop, Ship and Run Any Application, Anywhere</strong></p> <p><a href=\"https://www.docker.com\"><strong>Docker</strong></a> is a platform for developers and sysadmins to develop, ship, and run applications. Docker lets you quickly assemble applications from components and eliminates the friction that can come when shipping code. Docker lets you get your code tested and deployed into production as fast as possible.</p> <p>Docker consists of:</p> <ul> <li>The Docker Engine - our lightweight and powerful open source containerization technology combined with a work flow for building and containerizing your applications.</li> <li>\n<a href=\"https://hub.docker.com\">Docker Hub</a> - our SaaS service for sharing and managing your application stacks.</li> </ul> <h2 id=\"why-docker\">Why Docker?</h2> <p><em>Faster delivery of your applications</em></p> <ul> <li>We want your environment to work better. Docker containers, and the work flow that comes with them, help your developers, sysadmins, QA folks, and release engineers work together to get your code into production and make it useful. We’ve created a standard container format that lets developers care about their applications inside containers while sysadmins and operators can work on running the container in your deployment. This separation of duties streamlines and simplifies the management and deployment of code.</li> <li>We make it easy to build new containers, enable rapid iteration of your applications, and increase the visibility of changes. This helps everyone in your organization understand how an application works and how it is built.</li> <li>Docker containers are lightweight and fast! Containers have sub-second launch times, reducing the cycle time of development, testing, and deployment.</li> </ul> <p><em>Deploy and scale more easily</em></p> <ul> <li>Docker containers run (almost) everywhere. You can deploy containers on desktops, physical servers, virtual machines, into data centers, and up to public and private clouds.</li> <li>Since Docker runs on so many platforms, it’s easy to move your applications around. You can easily move an application from a testing environment into the cloud and back whenever you need.</li> <li>Docker’s lightweight containers also make scaling up and down fast and easy. You can quickly launch more containers when needed and then shut them down easily when they’re no longer needed.</li> </ul> <p><em>Get higher density and run more workloads</em></p> <ul> <li>Docker containers don’t need a hypervisor, so you can pack more of them onto your hosts. This means you get more value out of every server and can potentially reduce what you spend on equipment and licenses.</li> </ul> <p><em>Faster deployment makes for easier management</em></p> <ul> <li>As Docker speeds up your work flow, it gets easier to make lots of small changes instead of huge, big bang updates. Smaller changes mean reduced risk and more uptime.</li> </ul> <h2 id=\"about-this-guide\">About this guide</h2> <p>The <a href=\"understanding-docker/index\">Understanding Docker section</a> will help you:</p> <ul> <li>See how Docker works at a high level</li> <li>Understand the architecture of Docker</li> <li>Discover Docker’s features;</li> <li>See how Docker compares to virtual machines</li> <li>See some common use cases.</li> </ul> <h3 id=\"installation-guides\">Installation guides</h3> <p>The <a href=\"installation/index\">installation section</a> will show you how to install Docker on a variety of platforms.</p> <h3 id=\"docker-user-guide\">Docker user guide</h3> <p>To learn about Docker in more detail and to answer questions about usage and implementation, check out the <a href=\"userguide/index\">Docker User Guide</a>.</p> <h2 id=\"release-notes\">Release notes</h2> <p>A summary of the changes in each release in the current series can now be found on the separate <a href=\"https://docs.docker.com/release-notes\">Release Notes page</a></p> <h2 id=\"feature-deprecation-policy\">Feature Deprecation Policy</h2> <p>As changes are made to Docker there may be times when existing features will need to be removed or replaced with newer features. Before an existing feature is removed it will be labeled as “deprecated” within the documentation and will remain in Docker for, usually, at least 2 releases. After that time it may be removed.</p> <p>Users are expected to take note of the list of deprecated features each release and plan their migration away from those features, and (if applicable) towards the replacement features as soon as possible.</p> <p>The complete list of deprecated features can be found on the <a href=\"deprecated/index\">Deprecated Features page</a>.</p> <h2 id=\"licensing\">Licensing</h2> <p>Docker is licensed under the Apache License, Version 2.0. See <a href=\"https://github.com/docker/docker/blob/master/LICENSE\">LICENSE</a> for the full license text.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2013&ndash;2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>\n    <a href=\"https://docs.docker.com/v1.11/engine/\" class=\"_attribution-link\">https://docs.docker.com/v1.11/engine/</a>\n  </p>\n</div>\n"}