{"index":"<h1 id=\"tensorflow-python-reference-documentation\">TensorFlow Python reference documentation</h1> <ul> <li>\n<p><strong><a href=\"framework\">Building Graphs</a></strong>:</p> <ul> <li><a href=\"framework#add_to_collection\"><code>add_to_collection</code></a></li> <li><a href=\"framework#as_dtype\"><code>as_dtype</code></a></li> <li><a href=\"framework#bytes\"><code>bytes</code></a></li> <li><a href=\"framework#container\"><code>container</code></a></li> <li><a href=\"framework#control_dependencies\"><code>control_dependencies</code></a></li> <li><a href=\"framework#convert_to_tensor\"><code>convert_to_tensor</code></a></li> <li><a href=\"framework#convert_to_tensor_or_indexed_slices\"><code>convert_to_tensor_or_indexed_slices</code></a></li> <li><a href=\"framework#device\"><code>device</code></a></li> <li><a href=\"framework#DeviceSpec\"><code>DeviceSpec</code></a></li> <li><a href=\"framework#Dimension\"><code>Dimension</code></a></li> <li><a href=\"framework#DType\"><code>DType</code></a></li> <li><a href=\"framework#get_collection\"><code>get_collection</code></a></li> <li><a href=\"framework#get_collection_ref\"><code>get_collection_ref</code></a></li> <li><a href=\"framework#get_default_graph\"><code>get_default_graph</code></a></li> <li><a href=\"framework#get_seed\"><code>get_seed</code></a></li> <li><a href=\"framework#Graph\"><code>Graph</code></a></li> <li><a href=\"framework#GraphKeys\"><code>GraphKeys</code></a></li> <li><a href=\"framework#import_graph_def\"><code>import_graph_def</code></a></li> <li><a href=\"framework#load_file_system_library\"><code>load_file_system_library</code></a></li> <li><a href=\"framework#load_op_library\"><code>load_op_library</code></a></li> <li><a href=\"framework#name_scope\"><code>name_scope</code></a></li> <li><a href=\"framework#NoGradient\"><code>NoGradient</code></a></li> <li><a href=\"framework#op_scope\"><code>op_scope</code></a></li> <li><a href=\"framework#Operation\"><code>Operation</code></a></li> <li><a href=\"framework#register_tensor_conversion_function\"><code>register_tensor_conversion_function</code></a></li> <li><a href=\"framework#RegisterGradient\"><code>RegisterGradient</code></a></li> <li><a href=\"framework#RegisterShape\"><code>RegisterShape</code></a></li> <li><a href=\"framework#reset_default_graph\"><code>reset_default_graph</code></a></li> <li><a href=\"framework#Tensor\"><code>Tensor</code></a></li> <li><a href=\"framework#TensorShape\"><code>TensorShape</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"check_ops\">Asserts and boolean checks.</a></strong>:</p> <ul> <li><a href=\"check_ops#assert_equal\"><code>assert_equal</code></a></li> <li><a href=\"check_ops#assert_integer\"><code>assert_integer</code></a></li> <li><a href=\"check_ops#assert_less\"><code>assert_less</code></a></li> <li><a href=\"check_ops#assert_less_equal\"><code>assert_less_equal</code></a></li> <li><a href=\"check_ops#assert_negative\"><code>assert_negative</code></a></li> <li><a href=\"check_ops#assert_non_negative\"><code>assert_non_negative</code></a></li> <li><a href=\"check_ops#assert_non_positive\"><code>assert_non_positive</code></a></li> <li><a href=\"check_ops#assert_positive\"><code>assert_positive</code></a></li> <li><a href=\"check_ops#assert_proper_iterable\"><code>assert_proper_iterable</code></a></li> <li><a href=\"check_ops#assert_rank\"><code>assert_rank</code></a></li> <li><a href=\"check_ops#assert_rank_at_least\"><code>assert_rank_at_least</code></a></li> <li><a href=\"check_ops#assert_type\"><code>assert_type</code></a></li> <li><a href=\"check_ops#is_non_decreasing\"><code>is_non_decreasing</code></a></li> <li><a href=\"check_ops#is_numeric_tensor\"><code>is_numeric_tensor</code></a></li> <li><a href=\"check_ops#is_strictly_increasing\"><code>is_strictly_increasing</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"constant_op\">Constants, Sequences, and Random Values</a></strong>:</p> <ul> <li><a href=\"constant_op#constant\"><code>constant</code></a></li> <li><a href=\"constant_op#fill\"><code>fill</code></a></li> <li><a href=\"constant_op#linspace\"><code>linspace</code></a></li> <li><a href=\"constant_op#multinomial\"><code>multinomial</code></a></li> <li><a href=\"constant_op#ones\"><code>ones</code></a></li> <li><a href=\"constant_op#ones_like\"><code>ones_like</code></a></li> <li><a href=\"constant_op#ops\"><code>ops</code></a></li> <li><a href=\"constant_op#random_crop\"><code>random_crop</code></a></li> <li><a href=\"constant_op#random_gamma\"><code>random_gamma</code></a></li> <li><a href=\"constant_op#random_normal\"><code>random_normal</code></a></li> <li><a href=\"constant_op#random_shuffle\"><code>random_shuffle</code></a></li> <li><a href=\"constant_op#random_uniform\"><code>random_uniform</code></a></li> <li><a href=\"constant_op#range\"><code>range</code></a></li> <li><a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a></li> <li><a href=\"constant_op#truncated_normal\"><code>truncated_normal</code></a></li> <li><a href=\"constant_op#zeros\"><code>zeros</code></a></li> <li><a href=\"constant_op#zeros_like\"><code>zeros_like</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"state_ops\">Variables</a></strong>:</p> <ul> <li><a href=\"state_ops#all_variables\"><code>all_variables</code></a></li> <li><a href=\"state_ops#assert_variables_initialized\"><code>assert_variables_initialized</code></a></li> <li><a href=\"state_ops#assign\"><code>assign</code></a></li> <li><a href=\"state_ops#assign_add\"><code>assign_add</code></a></li> <li><a href=\"state_ops#assign_sub\"><code>assign_sub</code></a></li> <li><a href=\"state_ops#constant_initializer\"><code>constant_initializer</code></a></li> <li><a href=\"state_ops#count_up_to\"><code>count_up_to</code></a></li> <li><a href=\"state_ops#device\"><code>device</code></a></li> <li><a href=\"state_ops#export_meta_graph\"><code>export_meta_graph</code></a></li> <li><a href=\"state_ops#get_checkpoint_state\"><code>get_checkpoint_state</code></a></li> <li><a href=\"state_ops#get_variable\"><code>get_variable</code></a></li> <li><a href=\"state_ops#get_variable_scope\"><code>get_variable_scope</code></a></li> <li><a href=\"state_ops#import_meta_graph\"><code>import_meta_graph</code></a></li> <li><a href=\"state_ops#IndexedSlices\"><code>IndexedSlices</code></a></li> <li><a href=\"state_ops#initialize_all_variables\"><code>initialize_all_variables</code></a></li> <li><a href=\"state_ops#initialize_local_variables\"><code>initialize_local_variables</code></a></li> <li><a href=\"state_ops#initialize_variables\"><code>initialize_variables</code></a></li> <li><a href=\"state_ops#is_variable_initialized\"><code>is_variable_initialized</code></a></li> <li><a href=\"state_ops#latest_checkpoint\"><code>latest_checkpoint</code></a></li> <li><a href=\"state_ops#local_variables\"><code>local_variables</code></a></li> <li><a href=\"state_ops#make_template\"><code>make_template</code></a></li> <li><a href=\"state_ops#min_max_variable_partitioner\"><code>min_max_variable_partitioner</code></a></li> <li><a href=\"state_ops#moving_average_variables\"><code>moving_average_variables</code></a></li> <li><a href=\"state_ops#no_regularizer\"><code>no_regularizer</code></a></li> <li><a href=\"state_ops#ones_initializer\"><code>ones_initializer</code></a></li> <li><a href=\"state_ops#random_normal_initializer\"><code>random_normal_initializer</code></a></li> <li><a href=\"state_ops#random_uniform_initializer\"><code>random_uniform_initializer</code></a></li> <li><a href=\"state_ops#report_uninitialized_variables\"><code>report_uninitialized_variables</code></a></li> <li><a href=\"state_ops#Saver\"><code>Saver</code></a></li> <li><a href=\"state_ops#scatter_add\"><code>scatter_add</code></a></li> <li><a href=\"state_ops#scatter_sub\"><code>scatter_sub</code></a></li> <li><a href=\"state_ops#scatter_update\"><code>scatter_update</code></a></li> <li><a href=\"state_ops#sparse_mask\"><code>sparse_mask</code></a></li> <li><a href=\"state_ops#trainable_variables\"><code>trainable_variables</code></a></li> <li><a href=\"state_ops#truncated_normal_initializer\"><code>truncated_normal_initializer</code></a></li> <li><a href=\"state_ops#uniform_unit_scaling_initializer\"><code>uniform_unit_scaling_initializer</code></a></li> <li><a href=\"state_ops#update_checkpoint_state\"><code>update_checkpoint_state</code></a></li> <li><a href=\"state_ops#Variable\"><code>Variable</code></a></li> <li><a href=\"state_ops#variable_axis_size_partitioner\"><code>variable_axis_size_partitioner</code></a></li> <li><a href=\"state_ops#variable_op_scope\"><code>variable_op_scope</code></a></li> <li><a href=\"state_ops#variable_scope\"><code>variable_scope</code></a></li> <li><a href=\"state_ops#VariableScope\"><code>VariableScope</code></a></li> <li><a href=\"state_ops#zeros_initializer\"><code>zeros_initializer</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"array_ops\">Tensor Transformations</a></strong>:</p> <ul> <li><a href=\"array_ops#batch_to_space\"><code>batch_to_space</code></a></li> <li><a href=\"array_ops#bitcast\"><code>bitcast</code></a></li> <li><a href=\"array_ops#boolean_mask\"><code>boolean_mask</code></a></li> <li><a href=\"array_ops#cast\"><code>cast</code></a></li> <li><a href=\"array_ops#concat\"><code>concat</code></a></li> <li><a href=\"array_ops#copy\"><code>copy</code></a></li> <li><a href=\"array_ops#depth_to_space\"><code>depth_to_space</code></a></li> <li><a href=\"array_ops#dynamic_partition\"><code>dynamic_partition</code></a></li> <li><a href=\"array_ops#dynamic_stitch\"><code>dynamic_stitch</code></a></li> <li><a href=\"array_ops#expand_dims\"><code>expand_dims</code></a></li> <li><a href=\"array_ops#extract_image_patches\"><code>extract_image_patches</code></a></li> <li><a href=\"array_ops#gather\"><code>gather</code></a></li> <li><a href=\"array_ops#gather_nd\"><code>gather_nd</code></a></li> <li><a href=\"array_ops#meshgrid\"><code>meshgrid</code></a></li> <li><a href=\"array_ops#one_hot\"><code>one_hot</code></a></li> <li><a href=\"array_ops#pack\"><code>pack</code></a></li> <li><a href=\"array_ops#pad\"><code>pad</code></a></li> <li><a href=\"array_ops#rank\"><code>rank</code></a></li> <li><a href=\"array_ops#reshape\"><code>reshape</code></a></li> <li><a href=\"array_ops#reverse\"><code>reverse</code></a></li> <li><a href=\"array_ops#reverse_sequence\"><code>reverse_sequence</code></a></li> <li><a href=\"array_ops#saturate_cast\"><code>saturate_cast</code></a></li> <li><a href=\"array_ops#shape\"><code>shape</code></a></li> <li><a href=\"array_ops#shape_n\"><code>shape_n</code></a></li> <li><a href=\"array_ops#size\"><code>size</code></a></li> <li><a href=\"array_ops#slice\"><code>slice</code></a></li> <li><a href=\"array_ops#space_to_batch\"><code>space_to_batch</code></a></li> <li><a href=\"array_ops#space_to_depth\"><code>space_to_depth</code></a></li> <li><a href=\"array_ops#split\"><code>split</code></a></li> <li><a href=\"array_ops#squeeze\"><code>squeeze</code></a></li> <li><a href=\"array_ops#strided_slice\"><code>strided_slice</code></a></li> <li><a href=\"array_ops#string_to_number\"><code>string_to_number</code></a></li> <li><a href=\"array_ops#tile\"><code>tile</code></a></li> <li><a href=\"array_ops#to_bfloat16\"><code>to_bfloat16</code></a></li> <li><a href=\"array_ops#to_double\"><code>to_double</code></a></li> <li><a href=\"array_ops#to_float\"><code>to_float</code></a></li> <li><a href=\"array_ops#to_int32\"><code>to_int32</code></a></li> <li><a href=\"array_ops#to_int64\"><code>to_int64</code></a></li> <li><a href=\"array_ops#transpose\"><code>transpose</code></a></li> <li><a href=\"array_ops#unique_with_counts\"><code>unique_with_counts</code></a></li> <li><a href=\"array_ops#unpack\"><code>unpack</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"math_ops\">Math</a></strong>:</p> <ul> <li><a href=\"math_ops#abs\"><code>abs</code></a></li> <li><a href=\"math_ops#accumulate_n\"><code>accumulate_n</code></a></li> <li><a href=\"math_ops#acos\"><code>acos</code></a></li> <li><a href=\"math_ops#add\"><code>add</code></a></li> <li><a href=\"math_ops#add_n\"><code>add_n</code></a></li> <li><a href=\"math_ops#argmax\"><code>argmax</code></a></li> <li><a href=\"math_ops#argmin\"><code>argmin</code></a></li> <li><a href=\"math_ops#asin\"><code>asin</code></a></li> <li><a href=\"math_ops#atan\"><code>atan</code></a></li> <li><a href=\"math_ops#batch_cholesky\"><code>batch_cholesky</code></a></li> <li><a href=\"math_ops#batch_cholesky_solve\"><code>batch_cholesky_solve</code></a></li> <li><a href=\"math_ops#batch_fft\"><code>batch_fft</code></a></li> <li><a href=\"math_ops#batch_fft2d\"><code>batch_fft2d</code></a></li> <li><a href=\"math_ops#batch_fft3d\"><code>batch_fft3d</code></a></li> <li><a href=\"math_ops#batch_ifft\"><code>batch_ifft</code></a></li> <li><a href=\"math_ops#batch_ifft2d\"><code>batch_ifft2d</code></a></li> <li><a href=\"math_ops#batch_ifft3d\"><code>batch_ifft3d</code></a></li> <li><a href=\"math_ops#batch_matmul\"><code>batch_matmul</code></a></li> <li><a href=\"math_ops#batch_matrix_band_part\"><code>batch_matrix_band_part</code></a></li> <li><a href=\"math_ops#batch_matrix_determinant\"><code>batch_matrix_determinant</code></a></li> <li><a href=\"math_ops#batch_matrix_diag\"><code>batch_matrix_diag</code></a></li> <li><a href=\"math_ops#batch_matrix_diag_part\"><code>batch_matrix_diag_part</code></a></li> <li><a href=\"math_ops#batch_matrix_inverse\"><code>batch_matrix_inverse</code></a></li> <li><a href=\"math_ops#batch_matrix_set_diag\"><code>batch_matrix_set_diag</code></a></li> <li><a href=\"math_ops#batch_matrix_solve\"><code>batch_matrix_solve</code></a></li> <li><a href=\"math_ops#batch_matrix_solve_ls\"><code>batch_matrix_solve_ls</code></a></li> <li><a href=\"math_ops#batch_matrix_transpose\"><code>batch_matrix_transpose</code></a></li> <li><a href=\"math_ops#batch_matrix_triangular_solve\"><code>batch_matrix_triangular_solve</code></a></li> <li><a href=\"math_ops#batch_self_adjoint_eig\"><code>batch_self_adjoint_eig</code></a></li> <li><a href=\"math_ops#batch_self_adjoint_eigvals\"><code>batch_self_adjoint_eigvals</code></a></li> <li><a href=\"math_ops#batch_svd\"><code>batch_svd</code></a></li> <li><a href=\"math_ops#ceil\"><code>ceil</code></a></li> <li><a href=\"math_ops#cholesky\"><code>cholesky</code></a></li> <li><a href=\"math_ops#cholesky_solve\"><code>cholesky_solve</code></a></li> <li><a href=\"math_ops#complex\"><code>complex</code></a></li> <li><a href=\"math_ops#complex_abs\"><code>complex_abs</code></a></li> <li><a href=\"math_ops#conj\"><code>conj</code></a></li> <li><a href=\"math_ops#cos\"><code>cos</code></a></li> <li><a href=\"math_ops#cross\"><code>cross</code></a></li> <li><a href=\"math_ops#cumprod\"><code>cumprod</code></a></li> <li><a href=\"math_ops#cumsum\"><code>cumsum</code></a></li> <li><a href=\"math_ops#diag\"><code>diag</code></a></li> <li><a href=\"math_ops#diag_part\"><code>diag_part</code></a></li> <li><a href=\"math_ops#digamma\"><code>digamma</code></a></li> <li><a href=\"math_ops#div\"><code>div</code></a></li> <li><a href=\"math_ops#edit_distance\"><code>edit_distance</code></a></li> <li><a href=\"math_ops#erf\"><code>erf</code></a></li> <li><a href=\"math_ops#erfc\"><code>erfc</code></a></li> <li><a href=\"math_ops#exp\"><code>exp</code></a></li> <li><a href=\"math_ops#fft\"><code>fft</code></a></li> <li><a href=\"math_ops#fft2d\"><code>fft2d</code></a></li> <li><a href=\"math_ops#fft3d\"><code>fft3d</code></a></li> <li><a href=\"math_ops#floor\"><code>floor</code></a></li> <li><a href=\"math_ops#floordiv\"><code>floordiv</code></a></li> <li><a href=\"math_ops#ifft\"><code>ifft</code></a></li> <li><a href=\"math_ops#ifft2d\"><code>ifft2d</code></a></li> <li><a href=\"math_ops#ifft3d\"><code>ifft3d</code></a></li> <li><a href=\"math_ops#igamma\"><code>igamma</code></a></li> <li><a href=\"math_ops#igammac\"><code>igammac</code></a></li> <li><a href=\"math_ops#imag\"><code>imag</code></a></li> <li><a href=\"math_ops#inv\"><code>inv</code></a></li> <li><a href=\"math_ops#invert_permutation\"><code>invert_permutation</code></a></li> <li><a href=\"math_ops#lbeta\"><code>lbeta</code></a></li> <li><a href=\"math_ops#lgamma\"><code>lgamma</code></a></li> <li><a href=\"math_ops#listdiff\"><code>listdiff</code></a></li> <li><a href=\"math_ops#log\"><code>log</code></a></li> <li><a href=\"math_ops#matmul\"><code>matmul</code></a></li> <li><a href=\"math_ops#matrix_determinant\"><code>matrix_determinant</code></a></li> <li><a href=\"math_ops#matrix_inverse\"><code>matrix_inverse</code></a></li> <li><a href=\"math_ops#matrix_solve\"><code>matrix_solve</code></a></li> <li><a href=\"math_ops#matrix_solve_ls\"><code>matrix_solve_ls</code></a></li> <li><a href=\"math_ops#matrix_triangular_solve\"><code>matrix_triangular_solve</code></a></li> <li><a href=\"math_ops#maximum\"><code>maximum</code></a></li> <li><a href=\"math_ops#minimum\"><code>minimum</code></a></li> <li><a href=\"math_ops#mod\"><code>mod</code></a></li> <li><a href=\"math_ops#mul\"><code>mul</code></a></li> <li><a href=\"math_ops#neg\"><code>neg</code></a></li> <li><a href=\"math_ops#polygamma\"><code>polygamma</code></a></li> <li><a href=\"math_ops#pow\"><code>pow</code></a></li> <li><a href=\"math_ops#real\"><code>real</code></a></li> <li><a href=\"math_ops#reduce_all\"><code>reduce_all</code></a></li> <li><a href=\"math_ops#reduce_any\"><code>reduce_any</code></a></li> <li><a href=\"math_ops#reduce_max\"><code>reduce_max</code></a></li> <li><a href=\"math_ops#reduce_mean\"><code>reduce_mean</code></a></li> <li><a href=\"math_ops#reduce_min\"><code>reduce_min</code></a></li> <li><a href=\"math_ops#reduce_prod\"><code>reduce_prod</code></a></li> <li><a href=\"math_ops#reduce_sum\"><code>reduce_sum</code></a></li> <li><a href=\"math_ops#round\"><code>round</code></a></li> <li><a href=\"math_ops#rsqrt\"><code>rsqrt</code></a></li> <li><a href=\"math_ops#scalar_mul\"><code>scalar_mul</code></a></li> <li><a href=\"math_ops#segment_max\"><code>segment_max</code></a></li> <li><a href=\"math_ops#segment_mean\"><code>segment_mean</code></a></li> <li><a href=\"math_ops#segment_min\"><code>segment_min</code></a></li> <li><a href=\"math_ops#segment_prod\"><code>segment_prod</code></a></li> <li><a href=\"math_ops#segment_sum\"><code>segment_sum</code></a></li> <li><a href=\"math_ops#self_adjoint_eig\"><code>self_adjoint_eig</code></a></li> <li><a href=\"math_ops#self_adjoint_eigvals\"><code>self_adjoint_eigvals</code></a></li> <li><a href=\"math_ops#sign\"><code>sign</code></a></li> <li><a href=\"math_ops#sin\"><code>sin</code></a></li> <li><a href=\"math_ops#sparse_segment_mean\"><code>sparse_segment_mean</code></a></li> <li><a href=\"math_ops#sparse_segment_sqrt_n\"><code>sparse_segment_sqrt_n</code></a></li> <li><a href=\"math_ops#sparse_segment_sqrt_n_grad\"><code>sparse_segment_sqrt_n_grad</code></a></li> <li><a href=\"math_ops#sparse_segment_sum\"><code>sparse_segment_sum</code></a></li> <li><a href=\"math_ops#sqrt\"><code>sqrt</code></a></li> <li><a href=\"math_ops#square\"><code>square</code></a></li> <li><a href=\"math_ops#squared_difference\"><code>squared_difference</code></a></li> <li><a href=\"math_ops#sub\"><code>sub</code></a></li> <li><a href=\"math_ops#svd\"><code>svd</code></a></li> <li><a href=\"math_ops#tan\"><code>tan</code></a></li> <li><a href=\"math_ops#trace\"><code>trace</code></a></li> <li><a href=\"math_ops#transpose\"><code>transpose</code></a></li> <li><a href=\"math_ops#truediv\"><code>truediv</code></a></li> <li><a href=\"math_ops#unique\"><code>unique</code></a></li> <li><a href=\"math_ops#unsorted_segment_sum\"><code>unsorted_segment_sum</code></a></li> <li><a href=\"math_ops#where\"><code>where</code></a></li> <li><a href=\"math_ops#zeta\"><code>zeta</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"string_ops\">Strings</a></strong>:</p> <ul> <li><a href=\"string_ops#as_string\"><code>as_string</code></a></li> <li><a href=\"string_ops#reduce_join\"><code>reduce_join</code></a></li> <li><a href=\"string_ops#string_join\"><code>string_join</code></a></li> <li><a href=\"string_ops#string_to_hash_bucket\"><code>string_to_hash_bucket</code></a></li> <li><a href=\"string_ops#string_to_hash_bucket_fast\"><code>string_to_hash_bucket_fast</code></a></li> <li><a href=\"string_ops#string_to_hash_bucket_strong\"><code>string_to_hash_bucket_strong</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"histogram_ops\">Histograms</a></strong>:</p> <ul> <li><a href=\"histogram_ops#histogram_fixed_width\"><code>histogram_fixed_width</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"control_flow_ops\">Control Flow</a></strong>:</p> <ul> <li><a href=\"control_flow_ops#add_check_numerics_ops\"><code>add_check_numerics_ops</code></a></li> <li><a href=\"control_flow_ops#Assert\"><code>Assert</code></a></li> <li><a href=\"control_flow_ops#case\"><code>case</code></a></li> <li><a href=\"control_flow_ops#check_numerics\"><code>check_numerics</code></a></li> <li><a href=\"control_flow_ops#cond\"><code>cond</code></a></li> <li><a href=\"control_flow_ops#count_up_to\"><code>count_up_to</code></a></li> <li><a href=\"control_flow_ops#equal\"><code>equal</code></a></li> <li><a href=\"control_flow_ops#greater\"><code>greater</code></a></li> <li><a href=\"control_flow_ops#greater_equal\"><code>greater_equal</code></a></li> <li><a href=\"control_flow_ops#group\"><code>group</code></a></li> <li><a href=\"control_flow_ops#identity\"><code>identity</code></a></li> <li><a href=\"control_flow_ops#is_finite\"><code>is_finite</code></a></li> <li><a href=\"control_flow_ops#is_inf\"><code>is_inf</code></a></li> <li><a href=\"control_flow_ops#is_nan\"><code>is_nan</code></a></li> <li><a href=\"control_flow_ops#less\"><code>less</code></a></li> <li><a href=\"control_flow_ops#less_equal\"><code>less_equal</code></a></li> <li><a href=\"control_flow_ops#logical_and\"><code>logical_and</code></a></li> <li><a href=\"control_flow_ops#logical_not\"><code>logical_not</code></a></li> <li><a href=\"control_flow_ops#logical_or\"><code>logical_or</code></a></li> <li><a href=\"control_flow_ops#logical_xor\"><code>logical_xor</code></a></li> <li><a href=\"control_flow_ops#no_op\"><code>no_op</code></a></li> <li><a href=\"control_flow_ops#not_equal\"><code>not_equal</code></a></li> <li><a href=\"control_flow_ops#Print\"><code>Print</code></a></li> <li><a href=\"control_flow_ops#select\"><code>select</code></a></li> <li><a href=\"control_flow_ops#tuple\"><code>tuple</code></a></li> <li><a href=\"control_flow_ops#verify_tensor_all_finite\"><code>verify_tensor_all_finite</code></a></li> <li><a href=\"control_flow_ops#where\"><code>where</code></a></li> <li><a href=\"control_flow_ops#while_loop\"><code>while_loop</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"functional_ops\">Higher Order Functions</a></strong>:</p> <ul> <li><a href=\"functional_ops#foldl\"><code>foldl</code></a></li> <li><a href=\"functional_ops#foldr\"><code>foldr</code></a></li> <li><a href=\"functional_ops#map_fn\"><code>map_fn</code></a></li> <li><a href=\"functional_ops#scan\"><code>scan</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"tensor_array_ops\">TensorArray Operations</a></strong>:</p> <ul> <li><a href=\"tensor_array_ops#concat\"><code>concat</code></a></li> <li><a href=\"tensor_array_ops#pack\"><code>pack</code></a></li> <li><a href=\"tensor_array_ops#split\"><code>split</code></a></li> <li><a href=\"tensor_array_ops#TensorArray\"><code>TensorArray</code></a></li> <li><a href=\"tensor_array_ops#unpack\"><code>unpack</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"session_ops\">Tensor Handle Operations</a></strong>:</p> <ul> <li><a href=\"session_ops#delete_session_tensor\"><code>delete_session_tensor</code></a></li> <li><a href=\"session_ops#get_session_handle\"><code>get_session_handle</code></a></li> <li><a href=\"session_ops#get_session_tensor\"><code>get_session_tensor</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"image\">Images</a></strong>:</p> <ul> <li><a href=\"image#adjust_brightness\"><code>adjust_brightness</code></a></li> <li><a href=\"image#adjust_contrast\"><code>adjust_contrast</code></a></li> <li><a href=\"image#adjust_hue\"><code>adjust_hue</code></a></li> <li><a href=\"image#adjust_saturation\"><code>adjust_saturation</code></a></li> <li><a href=\"image#central_crop\"><code>central_crop</code></a></li> <li><a href=\"image#convert_image_dtype\"><code>convert_image_dtype</code></a></li> <li><a href=\"image#crop_and_resize\"><code>crop_and_resize</code></a></li> <li><a href=\"image#crop_to_bounding_box\"><code>crop_to_bounding_box</code></a></li> <li><a href=\"image#decode_jpeg\"><code>decode_jpeg</code></a></li> <li><a href=\"image#decode_png\"><code>decode_png</code></a></li> <li><a href=\"image#draw_bounding_boxes\"><code>draw_bounding_boxes</code></a></li> <li><a href=\"image#encode_jpeg\"><code>encode_jpeg</code></a></li> <li><a href=\"image#encode_png\"><code>encode_png</code></a></li> <li><a href=\"image#extract_glimpse\"><code>extract_glimpse</code></a></li> <li><a href=\"image#flip_left_right\"><code>flip_left_right</code></a></li> <li><a href=\"image#flip_up_down\"><code>flip_up_down</code></a></li> <li><a href=\"image#grayscale_to_rgb\"><code>grayscale_to_rgb</code></a></li> <li><a href=\"image#hsv_to_rgb\"><code>hsv_to_rgb</code></a></li> <li><a href=\"image#non_max_suppression\"><code>non_max_suppression</code></a></li> <li><a href=\"image#pad_to_bounding_box\"><code>pad_to_bounding_box</code></a></li> <li><a href=\"image#per_image_whitening\"><code>per_image_whitening</code></a></li> <li><a href=\"image#random_brightness\"><code>random_brightness</code></a></li> <li><a href=\"image#random_contrast\"><code>random_contrast</code></a></li> <li><a href=\"image#random_flip_left_right\"><code>random_flip_left_right</code></a></li> <li><a href=\"image#random_flip_up_down\"><code>random_flip_up_down</code></a></li> <li><a href=\"image#random_hue\"><code>random_hue</code></a></li> <li><a href=\"image#random_saturation\"><code>random_saturation</code></a></li> <li><a href=\"image#resize_area\"><code>resize_area</code></a></li> <li><a href=\"image#resize_bicubic\"><code>resize_bicubic</code></a></li> <li><a href=\"image#resize_bilinear\"><code>resize_bilinear</code></a></li> <li><a href=\"image#resize_image_with_crop_or_pad\"><code>resize_image_with_crop_or_pad</code></a></li> <li><a href=\"image#resize_images\"><code>resize_images</code></a></li> <li><a href=\"image#resize_nearest_neighbor\"><code>resize_nearest_neighbor</code></a></li> <li><a href=\"image#rgb_to_grayscale\"><code>rgb_to_grayscale</code></a></li> <li><a href=\"image#rgb_to_hsv\"><code>rgb_to_hsv</code></a></li> <li><a href=\"image#rot90\"><code>rot90</code></a></li> <li><a href=\"image#sample_distorted_bounding_box\"><code>sample_distorted_bounding_box</code></a></li> <li><a href=\"image#transpose_image\"><code>transpose_image</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"sparse_ops\">Sparse Tensors</a></strong>:</p> <ul> <li><a href=\"sparse_ops#shape\"><code>shape</code></a></li> <li><a href=\"sparse_ops#sparse_add\"><code>sparse_add</code></a></li> <li><a href=\"sparse_ops#sparse_concat\"><code>sparse_concat</code></a></li> <li><a href=\"sparse_ops#sparse_fill_empty_rows\"><code>sparse_fill_empty_rows</code></a></li> <li><a href=\"sparse_ops#sparse_maximum\"><code>sparse_maximum</code></a></li> <li><a href=\"sparse_ops#sparse_merge\"><code>sparse_merge</code></a></li> <li><a href=\"sparse_ops#sparse_minimum\"><code>sparse_minimum</code></a></li> <li><a href=\"sparse_ops#sparse_reduce_sum\"><code>sparse_reduce_sum</code></a></li> <li><a href=\"sparse_ops#sparse_reorder\"><code>sparse_reorder</code></a></li> <li><a href=\"sparse_ops#sparse_reset_shape\"><code>sparse_reset_shape</code></a></li> <li><a href=\"sparse_ops#sparse_reshape\"><code>sparse_reshape</code></a></li> <li><a href=\"sparse_ops#sparse_retain\"><code>sparse_retain</code></a></li> <li><a href=\"sparse_ops#sparse_softmax\"><code>sparse_softmax</code></a></li> <li><a href=\"sparse_ops#sparse_split\"><code>sparse_split</code></a></li> <li><a href=\"sparse_ops#sparse_tensor_dense_matmul\"><code>sparse_tensor_dense_matmul</code></a></li> <li><a href=\"sparse_ops#sparse_tensor_to_dense\"><code>sparse_tensor_to_dense</code></a></li> <li><a href=\"sparse_ops#sparse_to_dense\"><code>sparse_to_dense</code></a></li> <li><a href=\"sparse_ops#sparse_to_indicator\"><code>sparse_to_indicator</code></a></li> <li><a href=\"sparse_ops#SparseTensor\"><code>SparseTensor</code></a></li> <li><a href=\"sparse_ops#SparseTensorValue\"><code>SparseTensorValue</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"io_ops\">Inputs and Readers</a></strong>:</p> <ul> <li><a href=\"io_ops#batch\"><code>batch</code></a></li> <li><a href=\"io_ops#batch_join\"><code>batch_join</code></a></li> <li><a href=\"io_ops#decode_csv\"><code>decode_csv</code></a></li> <li><a href=\"io_ops#decode_json_example\"><code>decode_json_example</code></a></li> <li><a href=\"io_ops#decode_raw\"><code>decode_raw</code></a></li> <li><a href=\"io_ops#FIFOQueue\"><code>FIFOQueue</code></a></li> <li><a href=\"io_ops#FixedLenFeature\"><code>FixedLenFeature</code></a></li> <li><a href=\"io_ops#FixedLengthRecordReader\"><code>FixedLengthRecordReader</code></a></li> <li><a href=\"io_ops#FixedLenSequenceFeature\"><code>FixedLenSequenceFeature</code></a></li> <li><a href=\"io_ops#IdentityReader\"><code>IdentityReader</code></a></li> <li><a href=\"io_ops#input_producer\"><code>input_producer</code></a></li> <li><a href=\"io_ops#limit_epochs\"><code>limit_epochs</code></a></li> <li><a href=\"io_ops#match_filenames_once\"><code>match_filenames_once</code></a></li> <li><a href=\"io_ops#matching_files\"><code>matching_files</code></a></li> <li><a href=\"io_ops#PaddingFIFOQueue\"><code>PaddingFIFOQueue</code></a></li> <li><a href=\"io_ops#parse_example\"><code>parse_example</code></a></li> <li><a href=\"io_ops#parse_single_example\"><code>parse_single_example</code></a></li> <li><a href=\"io_ops#placeholder\"><code>placeholder</code></a></li> <li><a href=\"io_ops#placeholder_with_default\"><code>placeholder_with_default</code></a></li> <li><a href=\"io_ops#QueueBase\"><code>QueueBase</code></a></li> <li><a href=\"io_ops#RandomShuffleQueue\"><code>RandomShuffleQueue</code></a></li> <li><a href=\"io_ops#range_input_producer\"><code>range_input_producer</code></a></li> <li><a href=\"io_ops#read_file\"><code>read_file</code></a></li> <li><a href=\"io_ops#ReaderBase\"><code>ReaderBase</code></a></li> <li><a href=\"io_ops#shuffle_batch\"><code>shuffle_batch</code></a></li> <li><a href=\"io_ops#shuffle_batch_join\"><code>shuffle_batch_join</code></a></li> <li><a href=\"io_ops#size\"><code>size</code></a></li> <li><a href=\"io_ops#slice_input_producer\"><code>slice_input_producer</code></a></li> <li><a href=\"io_ops#sparse_placeholder\"><code>sparse_placeholder</code></a></li> <li><a href=\"io_ops#string_input_producer\"><code>string_input_producer</code></a></li> <li><a href=\"io_ops#TextLineReader\"><code>TextLineReader</code></a></li> <li><a href=\"io_ops#TFRecordReader\"><code>TFRecordReader</code></a></li> <li><a href=\"io_ops#VarLenFeature\"><code>VarLenFeature</code></a></li> <li><a href=\"io_ops#WholeFileReader\"><code>WholeFileReader</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"python_io\">Data IO (Python functions)</a></strong>:</p> <ul> <li><a href=\"python_io#tf_record_iterator\"><code>tf_record_iterator</code></a></li> <li><a href=\"python_io#TFRecordWriter\"><code>TFRecordWriter</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"nn\">Neural Network</a></strong>:</p> <ul> <li><a href=\"nn#atrous_conv2d\"><code>atrous_conv2d</code></a></li> <li><a href=\"nn#avg_pool\"><code>avg_pool</code></a></li> <li><a href=\"nn#avg_pool3d\"><code>avg_pool3d</code></a></li> <li><a href=\"nn#batch_normalization\"><code>batch_normalization</code></a></li> <li><a href=\"nn#bias_add\"><code>bias_add</code></a></li> <li><a href=\"nn#bidirectional_rnn\"><code>bidirectional_rnn</code></a></li> <li><a href=\"nn#compute_accidental_hits\"><code>compute_accidental_hits</code></a></li> <li><a href=\"nn#conv2d\"><code>conv2d</code></a></li> <li><a href=\"nn#conv2d_transpose\"><code>conv2d_transpose</code></a></li> <li><a href=\"nn#conv3d\"><code>conv3d</code></a></li> <li><a href=\"nn#ctc_beam_search_decoder\"><code>ctc_beam_search_decoder</code></a></li> <li><a href=\"nn#ctc_greedy_decoder\"><code>ctc_greedy_decoder</code></a></li> <li><a href=\"nn#ctc_loss\"><code>ctc_loss</code></a></li> <li><a href=\"nn#depthwise_conv2d\"><code>depthwise_conv2d</code></a></li> <li><a href=\"nn#depthwise_conv2d_native\"><code>depthwise_conv2d_native</code></a></li> <li><a href=\"nn#dilation2d\"><code>dilation2d</code></a></li> <li><a href=\"nn#dropout\"><code>dropout</code></a></li> <li><a href=\"nn#dynamic_rnn\"><code>dynamic_rnn</code></a></li> <li><a href=\"nn#elu\"><code>elu</code></a></li> <li><a href=\"nn#embedding_lookup\"><code>embedding_lookup</code></a></li> <li><a href=\"nn#embedding_lookup_sparse\"><code>embedding_lookup_sparse</code></a></li> <li><a href=\"nn#erosion2d\"><code>erosion2d</code></a></li> <li><a href=\"nn#fixed_unigram_candidate_sampler\"><code>fixed_unigram_candidate_sampler</code></a></li> <li><a href=\"nn#in_top_k\"><code>in_top_k</code></a></li> <li><a href=\"nn#l2_loss\"><code>l2_loss</code></a></li> <li><a href=\"nn#l2_normalize\"><code>l2_normalize</code></a></li> <li><a href=\"nn#learned_unigram_candidate_sampler\"><code>learned_unigram_candidate_sampler</code></a></li> <li><a href=\"nn#local_response_normalization\"><code>local_response_normalization</code></a></li> <li><a href=\"nn#log_poisson_loss\"><code>log_poisson_loss</code></a></li> <li><a href=\"nn#log_softmax\"><code>log_softmax</code></a></li> <li><a href=\"nn#log_uniform_candidate_sampler\"><code>log_uniform_candidate_sampler</code></a></li> <li><a href=\"nn#max_pool\"><code>max_pool</code></a></li> <li><a href=\"nn#max_pool3d\"><code>max_pool3d</code></a></li> <li><a href=\"nn#max_pool_with_argmax\"><code>max_pool_with_argmax</code></a></li> <li><a href=\"nn#moments\"><code>moments</code></a></li> <li><a href=\"nn#nce_loss\"><code>nce_loss</code></a></li> <li><a href=\"nn#normalize_moments\"><code>normalize_moments</code></a></li> <li><a href=\"nn#relu\"><code>relu</code></a></li> <li><a href=\"nn#relu6\"><code>relu6</code></a></li> <li><a href=\"nn#rnn\"><code>rnn</code></a></li> <li><a href=\"nn#sampled_softmax_loss\"><code>sampled_softmax_loss</code></a></li> <li><a href=\"nn#separable_conv2d\"><code>separable_conv2d</code></a></li> <li><a href=\"nn#sigmoid\"><code>sigmoid</code></a></li> <li><a href=\"nn#sigmoid_cross_entropy_with_logits\"><code>sigmoid_cross_entropy_with_logits</code></a></li> <li><a href=\"nn#softmax\"><code>softmax</code></a></li> <li><a href=\"nn#softmax_cross_entropy_with_logits\"><code>softmax_cross_entropy_with_logits</code></a></li> <li><a href=\"nn#softplus\"><code>softplus</code></a></li> <li><a href=\"nn#softsign\"><code>softsign</code></a></li> <li><a href=\"nn#sparse_softmax_cross_entropy_with_logits\"><code>sparse_softmax_cross_entropy_with_logits</code></a></li> <li><a href=\"nn#state_saving_rnn\"><code>state_saving_rnn</code></a></li> <li><a href=\"nn#sufficient_statistics\"><code>sufficient_statistics</code></a></li> <li><a href=\"nn#tanh\"><code>tanh</code></a></li> <li><a href=\"nn#top_k\"><code>top_k</code></a></li> <li><a href=\"nn#uniform_candidate_sampler\"><code>uniform_candidate_sampler</code></a></li> <li><a href=\"nn#weighted_cross_entropy_with_logits\"><code>weighted_cross_entropy_with_logits</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"rnn_cell\">Neural Network RNN Cells</a></strong>:</p> <ul> <li><a href=\"rnn_cell#BasicLSTMCell\"><code>BasicLSTMCell</code></a></li> <li><a href=\"rnn_cell#BasicRNNCell\"><code>BasicRNNCell</code></a></li> <li><a href=\"rnn_cell#DropoutWrapper\"><code>DropoutWrapper</code></a></li> <li><a href=\"rnn_cell#EmbeddingWrapper\"><code>EmbeddingWrapper</code></a></li> <li><a href=\"rnn_cell#GRUCell\"><code>GRUCell</code></a></li> <li><a href=\"rnn_cell#InputProjectionWrapper\"><code>InputProjectionWrapper</code></a></li> <li><a href=\"rnn_cell#LSTMCell\"><code>LSTMCell</code></a></li> <li><a href=\"rnn_cell#LSTMStateTuple\"><code>LSTMStateTuple</code></a></li> <li><a href=\"rnn_cell#MultiRNNCell\"><code>MultiRNNCell</code></a></li> <li><a href=\"rnn_cell#OutputProjectionWrapper\"><code>OutputProjectionWrapper</code></a></li> <li><a href=\"rnn_cell#RNNCell\"><code>RNNCell</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"client\">Running Graphs</a></strong>:</p> <ul> <li><a href=\"client#AbortedError\"><code>AbortedError</code></a></li> <li><a href=\"client#AlreadyExistsError\"><code>AlreadyExistsError</code></a></li> <li><a href=\"client#CancelledError\"><code>CancelledError</code></a></li> <li><a href=\"client#DataLossError\"><code>DataLossError</code></a></li> <li><a href=\"client#DeadlineExceededError\"><code>DeadlineExceededError</code></a></li> <li><a href=\"client#FailedPreconditionError\"><code>FailedPreconditionError</code></a></li> <li><a href=\"client#get_default_session\"><code>get_default_session</code></a></li> <li><a href=\"client#InteractiveSession\"><code>InteractiveSession</code></a></li> <li><a href=\"client#InternalError\"><code>InternalError</code></a></li> <li><a href=\"client#InvalidArgumentError\"><code>InvalidArgumentError</code></a></li> <li><a href=\"client#NotFoundError\"><code>NotFoundError</code></a></li> <li><a href=\"client#OpError\"><code>OpError</code></a></li> <li><a href=\"client#OutOfRangeError\"><code>OutOfRangeError</code></a></li> <li><a href=\"client#PermissionDeniedError\"><code>PermissionDeniedError</code></a></li> <li><a href=\"client#ResourceExhaustedError\"><code>ResourceExhaustedError</code></a></li> <li><a href=\"client#Session\"><code>Session</code></a></li> <li><a href=\"client#UnauthenticatedError\"><code>UnauthenticatedError</code></a></li> <li><a href=\"client#UnavailableError\"><code>UnavailableError</code></a></li> <li><a href=\"client#UnimplementedError\"><code>UnimplementedError</code></a></li> <li><a href=\"client#UnknownError\"><code>UnknownError</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"train\">Training</a></strong>:</p> <ul> <li><a href=\"train#AdadeltaOptimizer\"><code>AdadeltaOptimizer</code></a></li> <li><a href=\"train#AdagradOptimizer\"><code>AdagradOptimizer</code></a></li> <li><a href=\"train#AdamOptimizer\"><code>AdamOptimizer</code></a></li> <li><a href=\"train#add_queue_runner\"><code>add_queue_runner</code></a></li> <li><a href=\"train#AggregationMethod\"><code>AggregationMethod</code></a></li> <li><a href=\"train#audio_summary\"><code>audio_summary</code></a></li> <li><a href=\"train#clip_by_average_norm\"><code>clip_by_average_norm</code></a></li> <li><a href=\"train#clip_by_global_norm\"><code>clip_by_global_norm</code></a></li> <li><a href=\"train#clip_by_norm\"><code>clip_by_norm</code></a></li> <li><a href=\"train#clip_by_value\"><code>clip_by_value</code></a></li> <li><a href=\"train#ClusterSpec\"><code>ClusterSpec</code></a></li> <li><a href=\"train#Coordinator\"><code>Coordinator</code></a></li> <li><a href=\"train#do_quantize_training_on_graphdef\"><code>do_quantize_training_on_graphdef</code></a></li> <li><a href=\"train#exponential_decay\"><code>exponential_decay</code></a></li> <li><a href=\"train#ExponentialMovingAverage\"><code>ExponentialMovingAverage</code></a></li> <li><a href=\"train#FtrlOptimizer\"><code>FtrlOptimizer</code></a></li> <li><a href=\"train#generate_checkpoint_state_proto\"><code>generate_checkpoint_state_proto</code></a></li> <li><a href=\"train#global_norm\"><code>global_norm</code></a></li> <li><a href=\"train#global_step\"><code>global_step</code></a></li> <li><a href=\"train#GradientDescentOptimizer\"><code>GradientDescentOptimizer</code></a></li> <li><a href=\"train#gradients\"><code>gradients</code></a></li> <li><a href=\"train#histogram_summary\"><code>histogram_summary</code></a></li> <li><a href=\"train#image_summary\"><code>image_summary</code></a></li> <li><a href=\"train#LooperThread\"><code>LooperThread</code></a></li> <li><a href=\"train#merge_all_summaries\"><code>merge_all_summaries</code></a></li> <li><a href=\"train#merge_summary\"><code>merge_summary</code></a></li> <li><a href=\"train#MomentumOptimizer\"><code>MomentumOptimizer</code></a></li> <li><a href=\"train#Optimizer\"><code>Optimizer</code></a></li> <li><a href=\"train#QueueRunner\"><code>QueueRunner</code></a></li> <li><a href=\"train#replica_device_setter\"><code>replica_device_setter</code></a></li> <li><a href=\"train#RMSPropOptimizer\"><code>RMSPropOptimizer</code></a></li> <li><a href=\"train#scalar_summary\"><code>scalar_summary</code></a></li> <li><a href=\"train#Server\"><code>Server</code></a></li> <li><a href=\"train#SessionManager\"><code>SessionManager</code></a></li> <li><a href=\"train#start_queue_runners\"><code>start_queue_runners</code></a></li> <li><a href=\"train#stop_gradient\"><code>stop_gradient</code></a></li> <li><a href=\"train#summary_iterator\"><code>summary_iterator</code></a></li> <li><a href=\"train#SummaryWriter\"><code>SummaryWriter</code></a></li> <li><a href=\"train#Supervisor\"><code>Supervisor</code></a></li> <li><a href=\"train#write_graph\"><code>write_graph</code></a></li> <li><a href=\"train#zero_fraction\"><code>zero_fraction</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"script_ops\">Wraps python functions</a></strong>:</p> <ul> <li><a href=\"script_ops#py_func\"><code>py_func</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"summary\">Summary Operations</a></strong>:</p> <ul> <li><a href=\"summary#tensor_summary\"><code>tensor_summary</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"test\">Testing</a></strong>:</p> <ul> <li><a href=\"test#assert_equal_graph_def\"><code>assert_equal_graph_def</code></a></li> <li><a href=\"test#compute_gradient\"><code>compute_gradient</code></a></li> <li><a href=\"test#compute_gradient_error\"><code>compute_gradient_error</code></a></li> <li><a href=\"test#get_temp_dir\"><code>get_temp_dir</code></a></li> <li><a href=\"test#is_built_with_cuda\"><code>is_built_with_cuda</code></a></li> <li><a href=\"test#main\"><code>main</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.bayesflow.stochastic_graph\">BayesFlow Stochastic Graph (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.bayesflow.stochastic_graph#DistributionTensor\"><code>DistributionTensor</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#get_current_value_type\"><code>get_current_value_type</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#get_score_function_with_baseline\"><code>get_score_function_with_baseline</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#MeanValue\"><code>MeanValue</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#NoValueTypeSetError\"><code>NoValueTypeSetError</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#SampleAndReshapeValue\"><code>SampleAndReshapeValue</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#SampleValue\"><code>SampleValue</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#score_function\"><code>score_function</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#StochasticTensor\"><code>StochasticTensor</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#surrogate_loss\"><code>surrogate_loss</code></a></li> <li><a href=\"contrib.bayesflow.stochastic_graph#value_type\"><code>value_type</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.distributions\">Statistical distributions (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.distributions#BaseDistribution\"><code>BaseDistribution</code></a></li> <li><a href=\"contrib.distributions#batch_matrix_diag_transform\"><code>batch_matrix_diag_transform</code></a></li> <li><a href=\"contrib.distributions#Bernoulli\"><code>Bernoulli</code></a></li> <li><a href=\"contrib.distributions#Beta\"><code>Beta</code></a></li> <li><a href=\"contrib.distributions#Binomial\"><code>Binomial</code></a></li> <li><a href=\"contrib.distributions#Categorical\"><code>Categorical</code></a></li> <li><a href=\"contrib.distributions#Chi2\"><code>Chi2</code></a></li> <li><a href=\"contrib.distributions#Dirichlet\"><code>Dirichlet</code></a></li> <li><a href=\"contrib.distributions#DirichletMultinomial\"><code>DirichletMultinomial</code></a></li> <li><a href=\"contrib.distributions#Distribution\"><code>Distribution</code></a></li> <li><a href=\"contrib.distributions#Exponential\"><code>Exponential</code></a></li> <li><a href=\"contrib.distributions#Gamma\"><code>Gamma</code></a></li> <li><a href=\"contrib.distributions#InverseGamma\"><code>InverseGamma</code></a></li> <li><a href=\"contrib.distributions#kl\"><code>kl</code></a></li> <li><a href=\"contrib.distributions#Laplace\"><code>Laplace</code></a></li> <li><a href=\"contrib.distributions#Multinomial\"><code>Multinomial</code></a></li> <li><a href=\"contrib.distributions#MultivariateNormalCholesky\"><code>MultivariateNormalCholesky</code></a></li> <li><a href=\"contrib.distributions#MultivariateNormalDiag\"><code>MultivariateNormalDiag</code></a></li> <li><a href=\"contrib.distributions#MultivariateNormalDiagPlusVDVT\"><code>MultivariateNormalDiagPlusVDVT</code></a></li> <li><a href=\"contrib.distributions#MultivariateNormalFull\"><code>MultivariateNormalFull</code></a></li> <li><a href=\"contrib.distributions#Normal\"><code>Normal</code></a></li> <li><a href=\"contrib.distributions#normal_congugates_known_sigma_predictive\"><code>normal_congugates_known_sigma_predictive</code></a></li> <li><a href=\"contrib.distributions#normal_conjugates_known_sigma_posterior\"><code>normal_conjugates_known_sigma_posterior</code></a></li> <li><a href=\"contrib.distributions#RegisterKL\"><code>RegisterKL</code></a></li> <li><a href=\"contrib.distributions#StudentT\"><code>StudentT</code></a></li> <li><a href=\"contrib.distributions#TransformedDistribution\"><code>TransformedDistribution</code></a></li> <li><a href=\"contrib.distributions#Uniform\"><code>Uniform</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.ffmpeg\">FFmpeg (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.ffmpeg#decode_audio\"><code>decode_audio</code></a></li> <li><a href=\"contrib.ffmpeg#encode_audio\"><code>encode_audio</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.framework\">Framework (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.framework#add_arg_scope\"><code>add_arg_scope</code></a></li> <li><a href=\"contrib.framework#add_model_variable\"><code>add_model_variable</code></a></li> <li><a href=\"contrib.framework#arg_scope\"><code>arg_scope</code></a></li> <li><a href=\"contrib.framework#arg_scoped_arguments\"><code>arg_scoped_arguments</code></a></li> <li><a href=\"contrib.framework#assert_global_step\"><code>assert_global_step</code></a></li> <li><a href=\"contrib.framework#assert_or_get_global_step\"><code>assert_or_get_global_step</code></a></li> <li><a href=\"contrib.framework#assert_same_float_dtype\"><code>assert_same_float_dtype</code></a></li> <li><a href=\"contrib.framework#assert_scalar_int\"><code>assert_scalar_int</code></a></li> <li><a href=\"contrib.framework#convert_to_tensor_or_sparse_tensor\"><code>convert_to_tensor_or_sparse_tensor</code></a></li> <li><a href=\"contrib.framework#create_global_step\"><code>create_global_step</code></a></li> <li><a href=\"contrib.framework#deprecated\"><code>deprecated</code></a></li> <li><a href=\"contrib.framework#deprecated_arg_values\"><code>deprecated_arg_values</code></a></li> <li><a href=\"contrib.framework#get_global_step\"><code>get_global_step</code></a></li> <li><a href=\"contrib.framework#get_graph_from_inputs\"><code>get_graph_from_inputs</code></a></li> <li><a href=\"contrib.framework#get_local_variables\"><code>get_local_variables</code></a></li> <li><a href=\"contrib.framework#get_model_variables\"><code>get_model_variables</code></a></li> <li><a href=\"contrib.framework#get_or_create_global_step\"><code>get_or_create_global_step</code></a></li> <li><a href=\"contrib.framework#get_unique_variable\"><code>get_unique_variable</code></a></li> <li><a href=\"contrib.framework#get_variables\"><code>get_variables</code></a></li> <li><a href=\"contrib.framework#get_variables_by_name\"><code>get_variables_by_name</code></a></li> <li><a href=\"contrib.framework#get_variables_by_suffix\"><code>get_variables_by_suffix</code></a></li> <li><a href=\"contrib.framework#get_variables_to_restore\"><code>get_variables_to_restore</code></a></li> <li><a href=\"contrib.framework#has_arg_scope\"><code>has_arg_scope</code></a></li> <li><a href=\"contrib.framework#is_non_decreasing\"><code>is_non_decreasing</code></a></li> <li><a href=\"contrib.framework#is_numeric_tensor\"><code>is_numeric_tensor</code></a></li> <li><a href=\"contrib.framework#is_strictly_increasing\"><code>is_strictly_increasing</code></a></li> <li><a href=\"contrib.framework#is_tensor\"><code>is_tensor</code></a></li> <li><a href=\"contrib.framework#local_variable\"><code>local_variable</code></a></li> <li><a href=\"contrib.framework#model_variable\"><code>model_variable</code></a></li> <li><a href=\"contrib.framework#reduce_sum_n\"><code>reduce_sum_n</code></a></li> <li><a href=\"contrib.framework#safe_embedding_lookup_sparse\"><code>safe_embedding_lookup_sparse</code></a></li> <li><a href=\"contrib.framework#variable\"><code>variable</code></a></li> <li><a href=\"contrib.framework#VariableDeviceChooser\"><code>VariableDeviceChooser</code></a></li> <li><a href=\"contrib.framework#with_same_shape\"><code>with_same_shape</code></a></li> <li><a href=\"contrib.framework#with_shape\"><code>with_shape</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.graph_editor\">Graph Editor (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.graph_editor#bypass\"><code>bypass</code></a></li> <li><a href=\"contrib.graph_editor#connect\"><code>connect</code></a></li> <li><a href=\"contrib.graph_editor#detach\"><code>detach</code></a></li> <li><a href=\"contrib.graph_editor#detach_inputs\"><code>detach_inputs</code></a></li> <li><a href=\"contrib.graph_editor#detach_outputs\"><code>detach_outputs</code></a></li> <li><a href=\"contrib.graph_editor#matcher\"><code>matcher</code></a></li> <li><a href=\"contrib.graph_editor#ph\"><code>ph</code></a></li> <li><a href=\"contrib.graph_editor#reroute_a2b\"><code>reroute_a2b</code></a></li> <li><a href=\"contrib.graph_editor#reroute_a2b_inputs\"><code>reroute_a2b_inputs</code></a></li> <li><a href=\"contrib.graph_editor#reroute_a2b_outputs\"><code>reroute_a2b_outputs</code></a></li> <li><a href=\"contrib.graph_editor#reroute_b2a\"><code>reroute_b2a</code></a></li> <li><a href=\"contrib.graph_editor#reroute_b2a_inputs\"><code>reroute_b2a_inputs</code></a></li> <li><a href=\"contrib.graph_editor#reroute_b2a_outputs\"><code>reroute_b2a_outputs</code></a></li> <li><a href=\"contrib.graph_editor#select_ops\"><code>select_ops</code></a></li> <li><a href=\"contrib.graph_editor#select_ts\"><code>select_ts</code></a></li> <li><a href=\"contrib.graph_editor#sgv\"><code>sgv</code></a></li> <li><a href=\"contrib.graph_editor#sgv_scope\"><code>sgv_scope</code></a></li> <li><a href=\"contrib.graph_editor#SubGraphView\"><code>SubGraphView</code></a></li> <li><a href=\"contrib.graph_editor#swap\"><code>swap</code></a></li> <li><a href=\"contrib.graph_editor#swap_inputs\"><code>swap_inputs</code></a></li> <li><a href=\"contrib.graph_editor#swap_outputs\"><code>swap_outputs</code></a></li> <li><a href=\"contrib.graph_editor#Transformer\"><code>Transformer</code></a></li> <li><a href=\"contrib.graph_editor#ts\"><code>ts</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.layers\">Layers (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.layers#apply_regularization\"><code>apply_regularization</code></a></li> <li><a href=\"contrib.layers#avg_pool2d\"><code>avg_pool2d</code></a></li> <li><a href=\"contrib.layers#batch_norm\"><code>batch_norm</code></a></li> <li><a href=\"contrib.layers#convolution2d\"><code>convolution2d</code></a></li> <li><a href=\"contrib.layers#convolution2d_in_plane\"><code>convolution2d_in_plane</code></a></li> <li><a href=\"contrib.layers#convolution2d_transpose\"><code>convolution2d_transpose</code></a></li> <li><a href=\"contrib.layers#flatten\"><code>flatten</code></a></li> <li><a href=\"contrib.layers#fully_connected\"><code>fully_connected</code></a></li> <li><a href=\"contrib.layers#l1_regularizer\"><code>l1_regularizer</code></a></li> <li><a href=\"contrib.layers#l2_regularizer\"><code>l2_regularizer</code></a></li> <li><a href=\"contrib.layers#max_pool2d\"><code>max_pool2d</code></a></li> <li><a href=\"contrib.layers#one_hot_encoding\"><code>one_hot_encoding</code></a></li> <li><a href=\"contrib.layers#optimize_loss\"><code>optimize_loss</code></a></li> <li><a href=\"contrib.layers#repeat\"><code>repeat</code></a></li> <li><a href=\"contrib.layers#separable_convolution2d\"><code>separable_convolution2d</code></a></li> <li><a href=\"contrib.layers#stack\"><code>stack</code></a></li> <li><a href=\"contrib.layers#sum_regularizer\"><code>sum_regularizer</code></a></li> <li><a href=\"contrib.layers#summarize_activation\"><code>summarize_activation</code></a></li> <li><a href=\"contrib.layers#summarize_activations\"><code>summarize_activations</code></a></li> <li><a href=\"contrib.layers#summarize_collection\"><code>summarize_collection</code></a></li> <li><a href=\"contrib.layers#summarize_tensor\"><code>summarize_tensor</code></a></li> <li><a href=\"contrib.layers#summarize_tensors\"><code>summarize_tensors</code></a></li> <li><a href=\"contrib.layers#unit_norm\"><code>unit_norm</code></a></li> <li><a href=\"contrib.layers#variance_scaling_initializer\"><code>variance_scaling_initializer</code></a></li> <li><a href=\"contrib.layers#xavier_initializer\"><code>xavier_initializer</code></a></li> <li><a href=\"contrib.layers#xavier_initializer_conv2d\"><code>xavier_initializer_conv2d</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.learn\">Learn (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.learn#BaseEstimator\"><code>BaseEstimator</code></a></li> <li><a href=\"contrib.learn#DNNClassifier\"><code>DNNClassifier</code></a></li> <li><a href=\"contrib.learn#DNNRegressor\"><code>DNNRegressor</code></a></li> <li><a href=\"contrib.learn#Estimator\"><code>Estimator</code></a></li> <li><a href=\"contrib.learn#evaluate\"><code>evaluate</code></a></li> <li><a href=\"contrib.learn#extract_dask_data\"><code>extract_dask_data</code></a></li> <li><a href=\"contrib.learn#extract_dask_labels\"><code>extract_dask_labels</code></a></li> <li><a href=\"contrib.learn#extract_pandas_data\"><code>extract_pandas_data</code></a></li> <li><a href=\"contrib.learn#extract_pandas_labels\"><code>extract_pandas_labels</code></a></li> <li><a href=\"contrib.learn#extract_pandas_matrix\"><code>extract_pandas_matrix</code></a></li> <li><a href=\"contrib.learn#infer\"><code>infer</code></a></li> <li><a href=\"contrib.learn#LinearClassifier\"><code>LinearClassifier</code></a></li> <li><a href=\"contrib.learn#LinearRegressor\"><code>LinearRegressor</code></a></li> <li><a href=\"contrib.learn#ModeKeys\"><code>ModeKeys</code></a></li> <li><a href=\"contrib.learn#NanLossDuringTrainingError\"><code>NanLossDuringTrainingError</code></a></li> <li><a href=\"contrib.learn#read_batch_examples\"><code>read_batch_examples</code></a></li> <li><a href=\"contrib.learn#read_batch_features\"><code>read_batch_features</code></a></li> <li><a href=\"contrib.learn#read_batch_record_features\"><code>read_batch_record_features</code></a></li> <li><a href=\"contrib.learn#run_feeds\"><code>run_feeds</code></a></li> <li><a href=\"contrib.learn#run_n\"><code>run_n</code></a></li> <li><a href=\"contrib.learn#RunConfig\"><code>RunConfig</code></a></li> <li><a href=\"contrib.learn#TensorFlowClassifier\"><code>TensorFlowClassifier</code></a></li> <li><a href=\"contrib.learn#TensorFlowDNNClassifier\"><code>TensorFlowDNNClassifier</code></a></li> <li><a href=\"contrib.learn#TensorFlowDNNRegressor\"><code>TensorFlowDNNRegressor</code></a></li> <li><a href=\"contrib.learn#TensorFlowEstimator\"><code>TensorFlowEstimator</code></a></li> <li><a href=\"contrib.learn#TensorFlowLinearClassifier\"><code>TensorFlowLinearClassifier</code></a></li> <li><a href=\"contrib.learn#TensorFlowLinearRegressor\"><code>TensorFlowLinearRegressor</code></a></li> <li><a href=\"contrib.learn#TensorFlowRegressor\"><code>TensorFlowRegressor</code></a></li> <li><a href=\"contrib.learn#TensorFlowRNNClassifier\"><code>TensorFlowRNNClassifier</code></a></li> <li><a href=\"contrib.learn#TensorFlowRNNRegressor\"><code>TensorFlowRNNRegressor</code></a></li> <li><a href=\"contrib.learn#train\"><code>train</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.learn.monitors\">Monitors (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.learn.monitors#BaseMonitor\"><code>BaseMonitor</code></a></li> <li><a href=\"contrib.learn.monitors#CaptureVariable\"><code>CaptureVariable</code></a></li> <li><a href=\"contrib.learn.monitors#CheckpointSaver\"><code>CheckpointSaver</code></a></li> <li><a href=\"contrib.learn.monitors#EveryN\"><code>EveryN</code></a></li> <li><a href=\"contrib.learn.monitors#ExportMonitor\"><code>ExportMonitor</code></a></li> <li><a href=\"contrib.learn.monitors#get_default_monitors\"><code>get_default_monitors</code></a></li> <li><a href=\"contrib.learn.monitors#GraphDump\"><code>GraphDump</code></a></li> <li><a href=\"contrib.learn.monitors#LoggingTrainable\"><code>LoggingTrainable</code></a></li> <li><a href=\"contrib.learn.monitors#NanLoss\"><code>NanLoss</code></a></li> <li><a href=\"contrib.learn.monitors#PrintTensor\"><code>PrintTensor</code></a></li> <li><a href=\"contrib.learn.monitors#StepCounter\"><code>StepCounter</code></a></li> <li><a href=\"contrib.learn.monitors#StopAtStep\"><code>StopAtStep</code></a></li> <li><a href=\"contrib.learn.monitors#SummarySaver\"><code>SummarySaver</code></a></li> <li><a href=\"contrib.learn.monitors#SummaryWriterCache\"><code>SummaryWriterCache</code></a></li> <li><a href=\"contrib.learn.monitors#ValidationMonitor\"><code>ValidationMonitor</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.losses\">Losses (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.losses#absolute_difference\"><code>absolute_difference</code></a></li> <li><a href=\"contrib.losses#add_loss\"><code>add_loss</code></a></li> <li><a href=\"contrib.losses#cosine_distance\"><code>cosine_distance</code></a></li> <li><a href=\"contrib.losses#get_losses\"><code>get_losses</code></a></li> <li><a href=\"contrib.losses#get_regularization_losses\"><code>get_regularization_losses</code></a></li> <li><a href=\"contrib.losses#get_total_loss\"><code>get_total_loss</code></a></li> <li><a href=\"contrib.losses#hinge_loss\"><code>hinge_loss</code></a></li> <li><a href=\"contrib.losses#log_loss\"><code>log_loss</code></a></li> <li><a href=\"contrib.losses#sigmoid_cross_entropy\"><code>sigmoid_cross_entropy</code></a></li> <li><a href=\"contrib.losses#softmax_cross_entropy\"><code>softmax_cross_entropy</code></a></li> <li><a href=\"contrib.losses#sum_of_pairwise_squares\"><code>sum_of_pairwise_squares</code></a></li> <li><a href=\"contrib.losses#sum_of_squares\"><code>sum_of_squares</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.rnn\">RNN (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.rnn#AttentionCellWrapper\"><code>AttentionCellWrapper</code></a></li> <li><a href=\"contrib.rnn#CoupledInputForgetGateLSTMCell\"><code>CoupledInputForgetGateLSTMCell</code></a></li> <li><a href=\"contrib.rnn#GridLSTMCell\"><code>GridLSTMCell</code></a></li> <li><a href=\"contrib.rnn#LSTMFusedCell\"><code>LSTMFusedCell</code></a></li> <li><a href=\"contrib.rnn#TimeFreqLSTMCell\"><code>TimeFreqLSTMCell</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.metrics\">Metrics (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.metrics#accuracy\"><code>accuracy</code></a></li> <li><a href=\"contrib.metrics#aggregate_metric_map\"><code>aggregate_metric_map</code></a></li> <li><a href=\"contrib.metrics#aggregate_metrics\"><code>aggregate_metrics</code></a></li> <li><a href=\"contrib.metrics#auc_using_histogram\"><code>auc_using_histogram</code></a></li> <li><a href=\"contrib.metrics#confusion_matrix\"><code>confusion_matrix</code></a></li> <li><a href=\"contrib.metrics#set_difference\"><code>set_difference</code></a></li> <li><a href=\"contrib.metrics#set_intersection\"><code>set_intersection</code></a></li> <li><a href=\"contrib.metrics#set_size\"><code>set_size</code></a></li> <li><a href=\"contrib.metrics#set_union\"><code>set_union</code></a></li> <li><a href=\"contrib.metrics#streaming_accuracy\"><code>streaming_accuracy</code></a></li> <li><a href=\"contrib.metrics#streaming_auc\"><code>streaming_auc</code></a></li> <li><a href=\"contrib.metrics#streaming_mean\"><code>streaming_mean</code></a></li> <li><a href=\"contrib.metrics#streaming_mean_absolute_error\"><code>streaming_mean_absolute_error</code></a></li> <li><a href=\"contrib.metrics#streaming_mean_cosine_distance\"><code>streaming_mean_cosine_distance</code></a></li> <li><a href=\"contrib.metrics#streaming_mean_iou\"><code>streaming_mean_iou</code></a></li> <li><a href=\"contrib.metrics#streaming_mean_relative_error\"><code>streaming_mean_relative_error</code></a></li> <li><a href=\"contrib.metrics#streaming_mean_squared_error\"><code>streaming_mean_squared_error</code></a></li> <li><a href=\"contrib.metrics#streaming_percentage_less\"><code>streaming_percentage_less</code></a></li> <li><a href=\"contrib.metrics#streaming_precision\"><code>streaming_precision</code></a></li> <li><a href=\"contrib.metrics#streaming_recall\"><code>streaming_recall</code></a></li> <li><a href=\"contrib.metrics#streaming_recall_at_k\"><code>streaming_recall_at_k</code></a></li> <li><a href=\"contrib.metrics#streaming_root_mean_squared_error\"><code>streaming_root_mean_squared_error</code></a></li> <li><a href=\"contrib.metrics#streaming_sparse_precision_at_k\"><code>streaming_sparse_precision_at_k</code></a></li> <li><a href=\"contrib.metrics#streaming_sparse_recall_at_k\"><code>streaming_sparse_recall_at_k</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.util\">Utilities (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.util#constant_value\"><code>constant_value</code></a></li> <li><a href=\"contrib.util#make_ndarray\"><code>make_ndarray</code></a></li> <li><a href=\"contrib.util#make_tensor_proto\"><code>make_tensor_proto</code></a></li> <li><a href=\"contrib.util#ops_used_by_graph_def\"><code>ops_used_by_graph_def</code></a></li> <li><a href=\"contrib.util#stripped_op_list_for_graph\"><code>stripped_op_list_for_graph</code></a></li> </ul>\n</li> <li>\n<p><strong><a href=\"contrib.copy_graph\">Copying Graph Elements (contrib)</a></strong>:</p> <ul> <li><a href=\"contrib.copy_graph#copy_op_to_graph\"><code>copy_op_to_graph</code></a></li> <li><a href=\"contrib.copy_graph#copy_variable_to_graph\"><code>copy_variable_to_graph</code></a></li> <li><a href=\"contrib.copy_graph#get_copied_op\"><code>get_copied_op</code></a></li> </ul>\n</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/</a>\n  </p>\n</div>\n","framework":"<h1 id=\"building-graphs\">Building Graphs</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#building-graphs\">Building Graphs</a></li> <ul> <li><a href=\"#core-graph-data-structures\">Core graph data structures</a></li> <ul> <li><a href=\"#Graph\"><code>class tf.Graph</code></a></li> <li><a href=\"#Operation\"><code>class tf.Operation</code></a></li> <li><a href=\"#Tensor\"><code>class tf.Tensor</code></a></li> </ul> <li><a href=\"#tensor-types\">Tensor types</a></li> <ul> <li><a href=\"#DType\"><code>class tf.DType</code></a></li> <li><a href=\"#as_dtype\"><code>tf.as_dtype(type_value)</code></a></li> </ul> <li><a href=\"#utility-functions\">Utility functions</a></li> <ul> <li><a href=\"#device\"><code>tf.device(device_name_or_function)</code></a></li> <li><a href=\"#container\"><code>tf.container(container_name)</code></a></li> <li><a href=\"#name_scope\"><code>tf.name_scope(name)</code></a></li> <li><a href=\"#control_dependencies\"><code>tf.control_dependencies(control_inputs)</code></a></li> <li><a href=\"#convert_to_tensor\"><code>tf.convert_to_tensor(value, dtype=None, name=None, as_ref=False)</code></a></li> <li><a href=\"#convert_to_tensor_or_indexed_slices\"><code>tf.convert_to_tensor_or_indexed_slices(value, dtype=None, name=None, as_ref=False)</code></a></li> <li><a href=\"#get_default_graph\"><code>tf.get_default_graph()</code></a></li> <li><a href=\"#reset_default_graph\"><code>tf.reset_default_graph()</code></a></li> <li><a href=\"#import_graph_def\"><code>tf.import_graph_def(graph_def, input_map=None, return_elements=None, name=None, op_dict=None, producer_op_list=None)</code></a></li> <li><a href=\"#load_file_system_library\"><code>tf.load_file_system_library(library_filename)</code></a></li> <li><a href=\"#load_op_library\"><code>tf.load_op_library(library_filename)</code></a></li> </ul> <li><a href=\"#graph-collections\">Graph collections</a></li> <ul> <li><a href=\"#add_to_collection\"><code>tf.add_to_collection(name, value)</code></a></li> <li><a href=\"#get_collection\"><code>tf.get_collection(key, scope=None)</code></a></li> <li><a href=\"#get_collection_ref\"><code>tf.get_collection_ref(key)</code></a></li> <li><a href=\"#GraphKeys\"><code>class tf.GraphKeys</code></a></li> </ul> <li><a href=\"#defining-new-operations\">Defining new operations</a></li> <ul> <li><a href=\"#RegisterGradient\"><code>class tf.RegisterGradient</code></a></li> <li><a href=\"#NoGradient\"><code>tf.NoGradient(op_type)</code></a></li> <li><a href=\"#RegisterShape\"><code>class tf.RegisterShape</code></a></li> <li><a href=\"#TensorShape\"><code>class tf.TensorShape</code></a></li> <li><a href=\"#Dimension\"><code>class tf.Dimension</code></a></li> <li><a href=\"#op_scope\"><code>tf.op_scope(values, name, default_name=None)</code></a></li> <li><a href=\"#get_seed\"><code>tf.get_seed(op_seed)</code></a></li> </ul> <li><a href=\"#for-libraries-building-on-tensorflow\">For libraries building on TensorFlow</a></li> <ul> <li><a href=\"#register_tensor_conversion_function\"><code>tf.register_tensor_conversion_function(base_type, conversion_func, priority=100)</code></a></li> </ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#DeviceSpec\"><code>class tf.DeviceSpec</code></a></li> <li><a href=\"#bytes\"><code>class tf.bytes</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Classes and functions for building TensorFlow graphs.</p>  <h2 id=\"core-graph-data-structures\">Core graph data structures</h2>  <h3 id=\"Graph\"><code>class tf.Graph</code></h3> <p>A TensorFlow computation, represented as a dataflow graph.</p> <p>A <code>Graph</code> contains a set of <a href=\"framework#Operation\"><code>Operation</code></a> objects, which represent units of computation; and <a href=\"framework#Tensor\"><code>Tensor</code></a> objects, which represent the units of data that flow between operations.</p> <p>A default <code>Graph</code> is always registered, and accessible by calling <a href=\"framework#get_default_graph\"><code>tf.get_default_graph()</code></a>. To add an operation to the default graph, simply call one of the functions that defines a new <code>Operation</code>:</p> <pre class=\"\">c = tf.constant(4.0)\nassert c.graph is tf.get_default_graph()\n</pre> <p>Another typical usage involves the <a href=\"framework#Graph.as_default\"><code>Graph.as_default()</code></a> context manager, which overrides the current default graph for the lifetime of the context:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">g = tf.Graph()\nwith g.as_default():\n  # Define operations and tensors in `g`.\n  c = tf.constant(30.0)\n  assert c.graph is g\n</pre> <p>Important note: This class <em>is not</em> thread-safe for graph construction. All operations should be created from a single thread, or external synchronization must be provided. Unless otherwise specified, all methods are not thread-safe.</p>  <h4 id=\"Graph.__init__\"><code>tf.Graph.__init__()</code></h4> <p>Creates a new, empty Graph.</p>  <h4 id=\"Graph.as_default\"><code>tf.Graph.as_default()</code></h4> <p>Returns a context manager that makes this <code>Graph</code> the default graph.</p> <p>This method should be used if you want to create multiple graphs in the same process. For convenience, a global default graph is provided, and all ops will be added to this graph if you do not create a new graph explicitly. Use this method with the <code>with</code> keyword to specify that ops created within the scope of a block should be added to this graph.</p> <p>The default graph is a property of the current thread. If you create a new thread, and wish to use the default graph in that thread, you must explicitly add a <code>with g.as_default():</code> in that thread's function.</p> <p>The following code examples are equivalent:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 1. Using Graph.as_default():\ng = tf.Graph()\nwith g.as_default():\n  c = tf.constant(5.0)\n  assert c.graph is g\n\n# 2. Constructing and making default:\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0)\n  assert c.graph is g\n</pre> <h5 id=\"returns\">Returns:</h5> <p>A context manager for using this graph as the default graph.</p>  <h4 id=\"Graph.as_graph_def\"><code>tf.Graph.as_graph_def(from_version=None, add_shapes=False)</code></h4> <p>Returns a serialized <code>GraphDef</code> representation of this graph.</p> <p>The serialized <code>GraphDef</code> can be imported into another <code>Graph</code> (using <a href=\"#import_graph_def\"><code>import_graph_def()</code></a>) or used with the <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/cc/index.html\">C++ Session API</a>.</p> <p>This method is thread-safe.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>from_version</code>: Optional. If this is set, returns a <code>GraphDef</code> containing only the nodes that were added to this graph since its <code>version</code> property had the given value.</li> <li>\n<code>add_shapes</code>: If true, adds an \"_output_shapes\" list attr to each node with the inferred shapes of each of its outputs.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto\" rel=\"noreferrer\"><code>GraphDef</code></a> protocol buffer.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the <code>graph_def</code> would be too large.</li> </ul>  <h4 id=\"Graph.finalize\"><code>tf.Graph.finalize()</code></h4> <p>Finalizes this graph, making it read-only.</p> <p>After calling <code>g.finalize()</code>, no new operations can be added to <code>g</code>. This method is used to ensure that no operations are added to a graph when it is shared between multiple threads, for example when using a <a href=\"train#QueueRunner\"><code>QueueRunner</code></a>.</p>  <h4 id=\"Graph.finalized\"><code>tf.Graph.finalized</code></h4> <p>True if this graph has been finalized.</p>  <h4 id=\"Graph.control_dependencies\"><code>tf.Graph.control_dependencies(control_inputs)</code></h4> <p>Returns a context manager that specifies control dependencies.</p> <p>Use with the <code>with</code> keyword to specify that all operations constructed within the context should have control dependencies on <code>control_inputs</code>. For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with g.control_dependencies([a, b, c]):\n  # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n  d = ...\n  e = ...\n</pre> <p>Multiple calls to <code>control_dependencies()</code> can be nested, and in that case a new <code>Operation</code> will have control dependencies on the union of <code>control_inputs</code> from all active contexts.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with g.control_dependencies([a, b]):\n  # Ops constructed here run after `a` and `b`.\n  with g.control_dependencies([c, d]):\n    # Ops constructed here run after `a`, `b`, `c`, and `d`.\n</pre> <p>You can pass None to clear the control dependencies:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with g.control_dependencies([a, b]):\n  # Ops constructed here run after `a` and `b`.\n  with g.control_dependencies(None):\n    # Ops constructed here run normally, not waiting for either `a` or `b`.\n    with g.control_dependencies([c, d]):\n      # Ops constructed here run after `c` and `d`, also not waiting\n      # for either `a` or `b`.\n</pre> <p><em>N.B.</em> The control dependencies context applies <em>only</em> to ops that are constructed within the context. Merely using an op or tensor in the context does not add a control dependency. The following example illustrates this point:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># WRONG\ndef my_func(pred, tensor):\n  t = tf.matmul(tensor, tensor)\n  with tf.control_dependencies([pred]):\n    # The matmul op is created outside the context, so no control\n    # dependency will be added.\n    return t\n\n# RIGHT\ndef my_func(pred, tensor):\n  with tf.control_dependencies([pred]):\n    # The matmul op is created in the context, so a control dependency\n    # will be added.\n    return tf.matmul(tensor, tensor)\n</pre>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>control_inputs</code>: A list of <code>Operation</code> or <code>Tensor</code> objects which must be executed or computed before running the operations defined in the context. Can also be <code>None</code> to clear the control dependencies.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A context manager that specifies control dependencies for all operations constructed within the context.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>control_inputs</code> is not a list of <code>Operation</code> or <code>Tensor</code> objects.</li> </ul>  <h4 id=\"Graph.device\"><code>tf.Graph.device(device_name_or_function)</code></h4> <p>Returns a context manager that specifies the default device to use.</p> <p>The <code>device_name_or_function</code> argument may either be a device name string, a device function, or None:</p> <ul> <li>If it is a device name string, all operations constructed in this context will be assigned to the device with that name, unless overridden by a nested <code>device()</code> context.</li> <li>If it is a function, it will be treated as a function from Operation objects to device name strings, and invoked each time a new Operation is created. The Operation will be assigned to the device with the returned name.</li> <li>If it is None, all <code>device()</code> invocations from the enclosing context will be ignored.</li> </ul> <p>For information about the valid syntax of device name strings, see the documentation in <a href=\"https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h\" rel=\"noreferrer\"><code>DeviceNameUtils</code></a>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with g.device('/gpu:0'):\n  # All operations constructed in this context will be placed\n  # on GPU 0.\n  with g.device(None):\n    # All operations constructed in this context will have no\n    # assigned device.\n\n# Defines a function from `Operation` to device string.\ndef matmul_on_gpu(n):\n  if n.type == \"MatMul\":\n    return \"/gpu:0\"\n  else:\n    return \"/cpu:0\"\n\nwith g.device(matmul_on_gpu):\n  # All operations of type \"MatMul\" constructed in this context\n  # will be placed on GPU 0; all other operations will be placed\n  # on CPU 0.\n</pre> <p><strong>N.B.</strong> The device scope may be overridden by op wrappers or other library code. For example, a variable assignment op <code>v.assign()</code> must be colocated with the <code>tf.Variable</code> <code>v</code>, and incompatible device scopes will be ignored.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>device_name_or_function</code>: The device name or function to use in the context.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A context manager that specifies the default device to use for newly created ops.</p>  <h4 id=\"Graph.name_scope\"><code>tf.Graph.name_scope(name)</code></h4> <p>Returns a context manager that creates hierarchical names for operations.</p> <p>A graph maintains a stack of name scopes. A <code>with name_scope(...):</code> statement pushes a new name onto the stack for the lifetime of the context.</p> <p>The <code>name</code> argument will be interpreted as follows:</p> <ul> <li>A string (not ending with '/') will create a new name scope, in which <code>name</code> is appended to the prefix of all operations created in the context. If <code>name</code> has been used before, it will be made unique by calling <code>self.unique_name(name)</code>.</li> <li>A scope previously captured from a <code>with g.name_scope(...) as\nscope:</code> statement will be treated as an \"absolute\" name scope, which makes it possible to re-enter existing scopes.</li> <li>A value of <code>None</code> or the empty string will reset the current name scope to the top-level (empty) name scope.</li> </ul> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.Graph().as_default() as g:\n  c = tf.constant(5.0, name=\"c\")\n  assert c.op.name == \"c\"\n  c_1 = tf.constant(6.0, name=\"c\")\n  assert c_1.op.name == \"c_1\"\n\n  # Creates a scope called \"nested\"\n  with g.name_scope(\"nested\") as scope:\n    nested_c = tf.constant(10.0, name=\"c\")\n    assert nested_c.op.name == \"nested/c\"\n\n    # Creates a nested scope called \"inner\".\n    with g.name_scope(\"inner\"):\n      nested_inner_c = tf.constant(20.0, name=\"c\")\n      assert nested_inner_c.op.name == \"nested/inner/c\"\n\n    # Create a nested scope called \"inner_1\".\n    with g.name_scope(\"inner\"):\n      nested_inner_1_c = tf.constant(30.0, name=\"c\")\n      assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n\n      # Treats `scope` as an absolute name scope, and\n      # switches to the \"nested/\" scope.\n      with g.name_scope(scope):\n        nested_d = tf.constant(40.0, name=\"d\")\n        assert nested_d.op.name == \"nested/d\"\n\n        with g.name_scope(\"\"):\n          e = tf.constant(50.0, name=\"e\")\n          assert e.op.name == \"e\"\n</pre> <p>The name of the scope itself can be captured by <code>with\ng.name_scope(...) as scope:</code>, which stores the name of the scope in the variable <code>scope</code>. This value can be used to name an operation that represents the overall result of executing the ops in a scope. For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">inputs = tf.constant(...)\nwith g.name_scope('my_layer') as scope:\n  weights = tf.Variable(..., name=\"weights\")\n  biases = tf.Variable(..., name=\"biases\")\n  affine = tf.matmul(inputs, weights) + biases\n  output = tf.nn.relu(affine, name=scope)\n</pre> <p>NOTE: This constructor validates the given <code>name</code>. Valid scope names match one of the following regular expressions:</p> <pre class=\"\">[A-Za-z0-9.][A-Za-z0-9_.\\\\-/]* (for scopes at the root)\n[A-Za-z0-9_.\\\\-/]* (for other scopes)\n</pre>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the scope.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A context manager that installs <code>name</code> as a new name scope.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>name</code> is not a valid scope name. The rules are the</li> </ul> <p>A <code>Graph</code> instance supports an arbitrary number of \"collections\" that are identified by name. For convenience when building a large graph, collections can store groups of related objects: for example, the <code>tf.Variable</code> uses a collection (named <a href=\"framework#GraphKeys\"><code>tf.GraphKeys.VARIABLES</code></a>) for all variables that are created during the construction of a graph. The caller may define additional collections by specifying a new name.</p>  <h4 id=\"Graph.add_to_collection\"><code>tf.Graph.add_to_collection(name, value)</code></h4> <p>Stores <code>value</code> in the collection with the given <code>name</code>.</p> <p>Note that collections are not sets, so it is possible to add a value to a collection several times.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>name</code>: The key for the collection. The <code>GraphKeys</code> class contains many standard names for collections.</li> <li>\n<code>value</code>: The value to add to the collection.</li> </ul>  <h4 id=\"Graph.add_to_collections\"><code>tf.Graph.add_to_collections(names, value)</code></h4> <p>Stores <code>value</code> in the collections given by <code>names</code>.</p> <p>Note that collections are not sets, so it is possible to add a value to a collection several times. This function makes sure that duplicates in <code>names</code> are ignored, but it will not check for pre-existing membership of <code>value</code> in any of the collections in <code>names</code>.</p> <p><code>names</code> can be any iterable, but if <code>names</code> is a string, it is treated as a single collection name.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>names</code>: The keys for the collections to add to. The <code>GraphKeys</code> class contains many standard names for collections.</li> <li>\n<code>value</code>: The value to add to the collections.</li> </ul>  <h4 id=\"Graph.get_collection\"><code>tf.Graph.get_collection(name, scope=None)</code></h4> <p>Returns a list of values in the collection with the given <code>name</code>.</p> <p>This is different from <code>get_collection_ref()</code> which always returns the actual collection list if it exists in that it returns a new list each time it is called.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>name</code>: The key for the collection. For example, the <code>GraphKeys</code> class contains many standard names for collections.</li> <li>\n<code>scope</code>: (Optional.) If supplied, the resulting list is filtered to include only items whose <code>name</code> attribute matches using <code>re.match</code>. Items without a <code>name</code> attribute are never returned if a scope is supplied and the choice or <code>re.match</code> means that a <code>scope</code> without special tokens filters by prefix.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>The list of values in the collection with the given <code>name</code>, or an empty list if no value has been added to that collection. The list contains the values in the order under which they were collected.</p>  <h4 id=\"Graph.get_collection_ref\"><code>tf.Graph.get_collection_ref(name)</code></h4> <p>Returns a list of values in the collection with the given <code>name</code>.</p> <p>If the collection exists, this returns the list itself, which can be modified in place to change the collection. If the collection does not exist, it is created as an empty list and the list is returned.</p> <p>This is different from <code>get_collection()</code> which always returns a copy of the collection list if it exists and never creates an empty collection.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>name</code>: The key for the collection. For example, the <code>GraphKeys</code> class contains many standard names for collections.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>The list of values in the collection with the given <code>name</code>, or an empty list if no value has been added to that collection.</p>  <h4 id=\"Graph.as_graph_element\"><code>tf.Graph.as_graph_element(obj, allow_tensor=True, allow_operation=True)</code></h4> <p>Returns the object referred to by <code>obj</code>, as an <code>Operation</code> or <code>Tensor</code>.</p> <p>This function validates that <code>obj</code> represents an element of this graph, and gives an informative error message if it is not.</p> <p>This function is the canonical way to get/validate an object of one of the allowed types from an external argument reference in the Session API.</p> <p>This method may be called concurrently from multiple threads.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>obj</code>: A <code>Tensor</code>, an <code>Operation</code>, or the name of a tensor or operation. Can also be any object with an <code>_as_graph_element()</code> method that returns a value of one of these types.</li> <li>\n<code>allow_tensor</code>: If true, <code>obj</code> may refer to a <code>Tensor</code>.</li> <li>\n<code>allow_operation</code>: If true, <code>obj</code> may refer to an <code>Operation</code>.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>The <code>Tensor</code> or <code>Operation</code> in the Graph corresponding to <code>obj</code>.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>obj</code> is not a type we support attempting to convert to types.</li> <li>\n<code>ValueError</code>: If <code>obj</code> is of an appropriate type but invalid. For example, an invalid string.</li> <li>\n<code>KeyError</code>: If <code>obj</code> is not an object in the graph.</li> </ul>  <h4 id=\"Graph.get_operation_by_name\"><code>tf.Graph.get_operation_by_name(name)</code></h4> <p>Returns the <code>Operation</code> with the given <code>name</code>.</p> <p>This method may be called concurrently from multiple threads.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>name</code>: The name of the <code>Operation</code> to return.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>The <code>Operation</code> with the given <code>name</code>.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>name</code> is not a string.</li> <li>\n<code>KeyError</code>: If <code>name</code> does not correspond to an operation in this graph.</li> </ul>  <h4 id=\"Graph.get_tensor_by_name\"><code>tf.Graph.get_tensor_by_name(name)</code></h4> <p>Returns the <code>Tensor</code> with the given <code>name</code>.</p> <p>This method may be called concurrently from multiple threads.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>name</code>: The name of the <code>Tensor</code> to return.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>The <code>Tensor</code> with the given <code>name</code>.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>name</code> is not a string.</li> <li>\n<code>KeyError</code>: If <code>name</code> does not correspond to a tensor in this graph.</li> </ul>  <h4 id=\"Graph.get_operations\"><code>tf.Graph.get_operations()</code></h4> <p>Return the list of operations in the graph.</p> <p>You can modify the operations in place, but modifications to the list such as inserts/delete have no effect on the list of operations known to the graph.</p> <p>This method may be called concurrently from multiple threads.</p>  <h5 id=\"returns-11\">Returns:</h5> <p>A list of Operations.</p>  <h4 id=\"Graph.seed\"><code>tf.Graph.seed</code></h4> <p>The graph-level random seed of this graph.</p>  <h4 id=\"Graph.unique_name\"><code>tf.Graph.unique_name(name, mark_as_used=True)</code></h4> <p>Return a unique operation name for <code>name</code>.</p> <p>Note: You rarely need to call <code>unique_name()</code> directly. Most of the time you just need to create <code>with g.name_scope()</code> blocks to generate structured names.</p> <p><code>unique_name</code> is used to generate structured names, separated by <code>\"/\"</code>, to help identify operations when debugging a graph. Operation names are displayed in error messages reported by the TensorFlow runtime, and in various visualization tools such as TensorBoard.</p> <p>If <code>mark_as_used</code> is set to <code>True</code>, which is the default, a new unique name is created and marked as in use. If it's set to <code>False</code>, the unique name is returned without actually being marked as used. This is useful when the caller simply wants to know what the name to be created will be.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>name</code>: The name for an operation.</li> <li>\n<code>mark_as_used</code>: Whether to mark this name as being used.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>A string to be passed to <code>create_op()</code> that will be used to name the operation being created.</p>  <h4 id=\"Graph.version\"><code>tf.Graph.version</code></h4> <p>Returns a version number that increases as ops are added to the graph.</p> <p>Note that this is unrelated to the <a href=\"#Graph.graph_def_version\">GraphDef version</a>.</p>  <h4 id=\"Graph.graph_def_versions\"><code>tf.Graph.graph_def_versions</code></h4> <p>The GraphDef version information of this graph.</p> <p>For details on the meaning of each version, see <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto\" rel=\"noreferrer\"><code>GraphDef</code></a>.</p>  <h5 id=\"returns-13\">Returns:</h5> <p>A <code>VersionDef</code>.</p>  <h4 id=\"Graph.create_op\"><code>tf.Graph.create_op(op_type, inputs, dtypes, input_types=None, name=None, attrs=None, op_def=None, compute_shapes=True, compute_device=True)</code></h4> <p>Creates an <code>Operation</code> in this graph.</p> <p>This is a low-level interface for creating an <code>Operation</code>. Most programs will not call this method directly, and instead use the Python op constructors, such as <code>tf.constant()</code>, which add ops to the default graph.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>op_type</code>: The <code>Operation</code> type to create. This corresponds to the <code>OpDef.name</code> field for the proto that defines the operation.</li> <li>\n<code>inputs</code>: A list of <code>Tensor</code> objects that will be inputs to the <code>Operation</code>.</li> <li>\n<code>dtypes</code>: A list of <code>DType</code> objects that will be the types of the tensors that the operation produces.</li> <li>\n<code>input_types</code>: (Optional.) A list of <code>DType</code>s that will be the types of the tensors that the operation consumes. By default, uses the base <code>DType</code> of each input in <code>inputs</code>. Operations that expect reference-typed inputs must specify <code>input_types</code> explicitly.</li> <li>\n<code>name</code>: (Optional.) A string name for the operation. If not specified, a name is generated based on <code>op_type</code>.</li> <li>\n<code>attrs</code>: (Optional.) A dictionary where the key is the attribute name (a string) and the value is the respective <code>attr</code> attribute of the <code>NodeDef</code> proto that will represent the operation (an <code>AttrValue</code> proto).</li> <li>\n<code>op_def</code>: (Optional.) The <code>OpDef</code> proto that describes the <code>op_type</code> that the operation will have.</li> <li>\n<code>compute_shapes</code>: (Optional.) If True, shape inference will be performed to compute the shapes of the outputs.</li> <li>\n<code>compute_device</code>: (Optional.) If True, device functions will be executed to compute the device property of the Operation.</li> </ul>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if any of the inputs is not a <code>Tensor</code>.</li> <li>\n<code>ValueError</code>: if colocation conflicts with existing device assignment.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>An <code>Operation</code> object.</p>  <h4 id=\"Graph.gradient_override_map\"><code>tf.Graph.gradient_override_map(op_type_map)</code></h4> <p>EXPERIMENTAL: A context manager for overriding gradient functions.</p> <p>This context manager can be used to override the gradient function that will be used for ops within the scope of the context.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">@tf.RegisterGradient(\"CustomSquare\")\ndef _custom_square_grad(op, grad):\n  # ...\n\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0)\n  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n    s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n                          # gradient of s_2.\n</pre>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>op_type_map</code>: A dictionary mapping op type strings to alternative op type strings.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>A context manager that sets the alternative op type to be used for one or more ops created in that context.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>op_type_map</code> is not a dictionary mapping strings to strings.</li> </ul>  <h4 id=\"other-methods\">Other Methods</h4>  <h4 id=\"Graph.colocate_with\"><code>tf.Graph.colocate_with(op, ignore_existing=False)</code></h4> <p>Returns a context manager that specifies an op to colocate with.</p> <p>Note: this function is not for public use, only for internal libraries.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">a = tf.Variable([1.0])\nwith g.colocate_with(a):\n  b = tf.constant(1.0)\n  c = tf.add(a, b)\n</pre> <p><code>b</code> and <code>c</code> will always be colocated with <code>a</code>, no matter where <code>a</code> is eventually placed.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>op</code>: The op to colocate all created ops with.</li> <li>\n<code>ignore_existing</code>: If true, only applies colocation of this op within the context, rather than applying all colocation properties on the stack.</li> </ul>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if op is None.</li> </ul> <h5 id=\"yields\">Yields:</h5> <p>A context manager that specifies the op with which to colocate newly created ops.</p>  <h4 id=\"Graph.container\"><code>tf.Graph.container(container_name)</code></h4> <p>Returns a context manager that specifies the resource container to use.</p> <p>Stateful operations, such as variables and queues, can maintain their states on devices so that they can be shared by multiple processes. A resource container is a string name under which these stateful operations are tracked. These resources can be released or cleared with <code>tf.Session.reset()</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with g.container('experiment0'):\n  # All stateful Operations constructed in this context will be placed\n  # in resource container \"experiment0\".\n  v1 = tf.Variable([1.0])\n  v2 = tf.Variable([2.0])\n  with g.container(\"experiment1\"):\n    # All stateful Operations constructed in this context will be\n    # placed in resource container \"experiment1\".\n    v3 = tf.Variable([3.0])\n    q1 = tf.FIFOQueue(10, tf.float32)\n  # All stateful Operations constructed in this context will be\n  # be created in the \"experiment0\".\n  v4 = tf.Variable([4.0])\n  q1 = tf.FIFOQueue(20, tf.float32)\n  with g.container(\"\"):\n    # All stateful Operations constructed in this context will be\n    # be placed in the default resource container.\n    v5 = tf.Variable([5.0])\n    q3 = tf.FIFOQueue(30, tf.float32)\n\n# Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n# will become undefined (such as unitialized).\ntf.Session.reset(target, [\"experiment0\"])\n</pre>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>container_name</code>: container name string.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A context manager for defining resource containers for stateful ops, yields the container name.</p>  <h4 id=\"Graph.get_all_collection_keys\"><code>tf.Graph.get_all_collection_keys()</code></h4> <p>Returns a list of collections used in this graph.</p>  <h4 id=\"Graph.is_feedable\"><code>tf.Graph.is_feedable(tensor)</code></h4> <p>Returns <code>True</code> if and only if <code>tensor</code> is feedable.</p>  <h4 id=\"Graph.is_fetchable\"><code>tf.Graph.is_fetchable(tensor_or_op)</code></h4> <p>Returns <code>True</code> if and only if <code>tensor_or_op</code> is fetchable.</p>  <h4 id=\"Graph.prevent_feeding\"><code>tf.Graph.prevent_feeding(tensor)</code></h4> <p>Marks the given <code>tensor</code> as unfeedable in this graph.</p>  <h4 id=\"Graph.prevent_fetching\"><code>tf.Graph.prevent_fetching(op)</code></h4> <p>Marks the given <code>op</code> as unfetchable in this graph.</p>  <h3 id=\"Operation\"><code>class tf.Operation</code></h3> <p>Represents a graph node that performs computation on tensors.</p> <p>An <code>Operation</code> is a node in a TensorFlow <code>Graph</code> that takes zero or more <code>Tensor</code> objects as input, and produces zero or more <code>Tensor</code> objects as output. Objects of type <code>Operation</code> are created by calling a Python op constructor (such as <a href=\"math_ops#matmul\"><code>tf.matmul()</code></a>) or <a href=\"framework#Graph.create_op\"><code>Graph.create_op()</code></a>.</p> <p>For example <code>c = tf.matmul(a, b)</code> creates an <code>Operation</code> of type \"MatMul\" that takes tensors <code>a</code> and <code>b</code> as input, and produces <code>c</code> as output.</p> <p>After the graph has been launched in a session, an <code>Operation</code> can be executed by passing it to <a href=\"client#Session.run\"><code>Session.run()</code></a>. <code>op.run()</code> is a shortcut for calling <code>tf.get_default_session().run(op)</code>.</p>  <h4 id=\"Operation.name\"><code>tf.Operation.name</code></h4> <p>The full name of this operation.</p>  <h4 id=\"Operation.type\"><code>tf.Operation.type</code></h4> <p>The type of the op (e.g. <code>\"MatMul\"</code>).</p>  <h4 id=\"Operation.inputs\"><code>tf.Operation.inputs</code></h4> <p>The list of <code>Tensor</code> objects representing the data inputs of this op.</p>  <h4 id=\"Operation.control_inputs\"><code>tf.Operation.control_inputs</code></h4> <p>The <code>Operation</code> objects on which this op has a control dependency.</p> <p>Before this op is executed, TensorFlow will ensure that the operations in <code>self.control_inputs</code> have finished executing. This mechanism can be used to run ops sequentially for performance reasons, or to ensure that the side effects of an op are observed in the correct order.</p>  <h5 id=\"returns-17\">Returns:</h5> <p>A list of <code>Operation</code> objects.</p>  <h4 id=\"Operation.outputs\"><code>tf.Operation.outputs</code></h4> <p>The list of <code>Tensor</code> objects representing the outputs of this op.</p>  <h4 id=\"Operation.device\"><code>tf.Operation.device</code></h4> <p>The name of the device to which this op has been assigned, if any.</p>  <h5 id=\"returns-18\">Returns:</h5> <p>The string name of the device to which this op has been assigned, or an empty string if it has not been assigned to a device.</p>  <h4 id=\"Operation.graph\"><code>tf.Operation.graph</code></h4> <p>The <code>Graph</code> that contains this operation.</p>  <h4 id=\"Operation.run\"><code>tf.Operation.run(feed_dict=None, session=None)</code></h4> <p>Runs this operation in a <code>Session</code>.</p> <p>Calling this method will execute all preceding operations that produce the inputs needed for this operation.</p> <p><em>N.B.</em> Before invoking <code>Operation.run()</code>, its graph must have been launched in a session, and either a default session must be available, or <code>session</code> must be specified explicitly.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>feed_dict</code>: A dictionary that maps <code>Tensor</code> objects to feed values. See <a href=\"client#Session.run\"><code>Session.run()</code></a> for a description of the valid feed values.</li> <li>\n<code>session</code>: (Optional.) The <code>Session</code> to be used to run to this operation. If none, the default session will be used.</li> </ul>  <h4 id=\"Operation.get_attr\"><code>tf.Operation.get_attr(name)</code></h4> <p>Returns the value of the attr of this op with the given <code>name</code>.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>name</code>: The name of the attr to fetch.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>The value of the attr, as a Python object.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If this op does not have an attr with the given <code>name</code>.</li> </ul>  <h4 id=\"Operation.traceback\"><code>tf.Operation.traceback</code></h4> <p>Returns the call stack from when this operation was constructed.</p>  <h4 id=\"other-methods-2\">Other Methods</h4>  <h4 id=\"Operation.__init__\"><code>tf.Operation.__init__(node_def, g, inputs=None, output_types=None, control_inputs=None, input_types=None, original_op=None, op_def=None)</code></h4> <p>Creates an <code>Operation</code>.</p> <p>NOTE: This constructor validates the name of the <code>Operation</code> (passed as <code>node_def.name</code>). Valid <code>Operation</code> names match the following regular expression:</p> <pre class=\"\">[A-Za-z0-9.][A-Za-z0-9_.\\-/]*\n</pre>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>node_def</code>: <code>graph_pb2.NodeDef</code>. <code>NodeDef</code> for the <code>Operation</code>. Used for attributes of <code>graph_pb2.NodeDef</code>, typically <code>name</code>, <code>op</code>, and <code>device</code>. The <code>input</code> attribute is irrelevant here as it will be computed when generating the model.</li> <li>\n<code>g</code>: <code>Graph</code>. The parent graph.</li> <li>\n<code>inputs</code>: list of <code>Tensor</code> objects. The inputs to this <code>Operation</code>.</li> <li>\n<code>output_types</code>: list of <code>DType</code> objects. List of the types of the <code>Tensors</code> computed by this operation. The length of this list indicates the number of output endpoints of the <code>Operation</code>.</li> <li>\n<code>control_inputs</code>: list of operations or tensors from which to have a control dependency.</li> <li>\n<code>input_types</code>: List of <code>DType</code> objects representing the types of the tensors accepted by the <code>Operation</code>. By default uses <code>[x.dtype.base_dtype for x in inputs]</code>. Operations that expect reference-typed inputs must specify these explicitly.</li> <li>\n<code>original_op</code>: Optional. Used to associate the new <code>Operation</code> with an existing <code>Operation</code> (for example, a replica with the op that was replicated).</li> <li>\n<code>op_def</code>: Optional. The <code>op_def_pb2.OpDef</code> proto that describes the op type that this <code>Operation</code> represents.</li> </ul>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if control inputs are not Operations or Tensors, or if <code>node_def</code> is not a <code>NodeDef</code>, or if <code>g</code> is not a <code>Graph</code>, or if <code>inputs</code> are not tensors, or if <code>inputs</code> and <code>input_types</code> are incompatible.</li> <li>\n<code>ValueError</code>: if the <code>node_def</code> name is not valid.</li> </ul>  <h4 id=\"Operation.colocation_groups\"><code>tf.Operation.colocation_groups()</code></h4> <p>Returns the list of colocation groups of the op.</p>  <h4 id=\"Operation.node_def\"><code>tf.Operation.node_def</code></h4> <p>Returns a serialized <code>NodeDef</code> representation of this operation.</p>  <h5 id=\"returns-20\">Returns:</h5> <p>A <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto\" rel=\"noreferrer\"><code>NodeDef</code></a> protocol buffer.</p>  <h4 id=\"Operation.op_def\"><code>tf.Operation.op_def</code></h4> <p>Returns the <code>OpDef</code> proto that represents the type of this op.</p>  <h5 id=\"returns-21\">Returns:</h5> <p>An <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/op_def.proto\" rel=\"noreferrer\"><code>OpDef</code></a> protocol buffer.</p>  <h4 id=\"Operation.values\"><code>tf.Operation.values()</code></h4> <p>DEPRECATED: Use outputs.</p>  <h3 id=\"Tensor\"><code>class tf.Tensor</code></h3> <p>Represents one of the outputs of an <code>Operation</code>.</p> <p><em>Note:</em> the <code>Tensor</code> class will be replaced by <code>Output</code> in the future. Currently these two are aliases for each other.</p> <p>A <code>Tensor</code> is a symbolic handle to one of the outputs of an <code>Operation</code>. It does not hold the values of that operation's output, but instead provides a means of computing those values in a TensorFlow <a href=\"client#Session\"><code>Session</code></a>.</p> <p>This class has two primary purposes:</p> <ol> <li><p>A <code>Tensor</code> can be passed as an input to another <code>Operation</code>. This builds a dataflow connection between operations, which enables TensorFlow to execute an entire <code>Graph</code> that represents a large, multi-step computation.</p></li> <li><p>After the graph has been launched in a session, the value of the <code>Tensor</code> can be computed by passing it to <a href=\"client#Session.run\"><code>Session.run()</code></a>. <code>t.eval()</code> is a shortcut for calling <code>tf.get_default_session().run(t)</code>.</p></li> </ol> <p>In the following example, <code>c</code>, <code>d</code>, and <code>e</code> are symbolic <code>Tensor</code> objects, whereas <code>result</code> is a numpy array that stores a concrete value:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Build a dataflow graph.\nc = tf.constant([[1.0, 2.0], [3.0, 4.0]])\nd = tf.constant([[1.0, 1.0], [0.0, 1.0]])\ne = tf.matmul(c, d)\n\n# Construct a `Session` to execute the graph.\nsess = tf.Session()\n\n# Execute the graph and store the value that `e` represents in `result`.\nresult = sess.run(e)\n</pre>  <h4 id=\"Tensor.dtype\"><code>tf.Tensor.dtype</code></h4> <p>The <code>DType</code> of elements in this tensor.</p>  <h4 id=\"Tensor.name\"><code>tf.Tensor.name</code></h4> <p>The string name of this tensor.</p>  <h4 id=\"Tensor.value_index\"><code>tf.Tensor.value_index</code></h4> <p>The index of this tensor in the outputs of its <code>Operation</code>.</p>  <h4 id=\"Tensor.graph\"><code>tf.Tensor.graph</code></h4> <p>The <code>Graph</code> that contains this tensor.</p>  <h4 id=\"Tensor.op\"><code>tf.Tensor.op</code></h4> <p>The <code>Operation</code> that produces this tensor as an output.</p>  <h4 id=\"Tensor.consumers\"><code>tf.Tensor.consumers()</code></h4> <p>Returns a list of <code>Operation</code>s that consume this tensor.</p>  <h5 id=\"returns-22\">Returns:</h5> <p>A list of <code>Operation</code>s.</p>  <h4 id=\"Tensor.eval\"><code>tf.Tensor.eval(feed_dict=None, session=None)</code></h4> <p>Evaluates this tensor in a <code>Session</code>.</p> <p>Calling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.</p> <p><em>N.B.</em> Before invoking <code>Tensor.eval()</code>, its graph must have been launched in a session, and either a default session must be available, or <code>session</code> must be specified explicitly.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>feed_dict</code>: A dictionary that maps <code>Tensor</code> objects to feed values. See <a href=\"client#Session.run\"><code>Session.run()</code></a> for a description of the valid feed values.</li> <li>\n<code>session</code>: (Optional.) The <code>Session</code> to be used to evaluate this tensor. If none, the default session will be used.</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>A numpy array corresponding to the value of this tensor.</p>  <h4 id=\"Tensor.get_shape\"><code>tf.Tensor.get_shape()</code></h4> <p>Returns the <code>TensorShape</code> that represents the shape of this tensor.</p> <p>The shape is computed using shape inference functions that are registered for each <code>Operation</code> type using <code>tf.RegisterShape</code>. See <a href=\"framework#TensorShape\"><code>TensorShape</code></a> for more details of what a shape represents.</p> <p>The inferred shape of a tensor is used to provide shape information without having to launch the graph in a session. This can be used for debugging, and providing early error messages. For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n\nprint(c.get_shape())\n==&gt; TensorShape([Dimension(2), Dimension(3)])\n\nd = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\n\nprint(d.get_shape())\n==&gt; TensorShape([Dimension(4), Dimension(2)])\n\n# Raises a ValueError, because `c` and `d` do not have compatible\n# inner dimensions.\ne = tf.matmul(c, d)\n\nf = tf.matmul(c, d, transpose_a=True, transpose_b=True)\n\nprint(f.get_shape())\n==&gt; TensorShape([Dimension(3), Dimension(4)])\n</pre> <p>In some cases, the inferred shape may have unknown dimensions. If the caller has additional information about the values of these dimensions, <code>Tensor.set_shape()</code> can be used to augment the inferred shape.</p>  <h5 id=\"returns-24\">Returns:</h5> <p>A <code>TensorShape</code> representing the shape of this tensor.</p>  <h4 id=\"Tensor.set_shape\"><code>tf.Tensor.set_shape(shape)</code></h4> <p>Updates the shape of this tensor.</p> <p>This method can be called multiple times, and will merge the given <code>shape</code> with the current shape of this tensor. It can be used to provide additional information about the shape of this tensor that cannot be inferred from the graph alone. For example, this can be used to provide additional information about the shapes of images:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">_, image_data = tf.TFRecordReader(...).read(...)\nimage = tf.image.decode_png(image_data, channels=3)\n\n# The height and width dimensions of `image` are data dependent, and\n# cannot be computed without executing the op.\nprint(image.get_shape())\n==&gt; TensorShape([Dimension(None), Dimension(None), Dimension(3)])\n\n# We know that each image in this dataset is 28 x 28 pixels.\nimage.set_shape([28, 28, 3])\nprint(image.get_shape())\n==&gt; TensorShape([Dimension(28), Dimension(28), Dimension(3)])\n</pre>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>shape</code>: A <code>TensorShape</code> representing the shape of this tensor.</li> </ul>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>shape</code> is not compatible with the current shape of this tensor.</li> </ul>  <h4 id=\"other-methods-3\">Other Methods</h4>  <h4 id=\"Tensor.__init__\"><code>tf.Tensor.__init__(op, value_index, dtype)</code></h4> <p>Creates a new <code>Tensor</code>.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>op</code>: An <code>Operation</code>. <code>Operation</code> that computes this tensor.</li> <li>\n<code>value_index</code>: An <code>int</code>. Index of the operation's endpoint that produces this tensor.</li> <li>\n<code>dtype</code>: A <code>DType</code>. Type of elements stored in this tensor.</li> </ul>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If the op is not an <code>Operation</code>.</li> </ul>  <h4 id=\"Tensor.device\"><code>tf.Tensor.device</code></h4> <p>The name of the device on which this tensor will be produced, or None.</p>  <h2 id=\"tensor-types\">Tensor types</h2>  <h3 id=\"DType\"><code>class tf.DType</code></h3> <p>Represents the type of the elements in a <code>Tensor</code>.</p> <p>The following <code>DType</code> objects are defined:</p> <ul> <li>\n<code>tf.float16</code>: 16-bit half-precision floating-point.</li> <li>\n<code>tf.float32</code>: 32-bit single-precision floating-point.</li> <li>\n<code>tf.float64</code>: 64-bit double-precision floating-point.</li> <li>\n<code>tf.bfloat16</code>: 16-bit truncated floating-point.</li> <li>\n<code>tf.complex64</code>: 64-bit single-precision complex.</li> <li>\n<code>tf.complex128</code>: 128-bit double-precision complex.</li> <li>\n<code>tf.int8</code>: 8-bit signed integer.</li> <li>\n<code>tf.uint8</code>: 8-bit unsigned integer.</li> <li>\n<code>tf.uint16</code>: 16-bit unsigned integer.</li> <li>\n<code>tf.int16</code>: 16-bit signed integer.</li> <li>\n<code>tf.int32</code>: 32-bit signed integer.</li> <li>\n<code>tf.int64</code>: 64-bit signed integer.</li> <li>\n<code>tf.bool</code>: Boolean.</li> <li>\n<code>tf.string</code>: String.</li> <li>\n<code>tf.qint8</code>: Quantized 8-bit signed integer.</li> <li>\n<code>tf.quint8</code>: Quantized 8-bit unsigned integer.</li> <li>\n<code>tf.qint16</code>: Quantized 16-bit signed integer.</li> <li>\n<code>tf.quint16</code>: Quantized 16-bit unsigned integer.</li> <li>\n<code>tf.qint32</code>: Quantized 32-bit signed integer.</li> </ul> <p>In addition, variants of these types with the <code>_ref</code> suffix are defined for reference-typed tensors.</p> <p>The <code>tf.as_dtype()</code> function converts numpy types and string type names to a <code>DType</code> object.</p>  <h4 id=\"DType.is_compatible_with\"><code>tf.DType.is_compatible_with(other)</code></h4> <p>Returns True if the <code>other</code> DType will be converted to this DType.</p> <p>The conversion rules are as follows:</p> <pre class=\"lang-m no-auto-prettify\">DType(T)       .is_compatible_with(DType(T))        == True\nDType(T)       .is_compatible_with(DType(T).as_ref) == True\nDType(T).as_ref.is_compatible_with(DType(T))        == False\nDType(T).as_ref.is_compatible_with(DType(T).as_ref) == True\n</pre>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>other</code>: A <code>DType</code> (or object that may be converted to a <code>DType</code>).</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>True if a Tensor of the <code>other</code> <code>DType</code> will be implicitly converted to this <code>DType</code>.</p>  <h4 id=\"DType.name\"><code>tf.DType.name</code></h4> <p>Returns the string name for this <code>DType</code>.</p>  <h4 id=\"DType.base_dtype\"><code>tf.DType.base_dtype</code></h4> <p>Returns a non-reference <code>DType</code> based on this <code>DType</code>.</p>  <h4 id=\"DType.real_dtype\"><code>tf.DType.real_dtype</code></h4> <p>Returns the dtype correspond to this dtype's real part.</p>  <h4 id=\"DType.is_ref_dtype\"><code>tf.DType.is_ref_dtype</code></h4> <p>Returns <code>True</code> if this <code>DType</code> represents a reference type.</p>  <h4 id=\"DType.as_ref\"><code>tf.DType.as_ref</code></h4> <p>Returns a reference <code>DType</code> based on this <code>DType</code>.</p>  <h4 id=\"DType.is_floating\"><code>tf.DType.is_floating</code></h4> <p>Returns whether this is a (real) floating point type.</p>  <h4 id=\"DType.is_complex\"><code>tf.DType.is_complex</code></h4> <p>Returns whether this is a complex floating point type.</p>  <h4 id=\"DType.is_integer\"><code>tf.DType.is_integer</code></h4> <p>Returns whether this is a (non-quantized) integer type.</p>  <h4 id=\"DType.is_quantized\"><code>tf.DType.is_quantized</code></h4> <p>Returns whether this is a quantized data type.</p>  <h4 id=\"DType.is_unsigned\"><code>tf.DType.is_unsigned</code></h4> <p>Returns whether this type is unsigned.</p> <p>Non-numeric, unordered, and quantized types are not considered unsigned, and this function returns <code>False</code>.</p>  <h5 id=\"returns-26\">Returns:</h5> <p>Whether a <code>DType</code> is unsigned.</p>  <h4 id=\"DType.as_numpy_dtype\"><code>tf.DType.as_numpy_dtype</code></h4> <p>Returns a <code>numpy.dtype</code> based on this <code>DType</code>.</p>  <h4 id=\"DType.as_datatype_enum\"><code>tf.DType.as_datatype_enum</code></h4> <p>Returns a <code>types_pb2.DataType</code> enum value based on this <code>DType</code>.</p>  <h4 id=\"other-methods-4\">Other Methods</h4>  <h4 id=\"DType.__init__\"><code>tf.DType.__init__(type_enum)</code></h4> <p>Creates a new <code>DataType</code>.</p> <p>NOTE(mrry): In normal circumstances, you should not need to construct a <code>DataType</code> object directly. Instead, use the <code>tf.as_dtype()</code> function.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>type_enum</code>: A <code>types_pb2.DataType</code> enum value.</li> </ul>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>type_enum</code> is not a value <code>types_pb2.DataType</code>.</li> </ul>  <h4 id=\"DType.max\"><code>tf.DType.max</code></h4> <p>Returns the maximum representable value in this data type.</p>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if this is a non-numeric, unordered, or quantized type.</li> </ul>  <h4 id=\"DType.min\"><code>tf.DType.min</code></h4> <p>Returns the minimum representable value in this data type.</p>  <h5 id=\"raises-16\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if this is a non-numeric, unordered, or quantized type.</li> </ul>  <h4 id=\"DType.size\"><code>tf.DType.size</code></h4>  <h3 id=\"as_dtype\"><code>tf.as_dtype(type_value)</code></h3> <p>Converts the given <code>type_value</code> to a <code>DType</code>.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>type_value</code>: A value that can be converted to a <code>tf.DType</code> object. This may currently be a <code>tf.DType</code> object, a <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/types.proto\" rel=\"noreferrer\"><code>DataType</code> enum</a>, a string type name, or a <code>numpy.dtype</code>.</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>A <code>DType</code> corresponding to <code>type_value</code>.</p>  <h5 id=\"raises-17\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>type_value</code> cannot be converted to a <code>DType</code>.</li> </ul>  <h2 id=\"utility-functions\">Utility functions</h2>  <h3 id=\"device\"><code>tf.device(device_name_or_function)</code></h3> <p>Wrapper for <code>Graph.device()</code> using the default graph.</p> <p>See <a href=\"framework#Graph.device\"><code>Graph.device()</code></a> for more details.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>device_name_or_function</code>: The device name or function to use in the context.</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>A context manager that specifies the default device to use for newly created ops.</p>  <h3 id=\"container\"><code>tf.container(container_name)</code></h3> <p>Wrapper for <code>Graph.container()</code> using the default graph.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>container_name</code>: The container string to use in the context.</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>A context manager that specifies the default container to use for newly created stateful ops.</p>  <h3 id=\"name_scope\"><code>tf.name_scope(name)</code></h3> <p>Wrapper for <code>Graph.name_scope()</code> using the default graph.</p> <p>See <a href=\"framework#Graph.name_scope\"><code>Graph.name_scope()</code></a> for more details.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the scope.</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>A context manager that installs <code>name</code> as a new name scope in the default graph.</p>  <h3 id=\"control_dependencies\"><code>tf.control_dependencies(control_inputs)</code></h3> <p>Wrapper for <code>Graph.control_dependencies()</code> using the default graph.</p> <p>See <a href=\"framework#Graph.control_dependencies\"><code>Graph.control_dependencies()</code></a> for more details.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>control_inputs</code>: A list of <code>Operation</code> or <code>Tensor</code> objects which must be executed or computed before running the operations defined in the context. Can also be <code>None</code> to clear the control dependencies.</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>A context manager that specifies control dependencies for all operations constructed within the context.</p>  <h3 id=\"convert_to_tensor\"><code>tf.convert_to_tensor(value, dtype=None, name=None, as_ref=False)</code></h3> <p>Converts the given <code>value</code> to a <code>Tensor</code>.</p> <p>This function converts Python objects of various types to <code>Tensor</code> objects. It accepts <code>Tensor</code> objects, numpy arrays, Python lists, and Python scalars. For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">import numpy as np\n\ndef my_func(arg):\n  arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n  return tf.matmul(arg, arg) + arg\n\n# The following calls are equivalent.\nvalue_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))\nvalue_2 = my_func([[1.0, 2.0], [3.0, 4.0]])\nvalue_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n</pre> <p>This function can be useful when composing a new operation in Python (such as <code>my_func</code> in the example above). All standard Python op constructors apply this function to each of their Tensor-valued inputs, which allows those ops to accept numpy arrays, Python lists, and scalars in addition to <code>Tensor</code> objects.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>value</code>: An object whose type has a registered <code>Tensor</code> conversion function.</li> <li>\n<code>dtype</code>: Optional element type for the returned tensor. If missing, the type is inferred from the type of <code>value</code>.</li> <li>\n<code>name</code>: Optional name to use if a new <code>Tensor</code> is created.</li> <li>\n<code>as_ref</code>: True if we want the result as a ref tensor. Only used if a new <code>Tensor</code> is created.</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>A <code>Tensor</code> based on <code>value</code>.</p>  <h5 id=\"raises-18\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If no conversion function is registered for <code>value</code>.</li> <li>\n<code>RuntimeError</code>: If a registered conversion function returns an invalid value.</li> </ul>  <h3 id=\"convert_to_tensor_or_indexed_slices\"><code>tf.convert_to_tensor_or_indexed_slices(value, dtype=None, name=None, as_ref=False)</code></h3> <p>Converts the given object to a <code>Tensor</code> or an <code>IndexedSlices</code>.</p> <p>If <code>value</code> is an <code>IndexedSlices</code> or <code>SparseTensor</code> it is returned unmodified. Otherwise, it is converted to a <code>Tensor</code> using <code>convert_to_tensor()</code>.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>value</code>: An <code>IndexedSlices</code>, <code>SparseTensor</code>, or an object that can be consumed by <code>convert_to_tensor()</code>.</li> <li>\n<code>dtype</code>: (Optional.) The required <code>DType</code> of the returned <code>Tensor</code> or <code>IndexedSlices</code>.</li> <li>\n<code>name</code>: (Optional.) A name to use if a new <code>Tensor</code> is created.</li> <li>\n<code>as_ref</code>: True if the caller wants the results as ref tensors.</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>An <code>Tensor</code>, <code>IndexedSlices</code>, or <code>SparseTensor</code> based on <code>value</code>.</p>  <h5 id=\"raises-19\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>dtype</code> does not match the element type of <code>value</code>.</li> </ul>  <h3 id=\"get_default_graph\"><code>tf.get_default_graph()</code></h3> <p>Returns the default graph for the current thread.</p> <p>The returned graph will be the innermost graph on which a <code>Graph.as_default()</code> context has been entered, or a global default graph if none has been explicitly created.</p> <p>NOTE: The default graph is a property of the current thread. If you create a new thread, and wish to use the default graph in that thread, you must explicitly add a <code>with g.as_default():</code> in that thread's function.</p>  <h5 id=\"returns-34\">Returns:</h5> <p>The default <code>Graph</code> being used in the current thread.</p>  <h3 id=\"reset_default_graph\"><code>tf.reset_default_graph()</code></h3> <p>Clears the default graph stack and resets the global default graph.</p> <p>NOTE: The default graph is a property of the current thread. This function applies only to the current thread. Calling this function while a <code>tf.Session</code> or <code>tf.InteractiveSession</code> is active will result in undefined behavior. Using any previously created <code>tf.Operation</code> or <code>tf.Tensor</code> objects after calling this function will result in undefined behavior.</p>  <h3 id=\"import_graph_def\"><code>tf.import_graph_def(graph_def, input_map=None, return_elements=None, name=None, op_dict=None, producer_op_list=None)</code></h3> <p>Imports the TensorFlow graph in <code>graph_def</code> into the Python <code>Graph</code>.</p> <p>This function provides a way to import a serialized TensorFlow <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto\" rel=\"noreferrer\"><code>GraphDef</code></a> protocol buffer, and extract individual objects in the <code>GraphDef</code> as <a href=\"#Tensor\"><code>Tensor</code></a> and <a href=\"#Operation\"><code>Operation</code></a> objects. See <a href=\"#Graph.as_graph_def\"><code>Graph.as_graph_def()</code></a> for a way to create a <code>GraphDef</code> proto.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>graph_def</code>: A <code>GraphDef</code> proto containing operations to be imported into the default graph.</li> <li>\n<code>input_map</code>: A dictionary mapping input names (as strings) in <code>graph_def</code> to <code>Tensor</code> objects. The values of the named input tensors in the imported graph will be re-mapped to the respective <code>Tensor</code> values.</li> <li>\n<code>return_elements</code>: A list of strings containing operation names in <code>graph_def</code> that will be returned as <code>Operation</code> objects; and/or tensor names in <code>graph_def</code> that will be returned as <code>Tensor</code> objects.</li> <li>\n<code>name</code>: (Optional.) A prefix that will be prepended to the names in <code>graph_def</code>. Defaults to <code>\"import\"</code>.</li> <li>\n<code>op_dict</code>: (Optional.) A dictionary mapping op type names to <code>OpDef</code> protos. Must contain an <code>OpDef</code> proto for each op type named in <code>graph_def</code>. If omitted, uses the <code>OpDef</code> protos registered in the global registry.</li> <li>\n<code>producer_op_list</code>: (Optional.) An <code>OpList</code> proto with the (possibly stripped) list of <code>OpDef</code>s used by the producer of the graph. If provided, attrs for ops in <code>graph_def</code> that are not in <code>op_dict</code> that have their default value according to <code>producer_op_list</code> will be removed. This will allow some more <code>GraphDef</code>s produced by later binaries to be accepted by earlier binaries.</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>A list of <code>Operation</code> and/or <code>Tensor</code> objects from the imported graph, corresponding to the names in <code>return_elements</code>.</p>  <h5 id=\"raises-20\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>graph_def</code> is not a <code>GraphDef</code> proto, <code>input_map</code> is not a dictionary mapping strings to <code>Tensor</code> objects, or <code>return_elements</code> is not a list of strings.</li> <li>\n<code>ValueError</code>: If <code>input_map</code>, or <code>return_elements</code> contains names that do not appear in <code>graph_def</code>, or <code>graph_def</code> is not well-formed (e.g. it refers to an unknown tensor).</li> </ul>  <h3 id=\"load_file_system_library\"><code>tf.load_file_system_library(library_filename)</code></h3> <p>Loads a TensorFlow plugin, containing file system implementation.</p> <p>Pass <code>library_filename</code> to a platform-specific mechanism for dynamically loading a library. The rules for determining the exact location of the library are platform-specific and are not documented here.</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>library_filename</code>: Path to the plugin. Relative or absolute filesystem path to a dynamic library file.</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p>None.</p>  <h5 id=\"raises-21\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: when unable to load the library.</li> </ul>  <h3 id=\"load_op_library\"><code>tf.load_op_library(library_filename)</code></h3> <p>Loads a TensorFlow plugin, containing custom ops and kernels.</p> <p>Pass \"library_filename\" to a platform-specific mechanism for dynamically loading a library. The rules for determining the exact location of the library are platform-specific and are not documented here. When the library is loaded, ops and kernels registered in the library via the REGISTER_* macros are made available in the TensorFlow process. Note that ops with the same name as an existing op are rejected and not registered with the process.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>library_filename</code>: Path to the plugin. Relative or absolute filesystem path to a dynamic library file.</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <p>A python module containing the Python wrappers for Ops defined in the plugin.</p>  <h5 id=\"raises-22\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: when unable to load the library or get the python wrappers.</li> </ul>  <h2 id=\"graph-collections\">Graph collections</h2>  <h3 id=\"add_to_collection\"><code>tf.add_to_collection(name, value)</code></h3> <p>Wrapper for <code>Graph.add_to_collection()</code> using the default graph.</p> <p>See <a href=\"framework#Graph.add_to_collection\"><code>Graph.add_to_collection()</code></a> for more details.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>name</code>: The key for the collection. For example, the <code>GraphKeys</code> class contains many standard names for collections.</li> <li>\n<code>value</code>: The value to add to the collection.</li> </ul>  <h3 id=\"get_collection\"><code>tf.get_collection(key, scope=None)</code></h3> <p>Wrapper for <code>Graph.get_collection()</code> using the default graph.</p> <p>See <a href=\"framework#Graph.get_collection\"><code>Graph.get_collection()</code></a> for more details.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>key</code>: The key for the collection. For example, the <code>GraphKeys</code> class contains many standard names for collections.</li> <li>\n<code>scope</code>: (Optional.) If supplied, the resulting list is filtered to include only items whose <code>name</code> attribute matches using <code>re.match</code>. Items without a <code>name</code> attribute are never returned if a scope is supplied and the choice or <code>re.match</code> means that a <code>scope</code> without special tokens filters by prefix.</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>The list of values in the collection with the given <code>name</code>, or an empty list if no value has been added to that collection. The list contains the values in the order under which they were collected.</p>  <h3 id=\"get_collection_ref\"><code>tf.get_collection_ref(key)</code></h3> <p>Wrapper for <code>Graph.get_collection_ref()</code> using the default graph.</p> <p>See <a href=\"framework#Graph.get_collection_ref\"><code>Graph.get_collection_ref()</code></a> for more details.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>key</code>: The key for the collection. For example, the <code>GraphKeys</code> class contains many standard names for collections.</li> </ul>  <h5 id=\"returns-39\">Returns:</h5> <p>The list of values in the collection with the given <code>name</code>, or an empty list if no value has been added to that collection. Note that this returns the collection list itself, which can be modified in place to change the collection.</p>  <h3 id=\"GraphKeys\"><code>class tf.GraphKeys</code></h3> <p>Standard names to use for graph collections.</p> <p>The standard library uses various well-known names to collect and retrieve values associated with a graph. For example, the <code>tf.Optimizer</code> subclasses default to optimizing the variables collected under <code>tf.GraphKeys.TRAINABLE_VARIABLES</code> if none is specified, but it is also possible to pass an explicit list of variables.</p> <p>The following standard keys are defined:</p> <ul> <li>\n<code>VARIABLES</code>: the <code>Variable</code> objects that comprise a model, and must be saved and restored together. See <a href=\"state_ops#all_variables\"><code>tf.all_variables()</code></a> for more details.</li> <li>\n<code>TRAINABLE_VARIABLES</code>: the subset of <code>Variable</code> objects that will be trained by an optimizer. See <a href=\"state_ops#trainable_variables\"><code>tf.trainable_variables()</code></a> for more details.</li> <li>\n<code>SUMMARIES</code>: the summary <code>Tensor</code> objects that have been created in the graph. See <a href=\"train#merge_all_summaries\"><code>tf.merge_all_summaries()</code></a> for more details.</li> <li>\n<code>QUEUE_RUNNERS</code>: the <code>QueueRunner</code> objects that are used to produce input for a computation. See <a href=\"train#start_queue_runners\"><code>tf.start_queue_runners()</code></a> for more details.</li> <li>\n<code>MOVING_AVERAGE_VARIABLES</code>: the subset of <code>Variable</code> objects that will also keep moving averages. See <a href=\"state_ops#moving_average_variables\"><code>tf.moving_average_variables()</code></a> for more details.</li> <li>\n<code>REGULARIZATION_LOSSES</code>: regularization losses collected during graph construction.</li> <li>\n<code>WEIGHTS</code>: weights inside neural network layers</li> <li>\n<code>BIASES</code>: biases inside neural network layers</li> <li>\n<code>ACTIVATIONS</code>: activations of neural network layers</li> </ul>  <h2 id=\"defining-new-operations\">Defining new operations</h2>  <h3 id=\"RegisterGradient\"><code>class tf.RegisterGradient</code></h3> <p>A decorator for registering the gradient function for an op type.</p> <p>This decorator is only used when defining a new op type. For an op with <code>m</code> inputs and <code>n</code> outputs, the gradient function is a function that takes the original <code>Operation</code> and <code>n</code> <code>Tensor</code> objects (representing the gradients with respect to each output of the op), and returns <code>m</code> <code>Tensor</code> objects (representing the partial gradients with respect to each input of the op).</p> <p>For example, assuming that operations of type <code>\"Sub\"</code> take two inputs <code>x</code> and <code>y</code>, and return a single output <code>x - y</code>, the following gradient function would be registered:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">@tf.RegisterGradient(\"Sub\")\ndef _sub_grad(unused_op, grad):\n  return grad, tf.neg(grad)\n</pre> <p>The decorator argument <code>op_type</code> is the string type of an operation. This corresponds to the <code>OpDef.name</code> field for the proto that defines the operation.</p>  <h4 id=\"RegisterGradient.__init__\"><code>tf.RegisterGradient.__init__(op_type)</code></h4> <p>Creates a new decorator with <code>op_type</code> as the Operation type.</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>op_type</code>: The string type of an operation. This corresponds to the <code>OpDef.name</code> field for the proto that defines the operation.</li> </ul>  <h3 id=\"NoGradient\"><code>tf.NoGradient(op_type)</code></h3> <p>Specifies that ops of type <code>op_type</code> do not have a defined gradient.</p> <p>This function is only used when defining a new op type. It may be used for ops such as <code>tf.size()</code> that are not differentiable. For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">tf.NoGradient(\"Size\")\n</pre>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>op_type</code>: The string type of an operation. This corresponds to the <code>OpDef.name</code> field for the proto that defines the operation.</li> </ul>  <h5 id=\"raises-23\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>op_type</code> is not a string.</li> </ul>  <h3 id=\"RegisterShape\"><code>class tf.RegisterShape</code></h3> <p>A decorator for registering the shape function for an op type.</p> <p>This decorator is only used when defining a new op type. A shape function is a function from an <code>Operation</code> object to a list of <code>TensorShape</code> objects, with one <code>TensorShape</code> for each output of the operation.</p> <p>For example, assuming that operations of type <code>\"Sub\"</code> take two inputs <code>x</code> and <code>y</code>, and return a single output <code>x - y</code>, all with the same shape, the following shape function would be registered:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">@tf.RegisterShape(\"Sub\")\ndef _sub_shape(op):\n  return [op.inputs[0].get_shape().merge_with(op.inputs[1].get_shape())]\n</pre> <p>The decorator argument <code>op_type</code> is the string type of an operation. This corresponds to the <code>OpDef.name</code> field for the proto that defines the operation.</p>  <h4 id=\"RegisterShape.__init__\"><code>tf.RegisterShape.__init__(op_type)</code></h4> <p>Saves the <code>op_type</code> as the <code>Operation</code> type.</p>  <h3 id=\"TensorShape\"><code>class tf.TensorShape</code></h3> <p>Represents the shape of a <code>Tensor</code>.</p> <p>A <code>TensorShape</code> represents a possibly-partial shape specification for a <code>Tensor</code>. It may be one of the following:</p> <ul> <li>\n<em>Fully-known shape:</em> has a known number of dimensions and a known size for each dimension.</li> <li>\n<em>Partially-known shape:</em> has a known number of dimensions, and an unknown size for one or more dimension.</li> <li>\n<em>Unknown shape:</em> has an unknown number of dimensions, and an unknown size in all dimensions.</li> </ul> <p>If a tensor is produced by an operation of type <code>\"Foo\"</code>, its shape may be inferred if there is a registered shape function for <code>\"Foo\"</code>. See <a href=\"framework#RegisterShape\"><code>tf.RegisterShape()</code></a> for details of shape functions and how to register them. Alternatively, the shape may be set explicitly using <a href=\"framework#Tensor.set_shape\"><code>Tensor.set_shape()</code></a>.</p>  <h4 id=\"TensorShape.merge_with\"><code>tf.TensorShape.merge_with(other)</code></h4> <p>Returns a <code>TensorShape</code> combining the information in <code>self</code> and <code>other</code>.</p> <p>The dimensions in <code>self</code> and <code>other</code> are merged elementwise, according to the rules defined for <code>Dimension.merge_with()</code>.</p>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>other</code>: Another <code>TensorShape</code>.</li> </ul>  <h5 id=\"returns-40\">Returns:</h5> <p>A <code>TensorShape</code> containing the combined information of <code>self</code> and <code>other</code>.</p>  <h5 id=\"raises-24\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> and <code>other</code> are not compatible.</li> </ul>  <h4 id=\"TensorShape.concatenate\"><code>tf.TensorShape.concatenate(other)</code></h4> <p>Returns the concatenation of the dimension in <code>self</code> and <code>other</code>.</p> <p><em>N.B.</em> If either <code>self</code> or <code>other</code> is completely unknown, concatenation will discard information about the other shape. In future, we might support concatenation that preserves this information for use with slicing.</p>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>other</code>: Another <code>TensorShape</code>.</li> </ul>  <h5 id=\"returns-41\">Returns:</h5> <p>A <code>TensorShape</code> whose dimensions are the concatenation of the dimensions in <code>self</code> and <code>other</code>.</p>  <h4 id=\"TensorShape.ndims\"><code>tf.TensorShape.ndims</code></h4> <p>Returns the rank of this shape, or None if it is unspecified.</p>  <h4 id=\"TensorShape.dims\"><code>tf.TensorShape.dims</code></h4> <p>Returns a list of Dimensions, or None if the shape is unspecified.</p>  <h4 id=\"TensorShape.as_list\"><code>tf.TensorShape.as_list()</code></h4> <p>Returns a list of integers or None for each dimension.</p>  <h5 id=\"returns-42\">Returns:</h5> <p>A list of integers or None for each dimension.</p>  <h4 id=\"TensorShape.as_proto\"><code>tf.TensorShape.as_proto()</code></h4> <p>Returns this shape as a <code>TensorShapeProto</code>.</p>  <h4 id=\"TensorShape.is_compatible_with\"><code>tf.TensorShape.is_compatible_with(other)</code></h4> <p>Returns True iff <code>self</code> is compatible with <code>other</code>.</p> <p>Two possibly-partially-defined shapes are compatible if there exists a fully-defined shape that both shapes can represent. Thus, compatibility allows the shape inference code to reason about partially-defined shapes. For example:</p> <ul> <li><p>TensorShape(None) is compatible with all shapes.</p></li> <li><p>TensorShape([None, None]) is compatible with all two-dimensional shapes, such as TensorShape([32, 784]), and also TensorShape(None). It is not compatible with, for example, TensorShape([None]) or TensorShape([None, None, None]).</p></li> <li><p>TensorShape([32, None]) is compatible with all two-dimensional shapes with size 32 in the 0th dimension, and also TensorShape([None, None]) and TensorShape(None). It is not compatible with, for example, TensorShape([32]), TensorShape([32, None, 1]) or TensorShape([64, None]).</p></li> <li><p>TensorShape([32, 784]) is compatible with itself, and also TensorShape([32, None]), TensorShape([None, 784]), TensorShape([None, None]) and TensorShape(None). It is not compatible with, for example, TensorShape([32, 1, 784]) or TensorShape([None]).</p></li> </ul> <p>The compatibility relation is reflexive and symmetric, but not transitive. For example, TensorShape([32, 784]) is compatible with TensorShape(None), and TensorShape(None) is compatible with TensorShape([4, 4]), but TensorShape([32, 784]) is not compatible with TensorShape([4, 4]).</p>  <h5 id=\"args-42\">Args:</h5> <ul> <li>\n<code>other</code>: Another TensorShape.</li> </ul>  <h5 id=\"returns-43\">Returns:</h5> <p>True iff <code>self</code> is compatible with <code>other</code>.</p>  <h4 id=\"TensorShape.is_fully_defined\"><code>tf.TensorShape.is_fully_defined()</code></h4> <p>Returns True iff <code>self</code> is fully defined in every dimension.</p>  <h4 id=\"TensorShape.with_rank\"><code>tf.TensorShape.with_rank(rank)</code></h4> <p>Returns a shape based on <code>self</code> with the given rank.</p> <p>This method promotes a completely unknown shape to one with a known rank.</p>  <h5 id=\"args-43\">Args:</h5> <ul> <li>\n<code>rank</code>: An integer.</li> </ul>  <h5 id=\"returns-44\">Returns:</h5> <p>A shape that is at least as specific as <code>self</code> with the given rank.</p>  <h5 id=\"raises-25\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> does not represent a shape with the given <code>rank</code>.</li> </ul>  <h4 id=\"TensorShape.with_rank_at_least\"><code>tf.TensorShape.with_rank_at_least(rank)</code></h4> <p>Returns a shape based on <code>self</code> with at least the given rank.</p>  <h5 id=\"args-44\">Args:</h5> <ul> <li>\n<code>rank</code>: An integer.</li> </ul>  <h5 id=\"returns-45\">Returns:</h5> <p>A shape that is at least as specific as <code>self</code> with at least the given rank.</p>  <h5 id=\"raises-26\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> does not represent a shape with at least the given <code>rank</code>.</li> </ul>  <h4 id=\"TensorShape.with_rank_at_most\"><code>tf.TensorShape.with_rank_at_most(rank)</code></h4> <p>Returns a shape based on <code>self</code> with at most the given rank.</p>  <h5 id=\"args-45\">Args:</h5> <ul> <li>\n<code>rank</code>: An integer.</li> </ul>  <h5 id=\"returns-46\">Returns:</h5> <p>A shape that is at least as specific as <code>self</code> with at most the given rank.</p>  <h5 id=\"raises-27\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> does not represent a shape with at most the given <code>rank</code>.</li> </ul>  <h4 id=\"TensorShape.assert_has_rank\"><code>tf.TensorShape.assert_has_rank(rank)</code></h4> <p>Raises an exception if <code>self</code> is not compatible with the given <code>rank</code>.</p>  <h5 id=\"args-46\">Args:</h5> <ul> <li>\n<code>rank</code>: An integer.</li> </ul>  <h5 id=\"raises-28\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> does not represent a shape with the given <code>rank</code>.</li> </ul>  <h4 id=\"TensorShape.assert_same_rank\"><code>tf.TensorShape.assert_same_rank(other)</code></h4> <p>Raises an exception if <code>self</code> and <code>other</code> do not have compatible ranks.</p>  <h5 id=\"args-47\">Args:</h5> <ul> <li>\n<code>other</code>: Another <code>TensorShape</code>.</li> </ul>  <h5 id=\"raises-29\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> and <code>other</code> do not represent shapes with the same rank.</li> </ul>  <h4 id=\"TensorShape.assert_is_compatible_with\"><code>tf.TensorShape.assert_is_compatible_with(other)</code></h4> <p>Raises exception if <code>self</code> and <code>other</code> do not represent the same shape.</p> <p>This method can be used to assert that there exists a shape that both <code>self</code> and <code>other</code> represent.</p>  <h5 id=\"args-48\">Args:</h5> <ul> <li>\n<code>other</code>: Another TensorShape.</li> </ul>  <h5 id=\"raises-30\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> and <code>other</code> do not represent the same shape.</li> </ul>  <h4 id=\"TensorShape.assert_is_fully_defined\"><code>tf.TensorShape.assert_is_fully_defined()</code></h4> <p>Raises an exception if <code>self</code> is not fully defined in every dimension.</p>  <h5 id=\"raises-31\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> does not have a known value for every dimension.</li> </ul>  <h4 id=\"other-methods-5\">Other Methods</h4>  <h4 id=\"TensorShape.__init__\"><code>tf.TensorShape.__init__(dims)</code></h4> <p>Creates a new TensorShape with the given dimensions.</p>  <h5 id=\"args-49\">Args:</h5> <ul> <li>\n<code>dims</code>: A list of Dimensions, or None if the shape is unspecified.</li> <li>\n<code>DEPRECATED</code>: A single integer is treated as a singleton list.</li> </ul>  <h5 id=\"raises-32\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If dims cannot be converted to a list of dimensions.</li> </ul>  <h4 id=\"TensorShape.num_elements\"><code>tf.TensorShape.num_elements()</code></h4> <p>Returns the total number of elements, or none for incomplete shapes.</p>  <h3 id=\"Dimension\"><code>class tf.Dimension</code></h3> <p>Represents the value of one dimension in a TensorShape.</p>  <h4 id=\"Dimension.__init__\"><code>tf.Dimension.__init__(value)</code></h4> <p>Creates a new Dimension with the given value.</p>  <h4 id=\"Dimension.assert_is_compatible_with\"><code>tf.Dimension.assert_is_compatible_with(other)</code></h4> <p>Raises an exception if <code>other</code> is not compatible with this Dimension.</p>  <h5 id=\"args-50\">Args:</h5> <ul> <li>\n<code>other</code>: Another Dimension.</li> </ul>  <h5 id=\"raises-33\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> and <code>other</code> are not compatible (see is_compatible_with).</li> </ul>  <h4 id=\"Dimension.is_compatible_with\"><code>tf.Dimension.is_compatible_with(other)</code></h4> <p>Returns true if <code>other</code> is compatible with this Dimension.</p> <p>Two known Dimensions are compatible if they have the same value. An unknown Dimension is compatible with all other Dimensions.</p>  <h5 id=\"args-51\">Args:</h5> <ul> <li>\n<code>other</code>: Another Dimension.</li> </ul>  <h5 id=\"returns-47\">Returns:</h5> <p>True if this Dimension and <code>other</code> are compatible.</p>  <h4 id=\"Dimension.merge_with\"><code>tf.Dimension.merge_with(other)</code></h4> <p>Returns a Dimension that combines the information in <code>self</code> and <code>other</code>.</p> <p>Dimensions are combined as follows:</p> <pre class=\"lang-m no-auto-prettify\">Dimension(n)   .merge_with(Dimension(n))    == Dimension(n)\nDimension(n)   .merge_with(Dimension(None)) == Dimension(n)\nDimension(None).merge_with(Dimension(n))    == Dimension(n)\nDimension(None).merge_with(Dimension(None)) == Dimension(None)\nDimension(n)   .merge_with(Dimension(m)) raises ValueError for n != m\n</pre>  <h5 id=\"args-52\">Args:</h5> <ul> <li>\n<code>other</code>: Another Dimension.</li> </ul>  <h5 id=\"returns-48\">Returns:</h5> <p>A Dimension containing the combined information of <code>self</code> and <code>other</code>.</p>  <h5 id=\"raises-34\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>self</code> and <code>other</code> are not compatible (see is_compatible_with).</li> </ul>  <h4 id=\"Dimension.value\"><code>tf.Dimension.value</code></h4> <p>The value of this dimension, or None if it is unknown.</p>  <h3 id=\"op_scope\"><code>tf.op_scope(values, name, default_name=None)</code></h3> <p>Returns a context manager for use when defining a Python op.</p> <p>This context manager validates that the given <code>values</code> are from the same graph, ensures that graph is the default graph, and pushes a name scope.</p> <p>For example, to define a new Python op called <code>my_op</code>:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def my_op(a, b, c, name=None):\n  with tf.op_scope([a, b, c], name, \"MyOp\") as scope:\n    a = tf.convert_to_tensor(a, name=\"a\")\n    b = tf.convert_to_tensor(b, name=\"b\")\n    c = tf.convert_to_tensor(c, name=\"c\")\n    # Define some computation that uses `a`, `b`, and `c`.\n    return foo_op(..., name=scope)\n</pre>  <h5 id=\"args-53\">Args:</h5> <ul> <li>\n<code>values</code>: The list of <code>Tensor</code> arguments that are passed to the op function.</li> <li>\n<code>name</code>: The name argument that is passed to the op function.</li> <li>\n<code>default_name</code>: The default name to use if the <code>name</code> argument is <code>None</code>.</li> </ul>  <h5 id=\"returns-49\">Returns:</h5> <p>A context manager for use in defining Python ops. Yields the name scope.</p>  <h5 id=\"raises-35\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if neither <code>name</code> nor <code>default_name</code> is provided.</li> </ul>  <h3 id=\"get_seed\"><code>tf.get_seed(op_seed)</code></h3> <p>Returns the local seeds an operation should use given an op-specific seed.</p> <p>Given operation-specific seed, <code>op_seed</code>, this helper function returns two seeds derived from graph-level and op-level seeds. Many random operations internally use the two seeds to allow user to change the seed globally for a graph, or for only specific operations.</p> <p>For details on how the graph-level seed interacts with op seeds, see <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a>.</p>  <h5 id=\"args-54\">Args:</h5> <ul> <li>\n<code>op_seed</code>: integer.</li> </ul>  <h5 id=\"returns-50\">Returns:</h5> <p>A tuple of two integers that should be used for the local seed of this operation.</p>  <h2 id=\"for-libraries-building-on-tensorflow\">For libraries building on TensorFlow</h2>  <h3 id=\"register_tensor_conversion_function\"><code>tf.register_tensor_conversion_function(base_type, conversion_func, priority=100)</code></h3> <p>Registers a function for converting objects of <code>base_type</code> to <code>Tensor</code>.</p> <p>The conversion function must have the following signature:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def conversion_func(value, dtype=None, name=None, as_ref=False):\n  # ...\n</pre> <p>It must return a <code>Tensor</code> with the given <code>dtype</code> if specified. If the conversion function creates a new <code>Tensor</code>, it should use the given <code>name</code> if specified. All exceptions will be propagated to the caller.</p> <p>The conversion function may return <code>NotImplemented</code> for some inputs. In this case, the conversion process will continue to try subsequent conversion functions.</p> <p>If <code>as_ref</code> is true, the function must return a <code>Tensor</code> reference, such as a <code>Variable</code>.</p> <p>NOTE: The conversion functions will execute in order of priority, followed by order of registration. To ensure that a conversion function <code>F</code> runs before another conversion function <code>G</code>, ensure that <code>F</code> is registered with a smaller priority than <code>G</code>.</p>  <h5 id=\"args-55\">Args:</h5> <ul> <li>\n<code>base_type</code>: The base type or tuple of base types for all objects that <code>conversion_func</code> accepts.</li> <li>\n<code>conversion_func</code>: A function that converts instances of <code>base_type</code> to <code>Tensor</code>.</li> <li>\n<code>priority</code>: Optional integer that indicates the priority for applying this conversion function. Conversion functions with smaller priority values run earlier than conversion functions with larger priority values. Defaults to 100.</li> </ul>  <h5 id=\"raises-36\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If the arguments do not have the appropriate type.</li> </ul>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"DeviceSpec\"><code>class tf.DeviceSpec</code></h3> <p>Represents a (possibly partial) specification for a TensorFlow device.</p> <p><code>DeviceSpec</code>s are used throughout TensorFlow to describe where state is stored and computations occur. Using <code>DeviceSpec</code> allows you to parse device spec strings to verify their validity, merge them or compose them programmatically.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Place the operations on device \"GPU:0\" in the \"ps\" job.\ndevice_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\nwith tf.device(device_spec):\n  # Both my_var and squared_var will be placed on /job:ps/device:GPU:0.\n  my_var = tf.Variable(..., name=\"my_variable\")\n  squared_var = tf.square(my_var)\n</pre> <p>If a <code>DeviceSpec</code> is partially specified, it will be merged with other <code>DeviceSpec</code>s according to the scope in which it is defined. <code>DeviceSpec</code> components defined in inner scopes take precedence over those defined in outer scopes.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.device(DeviceSpec(job=\"train\", )):\n  with tf.device(DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0):\n    # Nodes created here will be assigned to /job:ps/device:GPU:0.\n  with tf.device(DeviceSpec(device_type=\"GPU\", device_index=1):\n    # Nodes created here will be assigned to /job:train/device:GPU:1.\n</pre> <p>A <code>DeviceSpec</code> consists of 5 components -- each of which is optionally specified:</p> <ul> <li>Job: The job name.</li> <li>Replica: The replica index.</li> <li>Task: The task index.</li> <li>Device type: The device type string (e.g. \"CPU\" or \"GPU\").</li> <li>Device index: The device index. - - -</li> </ul> <h4 id=\"DeviceSpec.__init__\"><code>tf.DeviceSpec.__init__(job=None, replica=None, task=None, device_type=None, device_index=None)</code></h4> <p>Create a new <code>DeviceSpec</code> object.</p>  <h5 id=\"args-56\">Args:</h5> <ul> <li>\n<code>job</code>: string. Optional job name.</li> <li>\n<code>replica</code>: int. Optional replica index.</li> <li>\n<code>task</code>: int. Optional task index.</li> <li>\n<code>device_type</code>: Optional device type string (e.g. \"CPU\" or \"GPU\")</li> <li>\n<code>device_index</code>: int. Optional device index. If left unspecified, device represents 'any' device_index.</li> </ul>  <h4 id=\"DeviceSpec.from_string\"><code>tf.DeviceSpec.from_string(spec)</code></h4> <p>Construct a <code>DeviceSpec</code> from a string.</p>  <h5 id=\"args-57\">Args:</h5> <ul> <li>\n<code>spec</code>: a string of the form /job:<name>/replica:<id>/task:<id>/device:CPU:<id> or /job:<name>/replica:<id>/task:<id>/device:GPU:<id> as cpu and gpu are mutually exclusive. All entries are optional.</id></id></id></name></id></id></id></name>\n</li> </ul>  <h5 id=\"returns-51\">Returns:</h5> <p>A DeviceSpec.</p>  <h4 id=\"DeviceSpec.job\"><code>tf.DeviceSpec.job</code></h4>  <h4 id=\"DeviceSpec.merge_from\"><code>tf.DeviceSpec.merge_from(dev)</code></h4> <p>Merge the properties of \"dev\" into this <code>DeviceSpec</code>.</p>  <h5 id=\"args-58\">Args:</h5> <ul> <li>\n<code>dev</code>: a <code>DeviceSpec</code>.</li> </ul>  <h4 id=\"DeviceSpec.parse_from_string\"><code>tf.DeviceSpec.parse_from_string(spec)</code></h4> <p>Parse a <code>DeviceSpec</code> name into its components.</p>  <h5 id=\"args-59\">Args:</h5> <ul> <li>\n<code>spec</code>: a string of the form /job:<name>/replica:<id>/task:<id>/device:CPU:<id> or /job:<name>/replica:<id>/task:<id>/device:GPU:<id> as cpu and gpu are mutually exclusive. All entries are optional.</id></id></id></name></id></id></id></name>\n</li> </ul>  <h5 id=\"returns-52\">Returns:</h5> <p>The <code>DeviceSpec</code>.</p>  <h5 id=\"raises-37\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the spec was not valid.</li> </ul>  <h4 id=\"DeviceSpec.replica\"><code>tf.DeviceSpec.replica</code></h4>  <h4 id=\"DeviceSpec.task\"><code>tf.DeviceSpec.task</code></h4>  <h4 id=\"DeviceSpec.to_string\"><code>tf.DeviceSpec.to_string()</code></h4> <p>Return a string representation of this <code>DeviceSpec</code>.</p>  <h5 id=\"returns-53\">Returns:</h5> <p>a string of the form /job:<name>/replica:<id>/task:<id>/device:<device_type>:<id>.</id></device_type></id></id></name></p>  <h3 id=\"bytes\"><code>class tf.bytes</code></h3> <p>str(object='') -&gt; string</p> <p>Return a nice string representation of the object. If the argument is a string, the return value is the same object.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html</a>\n  </p>\n</div>\n","check_ops":"<h1 id=\"asserts-and-boolean-checks\">Asserts and boolean checks.</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#asserts-and-boolean-checks\">Asserts and boolean checks.</a></li> <ul> <li><a href=\"#asserts-and-boolean-checks\">Asserts and Boolean Checks</a></li> <ul> <li><a href=\"#assert_negative\"><code>tf.assert_negative(x, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_positive\"><code>tf.assert_positive(x, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_proper_iterable\"><code>tf.assert_proper_iterable(values)</code></a></li> <li><a href=\"#assert_non_negative\"><code>tf.assert_non_negative(x, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_non_positive\"><code>tf.assert_non_positive(x, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_equal\"><code>tf.assert_equal(x, y, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_integer\"><code>tf.assert_integer(x, message=None, name=None)</code></a></li> <li><a href=\"#assert_less\"><code>tf.assert_less(x, y, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_less_equal\"><code>tf.assert_less_equal(x, y, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_rank\"><code>tf.assert_rank(x, rank, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_rank_at_least\"><code>tf.assert_rank_at_least(x, rank, data=None, summarize=None, message=None, name=None)</code></a></li> <li><a href=\"#assert_type\"><code>tf.assert_type(tensor, tf_type, message=None, name=None)</code></a></li> <li><a href=\"#is_non_decreasing\"><code>tf.is_non_decreasing(x, name=None)</code></a></li> <li><a href=\"#is_numeric_tensor\"><code>tf.is_numeric_tensor(tensor)</code></a></li> <li><a href=\"#is_strictly_increasing\"><code>tf.is_strictly_increasing(x, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"asserts-and-boolean-checks\">Asserts and Boolean Checks</h2>  <h3 id=\"assert_negative\"><code>tf.assert_negative(x, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert the condition <code>x &lt; 0</code> holds element-wise.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_negative(x)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_negative(x)], x)\n</pre> <p>Negative means, for every element <code>x[i]</code> of <code>x</code>, we have <code>x[i] &lt; 0</code>. If <code>x</code> is empty this is trivially satisfied.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_negative\".</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>Op raising <code>InvalidArgumentError</code> unless <code>x</code> is all negative.</p>  <h3 id=\"assert_positive\"><code>tf.assert_positive(x, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert the condition <code>x &gt; 0</code> holds element-wise.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_positive(x)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_positive(x)], x)\n</pre> <p>Positive means, for every element <code>x[i]</code> of <code>x</code>, we have <code>x[i] &gt; 0</code>. If <code>x</code> is empty this is trivially satisfied.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_positive\".</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>Op raising <code>InvalidArgumentError</code> unless <code>x</code> is all positive.</p>  <h3 id=\"assert_proper_iterable\"><code>tf.assert_proper_iterable(values)</code></h3> <p>Static assert that values is a \"proper\" iterable.</p> <p><code>Ops</code> that expect iterables of <code>Tensor</code> can call this to validate input. Useful since <code>Tensor</code>, <code>ndarray</code>, byte/text type are all iterables themselves.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>values</code>: Object to be checked.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>values</code> is not iterable or is one of <code>Tensor</code>, <code>SparseTensor</code>, <code>np.array</code>, <code>tf.compat.bytes_or_text_types</code>.</li> </ul>  <h3 id=\"assert_non_negative\"><code>tf.assert_non_negative(x, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert the condition <code>x &gt;= 0</code> holds element-wise.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_non_negative(x)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_non_negative(x)], x)\n</pre> <p>Non-negative means, for every element <code>x[i]</code> of <code>x</code>, we have <code>x[i] &gt;= 0</code>. If <code>x</code> is empty this is trivially satisfied.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_non_negative\".</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>Op raising <code>InvalidArgumentError</code> unless <code>x</code> is all non-negative.</p>  <h3 id=\"assert_non_positive\"><code>tf.assert_non_positive(x, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert the condition <code>x &lt;= 0</code> holds element-wise.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_non_positive(x)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_non_positive(x)], x)\n</pre> <p>Non-positive means, for every element <code>x[i]</code> of <code>x</code>, we have <code>x[i] &lt;= 0</code>. If <code>x</code> is empty this is trivially satisfied.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_non_positive\".</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>Op raising <code>InvalidArgumentError</code> unless <code>x</code> is all non-positive.</p>  <h3 id=\"assert_equal\"><code>tf.assert_equal(x, y, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert the condition <code>x == y</code> holds element-wise.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_equal(x, y)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_equal(x, y)], x)\n</pre> <p>This condition holds if for every pair of (possibly broadcast) elements <code>x[i]</code>, <code>y[i]</code>, we have <code>x[i] == y[i]</code>. If both <code>x</code> and <code>y</code> are empty, this is trivially satisfied.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>y</code>: Numeric <code>Tensor</code>, same dtype as and broadcastable to <code>x</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>, <code>y</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_equal\".</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>Op that raises <code>InvalidArgumentError</code> if <code>x == y</code> is False.</p>  <h3 id=\"assert_integer\"><code>tf.assert_integer(x, message=None, name=None)</code></h3> <p>Assert that <code>x</code> is of integer dtype.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_integer(x)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_integer(x)], x)\n</pre>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>x</code>: <code>Tensor</code> whose basetype is integer and is not quantized.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_integer\".</li> </ul>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>x.dtype</code> is anything other than non-quantized integer.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A <code>no_op</code> that does nothing. Type can be determined statically.</p>  <h3 id=\"assert_less\"><code>tf.assert_less(x, y, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert the condition <code>x &lt; y</code> holds element-wise.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_less(x, y)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_less(x, y)], x)\n</pre> <p>This condition holds if for every pair of (possibly broadcast) elements <code>x[i]</code>, <code>y[i]</code>, we have <code>x[i] &lt; y[i]</code>. If both <code>x</code> and <code>y</code> are empty, this is trivially satisfied.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>y</code>: Numeric <code>Tensor</code>, same dtype as and broadcastable to <code>x</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>, <code>y</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_less\".</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>Op that raises <code>InvalidArgumentError</code> if <code>x &lt; y</code> is False.</p>  <h3 id=\"assert_less_equal\"><code>tf.assert_less_equal(x, y, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert the condition <code>x &lt;= y</code> holds element-wise.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_less_equal(x, y)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_less_equal(x, y)], x)\n</pre> <p>This condition holds if for every pair of (possibly broadcast) elements <code>x[i]</code>, <code>y[i]</code>, we have <code>x[i] &lt;= y[i]</code>. If both <code>x</code> and <code>y</code> are empty, this is trivially satisfied.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>y</code>: Numeric <code>Tensor</code>, same dtype as and broadcastable to <code>x</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>, <code>y</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_less_equal\"</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>Op that raises <code>InvalidArgumentError</code> if <code>x &lt;= y</code> is False.</p>  <h3 id=\"assert_rank\"><code>tf.assert_rank(x, rank, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert <code>x</code> has rank equal to <code>rank</code>.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_rank(x, 2)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_rank(x, 2)], x)\n</pre>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>rank</code>: Scalar integer <code>Tensor</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_rank\".</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>Op raising <code>InvalidArgumentError</code> unless <code>x</code> has specified rank. If static checks determine <code>x</code> has correct rank, a <code>no_op</code> is returned.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If static checks determine <code>x</code> has wrong rank.</li> </ul>  <h3 id=\"assert_rank_at_least\"><code>tf.assert_rank_at_least(x, rank, data=None, summarize=None, message=None, name=None)</code></h3> <p>Assert <code>x</code> has rank equal to <code>rank</code> or higher.</p> <p>Example of adding a dependency to an operation:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.control_dependencies([tf.assert_rank_at_least(x, 2)]):\n  output = tf.reduce_sum(x)\n</pre> <p>Example of adding dependency to the tensor being checked:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.with_dependencies([tf.assert_rank_at_least(x, 2)], x)\n</pre>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>rank</code>: Scalar <code>Tensor</code>.</li> <li>\n<code>data</code>: The tensors to print out if the condition is False. Defaults to error message and first few entries of <code>x</code>.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"assert_rank_at_least\".</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>Op raising <code>InvalidArgumentError</code> unless <code>x</code> has specified rank or higher. If static checks determine <code>x</code> has correct rank, a <code>no_op</code> is returned.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If static checks determine <code>x</code> has wrong rank.</li> </ul>  <h3 id=\"assert_type\"><code>tf.assert_type(tensor, tf_type, message=None, name=None)</code></h3> <p>Statically asserts that the given <code>Tensor</code> is of the specified type.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>tensor</code>: A tensorflow <code>Tensor</code>.</li> <li>\n<code>tf_type</code>: A tensorflow type (dtypes.float32, tf.int64, dtypes.bool, etc).</li> <li>\n<code>message</code>: A string to prefix to the default message.</li> <li>\n<code>name</code>: A name to give this <code>Op</code>. Defaults to \"assert_type\"</li> </ul>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If the tensors data type doesn't match tf_type.</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A <code>no_op</code> that does nothing. Type can be determined statically.</p>  <h3 id=\"is_non_decreasing\"><code>tf.is_non_decreasing(x, name=None)</code></h3> <p>Returns <code>True</code> if <code>x</code> is non-decreasing.</p> <p>Elements of <code>x</code> are compared in row-major order. The tensor <code>[x[0],...]</code> is non-decreasing if for every adjacent pair we have <code>x[i] &lt;= x[i+1]</code>. If <code>x</code> has less than two elements, it is trivially non-decreasing.</p> <p>See also: <code>is_strictly_increasing</code></p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"is_non_decreasing\"</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>Boolean <code>Tensor</code>, equal to <code>True</code> iff <code>x</code> is non-decreasing.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> is not a numeric tensor.</li> </ul>  <h3 id=\"is_numeric_tensor\"><code>tf.is_numeric_tensor(tensor)</code></h3>  <h3 id=\"is_strictly_increasing\"><code>tf.is_strictly_increasing(x, name=None)</code></h3> <p>Returns <code>True</code> if <code>x</code> is strictly increasing.</p> <p>Elements of <code>x</code> are compared in row-major order. The tensor <code>[x[0],...]</code> is strictly increasing if for every adjacent pair we have <code>x[i] &lt; x[i+1]</code>. If <code>x</code> has less than two elements, it is trivially strictly increasing.</p> <p>See also: <code>is_non_decreasing</code></p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"is_strictly_increasing\"</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>Boolean <code>Tensor</code>, equal to <code>True</code> iff <code>x</code> is strictly increasing.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> is not a numeric tensor.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/check_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/check_ops.html</a>\n  </p>\n</div>\n","histogram_ops":"<h1 id=\"histograms\">Histograms</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#histograms\">Histograms</a></li> <ul> <li><a href=\"#histograms-2\">Histograms</a></li> <ul> <li><a href=\"#histogram_fixed_width\"><code>tf.histogram_fixed_width(values, value_range, nbins=100, dtype=tf.int32, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"histograms-2\">Histograms</h2>  <h3 id=\"histogram_fixed_width\"><code>tf.histogram_fixed_width(values, value_range, nbins=100, dtype=tf.int32, name=None)</code></h3> <p>Return histogram of values.</p> <p>Given the tensor <code>values</code>, this operation returns a rank 1 histogram counting the number of entries in <code>values</code> that fell into every bin. The bins are equal width and determined by the arguments <code>value_range</code> and <code>nbins</code>.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>values</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>value_range</code>: Shape [2] <code>Tensor</code>. new_values &lt;= value_range[0] will be mapped to hist[0], values &gt;= value_range[1] will be mapped to hist[-1]. Must be same dtype as new_values.</li> <li>\n<code>nbins</code>: Scalar <code>int32 Tensor</code>. Number of histogram bins.</li> <li>\n<code>dtype</code>: dtype for returned histogram.</li> <li>\n<code>name</code>: A name for this operation (defaults to 'histogram_fixed_width').</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A 1-D <code>Tensor</code> holding histogram of values.</p> <ul> <li>\n<code>Examples</code>: </li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)\nnbins = 5\nvalue_range = [0.0, 5.0]\nnew_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]\n\nwith tf.default_session() as sess:\n  hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)\n  variables.initialize_all_variables().run()\n  sess.run(hist) =&gt; [2, 1, 1, 0, 2]\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/histogram_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/histogram_ops.html</a>\n  </p>\n</div>\n","control_flow_ops":"<h1 id=\"control-flow\">Control Flow</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#control-flow\">Control Flow</a></li> <ul> <li><a href=\"#control-flow-operations\">Control Flow Operations</a></li> <ul> <li><a href=\"#identity\"><code>tf.identity(input, name=None)</code></a></li> <li><a href=\"#tuple\"><code>tf.tuple(tensors, name=None, control_inputs=None)</code></a></li> <li><a href=\"#group\"><code>tf.group(*inputs, **kwargs)</code></a></li> <li><a href=\"#no_op\"><code>tf.no_op(name=None)</code></a></li> <li><a href=\"#count_up_to\"><code>tf.count_up_to(ref, limit, name=None)</code></a></li> <li><a href=\"#cond\"><code>tf.cond(pred, fn1, fn2, name=None)</code></a></li> <li><a href=\"#case\"><code>tf.case(pred_fn_pairs, default, exclusive=False, name=case)</code></a></li> <li><a href=\"#while_loop\"><code>tf.while_loop(cond, body, loop_vars, parallel_iterations=10, back_prop=True, swap_memory=False, name=None)</code></a></li> </ul> <li><a href=\"#logical-operators\">Logical Operators</a></li> <ul> <li><a href=\"#logical_and\"><code>tf.logical_and(x, y, name=None)</code></a></li> <li><a href=\"#logical_not\"><code>tf.logical_not(x, name=None)</code></a></li> <li><a href=\"#logical_or\"><code>tf.logical_or(x, y, name=None)</code></a></li> <li><a href=\"#logical_xor\"><code>tf.logical_xor(x, y, name=LogicalXor)</code></a></li> </ul> <li><a href=\"#comparison-operators\">Comparison Operators</a></li> <ul> <li><a href=\"#equal\"><code>tf.equal(x, y, name=None)</code></a></li> <li><a href=\"#not_equal\"><code>tf.not_equal(x, y, name=None)</code></a></li> <li><a href=\"#less\"><code>tf.less(x, y, name=None)</code></a></li> <li><a href=\"#less_equal\"><code>tf.less_equal(x, y, name=None)</code></a></li> <li><a href=\"#greater\"><code>tf.greater(x, y, name=None)</code></a></li> <li><a href=\"#greater_equal\"><code>tf.greater_equal(x, y, name=None)</code></a></li> <li><a href=\"#select\"><code>tf.select(condition, t, e, name=None)</code></a></li> <li><a href=\"#where\"><code>tf.where(input, name=None)</code></a></li> </ul> <li><a href=\"#debugging-operations\">Debugging Operations</a></li> <ul> <li><a href=\"#is_finite\"><code>tf.is_finite(x, name=None)</code></a></li> <li><a href=\"#is_inf\"><code>tf.is_inf(x, name=None)</code></a></li> <li><a href=\"#is_nan\"><code>tf.is_nan(x, name=None)</code></a></li> <li><a href=\"#verify_tensor_all_finite\"><code>tf.verify_tensor_all_finite(t, msg, name=None)</code></a></li> <li><a href=\"#check_numerics\"><code>tf.check_numerics(tensor, message, name=None)</code></a></li> <li><a href=\"#add_check_numerics_ops\"><code>tf.add_check_numerics_ops()</code></a></li> <li><a href=\"#Assert\"><code>tf.Assert(condition, data, summarize=None, name=None)</code></a></li> <li><a href=\"#Print\"><code>tf.Print(input_, data, message=None, first_n=None, summarize=None, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"control-flow-operations\">Control Flow Operations</h2> <p>TensorFlow provides several operations and classes that you can use to control the execution of operations and add conditional dependencies to your graph.</p>  <h3 id=\"identity\"><code>tf.identity(input, name=None)</code></h3> <p>Return a tensor with the same shape and contents as the input tensor or value.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h3 id=\"tuple\"><code>tf.tuple(tensors, name=None, control_inputs=None)</code></h3> <p>Group tensors together.</p> <p>This creates a tuple of tensors with the same values as the <code>tensors</code> argument, except that the value of each tensor is only returned after the values of all tensors have been computed.</p> <p><code>control_inputs</code> contains additional ops that have to finish before this op finishes, but whose outputs are not returned.</p> <p>This can be used as a \"join\" mechanism for parallel computations: all the argument tensors can be computed in parallel, but the values of any tensor returned by <code>tuple</code> are only available after all the parallel computations are done.</p> <p>See also <code>group</code> and <code>with_dependencies</code>.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>tensors</code>: A list of <code>Tensor</code>s or <code>IndexedSlices</code>, some entries can be <code>None</code>.</li> <li>\n<code>name</code>: (optional) A name to use as a <code>name_scope</code> for the operation.</li> <li>\n<code>control_inputs</code>: List of additional ops to finish before returning.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>Same as <code>tensors</code>.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>tensors</code> does not contain any <code>Tensor</code> or <code>IndexedSlices</code>.</li> <li>\n<code>TypeError</code>: If <code>control_inputs</code> is not a list of <code>Operation</code> or <code>Tensor</code> objects.</li> </ul>  <h3 id=\"group\"><code>tf.group(*inputs, **kwargs)</code></h3> <p>Create an op that groups multiple operations.</p> <p>When this op finishes, all ops in <code>input</code> have finished. This op has no output.</p> <p>See also <code>tuple</code> and <code>with_dependencies</code>.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>*inputs</code>: Zero or more tensors to group.</li> <li>\n<code>**kwargs</code>: Optional parameters to pass when constructing the NodeDef.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>An Operation that executes all its inputs.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If an unknown keyword argument is provided.</li> </ul>  <h3 id=\"no_op\"><code>tf.no_op(name=None)</code></h3> <p>Does nothing. Only useful as a placeholder for control edges.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>The created Operation.</p>  <h3 id=\"count_up_to\"><code>tf.count_up_to(ref, limit, name=None)</code></h3> <p>Increments 'ref' until it reaches 'limit'.</p> <p>This operation outputs \"ref\" after the update is done. This makes it easier to chain operations that need to use the updated value.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>ref</code>: A mutable <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. Should be from a scalar <code>Variable</code> node.</li> <li>\n<code>limit</code>: An <code>int</code>. If incrementing ref would bring it above limit, instead generates an 'OutOfRange' error.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>ref</code>. A copy of the input before increment. If nothing else modifies the input, the values produced will all be distinct.</p>  <h3 id=\"cond\"><code>tf.cond(pred, fn1, fn2, name=None)</code></h3> <p>Return either fn1() or fn2() based on the boolean predicate <code>pred</code>.</p> <p><code>fn1</code> and <code>fn2</code> both return lists of output tensors. <code>fn1</code> and <code>fn2</code> must have the same non-zero number and type of outputs.</p> <p>Note that the conditional execution applies only to the operations defined in fn1 and fn2. Consider the following simple program:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">z = tf.mul(a, b)\nresult = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))\n</pre> <p>If x &lt; y, the tf.add operation will be executed and tf.square operation will not be executed. Since z is needed for at least one branch of the cond, the tf.mul operation is always executed, unconditionally. Although this behavior is consistent with the dataflow model of TensorFlow, it has occasionally surprised some users who expected a lazier semantics.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>pred</code>: A scalar determining whether to return the result of <code>fn1</code> or <code>fn2</code>.</li> <li>\n<code>fn1</code>: The callable to be performed if pred is true.</li> <li>\n<code>fn2</code>: The callable to be performed if pref is false.</li> <li>\n<code>name</code>: Optional name prefix for the returned tensors.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>Tensors returned by the call to either <code>fn1</code> or <code>fn2</code>. If the callables return a singleton list, the element is extracted from the list.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>fn1</code> or <code>fn2</code> is not callable.</li> <li><p><code>ValueError</code>: if <code>fn1</code> and <code>fn2</code> do not return the same number of tensors, or return tensors of different types.</p></li> <li><p><code>Example</code>: </p></li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.constant(2)\ny = tf.constant(5)\ndef f1(): return tf.mul(x, 17)\ndef f2(): return tf.add(y, 23)\nr = cond(tf.less(x, y), f1, f2)\n# r is set to f1().\n# Operations in f2 (e.g., tf.add) are not executed.\n</pre>  <h3 id=\"case\"><code>tf.case(pred_fn_pairs, default, exclusive=False, name='case')</code></h3> <p>Create a case operation.</p> <p>The <code>pred_fn_pairs</code> parameter is a dict or list of pairs of size N. Each pair contains a boolean scalar tensor and a python callable that creates the tensors to be returned if the boolean evaluates to True. <code>default</code> is a callable generating a list of tensors. All the callables in <code>pred_fn_pairs</code> as well as <code>default</code> should return the same number and types of tensors.</p> <p>If <code>exclusive==True</code>, all predicates are evaluated, and a logging operation with an error is returned if more than one of the predicates evaluates to True. If <code>exclusive==False</code>, execution stops are the first predicate which evaluates to True, and the tensors generated by the corresponding function are returned immediately. If none of the predicates evaluate to True, this operation returns the tensors generated by <code>default</code>.</p> <p>Example 1: Pseudocode: <code>if (x &lt; y) return 17;\n    else return 23;</code></p> <p>Expressions: <code>f1 = lambda: tf.constant(17)\n    f2 = lambda: tf.constant(23)\n    r = case([(tf.less(x, y), f1)], default=f2)</code></p> <p>Example 2: Pseudocode: <code>if (x &lt; y &amp;&amp; x &gt; z) raise OpError(\"Only one predicate may evaluate true\");\n    if (x &lt; y) return 17;\n    else if (x &gt; z) return 23;\n    else return -1;</code></p> <p>Expressions: <code>x = tf.constant(0)\n    y = tf.constant(1)\n    z = tf.constant(2)\n    def f1(): return tf.constant(17)\n    def f2(): return tf.constant(23)\n    def f3(): return tf.constant(-1)\n    r = case({tf.less(x, y): f1, tf.greater(x, z): f2},\n             default=f3, exclusive=True)</code></p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>pred_fn_pairs</code>: Dict or list of pairs of a boolean scalar tensor and a callable which returns a list of tensors.</li> <li>\n<code>default</code>: A callable that returns a list of tensors.</li> <li>\n<code>exclusive</code>: True iff more than one predicate is allowed to evaluate to True.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>The tensors returned by the first pair whose predicate evaluated to True, or those returned by <code>default</code> if none does.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>pred_fn_pairs</code> is not a list/dictionary.</li> <li>\n<code>TypeError</code>: If <code>pred_fn_pairs</code> is a list but does not contain 2-tuples.</li> <li>\n<code>TypeError</code>: If <code>fns[i]</code> is not callable for any i, or <code>default</code> is not callable.</li> </ul>  <h3 id=\"while_loop\"><code>tf.while_loop(cond, body, loop_vars, parallel_iterations=10, back_prop=True, swap_memory=False, name=None)</code></h3> <p>Repeat <code>body</code> while the condition <code>cond</code> is true.</p> <p><code>cond</code> is a callable returning a boolean scalar tensor. <code>body</code> is a callable returning a (possibly nested) tuple or list of tensors of the same arity (length and structure) and types as <code>loop_vars</code>. <code>loop_vars</code> is a (possibly nested) tuple or list of tensors that is passed to both <code>cond</code> and <code>body</code>. <code>cond</code> and <code>body</code> both take as many arguments as there are <code>loop_vars</code>.</p> <p>In addition to regular Tensors or IndexedSlices, the body may accept and return TensorArray objects. The flows of the TensorArray objects will be appropriately forwarded between loops and during gradient calculations.</p> <p>While <code>cond</code> evaluates to true, <code>body</code> is executed.</p> <p><code>while_loop</code> implements non-strict semantics, enabling multiple iterations to run in parallel. The maximum number of parallel iterations can be controlled by <code>parallel_iterations</code>, which gives users some control over memory consumption and execution order. For correct programs, <code>while_loop</code> should return the same result for any parallel_iterations &gt; 0.</p> <p>For training, TensorFlow remembers the tensors that are produced in the forward inference but needed in back propagation. These tensors can be a main source of memory consumption and often cause OOM problems when training on GPUs. When the flag swap_memory is true, we swap out these tensors from GPU to CPU. This for example allows us to train RNN models with very long sequences and large batches.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>cond</code>: A callable that represents the termination condition of the loop.</li> <li>\n<code>body</code>: A callable that represents the loop body.</li> <li>\n<code>loop_vars</code>: A (possibly nested) tuple or list of numpy array, <code>Tensor</code>, and <code>TensorArray</code> objects.</li> <li>\n<code>parallel_iterations</code>: The number of iterations allowed to run in parallel.</li> <li>\n<code>back_prop</code>: Whether backprop is enabled for this while loop.</li> <li>\n<code>swap_memory</code>: Whether GPU-CPU memory swap is enabled for this loop.</li> <li>\n<code>name</code>: Optional name prefix for the returned tensors.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>The output tensors for the loop variables after the loop. When the length of <code>loop_vars</code> is 1 this is a Tensor, TensorArry or IndexedSlice and when the length of <code>loop_vars</code> is greater than 1 it returns a list.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>cond</code> or <code>body</code> is not callable.</li> <li><p><code>ValueError</code>: if <code>loop_vars</code> is empty.</p></li> <li>\n<p><code>Example</code>: </p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">i = tf.constant(0)\nc = lambda i: tf.less(i, 10)\nb = lambda i: tf.add(i, 1)\nr = tf.while_loop(c, b, [i])\n</pre>\n</li> </ul> <p>Example with nesting:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">ijk_0 = (tf.constant(0), (tf.constant(1), tf.constant(2)))\nc = lambda i, (j, k): i &lt; 10\nb = lambda i, (j, k): (i + 1, ((j + k), (j - k)))\nijk_final = tf.while_loop(c, b, ijk_0)\n</pre>  <h2 id=\"logical-operators\">Logical Operators</h2> <p>TensorFlow provides several operations that you can use to add logical operators to your graph.</p>  <h3 id=\"logical_and\"><code>tf.logical_and(x, y, name=None)</code></h3> <p>Returns the truth value of x AND y element-wise.</p> <p><em>NOTE</em>: <code>LogicalAnd</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"logical_not\"><code>tf.logical_not(x, name=None)</code></h3> <p>Returns the truth value of NOT x element-wise.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"logical_or\"><code>tf.logical_or(x, y, name=None)</code></h3> <p>Returns the truth value of x OR y element-wise.</p> <p><em>NOTE</em>: <code>LogicalOr</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"logical_xor\"><code>tf.logical_xor(x, y, name='LogicalXor')</code></h3> <p>x ^ y = (x | y) &amp; ~(x &amp; y).</p>  <h2 id=\"comparison-operators\">Comparison Operators</h2> <p>TensorFlow provides several operations that you can use to add comparison operators to your graph.</p>  <h3 id=\"equal\"><code>tf.equal(x, y, name=None)</code></h3> <p>Returns the truth value of (x == y) element-wise.</p> <p><em>NOTE</em>: <code>Equal</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>quint8</code>, <code>qint8</code>, <code>qint32</code>, <code>string</code>, <code>bool</code>, <code>complex128</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"not_equal\"><code>tf.not_equal(x, y, name=None)</code></h3> <p>Returns the truth value of (x != y) element-wise.</p> <p><em>NOTE</em>: <code>NotEqual</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>quint8</code>, <code>qint8</code>, <code>qint32</code>, <code>string</code>, <code>bool</code>, <code>complex128</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"less\"><code>tf.less(x, y, name=None)</code></h3> <p>Returns the truth value of (x &lt; y) element-wise.</p> <p><em>NOTE</em>: <code>Less</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"less_equal\"><code>tf.less_equal(x, y, name=None)</code></h3> <p>Returns the truth value of (x &lt;= y) element-wise.</p> <p><em>NOTE</em>: <code>LessEqual</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"greater\"><code>tf.greater(x, y, name=None)</code></h3> <p>Returns the truth value of (x &gt; y) element-wise.</p> <p><em>NOTE</em>: <code>Greater</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"greater_equal\"><code>tf.greater_equal(x, y, name=None)</code></h3> <p>Returns the truth value of (x &gt;= y) element-wise.</p> <p><em>NOTE</em>: <code>GreaterEqual</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"select\"><code>tf.select(condition, t, e, name=None)</code></h3> <p>Selects elements from <code>t</code> or <code>e</code>, depending on <code>condition</code>.</p> <p>The <code>t</code>, and <code>e</code> tensors must all have the same shape, and the output will also have that shape. The <code>condition</code> tensor must be a scalar if <code>t</code> and <code>e</code> are scalars. If <code>t</code> and <code>e</code> are vectors or higher rank, then <code>condition</code> must be either a vector with size matching the first dimension of <code>t</code>, or must have the same shape as <code>t</code>.</p> <p>The <code>condition</code> tensor acts as a mask that chooses, based on the value at each element, whether the corresponding element / row in the output should be taken from <code>t</code> (if true) or <code>e</code> (if false).</p> <p>If <code>condition</code> is a vector and <code>t</code> and <code>e</code> are higher rank matrices, then it chooses which row (outer dimension) to copy from <code>t</code> and <code>e</code>. If <code>condition</code> has the same shape as <code>t</code> and <code>e</code>, then it chooses which element to copy from <code>t</code> and <code>e</code>.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 'condition' tensor is [[True,  False]\n#                        [False, True]]\n# 't' is [[1, 2],\n#         [3, 4]]\n# 'e' is [[5, 6],\n#         [7, 8]]\nselect(condition, t, e) ==&gt; [[1, 6],\n                             [7, 4]]\n\n\n# 'condition' tensor is [True, False]\n# 't' is [[1, 2],\n#         [3, 4]]\n# 'e' is [[5, 6],\n#         [7, 8]]\nselect(condition, t, e) ==&gt; [[1, 2],\n                             [7, 8]]\n\n</pre>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>condition</code>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>\n<code>t</code>: A <code>Tensor</code> which may have the same shape as <code>condition</code>. If <code>condition</code> is rank 1, <code>t</code> may have higher rank, but its first dimension must match the size of <code>condition</code>.</li> <li>\n<code>e</code>: A <code>Tensor</code> with the same type and shape as <code>t</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>A <code>Tensor</code> with the same type and shape as <code>t</code> and <code>e</code>.</p>  <h3 id=\"where\"><code>tf.where(input, name=None)</code></h3> <p>Returns locations of true values in a boolean tensor.</p> <p>This operation returns the coordinates of true elements in <code>input</code>. The coordinates are returned in a 2-D tensor where the first dimension (rows) represents the number of true elements, and the second dimension (columns) represents the coordinates of the true elements. Keep in mind, the shape of the output tensor can vary depending on how many true values there are in <code>input</code>. Indices are output in row-major order.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 'input' tensor is [[True, False]\n#                    [True, False]]\n# 'input' has two true values, so output has two coordinates.\n# 'input' has rank of 2, so coordinates have two indices.\nwhere(input) ==&gt; [[0, 0],\n                  [1, 0]]\n\n# `input` tensor is [[[True, False]\n#                     [True, False]]\n#                    [[False, True]\n#                     [False, True]]\n#                    [[False, False]\n#                     [False, True]]]\n# 'input' has 5 true values, so output has 5 coordinates.\n# 'input' has rank of 3, so coordinates have three indices.\nwhere(input) ==&gt; [[0, 0, 0],\n                  [0, 1, 0],\n                  [1, 0, 1],\n                  [1, 1, 1],\n                  [2, 1, 1]]\n</pre>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int64</code>.</p>  <h2 id=\"debugging-operations\">Debugging Operations</h2> <p>TensorFlow provides several operations that you can use to validate values and debug your graph.</p>  <h3 id=\"is_finite\"><code>tf.is_finite(x, name=None)</code></h3> <p>Returns which elements of x are finite.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"is_inf\"><code>tf.is_inf(x, name=None)</code></h3> <p>Returns which elements of x are Inf.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"is_nan\"><code>tf.is_nan(x, name=None)</code></h3> <p>Returns which elements of x are NaN.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>.</p>  <h3 id=\"verify_tensor_all_finite\"><code>tf.verify_tensor_all_finite(t, msg, name=None)</code></h3> <p>Assert that the tensor does not contain any NaN's or Inf's.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>t</code>: Tensor to check.</li> <li>\n<code>msg</code>: Message to log on failure.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>Same tensor as <code>t</code>.</p>  <h3 id=\"check_numerics\"><code>tf.check_numerics(tensor, message, name=None)</code></h3> <p>Checks a tensor for NaN and Inf values.</p> <p>When run, reports an <code>InvalidArgument</code> error if <code>tensor</code> has any values that are not a number (NaN) or infinity (Inf). Otherwise, passes <code>tensor</code> as-is.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>tensor</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>message</code>: A <code>string</code>. Prefix of the error message.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>tensor</code>.</p>  <h3 id=\"add_check_numerics_ops\"><code>tf.add_check_numerics_ops()</code></h3> <p>Connect a <code>check_numerics</code> to every floating point tensor.</p> <p><code>check_numerics</code> operations themselves are added for each <code>half</code>, <code>float</code>, or <code>double</code> tensor in the graph. For all ops in the graph, the <code>check_numerics</code> op for all of its (<code>half</code>, <code>float</code>, or <code>double</code>) inputs is guaranteed to run before the <code>check_numerics</code> op on any of its outputs.</p>  <h5 id=\"returns-25\">Returns:</h5> <p>A <code>group</code> op depending on all <code>check_numerics</code> ops added.</p>  <h3 id=\"Assert\"><code>tf.Assert(condition, data, summarize=None, name=None)</code></h3> <p>Asserts that the given condition is true.</p> <p>If <code>condition</code> evaluates to false, print the list of tensors in <code>data</code>. <code>summarize</code> determines how many entries of the tensors to print.</p> <p>NOTE: To ensure that Assert executes, one usually attaches a dependency:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"> # Ensure maximum element of x is smaller or equal to 1\nassert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])\nx = tf.with_dependencies([assert_op], x)\n</pre>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>condition</code>: The condition to evaluate.</li> <li>\n<code>data</code>: The tensors to print out when condition is false.</li> <li>\n<code>summarize</code>: Print this many entries of each tensor.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <ul> <li>\n<code>assert_op</code>: An <code>Operation</code> that, when executed, raises a <code>tf.errors.InvalidArgumentError</code> if <code>condition</code> is not true.</li> </ul>  <h3 id=\"Print\"><code>tf.Print(input_, data, message=None, first_n=None, summarize=None, name=None)</code></h3> <p>Prints a list of tensors.</p> <p>This is an identity op with the side effect of printing <code>data</code> when evaluating.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>input_</code>: A tensor passed through this op.</li> <li>\n<code>data</code>: A list of tensors to print out when op is evaluated.</li> <li>\n<code>message</code>: A string, prefix of the error message.</li> <li>\n<code>first_n</code>: Only log <code>first_n</code> number of times. Negative numbers log always; this is the default.</li> <li>\n<code>summarize</code>: Only print this many entries of each tensor. If None, then a maximum of 3 elements are printed per input tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>Same tensor as <code>input_</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/control_flow_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/control_flow_ops.html</a>\n  </p>\n</div>\n","array_ops":"<h1 id=\"tensor-transformations\">Tensor Transformations</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#tensor-transformations\">Tensor Transformations</a></li> <ul> <li><a href=\"#casting\">Casting</a></li> <ul> <li><a href=\"#string_to_number\"><code>tf.string_to_number(string_tensor, out_type=None, name=None)</code></a></li> <li><a href=\"#to_double\"><code>tf.to_double(x, name=ToDouble)</code></a></li> <li><a href=\"#to_float\"><code>tf.to_float(x, name=ToFloat)</code></a></li> <li><a href=\"#to_bfloat16\"><code>tf.to_bfloat16(x, name=ToBFloat16)</code></a></li> <li><a href=\"#to_int32\"><code>tf.to_int32(x, name=ToInt32)</code></a></li> <li><a href=\"#to_int64\"><code>tf.to_int64(x, name=ToInt64)</code></a></li> <li><a href=\"#cast\"><code>tf.cast(x, dtype, name=None)</code></a></li> <li><a href=\"#saturate_cast\"><code>tf.saturate_cast(value, dtype, name=None)</code></a></li> </ul> <li><a href=\"#shapes-and-shaping\">Shapes and Shaping</a></li> <ul> <li><a href=\"#shape\"><code>tf.shape(input, name=None)</code></a></li> <li><a href=\"#size\"><code>tf.size(input, name=None)</code></a></li> <li><a href=\"#rank\"><code>tf.rank(input, name=None)</code></a></li> <li><a href=\"#reshape\"><code>tf.reshape(tensor, shape, name=None)</code></a></li> <li><a href=\"#squeeze\"><code>tf.squeeze(input, squeeze_dims=None, name=None)</code></a></li> <li><a href=\"#expand_dims\"><code>tf.expand_dims(input, dim, name=None)</code></a></li> <li><a href=\"#meshgrid\"><code>tf.meshgrid(*args, **kwargs)</code></a></li> </ul> <li><a href=\"#slicing-and-joining\">Slicing and Joining</a></li> <ul> <li><a href=\"#slice\"><code>tf.slice(input_, begin, size, name=None)</code></a></li> <li><a href=\"#strided_slice\"><code>tf.strided_slice(input_, begin, end, strides, begin_mask=0, end_mask=0, ellipsis_mask=0, new_axis_mask=0, shrink_axis_mask=0, name=None)</code></a></li> <li><a href=\"#split\"><code>tf.split(split_dim, num_split, value, name=split)</code></a></li> <li><a href=\"#tile\"><code>tf.tile(input, multiples, name=None)</code></a></li> <li><a href=\"#pad\"><code>tf.pad(tensor, paddings, mode=CONSTANT, name=None)</code></a></li> <li><a href=\"#concat\"><code>tf.concat(concat_dim, values, name=concat)</code></a></li> <li><a href=\"#pack\"><code>tf.pack(values, axis=0, name=pack)</code></a></li> <li><a href=\"#unpack\"><code>tf.unpack(value, num=None, axis=0, name=unpack)</code></a></li> <li><a href=\"#reverse_sequence\"><code>tf.reverse_sequence(input, seq_lengths, seq_dim, batch_dim=None, name=None)</code></a></li> <li><a href=\"#reverse\"><code>tf.reverse(tensor, dims, name=None)</code></a></li> <li><a href=\"#transpose\"><code>tf.transpose(a, perm=None, name=transpose)</code></a></li> <li><a href=\"#extract_image_patches\"><code>tf.extract_image_patches(images, ksizes, strides, rates, padding, name=None)</code></a></li> <li><a href=\"#space_to_batch\"><code>tf.space_to_batch(input, paddings, block_size, name=None)</code></a></li> <li><a href=\"#batch_to_space\"><code>tf.batch_to_space(input, crops, block_size, name=None)</code></a></li> <li><a href=\"#space_to_depth\"><code>tf.space_to_depth(input, block_size, name=None)</code></a></li> <li><a href=\"#depth_to_space\"><code>tf.depth_to_space(input, block_size, name=None)</code></a></li> <li><a href=\"#gather\"><code>tf.gather(params, indices, validate_indices=None, name=None)</code></a></li> <li><a href=\"#gather_nd\"><code>tf.gather_nd(params, indices, name=None)</code></a></li> <li><a href=\"#dynamic_partition\"><code>tf.dynamic_partition(data, partitions, num_partitions, name=None)</code></a></li> <li><a href=\"#dynamic_stitch\"><code>tf.dynamic_stitch(indices, data, name=None)</code></a></li> <li><a href=\"#boolean_mask\"><code>tf.boolean_mask(tensor, mask, name=boolean_mask)</code></a></li> <li><a href=\"#one_hot\"><code>tf.one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)</code></a></li> </ul>\n</ul> <li><a href=\"#examples\">Examples</a></li> <ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#bitcast\"><code>tf.bitcast(input, type, name=None)</code></a></li> <li><a href=\"#copy\"><code>tf.contrib.graph_editor.copy(sgv, dst_graph=None, dst_scope=, src_scope=)</code></a></li> <li><a href=\"#shape_n\"><code>tf.shape_n(input, name=None)</code></a></li> <li><a href=\"#unique_with_counts\"><code>tf.unique_with_counts(x, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div> <h2 id=\"casting\">Casting</h2> <p>TensorFlow provides several operations that you can use to cast tensor data types in your graph.</p>  <h3 id=\"string_to_number\"><code>tf.string_to_number(string_tensor, out_type=None, name=None)</code></h3> <p>Converts each string in the input Tensor to the specified numeric type.</p> <p>(Note that int32 overflow results in an error while float overflow results in a rounded value.)</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>string_tensor</code>: A <code>Tensor</code> of type <code>string</code>.</li> <li>\n<code>out_type</code>: An optional <code>tf.DType</code> from: <code>tf.float32, tf.int32</code>. Defaults to <code>tf.float32</code>. The numeric type to interpret each string in string_tensor as.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code> of type <code>out_type</code>. A Tensor of the same shape as the input <code>string_tensor</code>.</p>  <h3 id=\"to_double\"><code>tf.to_double(x, name='ToDouble')</code></h3> <p>Casts a tensor to type <code>float64</code>.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code> with same shape as <code>x</code> with type <code>float64</code>.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>x</code> cannot be cast to the <code>float64</code>.</li> </ul>  <h3 id=\"to_float\"><code>tf.to_float(x, name='ToFloat')</code></h3> <p>Casts a tensor to type <code>float32</code>.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code> with same shape as <code>x</code> with type <code>float32</code>.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>x</code> cannot be cast to the <code>float32</code>.</li> </ul>  <h3 id=\"to_bfloat16\"><code>tf.to_bfloat16(x, name='ToBFloat16')</code></h3> <p>Casts a tensor to type <code>bfloat16</code>.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code> with same shape as <code>x</code> with type <code>bfloat16</code>.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>x</code> cannot be cast to the <code>bfloat16</code>.</li> </ul>  <h3 id=\"to_int32\"><code>tf.to_int32(x, name='ToInt32')</code></h3> <p>Casts a tensor to type <code>int32</code>.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code> with same shape as <code>x</code> with type <code>int32</code>.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>x</code> cannot be cast to the <code>int32</code>.</li> </ul>  <h3 id=\"to_int64\"><code>tf.to_int64(x, name='ToInt64')</code></h3> <p>Casts a tensor to type <code>int64</code>.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code> with same shape as <code>x</code> with type <code>int64</code>.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>x</code> cannot be cast to the <code>int64</code>.</li> </ul>  <h3 id=\"cast\"><code>tf.cast(x, dtype, name=None)</code></h3> <p>Casts a tensor to a new type.</p> <p>The operation casts <code>x</code> (in case of <code>Tensor</code>) or <code>x.values</code> (in case of <code>SparseTensor</code>) to <code>dtype</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># tensor `a` is [1.8, 2.2], dtype=tf.float\ntf.cast(a, tf.int32) ==&gt; [1, 2]  # dtype=tf.int32\n</pre>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>dtype</code>: The destination type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code> with same shape as <code>x</code>.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>x</code> cannot be cast to the <code>dtype</code>.</li> </ul>  <h3 id=\"saturate_cast\"><code>tf.saturate_cast(value, dtype, name=None)</code></h3> <p>Performs a safe saturating cast of <code>value</code> to <code>dtype</code>.</p> <p>This function casts the input to <code>dtype</code> without applying any scaling. If there is a danger that values would over or underflow in the cast, this op applies the appropriate clamping before the cast.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>value</code>: A <code>Tensor</code>.</li> <li>\n<code>dtype</code>: The desired output <code>DType</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p><code>value</code> safely cast to <code>dtype</code>.</p>  <h2 id=\"shapes-and-shaping\">Shapes and Shaping</h2> <p>TensorFlow provides several operations that you can use to determine the shape of a tensor and change the shape of a tensor.</p>  <h3 id=\"shape\"><code>tf.shape(input, name=None)</code></h3> <p>Returns the shape of a tensor.</p> <p>This operation returns a 1-D integer tensor representing the shape of <code>input</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\nshape(t) ==&gt; [2, 2, 3]\n</pre>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int32</code>.</p>  <h3 id=\"size\"><code>tf.size(input, name=None)</code></h3> <p>Returns the size of a tensor.</p> <p>This operation returns an integer representing the number of elements in <code>input</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]\nsize(t) ==&gt; 12\n</pre>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int32</code>.</p>  <h3 id=\"rank\"><code>tf.rank(input, name=None)</code></h3> <p>Returns the rank of a tensor.</p> <p>This operation returns an integer representing the rank of <code>input</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\n# shape of tensor 't' is [2, 2, 3]\nrank(t) ==&gt; 3\n</pre> <p><strong>Note</strong>: The rank of a tensor is not the same as the rank of a matrix. The rank of a tensor is the number of indices required to uniquely select each element of the tensor. Rank is also known as \"order\", \"degree\", or \"ndims.\"</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int32</code>.</p>  <h3 id=\"reshape\"><code>tf.reshape(tensor, shape, name=None)</code></h3> <p>Reshapes a tensor.</p> <p>Given <code>tensor</code>, this operation returns a tensor that has the same values as <code>tensor</code> with shape <code>shape</code>.</p> <p>If one component of <code>shape</code> is the special value -1, the size of that dimension is computed so that the total size remains constant. In particular, a <code>shape</code> of <code>[-1]</code> flattens into 1-D. At most one component of <code>shape</code> can be -1.</p> <p>If <code>shape</code> is 1-D or higher, then the operation returns a tensor with shape <code>shape</code> filled with the values of <code>tensor</code>. In this case, the number of elements implied by <code>shape</code> must be the same as the number of elements in <code>tensor</code>.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n# tensor 't' has shape [9]\nreshape(t, [3, 3]) ==&gt; [[1, 2, 3],\n                        [4, 5, 6],\n                        [7, 8, 9]]\n\n# tensor 't' is [[[1, 1], [2, 2]],\n#                [[3, 3], [4, 4]]]\n# tensor 't' has shape [2, 2, 2]\nreshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],\n                        [3, 3, 4, 4]]\n\n# tensor 't' is [[[1, 1, 1],\n#                 [2, 2, 2]],\n#                [[3, 3, 3],\n#                 [4, 4, 4]],\n#                [[5, 5, 5],\n#                 [6, 6, 6]]]\n# tensor 't' has shape [3, 2, 3]\n# pass '[-1]' to flatten 't'\nreshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n\n# -1 can also be used to infer the shape\n\n# -1 is inferred to be 9:\nreshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n# -1 is inferred to be 2:\nreshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n# -1 is inferred to be 3:\nreshape(t, [ 2, -1, 3]) ==&gt; [[[1, 1, 1],\n                              [2, 2, 2],\n                              [3, 3, 3]],\n                             [[4, 4, 4],\n                              [5, 5, 5],\n                              [6, 6, 6]]]\n\n# tensor 't' is [7]\n# shape `[]` reshapes to a scalar\nreshape(t, []) ==&gt; 7\n</pre>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>tensor</code>: A <code>Tensor</code>.</li> <li>\n<code>shape</code>: A <code>Tensor</code> of type <code>int32</code>. Defines the shape of the output tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>tensor</code>.</p>  <h3 id=\"squeeze\"><code>tf.squeeze(input, squeeze_dims=None, name=None)</code></h3> <p>Removes dimensions of size 1 from the shape of a tensor.</p> <p>Given a tensor <code>input</code>, this operation returns a tensor of the same type with all dimensions of size 1 removed. If you don't want to remove all size 1 dimensions, you can remove specific size 1 dimensions by specifying <code>squeeze_dims</code>.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t)) ==&gt; [2, 3]\n</pre> <p>Or, to remove specific size 1 dimensions:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t, [2, 4])) ==&gt; [1, 2, 3, 1]\n</pre>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. The <code>input</code> to squeeze.</li> <li>\n<code>squeeze_dims</code>: An optional list of <code>ints</code>. Defaults to <code>[]</code>. If specified, only squeezes the dimensions listed. The dimension index starts at 0. It is an error to squeeze a dimension that is not 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Contains the same data as <code>input</code>, but has one or more dimensions of size 1 removed.</p>  <h3 id=\"expand_dims\"><code>tf.expand_dims(input, dim, name=None)</code></h3> <p>Inserts a dimension of 1 into a tensor's shape.</p> <p>Given a tensor <code>input</code>, this operation inserts a dimension of 1 at the dimension index <code>dim</code> of <code>input</code>'s shape. The dimension index <code>dim</code> starts at zero; if you specify a negative number for <code>dim</code> it is counted backward from the end.</p> <p>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape <code>[height, width,\nchannels]</code>, you can make it a batch of 1 image with <code>expand_dims(image, 0)</code>, which will make the shape <code>[1, height, width, channels]</code>.</p> <p>Other examples:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 't' is a tensor of shape [2]\nshape(expand_dims(t, 0)) ==&gt; [1, 2]\nshape(expand_dims(t, 1)) ==&gt; [2, 1]\nshape(expand_dims(t, -1)) ==&gt; [2, 1]\n\n# 't2' is a tensor of shape [2, 3, 5]\nshape(expand_dims(t2, 0)) ==&gt; [1, 2, 3, 5]\nshape(expand_dims(t2, 2)) ==&gt; [2, 3, 1, 5]\nshape(expand_dims(t2, 3)) ==&gt; [2, 3, 5, 1]\n</pre> <p>This operation requires that:</p> <p><code>-1-input.dims() &lt;= dim &lt;= input.dims()</code></p> <p>This operation is related to <code>squeeze()</code>, which removes dimensions of size 1.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>.</li> <li>\n<code>dim</code>: A <code>Tensor</code> of type <code>int32</code>. 0-D (scalar). Specifies the dimension index at which to expand the shape of <code>input</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Contains the same data as <code>input</code>, but its shape has an additional dimension of size 1 added.</p>  <h3 id=\"meshgrid\"><code>tf.meshgrid(*args, **kwargs)</code></h3> <p>Broadcasts parameters for evaluation on an N-D grid.</p> <p>Given N one-dimensional coordinate arrays <code>*args</code>, returns a list <code>outputs</code> of N-D coordinate arrays for evaluating expressions on an N-D grid.</p> <p>Notes:</p> <p><code>meshgrid</code> supports cartesian ('xy') and matrix ('ij') indexing conventions. When the <code>indexing</code> argument is set to 'xy' (the default), the broadcasting instructions for the first two dimensions are swapped.</p> <p>Examples:</p> <p>Calling <code>X, Y = meshgrid(x, y)</code> with the tensors <code>prettyprint\n  x = [1, 2, 3]\n  y = [4, 5, 6]</code> results in <code>prettyprint\n  X = [[1, 1, 1],\n       [2, 2, 2],\n       [3, 3, 3]]\n  Y = [[4, 5, 6],\n       [4, 5, 6],\n       [4, 5, 6]]</code></p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>*args</code>: <code>Tensor</code>s with rank 1</li> <li>\n<code>indexing</code>: Either 'xy' or 'ij' (optional, default: 'xy')</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <ul> <li>\n<code>outputs</code>: A list of N <code>Tensor</code>s with rank N</li> </ul>  <h2 id=\"slicing-and-joining\">Slicing and Joining</h2> <p>TensorFlow provides several operations to slice or extract parts of a tensor, or join multiple tensors together.</p>  <h3 id=\"slice\"><code>tf.slice(input_, begin, size, name=None)</code></h3> <p>Extracts a slice from a tensor.</p> <p>This operation extracts a slice of size <code>size</code> from a tensor <code>input</code> starting at the location specified by <code>begin</code>. The slice <code>size</code> is represented as a tensor shape, where <code>size[i]</code> is the number of elements of the 'i'th dimension of <code>input</code> that you want to slice. The starting location (<code>begin</code>) for the slice is represented as an offset in each dimension of <code>input</code>. In other words, <code>begin[i]</code> is the offset into the 'i'th dimension of <code>input</code> that you want to slice from.</p> <p><code>begin</code> is zero-based; <code>size</code> is one-based. If <code>size[i]</code> is -1, all remaining elements in dimension i are included in the slice. In other words, this is equivalent to setting:</p> <p><code>size[i] = input.dim_size(i) - begin[i]</code></p> <p>This operation requires that:</p> <p><code>0 &lt;= begin[i] &lt;= begin[i] + size[i] &lt;= Di  for i in [0, n]</code></p> <p>For example:</p> <pre class=\"\"># 'input' is [[[1, 1, 1], [2, 2, 2]],\n#             [[3, 3, 3], [4, 4, 4]],\n#             [[5, 5, 5], [6, 6, 6]]]\ntf.slice(input, [1, 0, 0], [1, 1, 3]) ==&gt; [[[3, 3, 3]]]\ntf.slice(input, [1, 0, 0], [1, 2, 3]) ==&gt; [[[3, 3, 3],\n                                            [4, 4, 4]]]\ntf.slice(input, [1, 0, 0], [2, 1, 3]) ==&gt; [[[3, 3, 3]],\n                                           [[5, 5, 5]]]\n</pre>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>input_</code>: A <code>Tensor</code>.</li> <li>\n<code>begin</code>: An <code>int32</code> or <code>int64</code> <code>Tensor</code>.</li> <li>\n<code>size</code>: An <code>int32</code> or <code>int64</code> <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A <code>Tensor</code> the same type as <code>input</code>.</p>  <h3 id=\"strided_slice\"><code>tf.strided_slice(input_, begin, end, strides, begin_mask=0, end_mask=0, ellipsis_mask=0, new_axis_mask=0, shrink_axis_mask=0, name=None)</code></h3> <p>Extracts a strided slice from a tensor.</p> <p>To a first order, this operation extracts a slice of size <code>end - begin</code> from a tensor <code>input</code> starting at the location specified by <code>begin</code>. The slice continues by adding <code>stride</code> to the <code>begin</code> index until all dimensions are not less than <code>end</code>. Note that components of stride can be negative, which causes a reverse slice.</p> <p>This operation can be thought of an encoding of a numpy style sliced range. Given a python slice input[<spec0>, <spec1>, ..., <specn>] this function will be called as follows.</specn></spec1></spec0></p> <p><code>begin</code>, <code>end</code>, and <code>strides</code> will be all length n. n is in general not the same dimensionality as <code>input</code>.</p> <p>For the ith spec, <code>begin_mask</code>, <code>end_mask</code>, <code>ellipsis_mask</code>, <code>new_axis_mask</code>, and <code>shrink_axis_mask</code> will have the ith bit corrsponding to the ith spec.</p> <p>If the ith bit of <code>begin_mask</code> is non-zero, <code>begin[i]</code> is ignored and the fullest possible range in that dimension is used instead. <code>end_mask</code> works analogously, except with the end range.</p> <p><code>foo[5:,:,:3]</code> on a 7x8x9 tensor is equivalent to <code>foo[5:7,0:8,0:3]</code>. <code>foo[::-1]</code> reverses a tensor with shape 8.</p> <p>If the ith bit of <code>ellipsis_mask</code>, as many unspecified dimensions as needed will be inserted between other dimensions. Only one non-zero bit is allowed in <code>ellipsis_mask</code>.</p> <p>For example <code>foo[3:5,...,4:5]</code> on a shape 10x3x3x10 tensor is equivalent to <code>foo[3:5,:,:,4:5]</code> and <code>foo[3:5,...]</code> is equivalent to <code>foo[3:5,:,:,:]</code>.</p> <p>If the ith bit of <code>new_axis_mask</code> is one, then a <code>begin</code>, <code>end</code>, and <code>stride</code> are ignored and a new length 1 dimension is added at this point in the output tensor.</p> <p>For example <code>foo[3:5,4]</code> on a 10x8 tensor produces a shape 2 tensor whereas <code>foo[3:5,4:5]</code> produces a shape 2x1 tensor with shrink_mask being 1&lt;&lt;1 == 2.</p> <p>If the ith bit of <code>shrink_axis_mask</code> is one, then <code>begin</code>, <code>end[i]</code>, and <code>stride[i]</code> are used to do a slice in the appropriate dimension, but the output tensor will be reduced in dimensionality by one. This is only valid if the ith entry of slice[i]==1.</p> <p>NOTE: <code>begin</code> and <code>end</code> are zero-indexed<code>.</code>strides` entries must be non-zero.</p> <pre class=\"\"># 'input' is [[[1, 1, 1], [2, 2, 2]],\n#             [[3, 3, 3], [4, 4, 4]],\n#             [[5, 5, 5], [6, 6, 6]]]\ntf.slice(input, [1, 0, 0], [2, 1, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3]]]\ntf.slice(input, [1, 0, 0], [2, 2, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3],\n                                                       [4, 4, 4]]]\ntf.slice(input, [1, 1, 0], [2, -1, 3], [1, -1, 1]) ==&gt;[[[4, 4, 4],\n                                                        [3, 3, 3]]]\n</pre>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>input_</code>: A <code>Tensor</code>.</li> <li>\n<code>begin</code>: An <code>int32</code> or <code>int64</code> <code>Tensor</code>.</li> <li>\n<code>end</code>: An <code>int32</code> or <code>int64</code> <code>Tensor</code>.</li> <li>\n<code>strides</code>: An <code>int32</code> or <code>int64</code> <code>Tensor</code>.</li> <li>\n<code>begin_mask</code>: An <code>int32</code> mask.</li> <li>\n<code>end_mask</code>: An <code>int32</code> mask.</li> <li>\n<code>ellipsis_mask</code>: An <code>int32</code> mask.</li> <li>\n<code>new_axis_mask</code>: An <code>int32</code> mask.</li> <li>\n<code>shrink_axis_mask</code>: An <code>int32</code> mask.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A <code>Tensor</code> the same type as <code>input</code>.</p>  <h3 id=\"split\"><code>tf.split(split_dim, num_split, value, name='split')</code></h3> <p>Splits a tensor into <code>num_split</code> tensors along one dimension.</p> <p>Splits <code>value</code> along dimension <code>split_dim</code> into <code>num_split</code> smaller tensors. Requires that <code>num_split</code> evenly divide <code>value.shape[split_dim]</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'value' is a tensor with shape [5, 30]\n# Split 'value' into 3 tensors along dimension 1\nsplit0, split1, split2 = tf.split(1, 3, value)\ntf.shape(split0) ==&gt; [5, 10]\n</pre> <p>Note: If you are splitting along an axis by the length of that axis, consider using unpack, e.g.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">num_items = t.get_shape()[axis].value\n[tf.squeeze(s, [axis]) for s in tf.split(axis, num_items, t)]\n</pre> <p>can be rewritten as</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">tf.unpack(t, axis=axis)\n</pre>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>split_dim</code>: A 0-D <code>int32</code> <code>Tensor</code>. The dimension along which to split. Must be in the range <code>[0, rank(value))</code>.</li> <li>\n<code>num_split</code>: A Python integer. The number of ways to split.</li> <li>\n<code>value</code>: The <code>Tensor</code> to split.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p><code>num_split</code> <code>Tensor</code> objects resulting from splitting <code>value</code>.</p>  <h3 id=\"tile\"><code>tf.tile(input, multiples, name=None)</code></h3> <p>Constructs a tensor by tiling a given tensor.</p> <p>This operation creates a new tensor by replicating <code>input</code> <code>multiples</code> times. The output tensor's i'th dimension has <code>input.dims(i) * multiples[i]</code> elements, and the values of <code>input</code> are replicated <code>multiples[i]</code> times along the 'i'th dimension. For example, tiling <code>[a b c d]</code> by <code>[2]</code> produces <code>[a b c d a b c d]</code>.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. 1-D or higher.</li> <li>\n<code>multiples</code>: A <code>Tensor</code> of type <code>int32</code>. 1-D. Length must be the same as the number of dimensions in <code>input</code>\n</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h3 id=\"pad\"><code>tf.pad(tensor, paddings, mode='CONSTANT', name=None)</code></h3> <p>Pads a tensor.</p> <p>This operation pads a <code>tensor</code> according to the <code>paddings</code> you specify. <code>paddings</code> is an integer tensor with shape <code>[n, 2]</code>, where n is the rank of <code>tensor</code>. For each dimension D of <code>input</code>, <code>paddings[D, 0]</code> indicates how many values to add before the contents of <code>tensor</code> in that dimension, and <code>paddings[D, 1]</code> indicates how many values to add after the contents of <code>tensor</code> in that dimension. If <code>mode</code> is \"REFLECT\" then both <code>paddings[D, 0]</code> and <code>paddings[D, 1]</code> must be no greater than <code>tensor.dim_size(D) - 1</code>. If <code>mode</code> is \"SYMMETRIC\" then both <code>paddings[D, 0]</code> and <code>paddings[D, 1]</code> must be no greater than <code>tensor.dim_size(D)</code>.</p> <p>The padded size of each dimension D of the output is:</p> <p><code>paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]</code></p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 't' is [[1, 2, 3], [4, 5, 6]].\n# 'paddings' is [[1, 1,], [2, 2]].\n# rank of 't' is 2.\npad(t, paddings, \"CONSTANT\") ==&gt; [[0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 1, 2, 3, 0, 0],\n                                  [0, 0, 4, 5, 6, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0]]\n\npad(t, paddings, \"REFLECT\") ==&gt; [[6, 5, 4, 5, 6, 5, 4],\n                                 [3, 2, 1, 2, 3, 2, 1],\n                                 [6, 5, 4, 5, 6, 5, 4],\n                                 [3, 2, 1, 2, 3, 2, 1]]\n\npad(t, paddings, \"SYMMETRIC\") ==&gt; [[2, 1, 1, 2, 3, 3, 2],\n                                   [2, 1, 1, 2, 3, 3, 2],\n                                   [5, 4, 4, 5, 6, 6, 5],\n                                   [5, 4, 4, 5, 6, 6, 5]]\n</pre>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>tensor</code>: A <code>Tensor</code>.</li> <li>\n<code>paddings</code>: A <code>Tensor</code> of type <code>int32</code>.</li> <li>\n<code>mode</code>: One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\".</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>tensor</code>.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: When mode is not one of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\".</li> </ul>  <h3 id=\"concat\"><code>tf.concat(concat_dim, values, name='concat')</code></h3> <p>Concatenates tensors along one dimension.</p> <p>Concatenates the list of tensors <code>values</code> along dimension <code>concat_dim</code>. If <code>values[i].shape = [D0, D1, ... Dconcat_dim(i), ...Dn]</code>, the concatenated result has shape</p> <pre class=\"\">[D0, D1, ... Rconcat_dim, ...Dn]\n</pre> <p>where</p> <pre class=\"\">Rconcat_dim = sum(Dconcat_dim(i))\n</pre> <p>That is, the data from the input tensors is joined along the <code>concat_dim</code> dimension.</p> <p>The number of dimensions of the input tensors must match, and all dimensions except <code>concat_dim</code> must be equal.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">t1 = [[1, 2, 3], [4, 5, 6]]\nt2 = [[7, 8, 9], [10, 11, 12]]\ntf.concat(0, [t1, t2]) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\ntf.concat(1, [t1, t2]) ==&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]\n\n# tensor t3 with shape [2, 3]\n# tensor t4 with shape [2, 3]\ntf.shape(tf.concat(0, [t3, t4])) ==&gt; [4, 3]\ntf.shape(tf.concat(1, [t3, t4])) ==&gt; [2, 6]\n</pre> <p>Note: If you are concatenating along a new axis consider using pack. E.g.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">tf.concat(axis, [tf.expand_dims(t, axis) for t in tensors])\n</pre> <p>can be rewritten as</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">tf.pack(tensors, axis=axis)\n</pre>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>concat_dim</code>: 0-D <code>int32</code> <code>Tensor</code>. Dimension along which to concatenate.</li> <li>\n<code>values</code>: A list of <code>Tensor</code> objects or a single <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>A <code>Tensor</code> resulting from concatenation of the input tensors.</p>  <h3 id=\"pack\"><code>tf.pack(values, axis=0, name='pack')</code></h3> <p>Packs a list of rank-<code>R</code> tensors into one rank-<code>(R+1)</code> tensor.</p> <p>Packs the list of tensors in <code>values</code> into a tensor with rank one higher than each tensor in <code>values</code>, by packing them along the <code>axis</code> dimension. Given a list of length <code>N</code> of tensors of shape <code>(A, B, C)</code>;</p> <p>if <code>axis == 0</code> then the <code>output</code> tensor will have the shape <code>(N, A, B, C)</code>. if <code>axis == 1</code> then the <code>output</code> tensor will have the shape <code>(A, N, B, C)</code>. Etc.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 'x' is [1, 4]\n# 'y' is [2, 5]\n# 'z' is [3, 6]\npack([x, y, z]) =&gt; [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.\npack([x, y, z], axis=1) =&gt; [[1, 2, 3], [4, 5, 6]]\n</pre> <p>This is the opposite of unpack. The numpy equivalent is</p> <pre class=\"\">tf.pack([x, y, z]) = np.asarray([x, y, z])\n</pre>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>values</code>: A list of <code>Tensor</code> objects with the same shape and type.</li> <li>\n<code>axis</code>: An <code>int</code>. The axis to pack along. Defaults to the first dimension. Supports negative indexes.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <ul> <li>\n<code>output</code>: A packed <code>Tensor</code> with the same type as <code>values</code>.</li> </ul>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>axis</code> is out of the range [-(R+1), R+1).</li> </ul>  <h3 id=\"unpack\"><code>tf.unpack(value, num=None, axis=0, name='unpack')</code></h3> <p>Unpacks the given dimension of a rank-<code>R</code> tensor into rank-<code>(R-1)</code> tensors.</p> <p>Unpacks <code>num</code> tensors from <code>value</code> by chipping it along the <code>axis</code> dimension. If <code>num</code> is not specified (the default), it is inferred from <code>value</code>'s shape. If <code>value.shape[axis]</code> is not known, <code>ValueError</code> is raised.</p> <p>For example, given a tensor of shape <code>(A, B, C, D)</code>;</p> <p>If <code>axis == 0</code> then the i'th tensor in <code>output</code> is the slice <code>value[i, :, :, :]</code> and each tensor in <code>output</code> will have shape <code>(B, C, D)</code>. (Note that the dimension unpacked along is gone, unlike <code>split</code>).</p> <p>If <code>axis == 1</code> then the i'th tensor in <code>output</code> is the slice <code>value[:, i, :, :]</code> and each tensor in <code>output</code> will have shape <code>(A, C, D)</code>. Etc.</p> <p>This is the opposite of pack. The numpy equivalent is</p> <pre class=\"\">tf.unpack(x, n) = list(x)\n</pre>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>value</code>: A rank <code>R &gt; 0</code> <code>Tensor</code> to be unpacked.</li> <li>\n<code>num</code>: An <code>int</code>. The length of the dimension <code>axis</code>. Automatically inferred if <code>None</code> (the default).</li> <li>\n<code>axis</code>: An <code>int</code>. The axis to unpack along. Defaults to the first dimension. Supports negative indexes.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>The list of <code>Tensor</code> objects unpacked from <code>value</code>.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>num</code> is unspecified and cannot be inferred.</li> <li>\n<code>ValueError</code>: If <code>axis</code> is out of the range [-R, R).</li> </ul>  <h3 id=\"reverse_sequence\"><code>tf.reverse_sequence(input, seq_lengths, seq_dim, batch_dim=None, name=None)</code></h3> <p>Reverses variable length slices.</p> <p>This op first slices <code>input</code> along the dimension <code>batch_dim</code>, and for each slice <code>i</code>, reverses the first <code>seq_lengths[i]</code> elements along the dimension <code>seq_dim</code>.</p> <p>The elements of <code>seq_lengths</code> must obey <code>seq_lengths[i] &lt; input.dims[seq_dim]</code>, and <code>seq_lengths</code> must be a vector of length <code>input.dims[batch_dim]</code>.</p> <p>The output slice <code>i</code> along dimension <code>batch_dim</code> is then given by input slice <code>i</code>, with the first <code>seq_lengths[i]</code> slices along dimension <code>seq_dim</code> reversed.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># Given this:\nbatch_dim = 0\nseq_dim = 1\ninput.dims = (4, 8, ...)\nseq_lengths = [7, 2, 3, 5]\n\n# then slices of input are reversed on seq_dim, but only up to seq_lengths:\noutput[0, 0:7, :, ...] = input[0, 7:0:-1, :, ...]\noutput[1, 0:2, :, ...] = input[1, 2:0:-1, :, ...]\noutput[2, 0:3, :, ...] = input[2, 3:0:-1, :, ...]\noutput[3, 0:5, :, ...] = input[3, 5:0:-1, :, ...]\n\n# while entries past seq_lens are copied through:\noutput[0, 7:, :, ...] = input[0, 7:, :, ...]\noutput[1, 2:, :, ...] = input[1, 2:, :, ...]\noutput[2, 3:, :, ...] = input[2, 3:, :, ...]\noutput[3, 2:, :, ...] = input[3, 2:, :, ...]\n</pre> <p>In contrast, if:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># Given this:\nbatch_dim = 2\nseq_dim = 0\ninput.dims = (8, ?, 4, ...)\nseq_lengths = [7, 2, 3, 5]\n\n# then slices of input are reversed on seq_dim, but only up to seq_lengths:\noutput[0:7, :, 0, :, ...] = input[7:0:-1, :, 0, :, ...]\noutput[0:2, :, 1, :, ...] = input[2:0:-1, :, 1, :, ...]\noutput[0:3, :, 2, :, ...] = input[3:0:-1, :, 2, :, ...]\noutput[0:5, :, 3, :, ...] = input[5:0:-1, :, 3, :, ...]\n\n# while entries past seq_lens are copied through:\noutput[7:, :, 0, :, ...] = input[7:, :, 0, :, ...]\noutput[2:, :, 1, :, ...] = input[2:, :, 1, :, ...]\noutput[3:, :, 2, :, ...] = input[3:, :, 2, :, ...]\noutput[2:, :, 3, :, ...] = input[2:, :, 3, :, ...]\n</pre>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. The input to reverse.</li> <li>\n<code>seq_lengths</code>: A <code>Tensor</code> of type <code>int64</code>. 1-D with length <code>input.dims(batch_dim)</code> and <code>max(seq_lengths) &lt; input.dims(seq_dim)</code>\n</li> <li>\n<code>seq_dim</code>: An <code>int</code>. The dimension which is partially reversed.</li> <li>\n<code>batch_dim</code>: An optional <code>int</code>. Defaults to <code>0</code>. The dimension along which reversal is performed.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. The partially reversed input. It has the same shape as <code>input</code>.</p>  <h3 id=\"reverse\"><code>tf.reverse(tensor, dims, name=None)</code></h3> <p>Reverses specific dimensions of a tensor.</p> <p>Given a <code>tensor</code>, and a <code>bool</code> tensor <code>dims</code> representing the dimensions of <code>tensor</code>, this operation reverses each dimension i of <code>tensor</code> where <code>dims[i]</code> is <code>True</code>.</p> <p><code>tensor</code> can have up to 8 dimensions. The number of dimensions of <code>tensor</code> must equal the number of elements in <code>dims</code>. In other words:</p> <p><code>rank(tensor) = size(dims)</code></p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># tensor 't' is [[[[ 0,  1,  2,  3],\n#                  [ 4,  5,  6,  7],\n#                  [ 8,  9, 10, 11]],\n#                 [[12, 13, 14, 15],\n#                  [16, 17, 18, 19],\n#                  [20, 21, 22, 23]]]]\n# tensor 't' shape is [1, 2, 3, 4]\n\n# 'dims' is [False, False, False, True]\nreverse(t, dims) ==&gt; [[[[ 3,  2,  1,  0],\n                        [ 7,  6,  5,  4],\n                        [ 11, 10, 9, 8]],\n                       [[15, 14, 13, 12],\n                        [19, 18, 17, 16],\n                        [23, 22, 21, 20]]]]\n\n# 'dims' is [False, True, False, False]\nreverse(t, dims) ==&gt; [[[[12, 13, 14, 15],\n                        [16, 17, 18, 19],\n                        [20, 21, 22, 23]\n                       [[ 0,  1,  2,  3],\n                        [ 4,  5,  6,  7],\n                        [ 8,  9, 10, 11]]]]\n\n# 'dims' is [False, False, True, False]\nreverse(t, dims) ==&gt; [[[[8, 9, 10, 11],\n                        [4, 5, 6, 7],\n                        [0, 1, 2, 3]]\n                       [[20, 21, 22, 23],\n                        [16, 17, 18, 19],\n                        [12, 13, 14, 15]]]]\n</pre>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>tensor</code>: A <code>Tensor</code>. Must be one of the following types: <code>uint8</code>, <code>int8</code>, <code>int32</code>, <code>bool</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>complex64</code>, <code>complex128</code>. Up to 8-D.</li> <li>\n<code>dims</code>: A <code>Tensor</code> of type <code>bool</code>. 1-D. The dimensions to reverse.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>tensor</code>. The same shape as <code>tensor</code>.</p>  <h3 id=\"transpose\"><code>tf.transpose(a, perm=None, name='transpose')</code></h3> <p>Transposes <code>a</code>. Permutes the dimensions according to <code>perm</code>.</p> <p>The returned tensor's dimension i will correspond to the input dimension <code>perm[i]</code>. If <code>perm</code> is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on 2-D input Tensors.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'x' is [[1 2 3]\n#         [4 5 6]]\ntf.transpose(x) ==&gt; [[1 4]\n                     [2 5]\n                     [3 6]]\n\n# Equivalently\ntf.transpose(x, perm=[1, 0]) ==&gt; [[1 4]\n                                  [2 5]\n                                  [3 6]]\n\n# 'perm' is more useful for n-dimensional tensors, for n &gt; 2\n# 'x' is   [[[1  2  3]\n#            [4  5  6]]\n#           [[7  8  9]\n#            [10 11 12]]]\n# Take the transpose of the matrices in dimension-0\ntf.transpose(x, perm=[0, 2, 1]) ==&gt; [[[1  4]\n                                      [2  5]\n                                      [3  6]]\n\n                                     [[7 10]\n                                      [8 11]\n                                      [9 12]]]\n</pre>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>a</code>: A <code>Tensor</code>.</li> <li>\n<code>perm</code>: A permutation of the dimensions of <code>a</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <p>A transposed <code>Tensor</code>.</p>  <h3 id=\"extract_image_patches\"><code>tf.extract_image_patches(images, ksizes, strides, rates, padding, name=None)</code></h3> <p>Extract <code>patches</code> from <code>images</code> and put them in the \"depth\" output dimension.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>images</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>. 4-D Tensor with shape <code>[batch, in_rows, in_cols, depth]</code>.</li> <li>\n<code>ksizes</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. The size of the sliding window for each dimension of <code>images</code>.</li> <li>\n<code>strides</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. 1-D of length 4. How far the centers of two consecutive patches are in the images. Must be: <code>[1, stride_rows, stride_cols, 1]</code>.</li> <li>\n<code>rates</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. 1-D of length 4. Must be: <code>[1, rate_rows, rate_cols, 1]</code>. This is the input stride, specifying how far two consecutive patch samples are in the input. Equivalent to extracting patches with <code>patch_sizes_eff = patch_sizes + (patch_sizes - 1) * (rates - 1), followed by\nsubsampling them spatially by a factor of</code>rates`.</li> <li>\n<p><code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</p> <p>We specify the size-related attributes as:</p> <pre class=\"\">ksizes = [1, ksize_rows, ksize_cols, 1]\nstrides = [1, strides_rows, strides_cols, 1]\nrates = [1, rates_rows, rates_cols, 1]\n</pre>\n</li> <li><p><code>name</code>: A name for the operation (optional).</p></li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>images</code>. 4-D Tensor with shape <code>[batch, out_rows, out_cols, ksize_rows *\n  ksize_cols * depth]</code> containing image patches with size <code>ksize_rows x ksize_cols x depth</code> vectorized in the \"depth\" dimension.</p>  <h3 id=\"space_to_batch\"><code>tf.space_to_batch(input, paddings, block_size, name=None)</code></h3> <p>SpaceToBatch for 4-D tensors of type T.</p> <p>Zero-pads and then rearranges (permutes) blocks of spatial data into batch. More specifically, this op outputs a copy of the input tensor where values from the <code>height</code> and <code>width</code> dimensions are moved to the <code>batch</code> dimension. After the zero-padding, both <code>height</code> and <code>width</code> of the input must be divisible by the block size.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. 4-D with shape <code>[batch, height, width, depth]</code>.</li> <li>\n<p><code>paddings</code>: A <code>Tensor</code> of type <code>int32</code>. 2-D tensor of non-negative integers with shape <code>[2, 2]</code>. It specifies the padding of the input with zeros across the spatial dimensions as follows:</p> <pre class=\"\">paddings = [[pad_top, pad_bottom], [pad_left, pad_right]]\n</pre> <p>The effective spatial dimensions of the zero-padded input tensor will be:</p> <pre class=\"\">height_pad = pad_top + height + pad_bottom\nwidth_pad = pad_left + width + pad_right\n</pre> <p>The attr <code>block_size</code> must be greater than one. It indicates the block size.</p> <ul> <li>Non-overlapping blocks of size <code>block_size x block size</code> in the height and width dimensions are rearranged into the batch dimension at each location.</li> <li>The batch of the output tensor is <code>batch * block_size * block_size</code>.</li> <li>Both height_pad and width_pad must be divisible by block_size.</li> </ul> <p>The shape of the output will be:</p> <pre class=\"\">[batch*block_size*block_size, height_pad/block_size, width_pad/block_size,\n depth]\n</pre> <p>Some examples:</p> <p>(1) For the following input of shape <code>[1, 2, 2, 1]</code> and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1], [2]], [[3], [4]]]]\n</pre> <p>The output tensor has shape <code>[4, 1, 1, 1]</code> and value:</p> <pre class=\"lang-prettyprint no-auto-prettify\">[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n</pre> <p>(2) For the following input of shape <code>[1, 2, 2, 3]</code> and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n</pre> <p>The output tensor has shape <code>[4, 1, 1, 3]</code> and value:</p> <pre class=\"lang-prettyprint no-auto-prettify\">[[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]], [[10, 11, 12]]]\n</pre> <p>(3) For the following input of shape <code>[1, 4, 4, 1]</code> and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]],\n      [[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n</pre> <p>The output tensor has shape <code>[4, 2, 2, 1]</code> and value:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1], [3]], [[5], [7]]],\n     [[[2], [4]], [[10], [12]]],\n     [[[5], [7]], [[13], [15]]],\n     [[[6], [8]], [[14], [16]]]]\n</pre> <p>(4) For the following input of shape <code>[2, 2, 4, 1]</code> and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]]],\n     [[[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n</pre> <p>The output tensor has shape <code>[8, 1, 2, 1]</code> and value:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1], [3]]], [[[9], [11]]], [[[2], [4]]], [[[10], [12]]],\n     [[[5], [7]]], [[[13], [15]]], [[[6], [8]]], [[[14], [16]]]]\n</pre> <p>Among others, this operation is useful for reducing atrous convolution into regular convolution.</p>\n</li> <li><p><code>block_size</code>: An <code>int</code> that is <code>&gt;= 2</code>.</p></li> <li><p><code>name</code>: A name for the operation (optional).</p></li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h3 id=\"batch_to_space\"><code>tf.batch_to_space(input, crops, block_size, name=None)</code></h3> <p>BatchToSpace for 4-D tensors of type T.</p> <p>Rearranges (permutes) data from batch into blocks of spatial data, followed by cropping. This is the reverse transformation of SpaceToBatch. More specifically, this op outputs a copy of the input tensor where values from the <code>batch</code> dimension are moved in spatial blocks to the <code>height</code> and <code>width</code> dimensions, followed by cropping along the <code>height</code> and <code>width</code> dimensions.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. 4-D tensor with shape <code>[batch*block_size*block_size, height_pad/block_size, width_pad/block_size,\n  depth]</code>. Note that the batch size of the input tensor must be divisible by <code>block_size * block_size</code>.</li> <li>\n<p><code>crops</code>: A <code>Tensor</code> of type <code>int32</code>. 2-D tensor of non-negative integers with shape <code>[2, 2]</code>. It specifies how many elements to crop from the intermediate result across the spatial dimensions as follows:</p> <pre class=\"\">crops = [[crop_top, crop_bottom], [crop_left, crop_right]]\n</pre>\n</li> <li><p><code>block_size</code>: An <code>int</code> that is <code>&gt;= 2</code>.</p></li> <li><p><code>name</code>: A name for the operation (optional).</p></li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. 4-D with shape <code>[batch, height, width, depth]</code>, where:</p> <pre class=\"\">height = height_pad - crop_top - crop_bottom\nwidth = width_pad - crop_left - crop_right\n</pre> <p>The attr <code>block_size</code> must be greater than one. It indicates the block size.</p> <p>Some examples:</p> <p>(1) For the following input of shape <code>[4, 1, 1, 1]</code> and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n</pre> <p>The output tensor has shape <code>[1, 2, 2, 1]</code> and value:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1], [2]], [[3], [4]]]]\n</pre> <p>(2) For the following input of shape <code>[4, 1, 1, 3]</code> and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">[[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]], [[10, 11, 12]]]\n</pre> <p>The output tensor has shape <code>[1, 2, 2, 3]</code> and value:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n</pre> <p>(3) For the following input of shape <code>[4, 2, 2, 1]</code> and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1], [3]], [[5], [7]]],\n     [[[2], [4]], [[10], [12]]],\n     [[[5], [7]], [[13], [15]]],\n     [[[6], [8]], [[14], [16]]]]\n</pre> <p>The output tensor has shape <code>[1, 4, 4, 1]</code> and value:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[1],   [2],  [3],  [4]],\n     [[5],   [6],  [7],  [8]],\n     [[9],  [10], [11],  [12]],\n     [[13], [14], [15],  [16]]]\n</pre> <p>(4) For the following input of shape <code>[8, 1, 2, 1]</code> and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1], [3]]], [[[9], [11]]], [[[2], [4]]], [[[10], [12]]],\n     [[[5], [7]]], [[[13], [15]]], [[[6], [8]]], [[[14], [16]]]]\n</pre> <p>The output tensor has shape <code>[2, 2, 4, 1]</code> and value:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1], [3]], [[5], [7]]],\n     [[[2], [4]], [[10], [12]]],\n     [[[5], [7]], [[13], [15]]],\n     [[[6], [8]], [[14], [16]]]]\n</pre>  <h3 id=\"space_to_depth\"><code>tf.space_to_depth(input, block_size, name=None)</code></h3> <p>SpaceToDepth for tensors of type T.</p> <p>Rearranges blocks of spatial data, into depth. More specifically, this op outputs a copy of the input tensor where values from the <code>height</code> and <code>width</code> dimensions are moved to the <code>depth</code> dimension. The attr <code>block_size</code> indicates the input block size and how the data is moved.</p> <ul> <li>Non-overlapping blocks of size <code>block_size x block size</code> are rearranged into depth at each location.</li> <li>The depth of the output tensor is <code>input_depth * block_size * block_size</code>.</li> <li>The input tensor's height and width must be divisible by block_size.</li> </ul> <p>That is, assuming the input is in the shape: <code>[batch, height, width, depth]</code>, the shape of the output will be: <code>[batch, height/block_size, width/block_size, depth*block_size*block_size]</code></p> <p>This operation requires that the input tensor be of rank 4, and that <code>block_size</code> be &gt;=1 and a divisor of both the input <code>height</code> and <code>width</code>.</p> <p>This operation is useful for resizing the activations between convolutions (but keeping all data), e.g. instead of pooling. It is also useful for training purely convolutional models.</p> <p>For example, given this input of shape <code>[1, 2, 2, 1]</code>, and block_size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1], [2]],\n      [[3], [4]]]]\n</pre> <p>This operation will output a tensor of shape <code>[1, 1, 1, 4]</code>:</p> <pre class=\"lang-prettyprint no-auto-prettify\">[[[[1, 2, 3, 4]]]]\n</pre> <p>Here, the input has a batch of 1 and each batch element has shape <code>[2, 2, 1]</code>, the corresponding output will have a single element (i.e. width and height are both 1) and will have a depth of 4 channels (1 * block_size * block_size). The output element shape is <code>[1, 1, 4]</code>.</p> <p>For an input tensor with larger depth, here of shape <code>[1, 2, 2, 3]</code>, e.g.</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n</pre> <p>This operation, for block_size of 2, will return the following tensor of shape <code>[1, 1, 1, 12]</code></p> <pre class=\"lang-prettyprint no-auto-prettify\">[[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]]]\n</pre> <p>Similarly, for the following input of shape <code>[1 4 4 1]</code>, and a block size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1],   [2],  [5],  [6]],\n      [[3],   [4],  [7],  [8]],\n      [[9],  [10], [13],  [14]],\n      [[11], [12], [15],  [16]]]]\n</pre> <p>the operator will return the following tensor of shape <code>[1 2 2 4]</code>:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1, 2, 3, 4],\n       [5, 6, 7, 8]],\n      [[9, 10, 11, 12],\n       [13, 14, 15, 16]]]]\n</pre>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>.</li> <li>\n<code>block_size</code>: An <code>int</code> that is <code>&gt;= 2</code>. The size of the spatial block.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h3 id=\"depth_to_space\"><code>tf.depth_to_space(input, block_size, name=None)</code></h3> <p>DepthToSpace for tensors of type T.</p> <p>Rearranges data from depth into blocks of spatial data. This is the reverse transformation of SpaceToDepth. More specifically, this op outputs a copy of the input tensor where values from the <code>depth</code> dimension are moved in spatial blocks to the <code>height</code> and <code>width</code> dimensions. The attr <code>block_size</code> indicates the input block size and how the data is moved.</p> <ul> <li>Chunks of data of size <code>block_size * block_size</code> from depth are rearranged into non-overlapping blocks of size <code>block_size x block_size</code>\n</li> <li>The width the output tensor is <code>input_depth * block_size</code>, whereas the height is <code>input_height * block_size</code>.</li> <li>The depth of the input tensor must be divisible by <code>block_size * block_size</code>.</li> </ul> <p>That is, assuming the input is in the shape: <code>[batch, height, width, depth]</code>, the shape of the output will be: <code>[batch, height*block_size, width*block_size, depth/(block_size*block_size)]</code></p> <p>This operation requires that the input tensor be of rank 4, and that <code>block_size</code> be &gt;=1 and that <code>block_size * block_size</code> be a divisor of the input depth.</p> <p>This operation is useful for resizing the activations between convolutions (but keeping all data), e.g. instead of pooling. It is also useful for training purely convolutional models.</p> <p>For example, given this input of shape <code>[1, 1, 1, 4]</code>, and a block size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1, 2, 3, 4]]]]\n\n</pre> <p>This operation will output a tensor of shape <code>[1, 2, 2, 1]</code>:</p> <pre class=\"lang-prettyprint no-auto-prettify\">[[[[1], [2]],\n  [[3], [4]]]]\n</pre> <p>Here, the input has a batch of 1 and each batch element has shape <code>[1, 1, 4]</code>, the corresponding output will have 2x2 elements and will have a depth of 1 channel (1 = <code>4 / (block_size * block_size)</code>). The output element shape is <code>[2, 2, 1]</code>.</p> <p>For an input tensor with larger depth, here of shape <code>[1, 1, 1, 12]</code>, e.g.</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]]]\n</pre> <p>This operation, for block size of 2, will return the following tensor of shape <code>[1, 2, 2, 3]</code></p> <pre class=\"lang-prettyprint no-auto-prettify\">[[[[1, 2, 3], [4, 5, 6]],\n  [[7, 8, 9], [10, 11, 12]]]]\n\n</pre> <p>Similarly, for the following input of shape <code>[1 2 2 4]</code>, and a block size of 2:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x =  [[[[1, 2, 3, 4],\n       [5, 6, 7, 8]],\n      [[9, 10, 11, 12],\n       [13, 14, 15, 16]]]]\n</pre> <p>the operator will return the following tensor of shape <code>[1 4 4 1]</code>:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [[ [1],   [2],  [5],  [6]],\n     [ [3],   [4],  [7],  [8]],\n     [ [9],  [10], [13],  [14]],\n     [ [11], [12], [15],  [16]]]\n\n</pre>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>.</li> <li>\n<code>block_size</code>: An <code>int</code> that is <code>&gt;= 2</code>. The size of the spatial block, same as in Space2Depth.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h3 id=\"gather\"><code>tf.gather(params, indices, validate_indices=None, name=None)</code></h3> <p>Gather slices from <code>params</code> according to <code>indices</code>.</p> <p><code>indices</code> must be an integer tensor of any dimension (usually 0-D or 1-D). Produces an output tensor with shape <code>indices.shape + params.shape[1:]</code> where:</p> <pre class=\"\"># Scalar indices\noutput[:, ..., :] = params[indices, :, ... :]\n\n# Vector indices\noutput[i, :, ..., :] = params[indices[i], :, ... :]\n\n# Higher rank indices\noutput[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n</pre> <p>If <code>indices</code> is a permutation and <code>len(indices) == params.shape[0]</code> then this operation will permute <code>params</code> accordingly.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/Gather.png\" alt> </div>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>params</code>: A <code>Tensor</code>.</li> <li>\n<code>indices</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>.</li> <li>\n<code>validate_indices</code>: An optional <code>bool</code>. Defaults to <code>True</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>params</code>.</p>  <h3 id=\"gather_nd\"><code>tf.gather_nd(params, indices, name=None)</code></h3> <p>Gather values or slices from <code>params</code> according to <code>indices</code>.</p> <p><code>params</code> is a Tensor of rank <code>R</code> and <code>indices</code> is a Tensor of rank <code>M</code>.</p> <p><code>indices</code> must be integer tensor, containing indices into <code>params</code>. It must be shape <code>[d_0, ..., d_N, R]</code> where <code>0 &lt; R &lt;= M</code>.</p> <p>The innermost dimension of <code>indices</code> (with length <code>R</code>) corresponds to indices into elements (if <code>R = M</code>) or slices (if <code>R &lt; M</code>) along the <code>N</code>th dimension of <code>params</code>.</p> <p>Produces an output tensor with shape</p> <pre class=\"\">[d_0, ..., d_{n-1}, params.shape[R], ..., params.shape[M-1]].\n</pre> <p>Some examples below.</p> <p>Simple indexing into a matrix:</p> <pre class=\"\">indices = [[0, 0], [1, 1]]\nparams = [['a', 'b'], ['c', 'd']]\noutput = ['a', 'd']\n</pre> <p>Slice indexing into a matrix:</p> <pre class=\"lang-tcl no-auto-prettify\">indices = [[1], [0]]\nparams = [['a', 'b'], ['c', 'd']]\noutput = [['c', 'd'], ['a', 'b']]\n</pre> <p>Indexing into a 3-tensor:</p> <pre class=\"\">indices = [[1]]\nparams = [[['a0', 'b0'], ['c0', 'd0']],\n          [['a1', 'b1'], ['c1', 'd1']]]\noutput = [[['a1', 'b1'], ['c1', 'd1']]]\n\n\nindices = [[0, 1], [1, 0]]\nparams = [[['a0', 'b0'], ['c0', 'd0']],\n          [['a1', 'b1'], ['c1', 'd1']]]\noutput = [['c0', 'd0'], ['a1', 'b1']]\n\n\nindices = [[0, 0, 1], [1, 0, 1]]\nparams = [[['a0', 'b0'], ['c0', 'd0']],\n          [['a1', 'b1'], ['c1', 'd1']]]\noutput = ['b0', 'b1']\n</pre> <p>Batched indexing into a matrix:</p> <pre class=\"\">indices = [[[0, 0]], [[0, 1]]]\nparams = [['a', 'b'], ['c', 'd']]\noutput = [['a'], ['b']]\n</pre> <p>Batched slice indexing into a matrix:</p> <pre class=\"\">indices = [[[1]], [[0]]]\nparams = [['a', 'b'], ['c', 'd']]\noutput = [[['c', 'd']], [['a', 'b']]]\n</pre> <p>Batched indexing into a 3-tensor:</p> <pre class=\"\">indices = [[[1]], [[0]]]\nparams = [[['a0', 'b0'], ['c0', 'd0']],\n          [['a1', 'b1'], ['c1', 'd1']]]\noutput = [[[['a1', 'b1'], ['c1', 'd1']]],\n          [[['a0', 'b0'], ['c0', 'd0']]]]\n\n\nindices = [[[0, 1], [1, 0]], [[0, 0], [1, 1]]]\nparams = [[['a0', 'b0'], ['c0', 'd0']],\n          [['a1', 'b1'], ['c1', 'd1']]]\noutput = [[['c0', 'd0'], ['a1', 'b1']],\n          [['a0', 'b0'], ['c1', 'd1']]]\n\n\nindices = [[[0, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 1, 0]]]\nparams = [[['a0', 'b0'], ['c0', 'd0']],\n          [['a1', 'b1'], ['c1', 'd1']]]\noutput = [['b0', 'b1'], ['d0', 'c1']]\n</pre>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>params</code>: A <code>Tensor</code>. <code>M-D</code>. The tensor from which to gather values.</li> <li>\n<code>indices</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. <code>(N+1)-D</code>. Index tensor having shape <code>[d_0, ..., d_N, R]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>params</code>. <code>(N+M-R)-D</code>. Values from <code>params</code> gathered from indices given by <code>indices</code>.</p>  <h3 id=\"dynamic_partition\"><code>tf.dynamic_partition(data, partitions, num_partitions, name=None)</code></h3> <p>Partitions <code>data</code> into <code>num_partitions</code> tensors using indices from <code>partitions</code>.</p> <p>For each index tuple <code>js</code> of size <code>partitions.ndim</code>, the slice <code>data[js, ...]</code> becomes part of <code>outputs[partitions[js]]</code>. The slices with <code>partitions[js] = i</code> are placed in <code>outputs[i]</code> in lexicographic order of <code>js</code>, and the first dimension of <code>outputs[i]</code> is the number of entries in <code>partitions</code> equal to <code>i</code>. In detail,</p> <pre class=\"\">outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]\n\noutputs[i] = pack([data[js, ...] for js if partitions[js] == i])\n</pre> <p><code>data.shape</code> must start with <code>partitions.shape</code>.</p> <p>For example:</p> <pre class=\"lang-matlab no-auto-prettify\"># Scalar partitions\npartitions = 1\nnum_partitions = 2\ndata = [10, 20]\noutputs[0] = []  # Empty with shape [0, 2]\noutputs[1] = [[10, 20]]\n\n# Vector partitions\npartitions = [0, 0, 1, 1, 0]\nnum_partitions = 2\ndata = [10, 20, 30, 40, 50]\noutputs[0] = [10, 20, 50]\noutputs[1] = [30, 40]\n</pre> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/DynamicPartition.png\" alt> </div>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>.</li> <li>\n<code>partitions</code>: A <code>Tensor</code> of type <code>int32</code>. Any shape. Indices in the range <code>[0, num_partitions)</code>.</li> <li>\n<code>num_partitions</code>: An <code>int</code> that is <code>&gt;= 1</code>. The number of partitions to output.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-34\">Returns:</h5> <p>A list of <code>num_partitions</code> <code>Tensor</code> objects of the same type as data.</p>  <h3 id=\"dynamic_stitch\"><code>tf.dynamic_stitch(indices, data, name=None)</code></h3> <p>Interleave the values from the <code>data</code> tensors into a single tensor.</p> <p>Builds a merged tensor such that</p> <pre class=\"\">merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]\n</pre> <p>For example, if each <code>indices[m]</code> is scalar or vector, we have</p> <pre class=\"\"># Scalar indices\nmerged[indices[m], ...] = data[m][...]\n\n# Vector indices\nmerged[indices[m][i], ...] = data[m][i, ...]\n</pre> <p>Each <code>data[i].shape</code> must start with the corresponding <code>indices[i].shape</code>, and the rest of <code>data[i].shape</code> must be constant w.r.t. <code>i</code>. That is, we must have <code>data[i].shape = indices[i].shape + constant</code>. In terms of this <code>constant</code>, the output shape is</p> <pre class=\"\">merged.shape = [max(indices)] + constant\n</pre> <p>Values are merged in order, so if an index appears in both <code>indices[m][i]</code> and <code>indices[n][j]</code> for <code>(m,i) &lt; (n,j)</code> the slice <code>data[n][j]</code> will appear in the merged result.</p> <p>For example:</p> <pre class=\"\">indices[0] = 6\nindices[1] = [4, 1]\nindices[2] = [[5, 2], [0, 3]]\ndata[0] = [61, 62]\ndata[1] = [[41, 42], [11, 12]]\ndata[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]\nmerged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],\n          [51, 52], [61, 62]]\n</pre> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/DynamicStitch.png\" alt> </div>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>indices</code>: A list of at least 2 <code>Tensor</code> objects of type <code>int32</code>.</li> <li>\n<code>data</code>: A list with the same number of <code>Tensor</code> objects as <code>indices</code> of <code>Tensor</code> objects of the same type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>.</p>  <h3 id=\"boolean_mask\"><code>tf.boolean_mask(tensor, mask, name='boolean_mask')</code></h3> <p>Apply boolean mask to tensor. Numpy equivalent is <code>tensor[mask]</code>.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 1-D example\ntensor = [0, 1, 2, 3]\nmask = [True, False, True, False]\nboolean_mask(tensor, mask) ==&gt; [0, 2]\n</pre> <p>In general, <code>0 &lt; dim(mask) = K &lt;= dim(tensor)</code>, and <code>mask</code>'s shape must match the first K dimensions of <code>tensor</code>'s shape. We then have: <code>boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd]</code> where <code>(i1,...,iK)</code> is the ith <code>True</code> entry of <code>mask</code> (row-major order).</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>tensor</code>: N-D tensor.</li> <li>\n<code>mask</code>: K-D boolean tensor, K &lt;= N and K must be known statically.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p>Tensor populated by entries in <code>tensor</code> corresponding to <code>True</code> values in <code>mask</code>.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li><p><code>ValueError</code>: If shapes do not conform.</p></li> <li><p><code>Examples</code>: </p></li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 2-D example\ntensor = [[1, 2], [3, 4], [5, 6]]\nmask = [True, False, True]\nboolean_mask(tensor, mask) ==&gt; [[1, 2], [5, 6]]\n</pre>  <h3 id=\"one_hot\"><code>tf.one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)</code></h3> <p>Returns a one-hot tensor.</p> <p>The locations represented by indices in <code>indices</code> take value <code>on_value</code>, while all other locations take value <code>off_value</code>.</p> <p><code>on_value</code> and <code>off_value</code> must have matching data types. If <code>dtype</code> is also provided, they must be the same data type as specified by <code>dtype</code>.</p> <p>If <code>on_value</code> is not provided, it will default to the value <code>1</code> with type <code>dtype</code></p> <p>If <code>off_value</code> is not provided, it will default to the value <code>0</code> with type <code>dtype</code></p> <p>If the input <code>indices</code> is rank <code>N</code>, the output will have rank <code>N+1</code>. The new axis is created at dimension <code>axis</code> (default: the new axis is appended at the end).</p> <p>If <code>indices</code> is a scalar the output shape will be a vector of length <code>depth</code></p> <p>If <code>indices</code> is a vector of length <code>features</code>, the output shape will be: <code>features x depth if axis == -1\n  depth x features if axis == 0</code></p> <p>If <code>indices</code> is a matrix (batch) with shape <code>[batch, features]</code>, the output shape will be: <code>batch x features x depth if axis == -1\n  batch x depth x features if axis == 1\n  depth x batch x features if axis == 0</code></p> <p>If <code>dtype</code> is not provided, it will attempt to assume the data type of <code>on_value</code> or <code>off_value</code>, if one or both are passed in. If none of <code>on_value</code>, <code>off_value</code>, or <code>dtype</code> are provided, <code>dtype</code> will default to the value <code>tf.float32</code></p> <p>Note: If a non-numeric data type output is desired (tf.string, tf.bool, etc.), both <code>on_value</code> and <code>off_value</code> <em>must</em> be provided to <code>one_hot</code></p> <h1 id=\"examples\">Examples</h1> <p>Suppose that</p> <pre class=\"\">indices = [0, 2, -1, 1]\ndepth = 3\non_value = 5.0\noff_value = 0.0\naxis = -1\n</pre> <p>Then output is <code>[4 x 3]</code>:</p> <pre class=\"\">output =\n[5.0 0.0 0.0]  // one_hot(0)\n[0.0 0.0 5.0]  // one_hot(2)\n[0.0 0.0 0.0]  // one_hot(-1)\n[0.0 5.0 0.0]  // one_hot(1)\n</pre> <p>Suppose that</p> <pre class=\"\">indices = [[0, 2], [1, -1]]\ndepth = 3\non_value = 1.0\noff_value = 0.0\naxis = -1\n</pre> <p>Then output is <code>[2 x 2 x 3]</code>:</p> <pre class=\"\">output =\n[\n  [1.0, 0.0, 0.0]  // one_hot(0)\n  [0.0, 0.0, 1.0]  // one_hot(2)\n][\n  [0.0, 1.0, 0.0]  // one_hot(1)\n  [0.0, 0.0, 0.0]  // one_hot(-1)\n]\n</pre> <p>Using default values for <code>on_value</code> and <code>off_value</code>:</p> <pre class=\"\">indices = [0, 1, 2]\ndepth = 3\n</pre> <p>The output will be</p> <pre class=\"\">output =\n[[1., 0., 0.],\n [0., 1., 0.],\n [0., 0., 1.]]\n</pre>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>indices</code>: A <code>Tensor</code> of indices.</li> <li>\n<code>depth</code>: A scalar defining the depth of the one hot dimension.</li> <li>\n<code>on_value</code>: A scalar defining the value to fill in output when <code>indices[j]\n= i</code>. (default: 1)</li> <li>\n<code>off_value</code>: A scalar defining the value to fill in output when <code>indices[j]\n!= i</code>. (default: 0)</li> <li>\n<code>axis</code>: The axis to fill (default: -1, a new inner-most axis).</li> <li>\n<code>dtype</code>: The data type of the output tensor.</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <ul> <li>\n<code>output</code>: The one-hot tensor.</li> </ul>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If dtype of either <code>on_value</code> or <code>off_value</code> don't match <code>dtype</code>\n</li> <li>\n<code>TypeError</code>: If dtype of <code>on_value</code> and <code>off_value</code> don't match one another</li> </ul>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"bitcast\"><code>tf.bitcast(input, type, name=None)</code></h3> <p>Bitcasts a tensor from one type to another without copying data.</p> <p>Given a tensor <code>input</code>, this operation returns a tensor that has the same buffer data as <code>input</code> with datatype <code>type</code>.</p> <p>If the input datatype <code>T</code> is larger than the output datatype <code>type</code> then the shape changes from [...] to [..., sizeof(<code>T</code>)/sizeof(<code>type</code>)].</p> <p>If <code>T</code> is smaller than <code>type</code>, the operator requires that the rightmost dimension be equal to sizeof(<code>type</code>)/sizeof(<code>T</code>). The shape then goes from [..., sizeof(<code>type</code>)/sizeof(<code>T</code>)] to [...].</p> <p><em>NOTE</em>: Bitcast is implemented as a low-level cast, so machines with different endian orderings will give different results.</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.</li> <li>\n<code>type</code>: A <code>tf.DType</code> from: <code>tf.float32, tf.float64, tf.int64, tf.int32, tf.uint8, tf.uint16, tf.int16, tf.int8, tf.complex64, tf.complex128, tf.qint8, tf.quint8, tf.qint32, tf.half</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>A <code>Tensor</code> of type <code>type</code>.</p>  <h3 id=\"copy\"><code>tf.contrib.graph_editor.copy(sgv, dst_graph=None, dst_scope='', src_scope='')</code></h3> <p>Copy a subgraph.</p>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>sgv</code>: the source subgraph-view. This argument is converted to a subgraph using the same rules than the function subgraph.make_view.</li> <li>\n<code>dst_graph</code>: the destination graph.</li> <li>\n<code>dst_scope</code>: the destination scope.</li> <li>\n<code>src_scope</code>: the source scope.</li> </ul>  <h5 id=\"returns-39\">Returns:</h5> <p>the subgraph view of the copied subgraph.</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if dst_graph is not a tf.Graph.</li> <li>\n<code>StandardError</code>: if sgv cannot be converted to a SubGraphView using the same rules than the function subgraph.make_view.</li> </ul>  <h3 id=\"shape_n\"><code>tf.shape_n(input, name=None)</code></h3> <p>Returns shape of tensors.</p> <p>This operation returns N 1-D integer tensors representing shape of <code>input[i]s</code>.</p>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>input</code>: A list of at least 1 <code>Tensor</code> objects of the same type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-40\">Returns:</h5> <p>A list with the same number of <code>Tensor</code> objects as <code>input</code> of <code>Tensor</code> objects of type <code>int32</code>.</p>  <h3 id=\"unique_with_counts\"><code>tf.unique_with_counts(x, name=None)</code></h3> <p>Finds unique elements in a 1-D tensor.</p> <p>This operation returns a tensor <code>y</code> containing all of the unique elements of <code>x</code> sorted in the same order that they occur in <code>x</code>. This operation also returns a tensor <code>idx</code> the same size as <code>x</code> that contains the index of each value of <code>x</code> in the unique output <code>y</code>. Finally, it returns a third tensor <code>count</code> that contains the count of each element of <code>y</code> in <code>x</code>. In other words:</p> <p><code>y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]</code></p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx, count = unique_with_counts(x)\ny ==&gt; [1, 2, 4, 7, 8]\nidx ==&gt; [0, 0, 1, 2, 2, 2, 3, 4, 4]\ncount ==&gt; [2, 1, 3, 1, 2]\n</pre>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. 1-D.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-41\">Returns:</h5> <p>A tuple of <code>Tensor</code> objects (y, idx, count).</p> <ul> <li>\n<code>y</code>: A <code>Tensor</code>. Has the same type as <code>x</code>. 1-D.</li> <li>\n<code>idx</code>: A <code>Tensor</code> of type <code>int32</code>. 1-D.</li> <li>\n<code>count</code>: A <code>Tensor</code> of type <code>int32</code>. 1-D.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html</a>\n  </p>\n</div>\n","image":"<h1 id=\"images\">Images</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#images\">Images</a></li> <ul> <li><a href=\"#encoding-and-decoding\">Encoding and Decoding</a></li> <ul> <li><a href=\"#decode_jpeg\"><code>tf.image.decode_jpeg(contents, channels=None, ratio=None, fancy_upscaling=None, try_recover_truncated=None, acceptable_fraction=None, name=None)</code></a></li> <li><a href=\"#encode_jpeg\"><code>tf.image.encode_jpeg(image, format=None, quality=None, progressive=None, optimize_size=None, chroma_downsampling=None, density_unit=None, x_density=None, y_density=None, xmp_metadata=None, name=None)</code></a></li> <li><a href=\"#decode_png\"><code>tf.image.decode_png(contents, channels=None, dtype=None, name=None)</code></a></li> <li><a href=\"#encode_png\"><code>tf.image.encode_png(image, compression=None, name=None)</code></a></li> </ul> <li><a href=\"#resizing\">Resizing</a></li> <ul> <li><a href=\"#resize_images\"><code>tf.image.resize_images(images, new_height, new_width, method=0, align_corners=False)</code></a></li> <li><a href=\"#resize_area\"><code>tf.image.resize_area(images, size, align_corners=None, name=None)</code></a></li> <li><a href=\"#resize_bicubic\"><code>tf.image.resize_bicubic(images, size, align_corners=None, name=None)</code></a></li> <li><a href=\"#resize_bilinear\"><code>tf.image.resize_bilinear(images, size, align_corners=None, name=None)</code></a></li> <li><a href=\"#resize_nearest_neighbor\"><code>tf.image.resize_nearest_neighbor(images, size, align_corners=None, name=None)</code></a></li> </ul> <li><a href=\"#cropping\">Cropping</a></li> <ul> <li><a href=\"#resize_image_with_crop_or_pad\"><code>tf.image.resize_image_with_crop_or_pad(image, target_height, target_width)</code></a></li> <li><a href=\"#central_crop\"><code>tf.image.central_crop(image, central_fraction)</code></a></li> <li><a href=\"#pad_to_bounding_box\"><code>tf.image.pad_to_bounding_box(image, offset_height, offset_width, target_height, target_width)</code></a></li> <li><a href=\"#crop_to_bounding_box\"><code>tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)</code></a></li> <li><a href=\"#extract_glimpse\"><code>tf.image.extract_glimpse(input, size, offsets, centered=None, normalized=None, uniform_noise=None, name=None)</code></a></li> <li><a href=\"#crop_and_resize\"><code>tf.image.crop_and_resize(image, boxes, box_ind, crop_size, method=None, extrapolation_value=None, name=None)</code></a></li> </ul> <li><a href=\"#flipping-rotating-and-transposing\">Flipping, Rotating and Transposing</a></li> <ul> <li><a href=\"#flip_up_down\"><code>tf.image.flip_up_down(image)</code></a></li> <li><a href=\"#random_flip_up_down\"><code>tf.image.random_flip_up_down(image, seed=None)</code></a></li> <li><a href=\"#flip_left_right\"><code>tf.image.flip_left_right(image)</code></a></li> <li><a href=\"#random_flip_left_right\"><code>tf.image.random_flip_left_right(image, seed=None)</code></a></li> <li><a href=\"#transpose_image\"><code>tf.image.transpose_image(image)</code></a></li> <li><a href=\"#rot90\"><code>tf.image.rot90(image, k=1)</code></a></li> </ul> <li><a href=\"#converting-between-colorspaces\">Converting Between Colorspaces.</a></li> <ul> <li><a href=\"#rgb_to_grayscale\"><code>tf.image.rgb_to_grayscale(images, name=None)</code></a></li> <li><a href=\"#grayscale_to_rgb\"><code>tf.image.grayscale_to_rgb(images, name=None)</code></a></li> <li><a href=\"#hsv_to_rgb\"><code>tf.image.hsv_to_rgb(images, name=None)</code></a></li> <li><a href=\"#rgb_to_hsv\"><code>tf.image.rgb_to_hsv(images, name=None)</code></a></li> <li><a href=\"#convert_image_dtype\"><code>tf.image.convert_image_dtype(image, dtype, saturate=False, name=None)</code></a></li> </ul> <li><a href=\"#image-adjustments\">Image Adjustments</a></li> <ul> <li><a href=\"#adjust_brightness\"><code>tf.image.adjust_brightness(image, delta)</code></a></li> <li><a href=\"#random_brightness\"><code>tf.image.random_brightness(image, max_delta, seed=None)</code></a></li> <li><a href=\"#adjust_contrast\"><code>tf.image.adjust_contrast(images, contrast_factor)</code></a></li> <li><a href=\"#random_contrast\"><code>tf.image.random_contrast(image, lower, upper, seed=None)</code></a></li> <li><a href=\"#adjust_hue\"><code>tf.image.adjust_hue(image, delta, name=None)</code></a></li> <li><a href=\"#random_hue\"><code>tf.image.random_hue(image, max_delta, seed=None)</code></a></li> <li><a href=\"#adjust_saturation\"><code>tf.image.adjust_saturation(image, saturation_factor, name=None)</code></a></li> <li><a href=\"#random_saturation\"><code>tf.image.random_saturation(image, lower, upper, seed=None)</code></a></li> <li><a href=\"#per_image_whitening\"><code>tf.image.per_image_whitening(image)</code></a></li> </ul> <li><a href=\"#working-with-bounding-boxes\">Working with Bounding Boxes</a></li> <ul> <li><a href=\"#draw_bounding_boxes\"><code>tf.image.draw_bounding_boxes(images, boxes, name=None)</code></a></li> <li><a href=\"#non_max_suppression\"><code>tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=None, name=None)</code></a></li> <li><a href=\"#sample_distorted_bounding_box\"><code>tf.image.sample_distorted_bounding_box(image_size, bounding_boxes, seed=None, seed2=None, min_object_covered=None, aspect_ratio_range=None, area_range=None, max_attempts=None, use_image_if_no_bounding_boxes=None, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"encoding-and-decoding\">Encoding and Decoding</h2> <p>TensorFlow provides Ops to decode and encode JPEG and PNG formats. Encoded images are represented by scalar string Tensors, decoded images by 3-D uint8 tensors of shape <code>[height, width, channels]</code>. (PNG also supports uint16.)</p> <p>The encode and decode Ops apply to one image at a time. Their input and output are all of variable size. If you need fixed size images, pass the output of the decode Ops to one of the cropping and resizing Ops.</p> <p>Note: The PNG encode and decode Ops support RGBA, but the conversions Ops presently only support RGB, HSV, and GrayScale. Presently, the alpha channel has to be stripped from the image and re-attached using slicing ops.</p>  <h3 id=\"decode_jpeg\"><code>tf.image.decode_jpeg(contents, channels=None, ratio=None, fancy_upscaling=None, try_recover_truncated=None, acceptable_fraction=None, name=None)</code></h3> <p>Decode a JPEG-encoded image to a uint8 tensor.</p> <p>The attr <code>channels</code> indicates the desired number of color channels for the decoded image.</p> <p>Accepted values are:</p> <ul> <li>0: Use the number of channels in the JPEG-encoded image.</li> <li>1: output a grayscale image.</li> <li>3: output an RGB image.</li> </ul> <p>If needed, the JPEG-encoded image is transformed to match the requested number of color channels.</p> <p>The attr <code>ratio</code> allows downscaling the image by an integer factor during decoding. Allowed values are: 1, 2, 4, and 8. This is much faster than downscaling the image later.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>contents</code>: A <code>Tensor</code> of type <code>string</code>. 0-D. The JPEG-encoded image.</li> <li>\n<code>channels</code>: An optional <code>int</code>. Defaults to <code>0</code>. Number of color channels for the decoded image.</li> <li>\n<code>ratio</code>: An optional <code>int</code>. Defaults to <code>1</code>. Downscaling ratio.</li> <li>\n<code>fancy_upscaling</code>: An optional <code>bool</code>. Defaults to <code>True</code>. If true use a slower but nicer upscaling of the chroma planes (yuv420/422 only).</li> <li>\n<code>try_recover_truncated</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If true try to recover an image from truncated input.</li> <li>\n<code>acceptable_fraction</code>: An optional <code>float</code>. Defaults to <code>1</code>. The minimum required fraction of lines before a truncated input is accepted.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code> of type <code>uint8</code>. 3-D with shape <code>[height, width, channels]</code>..</p>  <h3 id=\"encode_jpeg\"><code>tf.image.encode_jpeg(image, format=None, quality=None, progressive=None, optimize_size=None, chroma_downsampling=None, density_unit=None, x_density=None, y_density=None, xmp_metadata=None, name=None)</code></h3> <p>JPEG-encode an image.</p> <p><code>image</code> is a 3-D uint8 Tensor of shape <code>[height, width, channels]</code>.</p> <p>The attr <code>format</code> can be used to override the color format of the encoded output. Values can be:</p> <ul> <li>\n<code>''</code>: Use a default format based on the number of channels in the image.</li> <li>\n<code>grayscale</code>: Output a grayscale JPEG image. The <code>channels</code> dimension of <code>image</code> must be 1.</li> <li>\n<code>rgb</code>: Output an RGB JPEG image. The <code>channels</code> dimension of <code>image</code> must be 3.</li> </ul> <p>If <code>format</code> is not specified or is the empty string, a default format is picked in function of the number of channels in <code>image</code>:</p> <ul> <li>1: Output a grayscale image.</li> <li>3: Output an RGB image.</li> </ul>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>image</code>: A <code>Tensor</code> of type <code>uint8</code>. 3-D with shape <code>[height, width, channels]</code>.</li> <li>\n<code>format</code>: An optional <code>string</code> from: <code>\"\", \"grayscale\", \"rgb\"</code>. Defaults to <code>\"\"</code>. Per pixel image format.</li> <li>\n<code>quality</code>: An optional <code>int</code>. Defaults to <code>95</code>. Quality of the compression from 0 to 100 (higher is better and slower).</li> <li>\n<code>progressive</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If True, create a JPEG that loads progressively (coarse to fine).</li> <li>\n<code>optimize_size</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If True, spend CPU/RAM to reduce size with no quality change.</li> <li>\n<code>chroma_downsampling</code>: An optional <code>bool</code>. Defaults to <code>True</code>. See <a target=\"_blank\" href=\"https://www.google.com/url?q=http://en.wikipedia.org/wiki/Chroma_subsampling&amp;usg=AFQjCNHsxo2igvsqZa9KTxD2XZCCYUsn9g\">http://en.wikipedia.org/wiki/Chroma_subsampling</a>.</li> <li>\n<code>density_unit</code>: An optional <code>string</code> from: <code>\"in\", \"cm\"</code>. Defaults to <code>\"in\"</code>. Unit used to specify <code>x_density</code> and <code>y_density</code>: pixels per inch (<code>'in'</code>) or centimeter (<code>'cm'</code>).</li> <li>\n<code>x_density</code>: An optional <code>int</code>. Defaults to <code>300</code>. Horizontal pixels per density unit.</li> <li>\n<code>y_density</code>: An optional <code>int</code>. Defaults to <code>300</code>. Vertical pixels per density unit.</li> <li>\n<code>xmp_metadata</code>: An optional <code>string</code>. Defaults to <code>\"\"</code>. If not empty, embed this XMP metadata in the image header.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code> of type <code>string</code>. 0-D. JPEG-encoded image.</p>  <h3 id=\"decode_png\"><code>tf.image.decode_png(contents, channels=None, dtype=None, name=None)</code></h3> <p>Decode a PNG-encoded image to a uint8 or uint16 tensor.</p> <p>The attr <code>channels</code> indicates the desired number of color channels for the decoded image.</p> <p>Accepted values are:</p> <ul> <li>0: Use the number of channels in the PNG-encoded image.</li> <li>1: output a grayscale image.</li> <li>3: output an RGB image.</li> <li>4: output an RGBA image.</li> </ul> <p>If needed, the PNG-encoded image is transformed to match the requested number of color channels.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>contents</code>: A <code>Tensor</code> of type <code>string</code>. 0-D. The PNG-encoded image.</li> <li>\n<code>channels</code>: An optional <code>int</code>. Defaults to <code>0</code>. Number of color channels for the decoded image.</li> <li>\n<code>dtype</code>: An optional <code>tf.DType</code> from: <code>tf.uint8, tf.uint16</code>. Defaults to <code>tf.uint8</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>Tensor</code> of type <code>dtype</code>. 3-D with shape <code>[height, width, channels]</code>.</p>  <h3 id=\"encode_png\"><code>tf.image.encode_png(image, compression=None, name=None)</code></h3> <p>PNG-encode an image.</p> <p><code>image</code> is a 3-D uint8 or uint16 Tensor of shape <code>[height, width, channels]</code> where <code>channels</code> is:</p> <ul> <li>1: for grayscale.</li> <li>2: for grayscale + alpha.</li> <li>3: for RGB.</li> <li>4: for RGBA.</li> </ul> <p>The ZLIB compression level, <code>compression</code>, can be -1 for the PNG-encoder default or a value from 0 to 9. 9 is the highest compression level, generating the smallest output, but is slower.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>image</code>: A <code>Tensor</code>. Must be one of the following types: <code>uint8</code>, <code>uint16</code>. 3-D with shape <code>[height, width, channels]</code>.</li> <li>\n<code>compression</code>: An optional <code>int</code>. Defaults to <code>-1</code>. Compression level.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>Tensor</code> of type <code>string</code>. 0-D. PNG-encoded image.</p> <h2 id=\"resizing\">Resizing</h2> <p>The resizing Ops accept input images as tensors of several types. They always output resized images as float32 tensors.</p> <p>The convenience function <a href=\"#resize_images\"><code>resize_images()</code></a> supports both 4-D and 3-D tensors as input and output. 4-D tensors are for batches of images, 3-D tensors for individual images.</p> <p>Other resizing Ops only support 4-D batches of images as input: <a href=\"#resize_area\"><code>resize_area</code></a>, <a href=\"#resize_bicubic\"><code>resize_bicubic</code></a>, <a href=\"#resize_bilinear\"><code>resize_bilinear</code></a>, <a href=\"#resize_nearest_neighbor\"><code>resize_nearest_neighbor</code></a>.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Decode a JPG image and resize it to 299 by 299 using default method.\nimage = tf.image.decode_jpeg(...)\nresized_image = tf.image.resize_images(image, 299, 299)\n</pre>  <h3 id=\"resize_images\"><code>tf.image.resize_images(images, new_height, new_width, method=0, align_corners=False)</code></h3> <p>Resize <code>images</code> to <code>new_width</code>, <code>new_height</code> using the specified <code>method</code>.</p> <p>Resized images will be distorted if their original aspect ratio is not the same as <code>new_width</code>, <code>new_height</code>. To avoid distortions see <a href=\"#resize_image_with_crop_or_pad\"><code>resize_image_with_crop_or_pad</code></a>.</p> <p><code>method</code> can be one of:</p> <ul> <li>\n<code>ResizeMethod.BILINEAR</code>: <a href=\"https://en.wikipedia.org/wiki/Bilinear_interpolation\" rel=\"noreferrer\">Bilinear interpolation.</a>\n</li> <li>\n<code>ResizeMethod.NEAREST_NEIGHBOR</code>: <a href=\"https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation\" rel=\"noreferrer\">Nearest neighbor interpolation.</a>\n</li> <li>\n<code>ResizeMethod.BICUBIC</code>: <a href=\"https://en.wikipedia.org/wiki/Bicubic_interpolation\" rel=\"noreferrer\">Bicubic interpolation.</a>\n</li> <li>\n<code>ResizeMethod.AREA</code>: Area interpolation.</li> </ul>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>images</code>: 4-D Tensor of shape <code>[batch, height, width, channels]</code> or 3-D Tensor of shape <code>[height, width, channels]</code>.</li> <li>\n<code>new_height</code>: integer.</li> <li>\n<code>new_width</code>: integer.</li> <li>\n<code>method</code>: ResizeMethod. Defaults to <code>ResizeMethod.BILINEAR</code>.</li> <li>\n<code>align_corners</code>: bool. If true, exactly align all 4 corners of the input and output. Defaults to <code>false</code>.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape of <code>images</code> is incompatible with the shape arguments to this function</li> <li>\n<code>ValueError</code>: if an unsupported resize method is specified.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>If <code>images</code> was 4-D, a 4-D float Tensor of shape <code>[batch, new_height, new_width, channels]</code>. If <code>images</code> was 3-D, a 3-D float Tensor of shape <code>[new_height, new_width, channels]</code>.</p>  <h3 id=\"resize_area\"><code>tf.image.resize_area(images, size, align_corners=None, name=None)</code></h3> <p>Resize <code>images</code> to <code>size</code> using area interpolation.</p> <p>Input images can be of different types but output images are always float.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>images</code>: A <code>Tensor</code>. Must be one of the following types: <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>half</code>, <code>float32</code>, <code>float64</code>. 4-D with shape <code>[batch, height, width, channels]</code>.</li> <li>\n<code>size</code>: A 1-D int32 Tensor of 2 elements: <code>new_height, new_width</code>. The new size for the images.</li> <li>\n<code>align_corners</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If true, rescale input by (new_height - 1) / (height - 1), which exactly aligns the 4 corners of images and resized images. If false, rescale by new_height / height. Treat similarly the width dimension.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A <code>Tensor</code> of type <code>float32</code>. 4-D with shape <code>[batch, new_height, new_width, channels]</code>.</p>  <h3 id=\"resize_bicubic\"><code>tf.image.resize_bicubic(images, size, align_corners=None, name=None)</code></h3> <p>Resize <code>images</code> to <code>size</code> using bicubic interpolation.</p> <p>Input images can be of different types but output images are always float.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>images</code>: A <code>Tensor</code>. Must be one of the following types: <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>half</code>, <code>float32</code>, <code>float64</code>. 4-D with shape <code>[batch, height, width, channels]</code>.</li> <li>\n<code>size</code>: A 1-D int32 Tensor of 2 elements: <code>new_height, new_width</code>. The new size for the images.</li> <li>\n<code>align_corners</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If true, rescale input by (new_height - 1) / (height - 1), which exactly aligns the 4 corners of images and resized images. If false, rescale by new_height / height. Treat similarly the width dimension.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A <code>Tensor</code> of type <code>float32</code>. 4-D with shape <code>[batch, new_height, new_width, channels]</code>.</p>  <h3 id=\"resize_bilinear\"><code>tf.image.resize_bilinear(images, size, align_corners=None, name=None)</code></h3> <p>Resize <code>images</code> to <code>size</code> using bilinear interpolation.</p> <p>Input images can be of different types but output images are always float.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>images</code>: A <code>Tensor</code>. Must be one of the following types: <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>half</code>, <code>float32</code>, <code>float64</code>. 4-D with shape <code>[batch, height, width, channels]</code>.</li> <li>\n<code>size</code>: A 1-D int32 Tensor of 2 elements: <code>new_height, new_width</code>. The new size for the images.</li> <li>\n<code>align_corners</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If true, rescale input by (new_height - 1) / (height - 1), which exactly aligns the 4 corners of images and resized images. If false, rescale by new_height / height. Treat similarly the width dimension.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A <code>Tensor</code> of type <code>float32</code>. 4-D with shape <code>[batch, new_height, new_width, channels]</code>.</p>  <h3 id=\"resize_nearest_neighbor\"><code>tf.image.resize_nearest_neighbor(images, size, align_corners=None, name=None)</code></h3> <p>Resize <code>images</code> to <code>size</code> using nearest neighbor interpolation.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>images</code>: A <code>Tensor</code>. Must be one of the following types: <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>half</code>, <code>float32</code>, <code>float64</code>. 4-D with shape <code>[batch, height, width, channels]</code>.</li> <li>\n<code>size</code>: A 1-D int32 Tensor of 2 elements: <code>new_height, new_width</code>. The new size for the images.</li> <li>\n<code>align_corners</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If true, rescale input by (new_height - 1) / (height - 1), which exactly aligns the 4 corners of images and resized images. If false, rescale by new_height / height. Treat similarly the width dimension.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>images</code>. 4-D with shape <code>[batch, new_height, new_width, channels]</code>.</p> <h2 id=\"cropping\">Cropping</h2>  <h3 id=\"resize_image_with_crop_or_pad\"><code>tf.image.resize_image_with_crop_or_pad(image, target_height, target_width)</code></h3> <p>Crops and/or pads an image to a target width and height.</p> <p>Resizes an image to a target width and height by either centrally cropping the image or padding it evenly with zeros.</p> <p>If <code>width</code> or <code>height</code> is greater than the specified <code>target_width</code> or <code>target_height</code> respectively, this op centrally crops along that dimension. If <code>width</code> or <code>height</code> is smaller than the specified <code>target_width</code> or <code>target_height</code> respectively, this op centrally pads with 0 along that dimension.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>image</code>: 3-D tensor of shape <code>[height, width, channels]</code>\n</li> <li>\n<code>target_height</code>: Target height.</li> <li>\n<code>target_width</code>: Target width.</li> </ul>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>target_height</code> or <code>target_width</code> are zero or negative.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>Cropped and/or padded image of shape <code>[target_height, target_width, channels]</code></p>  <h3 id=\"central_crop\"><code>tf.image.central_crop(image, central_fraction)</code></h3> <p>Crop the central region of the image.</p> <p>Remove the outer parts of an image but retain the central region of the image along each dimension. If we specify central_fraction = 0.5, this function returns the region marked with \"X\" in the below diagram.</p> <pre class=\"\"> --------\n|        |\n|  XXXX  |\n|  XXXX  |\n|        |   where \"X\" is the central 50% of the image.\n --------\n</pre>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>image</code>: 3-D float Tensor of shape [height, width, depth]</li> <li>\n<code>central_fraction</code>: float (0, 1], fraction of size to crop</li> </ul>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if central_crop_fraction is not within (0, 1].</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>3-D float Tensor</p>  <h3 id=\"pad_to_bounding_box\"><code>tf.image.pad_to_bounding_box(image, offset_height, offset_width, target_height, target_width)</code></h3> <p>Pad <code>image</code> with zeros to the specified <code>height</code> and <code>width</code>.</p> <p>Adds <code>offset_height</code> rows of zeros on top, <code>offset_width</code> columns of zeros on the left, and then pads the image on the bottom and right with zeros until it has dimensions <code>target_height</code>, <code>target_width</code>.</p> <p>This op does nothing if <code>offset_*</code> is zero and the image already has size <code>target_height</code> by <code>target_width</code>.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>image</code>: 3-D tensor with shape <code>[height, width, channels]</code>\n</li> <li>\n<code>offset_height</code>: Number of rows of zeros to add on top.</li> <li>\n<code>offset_width</code>: Number of columns of zeros to add on the left.</li> <li>\n<code>target_height</code>: Height of output image.</li> <li>\n<code>target_width</code>: Width of output image.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>3-D tensor of shape <code>[target_height, target_width, channels]</code></p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>image</code> is incompatible with the <code>offset_*</code> or <code>target_*</code> arguments, or either <code>offset_height</code> or <code>offset_width</code> is negative.</li> </ul>  <h3 id=\"crop_to_bounding_box\"><code>tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)</code></h3> <p>Crops an image to a specified bounding box.</p> <p>This op cuts a rectangular part out of <code>image</code>. The top-left corner of the returned image is at <code>offset_height, offset_width</code> in <code>image</code>, and its lower-right corner is at <code>offset_height + target_height, offset_width + target_width</code>.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>image</code>: 3-D tensor with shape <code>[height, width, channels]</code>\n</li> <li>\n<code>offset_height</code>: Vertical coordinate of the top-left corner of the result in the input.</li> <li>\n<code>offset_width</code>: Horizontal coordinate of the top-left corner of the result in the input.</li> <li>\n<code>target_height</code>: Height of the result.</li> <li>\n<code>target_width</code>: Width of the result.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>3-D tensor of image with shape <code>[target_height, target_width, channels]</code></p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>image</code> is incompatible with the <code>offset_*</code> or <code>target_*</code> arguments, or either <code>offset_height</code> or <code>offset_width</code> is negative, or either <code>target_height</code> or <code>target_width</code> is not positive.</li> </ul>  <h3 id=\"extract_glimpse\"><code>tf.image.extract_glimpse(input, size, offsets, centered=None, normalized=None, uniform_noise=None, name=None)</code></h3> <p>Extracts a glimpse from the input tensor.</p> <p>Returns a set of windows called glimpses extracted at location <code>offsets</code> from the input tensor. If the windows only partially overlaps the inputs, the non overlapping areas will be filled with random noise.</p> <p>The result is a 4-D tensor of shape <code>[batch_size, glimpse_height,\nglimpse_width, channels]</code>. The channels and batch dimensions are the same as that of the input tensor. The height and width of the output windows are specified in the <code>size</code> parameter.</p> <p>The argument <code>normalized</code> and <code>centered</code> controls how the windows are built:</p> <ul> <li>If the coordinates are normalized but not centered, 0.0 and 1.0 correspond to the minimum and maximum of each height and width dimension.</li> <li>If the coordinates are both normalized and centered, they range from -1.0 to 1.0. The coordinates (-1.0, -1.0) correspond to the upper left corner, the lower right corner is located at (1.0, 1.0) and the center is at (0, 0).</li> <li>If the coordinates are not normalized they are interpreted as numbers of pixels.</li> </ul>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>float32</code>. A 4-D float tensor of shape <code>[batch_size, height, width, channels]</code>.</li> <li>\n<code>size</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor of 2 elements containing the size of the glimpses to extract. The glimpse height must be specified first, following by the glimpse width.</li> <li>\n<code>offsets</code>: A <code>Tensor</code> of type <code>float32</code>. A 2-D integer tensor of shape <code>[batch_size, 2]</code> containing the x, y locations of the center of each window.</li> <li>\n<code>centered</code>: An optional <code>bool</code>. Defaults to <code>True</code>. indicates if the offset coordinates are centered relative to the image, in which case the (0, 0) offset is relative to the center of the input images. If false, the (0,0) offset corresponds to the upper left corner of the input images.</li> <li>\n<code>normalized</code>: An optional <code>bool</code>. Defaults to <code>True</code>. indicates if the offset coordinates are normalized.</li> <li>\n<code>uniform_noise</code>: An optional <code>bool</code>. Defaults to <code>True</code>. indicates if the noise should be generated using a uniform distribution or a gaussian distribution.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A <code>Tensor</code> of type <code>float32</code>. A tensor representing the glimpses <code>[batch_size,\n  glimpse_height, glimpse_width, channels]</code>.</p>  <h3 id=\"crop_and_resize\"><code>tf.image.crop_and_resize(image, boxes, box_ind, crop_size, method=None, extrapolation_value=None, name=None)</code></h3> <p>Extracts crops from the input image tensor and bilinearly resizes them (possibly</p> <p>with aspect ratio change) to a common output size specified by <code>crop_size</code>. This is more general than the <code>crop_to_bounding_box</code> op which extracts a fixed size slice from the input image and does not allow resizing or aspect ratio change.</p> <p>Returns a tensor with <code>crops</code> from the input <code>image</code> at positions defined at the bounding box locations in <code>boxes</code>. The cropped boxes are all resized (with bilinear interpolation) to a fixed <code>size = [crop_height, crop_width]</code>. The result is a 4-D tensor <code>[num_boxes, crop_height, crop_width, depth]</code>.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>image</code>: A <code>Tensor</code>. Must be one of the following types: <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>half</code>, <code>float32</code>, <code>float64</code>. A 4-D tensor of shape <code>[batch, image_height, image_width, depth]</code>. Both <code>image_height</code> and <code>image_width</code> need to be positive.</li> <li>\n<code>boxes</code>: A <code>Tensor</code> of type <code>float32</code>. A 2-D tensor of shape <code>[num_boxes, 4]</code>. The <code>i</code>-th row of the tensor specifies the coordinates of a box in the <code>box_ind[i]</code> image and is specified in normalized coordinates <code>[y1, x1, y2, x2]</code>. A normalized coordinate value of <code>y</code> is mapped to the image coordinate at <code>y * (image_height - 1)</code>, so as the <code>[0, 1]</code> interval of normalized image height is mapped to <code>[0, image_height - 1] in image height coordinates. We do allow y1 &gt; y2, in\nwhich case the sampled crop is an up-down flipped version of the original\nimage. The width dimension is treated similarly. Normalized coordinates\noutside the</code>[0, 1]<code>range are allowed, in which case we use</code>extrapolation_value` to extrapolate the input image values.</li> <li>\n<code>box_ind</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor of shape <code>[num_boxes]</code> with int32 values in <code>[0, batch)</code>. The value of <code>box_ind[i]</code> specifies the image that the <code>i</code>-th box refers to.</li> <li>\n<code>crop_size</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor of 2 elements, <code>size = [crop_height, crop_width]</code>. All cropped image patches are resized to this size. The aspect ratio of the image content is not preserved. Both <code>crop_height</code> and <code>crop_width</code> need to be positive.</li> <li>\n<code>method</code>: An optional <code>string</code> from: <code>\"bilinear\"</code>. Defaults to <code>\"bilinear\"</code>. A string specifying the interpolation method. Only 'bilinear' is supported for now.</li> <li>\n<code>extrapolation_value</code>: An optional <code>float</code>. Defaults to <code>0</code>. Value used for extrapolation, when applicable.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>A <code>Tensor</code> of type <code>float32</code>. A 4-D tensor of shape <code>[num_boxes, crop_height, crop_width, depth]</code>.</p>  <h2 id=\"flipping-rotating-and-transposing\">Flipping, Rotating and Transposing</h2>  <h3 id=\"flip_up_down\"><code>tf.image.flip_up_down(image)</code></h3> <p>Flip an image horizontally (upside down).</p> <p>Outputs the contents of <code>image</code> flipped along the first dimension, which is <code>height</code>.</p> <p>See also <code>reverse()</code>.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>image</code>: A 3-D tensor of shape <code>[height, width, channels].</code>\n</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A 3-D tensor of the same type and shape as <code>image</code>.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape of <code>image</code> not supported.</li> </ul>  <h3 id=\"random_flip_up_down\"><code>tf.image.random_flip_up_down(image, seed=None)</code></h3> <p>Randomly flips an image vertically (upside down).</p> <p>With a 1 in 2 chance, outputs the contents of <code>image</code> flipped along the first dimension, which is <code>height</code>. Otherwise output the image as-is.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>image</code>: A 3-D tensor of shape <code>[height, width, channels].</code>\n</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A 3-D tensor of the same type and shape as <code>image</code>.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape of <code>image</code> not supported.</li> </ul>  <h3 id=\"flip_left_right\"><code>tf.image.flip_left_right(image)</code></h3> <p>Flip an image horizontally (left to right).</p> <p>Outputs the contents of <code>image</code> flipped along the second dimension, which is <code>width</code>.</p> <p>See also <code>reverse()</code>.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>image</code>: A 3-D tensor of shape <code>[height, width, channels].</code>\n</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>A 3-D tensor of the same type and shape as <code>image</code>.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape of <code>image</code> not supported.</li> </ul>  <h3 id=\"random_flip_left_right\"><code>tf.image.random_flip_left_right(image, seed=None)</code></h3> <p>Randomly flip an image horizontally (left to right).</p> <p>With a 1 in 2 chance, outputs the contents of <code>image</code> flipped along the second dimension, which is <code>width</code>. Otherwise output the image as-is.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>image</code>: A 3-D tensor of shape <code>[height, width, channels].</code>\n</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>A 3-D tensor of the same type and shape as <code>image</code>.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape of <code>image</code> not supported.</li> </ul>  <h3 id=\"transpose_image\"><code>tf.image.transpose_image(image)</code></h3> <p>Transpose an image by swapping the first and second dimension.</p> <p>See also <code>transpose()</code>.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>image</code>: 3-D tensor of shape <code>[height, width, channels]</code>\n</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A 3-D tensor of shape <code>[width, height, channels]</code></p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape of <code>image</code> not supported.</li> </ul>  <h3 id=\"rot90\"><code>tf.image.rot90(image, k=1)</code></h3> <p>Rotate an image counter-clockwise by 90 degrees.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>image</code>: A 3-D tensor of shape <code>[height, width, channels].</code>\n</li> <li>\n<code>k</code>: Number of times the image is rotated by 90 degrees.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>A rotated 3-D tensor of the same type and shape as <code>image</code>.</p>  <h2 id=\"converting-between-colorspaces\">Converting Between Colorspaces.</h2> <p>Image ops work either on individual images or on batches of images, depending on the shape of their input Tensor.</p> <p>If 3-D, the shape is <code>[height, width, channels]</code>, and the Tensor represents one image. If 4-D, the shape is <code>[batch_size, height, width, channels]</code>, and the Tensor represents <code>batch_size</code> images.</p> <p>Currently, <code>channels</code> can usefully be 1, 2, 3, or 4. Single-channel images are grayscale, images with 3 channels are encoded as either RGB or HSV. Images with 2 or 4 channels include an alpha channel, which has to be stripped from the image before passing the image to most image processing functions (and can be re-attached later).</p> <p>Internally, images are either stored in as one <code>float32</code> per channel per pixel (implicitly, values are assumed to lie in <code>[0,1)</code>) or one <code>uint8</code> per channel per pixel (values are assumed to lie in <code>[0,255]</code>).</p> <p>TensorFlow can convert between images in RGB or HSV. The conversion functions work only on float images, so you need to convert images in other formats using <a href=\"#convert-image-dtype\"><code>convert_image_dtype</code></a>.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Decode an image and convert it to HSV.\nrgb_image = tf.image.decode_png(...,  channels=3)\nrgb_image_float = tf.image.convert_image_dtype(rgb_image, tf.float32)\nhsv_image = tf.image.rgb_to_hsv(rgb_image)\n</pre>  <h3 id=\"rgb_to_grayscale\"><code>tf.image.rgb_to_grayscale(images, name=None)</code></h3> <p>Converts one or more images from RGB to Grayscale.</p> <p>Outputs a tensor of the same <code>DType</code> and rank as <code>images</code>. The size of the last dimension of the output is 1, containing the Grayscale value of the pixels.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>images</code>: The RGB tensor to convert. Last dimension must have size 3 and should contain RGB values.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>The converted grayscale image(s).</p>  <h3 id=\"grayscale_to_rgb\"><code>tf.image.grayscale_to_rgb(images, name=None)</code></h3> <p>Converts one or more images from Grayscale to RGB.</p> <p>Outputs a tensor of the same <code>DType</code> and rank as <code>images</code>. The size of the last dimension of the output is 3, containing the RGB value of the pixels.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>images</code>: The Grayscale tensor to convert. Last dimension must be size 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>The converted grayscale image(s).</p>  <h3 id=\"hsv_to_rgb\"><code>tf.image.hsv_to_rgb(images, name=None)</code></h3> <p>Convert one or more images from HSV to RGB.</p> <p>Outputs a tensor of the same shape as the <code>images</code> tensor, containing the RGB value of the pixels. The output is only well defined if the value in <code>images</code> are in <code>[0,1]</code>.</p> <p>See <code>rgb_to_hsv</code> for a description of the HSV encoding.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>images</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>. 1-D or higher rank. HSV data to convert. Last dimension must be size 3.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>images</code>. <code>images</code> converted to RGB.</p>  <h3 id=\"rgb_to_hsv\"><code>tf.image.rgb_to_hsv(images, name=None)</code></h3> <p>Converts one or more images from RGB to HSV.</p> <p>Outputs a tensor of the same shape as the <code>images</code> tensor, containing the HSV value of the pixels. The output is only well defined if the value in <code>images</code> are in <code>[0,1]</code>.</p> <p><code>output[..., 0]</code> contains hue, <code>output[..., 1]</code> contains saturation, and <code>output[..., 2]</code> contains value. All HSV values are in <code>[0,1]</code>. A hue of 0 corresponds to pure red, hue 1/3 is pure green, and 2/3 is pure blue.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>images</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>. 1-D or higher rank. RGB data to convert. Last dimension must be size 3.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>images</code>. <code>images</code> converted to HSV.</p>  <h3 id=\"convert_image_dtype\"><code>tf.image.convert_image_dtype(image, dtype, saturate=False, name=None)</code></h3> <p>Convert <code>image</code> to <code>dtype</code>, scaling its values if needed.</p> <p>Images that are represented using floating point values are expected to have values in the range [0,1). Image data stored in integer data types are expected to have values in the range <code>[0,MAX]</code>, where <code>MAX</code> is the largest positive representable number for the data type.</p> <p>This op converts between data types, scaling the values appropriately before casting.</p> <p>Note that converting from floating point inputs to integer types may lead to over/underflow problems. Set saturate to <code>True</code> to avoid such problem in problematic conversions. If enabled, saturation will clip the output into the allowed range before performing a potentially dangerous cast (and only before performing such a cast, i.e., when casting from a floating point to an integer type, and when casting from a signed to an unsigned type; <code>saturate</code> has no effect on casts between floats, or on casts that increase the type's range).</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>image</code>: An image.</li> <li>\n<code>dtype</code>: A <code>DType</code> to convert <code>image</code> to.</li> <li>\n<code>saturate</code>: If <code>True</code>, clip the input before casting (if necessary).</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <p><code>image</code>, converted to <code>dtype</code>.</p>  <h2 id=\"image-adjustments\">Image Adjustments</h2> <p>TensorFlow provides functions to adjust images in various ways: brightness, contrast, hue, and saturation. Each adjustment can be done with predefined parameters or with random parameters picked from predefined intervals. Random adjustments are often useful to expand a training set and reduce overfitting.</p> <p>If several adjustments are chained it is advisable to minimize the number of redundant conversions by first converting the images to the most natural data type and representation (RGB or HSV).</p>  <h3 id=\"adjust_brightness\"><code>tf.image.adjust_brightness(image, delta)</code></h3> <p>Adjust the brightness of RGB or Grayscale images.</p> <p>This is a convenience method that converts an RGB image to float representation, adjusts its brightness, and then converts it back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</p> <p>The value <code>delta</code> is added to all components of the tensor <code>image</code>. Both <code>image</code> and <code>delta</code> are converted to <code>float</code> before adding (and <code>image</code> is scaled appropriately if it is in fixed-point representation). For regular images, <code>delta</code> should be in the range <code>[0,1)</code>, as it is added to the image in floating point representation, where pixel values are in the <code>[0,1)</code> range.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>image</code>: A tensor.</li> <li>\n<code>delta</code>: A scalar. Amount to add to the pixel values.</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>A brightness-adjusted tensor of the same shape and type as <code>image</code>.</p>  <h3 id=\"random_brightness\"><code>tf.image.random_brightness(image, max_delta, seed=None)</code></h3> <p>Adjust the brightness of images by a random factor.</p> <p>Equivalent to <code>adjust_brightness()</code> using a <code>delta</code> randomly picked in the interval <code>[-max_delta, max_delta)</code>.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>image</code>: An image.</li> <li>\n<code>max_delta</code>: float, must be non-negative.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>The brightness-adjusted image.</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>max_delta</code> is negative.</li> </ul>  <h3 id=\"adjust_contrast\"><code>tf.image.adjust_contrast(images, contrast_factor)</code></h3> <p>Adjust contrast of RGB or grayscale images.</p> <p>This is a convenience method that converts an RGB image to float representation, adjusts its contrast, and then converts it back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</p> <p><code>images</code> is a tensor of at least 3 dimensions. The last 3 dimensions are interpreted as <code>[height, width, channels]</code>. The other dimensions only represent a collection of images, such as <code>[batch, height, width, channels].</code></p> <p>Contrast is adjusted independently for each channel of each image.</p> <p>For each channel, this Op computes the mean of the image pixels in the channel and then adjusts each component <code>x</code> of each pixel to <code>(x - mean) * contrast_factor + mean</code>.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>images</code>: Images to adjust. At least 3-D.</li> <li>\n<code>contrast_factor</code>: A float multiplier for adjusting contrast.</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>The contrast-adjusted image or images.</p>  <h3 id=\"random_contrast\"><code>tf.image.random_contrast(image, lower, upper, seed=None)</code></h3> <p>Adjust the contrast of an image by a random factor.</p> <p>Equivalent to <code>adjust_contrast()</code> but uses a <code>contrast_factor</code> randomly picked in the interval <code>[lower, upper]</code>.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>image</code>: An image tensor with 3 or more dimensions.</li> <li>\n<code>lower</code>: float. Lower bound for the random contrast factor.</li> <li>\n<code>upper</code>: float. Upper bound for the random contrast factor.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>The contrast-adjusted tensor.</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>upper &lt;= lower</code> or if <code>lower &lt; 0</code>.</li> </ul>  <h3 id=\"adjust_hue\"><code>tf.image.adjust_hue(image, delta, name=None)</code></h3> <p>Adjust hue of an RGB image.</p> <p>This is a convenience method that converts an RGB image to float representation, converts it to HSV, add an offset to the hue channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</p> <p><code>image</code> is an RGB image. The image hue is adjusted by converting the image to HSV and rotating the hue channel (H) by <code>delta</code>. The image is then converted back to RGB.</p> <p><code>delta</code> must be in the interval <code>[-1, 1]</code>.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>image</code>: RGB image or images. Size of the last dimension must be 3.</li> <li>\n<code>delta</code>: float. How much to add to the hue channel.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>Adjusted image(s), same shape and DType as <code>image</code>.</p>  <h3 id=\"random_hue\"><code>tf.image.random_hue(image, max_delta, seed=None)</code></h3> <p>Adjust the hue of an RGB image by a random factor.</p> <p>Equivalent to <code>adjust_hue()</code> but uses a <code>delta</code> randomly picked in the interval <code>[-max_delta, max_delta]</code>.</p> <p><code>max_delta</code> must be in the interval <code>[0, 0.5]</code>.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>image</code>: RGB image or images. Size of the last dimension must be 3.</li> <li>\n<code>max_delta</code>: float. Maximum value for the random delta.</li> <li>\n<code>seed</code>: An operation-specific seed. It will be used in conjunction with the graph-level seed to determine the real seeds that will be used in this operation. Please see the documentation of set_random_seed for its interaction with the graph-level random seed.</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>3-D float tensor of shape <code>[height, width, channels]</code>.</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>max_delta</code> is invalid.</li> </ul>  <h3 id=\"adjust_saturation\"><code>tf.image.adjust_saturation(image, saturation_factor, name=None)</code></h3> <p>Adjust saturation of an RGB image.</p> <p>This is a convenience method that converts an RGB image to float representation, converts it to HSV, add an offset to the saturation channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</p> <p><code>image</code> is an RGB image. The image saturation is adjusted by converting the image to HSV and multiplying the saturation (S) channel by <code>saturation_factor</code> and clipping. The image is then converted back to RGB.</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>image</code>: RGB image or images. Size of the last dimension must be 3.</li> <li>\n<code>saturation_factor</code>: float. Factor to multiply the saturation by.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>Adjusted image(s), same shape and DType as <code>image</code>.</p>  <h3 id=\"random_saturation\"><code>tf.image.random_saturation(image, lower, upper, seed=None)</code></h3> <p>Adjust the saturation of an RGB image by a random factor.</p> <p>Equivalent to <code>adjust_saturation()</code> but uses a <code>saturation_factor</code> randomly picked in the interval <code>[lower, upper]</code>.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>image</code>: RGB image or images. Size of the last dimension must be 3.</li> <li>\n<code>lower</code>: float. Lower bound for the random saturation factor.</li> <li>\n<code>upper</code>: float. Upper bound for the random saturation factor.</li> <li>\n<code>seed</code>: An operation-specific seed. It will be used in conjunction with the graph-level seed to determine the real seeds that will be used in this operation. Please see the documentation of set_random_seed for its interaction with the graph-level random seed.</li> </ul>  <h5 id=\"returns-34\">Returns:</h5> <p>Adjusted image(s), same shape and DType as <code>image</code>.</p>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>upper &lt;= lower</code> or if <code>lower &lt; 0</code>.</li> </ul>  <h3 id=\"per_image_whitening\"><code>tf.image.per_image_whitening(image)</code></h3> <p>Linearly scales <code>image</code> to have zero mean and unit norm.</p> <p>This op computes <code>(x - mean) / adjusted_stddev</code>, where <code>mean</code> is the average of all values in image, and <code>adjusted_stddev = max(stddev, 1.0/sqrt(image.NumElements()))</code>.</p> <p><code>stddev</code> is the standard deviation of all values in <code>image</code>. It is capped away from zero to protect against division by 0 when handling uniform images.</p> <p>Note that this implementation is limited: * It only whitens based on the statistics of an individual image. * It does not take into account the covariance structure.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>image</code>: 3-D tensor of shape <code>[height, width, channels]</code>.</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>The whitened image with same shape as <code>image</code>.</p>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape of 'image' is incompatible with this function.</li> </ul>  <h2 id=\"working-with-bounding-boxes\">Working with Bounding Boxes</h2>  <h3 id=\"draw_bounding_boxes\"><code>tf.image.draw_bounding_boxes(images, boxes, name=None)</code></h3> <p>Draw bounding boxes on a batch of images.</p> <p>Outputs a copy of <code>images</code> but draws on top of the pixels zero or more bounding boxes specified by the locations in <code>boxes</code>. The coordinates of the each bounding box in <code>boxes</code> are encoded as <code>[y_min, x_min, y_max, x_max]</code>. The bounding box coordinates are floats in <code>[0.0, 1.0]</code> relative to the width and height of the underlying image.</p> <p>For example, if an image is 100 x 200 pixels and the bounding box is <code>[0.1, 0.5, 0.2, 0.9]</code>, the bottom-left and upper-right coordinates of the bounding box will be <code>(10, 40)</code> to <code>(50, 180)</code>.</p> <p>Parts of the bounding box may fall outside the image.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>images</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>half</code>. 4-D with shape <code>[batch, height, width, depth]</code>. A batch of images.</li> <li>\n<code>boxes</code>: A <code>Tensor</code> of type <code>float32</code>. 3-D with shape <code>[batch, num_bounding_boxes, 4]</code> containing bounding boxes.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>images</code>. 4-D with the same shape as <code>images</code>. The batch of input images with bounding boxes drawn on the images.</p>  <h3 id=\"non_max_suppression\"><code>tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=None, name=None)</code></h3> <p>Greedily selects a subset of bounding boxes in descending order of score,</p> <p>pruning away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm.</p> <p>The output of this operation is a set of integers indexing into the input collection of bounding boxes representing the selected boxes. The bounding box coordinates corresponding to the selected indices can then be obtained using the tf.gather operation. For example:</p> <p>selected_indices = tf.image.non_max_suppression( boxes, scores, max_output_size, iou_threshold) selected_boxes = tf.gather(boxes, selected_indices)</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>boxes</code>: A <code>Tensor</code> of type <code>float32</code>. A 2-D float tensor of shape <code>[num_boxes, 4]</code>.</li> <li>\n<code>scores</code>: A <code>Tensor</code> of type <code>float32</code>. A 1-D float tensor of shape <code>[num_boxes]</code> representing a single score corresponding to each box (each row of boxes).</li> <li>\n<code>max_output_size</code>: A <code>Tensor</code> of type <code>int32</code>. A scalar integer tensor representing the maximum number of boxes to be selected by non max suppression.</li> <li>\n<code>iou_threshold</code>: An optional <code>float</code>. Defaults to <code>0.5</code>. A float representing the threshold for deciding whether boxes overlap too much with respect to IOU.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int32</code>. A 1-D integer tensor of shape <code>[M]</code> representing the selected indices from the boxes tensor, where <code>M &lt;= max_output_size</code>.</p>  <h3 id=\"sample_distorted_bounding_box\"><code>tf.image.sample_distorted_bounding_box(image_size, bounding_boxes, seed=None, seed2=None, min_object_covered=None, aspect_ratio_range=None, area_range=None, max_attempts=None, use_image_if_no_bounding_boxes=None, name=None)</code></h3> <p>Generate a single randomly distorted bounding box for an image.</p> <p>Bounding box annotations are often supplied in addition to ground-truth labels in image recognition or object localization tasks. A common technique for training such a system is to randomly distort an image while preserving its content, i.e. <em>data augmentation</em>. This Op outputs a randomly distorted localization of an object, i.e. bounding box, given an <code>image_size</code>, <code>bounding_boxes</code> and a series of constraints.</p> <p>The output of this Op is a single bounding box that may be used to crop the original image. The output is returned as 3 tensors: <code>begin</code>, <code>size</code> and <code>bboxes</code>. The first 2 tensors can be fed directly into <code>tf.slice</code> to crop the image. The latter may be supplied to <code>tf.image.draw_bounding_box</code> to visualize what the bounding box looks like.</p> <p>Bounding boxes are supplied and returned as <code>[y_min, x_min, y_max, x_max]</code>. The bounding box coordinates are floats in <code>[0.0, 1.0]</code> relative to the width and height of the underlying image.</p> <p>For example,</p> <pre class=\"\"># Generate a single distorted bounding box.\nbegin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(\n    tf.shape(image),\n    bounding_boxes=bounding_boxes)\n\n# Draw the bounding box in an image summary.\nimage_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                              bbox_for_draw)\ntf.image_summary('images_with_box', image_with_box)\n\n# Employ the bounding box to distort the image.\ndistorted_image = tf.slice(image, begin, size)\n</pre> <p>Note that if no bounding box information is available, setting <code>use_image_if_no_bounding_boxes = true</code> will assume there is a single implicit bounding box covering the whole image. If <code>use_image_if_no_bounding_boxes</code> is false and no bounding boxes are supplied, an error is raised.</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>image_size</code>: A <code>Tensor</code>. Must be one of the following types: <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>. 1-D, containing <code>[height, width, channels]</code>.</li> <li>\n<code>bounding_boxes</code>: A <code>Tensor</code> of type <code>float32</code>. 3-D with shape <code>[batch, N, 4]</code> describing the N bounding boxes associated with the image.</li> <li>\n<code>seed</code>: An optional <code>int</code>. Defaults to <code>0</code>. If either <code>seed</code> or <code>seed2</code> are set to non-zero, the random number generator is seeded by the given <code>seed</code>. Otherwise, it is seeded by a random seed.</li> <li>\n<code>seed2</code>: An optional <code>int</code>. Defaults to <code>0</code>. A second seed to avoid seed collision.</li> <li>\n<code>min_object_covered</code>: An optional <code>float</code>. Defaults to <code>0.1</code>. The cropped area of the image must contain at least this fraction of any bounding box supplied.</li> <li>\n<code>aspect_ratio_range</code>: An optional list of <code>floats</code>. Defaults to <code>[0.75, 1.33]</code>. The cropped area of the image must have an aspect ratio = width / height within this range.</li> <li>\n<code>area_range</code>: An optional list of <code>floats</code>. Defaults to <code>[0.05, 1]</code>. The cropped area of the image must contain a fraction of the supplied image within in this range.</li> <li>\n<code>max_attempts</code>: An optional <code>int</code>. Defaults to <code>100</code>. Number of attempts at generating a cropped region of the image of the specified constraints. After <code>max_attempts</code> failures, return the entire image.</li> <li>\n<code>use_image_if_no_bounding_boxes</code>: An optional <code>bool</code>. Defaults to <code>False</code>. Controls behavior if no bounding boxes supplied. If true, assume an implicit bounding box covering the whole input. If false, raise an error.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>A tuple of <code>Tensor</code> objects (begin, size, bboxes).</p> <ul> <li>\n<code>begin</code>: A <code>Tensor</code>. Has the same type as <code>image_size</code>. 1-D, containing <code>[offset_height, offset_width, 0]</code>. Provide as input to <code>tf.slice</code>.</li> <li>\n<code>size</code>: A <code>Tensor</code>. Has the same type as <code>image_size</code>. 1-D, containing <code>[target_height, target_width, -1]</code>. Provide as input to <code>tf.slice</code>.</li> <li>\n<code>bboxes</code>: A <code>Tensor</code> of type <code>float32</code>. 3-D with shape <code>[1, 1, 4]</code> containing the distorted bounding box. Provide as input to <code>tf.image.draw_bounding_boxes</code>.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html</a>\n  </p>\n</div>\n","client":"<h1 id=\"running-graphs\">Running Graphs</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#running-graphs\">Running Graphs</a></li> <ul> <li><a href=\"#session-management\">Session management</a></li> <ul> <li><a href=\"#Session\"><code>class tf.Session</code></a></li> <li><a href=\"#InteractiveSession\"><code>class tf.InteractiveSession</code></a></li> <li><a href=\"#get_default_session\"><code>tf.get_default_session()</code></a></li> </ul> <li><a href=\"#error-classes\">Error classes</a></li> <ul> <li><a href=\"#OpError\"><code>class tf.OpError</code></a></li> <li><a href=\"#CancelledError\"><code>class tf.errors.CancelledError</code></a></li> <li><a href=\"#UnknownError\"><code>class tf.errors.UnknownError</code></a></li> <li><a href=\"#InvalidArgumentError\"><code>class tf.errors.InvalidArgumentError</code></a></li> <li><a href=\"#DeadlineExceededError\"><code>class tf.errors.DeadlineExceededError</code></a></li> <li><a href=\"#NotFoundError\"><code>class tf.errors.NotFoundError</code></a></li> <li><a href=\"#AlreadyExistsError\"><code>class tf.errors.AlreadyExistsError</code></a></li> <li><a href=\"#PermissionDeniedError\"><code>class tf.errors.PermissionDeniedError</code></a></li> <li><a href=\"#UnauthenticatedError\"><code>class tf.errors.UnauthenticatedError</code></a></li> <li><a href=\"#ResourceExhaustedError\"><code>class tf.errors.ResourceExhaustedError</code></a></li> <li><a href=\"#FailedPreconditionError\"><code>class tf.errors.FailedPreconditionError</code></a></li> <li><a href=\"#AbortedError\"><code>class tf.errors.AbortedError</code></a></li> <li><a href=\"#OutOfRangeError\"><code>class tf.errors.OutOfRangeError</code></a></li> <li><a href=\"#UnimplementedError\"><code>class tf.errors.UnimplementedError</code></a></li> <li><a href=\"#InternalError\"><code>class tf.errors.InternalError</code></a></li> <li><a href=\"#UnavailableError\"><code>class tf.errors.UnavailableError</code></a></li> <li><a href=\"#DataLossError\"><code>class tf.errors.DataLossError</code></a></li> </ul>\n</ul>\n</ul> </div> <p>This library contains classes for launching graphs and executing operations.</p> <p>The <a href=\"https://www.tensorflow.org/versions/r0.10/get_started/index.html#basic-usage\">basic usage</a> guide has examples of how a graph is launched in a <a href=\"#Session\"><code>tf.Session</code></a>.</p>  <h2 id=\"session-management\">Session management</h2>  <h3 id=\"Session\"><code>class tf.Session</code></h3> <p>A class for running TensorFlow operations.</p> <p>A <code>Session</code> object encapsulates the environment in which <code>Operation</code> objects are executed, and <code>Tensor</code> objects are evaluated. For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Build a graph.\na = tf.constant(5.0)\nb = tf.constant(6.0)\nc = a * b\n\n# Launch the graph in a session.\nsess = tf.Session()\n\n# Evaluate the tensor `c`.\nprint(sess.run(c))\n</pre> <p>A session may own resources, such as <a href=\"state_ops#Variable\">variables</a>, <a href=\"io_ops#QueueBase\">queues</a>, and <a href=\"io_ops#ReaderBase\">readers</a>. It is important to release these resources when they are no longer required. To do this, either invoke the <a href=\"#Session.close\"><code>close()</code></a> method on the session, or use the session as a context manager. The following two examples are equivalent:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Using the `close()` method.\nsess = tf.Session()\nsess.run(...)\nsess.close()\n\n# Using the context manager.\nwith tf.Session() as sess:\n  sess.run(...)\n</pre> <p>The <a href=\"https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto\" rel=\"noreferrer\"><code>ConfigProto</code></a> protocol buffer exposes various configuration options for a session. For example, to create a session that uses soft constraints for device placement, and log the resulting placement decisions, create a session as follows:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Launch the graph in a session that allows soft device placement and\n# logs the placement decisions.\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                        log_device_placement=True))\n</pre>  <h4 id=\"Session.__init__\"><code>tf.Session.__init__(target='', graph=None, config=None)</code></h4> <p>Creates a new TensorFlow session.</p> <p>If no <code>graph</code> argument is specified when constructing the session, the default graph will be launched in the session. If you are using more than one graph (created with <code>tf.Graph()</code> in the same process, you will have to use different sessions for each graph, but each graph can be used in multiple sessions. In this case, it is often clearer to pass the graph to be launched explicitly to the session constructor.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>target</code>: (Optional.) The execution engine to connect to. Defaults to using an in-process engine. See <a href=\"https://www.tensorflow.org/how_tos/distributed/index.html\" rel=\"noreferrer\">Distributed Tensorflow</a> for more examples.</li> <li>\n<code>graph</code>: (Optional.) The <code>Graph</code> to be launched (described above).</li> <li>\n<code>config</code>: (Optional.) A <a href=\"https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto\" rel=\"noreferrer\"><code>ConfigProto</code></a> protocol buffer with configuration options for the session.</li> </ul>  <h4 id=\"Session.run\"><code>tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)</code></h4> <p>Runs operations and evaluates tensors in <code>fetches</code>.</p> <p>This method runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every <code>Operation</code> and evaluate every <code>Tensor</code> in <code>fetches</code>, substituting the values in <code>feed_dict</code> for the corresponding input values.</p> <p>The <code>fetches</code> argument may be a single graph element, or an arbitrarily nested list, tuple, namedtuple, or dict containing graph elements at its leaves. A graph element can be one of the following types:</p> <ul> <li>An <a href=\"framework#Operation\"><code>Operation</code></a>. The corresponding fetched value will be <code>None</code>.</li> <li>A <a href=\"framework#Tensor\"><code>Tensor</code></a>. The corresponding fetched value will be a numpy ndarray containing the value of that tensor.</li> <li>A <a href=\"sparse_ops#SparseTensor\"><code>SparseTensor</code></a>. The corresponding fetched value will be a <a href=\"sparse_ops#SparseTensorValue\"><code>SparseTensorValue</code></a> containing the value of that sparse tensor.</li> <li>A <code>get_tensor_handle</code> op. The corresponding fetched value will be a numpy ndarray containing the handle of that tensor.</li> <li>A <code>string</code> which is the name of a tensor or operation in the graph.</li> </ul> <p>The value returned by <code>run()</code> has the same shape as the <code>fetches</code> argument, where the leaves are replaced by the corresponding values returned by TensorFlow.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">a = tf.constant([10, 20])\nb = tf.constant([1.0, 2.0])\n# 'fetches' can be a singleton\nv = session.run(a)\n# v is the numpy array [10, 20]\n# 'fetches' can be a list.\nv = session.run([a, b])\n# v a Python list with 2 numpy arrays: the numpy array [10, 20] and the\n# 1-D array [1.0, 2.0]\n# 'fetches' can be arbitrary lists, tuples, namedtuple, dicts:\nMyData = collections.namedtuple('MyData', ['a', 'b'])\nv = session.run({'k1': MyData(a, b), 'k2': [b, a]})\n# v is a dict with\n# v['k1'] is a MyData namedtuple with 'a' the numpy array [10, 20] and\n# 'b' the numpy array [1.0, 2.0]\n# v['k2'] is a list with the numpy array [1.0, 2.0] and the numpy array\n# [10, 20].\n</pre> <p>The optional <code>feed_dict</code> argument allows the caller to override the value of tensors in the graph. Each key in <code>feed_dict</code> can be one of the following types:</p> <ul> <li>If the key is a <a href=\"framework#Tensor\"><code>Tensor</code></a>, the value may be a Python scalar, string, list, or numpy ndarray that can be converted to the same <code>dtype</code> as that tensor. Additionally, if the key is a <a href=\"io_ops#placeholder\">placeholder</a>, the shape of the value will be checked for compatibility with the placeholder.</li> <li>If the key is a <a href=\"sparse_ops#SparseTensor\"><code>SparseTensor</code></a>, the value should be a <a href=\"sparse_ops#SparseTensorValue\"><code>SparseTensorValue</code></a>.</li> <li>If the key is a nested tuple of <code>Tensor</code>s or <code>SparseTensor</code>s, the value should be a nested tuple with the same structure that maps to their corresponding values as above.</li> </ul> <p>Each value in <code>feed_dict</code> must be convertible to a numpy array of the dtype of the corresponding key.</p> <p>The optional <code>options</code> argument expects a [<code>RunOptions</code>] proto. The options allow controlling the behavior of this particular step (e.g. turning tracing on).</p> <p>The optional <code>run_metadata</code> argument expects a [<code>RunMetadata</code>] proto. When appropriate, the non-Tensor output of this step will be collected there. For example, when users turn on tracing in <code>options</code>, the profiled info will be collected into this argument and passed back.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>fetches</code>: A single graph element, a list of graph elements, or a dictionary whose values are graph elements or lists of graph elements (described above).</li> <li>\n<code>feed_dict</code>: A dictionary that maps graph elements to values (described above).</li> <li>\n<code>options</code>: A [<code>RunOptions</code>] protocol buffer</li> <li>\n<code>run_metadata</code>: A [<code>RunMetadata</code>] protocol buffer</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>Either a single value if <code>fetches</code> is a single graph element, or a list of values if <code>fetches</code> is a list, or a dictionary with the same keys as <code>fetches</code> if that is a dictionary (described above).</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: If this <code>Session</code> is in an invalid state (e.g. has been closed).</li> <li>\n<code>TypeError</code>: If <code>fetches</code> or <code>feed_dict</code> keys are of an inappropriate type.</li> <li>\n<code>ValueError</code>: If <code>fetches</code> or <code>feed_dict</code> keys are invalid or refer to a <code>Tensor</code> that doesn't exist.</li> </ul>  <h4 id=\"Session.close\"><code>tf.Session.close()</code></h4> <p>Closes this session.</p> <p>Calling this method frees all resources associated with the session.</p>  <h5 id=\"raises-2\">Raises:</h5> <p>tf.errors.OpError: Or one of its subclasses if an error occurs while closing the TensorFlow session.</p>  <h4 id=\"Session.graph\"><code>tf.Session.graph</code></h4> <p>The graph that was launched in this session.</p>  <h4 id=\"Session.as_default\"><code>tf.Session.as_default()</code></h4> <p>Returns a context manager that makes this object the default session.</p> <p>Use with the <code>with</code> keyword to specify that calls to <a href=\"framework#Operation.run\"><code>Operation.run()</code></a> or <a href=\"framework#Tensor.eval\"><code>Tensor.eval()</code></a> should be executed in this session.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">c = tf.constant(..)\nsess = tf.Session()\n\nwith sess.as_default():\n  assert tf.get_default_session() is sess\n  print(c.eval())\n</pre> <p>To get the current default session, use <a href=\"#get_default_session\"><code>tf.get_default_session()</code></a>.</p> <p><em>N.B.</em> The <code>as_default</code> context manager <em>does not</em> close the session when you exit the context, and you must close the session explicitly.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">c = tf.constant(...)\nsess = tf.Session()\nwith sess.as_default():\n  print(c.eval())\n# ...\nwith sess.as_default():\n  print(c.eval())\n\nsess.close()\n</pre> <p>Alternatively, you can use <code>with tf.Session():</code> to create a session that is automatically closed on exiting the context, including when an uncaught exception is raised.</p> <p><em>N.B.</em> The default graph is a property of the current thread. If you create a new thread, and wish to use the default session in that thread, you must explicitly add a <code>with sess.as_default():</code> in that thread's function.</p>  <h5 id=\"returns-2\">Returns:</h5> <p>A context manager using this session as the default session.</p>  <h4 id=\"Session.reset\"><code>tf.Session.reset(target, containers=None, config=None)</code></h4> <p>Resets resource containers on <code>target</code>, and close all connected sessions.</p> <p>A resource container is distributed across all workers in the same cluster as <code>target</code>. When a resource container on <code>target</code> is reset, resources associated with that container will be cleared. In particular, all Variables in the container will become undefined: they lose their values and shapes.</p> <p>NOTE: (i) reset() is currently only implemented for distributed sessions. (ii) Any sessions on the master named by <code>target</code> will be closed.</p> <p>If no resource containers are provided, all containers are reset.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>target</code>: The execution engine to connect to.</li> <li>\n<code>containers</code>: A list of resource container name strings, or <code>None</code> if all of all the containers are to be reset.</li> <li>\n<code>config</code>: (Optional.) Protocol buffer with configuration options.</li> </ul>  <h5 id=\"raises-3\">Raises:</h5> <p>tf.errors.OpError: Or one of its subclasses if an error occurs while resetting containers.</p>  <h3 id=\"InteractiveSession\"><code>class tf.InteractiveSession</code></h3> <p>A TensorFlow <code>Session</code> for use in interactive contexts, such as a shell.</p> <p>The only difference with a regular <code>Session</code> is that an <code>InteractiveSession</code> installs itself as the default session on construction. The methods <a href=\"framework#Tensor.eval\"><code>Tensor.eval()</code></a> and <a href=\"framework#Operation.run\"><code>Operation.run()</code></a> will use that session to run ops.</p> <p>This is convenient in interactive shells and <a href=\"http://ipython.org\" rel=\"noreferrer\">IPython notebooks</a>, as it avoids having to pass an explicit <code>Session</code> object to run ops.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">sess = tf.InteractiveSession()\na = tf.constant(5.0)\nb = tf.constant(6.0)\nc = a * b\n# We can just use 'c.eval()' without passing 'sess'\nprint(c.eval())\nsess.close()\n</pre> <p>Note that a regular session installs itself as the default session when it is created in a <code>with</code> statement. The common usage in non-interactive programs is to follow that pattern:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">a = tf.constant(5.0)\nb = tf.constant(6.0)\nc = a * b\nwith tf.Session():\n  # We can also use 'c.eval()' here.\n  print(c.eval())\n</pre>  <h4 id=\"InteractiveSession.__init__\"><code>tf.InteractiveSession.__init__(target='', graph=None, config=None)</code></h4> <p>Creates a new interactive TensorFlow session.</p> <p>If no <code>graph</code> argument is specified when constructing the session, the default graph will be launched in the session. If you are using more than one graph (created with <code>tf.Graph()</code> in the same process, you will have to use different sessions for each graph, but each graph can be used in multiple sessions. In this case, it is often clearer to pass the graph to be launched explicitly to the session constructor.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>target</code>: (Optional.) The execution engine to connect to. Defaults to using an in-process engine.</li> <li>\n<code>graph</code>: (Optional.) The <code>Graph</code> to be launched (described above).</li> <li>\n<code>config</code>: (Optional) <code>ConfigProto</code> proto used to configure the session.</li> </ul>  <h4 id=\"InteractiveSession.close\"><code>tf.InteractiveSession.close()</code></h4> <p>Closes an <code>InteractiveSession</code>.</p>  <h3 id=\"get_default_session\"><code>tf.get_default_session()</code></h3> <p>Returns the default session for the current thread.</p> <p>The returned <code>Session</code> will be the innermost session on which a <code>Session</code> or <code>Session.as_default()</code> context has been entered.</p> <p>NOTE: The default session is a property of the current thread. If you create a new thread, and wish to use the default session in that thread, you must explicitly add a <code>with sess.as_default():</code> in that thread's function.</p>  <h5 id=\"returns-3\">Returns:</h5> <p>The default <code>Session</code> being used in the current thread.</p>  <h2 id=\"error-classes\">Error classes</h2>  <h3 id=\"OpError\"><code>class tf.OpError</code></h3> <p>A generic error that is raised when TensorFlow execution fails.</p> <p>Whenever possible, the session will raise a more specific subclass of <code>OpError</code> from the <code>tf.errors</code> module.</p>  <h4 id=\"OpError.op\"><code>tf.OpError.op</code></h4> <p>The operation that failed, if known.</p> <p><em>N.B.</em> If the failed op was synthesized at runtime, e.g. a <code>Send</code> or <code>Recv</code> op, there will be no corresponding <a href=\"framework#Operation\"><code>Operation</code></a> object. In that case, this will return <code>None</code>, and you should instead use the <a href=\"#OpError.node_def\"><code>OpError.node_def</code></a> to discover information about the op.</p>  <h5 id=\"returns-4\">Returns:</h5> <p>The <code>Operation</code> that failed, or None.</p>  <h4 id=\"OpError.node_def\"><code>tf.OpError.node_def</code></h4> <p>The <code>NodeDef</code> proto representing the op that failed.</p>  <h4 id=\"other-methods\">Other Methods</h4>  <h4 id=\"OpError.__init__\"><code>tf.OpError.__init__(node_def, op, message, error_code)</code></h4> <p>Creates a new <code>OpError</code> indicating that a particular op failed.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>node_def</code>: The <code>graph_pb2.NodeDef</code> proto representing the op that failed, if known; otherwise None.</li> <li>\n<code>op</code>: The <code>ops.Operation</code> that failed, if known; otherwise None.</li> <li>\n<code>message</code>: The message string describing the failure.</li> <li>\n<code>error_code</code>: The <code>error_codes_pb2.Code</code> describing the error.</li> </ul>  <h4 id=\"OpError.error_code\"><code>tf.OpError.error_code</code></h4> <p>The integer error code that describes the error.</p>  <h4 id=\"OpError.message\"><code>tf.OpError.message</code></h4> <p>The error message that describes the error.</p>  <h3 id=\"CancelledError\"><code>class tf.errors.CancelledError</code></h3> <p>Raised when an operation or step is cancelled.</p> <p>For example, a long-running operation (e.g. <a href=\"io_ops#QueueBase.enqueue\"><code>queue.enqueue()</code></a> may be cancelled by running another operation (e.g. <a href=\"io_ops#QueueBase.close\"><code>queue.close(cancel_pending_enqueues=True)</code></a>, or by <a href=\"client#Session.close\">closing the session</a>. A step that is running such a long-running operation will fail by raising <code>CancelledError</code>.</p>  <h4 id=\"CancelledError.__init__\"><code>tf.errors.CancelledError.__init__(node_def, op, message)</code></h4> <p>Creates a <code>CancelledError</code>.</p>  <h3 id=\"UnknownError\"><code>class tf.errors.UnknownError</code></h3> <p>Unknown error.</p> <p>An example of where this error may be returned is if a Status value received from another address space belongs to an error-space that is not known to this address space. Also errors raised by APIs that do not return enough error information may be converted to this error.</p>  <h4 id=\"UnknownError.__init__\"><code>tf.errors.UnknownError.__init__(node_def, op, message, error_code=2)</code></h4> <p>Creates an <code>UnknownError</code>.</p>  <h3 id=\"InvalidArgumentError\"><code>class tf.errors.InvalidArgumentError</code></h3> <p>Raised when an operation receives an invalid argument.</p> <p>This may occur, for example, if an operation is receives an input tensor that has an invalid value or shape. For example, the <a href=\"math_ops#matmul\"><code>tf.matmul()</code></a> op will raise this error if it receives an input that is not a matrix, and the <a href=\"array_ops#reshape\"><code>tf.reshape()</code></a> op will raise this error if the new shape does not match the number of elements in the input tensor.</p>  <h4 id=\"InvalidArgumentError.__init__\"><code>tf.errors.InvalidArgumentError.__init__(node_def, op, message)</code></h4> <p>Creates an <code>InvalidArgumentError</code>.</p>  <h3 id=\"DeadlineExceededError\"><code>class tf.errors.DeadlineExceededError</code></h3> <p>Raised when a deadline expires before an operation could complete.</p> <p>This exception is not currently used.</p>  <h4 id=\"DeadlineExceededError.__init__\"><code>tf.errors.DeadlineExceededError.__init__(node_def, op, message)</code></h4> <p>Creates a <code>DeadlineExceededError</code>.</p>  <h3 id=\"NotFoundError\"><code>class tf.errors.NotFoundError</code></h3> <p>Raised when a requested entity (e.g., a file or directory) was not found.</p> <p>For example, running the <a href=\"io_ops#WholeFileReader\"><code>tf.WholeFileReader.read()</code></a> operation could raise <code>NotFoundError</code> if it receives the name of a file that does not exist.</p>  <h4 id=\"NotFoundError.__init__\"><code>tf.errors.NotFoundError.__init__(node_def, op, message)</code></h4> <p>Creates a <code>NotFoundError</code>.</p>  <h3 id=\"AlreadyExistsError\"><code>class tf.errors.AlreadyExistsError</code></h3> <p>Raised when an entity that we attempted to create already exists.</p> <p>For example, running an operation that saves a file (e.g. <a href=\"train#Saver.save\"><code>tf.train.Saver.save()</code></a>) could potentially raise this exception if an explicit filename for an existing file was passed.</p>  <h4 id=\"AlreadyExistsError.__init__\"><code>tf.errors.AlreadyExistsError.__init__(node_def, op, message)</code></h4> <p>Creates an <code>AlreadyExistsError</code>.</p>  <h3 id=\"PermissionDeniedError\"><code>class tf.errors.PermissionDeniedError</code></h3> <p>Raised when the caller does not have permission to run an operation.</p> <p>For example, running the <a href=\"io_ops#WholeFileReader\"><code>tf.WholeFileReader.read()</code></a> operation could raise <code>PermissionDeniedError</code> if it receives the name of a file for which the user does not have the read file permission.</p>  <h4 id=\"PermissionDeniedError.__init__\"><code>tf.errors.PermissionDeniedError.__init__(node_def, op, message)</code></h4> <p>Creates a <code>PermissionDeniedError</code>.</p>  <h3 id=\"UnauthenticatedError\"><code>class tf.errors.UnauthenticatedError</code></h3> <p>The request does not have valid authentication credentials.</p> <p>This exception is not currently used.</p>  <h4 id=\"UnauthenticatedError.__init__\"><code>tf.errors.UnauthenticatedError.__init__(node_def, op, message)</code></h4> <p>Creates an <code>UnauthenticatedError</code>.</p>  <h3 id=\"ResourceExhaustedError\"><code>class tf.errors.ResourceExhaustedError</code></h3> <p>Some resource has been exhausted.</p> <p>For example, this error might be raised if a per-user quota is exhausted, or perhaps the entire file system is out of space.</p>  <h4 id=\"ResourceExhaustedError.__init__\"><code>tf.errors.ResourceExhaustedError.__init__(node_def, op, message)</code></h4> <p>Creates a <code>ResourceExhaustedError</code>.</p>  <h3 id=\"FailedPreconditionError\"><code>class tf.errors.FailedPreconditionError</code></h3> <p>Operation was rejected because the system is not in a state to execute it.</p> <p>This exception is most commonly raised when running an operation that reads a <a href=\"state_ops#Variable\"><code>tf.Variable</code></a> before it has been initialized.</p>  <h4 id=\"FailedPreconditionError.__init__\"><code>tf.errors.FailedPreconditionError.__init__(node_def, op, message)</code></h4> <p>Creates a <code>FailedPreconditionError</code>.</p>  <h3 id=\"AbortedError\"><code>class tf.errors.AbortedError</code></h3> <p>The operation was aborted, typically due to a concurrent action.</p> <p>For example, running a <a href=\"io_ops#QueueBase.enqueue\"><code>queue.enqueue()</code></a> operation may raise <code>AbortedError</code> if a <a href=\"io_ops#QueueBase.close\"><code>queue.close()</code></a> operation previously ran.</p>  <h4 id=\"AbortedError.__init__\"><code>tf.errors.AbortedError.__init__(node_def, op, message)</code></h4> <p>Creates an <code>AbortedError</code>.</p>  <h3 id=\"OutOfRangeError\"><code>class tf.errors.OutOfRangeError</code></h3> <p>Raised when an operation iterates past the valid input range.</p> <p>This exception is raised in \"end-of-file\" conditions, such as when a <a href=\"io_ops#QueueBase.dequeue\"><code>queue.dequeue()</code></a> operation is blocked on an empty queue, and a <a href=\"io_ops#QueueBase.close\"><code>queue.close()</code></a> operation executes.</p>  <h4 id=\"OutOfRangeError.__init__\"><code>tf.errors.OutOfRangeError.__init__(node_def, op, message)</code></h4> <p>Creates an <code>OutOfRangeError</code>.</p>  <h3 id=\"UnimplementedError\"><code>class tf.errors.UnimplementedError</code></h3> <p>Raised when an operation has not been implemented.</p> <p>Some operations may raise this error when passed otherwise-valid arguments that it does not currently support. For example, running the <a href=\"nn#max_pool\"><code>tf.nn.max_pool()</code></a> operation would raise this error if pooling was requested on the batch dimension, because this is not yet supported.</p>  <h4 id=\"UnimplementedError.__init__\"><code>tf.errors.UnimplementedError.__init__(node_def, op, message)</code></h4> <p>Creates an <code>UnimplementedError</code>.</p>  <h3 id=\"InternalError\"><code>class tf.errors.InternalError</code></h3> <p>Raised when the system experiences an internal error.</p> <p>This exception is raised when some invariant expected by the runtime has been broken. Catching this exception is not recommended.</p>  <h4 id=\"InternalError.__init__\"><code>tf.errors.InternalError.__init__(node_def, op, message)</code></h4> <p>Creates an <code>InternalError</code>.</p>  <h3 id=\"UnavailableError\"><code>class tf.errors.UnavailableError</code></h3> <p>Raised when the runtime is currently unavailable.</p> <p>This exception is not currently used.</p>  <h4 id=\"UnavailableError.__init__\"><code>tf.errors.UnavailableError.__init__(node_def, op, message)</code></h4> <p>Creates an <code>UnavailableError</code>.</p>  <h3 id=\"DataLossError\"><code>class tf.errors.DataLossError</code></h3> <p>Raised when unrecoverable data loss or corruption is encountered.</p> <p>For example, this may be raised by running a <a href=\"io_ops#WholeFileReader\"><code>tf.WholeFileReader.read()</code></a> operation, if the file is truncated while it is being read.</p>  <h4 id=\"DataLossError.__init__\"><code>tf.errors.DataLossError.__init__(node_def, op, message)</code></h4> <p>Creates a <code>DataLossError</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/client.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/client.html</a>\n  </p>\n</div>\n","session_ops":"<h1 id=\"tensor-handle-operations\">Tensor Handle Operations</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#tensor-handle-operations\">Tensor Handle Operations</a></li> <ul> <li><a href=\"#tensor-handle-operations\">Tensor Handle Operations.</a></li> <ul> <li><a href=\"#get_session_handle\"><code>tf.get_session_handle(data, name=None)</code></a></li> <li><a href=\"#get_session_tensor\"><code>tf.get_session_tensor(handle, dtype, name=None)</code></a></li> <li><a href=\"#delete_session_tensor\"><code>tf.delete_session_tensor(handle, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"tensor-handle-operations\">Tensor Handle Operations.</h2> <p>TensorFlow provides several operators that allows the user to keep tensors \"in-place\" across run calls.</p>  <h3 id=\"get_session_handle\"><code>tf.get_session_handle(data, name=None)</code></h3> <p>Return the handle of <code>data</code>.</p> <p>This is EXPERIMENTAL and subject to change.</p> <p>Keep <code>data</code> \"in-place\" in the runtime and create a handle that can be used to retrieve <code>data</code> in a subsequent run().</p> <p>Combined with <code>get_session_tensor</code>, we can keep a tensor produced in one run call in place, and use it as the input in a future run call.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>data</code>: A tensor to be stored in the session.</li> <li>\n<code>name</code>: Optional name prefix for the return tensor.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A scalar string tensor representing a unique handle for <code>data</code>.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li><p><code>TypeError</code>: if <code>data</code> is not a Tensor.</p></li> <li><p><code>Example</code>: </p></li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">c = tf.mul(a, b)\nh = tf.get_session_handle(c)\nh = sess.run(h)\n\np, a = tf.get_session_tensor(h.handle, tf.float32)\nb = tf.mul(a, 10)\nc = sess.run(b, feed_dict={p: h.handle})\n</pre>  <h3 id=\"get_session_tensor\"><code>tf.get_session_tensor(handle, dtype, name=None)</code></h3> <p>Get the tensor of type <code>dtype</code> by feeding a tensor handle.</p> <p>This is EXPERIMENTAL and subject to change.</p> <p>Get the value of the tensor from a tensor handle. The tensor is produced in a previous run() and stored in the state of the session.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>handle</code>: The string representation of a persistent tensor handle.</li> <li>\n<code>dtype</code>: The type of the output tensor.</li> <li>\n<code>name</code>: Optional name prefix for the return tensor.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A pair of tensors. The first is a placeholder for feeding a tensor handle and the second is the tensor in the session state keyed by the tensor handle.</p> <ul> <li>\n<code>Example</code>: </li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">c = tf.mul(a, b)\nh = tf.get_session_handle(c)\nh = sess.run(h)\n\np, a = tf.get_session_tensor(h.handle, tf.float32)\nb = tf.mul(a, 10)\nc = sess.run(b, feed_dict={p: h.handle})\n</pre>  <h3 id=\"delete_session_tensor\"><code>tf.delete_session_tensor(handle, name=None)</code></h3> <p>Delete the tensor for the given tensor handle.</p> <p>This is EXPERIMENTAL and subject to change.</p> <p>Delete the tensor of a given tensor handle. The tensor is produced in a previous run() and stored in the state of the session.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>handle</code>: The string representation of a persistent tensor handle.</li> <li>\n<code>name</code>: Optional name prefix for the return tensor.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A pair of graph elements. The first is a placeholder for feeding a tensor handle and the second is a deletion operation.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/session_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/session_ops.html</a>\n  </p>\n</div>\n","python_io":"<h1 id=\"data-io-python-functions\">Data IO (Python functions)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#data-io-python-functions\">Data IO (Python functions)</a></li> <ul> <li><a href=\"#data-io-python-functions\">Data IO (Python Functions)</a></li> <ul> <li><a href=\"#TFRecordWriter\"><code>class tf.python_io.TFRecordWriter</code></a></li> <li><a href=\"#tf_record_iterator\"><code>tf.python_io.tf_record_iterator(path, options=None)</code></a></li> <li><a href=\"#tfrecords-format-details\">TFRecords Format Details</a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"data-io-python-functions\">Data IO (Python Functions)</h2> <p>A TFRecords file represents a sequence of (binary) strings. The format is not random access, so it is suitable for streaming large amounts of data but not suitable if fast sharding or other non-sequential access is desired.</p>  <h3 id=\"TFRecordWriter\"><code>class tf.python_io.TFRecordWriter</code></h3> <p>A class to write records to a TFRecords file.</p> <p>This class implements <code>__enter__</code> and <code>__exit__</code>, and can be used in <code>with</code> blocks like a normal file.</p>  <h4 id=\"TFRecordWriter.__init__\"><code>tf.python_io.TFRecordWriter.__init__(path, options=None)</code></h4> <p>Opens file <code>path</code> and creates a <code>TFRecordWriter</code> writing to it.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>path</code>: The path to the TFRecords file.</li> <li>\n<code>options</code>: (optional) A TFRecordOptions object.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>IOError</code>: If <code>path</code> cannot be opened for writing.</li> </ul>  <h4 id=\"TFRecordWriter.write\"><code>tf.python_io.TFRecordWriter.write(record)</code></h4> <p>Write a string record to the file.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>record</code>: str</li> </ul>  <h4 id=\"TFRecordWriter.close\"><code>tf.python_io.TFRecordWriter.close()</code></h4> <p>Close the file.</p>  <h3 id=\"tf_record_iterator\"><code>tf.python_io.tf_record_iterator(path, options=None)</code></h3> <p>An iterator that read the records from a TFRecords file.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>path</code>: The path to the TFRecords file.</li> <li>\n<code>options</code>: (optional) A TFRecordOptions object.</li> </ul> <h5 id=\"yields\">Yields:</h5> <p>Strings.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>IOError</code>: If <code>path</code> cannot be opened for reading.</li> </ul>   <h3 id=\"tfrecords-format-details\">TFRecords Format Details</h3> <p>A TFRecords file contains a sequence of strings with CRC hashes. Each record has the format</p> <pre class=\"\">uint64 length\nuint32 masked_crc32_of_length\nbyte   data[length]\nuint32 masked_crc32_of_data\n</pre> <p>and the records are concatenated together to produce the file. The CRC32s are <a href=\"https://en.wikipedia.org/wiki/Cyclic_redundancy_check\" rel=\"noreferrer\">described here</a>, and the mask of a CRC is</p> <pre class=\"\">masked_crc = ((crc &gt;&gt; 15) | (crc &lt;&lt; 17)) + 0xa282ead8ul\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/python_io.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/python_io.html</a>\n  </p>\n</div>\n","tensor_array_ops":"<h1 id=\"tensorarray-operations\">TensorArray Operations</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#tensorarray-operations\">TensorArray Operations</a></li> <ul> <li><a href=\"#classes-containing-dynamically-sized-arrays-of-tensors\">Classes containing dynamically sized arrays of Tensors.</a></li> <ul> <li><a href=\"#TensorArray\"><code>class tf.TensorArray</code></a></li> </ul>\n</ul>\n</ul> </div> <p>TensorArray operations.</p>  <h2 id=\"classes-containing-dynamically-sized-arrays-of-tensors\">Classes containing dynamically sized arrays of Tensors.</h2>  <h3 id=\"TensorArray\"><code>class tf.TensorArray</code></h3> <p>Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p> <p>This class is meant to be used with dynamic iteration primitives such as <code>while_loop</code> and <code>map_fn</code>. It supports gradient back-propagation via special \"flow\" control flow dependencies.</p>  <h4 id=\"TensorArray.handle\"><code>tf.TensorArray.handle</code></h4> <p>The reference to the TensorArray.</p>  <h4 id=\"TensorArray.flow\"><code>tf.TensorArray.flow</code></h4> <p>The flow <code>Tensor</code> forcing ops leading to this TensorArray state.</p>  <h4 id=\"TensorArray.read\"><code>tf.TensorArray.read(index, name=None)</code></h4> <p>Read the value at location <code>index</code> in the TensorArray.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>index</code>: 0-D. int32 tensor with the index to read from.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>The tensor at index <code>index</code>.</p>  <h4 id=\"TensorArray.unpack\"><code>tf.TensorArray.unpack(value, name=None)</code></h4> <p>Pack the values of a <code>Tensor</code> in the TensorArray.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>value</code>: (N+1)-D. Tensor of type <code>dtype</code>. The Tensor to unpack.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A new TensorArray object with flow that ensures the unpack occurs. Use this object all for subsequent operations.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape inference fails.</li> </ul>  <h4 id=\"TensorArray.split\"><code>tf.TensorArray.split(value, lengths, name=None)</code></h4> <p>Split the values of a <code>Tensor</code> into the TensorArray.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>value</code>: (N+1)-D. Tensor of type <code>dtype</code>. The Tensor to split.</li> <li>\n<code>lengths</code>: 1-D. int32 vector with the lengths to use when splitting <code>value</code> along its first dimension.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A new TensorArray object with flow that ensures the split occurs. Use this object all for subsequent operations.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the shape inference fails.</li> </ul>  <h4 id=\"TensorArray.write\"><code>tf.TensorArray.write(index, value, name=None)</code></h4> <p>Write <code>value</code> into index <code>index</code> of the TensorArray.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>index</code>: 0-D. int32 scalar with the index to write to.</li> <li>\n<code>value</code>: N-D. Tensor of type <code>dtype</code>. The Tensor to write to this index.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A new TensorArray object with flow that ensures the write occurs. Use this object all for subsequent operations.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if there are more writers than specified.</li> </ul>  <h4 id=\"TensorArray.pack\"><code>tf.TensorArray.pack(name=None)</code></h4> <p>Return the values in the TensorArray as a packed <code>Tensor</code>.</p> <p>All of the values must have been written and their shapes must all match.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>All the tensors in the TensorArray packed into one tensor.</p>  <h4 id=\"TensorArray.concat\"><code>tf.TensorArray.concat(name=None)</code></h4> <p>Return the values in the TensorArray as a concatenated <code>Tensor</code>.</p> <p>All of the values must have been written, their ranks must match, and and their shapes must all match for all dimensions except the first.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>All the tensors in the TensorArray concatenated into one tensor.</p>  <h4 id=\"TensorArray.grad\"><code>tf.TensorArray.grad(source, flow=None, name=None)</code></h4>  <h4 id=\"other-methods\">Other Methods</h4>  <h4 id=\"TensorArray.__init__\"><code>tf.TensorArray.__init__(dtype, size=None, dynamic_size=None, clear_after_read=None, tensor_array_name=None, handle=None, flow=None, infer_shape=True, name=None)</code></h4> <p>Construct a new TensorArray or wrap an existing TensorArray handle.</p> <p>A note about the parameter <code>name</code>:</p> <p>The name of the <code>TensorArray</code> (even if passed in) is uniquified: each time a new <code>TensorArray</code> is created at runtime it is assigned its own name for the duration of the run. This avoids name collissions if a <code>TensorArray</code> is created within a <code>while_loop</code>.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>dtype</code>: (required) data type of the TensorArray.</li> <li>\n<code>size</code>: (optional) int32 scalar <code>Tensor</code>: the size of the TensorArray. Required if handle is not provided.</li> <li>\n<code>dynamic_size</code>: (optional) Python bool: If true, writes to the TensorArray can grow the TensorArray past its initial size. Default: False.</li> <li>\n<code>clear_after_read</code>: Boolean (optional, default: True). If True, clear TensorArray values after reading them. This disables read-many semantics, but allows early release of memory.</li> <li>\n<code>tensor_array_name</code>: (optional) Python string: the name of the TensorArray. This is used when creating the TensorArray handle. If this value is set, handle should be None.</li> <li>\n<code>handle</code>: (optional) A <code>Tensor</code> handle to an existing TensorArray. If this is set, tensor_array_name should be None.</li> <li>\n<code>flow</code>: (optional) A float <code>Tensor</code> scalar coming from an existing <code>TensorArray.flow</code>.</li> <li>\n<code>infer_shape</code>: (optional, default: True) If True, shape inference is enabled. In this case, all elements must have the same shape.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if both handle and tensor_array_name are provided.</li> <li>\n<code>TypeError</code>: if handle is provided but is not a Tensor.</li> </ul>  <h4 id=\"TensorArray.close\"><code>tf.TensorArray.close(name=None)</code></h4> <p>Close the current TensorArray.</p>  <h4 id=\"TensorArray.dtype\"><code>tf.TensorArray.dtype</code></h4> <p>The data type of this TensorArray.</p>  <h4 id=\"TensorArray.size\"><code>tf.TensorArray.size(name=None)</code></h4> <p>Return the size of the TensorArray.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/tensor_array_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/tensor_array_ops.html</a>\n  </p>\n</div>\n","functional_ops":"<h1 id=\"higher-order-functions\">Higher Order Functions</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#higher-order-functions\">Higher Order Functions</a></li> <ul> <li><a href=\"#higher-order-operators\">Higher Order Operators</a></li> <ul> <li><a href=\"#map_fn\"><code>tf.map_fn(fn, elems, dtype=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None)</code></a></li> <li><a href=\"#foldl\"><code>tf.foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, name=None)</code></a></li> <li><a href=\"#foldr\"><code>tf.foldr(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, name=None)</code></a></li> <li><a href=\"#scan\"><code>tf.scan(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Functional operations.</p>  <h2 id=\"higher-order-operators\">Higher Order Operators</h2> <p>TensorFlow provides several higher order operators to simplify the common map-reduce programming patterns.</p>  <h3 id=\"map_fn\"><code>tf.map_fn(fn, elems, dtype=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None)</code></h3> <p>map on the list of tensors unpacked from <code>elems</code> on dimension 0.</p> <p>The simplest version of <code>map</code> repeatedly applies the callable <code>fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code>elems</code>. <code>dtype</code> is the data type of the return value of <code>fn</code>. Users must provide <code>dtype</code> if it is different from the data type of <code>elems</code>.</p> <p>Suppose that <code>elems</code> is unpacked into <code>values</code>, a list of tensors. The shape of the result tensor is <code>[values.shape[0]] + fn(values[0]).shape</code>.</p> <p>This method also allows multi-arity <code>elems</code> and output of <code>fn</code>. If <code>elems</code> is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of <code>fn</code> may match the structure of <code>elems</code>. That is, if <code>elems</code> is <code>(t1, [t2, t3, [t4, t5]])</code>, then an appropriate signature for <code>fn</code> is: <code>fn = lambda (t1, [t2, t3, [t4, t5]]):</code>.</p> <p>Furthermore, <code>fn</code> may emit a different structure than its input. For example, <code>fn</code> may look like: <code>fn = lambda t1: return (t1 + 1, t1 - 1)</code>. In this case, the <code>dtype</code> parameter is not optional: <code>dtype</code> must be a type or (possibly nested) tuple of types matching the output of <code>fn</code>.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>fn</code>: The callable to be performed. It accepts one argument, which will have the same (possibly nested) structure as <code>elems</code>. Its output must have the same structure as <code>dtype</code> if one is provided, otherwise it must have the same structure as <code>elems</code>.</li> <li>\n<code>elems</code>: A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be applied to <code>fn</code>.</li> <li>\n<code>dtype</code>: (optional) The output type(s) of <code>fn</code>. If <code>fn</code> returns a structure of Tensors differing from the structure of <code>elems</code>, then <code>dtype</code> is not optional and must have the same structure as the output of <code>fn</code>.</li> <li>\n<code>parallel_iterations</code>: (optional) The number of iterations allowed to run in parallel.</li> <li>\n<code>back_prop</code>: (optional) True enables support for back propagation.</li> <li>\n<code>swap_memory</code>: (optional) True enables GPU-CPU memory swapping.</li> <li>\n<code>infer_shape</code>: (optional) False disables tests for consistent output shapes.</li> <li>\n<code>name</code>: (optional) Name prefix for the returned tensors.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A tensor or (possibly nested) sequence of tensors. Each tensor packs the results of applying <code>fn</code> to tensors unpacked from <code>elems</code> along the first dimension, from first to last.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>fn</code> is not callable or the structure of the output of <code>fn</code> and <code>dtype</code> do not match.</li> <li>\n<code>ValueError</code>: if the lengths of the output of <code>fn</code> and <code>dtype</code> do not match.</li> </ul> <h5 id=\"examples\">Examples:</h5> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">elems = np.array([1, 2, 3, 4, 5, 6])\nsquares = map_fn(lambda x: x * x, elems)\n# squares == [1, 4, 9, 16, 25, 36]\n</pre> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))\nalternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\n# alternate == [-1, 2, -3]\n</pre> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">elems = np.array([1, 2, 3])\nalternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))\n# alternates[0] == [1, 2, 3]\n# alternates[1] == [-1, -2, -3]\n</pre>  <h3 id=\"foldl\"><code>tf.foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, name=None)</code></h3> <p>foldl on the list of tensors unpacked from <code>elems</code> on dimension 0.</p> <p>This foldl operator repeatedly applies the callable <code>fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code>elems</code> on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn. If <code>initializer</code> is None, <code>elems</code> must contain at least one element, and its first element is used as the initializer.</p> <p>Suppose that <code>elems</code> is unpacked into <code>values</code>, a list of tensors. The shape of the result tensor is fn(initializer, values[0]).shape`.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>fn</code>: The callable to be performed.</li> <li>\n<code>elems</code>: A tensor to be unpacked on dimension 0.</li> <li>\n<code>initializer</code>: (optional) The initial value for the accumulator.</li> <li>\n<code>parallel_iterations</code>: (optional) The number of iterations allowed to run in parallel.</li> <li>\n<code>back_prop</code>: (optional) True enables support for back propagation.</li> <li>\n<code>swap_memory</code>: (optional) True enables GPU-CPU memory swapping.</li> <li>\n<code>name</code>: (optional) Name prefix for the returned tensors.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A tensor resulting from applying <code>fn</code> consecutively to the list of tensors unpacked from <code>elems</code>, from first to last.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>fn</code> is not callable.</li> </ul> <h5 id=\"example\">Example:</h5> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">elems = [1, 2, 3, 4, 5, 6]\nsum = foldl(lambda a, x: a + x, elems)\n# sum == 21\n</pre>  <h3 id=\"foldr\"><code>tf.foldr(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, name=None)</code></h3> <p>foldr on the list of tensors unpacked from <code>elems</code> on dimension 0.</p> <p>This foldr operator repeatedly applies the callable <code>fn</code> to a sequence of elements from last to first. The elements are made of the tensors unpacked from <code>elems</code>. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn. If <code>initializer</code> is None, <code>elems</code> must contain at least one element, and its first element is used as the initializer.</p> <p>Suppose that <code>elems</code> is unpacked into <code>values</code>, a list of tensors. The shape of the result tensor is <code>fn(initializer, values[0]).shape</code>.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>fn</code>: The callable to be performed.</li> <li>\n<code>elems</code>: A tensor that is unpacked into a sequence of tensors to apply <code>fn</code>.</li> <li>\n<code>initializer</code>: (optional) The initial value for the accumulator.</li> <li>\n<code>parallel_iterations</code>: (optional) The number of iterations allowed to run in parallel.</li> <li>\n<code>back_prop</code>: (optional) True enables support for back propagation.</li> <li>\n<code>swap_memory</code>: (optional) True enables GPU-CPU memory swapping.</li> <li>\n<code>name</code>: (optional) Name prefix for the returned tensors.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A tensor resulting from applying <code>fn</code> consecutively to the list of tensors unpacked from <code>elems</code>, from last to first.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>fn</code> is not callable.</li> </ul>  <h5 id=\"example-2\">Example:</h5> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">elems = [1, 2, 3, 4, 5, 6]\nsum = foldr(lambda a, x: a + x, elems)\n# sum == 21\n</pre>  <h3 id=\"scan\"><code>tf.scan(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None)</code></h3> <p>scan on the list of tensors unpacked from <code>elems</code> on dimension 0.</p> <p>The simplest version of <code>scan</code> repeatedly applies the callable <code>fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code>elems</code> on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn. If <code>initializer</code> is None, <code>elems</code> must contain at least one element, and its first element is used as the initializer.</p> <p>Suppose that <code>elems</code> is unpacked into <code>values</code>, a list of tensors. The shape of the result tensor is <code>[len(values)] + fn(initializer, values[0]).shape</code>.</p> <p>This method also allows multi-arity <code>elems</code> and accumulator. If <code>elems</code> is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The second argument of <code>fn</code> must match the structure of <code>elems</code>.</p> <p>If no <code>initializer</code> is provided, the output structure and dtypes of <code>fn</code> are assumed to be the same as its input; and in this case, the first argument of <code>fn</code> must match the structure of <code>elems</code>.</p> <p>If an <code>initializer</code> is provided, then the output of <code>fn</code> must have the same structure as <code>initializer</code>; and the first argument of <code>fn</code> must match this structure.</p> <p>For example, if <code>elems</code> is <code>(t1, [t2, t3])</code> and <code>initializer</code> is <code>[i1, i2]</code> then an appropriate signature for <code>fn</code> in <code>python2</code> is: <code>fn = lambda (acc_p1, acc_p2), (t1 [t2, t3]):</code> and <code>fn</code> must return a list, <code>[acc_n1, acc_n2]</code>. An alternative correct signature for <code>fn</code>, and the one that works in <code>python3</code>, is: <code>fn = lambda a, t:</code>, where <code>a</code> and <code>t</code> correspond to the input tuples.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>fn</code>: The callable to be performed. It accepts two arguments. The first will have the same (possibly nested) structure as <code>elems</code>. The second will have the same structure as <code>initializer</code> if one is provided, otherwise it will have the same structure as <code>elems</code>. Its output must have the same structure as <code>initializer</code> if one is provided, otherwise it must have the same structure as <code>elems</code>.</li> <li>\n<code>elems</code>: A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be the first argument to <code>fn</code>.</li> <li>\n<code>initializer</code>: (optional) A tensor or (possibly nested) sequence of tensors, initial value for the accumulator, and the expected output type of <code>fn</code>.</li> <li>\n<code>parallel_iterations</code>: (optional) The number of iterations allowed to run in parallel.</li> <li>\n<code>back_prop</code>: (optional) True enables support for back propagation.</li> <li>\n<code>swap_memory</code>: (optional) True enables GPU-CPU memory swapping.</li> <li>\n<code>infer_shape</code>: (optional) False disables tests for consistent output shapes.</li> <li>\n<code>name</code>: (optional) Name prefix for the returned tensors.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A tensor or (possibly nested) sequence of tensors. Each tensor packs the results of applying <code>fn</code> to tensors unpacked from <code>elems</code> along the first dimension, and the previous accumulator value(s), from first to last.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>fn</code> is not callable or the structure of the output of <code>fn</code> and <code>initializer</code> do not match.</li> <li>\n<code>ValueError</code>: if the lengths of the output of <code>fn</code> and <code>initializer</code> do not match.</li> </ul>  <h5 id=\"examples-2\">Examples:</h5> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">elems = np.array([1, 2, 3, 4, 5, 6])\nsum = scan(lambda a, x: a + x, elems)\n# sum == [1, 3, 6, 10, 15, 21]\n</pre> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">elems = np.array([1, 2, 3, 4, 5, 6])\ninitializer = np.array(0)\nsum_one = scan(\n    lambda a, x: x[0] - x[1] + a, (elems + 1, elems), initializer)\n# sum_one == [1, 2, 3, 4, 5, 6]\n</pre> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">elems = np.array([1, 0, 0, 0, 0, 0])\ninitializer = (np.array(0), np.array(1))\nfibonaccis = scan(lambda a, _: (a[1], a[0] + a[1]), elems, initializer)\n# fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/functional_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/functional_ops.html</a>\n  </p>\n</div>\n","rnn_cell":"<h1 id=\"neural-network-rnn-cells\">Neural Network RNN Cells</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#neural-network-rnn-cells\">Neural Network RNN Cells</a></li> <ul> <li><a href=\"#base-interface-for-all-rnn-cells\">Base interface for all RNN Cells</a></li> <ul> <li><a href=\"#RNNCell\"><code>class tf.nn.rnn_cell.RNNCell</code></a></li> </ul> <li><a href=\"#rnn-cells-for-use-with-tensorflows-core-rnn-methods\">RNN Cells for use with TensorFlow's core RNN methods</a></li> <ul> <li><a href=\"#BasicRNNCell\"><code>class tf.nn.rnn_cell.BasicRNNCell</code></a></li> <li><a href=\"#BasicLSTMCell\"><code>class tf.nn.rnn_cell.BasicLSTMCell</code></a></li> <li><a href=\"#GRUCell\"><code>class tf.nn.rnn_cell.GRUCell</code></a></li> <li><a href=\"#LSTMCell\"><code>class tf.nn.rnn_cell.LSTMCell</code></a></li> </ul> <li><a href=\"#classes-storing-split-rnncell-state\">Classes storing split <code>RNNCell</code> state</a></li> <ul> <li><a href=\"#LSTMStateTuple\"><code>class tf.nn.rnn_cell.LSTMStateTuple</code></a></li> </ul> <li><a href=\"#rnn-cell-wrappers-rnncells-that-wrap-other-rnncells\">RNN Cell wrappers (RNNCells that wrap other RNNCells)</a></li> <ul> <li><a href=\"#MultiRNNCell\"><code>class tf.nn.rnn_cell.MultiRNNCell</code></a></li> <li><a href=\"#DropoutWrapper\"><code>class tf.nn.rnn_cell.DropoutWrapper</code></a></li> <li><a href=\"#EmbeddingWrapper\"><code>class tf.nn.rnn_cell.EmbeddingWrapper</code></a></li> <li><a href=\"#InputProjectionWrapper\"><code>class tf.nn.rnn_cell.InputProjectionWrapper</code></a></li> <li><a href=\"#OutputProjectionWrapper\"><code>class tf.nn.rnn_cell.OutputProjectionWrapper</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Module for constructing RNN Cells.</p>  <h2 id=\"base-interface-for-all-rnn-cells\">Base interface for all RNN Cells</h2>  <h3 id=\"RNNCell\"><code>class tf.nn.rnn_cell.RNNCell</code></h3> <p>Abstract object representing an RNN cell.</p> <p>The definition of cell in this package differs from the definition used in the literature. In the literature, cell refers to an object with a single scalar output. The definition in this package refers to a horizontal array of such units.</p> <p>An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs. This operation results in an output matrix with <code>self.output_size</code> columns. If <code>self.state_size</code> is an integer, this operation also results in a new state matrix with <code>self.state_size</code> columns. If <code>self.state_size</code> is a tuple of integers, then it results in a tuple of <code>len(state_size)</code> state matrices, each with the a column size corresponding to values in <code>state_size</code>.</p> <p>This module provides a number of basic commonly used RNN cells, such as LSTM (Long Short Term Memory) or GRU (Gated Recurrent Unit), and a number of operators that allow add dropouts, projections, or embeddings for inputs. Constructing multi-layer cells is supported by the class <code>MultiRNNCell</code>, or by calling the <code>rnn</code> ops several times. Every <code>RNNCell</code> must have the properties below and and implement <code>__call__</code> with the following signature.</p>  <h4 id=\"RNNCell.output_size\"><code>tf.nn.rnn_cell.RNNCell.output_size</code></h4> <p>Integer or TensorShape: size of outputs produced by this cell.</p>  <h4 id=\"RNNCell.state_size\"><code>tf.nn.rnn_cell.RNNCell.state_size</code></h4> <p>size(s) of state(s) used by this cell.</p> <p>It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.</p>  <h4 id=\"RNNCell.zero_state\"><code>tf.nn.rnn_cell.RNNCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h2 id=\"rnn-cells-for-use-with-tensorflows-core-rnn-methods\">RNN Cells for use with TensorFlow's core RNN methods</h2>  <h3 id=\"BasicRNNCell\"><code>class tf.nn.rnn_cell.BasicRNNCell</code></h3> <p>The most basic RNN cell.</p>  <h4 id=\"BasicRNNCell.__init__\"><code>tf.nn.rnn_cell.BasicRNNCell.__init__(num_units, input_size=None, activation=tanh)</code></h4>  <h4 id=\"BasicRNNCell.output_size\"><code>tf.nn.rnn_cell.BasicRNNCell.output_size</code></h4>  <h4 id=\"BasicRNNCell.state_size\"><code>tf.nn.rnn_cell.BasicRNNCell.state_size</code></h4>  <h4 id=\"BasicRNNCell.zero_state\"><code>tf.nn.rnn_cell.BasicRNNCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"BasicLSTMCell\"><code>class tf.nn.rnn_cell.BasicLSTMCell</code></h3> <p>Basic LSTM recurrent network cell.</p> <p>The implementation is based on: <a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/abs/1409.2329&amp;usg=AFQjCNFrC4k5oZBQOzm-F3FaU62wXp_VYQ\">http://arxiv.org/abs/1409.2329</a>.</p> <p>We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training.</p> <p>It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.</p> <p>For advanced models, please use the full LSTMCell that follows.</p>  <h4 id=\"BasicLSTMCell.__init__\"><code>tf.nn.rnn_cell.BasicLSTMCell.__init__(num_units, forget_bias=1.0, input_size=None, state_is_tuple=False, activation=tanh)</code></h4> <p>Initialize the basic LSTM cell.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>num_units</code>: int, The number of units in the LSTM cell.</li> <li>\n<code>forget_bias</code>: float, The bias added to forget gates (see above).</li> <li>\n<code>input_size</code>: Deprecated and unused.</li> <li>\n<code>state_is_tuple</code>: If True, accepted and returned states are 2-tuples of the <code>c_state</code> and <code>m_state</code>. By default (False), they are concatenated along the column axis. This default behavior will soon be deprecated.</li> <li>\n<code>activation</code>: Activation function of the inner states.</li> </ul>  <h4 id=\"BasicLSTMCell.output_size\"><code>tf.nn.rnn_cell.BasicLSTMCell.output_size</code></h4>  <h4 id=\"BasicLSTMCell.state_size\"><code>tf.nn.rnn_cell.BasicLSTMCell.state_size</code></h4>  <h4 id=\"BasicLSTMCell.zero_state\"><code>tf.nn.rnn_cell.BasicLSTMCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"GRUCell\"><code>class tf.nn.rnn_cell.GRUCell</code></h3> <p>Gated Recurrent Unit cell (cf. <a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/abs/1406.1078&amp;usg=AFQjCNGiRGjnEcm1jvlhYAz0d-dnjHe84A\">http://arxiv.org/abs/1406.1078</a>).</p>  <h4 id=\"GRUCell.__init__\"><code>tf.nn.rnn_cell.GRUCell.__init__(num_units, input_size=None, activation=tanh)</code></h4>  <h4 id=\"GRUCell.output_size\"><code>tf.nn.rnn_cell.GRUCell.output_size</code></h4>  <h4 id=\"GRUCell.state_size\"><code>tf.nn.rnn_cell.GRUCell.state_size</code></h4>  <h4 id=\"GRUCell.zero_state\"><code>tf.nn.rnn_cell.GRUCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"LSTMCell\"><code>class tf.nn.rnn_cell.LSTMCell</code></h3> <p>Long short-term memory unit (LSTM) recurrent network cell.</p> <p>The default non-peephole implementation is based on:</p> <p><a target=\"_blank\" href=\"https://www.google.com/url?q=http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf&amp;usg=AFQjCNFzIBcb7U6rkXHAUGBqzkT4YWuNGw\">http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf</a></p> <p>S. Hochreiter and J. Schmidhuber. \"Long Short-Term Memory\". Neural Computation, 9(8):1735-1780, 1997.</p> <p>The peephole implementation is based on:</p> <p><a target=\"_blank\" href=\"https://research.google.com/pubs/archive/43905.pdf\">https://research.google.com/pubs/archive/43905.pdf</a></p> <p>Hasim Sak, Andrew Senior, and Francoise Beaufays. \"Long short-term memory recurrent neural network architectures for large scale acoustic modeling.\" INTERSPEECH, 2014.</p> <p>The class uses optional peep-hole connections, optional cell clipping, and an optional projection layer.</p>  <h4 id=\"LSTMCell.__init__\"><code>tf.nn.rnn_cell.LSTMCell.__init__(num_units, input_size=None, use_peepholes=False, cell_clip=None, initializer=None, num_proj=None, proj_clip=None, num_unit_shards=1, num_proj_shards=1, forget_bias=1.0, state_is_tuple=False, activation=tanh)</code></h4> <p>Initialize the parameters for an LSTM cell.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>num_units</code>: int, The number of units in the LSTM cell</li> <li>\n<code>input_size</code>: Deprecated and unused.</li> <li>\n<code>use_peepholes</code>: bool, set True to enable diagonal/peephole connections.</li> <li>\n<code>cell_clip</code>: (optional) A float value, if provided the cell state is clipped by this value prior to the cell output activation.</li> <li>\n<code>initializer</code>: (optional) The initializer to use for the weight and projection matrices.</li> <li>\n<code>num_proj</code>: (optional) int, The output dimensionality for the projection matrices. If None, no projection is performed.</li> <li><p><code>proj_clip</code>: (optional) A float value. If <code>num_proj &gt; 0</code> and <code>proj_clip</code> is provided, then the projected values are clipped elementwise to within <code>[-proj_clip, proj_clip]</code>.</p></li> <li><p><code>num_unit_shards</code>: How to split the weight matrix. If &gt;1, the weight matrix is stored across num_unit_shards.</p></li> <li><p><code>num_proj_shards</code>: How to split the projection matrix. If &gt;1, the projection matrix is stored across num_proj_shards.</p></li> <li><p><code>forget_bias</code>: Biases of the forget gate are initialized by default to 1 in order to reduce the scale of forgetting at the beginning of the training.</p></li> <li><p><code>state_is_tuple</code>: If True, accepted and returned states are 2-tuples of the <code>c_state</code> and <code>m_state</code>. By default (False), they are concatenated along the column axis. This default behavior will soon be deprecated.</p></li> <li><p><code>activation</code>: Activation function of the inner states.</p></li> </ul>  <h4 id=\"LSTMCell.output_size\"><code>tf.nn.rnn_cell.LSTMCell.output_size</code></h4>  <h4 id=\"LSTMCell.state_size\"><code>tf.nn.rnn_cell.LSTMCell.state_size</code></h4>  <h4 id=\"LSTMCell.zero_state\"><code>tf.nn.rnn_cell.LSTMCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h2 id=\"classes-storing-split-rnncell-state\">Classes storing split <code>RNNCell</code> state</h2>  <h3 id=\"LSTMStateTuple\"><code>class tf.nn.rnn_cell.LSTMStateTuple</code></h3> <p>Tuple used by LSTM Cells for <code>state_size</code>, <code>zero_state</code>, and output state.</p> <p>Stores two elements: <code>(c, h)</code>, in that order.</p> <p>Only used when <code>state_is_tuple=True</code>.</p>  <h4 id=\"LSTMStateTuple.c\"><code>tf.nn.rnn_cell.LSTMStateTuple.c</code></h4> <p>Alias for field number 0</p>  <h4 id=\"LSTMStateTuple.dtype\"><code>tf.nn.rnn_cell.LSTMStateTuple.dtype</code></h4>  <h4 id=\"LSTMStateTuple.h\"><code>tf.nn.rnn_cell.LSTMStateTuple.h</code></h4> <p>Alias for field number 1</p>  <h2 id=\"rnn-cell-wrappers-rnncells-that-wrap-other-rnncells\">RNN Cell wrappers (RNNCells that wrap other RNNCells)</h2>  <h3 id=\"MultiRNNCell\"><code>class tf.nn.rnn_cell.MultiRNNCell</code></h3> <p>RNN cell composed sequentially of multiple simple cells.</p>  <h4 id=\"MultiRNNCell.__init__\"><code>tf.nn.rnn_cell.MultiRNNCell.__init__(cells, state_is_tuple=False)</code></h4> <p>Create a RNN cell composed sequentially of a number of RNNCells.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>cells</code>: list of RNNCells that will be composed in this order.</li> <li>\n<code>state_is_tuple</code>: If True, accepted and returned states are n-tuples, where <code>n = len(cells)</code>. By default (False), the states are all concatenated along the column axis.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if cells is empty (not allowed), or at least one of the cells returns a state tuple but the flag <code>state_is_tuple</code> is <code>False</code>.</li> </ul>  <h4 id=\"MultiRNNCell.output_size\"><code>tf.nn.rnn_cell.MultiRNNCell.output_size</code></h4>  <h4 id=\"MultiRNNCell.state_size\"><code>tf.nn.rnn_cell.MultiRNNCell.state_size</code></h4>  <h4 id=\"MultiRNNCell.zero_state\"><code>tf.nn.rnn_cell.MultiRNNCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"DropoutWrapper\"><code>class tf.nn.rnn_cell.DropoutWrapper</code></h3> <p>Operator adding dropout to inputs and outputs of the given cell.</p>  <h4 id=\"DropoutWrapper.__init__\"><code>tf.nn.rnn_cell.DropoutWrapper.__init__(cell, input_keep_prob=1.0, output_keep_prob=1.0, seed=None)</code></h4> <p>Create a cell with added input and/or output dropout.</p> <p>Dropout is never used on the state.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>cell</code>: an RNNCell, a projection to output_size is added to it.</li> <li>\n<code>input_keep_prob</code>: unit Tensor or float between 0 and 1, input keep probability; if it is float and 1, no input dropout will be added.</li> <li>\n<code>output_keep_prob</code>: unit Tensor or float between 0 and 1, output keep probability; if it is float and 1, no output dropout will be added.</li> <li>\n<code>seed</code>: (optional) integer, the randomness seed.</li> </ul>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if cell is not an RNNCell.</li> <li>\n<code>ValueError</code>: if keep_prob is not between 0 and 1.</li> </ul>  <h4 id=\"DropoutWrapper.output_size\"><code>tf.nn.rnn_cell.DropoutWrapper.output_size</code></h4>  <h4 id=\"DropoutWrapper.state_size\"><code>tf.nn.rnn_cell.DropoutWrapper.state_size</code></h4>  <h4 id=\"DropoutWrapper.zero_state\"><code>tf.nn.rnn_cell.DropoutWrapper.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"EmbeddingWrapper\"><code>class tf.nn.rnn_cell.EmbeddingWrapper</code></h3> <p>Operator adding input embedding to the given cell.</p> <p>Note: in many cases it may be more efficient to not use this wrapper, but instead concatenate the whole sequence of your inputs in time, do the embedding on this batch-concatenated sequence, then split it and feed into your RNN.</p>  <h4 id=\"EmbeddingWrapper.__init__\"><code>tf.nn.rnn_cell.EmbeddingWrapper.__init__(cell, embedding_classes, embedding_size, initializer=None)</code></h4> <p>Create a cell with an added input embedding.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>cell</code>: an RNNCell, an embedding will be put before its inputs.</li> <li>\n<code>embedding_classes</code>: integer, how many symbols will be embedded.</li> <li>\n<code>embedding_size</code>: integer, the size of the vectors we embed into.</li> <li>\n<code>initializer</code>: an initializer to use when creating the embedding; if None, the initializer from variable scope or a default one is used.</li> </ul>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if cell is not an RNNCell.</li> <li>\n<code>ValueError</code>: if embedding_classes is not positive.</li> </ul>  <h4 id=\"EmbeddingWrapper.output_size\"><code>tf.nn.rnn_cell.EmbeddingWrapper.output_size</code></h4>  <h4 id=\"EmbeddingWrapper.state_size\"><code>tf.nn.rnn_cell.EmbeddingWrapper.state_size</code></h4>  <h4 id=\"EmbeddingWrapper.zero_state\"><code>tf.nn.rnn_cell.EmbeddingWrapper.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"InputProjectionWrapper\"><code>class tf.nn.rnn_cell.InputProjectionWrapper</code></h3> <p>Operator adding an input projection to the given cell.</p> <p>Note: in many cases it may be more efficient to not use this wrapper, but instead concatenate the whole sequence of your inputs in time, do the projection on this batch-concatenated sequence, then split it.</p>  <h4 id=\"InputProjectionWrapper.__init__\"><code>tf.nn.rnn_cell.InputProjectionWrapper.__init__(cell, num_proj, input_size=None)</code></h4> <p>Create a cell with input projection.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>cell</code>: an RNNCell, a projection of inputs is added before it.</li> <li>\n<code>num_proj</code>: Python integer. The dimension to project to.</li> <li>\n<code>input_size</code>: Deprecated and unused.</li> </ul>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if cell is not an RNNCell.</li> </ul>  <h4 id=\"InputProjectionWrapper.output_size\"><code>tf.nn.rnn_cell.InputProjectionWrapper.output_size</code></h4>  <h4 id=\"InputProjectionWrapper.state_size\"><code>tf.nn.rnn_cell.InputProjectionWrapper.state_size</code></h4>  <h4 id=\"InputProjectionWrapper.zero_state\"><code>tf.nn.rnn_cell.InputProjectionWrapper.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"OutputProjectionWrapper\"><code>class tf.nn.rnn_cell.OutputProjectionWrapper</code></h3> <p>Operator adding an output projection to the given cell.</p> <p>Note: in many cases it may be more efficient to not use this wrapper, but instead concatenate the whole sequence of your outputs in time, do the projection on this batch-concatenated sequence, then split it if needed or directly feed into a softmax.</p>  <h4 id=\"OutputProjectionWrapper.__init__\"><code>tf.nn.rnn_cell.OutputProjectionWrapper.__init__(cell, output_size)</code></h4> <p>Create a cell with output projection.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>cell</code>: an RNNCell, a projection to output_size is added to it.</li> <li>\n<code>output_size</code>: integer, the size of the output after projection.</li> </ul>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if cell is not an RNNCell.</li> <li>\n<code>ValueError</code>: if output_size is not positive.</li> </ul>  <h4 id=\"OutputProjectionWrapper.output_size\"><code>tf.nn.rnn_cell.OutputProjectionWrapper.output_size</code></h4>  <h4 id=\"OutputProjectionWrapper.state_size\"><code>tf.nn.rnn_cell.OutputProjectionWrapper.state_size</code></h4>  <h4 id=\"OutputProjectionWrapper.zero_state\"><code>tf.nn.rnn_cell.OutputProjectionWrapper.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/rnn_cell.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/rnn_cell.html</a>\n  </p>\n</div>\n","constant_op":"<h1 id=\"constants-sequences-and-random-values\">Constants, Sequences, and Random Values</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#constants-sequences-and-random-values\">Constants, Sequences, and Random Values</a></li> <ul> <li><a href=\"#constant-value-tensors\">Constant Value Tensors</a></li> <ul> <li><a href=\"#zeros\"><code>tf.zeros(shape, dtype=tf.float32, name=None)</code></a></li> <li><a href=\"#zeros_like\"><code>tf.zeros_like(tensor, dtype=None, name=None)</code></a></li> <li><a href=\"#ones\"><code>tf.ones(shape, dtype=tf.float32, name=None)</code></a></li> <li><a href=\"#ones_like\"><code>tf.ones_like(tensor, dtype=None, name=None)</code></a></li> <li><a href=\"#fill\"><code>tf.fill(dims, value, name=None)</code></a></li> <li><a href=\"#constant\"><code>tf.constant(value, dtype=None, shape=None, name=Const)</code></a></li> </ul> <li><a href=\"#sequences\">Sequences</a></li> <ul> <li><a href=\"#linspace\"><code>tf.linspace(start, stop, num, name=None)</code></a></li> <li><a href=\"#range\"><code>tf.range(start, limit=None, delta=1, name=range)</code></a></li> </ul> <li><a href=\"#random-tensors\">Random Tensors</a></li> <ul> <li><a href=\"#examples\">Examples:</a></li> <li><a href=\"#random_normal\"><code>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code></a></li> <li><a href=\"#truncated_normal\"><code>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code></a></li> <li><a href=\"#random_uniform\"><code>tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)</code></a></li> <li><a href=\"#random_shuffle\"><code>tf.random_shuffle(value, seed=None, name=None)</code></a></li> <li><a href=\"#random_crop\"><code>tf.random_crop(value, size, seed=None, name=None)</code></a></li> <li><a href=\"#multinomial\"><code>tf.multinomial(logits, num_samples, seed=None, name=None)</code></a></li> <li><a href=\"#random_gamma\"><code>tf.random_gamma(shape, alpha, beta=None, dtype=tf.float32, seed=None, name=None)</code></a></li> <li><a href=\"#set_random_seed\"><code>tf.set_random_seed(seed)</code></a></li> </ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#ops\"><code>tf.contrib.graph_editor.ops(*args, **kwargs)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"constant-value-tensors\">Constant Value Tensors</h2> <p>TensorFlow provides several operations that you can use to generate constants.</p>  <h3 id=\"zeros\"><code>tf.zeros(shape, dtype=tf.float32, name=None)</code></h3> <p>Creates a tensor with all elements set to zero.</p> <p>This operation returns a tensor of type <code>dtype</code> with shape <code>shape</code> and all elements set to zero.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">tf.zeros([3, 4], int32) ==&gt; [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n</pre> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>shape</code>: Either a list of integers, or a 1-D <code>Tensor</code> of type <code>int32</code>.</li> <li>\n<code>dtype</code>: The type of an element in the resulting <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code> with all elements set to zero.</p>  <h3 id=\"zeros_like\"><code>tf.zeros_like(tensor, dtype=None, name=None)</code></h3> <p>Creates a tensor with all elements set to zero.</p> <p>Given a single tensor (<code>tensor</code>), this operation returns a tensor of the same type and shape as <code>tensor</code> with all elements set to zero. Optionally, you can use <code>dtype</code> to specify a new type for the returned tensor.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'tensor' is [[1, 2, 3], [4, 5, 6]]\ntf.zeros_like(tensor) ==&gt; [[0, 0, 0], [0, 0, 0]]\n</pre>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>tensor</code>: A <code>Tensor</code>.</li> <li><p><code>dtype</code>: A type for the returned <code>Tensor</code>. Must be <code>float32</code>, <code>float64</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>complex64</code>, or <code>complex128</code>.</p></li> <li><p><code>name</code>: A name for the operation (optional).</p></li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code> with all elements set to zero.</p>  <h3 id=\"ones\"><code>tf.ones(shape, dtype=tf.float32, name=None)</code></h3> <p>Creates a tensor with all elements set to 1.</p> <p>This operation returns a tensor of type <code>dtype</code> with shape <code>shape</code> and all elements set to 1.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">tf.ones([2, 3], int32) ==&gt; [[1, 1, 1], [1, 1, 1]]\n</pre>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>shape</code>: Either a list of integers, or a 1-D <code>Tensor</code> of type <code>int32</code>.</li> <li>\n<code>dtype</code>: The type of an element in the resulting <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>Tensor</code> with all elements set to 1.</p>  <h3 id=\"ones_like\"><code>tf.ones_like(tensor, dtype=None, name=None)</code></h3> <p>Creates a tensor with all elements set to 1.</p> <p>Given a single tensor (<code>tensor</code>), this operation returns a tensor of the same type and shape as <code>tensor</code> with all elements set to 1. Optionally, you can specify a new type (<code>dtype</code>) for the returned tensor.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'tensor' is [[1, 2, 3], [4, 5, 6]]\ntf.ones_like(tensor) ==&gt; [[1, 1, 1], [1, 1, 1]]\n</pre>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>tensor</code>: A <code>Tensor</code>.</li> <li><p><code>dtype</code>: A type for the returned <code>Tensor</code>. Must be <code>float32</code>, <code>float64</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>complex64</code>, or <code>complex128</code>.</p></li> <li><p><code>name</code>: A name for the operation (optional).</p></li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>Tensor</code> with all elements set to 1.</p>  <h3 id=\"fill\"><code>tf.fill(dims, value, name=None)</code></h3> <p>Creates a tensor filled with a scalar value.</p> <p>This operation creates a tensor of shape <code>dims</code> and fills it with <code>value</code>.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># Output tensor has shape [2, 3].\nfill([2, 3], 9) ==&gt; [[9, 9, 9]\n                     [9, 9, 9]]\n</pre>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>dims</code>: A <code>Tensor</code> of type <code>int32</code>. 1-D. Represents the shape of the output tensor.</li> <li>\n<code>value</code>: A <code>Tensor</code>. 0-D (scalar). Value to fill the returned tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>value</code>.</p>  <h3 id=\"constant\"><code>tf.constant(value, dtype=None, shape=None, name='Const')</code></h3> <p>Creates a constant tensor.</p> <p>The resulting tensor is populated with values of type <code>dtype</code>, as specified by arguments <code>value</code> and (optionally) <code>shape</code> (see examples below).</p> <p>The argument <code>value</code> can be a constant value, or a list of values of type <code>dtype</code>. If <code>value</code> is a list, then the length of the list must be less than or equal to the number of elements implied by the <code>shape</code> argument (if specified). In the case where the list length is less than the number of elements specified by <code>shape</code>, the last element in the list will be used to fill the remaining entries.</p> <p>The argument <code>shape</code> is optional. If present, it specifies the dimensions of the resulting tensor. If not present, the shape of <code>value</code> is used.</p> <p>If the argument <code>dtype</code> is not specified, then the type is inferred from the type of <code>value</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Constant 1-D Tensor populated with value list.\ntensor = tf.constant([1, 2, 3, 4, 5, 6, 7]) =&gt; [1 2 3 4 5 6 7]\n\n# Constant 2-D tensor populated with scalar value -1.\ntensor = tf.constant(-1.0, shape=[2, 3]) =&gt; [[-1. -1. -1.]\n                                             [-1. -1. -1.]]\n</pre>  <h5 id=\"args-6\">Args:</h5> <ul> <li><p><code>value</code>: A constant value (or list) of output type <code>dtype</code>.</p></li> <li><p><code>dtype</code>: The type of the elements of the resulting tensor.</p></li> <li><p><code>shape</code>: Optional dimensions of resulting tensor.</p></li> <li><p><code>name</code>: Optional name for the tensor.</p></li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A Constant Tensor.</p> <h2 id=\"sequences\">Sequences</h2>  <h3 id=\"linspace\"><code>tf.linspace(start, stop, num, name=None)</code></h3> <p>Generates values in an interval.</p> <p>A sequence of <code>num</code> evenly-spaced values are generated beginning at <code>start</code>. If <code>num &gt; 1</code>, the values in the sequence increase by <code>stop - start / num - 1</code>, so that the last one is exactly <code>stop</code>.</p> <p>For example:</p> <pre class=\"lang-matlab no-auto-prettify\">tf.linspace(10.0, 12.0, 3, name=\"linspace\") =&gt; [ 10.0  11.0  12.0]\n</pre>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>start</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>. First entry in the range.</li> <li>\n<code>stop</code>: A <code>Tensor</code>. Must have the same type as <code>start</code>. Last entry in the range.</li> <li>\n<code>num</code>: A <code>Tensor</code> of type <code>int32</code>. Number of values to generate.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>start</code>. 1-D. The generated values.</p>  <h3 id=\"range\"><code>tf.range(start, limit=None, delta=1, name='range')</code></h3> <p>Creates a sequence of integers.</p> <p>Creates a sequence of integers that begins at <code>start</code> and extends by increments of <code>delta</code> up to but not including <code>limit</code>.</p> <p>Like the Python builtin <code>range</code>, <code>start</code> defaults to 0, so that <code>range(n) = range(0, n)</code>.</p> <p>For example:</p> <pre class=\"lang-matlab no-auto-prettify\"># 'start' is 3\n# 'limit' is 18\n# 'delta' is 3\ntf.range(start, limit, delta) ==&gt; [3, 6, 9, 12, 15]\n\n# 'limit' is 5\ntf.range(limit) ==&gt; [0, 1, 2, 3, 4]\n</pre>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>start</code>: A 0-D (scalar) of type <code>int32</code>. First entry in sequence. Defaults to 0.</li> <li>\n<code>limit</code>: A 0-D (scalar) of type <code>int32</code>. Upper limit of sequence, exclusive.</li> <li>\n<code>delta</code>: A 0-D <code>Tensor</code> (scalar) of type <code>int32</code>. Optional. Default is 1. Number that increments <code>start</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>An 1-D <code>int32</code> <code>Tensor</code>.</p>  <h2 id=\"random-tensors\">Random Tensors</h2> <p>TensorFlow has several ops that create random tensors with different distributions. The random ops are stateful, and create new random values each time they are evaluated.</p> <p>The <code>seed</code> keyword argument in these functions acts in conjunction with the graph-level random seed. Changing either the graph-level seed using <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> or the op-level seed will change the underlying seed of these operations. Setting neither graph-level nor op-level seed, results in a random seed for all operations. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for details on the interaction between operation-level and graph-level random seeds.</p> <h3 id=\"examples\">Examples:</h3> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Create a tensor of shape [2, 3] consisting of random normal values, with mean\n# -1 and standard deviation 4.\nnorm = tf.random_normal([2, 3], mean=-1, stddev=4)\n\n# Shuffle the first dimension of a tensor\nc = tf.constant([[1, 2], [3, 4], [5, 6]])\nshuff = tf.random_shuffle(c)\n\n# Each time we run these ops, different results are generated\nsess = tf.Session()\nprint(sess.run(norm))\nprint(sess.run(norm))\n\n# Set an op-level seed to generate repeatable sequences across sessions.\nnorm = tf.random_normal([2, 3], seed=1234)\nsess = tf.Session()\nprint(sess.run(norm))\nprint(sess.run(norm))\nsess = tf.Session()\nprint(sess.run(norm))\nprint(sess.run(norm))\n</pre> <p>Another common use of random values is the initialization of variables. Also see the <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html\">Variables How To</a>.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Use random uniform values in [0, 1) as the initializer for a variable of shape\n# [2, 3]. The default type is float32.\nvar = tf.Variable(tf.random_uniform([2, 3]), name=\"var\")\ninit = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init)\nprint(sess.run(var))\n</pre>  <h3 id=\"random_normal\"><code>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code></h3> <p>Outputs random values from a normal distribution.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>shape</code>: A 1-D integer Tensor or Python array. The shape of the output tensor.</li> <li>\n<code>mean</code>: A 0-D Tensor or Python value of type <code>dtype</code>. The mean of the normal distribution.</li> <li>\n<code>stddev</code>: A 0-D Tensor or Python value of type <code>dtype</code>. The standard deviation of the normal distribution.</li> <li>\n<code>dtype</code>: The type of the output.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed for the distribution. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A tensor of the specified shape filled with random normal values.</p>  <h3 id=\"truncated_normal\"><code>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code></h3> <p>Outputs random values from a truncated normal distribution.</p> <p>The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>shape</code>: A 1-D integer Tensor or Python array. The shape of the output tensor.</li> <li>\n<code>mean</code>: A 0-D Tensor or Python value of type <code>dtype</code>. The mean of the truncated normal distribution.</li> <li>\n<code>stddev</code>: A 0-D Tensor or Python value of type <code>dtype</code>. The standard deviation of the truncated normal distribution.</li> <li>\n<code>dtype</code>: The type of the output.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed for the distribution. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A tensor of the specified shape filled with random truncated normal values.</p>  <h3 id=\"random_uniform\"><code>tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)</code></h3> <p>Outputs random values from a uniform distribution.</p> <p>The generated values follow a uniform distribution in the range <code>[minval, maxval)</code>. The lower bound <code>minval</code> is included in the range, while the upper bound <code>maxval</code> is excluded.</p> <p>For floats, the default range is <code>[0, 1)</code>. For ints, at least <code>maxval</code> must be specified explicitly.</p> <p>In the integer case, the random integers are slightly biased unless <code>maxval - minval</code> is an exact power of two. The bias is small for values of <code>maxval - minval</code> significantly smaller than the range of the output (either <code>2**32</code> or <code>2**64</code>).</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>shape</code>: A 1-D integer Tensor or Python array. The shape of the output tensor.</li> <li>\n<code>minval</code>: A 0-D Tensor or Python value of type <code>dtype</code>. The lower bound on the range of random values to generate. Defaults to 0.</li> <li>\n<code>maxval</code>: A 0-D Tensor or Python value of type <code>dtype</code>. The upper bound on the range of random values to generate. Defaults to 1 if <code>dtype</code> is floating point.</li> <li>\n<code>dtype</code>: The type of the output: <code>float32</code>, <code>float64</code>, <code>int32</code>, or <code>int64</code>.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed for the distribution. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A tensor of the specified shape filled with random uniform values.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>dtype</code> is integral and <code>maxval</code> is not specified.</li> </ul>  <h3 id=\"random_shuffle\"><code>tf.random_shuffle(value, seed=None, name=None)</code></h3> <p>Randomly shuffles a tensor along its first dimension.</p> <p>The tensor is shuffled along dimension 0, such that each <code>value[j]</code> is mapped to one and only one <code>output[i]</code>. For example, a mapping that might occur for a 3x2 tensor is:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">[[1, 2],       [[5, 6],\n [3, 4],  ==&gt;   [1, 2],\n [5, 6]]        [3, 4]]\n</pre>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>value</code>: A Tensor to be shuffled.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed for the distribution. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>A tensor of same shape and type as <code>value</code>, shuffled along its first dimension.</p>  <h3 id=\"random_crop\"><code>tf.random_crop(value, size, seed=None, name=None)</code></h3> <p>Randomly crops a tensor to a given size.</p> <p>Slices a shape <code>size</code> portion out of <code>value</code> at a uniformly chosen offset. Requires <code>value.shape &gt;= size</code>.</p> <p>If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with <code>size = [crop_height, crop_width, 3]</code>.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>value</code>: Input tensor to crop.</li> <li>\n<code>size</code>: 1-D tensor with size the rank of <code>value</code>.</li> <li>\n<code>seed</code>: Python integer. Used to create a random seed. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A cropped tensor of the same rank as <code>value</code> and shape <code>size</code>.</p>  <h3 id=\"multinomial\"><code>tf.multinomial(logits, num_samples, seed=None, name=None)</code></h3> <p>Draws samples from a multinomial distribution.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># samples has shape [1, 5], where each value is either 0 or 1 with equal\n# probability.\nsamples = tf.multinomial(tf.log([[10., 10.]]), 5)\n</pre>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>logits</code>: 2-D Tensor with shape <code>[batch_size, num_classes]</code>. Each slice <code>[i, :]</code> represents the unnormalized log probabilities for all classes.</li> <li>\n<code>num_samples</code>: 0-D. Number of independent samples to draw for each row slice.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed for the distribution. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>name</code>: Optional name for the operation.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>The drawn samples of shape <code>[batch_size, num_samples]</code>.</p>  <h3 id=\"random_gamma\"><code>tf.random_gamma(shape, alpha, beta=None, dtype=tf.float32, seed=None, name=None)</code></h3> <p>Draws <code>shape</code> samples from each of the given Gamma distribution(s).</p> <p><code>alpha</code> is the shape parameter describing the distribution(s), and <code>beta</code> is the inverse scale parameter(s).</p> <p>Example:</p> <p>samples = tf.random_gamma([10], [0.5, 1.5]) # samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents # the samples drawn from each distribution</p> <p>samples = tf.random_gamma([7, 5], [0.5, 1.5]) # samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1] # represents the 7x5 samples drawn from each of the two distributions</p> <p>samples = tf.random_gamma([30], [[1.],[3.],[5.]], beta=[[3., 4.]]) # samples has shape [30, 3, 2], with 30 samples each of 3x2 distributions.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>shape</code>: A 1-D integer Tensor or Python array. The shape of the output samples to be drawn per alpha/beta-parameterized distribution.</li> <li>\n<code>alpha</code>: A Tensor or Python value or N-D array of type <code>dtype</code>. <code>alpha</code> provides the shape parameter(s) describing the gamma distribution(s) to sample. Must be broadcastable with <code>beta</code>.</li> <li>\n<code>beta</code>: A Tensor or Python value or N-D array of type <code>dtype</code>. Defaults to 1. <code>beta</code> provides the inverse scale parameter(s) of the gamma distribution(s) to sample. Must be broadcastable with <code>alpha</code>.</li> <li>\n<code>dtype</code>: The type of alpha, beta, and the output: <code>float16</code>, <code>float32</code>, or <code>float64</code>.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed for the distributions. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>name</code>: Optional name for the operation.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>tf.concat(shape, tf.shape(alpha + beta))</code> with values of type <code>dtype</code>.</li> </ul>  <h3 id=\"set_random_seed\"><code>tf.set_random_seed(seed)</code></h3> <p>Sets the graph-level random seed.</p> <p>Operations that rely on a random seed actually derive it from two seeds: the graph-level and operation-level seeds. This sets the graph-level seed.</p> <p>Its interactions with operation-level seeds is as follows:</p> <ol> <li>If neither the graph-level nor the operation seed is set: A random seed is used for this op.</li> <li>If the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence.</li> <li>If the graph-level seed is not set, but the operation seed is set: A default graph-level seed and the specified operation seed are used to determine the random sequence.</li> <li>If both the graph-level and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.</li> </ol> <p>To illustrate the user-visible effects, consider these examples:</p> <p>To generate different sequences across sessions, set neither graph-level nor op-level seeds:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">a = tf.random_uniform([1])\nb = tf.random_normal([1])\n\nprint(\"Session 1\")\nwith tf.Session() as sess1:\n  print(sess1.run(a))  # generates 'A1'\n  print(sess1.run(a))  # generates 'A2'\n  print(sess1.run(b))  # generates 'B1'\n  print(sess1.run(b))  # generates 'B2'\n\nprint(\"Session 2\")\nwith tf.Session() as sess2:\n  print(sess2.run(a))  # generates 'A3'\n  print(sess2.run(a))  # generates 'A4'\n  print(sess2.run(b))  # generates 'B3'\n  print(sess2.run(b))  # generates 'B4'\n</pre> <p>To generate the same repeatable sequence for an op across sessions, set the seed for the op:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">a = tf.random_uniform([1], seed=1)\nb = tf.random_normal([1])\n\n# Repeatedly running this block with the same graph will generate the same\n# sequence of values for 'a', but different sequences of values for 'b'.\nprint(\"Session 1\")\nwith tf.Session() as sess1:\n  print(sess1.run(a))  # generates 'A1'\n  print(sess1.run(a))  # generates 'A2'\n  print(sess1.run(b))  # generates 'B1'\n  print(sess1.run(b))  # generates 'B2'\n\nprint(\"Session 2\")\nwith tf.Session() as sess2:\n  print(sess2.run(a))  # generates 'A1'\n  print(sess2.run(a))  # generates 'A2'\n  print(sess2.run(b))  # generates 'B3'\n  print(sess2.run(b))  # generates 'B4'\n</pre> <p>To make the random sequences generated by all ops be repeatable across sessions, set a graph-level seed:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">tf.set_random_seed(1234)\na = tf.random_uniform([1])\nb = tf.random_normal([1])\n\n# Repeatedly running this block with the same graph will generate different\n# sequences of 'a' and 'b'.\nprint(\"Session 1\")\nwith tf.Session() as sess1:\n  print(sess1.run(a))  # generates 'A1'\n  print(sess1.run(a))  # generates 'A2'\n  print(sess1.run(b))  # generates 'B1'\n  print(sess1.run(b))  # generates 'B2'\n\nprint(\"Session 2\")\nwith tf.Session() as sess2:\n  print(sess2.run(a))  # generates 'A1'\n  print(sess2.run(a))  # generates 'A2'\n  print(sess2.run(b))  # generates 'B1'\n  print(sess2.run(b))  # generates 'B2'\n</pre>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>seed</code>: integer.</li> </ul>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"ops\"><code>tf.contrib.graph_editor.ops(*args, **kwargs)</code></h3> <p>Helper to select operations.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>*args</code>: list of 1) regular expressions (compiled or not) or 2) (array of) tf.Operation. tf.Tensor instances are silently ignored.</li> <li>\n<code>**kwargs</code>: 'graph': tf.Graph in which to perform the regex query.This is required when using regex. 'positive_filter': an elem if selected only if positive_filter(elem) is True. This is optional. 'restrict_ops_regex': a regular expression is ignored if it doesn't start with the substring \"(?#ops)\".</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>list of tf.Operation</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if the optional keyword argument graph is not a tf.Graph or if an argument in args is not an (array of) tf.Operation or an (array of) tf.Tensor (silently ignored) or a string or a regular expression.</li> <li>\n<code>ValueError</code>: if one of the keyword arguments is unexpected or if a regular expression is used without passing a graph as a keyword argument.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/constant_op.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/constant_op.html</a>\n  </p>\n</div>\n","script_ops":"<h1 id=\"wraps-python-functions\">Wraps python functions</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#wraps-python-functions\">Wraps python functions</a></li> <ul> <li><a href=\"#script-language-operators\">Script Language Operators.</a></li> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#py_func\"><code>tf.py_func(func, inp, Tout, stateful=True, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"script-language-operators\">Script Language Operators.</h2> <p>TensorFlow provides allows you to wrap python/numpy functions as TensorFlow operators.</p>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"py_func\"><code>tf.py_func(func, inp, Tout, stateful=True, name=None)</code></h3> <p>Wraps a python function and uses it as a tensorflow op.</p> <p>Given a python function <code>func</code>, which takes numpy arrays as its inputs and returns numpy arrays as its outputs. E.g.,</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def my_func(x):\n  # x will be a numpy array with the contents of the placeholder below\n  return np.sinh(x)\ninp = tf.placeholder(tf.float32, [...])\ny = py_func(my_func, [inp], [tf.float32])\n</pre> <p>The above snippet constructs a tf graph which invokes a numpy sinh(x) as an op in the graph.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>func</code>: A python function.</li> <li>\n<code>inp</code>: A list of <code>Tensor</code>.</li> <li>\n<code>Tout</code>: A list of tensorflow data types indicating what <code>func</code> returns.</li> <li>\n<code>stateful</code>: A boolean indicating whether the function should be considered stateful or stateless. I.e. whether it, given the same input, will return the same output and at the same time does not change state in an observable way. Optimizations such as common subexpression elimination are only possible when operations are stateless.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A list of <code>Tensor</code> which <code>func</code> computes.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/script_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/script_ops.html</a>\n  </p>\n</div>\n","sparse_ops":"<h1 id=\"sparse-tensors\">Sparse Tensors</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#sparse-tensors\">Sparse Tensors</a></li> <ul> <li><a href=\"#sparse-tensor-representation\">Sparse Tensor Representation</a></li> <ul> <li><a href=\"#SparseTensor\"><code>class tf.SparseTensor</code></a></li> <li><a href=\"#SparseTensorValue\"><code>class tf.SparseTensorValue</code></a></li> </ul> <li><a href=\"#conversion\">Conversion</a></li> <ul> <li><a href=\"#sparse_to_dense\"><code>tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0, validate_indices=True, name=None)</code></a></li> <li><a href=\"#sparse_tensor_to_dense\"><code>tf.sparse_tensor_to_dense(sp_input, default_value=0, validate_indices=True, name=None)</code></a></li> <li><a href=\"#sparse_to_indicator\"><code>tf.sparse_to_indicator(sp_input, vocab_size, name=None)</code></a></li> <li><a href=\"#sparse_merge\"><code>tf.sparse_merge(sp_ids, sp_values, vocab_size, name=None, already_sorted=False)</code></a></li> </ul> <li><a href=\"#manipulation\">Manipulation</a></li> <ul> <li><a href=\"#sparse_concat\"><code>tf.sparse_concat(concat_dim, sp_inputs, name=None, expand_nonconcat_dim=False)</code></a></li> <li><a href=\"#sparse_reorder\"><code>tf.sparse_reorder(sp_input, name=None)</code></a></li> <li><a href=\"#sparse_reshape\"><code>tf.sparse_reshape(sp_input, shape, name=None)</code></a></li> <li><a href=\"#sparse_split\"><code>tf.sparse_split(split_dim, num_split, sp_input, name=None)</code></a></li> <li><a href=\"#sparse_retain\"><code>tf.sparse_retain(sp_input, to_retain)</code></a></li> <li><a href=\"#sparse_reset_shape\"><code>tf.sparse_reset_shape(sp_input, new_shape=None)</code></a></li> <li><a href=\"#sparse_fill_empty_rows\"><code>tf.sparse_fill_empty_rows(sp_input, default_value, name=None)</code></a></li> </ul> <li><a href=\"#reduction\">Reduction</a></li> <ul> <li><a href=\"#sparse_reduce_sum\"><code>tf.sparse_reduce_sum(sp_input, reduction_axes=None, keep_dims=False)</code></a></li> </ul> <li><a href=\"#math-operations\">Math Operations</a></li> <ul> <li><a href=\"#sparse_add\"><code>tf.sparse_add(a, b, thresh=0)</code></a></li> <li><a href=\"#sparse_softmax\"><code>tf.sparse_softmax(sp_input, name=None)</code></a></li> <li><a href=\"#sparse_tensor_dense_matmul\"><code>tf.sparse_tensor_dense_matmul(sp_a, b, adjoint_a=False, adjoint_b=False, name=None)</code></a></li> <li><a href=\"#sparse_maximum\"><code>tf.sparse_maximum(sp_a, sp_b, name=None)</code></a></li> <li><a href=\"#sparse_minimum\"><code>tf.sparse_minimum(sp_a, sp_b, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"sparse-tensor-representation\">Sparse Tensor Representation</h2> <p>TensorFlow supports a <code>SparseTensor</code> representation for data that is sparse in multiple dimensions. Contrast this representation with <code>IndexedSlices</code>, which is efficient for representing tensors that are sparse in their first dimension, and dense along all other dimensions.</p>  <h3 id=\"SparseTensor\"><code>class tf.SparseTensor</code></h3> <p>Represents a sparse tensor.</p> <p>TensorFlow represents a sparse tensor as three separate dense tensors: <code>indices</code>, <code>values</code>, and <code>shape</code>. In Python, the three tensors are collected into a <code>SparseTensor</code> class for ease of use. If you have separate <code>indices</code>, <code>values</code>, and <code>shape</code> tensors, wrap them in a <code>SparseTensor</code> object before passing to the ops below.</p> <p>Concretely, the sparse tensor <code>SparseTensor(indices, values, shape)</code> is</p> <ul> <li>\n<code>indices</code>: A 2-D int64 tensor of shape <code>[N, ndims]</code>.</li> <li>\n<code>values</code>: A 1-D tensor of any type and shape <code>[N]</code>.</li> <li>\n<code>shape</code>: A 1-D int64 tensor of shape <code>[ndims]</code>.</li> </ul> <p>where <code>N</code> and <code>ndims</code> are the number of values, and number of dimensions in the <code>SparseTensor</code> respectively.</p> <p>The corresponding dense tensor satisfies</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">dense.shape = shape\ndense[tuple(indices[i])] = values[i]\n</pre> <p>By convention, <code>indices</code> should be sorted in row-major order (or equivalently lexicographic order on the tuples <code>indices[i]</code>). This is not enforced when <code>SparseTensor</code> objects are constructed, but most ops assume correct ordering. If the ordering of sparse tensor <code>st</code> is wrong, a fixed version can be obtained by calling <code>tf.sparse_reorder(st)</code>.</p> <p>Example: The sparse tensor</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], shape=[3, 4])\n</pre> <p>represents the dense tensor</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">[[1, 0, 0, 0]\n [0, 0, 2, 0]\n [0, 0, 0, 0]]\n</pre>  <h4 id=\"SparseTensor.__init__\"><code>tf.SparseTensor.__init__(indices, values, shape)</code></h4> <p>Creates a <code>SparseTensor</code>.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>indices</code>: A 2-D int64 tensor of shape <code>[N, ndims]</code>.</li> <li>\n<code>values</code>: A 1-D tensor of any type and shape <code>[N]</code>.</li> <li>\n<code>shape</code>: A 1-D int64 tensor of shape <code>[ndims]</code>.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>SparseTensor</code></p>  <h4 id=\"SparseTensor.indices\"><code>tf.SparseTensor.indices</code></h4> <p>The indices of non-zero values in the represented dense tensor.</p>  <h5 id=\"returns-2\">Returns:</h5> <p>A 2-D Tensor of int64 with shape <code>[N, ndims]</code>, where <code>N</code> is the number of non-zero values in the tensor, and <code>ndims</code> is the rank.</p>  <h4 id=\"SparseTensor.values\"><code>tf.SparseTensor.values</code></h4> <p>The non-zero values in the represented dense tensor.</p>  <h5 id=\"returns-3\">Returns:</h5> <p>A 1-D Tensor of any data type.</p>  <h4 id=\"SparseTensor.shape\"><code>tf.SparseTensor.shape</code></h4> <p>A 1-D Tensor of int64 representing the shape of the dense tensor.</p>  <h4 id=\"SparseTensor.dtype\"><code>tf.SparseTensor.dtype</code></h4> <p>The <code>DType</code> of elements in this tensor.</p>  <h4 id=\"SparseTensor.op\"><code>tf.SparseTensor.op</code></h4> <p>The <code>Operation</code> that produces <code>values</code> as an output.</p>  <h4 id=\"SparseTensor.graph\"><code>tf.SparseTensor.graph</code></h4> <p>The <code>Graph</code> that contains the index, value, and shape tensors.</p>  <h4 id=\"other-methods\">Other Methods</h4>  <h4 id=\"SparseTensor.eval\"><code>tf.SparseTensor.eval(feed_dict=None, session=None)</code></h4> <p>Evaluates this sparse tensor in a <code>Session</code>.</p> <p>Calling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.</p> <p><em>N.B.</em> Before invoking <code>SparseTensor.eval()</code>, its graph must have been launched in a session, and either a default session must be available, or <code>session</code> must be specified explicitly.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>feed_dict</code>: A dictionary that maps <code>Tensor</code> objects to feed values. See <a href=\"client#Session.run\"><code>Session.run()</code></a> for a description of the valid feed values.</li> <li>\n<code>session</code>: (Optional.) The <code>Session</code> to be used to evaluate this sparse tensor. If none, the default session will be used.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>SparseTensorValue</code> object.</p>  <h4 id=\"SparseTensor.from_value\"><code>tf.SparseTensor.from_value(cls, sparse_tensor_value)</code></h4>  <h3 id=\"SparseTensorValue\"><code>class tf.SparseTensorValue</code></h3> <p>SparseTensorValue(indices, values, shape)</p>  <h4 id=\"SparseTensorValue.indices\"><code>tf.SparseTensorValue.indices</code></h4> <p>Alias for field number 0</p>  <h4 id=\"SparseTensorValue.shape\"><code>tf.SparseTensorValue.shape</code></h4> <p>Alias for field number 2</p>  <h4 id=\"SparseTensorValue.values\"><code>tf.SparseTensorValue.values</code></h4> <p>Alias for field number 1</p> <h2 id=\"conversion\">Conversion</h2>  <h3 id=\"sparse_to_dense\"><code>tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0, validate_indices=True, name=None)</code></h3> <p>Converts a sparse representation into a dense tensor.</p> <p>Builds an array <code>dense</code> with shape <code>output_shape</code> such that</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># If sparse_indices is scalar\ndense[i] = (i == sparse_indices ? sparse_values : default_value)\n\n# If sparse_indices is a vector, then for each i\ndense[sparse_indices[i]] = sparse_values[i]\n\n# If sparse_indices is an n by d matrix, then for each i in [0, n)\ndense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\n</pre> <p>All other values in <code>dense</code> are set to <code>default_value</code>. If <code>sparse_values</code> is a scalar, all sparse indices are set to this single value.</p> <p>Indices should be sorted in lexicographic order, and indices must not contain any repeats. If <code>validate_indices</code> is True, these properties are checked during execution.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>sparse_indices</code>: A 0-D, 1-D, or 2-D <code>Tensor</code> of type <code>int32</code> or <code>int64</code>. <code>sparse_indices[i]</code> contains the complete index where <code>sparse_values[i]</code> will be placed.</li> <li>\n<code>output_shape</code>: A 1-D <code>Tensor</code> of the same type as <code>sparse_indices</code>. Shape of the dense output tensor.</li> <li>\n<code>sparse_values</code>: A 0-D or 1-D <code>Tensor</code>. Values corresponding to each row of <code>sparse_indices</code>, or a scalar value to be used for all sparse indices.</li> <li>\n<code>default_value</code>: A 0-D <code>Tensor</code> of the same type as <code>sparse_values</code>. Value to set for indices not specified in <code>sparse_indices</code>. Defaults to zero.</li> <li>\n<code>validate_indices</code>: A boolean value. If True, indices are checked to make sure they are sorted in lexicographic order and that there are no repeats.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>Dense <code>Tensor</code> of shape <code>output_shape</code>. Has the same type as <code>sparse_values</code>.</p>  <h3 id=\"sparse_tensor_to_dense\"><code>tf.sparse_tensor_to_dense(sp_input, default_value=0, validate_indices=True, name=None)</code></h3> <p>Converts a <code>SparseTensor</code> into a dense tensor.</p> <p>This op is a convenience wrapper around <code>sparse_to_dense</code> for <code>SparseTensor</code>s.</p> <p>For example, if <code>sp_input</code> has shape <code>[3, 5]</code> and non-empty string values:</p> <pre class=\"\">[0, 1]: a\n[0, 3]: b\n[2, 0]: c\n</pre> <p>and <code>default_value</code> is <code>x</code>, then the output will be a dense <code>[3, 5]</code> string tensor with values:</p> <pre class=\"\">[[x a x b x]\n [x x x x x]\n [c x x x x]]\n</pre> <p>Indices must be without repeats. This is only tested if validate_indices is True.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>sp_input</code>: The input <code>SparseTensor</code>.</li> <li>\n<code>default_value</code>: Scalar value to set for indices not specified in <code>sp_input</code>. Defaults to zero.</li> <li>\n<code>validate_indices</code>: A boolean value. If <code>True</code>, indices are checked to make sure they are sorted in lexicographic order and that there are no repeats.</li> <li>\n<code>name</code>: A name prefix for the returned tensors (optional).</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A dense tensor with shape <code>sp_input.shape</code> and values specified by the non-empty values in <code>sp_input</code>. Indices not in <code>sp_input</code> are assigned <code>default_value</code>.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_input</code> is not a <code>SparseTensor</code>.</li> </ul>  <h3 id=\"sparse_to_indicator\"><code>tf.sparse_to_indicator(sp_input, vocab_size, name=None)</code></h3> <p>Converts a <code>SparseTensor</code> of ids into a dense bool indicator tensor.</p> <p>The last dimension of <code>sp_input.indices</code> is discarded and replaced with the values of <code>sp_input</code>. If <code>sp_input.shape = [D0, D1, ..., Dn, K]</code>, then <code>output.shape = [D0, D1, ..., Dn, vocab_size]</code>, where</p> <pre class=\"\">output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True\n</pre> <p>and False elsewhere in <code>output</code>.</p> <p>For example, if <code>sp_input.shape = [2, 3, 4]</code> with non-empty values:</p> <pre class=\"\">[0, 0, 0]: 0\n[0, 1, 0]: 10\n[1, 0, 3]: 103\n[1, 1, 2]: 150\n[1, 1, 3]: 149\n[1, 1, 4]: 150\n[1, 2, 1]: 121\n</pre> <p>and <code>vocab_size = 200</code>, then the output will be a <code>[2, 3, 200]</code> dense bool tensor with False everywhere except at positions</p> <pre class=\"\">(0, 0, 0), (0, 1, 10), (1, 0, 103), (1, 1, 149), (1, 1, 150),\n(1, 2, 121).\n</pre> <p>Note that repeats are allowed in the input SparseTensor. This op is useful for converting <code>SparseTensor</code>s into dense formats for compatibility with ops that expect dense tensors.</p> <p>The input <code>SparseTensor</code> must be in row-major order.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>sp_input</code>: A <code>SparseTensor</code> with <code>values</code> property of type <code>int32</code> or <code>int64</code>.</li> <li>\n<code>vocab_size</code>: A scalar int64 Tensor (or Python int) containing the new size of the last dimension, <code>all(0 &lt;= sp_input.values &lt; vocab_size)</code>.</li> <li>\n<code>name</code>: A name prefix for the returned tensors (optional)</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A dense bool indicator tensor representing the indices with specified value.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_input</code> is not a <code>SparseTensor</code>.</li> </ul>  <h3 id=\"sparse_merge\"><code>tf.sparse_merge(sp_ids, sp_values, vocab_size, name=None, already_sorted=False)</code></h3> <p>Combines a batch of feature ids and values into a single <code>SparseTensor</code>.</p> <p>The most common use case for this function occurs when feature ids and their corresponding values are stored in <code>Example</code> protos on disk. <code>parse_example</code> will return a batch of ids and a batch of values, and this function joins them into a single logical <code>SparseTensor</code> for use in functions such as <code>sparse_tensor_dense_matmul</code>, <code>sparse_to_dense</code>, etc.</p> <p>The <code>SparseTensor</code> returned by this function has the following properties:</p> <ul> <li>\n<code>indices</code> is equivalent to <code>sp_ids.indices</code> with the last dimension discarded and replaced with <code>sp_ids.values</code>.</li> <li>\n<code>values</code> is simply <code>sp_values.values</code>.</li> <li>If <code>sp_ids.shape = [D0, D1, ..., Dn, K]</code>, then <code>output.shape = [D0, D1, ..., Dn, vocab_size]</code>.</li> </ul> <p>For example, consider the following feature vectors:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">vector1 = [-3, 0, 0, 0, 0, 0]\nvector2 = [ 0, 1, 0, 4, 1, 0]\nvector3 = [ 5, 0, 0, 9, 0, 0]\n</pre> <p>These might be stored sparsely in the following Example protos by storing only the feature ids (column number if the vectors are treated as a matrix) of the non-zero elements and the corresponding values:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">examples = [Example(features={\n                \"ids\": Feature(int64_list=Int64List(value=[0])),\n                \"values\": Feature(float_list=FloatList(value=[-3]))}),\n            Example(features={\n                \"ids\": Feature(int64_list=Int64List(value=[1, 4, 3])),\n                \"values\": Feature(float_list=FloatList(value=[1, 1, 4]))}),\n            Example(features={\n                \"ids\": Feature(int64_list=Int64List(value=[0, 3])),\n                \"values\": Feature(float_list=FloatList(value=[5, 9]))})]\n</pre> <p>The result of calling parse_example on these examples will produce a dictionary with entries for \"ids\" and \"values\". Passing those two objects to this function along with vocab_size=6, will produce a <code>SparseTensor</code> that sparsely represents all three instances. Namely, the <code>indices</code> property will contain the coordinates of the non-zero entries in the feature matrix (the first dimension is the row number in the matrix, i.e., the index within the batch, and the second dimension is the column number, i.e., the feature id); <code>values</code> will contain the actual values. <code>shape</code> will be the shape of the original matrix, i.e., (3, 6). For our example above, the output will be equal to:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">SparseTensor(indices=[[0, 0], [1, 1], [1, 3], [1, 4], [2, 0], [2, 3]],\n             values=[-3, 1, 4, 1, 5, 9],\n             shape=[3, 6])\n</pre>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>sp_ids</code>: A <code>SparseTensor</code> with <code>values</code> property of type <code>int32</code> or <code>int64</code>.</li> <li>\n<code>sp_values</code>: A<code>SparseTensor</code> of any type.</li> <li>\n<code>vocab_size</code>: A scalar <code>int64</code> Tensor (or Python int) containing the new size of the last dimension, <code>all(0 &lt;= sp_ids.values &lt; vocab_size)</code>.</li> <li>\n<code>name</code>: A name prefix for the returned tensors (optional)</li> <li>\n<code>already_sorted</code>: A boolean to specify whether the per-batch values in <code>sp_values</code> are already sorted. If so skip sorting, False by default (optional).</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A <code>SparseTensor</code> compactly representing a batch of feature ids and values, useful for passing to functions that expect such a <code>SparseTensor</code>.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_ids</code> or <code>sp_values</code> are not a <code>SparseTensor</code>.</li> </ul> <h2 id=\"manipulation\">Manipulation</h2>  <h3 id=\"sparse_concat\"><code>tf.sparse_concat(concat_dim, sp_inputs, name=None, expand_nonconcat_dim=False)</code></h3> <p>Concatenates a list of <code>SparseTensor</code> along the specified dimension.</p> <p>Concatenation is with respect to the dense versions of each sparse input. It is assumed that each inputs is a <code>SparseTensor</code> whose elements are ordered along increasing dimension number.</p> <p>If expand_nonconcat_dim is False, all inputs' shapes must match, except for the concat dimension. If expand_nonconcat_dim is True, then inputs' shapes are allowd to vary among all inputs.</p> <p>The <code>indices</code>, <code>values</code>, and <code>shapes</code> lists must have the same length.</p> <p>If expand_nonconcat_dim is False, then the output shape is identical to the inputs', except along the concat dimension, where it is the sum of the inputs' sizes along that dimension.</p> <p>If expand_nonconcat_dim is True, then the output shape along the non-concat dimensions will be expand to be the largest among all inputs, and it is the sum of the inputs sizes along the concat dimension.</p> <p>The output elements will be resorted to preserve the sort order along increasing dimension number.</p> <p>This op runs in <code>O(M log M)</code> time, where <code>M</code> is the total number of non-empty values across all inputs. This is due to the need for an internal sort in order to concatenate efficiently across an arbitrary dimension.</p> <p>For example, if <code>concat_dim = 1</code> and the inputs are</p> <pre class=\"\">sp_inputs[0]: shape = [2, 3]\n[0, 2]: \"a\"\n[1, 0]: \"b\"\n[1, 1]: \"c\"\n\nsp_inputs[1]: shape = [2, 4]\n[0, 1]: \"d\"\n[0, 2]: \"e\"\n</pre> <p>then the output will be</p> <pre class=\"\">shape = [2, 7]\n[0, 2]: \"a\"\n[0, 4]: \"d\"\n[0, 5]: \"e\"\n[1, 0]: \"b\"\n[1, 1]: \"c\"\n</pre> <p>Graphically this is equivalent to doing</p> <pre class=\"\">[    a] concat [  d e  ] = [    a   d e  ]\n[b c  ]        [       ]   [b c          ]\n</pre> <p>Another example, if 'concat_dim = 1' and the inputs are</p> <pre class=\"\">sp_inputs[0]: shape = [3, 3]\n[0, 2]: \"a\"\n[1, 0]: \"b\"\n[2, 1]: \"c\"\n\nsp_inputs[1]: shape = [2, 4]\n[0, 1]: \"d\"\n[0, 2]: \"e\"\n</pre> <p>if expand_nonconcat_dim = False, this will result in an error. But if expand_nonconcat_dim = True, this will result in:</p> <pre class=\"\">shape = [3, 7]\n[0, 2]: \"a\"\n[0, 4]: \"d\"\n[0, 5]: \"e\"\n[1, 0]: \"b\"\n[2, 1]: \"c\"\n</pre> <p>Graphically this is equivalent to doing</p> <pre class=\"\">[    a] concat [  d e  ] = [    a   d e  ]\n[b    ]        [       ]   [b            ]\n[  c  ]                    [  c          ]\n</pre>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>concat_dim</code>: Dimension to concatenate along.</li> <li>\n<code>sp_inputs</code>: List of <code>SparseTensor</code> to concatenate.</li> <li>\n<code>name</code>: A name prefix for the returned tensors (optional).</li> <li>\n<code>expand_nonconcat_dim</code>: Whether to allow the expansion in the non-concat dimensions. Defaulted to False.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A <code>SparseTensor</code> with the concatenated output.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_inputs</code> is not a list of <code>SparseTensor</code>.</li> </ul>  <h3 id=\"sparse_reorder\"><code>tf.sparse_reorder(sp_input, name=None)</code></h3> <p>Reorders a <code>SparseTensor</code> into the canonical, row-major ordering.</p> <p>Note that by convention, all sparse ops preserve the canonical ordering along increasing dimension number. The only time ordering can be violated is during manual manipulation of the indices and values to add entries.</p> <p>Reordering does not affect the shape of the <code>SparseTensor</code>.</p> <p>For example, if <code>sp_input</code> has shape <code>[4, 5]</code> and <code>indices</code> / <code>values</code>:</p> <pre class=\"\">[0, 3]: b\n[0, 1]: a\n[3, 1]: d\n[2, 0]: c\n</pre> <p>then the output will be a <code>SparseTensor</code> of shape <code>[4, 5]</code> and <code>indices</code> / <code>values</code>:</p> <pre class=\"\">[0, 1]: a\n[0, 3]: b\n[2, 0]: c\n[3, 1]: d\n</pre>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>sp_input</code>: The input <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name prefix for the returned tensors (optional)</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A <code>SparseTensor</code> with the same shape and non-empty values, but in canonical ordering.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_input</code> is not a <code>SparseTensor</code>.</li> </ul>  <h3 id=\"sparse_reshape\"><code>tf.sparse_reshape(sp_input, shape, name=None)</code></h3> <p>Reshapes a <code>SparseTensor</code> to represent values in a new dense shape.</p> <p>This operation has the same semantics as <code>reshape</code> on the represented dense tensor. The indices of non-empty values in <code>sp_input</code> are recomputed based on the new dense shape, and a new <code>SparseTensor</code> is returned containing the new indices and new shape. The order of non-empty values in <code>sp_input</code> is unchanged.</p> <p>If one component of <code>shape</code> is the special value -1, the size of that dimension is computed so that the total dense size remains constant. At most one component of <code>shape</code> can be -1. The number of dense elements implied by <code>shape</code> must be the same as the number of dense elements originally represented by <code>sp_input</code>.</p> <p>For example, if <code>sp_input</code> has shape <code>[2, 3, 6]</code> and <code>indices</code> / <code>values</code>:</p> <pre class=\"\">[0, 0, 0]: a\n[0, 0, 1]: b\n[0, 1, 0]: c\n[1, 0, 0]: d\n[1, 2, 3]: e\n</pre> <p>and <code>shape</code> is <code>[9, -1]</code>, then the output will be a <code>SparseTensor</code> of shape <code>[9, 4]</code> and <code>indices</code> / <code>values</code>:</p> <pre class=\"\">[0, 0]: a\n[0, 1]: b\n[1, 2]: c\n[4, 2]: d\n[8, 1]: e\n</pre>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>sp_input</code>: The input <code>SparseTensor</code>.</li> <li>\n<code>shape</code>: A 1-D (vector) int64 <code>Tensor</code> specifying the new dense shape of the represented <code>SparseTensor</code>.</li> <li>\n<code>name</code>: A name prefix for the returned tensors (optional)</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A <code>SparseTensor</code> with the same non-empty values but with indices calculated by the new dense shape.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_input</code> is not a <code>SparseTensor</code>.</li> </ul>  <h3 id=\"sparse_split\"><code>tf.sparse_split(split_dim, num_split, sp_input, name=None)</code></h3> <p>Split a <code>SparseTensor</code> into <code>num_split</code> tensors along <code>split_dim</code>.</p> <p>If the <code>sp_input.shape[split_dim]</code> is not an integer multiple of <code>num_split</code> each slice starting from 0:<code>shape[split_dim] % num_split</code> gets extra one dimension. For example, if <code>split_dim = 1</code> and <code>num_split = 2</code> and the input is:</p> <pre class=\"\">input_tensor = shape = [2, 7]\n[    a   d e  ]\n[b c          ]\n</pre> <p>Graphically the output tensors are:</p> <pre class=\"\">output_tensor[0] =\n[    a ]\n[b c   ]\n\noutput_tensor[1] =\n[ d e  ]\n[      ]\n</pre>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>split_dim</code>: A 0-D <code>int32</code> <code>Tensor</code>. The dimension along which to split.</li> <li>\n<code>num_split</code>: A Python integer. The number of ways to split.</li> <li>\n<code>sp_input</code>: The <code>SparseTensor</code> to split.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p><code>num_split</code> <code>SparseTensor</code> objects resulting from splitting <code>value</code>.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_input</code> is not a <code>SparseTensor</code>.</li> </ul>  <h3 id=\"sparse_retain\"><code>tf.sparse_retain(sp_input, to_retain)</code></h3> <p>Retains specified non-empty values within a <code>SparseTensor</code>.</p> <p>For example, if <code>sp_input</code> has shape <code>[4, 5]</code> and 4 non-empty string values:</p> <pre class=\"\">[0, 1]: a\n[0, 3]: b\n[2, 0]: c\n[3, 1]: d\n</pre> <p>and <code>to_retain = [True, False, False, True]</code>, then the output will be a <code>SparseTensor</code> of shape <code>[4, 5]</code> with 2 non-empty values:</p> <pre class=\"\">[0, 1]: a\n[3, 1]: d\n</pre>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>sp_input</code>: The input <code>SparseTensor</code> with <code>N</code> non-empty elements.</li> <li>\n<code>to_retain</code>: A bool vector of length <code>N</code> with <code>M</code> true values.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A <code>SparseTensor</code> with the same shape as the input and <code>M</code> non-empty elements corresponding to the true positions in <code>to_retain</code>.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_input</code> is not a <code>SparseTensor</code>.</li> </ul>  <h3 id=\"sparse_reset_shape\"><code>tf.sparse_reset_shape(sp_input, new_shape=None)</code></h3> <p>Resets the shape of a <code>SparseTensor</code> with indices and values unchanged.</p> <p>If <code>new_shape</code> is None, returns a copy of <code>sp_input</code> with its shape reset to the tight bounding box of <code>sp_input</code>.</p> <p>If <code>new_shape</code> is provided, then it must be larger or equal in all dimensions compared to the shape of <code>sp_input</code>. When this condition is met, the returned SparseTensor will have its shape reset to <code>new_shape</code> and its indices and values unchanged from that of <code>sp_input.</code></p> <p>For example:</p> <p>Consider a <code>sp_input</code> with shape [2, 3, 5]:</p> <pre class=\"\">[0, 0, 1]: a\n[0, 1, 0]: b\n[0, 2, 2]: c\n[1, 0, 3]: d\n</pre> <ul> <li><p>It is an error to set <code>new_shape</code> as [3, 7] since this represents a rank-2 tensor while <code>sp_input</code> is rank-3. This is either a ValueError during graph construction (if both shapes are known) or an OpError during run time.</p></li> <li><p>Setting <code>new_shape</code> as [2, 3, 6] will be fine as this shape is larger or eqaul in every dimension compared to the original shape [2, 3, 5].</p></li> <li><p>On the other hand, setting new_shape as [2, 3, 4] is also an error: The third dimension is smaller than the original shape <a href=\"and%20an%0a%60invalidargumenterror%60%20will%20be%20raised\">2, 3, 5</a>.</p></li> <li><p>If <code>new_shape</code> is None, the returned SparseTensor will have a shape [2, 3, 4], which is the tight bounding box of <code>sp_input</code>.</p></li> </ul>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>sp_input</code>: The input <code>SparseTensor</code>.</li> <li>\n<code>new_shape</code>: None or a vector representing the new shape for the returned <code>SpraseTensor</code>.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A <code>SparseTensor</code> indices and values unchanged from <code>input_sp</code>. Its shape is <code>new_shape</code> if that is set. Otherwise it is the tight bounding box of <code>input_sp</code></p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_input</code> is not a <code>SparseTensor</code>.</li> <li>\n<code>ValueError</code>: If <code>new_shape</code> represents a tensor with a different rank from that of <code>sp_input</code> (if shapes are known when graph is constructed).</li> <li>\n<code>OpError</code>: <ul> <li>If <code>new_shape</code> has dimension sizes that are too small.</li> <li>If shapes are not known during graph construction time, and during run time it is found out that the ranks do not match.</li> </ul>\n</li> </ul>  <h3 id=\"sparse_fill_empty_rows\"><code>tf.sparse_fill_empty_rows(sp_input, default_value, name=None)</code></h3> <p>Fills empty rows in the input 2-D <code>SparseTensor</code> with a default value.</p> <p>This op adds entries with the specified <code>default_value</code> at index <code>[row, 0]</code> for any row in the input that does not already have a value.</p> <p>For example, suppose <code>sp_input</code> has shape <code>[5, 6]</code> and non-empty values:</p> <pre class=\"\">[0, 1]: a\n[0, 3]: b\n[2, 0]: c\n[3, 1]: d\n</pre> <p>Rows 1 and 4 are empty, so the output will be of shape <code>[5, 6]</code> with values:</p> <pre class=\"\">[0, 1]: a\n[0, 3]: b\n[1, 0]: default_value\n[2, 0]: c\n[3, 1]: d\n[4, 0]: default_value\n</pre> <p>Note that the input may have empty columns at the end, with no effect on this op.</p> <p>The output <code>SparseTensor</code> will be in row-major order and will have the same shape as the input.</p> <p>This op also returns an indicator vector such that</p> <pre class=\"\">empty_row_indicator[i] = True iff row i was an empty row.\n</pre>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>sp_input</code>: A <code>SparseTensor</code> with shape <code>[N, M]</code>.</li> <li>\n<code>default_value</code>: The value to fill for empty rows, with the same type as <code>sp_input.</code>\n</li> <li>\n<code>name</code>: A name prefix for the returned tensors (optional)</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <ul> <li>\n<code>sp_ordered_output</code>: A <code>SparseTensor</code> with shape <code>[N, M]</code>, and with all empty rows filled in with <code>default_value</code>.</li> <li>\n<code>empty_row_indicator</code>: A bool vector of length <code>N</code> indicating whether each input row was empty.</li> </ul>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sp_input</code> is not a <code>SparseTensor</code>.</li> </ul> <h2 id=\"reduction\">Reduction</h2>  <h3 id=\"sparse_reduce_sum\"><code>tf.sparse_reduce_sum(sp_input, reduction_axes=None, keep_dims=False)</code></h3> <p>Computes the sum of elements across dimensions of a SparseTensor.</p> <p>This Op takes a SparseTensor and is the sparse counterpart to <code>tf.reduce_sum()</code>. In particular, this Op also returns a dense <code>Tensor</code> instead of a sparse one.</p> <p>Reduces <code>sp_input</code> along the dimensions given in <code>reduction_axes</code>. Unless <code>keep_dims</code> is true, the rank of the tensor is reduced by 1 for each entry in <code>reduction_axes</code>. If <code>keep_dims</code> is true, the reduced dimensions are retained with length 1.</p> <p>If <code>reduction_axes</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned. Additionally, the axes can be negative, similar to the indexing rules in Python.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'x' represents [[1, ?, 1]\n#                 [?, 1, ?]]\n# where ? is implictly-zero.\ntf.sparse_reduce_sum(x) ==&gt; 3\ntf.sparse_reduce_sum(x, 0) ==&gt; [1, 1, 1]\ntf.sparse_reduce_sum(x, 1) ==&gt; [2, 1]  # Can also use -1 as the axis.\ntf.sparse_reduce_sum(x, 1, keep_dims=True) ==&gt; [[2], [1]]\ntf.sparse_reduce_sum(x, [0, 1]) ==&gt; 3\n</pre>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>sp_input</code>: The SparseTensor to reduce. Should have numeric type.</li> <li>\n<code>reduction_axes</code>: The dimensions to reduce; list or scalar. If <code>None</code> (the default), reduces all dimensions.</li> <li>\n<code>keep_dims</code>: If true, retain reduced dimensions with length 1.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>The reduced Tensor.</p>  <h2 id=\"math-operations\">Math Operations</h2>  <h3 id=\"sparse_add\"><code>tf.sparse_add(a, b, thresh=0)</code></h3> <p>Adds two tensors, at least one of each is a <code>SparseTensor</code>.</p> <p>If one <code>SparseTensor</code> and one <code>Tensor</code> are passed in, returns a <code>Tensor</code>. If both arguments are <code>SparseTensor</code>s, this returns a <code>SparseTensor</code>. The order of arguments does not matter. Use vanilla <code>tf.add()</code> for adding two dense <code>Tensor</code>s.</p> <p>The indices of any input <code>SparseTensor</code> are assumed ordered in standard lexicographic order. If this is not the case, before this step run <code>SparseReorder</code> to restore index ordering.</p> <p>If both arguments are sparse, we perform \"clipping\" as follows. By default, if two values sum to zero at some index, the output <code>SparseTensor</code> would still include that particular location in its index, storing a zero in the corresponding value slot. To override this, callers can specify <code>thresh</code>, indicating that if the sum has a magnitude strictly smaller than <code>thresh</code>, its corresponding value and index would then not be included. In particular, <code>thresh == 0.0</code> (default) means everything is kept and actual thresholding happens only for a positive value.</p> <p>For example, suppose the logical sum of two sparse operands is (densified):</p> <pre class=\"\">[       2]\n[.1     0]\n[ 6   -.2]\n</pre> <p>Then,</p> <pre class=\"\">- thresh == 0 (the default): all 5 index/value pairs will be returned.\n- thresh == 0.11: only .1 and 0  will vanish, and the remaining three\n    index/value pairs will be returned.\n- thresh == 0.21: .1, 0, and -.2 will vanish.\n</pre>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>a</code>: The first operand; <code>SparseTensor</code> or <code>Tensor</code>.</li> <li>\n<code>b</code>: The second operand; <code>SparseTensor</code> or <code>Tensor</code>. At least one operand must be sparse.</li> <li>\n<code>thresh</code>: A 0-D <code>Tensor</code>. The magnitude threshold that determines if an output value/index pair takes space. Its dtype should match that of the values if they are real; if the latter are complex64/complex128, then the dtype should be float32/float64, correspondingly.</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A <code>SparseTensor</code> or a <code>Tensor</code>, representing the sum.</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If both <code>a</code> and <code>b</code> are <code>Tensor</code>s. Use <code>tf.add()</code> instead.</li> </ul>  <h3 id=\"sparse_softmax\"><code>tf.sparse_softmax(sp_input, name=None)</code></h3> <p>Applies softmax to a batched N-D <code>SparseTensor</code>.</p> <p>The inputs represent an N-D SparseTensor with logical shape <code>[..., B, C]</code> (where <code>N &gt;= 2</code>), and with indices sorted in the canonical lexicographic order.</p> <p>This op is equivalent to applying the normal <code>tf.nn.softmax()</code> to each innermost logical submatrix with shape <code>[B, C]</code>, but with the catch that <em>the implicitly zero elements do not participate</em>. Specifically, the algorithm is equivalent to:</p> <p>(1) Applies <code>tf.nn.softmax()</code> to a densified view of each innermost submatrix with shape <code>[B, C]</code>, along the size-C dimension; (2) Masks out the original implicitly-zero locations; (3) Renormalizes the remaining elements.</p> <p>Hence, the <code>SparseTensor</code> result has exactly the same non-zero indices and shape.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># First batch:\n# [?   e.]\n# [1.  ? ]\n# Second batch:\n# [e   ? ]\n# [e   e ]\nshape = [2, 2, 2]  # 3-D SparseTensor\nvalues = np.asarray([[[0., np.e], [1., 0.]], [[np.e, 0.], [np.e, np.e]]])\nindices = np.vstack(np.where(values)).astype(np.int64).T\n\nresult = tf.sparse_softmax(tf.SparseTensor(indices, values, shape))\n# ...returning a 3-D SparseTensor, equivalent to:\n# [?   1.]     [1    ?]\n# [1.  ? ] and [.5  .5]\n# where ? means implicitly zero.\n</pre>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>sp_input</code>: N-D <code>SparseTensor</code>, where <code>N &gt;= 2</code>.</li> <li>\n<code>name</code>: optional name of the operation.</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <ul> <li>\n<code>output</code>: N-D <code>SparseTensor</code> representing the results.</li> </ul>  <h3 id=\"sparse_tensor_dense_matmul\"><code>tf.sparse_tensor_dense_matmul(sp_a, b, adjoint_a=False, adjoint_b=False, name=None)</code></h3> <p>Multiply SparseTensor (of rank 2) \"A\" by dense matrix \"B\".</p> <p>No validity checking is performed on the indices of A. However, the following input format is recommended for optimal behavior:</p> <p>if adjoint_a == false: A should be sorted in lexicographically increasing order. Use sparse_reorder if you're not sure. if adjoint_a == true: A should be sorted in order of increasing dimension 1 (i.e., \"column major\" order instead of \"row major\" order).</p> <p>Deciding when to use sparse_tensor_dense_matmul vs. matmul(sp_a=True):</p> <p>There are a number of questions to ask in the decision process, including:</p> <ul> <li>Will the SparseTensor A fit in memory if densified?</li> <li>Is the column count of the product large (&gt;&gt; 1)?</li> <li>Is the density of A larger than approximately 15%?</li> </ul> <p>If the answer to several of these questions is yes, consider converting the SparseTensor to a dense one and using tf.matmul with sp_a=True.</p> <p>This operation tends to perform well when A is more sparse, if the column size of the product is small (e.g. matrix-vector multiplication), if sp_a.shape takes on large values.</p> <p>Below is a rough speed comparison between sparse_tensor_dense_matmul, labelled 'sparse', and matmul(sp_a=True), labelled 'dense'. For purposes of the comparison, the time spent converting from a SparseTensor to a dense Tensor is not included, so it is overly conservative with respect to the time ratio.</p> <p>Benchmark system: CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB GPU: NVidia Tesla k40c</p> <p>Compiled with: -c opt --config=cuda --copt=-mavx</p> <pre class=\"lang-tensorflow/python/sparse_tensor_dense_matmul_op_test no-auto-prettify\">A sparse [m, k] with % nonzero values between 1% and 80%\nB dense [k, n]\n\n% nnz    n       gpu     m       k       dt(dense)       dt(sparse)      dt(sparse)/dt(dense)\n0.01     1       True    100     100     0.000221166     0.00010154      0.459112\n0.01     1       True    100     1000    0.00033858      0.000109275     0.322745\n0.01     1       True    1000    100     0.000310557     9.85661e-05     0.317385\n0.01     1       True    1000    1000    0.0008721       0.000100875     0.115669\n0.01     1       False   100     100     0.000208085     0.000107603     0.51711\n0.01     1       False   100     1000    0.000327112     9.51118e-05     0.290762\n0.01     1       False   1000    100     0.000308222     0.00010345      0.335635\n0.01     1       False   1000    1000    0.000865721     0.000101397     0.117124\n0.01     10      True    100     100     0.000218522     0.000105537     0.482958\n0.01     10      True    100     1000    0.000340882     0.000111641     0.327506\n0.01     10      True    1000    100     0.000315472     0.000117376     0.372064\n0.01     10      True    1000    1000    0.000905493     0.000123263     0.136128\n0.01     10      False   100     100     0.000221529     9.82571e-05     0.44354\n0.01     10      False   100     1000    0.000330552     0.000112615     0.340687\n0.01     10      False   1000    100     0.000341277     0.000114097     0.334324\n0.01     10      False   1000    1000    0.000819944     0.000120982     0.147549\n0.01     25      True    100     100     0.000207806     0.000105977     0.509981\n0.01     25      True    100     1000    0.000322879     0.00012921      0.400181\n0.01     25      True    1000    100     0.00038262      0.000141583     0.370035\n0.01     25      True    1000    1000    0.000865438     0.000202083     0.233504\n0.01     25      False   100     100     0.000209401     0.000104696     0.499979\n0.01     25      False   100     1000    0.000321161     0.000130737     0.407076\n0.01     25      False   1000    100     0.000377012     0.000136801     0.362856\n0.01     25      False   1000    1000    0.000861125     0.00020272      0.235413\n0.2      1       True    100     100     0.000206952     9.69219e-05     0.46833\n0.2      1       True    100     1000    0.000348674     0.000147475     0.422959\n0.2      1       True    1000    100     0.000336908     0.00010122      0.300439\n0.2      1       True    1000    1000    0.001022        0.000203274     0.198898\n0.2      1       False   100     100     0.000207532     9.5412e-05      0.459746\n0.2      1       False   100     1000    0.000356127     0.000146824     0.41228\n0.2      1       False   1000    100     0.000322664     0.000100918     0.312764\n0.2      1       False   1000    1000    0.000998987     0.000203442     0.203648\n0.2      10      True    100     100     0.000211692     0.000109903     0.519165\n0.2      10      True    100     1000    0.000372819     0.000164321     0.440753\n0.2      10      True    1000    100     0.000338651     0.000144806     0.427596\n0.2      10      True    1000    1000    0.00108312      0.000758876     0.70064\n0.2      10      False   100     100     0.000215727     0.000110502     0.512231\n0.2      10      False   100     1000    0.000375419     0.0001613       0.429653\n0.2      10      False   1000    100     0.000336999     0.000145628     0.432132\n0.2      10      False   1000    1000    0.00110502      0.000762043     0.689618\n0.2      25      True    100     100     0.000218705     0.000129913     0.594009\n0.2      25      True    100     1000    0.000394794     0.00029428      0.745402\n0.2      25      True    1000    100     0.000404483     0.0002693       0.665788\n0.2      25      True    1000    1000    0.0012002       0.00194494      1.62052\n0.2      25      False   100     100     0.000221494     0.0001306       0.589632\n0.2      25      False   100     1000    0.000396436     0.000297204     0.74969\n0.2      25      False   1000    100     0.000409346     0.000270068     0.659754\n0.2      25      False   1000    1000    0.00121051      0.00193737      1.60046\n0.5      1       True    100     100     0.000214981     9.82111e-05     0.456836\n0.5      1       True    100     1000    0.000415328     0.000223073     0.537101\n0.5      1       True    1000    100     0.000358324     0.00011269      0.314492\n0.5      1       True    1000    1000    0.00137612      0.000437401     0.317851\n0.5      1       False   100     100     0.000224196     0.000101423     0.452386\n0.5      1       False   100     1000    0.000400987     0.000223286     0.556841\n0.5      1       False   1000    100     0.000368825     0.00011224      0.304318\n0.5      1       False   1000    1000    0.00136036      0.000429369     0.31563\n0.5      10      True    100     100     0.000222125     0.000112308     0.505608\n0.5      10      True    100     1000    0.000461088     0.00032357      0.701753\n0.5      10      True    1000    100     0.000394624     0.000225497     0.571422\n0.5      10      True    1000    1000    0.00158027      0.00190898      1.20801\n0.5      10      False   100     100     0.000232083     0.000114978     0.495418\n0.5      10      False   100     1000    0.000454574     0.000324632     0.714146\n0.5      10      False   1000    100     0.000379097     0.000227768     0.600817\n0.5      10      False   1000    1000    0.00160292      0.00190168      1.18638\n0.5      25      True    100     100     0.00023429      0.000151703     0.647501\n0.5      25      True    100     1000    0.000497462     0.000598873     1.20386\n0.5      25      True    1000    100     0.000460778     0.000557038     1.20891\n0.5      25      True    1000    1000    0.00170036      0.00467336      2.74845\n0.5      25      False   100     100     0.000228981     0.000155334     0.678371\n0.5      25      False   100     1000    0.000496139     0.000620789     1.25124\n0.5      25      False   1000    100     0.00045473      0.000551528     1.21287\n0.5      25      False   1000    1000    0.00171793      0.00467152      2.71927\n0.8      1       True    100     100     0.000222037     0.000105301     0.47425\n0.8      1       True    100     1000    0.000410804     0.000329327     0.801664\n0.8      1       True    1000    100     0.000349735     0.000131225     0.375212\n0.8      1       True    1000    1000    0.00139219      0.000677065     0.48633\n0.8      1       False   100     100     0.000214079     0.000107486     0.502085\n0.8      1       False   100     1000    0.000413746     0.000323244     0.781261\n0.8      1       False   1000    100     0.000348983     0.000131983     0.378193\n0.8      1       False   1000    1000    0.00136296      0.000685325     0.50282\n0.8      10      True    100     100     0.000229159     0.00011825      0.516017\n0.8      10      True    100     1000    0.000498845     0.000532618     1.0677\n0.8      10      True    1000    100     0.000383126     0.00029935      0.781336\n0.8      10      True    1000    1000    0.00162866      0.00307312      1.88689\n0.8      10      False   100     100     0.000230783     0.000124958     0.541452\n0.8      10      False   100     1000    0.000493393     0.000550654     1.11606\n0.8      10      False   1000    100     0.000377167     0.000298581     0.791642\n0.8      10      False   1000    1000    0.00165795      0.00305103      1.84024\n0.8      25      True    100     100     0.000233496     0.000175241     0.75051\n0.8      25      True    100     1000    0.00055654      0.00102658      1.84458\n0.8      25      True    1000    100     0.000463814     0.000783267     1.68875\n0.8      25      True    1000    1000    0.00186905      0.00755344      4.04132\n0.8      25      False   100     100     0.000240243     0.000175047     0.728625\n0.8      25      False   100     1000    0.000578102     0.00104499      1.80763\n0.8      25      False   1000    100     0.000485113     0.000776849     1.60138\n0.8      25      False   1000    1000    0.00211448      0.00752736      3.55992\n</pre>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>sp_a</code>: SparseTensor A, of rank 2.</li> <li>\n<code>b</code>: A dense Matrix with the same dtype as sp_a.</li> <li>\n<code>adjoint_a</code>: Use the adjoint of A in the matrix multiply. If A is complex, this is transpose(conj(A)). Otherwise it's transpose(A).</li> <li>\n<code>adjoint_b</code>: Use the adjoint of B in the matrix multiply. If B is complex, this is transpose(conj(B)). Otherwise it's transpose(B).</li> <li>\n<code>name</code>: A name prefix for the returned tensors (optional)</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>A dense matrix (pseudo-code in dense np.matrix notation): A = A.H if adjoint_a else A B = B.H if adjoint_b else B return A*B</p>  <h3 id=\"sparse_maximum\"><code>tf.sparse_maximum(sp_a, sp_b, name=None)</code></h3> <p>Returns the element-wise max of two SparseTensors.</p> <p>Assumes the two SparseTensors have the same shape, i.e., no broadcasting. Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">sp_zero = ops.SparseTensor([[0]], [0], [7])\nsp_one = ops.SparseTensor([[1]], [1], [7])\nres = tf.sparse_maximum(sp_zero, sp_one).eval()\n# \"res\" should be equal to SparseTensor([[0], [1]], [0, 1], [7]).\n</pre>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>sp_a</code>: a <code>SparseTensor</code> operand whose dtype is real, and indices lexicographically ordered.</li> <li>\n<code>sp_b</code>: the other <code>SparseTensor</code> operand with the same requirements (and the same shape).</li> <li>\n<code>name</code>: optional name of the operation.</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <ul> <li>\n<code>output</code>: the output SparseTensor.</li> </ul>  <h3 id=\"sparse_minimum\"><code>tf.sparse_minimum(sp_a, sp_b, name=None)</code></h3> <p>Returns the element-wise min of two SparseTensors.</p> <p>Assumes the two SparseTensors have the same shape, i.e., no broadcasting. Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">sp_zero = ops.SparseTensor([[0]], [0], [7])\nsp_one = ops.SparseTensor([[1]], [1], [7])\nres = tf.sparse_minimum(sp_zero, sp_one).eval()\n# \"res\" should be equal to SparseTensor([[0], [1]], [0, 0], [7]).\n</pre>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>sp_a</code>: a <code>SparseTensor</code> operand whose dtype is real, and indices lexicographically ordered.</li> <li>\n<code>sp_b</code>: the other <code>SparseTensor</code> operand with the same requirements (and the same shape).</li> <li>\n<code>name</code>: optional name of the operation.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <ul> <li>\n<code>output</code>: the output SparseTensor.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/sparse_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/sparse_ops.html</a>\n  </p>\n</div>\n","math_ops":"<h1 id=\"math\">Math</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#math\">Math</a></li> <ul> <li><a href=\"#arithmetic-operators\">Arithmetic Operators</a></li> <ul> <li><a href=\"#add\"><code>tf.add(x, y, name=None)</code></a></li> <li><a href=\"#sub\"><code>tf.sub(x, y, name=None)</code></a></li> <li><a href=\"#mul\"><code>tf.mul(x, y, name=None)</code></a></li> <li><a href=\"#div\"><code>tf.div(x, y, name=None)</code></a></li> <li><a href=\"#truediv\"><code>tf.truediv(x, y, name=None)</code></a></li> <li><a href=\"#floordiv\"><code>tf.floordiv(x, y, name=None)</code></a></li> <li><a href=\"#mod\"><code>tf.mod(x, y, name=None)</code></a></li> <li><a href=\"#cross\"><code>tf.cross(a, b, name=None)</code></a></li> </ul> <li><a href=\"#basic-math-functions\">Basic Math Functions</a></li> <ul> <li><a href=\"#add_n\"><code>tf.add_n(inputs, name=None)</code></a></li> <li><a href=\"#abs\"><code>tf.abs(x, name=None)</code></a></li> <li><a href=\"#neg\"><code>tf.neg(x, name=None)</code></a></li> <li><a href=\"#sign\"><code>tf.sign(x, name=None)</code></a></li> <li><a href=\"#inv\"><code>tf.inv(x, name=None)</code></a></li> <li><a href=\"#square\"><code>tf.square(x, name=None)</code></a></li> <li><a href=\"#round\"><code>tf.round(x, name=None)</code></a></li> <li><a href=\"#sqrt\"><code>tf.sqrt(x, name=None)</code></a></li> <li><a href=\"#rsqrt\"><code>tf.rsqrt(x, name=None)</code></a></li> <li><a href=\"#pow\"><code>tf.pow(x, y, name=None)</code></a></li> <li><a href=\"#exp\"><code>tf.exp(x, name=None)</code></a></li> <li><a href=\"#log\"><code>tf.log(x, name=None)</code></a></li> <li><a href=\"#ceil\"><code>tf.ceil(x, name=None)</code></a></li> <li><a href=\"#floor\"><code>tf.floor(x, name=None)</code></a></li> <li><a href=\"#maximum\"><code>tf.maximum(x, y, name=None)</code></a></li> <li><a href=\"#minimum\"><code>tf.minimum(x, y, name=None)</code></a></li> <li><a href=\"#cos\"><code>tf.cos(x, name=None)</code></a></li> <li><a href=\"#sin\"><code>tf.sin(x, name=None)</code></a></li> <li><a href=\"#lbeta\"><code>tf.lbeta(x, name=lbeta)</code></a></li> <li><a href=\"#tan\"><code>tf.tan(x, name=None)</code></a></li> <li><a href=\"#acos\"><code>tf.acos(x, name=None)</code></a></li> <li><a href=\"#asin\"><code>tf.asin(x, name=None)</code></a></li> <li><a href=\"#atan\"><code>tf.atan(x, name=None)</code></a></li> <li><a href=\"#lgamma\"><code>tf.lgamma(x, name=None)</code></a></li> <li><a href=\"#digamma\"><code>tf.digamma(x, name=None)</code></a></li> <li><a href=\"#erf\"><code>tf.erf(x, name=None)</code></a></li> <li><a href=\"#erfc\"><code>tf.erfc(x, name=None)</code></a></li> <li><a href=\"#squared_difference\"><code>tf.squared_difference(x, y, name=None)</code></a></li> <li><a href=\"#igamma\"><code>tf.igamma(a, x, name=None)</code></a></li> <li><a href=\"#igammac\"><code>tf.igammac(a, x, name=None)</code></a></li> <li><a href=\"#zeta\"><code>tf.zeta(x, q, name=None)</code></a></li> <li><a href=\"#polygamma\"><code>tf.polygamma(a, x, name=None)</code></a></li> </ul> <li><a href=\"#matrix-math-functions\">Matrix Math Functions</a></li> <ul> <li><a href=\"#batch_matrix_diag\"><code>tf.batch_matrix_diag(diagonal, name=None)</code></a></li> <li><a href=\"#batch_matrix_diag_part\"><code>tf.batch_matrix_diag_part(input, name=None)</code></a></li> <li><a href=\"#batch_matrix_band_part\"><code>tf.batch_matrix_band_part(input, num_lower, num_upper, name=None)</code></a></li> <li><a href=\"#batch_matrix_set_diag\"><code>tf.batch_matrix_set_diag(input, diagonal, name=None)</code></a></li> <li><a href=\"#diag\"><code>tf.diag(diagonal, name=None)</code></a></li> <li><a href=\"#diag_part\"><code>tf.diag_part(input, name=None)</code></a></li> <li><a href=\"#trace\"><code>tf.trace(x, name=None)</code></a></li> <li><a href=\"#transpose\"><code>tf.transpose(a, perm=None, name=transpose)</code></a></li> <li><a href=\"#batch_matrix_transpose\"><code>tf.batch_matrix_transpose(a, name=batch_matrix_transpose)</code></a></li> <li><a href=\"#matmul\"><code>tf.matmul(a, b, transpose_a=False, transpose_b=False, a_is_sparse=False, b_is_sparse=False, name=None)</code></a></li> <li><a href=\"#batch_matmul\"><code>tf.batch_matmul(x, y, adj_x=None, adj_y=None, name=None)</code></a></li> <li><a href=\"#matrix_determinant\"><code>tf.matrix_determinant(input, name=None)</code></a></li> <li><a href=\"#batch_matrix_determinant\"><code>tf.batch_matrix_determinant(input, name=None)</code></a></li> <li><a href=\"#matrix_inverse\"><code>tf.matrix_inverse(input, adjoint=None, name=None)</code></a></li> <li><a href=\"#batch_matrix_inverse\"><code>tf.batch_matrix_inverse(input, adjoint=None, name=None)</code></a></li> <li><a href=\"#cholesky\"><code>tf.cholesky(input, name=None)</code></a></li> <li><a href=\"#batch_cholesky\"><code>tf.batch_cholesky(input, name=None)</code></a></li> <li><a href=\"#cholesky_solve\"><code>tf.cholesky_solve(chol, rhs, name=None)</code></a></li> <li><a href=\"#batch_cholesky_solve\"><code>tf.batch_cholesky_solve(chol, rhs, name=None)</code></a></li> <li><a href=\"#matrix_solve\"><code>tf.matrix_solve(matrix, rhs, adjoint=None, name=None)</code></a></li> <li><a href=\"#batch_matrix_solve\"><code>tf.batch_matrix_solve(matrix, rhs, adjoint=None, name=None)</code></a></li> <li><a href=\"#matrix_triangular_solve\"><code>tf.matrix_triangular_solve(matrix, rhs, lower=None, adjoint=None, name=None)</code></a></li> <li><a href=\"#batch_matrix_triangular_solve\"><code>tf.batch_matrix_triangular_solve(matrix, rhs, lower=None, adjoint=None, name=None)</code></a></li> <li><a href=\"#matrix_solve_ls\"><code>tf.matrix_solve_ls(matrix, rhs, l2_regularizer=0.0, fast=True, name=None)</code></a></li> <li><a href=\"#batch_matrix_solve_ls\"><code>tf.batch_matrix_solve_ls(matrix, rhs, l2_regularizer=0.0, fast=True, name=None)</code></a></li> <li><a href=\"#self_adjoint_eig\"><code>tf.self_adjoint_eig(matrix, name=None)</code></a></li> <li><a href=\"#batch_self_adjoint_eig\"><code>tf.batch_self_adjoint_eig(tensor, name=None)</code></a></li> <li><a href=\"#self_adjoint_eigvals\"><code>tf.self_adjoint_eigvals(matrix, name=None)</code></a></li> <li><a href=\"#batch_self_adjoint_eigvals\"><code>tf.batch_self_adjoint_eigvals(tensor, name=None)</code></a></li> <li><a href=\"#svd\"><code>tf.svd(matrix, compute_uv=True, full_matrices=False, name=None)</code></a></li> <li><a href=\"#batch_svd\"><code>tf.batch_svd(tensor, compute_uv=True, full_matrices=False, name=None)</code></a></li> </ul> <li><a href=\"#complex-number-functions\">Complex Number Functions</a></li> <ul> <li><a href=\"#complex\"><code>tf.complex(real, imag, name=None)</code></a></li> <li><a href=\"#complex_abs\"><code>tf.complex_abs(x, name=None)</code></a></li> <li><a href=\"#conj\"><code>tf.conj(input, name=None)</code></a></li> <li><a href=\"#imag\"><code>tf.imag(input, name=None)</code></a></li> <li><a href=\"#real\"><code>tf.real(input, name=None)</code></a></li> <li><a href=\"#fft\"><code>tf.fft(input, name=None)</code></a></li> <li><a href=\"#ifft\"><code>tf.ifft(input, name=None)</code></a></li> <li><a href=\"#fft2d\"><code>tf.fft2d(input, name=None)</code></a></li> <li><a href=\"#ifft2d\"><code>tf.ifft2d(input, name=None)</code></a></li> <li><a href=\"#fft3d\"><code>tf.fft3d(input, name=None)</code></a></li> <li><a href=\"#ifft3d\"><code>tf.ifft3d(input, name=None)</code></a></li> <li><a href=\"#batch_fft\"><code>tf.batch_fft(input, name=None)</code></a></li> <li><a href=\"#batch_ifft\"><code>tf.batch_ifft(input, name=None)</code></a></li> <li><a href=\"#batch_fft2d\"><code>tf.batch_fft2d(input, name=None)</code></a></li> <li><a href=\"#batch_ifft2d\"><code>tf.batch_ifft2d(input, name=None)</code></a></li> <li><a href=\"#batch_fft3d\"><code>tf.batch_fft3d(input, name=None)</code></a></li> <li><a href=\"#batch_ifft3d\"><code>tf.batch_ifft3d(input, name=None)</code></a></li> </ul> <li><a href=\"#reduction\">Reduction</a></li> <ul> <li><a href=\"#reduce_sum\"><code>tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></a></li> <li><a href=\"#reduce_prod\"><code>tf.reduce_prod(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></a></li> <li><a href=\"#reduce_min\"><code>tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></a></li> <li><a href=\"#reduce_max\"><code>tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></a></li> <li><a href=\"#reduce_mean\"><code>tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></a></li> <li><a href=\"#reduce_all\"><code>tf.reduce_all(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></a></li> <li><a href=\"#reduce_any\"><code>tf.reduce_any(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></a></li> <li><a href=\"#accumulate_n\"><code>tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None)</code></a></li> </ul> <li><a href=\"#scan\">Scan</a></li> <ul> <li><a href=\"#cumsum\"><code>tf.cumsum(x, axis=0, exclusive=False, reverse=False, name=None)</code></a></li> <li><a href=\"#cumprod\"><code>tf.cumprod(x, axis=0, exclusive=False, reverse=False, name=None)</code></a></li> </ul> <li><a href=\"#segmentation\">Segmentation</a></li> <ul> <li><a href=\"#segment_sum\"><code>tf.segment_sum(data, segment_ids, name=None)</code></a></li> <li><a href=\"#segment_prod\"><code>tf.segment_prod(data, segment_ids, name=None)</code></a></li> <li><a href=\"#segment_min\"><code>tf.segment_min(data, segment_ids, name=None)</code></a></li> <li><a href=\"#segment_max\"><code>tf.segment_max(data, segment_ids, name=None)</code></a></li> <li><a href=\"#segment_mean\"><code>tf.segment_mean(data, segment_ids, name=None)</code></a></li> <li><a href=\"#unsorted_segment_sum\"><code>tf.unsorted_segment_sum(data, segment_ids, num_segments, name=None)</code></a></li> <li><a href=\"#sparse_segment_sum\"><code>tf.sparse_segment_sum(data, indices, segment_ids, name=None)</code></a></li> <li><a href=\"#sparse_segment_mean\"><code>tf.sparse_segment_mean(data, indices, segment_ids, name=None)</code></a></li> <li><a href=\"#sparse_segment_sqrt_n\"><code>tf.sparse_segment_sqrt_n(data, indices, segment_ids, name=None)</code></a></li> </ul> <li><a href=\"#sequence-comparison-and-indexing\">Sequence Comparison and Indexing</a></li> <ul> <li><a href=\"#argmin\"><code>tf.argmin(input, dimension, name=None)</code></a></li> <li><a href=\"#argmax\"><code>tf.argmax(input, dimension, name=None)</code></a></li> <li><a href=\"#listdiff\"><code>tf.listdiff(x, y, name=None)</code></a></li> <li><a href=\"#where\"><code>tf.where(input, name=None)</code></a></li> <li><a href=\"#unique\"><code>tf.unique(x, name=None)</code></a></li> <li><a href=\"#edit_distance\"><code>tf.edit_distance(hypothesis, truth, normalize=True, name=edit_distance)</code></a></li> <li><a href=\"#invert_permutation\"><code>tf.invert_permutation(x, name=None)</code></a></li> </ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#scalar_mul\"><code>tf.scalar_mul(scalar, x)</code></a></li> <li><a href=\"#sparse_segment_sqrt_n_grad\"><code>tf.sparse_segment_sqrt_n_grad(grad, indices, segment_ids, output_dim0, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Note: Elementwise binary operations in TensorFlow follow <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">numpy-style broadcasting</a>.</p>  <h2 id=\"arithmetic-operators\">Arithmetic Operators</h2> <p>TensorFlow provides several operations that you can use to add basic arithmetic operators to your graph.</p>  <h3 id=\"add\"><code>tf.add(x, y, name=None)</code></h3> <p>Returns x + y element-wise.</p> <p><em>NOTE</em>: <code>Add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"sub\"><code>tf.sub(x, y, name=None)</code></h3> <p>Returns x - y element-wise.</p> <p><em>NOTE</em>: <code>Sub</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"mul\"><code>tf.mul(x, y, name=None)</code></h3> <p>Returns x * y element-wise.</p> <p><em>NOTE</em>: <code>Mul</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"div\"><code>tf.div(x, y, name=None)</code></h3> <p>Returns x / y element-wise.</p> <p><em>NOTE</em>: <code>Div</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"truediv\"><code>tf.truediv(x, y, name=None)</code></h3> <p>Divides x / y elementwise, always producing floating point results.</p> <p>The same as <code>tf.div</code> for floating point arguments, but casts integer arguments to floating point before dividing so that the result is always floating point. This op is generated by normal <code>x / y</code> division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>. If you want integer division that rounds down, use <code>x // y</code> or <code>tf.floordiv</code>.</p> <p><code>x</code> and <code>y</code> must have the same numeric type. If the inputs are floating point, the output will have the same type. If the inputs are integral, the inputs are cast to <code>float32</code> for <code>int8</code> and <code>int16</code> and <code>float64</code> for <code>int32</code> and <code>int64</code> (matching the behavior of Numpy).</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>x</code>: <code>Tensor</code> numerator of numeric type.</li> <li>\n<code>y</code>: <code>Tensor</code> denominator of numeric type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p><code>x / y</code> evaluated in floating point.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>x</code> and <code>y</code> have different dtypes.</li> </ul>  <h3 id=\"floordiv\"><code>tf.floordiv(x, y, name=None)</code></h3> <p>Divides <code>x / y</code> elementwise, rounding down for floating point.</p> <p>The same as <code>tf.div(x,y)</code> for integers, but uses <code>tf.floor(tf.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p> <p>Note that for efficiency, <code>floordiv</code> uses C semantics for negative numbers (unlike Python and Numpy).</p> <p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>x</code>: <code>Tensor</code> numerator of real numeric type.</li> <li>\n<code>y</code>: <code>Tensor</code> denominator of real numeric type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p><code>x / y</code> rounded down (except possibly towards zero for negative integers).</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If the inputs are complex.</li> </ul>  <h3 id=\"mod\"><code>tf.mod(x, y, name=None)</code></h3> <p>Returns element-wise remainder of division.</p> <p><em>NOTE</em>: <code>Mod</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"cross\"><code>tf.cross(a, b, name=None)</code></h3> <p>Compute the pairwise cross product.</p> <p><code>a</code> and <code>b</code> must be the same shape; they can either be simple 3-element vectors, or any shape where the innermost dimension is 3. In the latter case, each pair of corresponding 3-element vectors is cross-multiplied independently.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>a</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>. A tensor containing 3-element vectors.</li> <li>\n<code>b</code>: A <code>Tensor</code>. Must have the same type as <code>a</code>. Another tensor, of same type and shape as <code>a</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>a</code>. Pairwise cross product of the vectors in <code>a</code> and <code>b</code>.</p>  <h2 id=\"basic-math-functions\">Basic Math Functions</h2> <p>TensorFlow provides several operations that you can use to add basic mathematical functions to your graph.</p>  <h3 id=\"add_n\"><code>tf.add_n(inputs, name=None)</code></h3> <p>Adds all input tensors element-wise.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>inputs</code>: A list of <code>Tensor</code> objects, each with same shape and type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A <code>Tensor</code> of same shape and type as the elements of <code>inputs</code>.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>inputs</code> don't all have same shape and dtype or the shape cannot be inferred.</li> </ul>  <h3 id=\"abs\"><code>tf.abs(x, name=None)</code></h3> <p>Computes the absolute value of a tensor.</p> <p>Given a tensor of real numbers <code>x</code>, this operation returns a tensor containing the absolute value of each element in <code>x</code>. For example, if x is an input element and y is an output element, this operation computes \\(y = |x|\\).</p> <p>See <a href=\"#tf_complex_abs\"><code>tf.complex_abs()</code></a> to compute the absolute value of a complex number.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, or <code>int64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code> the same size and type as <code>x</code> with absolute values.</p>  <h3 id=\"neg\"><code>tf.neg(x, name=None)</code></h3> <p>Computes numerical negative value element-wise.</p> <p>I.e., (y = -x).</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code>, respectively. Has the same type as <code>x</code>.</p>  <h3 id=\"sign\"><code>tf.sign(x, name=None)</code></h3> <p>Returns an element-wise indication of the sign of a number.</p> <p><code>y = sign(x) = -1</code> if <code>x &lt; 0</code>; 0 if <code>x == 0</code>; 1 if <code>x &gt; 0</code>.</p> <p>For complex numbers, <code>y = sign(x) = x / |x|</code> if <code>x != 0</code>, otherwise <code>y = 0</code>.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code>, respectively. Has the same type as <code>x</code>.</p>  <h3 id=\"inv\"><code>tf.inv(x, name=None)</code></h3> <p>Computes the reciprocal of x element-wise.</p> <p>I.e., \\(y = 1 / x\\).</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"square\"><code>tf.square(x, name=None)</code></h3> <p>Computes square of x element-wise.</p> <p>I.e., (y = x * x = x^2).</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"round\"><code>tf.round(x, name=None)</code></h3> <p>Rounds the values of a tensor to the nearest integer, element-wise.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'a' is [0.9, 2.5, 2.3, -4.4]\ntf.round(a) ==&gt; [ 1.0, 3.0, 2.0, -4.0 ]\n</pre>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>A <code>Tensor</code> of same shape and type as <code>x</code>.</p>  <h3 id=\"sqrt\"><code>tf.sqrt(x, name=None)</code></h3> <p>Computes square root of x element-wise.</p> <p>I.e., (y = \\sqrt{x} = x^{1/2}).</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> or <code>SparseTensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code>, respectively. Has the same type as <code>x</code>.</p>  <h3 id=\"rsqrt\"><code>tf.rsqrt(x, name=None)</code></h3> <p>Computes reciprocal of square root of x element-wise.</p> <p>I.e., \\(y = 1 / \\sqrt{x}\\).</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"pow\"><code>tf.pow(x, y, name=None)</code></h3> <p>Computes the power of one value to another.</p> <p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \\(x^y\\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p> <pre class=\"\"># tensor 'x' is [[2, 2], [3, 3]]\n# tensor 'y' is [[8, 16], [2, 3]]\ntf.pow(x, y) ==&gt; [[256, 65536], [9, 27]]\n</pre>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>A <code>Tensor</code>.</p>  <h3 id=\"exp\"><code>tf.exp(x, name=None)</code></h3> <p>Computes exponential of x element-wise. \\(y = e^x\\).</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"log\"><code>tf.log(x, name=None)</code></h3> <p>Computes natural logarithm of x element-wise.</p> <p>I.e., \\(y = \\log_e x\\).</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"ceil\"><code>tf.ceil(x, name=None)</code></h3> <p>Returns element-wise smallest integer in not less than x.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"floor\"><code>tf.floor(x, name=None)</code></h3> <p>Returns element-wise largest integer not greater than x.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"maximum\"><code>tf.maximum(x, y, name=None)</code></h3> <p>Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.</p> <p><em>NOTE</em>: <code>Maximum</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"minimum\"><code>tf.minimum(x, y, name=None)</code></h3> <p>Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.</p> <p><em>NOTE</em>: <code>Minimum</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"cos\"><code>tf.cos(x, name=None)</code></h3> <p>Computes cos of x element-wise.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"sin\"><code>tf.sin(x, name=None)</code></h3> <p>Computes sin of x element-wise.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"lbeta\"><code>tf.lbeta(x, name='lbeta')</code></h3> <p>Computes <code>ln(|Beta(x)|)</code>, reducing along the last dimension.</p> <p>Given one-dimensional <code>z = [z_0,...,z_{K-1}]</code>, we define</p> <p><code>Beta(z) = \\prod_j Gamma(z_j) / Gamma(\\sum_j z_j)</code></p> <p>And for <code>n + 1</code> dimensional <code>x</code> with shape <code>[N1, ..., Nn, K]</code>, we define <code>lbeta(x)[i1, ..., in] = Log(|Beta(x[i1, ..., in, :])|)</code>. In other words, the last dimension is treated as the <code>z</code> vector.</p> <p>Note that if <code>z = [u, v]</code>, then <code>Beta(z) = int_0^1 t^{u-1} (1 - t)^{v-1} dt</code>, which defines the traditional bivariate beta function.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>x</code>: A rank <code>n + 1</code> <code>Tensor</code> with type <code>float</code>, or <code>double</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>The logarithm of <code>|Beta(x)|</code> reducing along the last dimension.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>x</code> is empty with rank one or less.</li> </ul>  <h3 id=\"tan\"><code>tf.tan(x, name=None)</code></h3> <p>Computes tan of x element-wise.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"acos\"><code>tf.acos(x, name=None)</code></h3> <p>Computes acos of x element-wise.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"asin\"><code>tf.asin(x, name=None)</code></h3> <p>Computes asin of x element-wise.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"atan\"><code>tf.atan(x, name=None)</code></h3> <p>Computes atan of x element-wise.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"lgamma\"><code>tf.lgamma(x, name=None)</code></h3> <p>Computes the log of the absolute value of <code>Gamma(x)</code> element-wise.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"digamma\"><code>tf.digamma(x, name=None)</code></h3> <p>Computes Psi, the derivative of Lgamma (the log of the absolute value of</p> <p><code>Gamma(x)</code>), element-wise.</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"erf\"><code>tf.erf(x, name=None)</code></h3> <p>Computes the Gauss error function of <code>x</code> element-wise.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> of <code>SparseTensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-34\">Returns:</h5> <p>A <code>Tensor</code> or <code>SparseTensor</code>, respectively. Has the same type as <code>x</code>.</p>  <h3 id=\"erfc\"><code>tf.erfc(x, name=None)</code></h3> <p>Computes the complementary error function of <code>x</code> element-wise.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"squared_difference\"><code>tf.squared_difference(x, y, name=None)</code></h3> <p>Returns (x - y)(x - y) element-wise.</p> <p><em>NOTE</em>: <code>SquaredDifference</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">here</a></p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"igamma\"><code>tf.igamma(a, x, name=None)</code></h3> <p>Compute the lower regularized incomplete Gamma function <code>Q(a, x)</code>.</p> <p>The lower regularized incomplete Gamma function is defined as:</p> <pre class=\"\">P(a, x) = gamma(a, x) / Gamma(x) = 1 - Q(a, x)\n</pre> <p>where <code>gamma(a, x) = int_{0}^{x} t^{a-1} exp(-t) dt</code> is the lower incomplete Gamma function.</p> <p>Note, above <code>Q(a, x)</code> (<code>Igammac</code>) is the upper regularized complete Gamma function.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>a</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>x</code>: A <code>Tensor</code>. Must have the same type as <code>a</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>a</code>.</p>  <h3 id=\"igammac\"><code>tf.igammac(a, x, name=None)</code></h3> <p>Compute the upper regularized incomplete Gamma function <code>Q(a, x)</code>.</p> <p>The upper regularized incomplete Gamma function is defined as:</p> <pre class=\"lang-m no-auto-prettify\">Q(a, x) = Gamma(a, x) / Gamma(x) = 1 - P(a, x)\n</pre> <p>where <code>Gamma(a, x) = int_{x}^{\\infty} t^{a-1} exp(-t) dt</code> is the upper incomplete Gama function.</p> <p>Note, above <code>P(a, x)</code> (<code>Igamma</code>) is the lower regularized complete Gamma function.</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>a</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>x</code>: A <code>Tensor</code>. Must have the same type as <code>a</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>a</code>.</p>  <h3 id=\"zeta\"><code>tf.zeta(x, q, name=None)</code></h3> <p>Compute the Hurwitz zeta function \\(\\zeta(x, q)\\).</p> <p>The Hurwitz zeta function is defined as:</p> <pre class=\"lang-forth no-auto-prettify\">\\zeta(x, q) = \\sum_{n=0}^{\\infty} (q + n)^{-x}\n</pre>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>q</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-39\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"polygamma\"><code>tf.polygamma(a, x, name=None)</code></h3> <p>Compute the polygamma function \\(\\psi^{(n)}(x)\\).</p> <p>The polygamma function is defined as:</p> <pre class=\"lang-forth no-auto-prettify\">\\psi^{(n)}(x) = \\frac{d^n}{dx^n} \\psi(x)\n</pre> <p>where \\(\\psi(x)\\) is the digamma function.</p>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>a</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>x</code>: A <code>Tensor</code>. Must have the same type as <code>a</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-40\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>a</code>.</p>  <h2 id=\"matrix-math-functions\">Matrix Math Functions</h2> <p>TensorFlow provides several operations that you can use to add linear algebra functions on matrices to your graph.</p>  <h3 id=\"batch_matrix_diag\"><code>tf.batch_matrix_diag(diagonal, name=None)</code></h3> <p>Returns a batched diagonal tensor with a given batched diagonal values.</p> <p>Given a <code>diagonal</code>, this operation returns a tensor with the <code>diagonal</code> and everything else padded with zeros. The diagonal is computed as follows:</p> <p>Assume <code>diagonal</code> has <code>k</code> dimensions <code>[I, J, K, ..., N]</code>, then the output is a tensor of rank <code>k+1</code> with dimensions [I, J, K, ..., N, N]` where:</p> <p><code>output[i, j, k, ..., m, n] = 1{m=n} * diagonal[i, j, k, ..., n]</code>.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 'diagonal' is [[1, 2, 3, 4], [5, 6, 7, 8]]\n\nand diagonal.shape = (2, 4)\n\ntf.batch_matrix_diag(diagonal) ==&gt; [[[1, 0, 0, 0]\n                                     [0, 2, 0, 0]\n                                     [0, 0, 3, 0]\n                                     [0, 0, 0, 4]],\n                                    [[5, 0, 0, 0]\n                                     [0, 6, 0, 0]\n                                     [0, 0, 7, 0]\n                                     [0, 0, 0, 8]]]\n\nwhich has shape (2, 4, 4)\n</pre>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>diagonal</code>: A <code>Tensor</code>. Rank <code>k</code>, where <code>k &gt;= 1</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-41\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>diagonal</code>. Rank <code>k+1</code>, with <code>output.shape = diagonal.shape + [diagonal.shape[-1]]</code>.</p>  <h3 id=\"batch_matrix_diag_part\"><code>tf.batch_matrix_diag_part(input, name=None)</code></h3> <p>Returns the batched diagonal part of a batched tensor.</p> <p>This operation returns a tensor with the <code>diagonal</code> part of the batched <code>input</code>. The <code>diagonal</code> part is computed as follows:</p> <p>Assume <code>input</code> has <code>k</code> dimensions <code>[I, J, K, ..., N, N]</code>, then the output is a tensor of rank <code>k - 1</code> with dimensions <code>[I, J, K, ..., N]</code> where:</p> <p><code>diagonal[i, j, k, ..., n] = input[i, j, k, ..., n, n]</code>.</p> <p>The input must be at least a matrix.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 'input' is [[[1, 0, 0, 0]\n               [0, 2, 0, 0]\n               [0, 0, 3, 0]\n               [0, 0, 0, 4]],\n              [[5, 0, 0, 0]\n               [0, 6, 0, 0]\n               [0, 0, 7, 0]\n               [0, 0, 0, 8]]]\n\nand input.shape = (2, 4, 4)\n\ntf.batch_matrix_diag_part(input) ==&gt; [[1, 2, 3, 4], [5, 6, 7, 8]]\n\nwhich has shape (2, 4)\n</pre>  <h5 id=\"args-42\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Rank <code>k</code> tensor where <code>k &gt;= 2</code> and the last two dimensions are equal.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-42\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. The extracted diagonal(s) having shape <code>diagonal.shape = input.shape[:-1]</code>.</p>  <h3 id=\"batch_matrix_band_part\"><code>tf.batch_matrix_band_part(input, num_lower, num_upper, name=None)</code></h3> <p>Copy a tensor setting everything outside a central band in each innermost matrix</p> <p>to zero.</p> <p>The <code>band</code> part is computed as follows: Assume <code>input</code> has <code>k</code> dimensions <code>[I, J, K, ..., M, N]</code>, then the output is a tensor with the same shape where</p> <p><code>band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]</code>.</p> <p>The indicator function 'in_band(m, n)<code>is one if</code>(num_lower &lt; 0 || (m-n) &lt;= num_lower)) &amp;&amp; (num_upper &lt; 0 || (n-m) &lt;= num_upper)`, and zero otherwise.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># if 'input' is [[ 0,  1,  2, 3]\n                 [-1,  0,  1, 2]\n                 [-2, -1,  0, 1]\n                 [-3, -2, -1, 0]],\n\ntf.batch_matrix_band_part(input, 1, -1) ==&gt; [[ 0,  1,  2, 3]\n                                             [-1,  0,  1, 2]\n                                             [ 0, -1,  0, 1]\n                                             [ 0,  0, -1, 0]],\n\ntf.batch_matrix_band_part(input, 2, 1) ==&gt; [[ 0,  1,  0, 0]\n                                            [-1,  0,  1, 0]\n                                            [-2, -1,  0, 1]\n                                            [ 0, -2, -1, 0]]\n</pre> <p>Useful special cases:</p> <pre class=\"lang-prettyprint no-auto-prettify\">tf.batch_matrix_band_part(input, 0, -1) ==&gt; Upper triangular part.\ntf.batch_matrix_band_part(input, -1, 0) ==&gt; Lower triangular part.\ntf.batch_matrix_band_part(input, 0, 0) ==&gt; Diagonal.\n</pre>  <h5 id=\"args-43\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Rank <code>k</code> tensor.</li> <li>\n<code>num_lower</code>: A <code>Tensor</code> of type <code>int64</code>. 0-D tensor. Number of subdiagonals to keep. If negative, keep entire lower triangle.</li> <li>\n<code>num_upper</code>: A <code>Tensor</code> of type <code>int64</code>. 0-D tensor. Number of superdiagonals to keep. If negative, keep entire upper triangle.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-43\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Rank <code>k</code> tensor of the same shape as input. The extracted banded tensor.</p>  <h3 id=\"batch_matrix_set_diag\"><code>tf.batch_matrix_set_diag(input, diagonal, name=None)</code></h3> <p>Returns a batched matrix tensor with new batched diagonal values.</p> <p>Given <code>input</code> and <code>diagonal</code>, this operation returns a tensor with the same shape and values as <code>input</code>, except for the diagonals of the innermost matrices. These will be overwritten by the values in <code>diagonal</code>. The batched matrices must be square.</p> <p>The output is computed as follows:</p> <p>Assume <code>input</code> has <code>k+1</code> dimensions <code>[I, J, K, ..., N, N]</code> and <code>diagonal</code> has <code>k</code> dimensions <code>[I, J, K, ..., N]</code>. Then the output is a tensor of rank <code>k+1</code> with dimensions [I, J, K, ..., N, N]` where:</p> <ul> <li>\n<code>output[i, j, k, ..., m, n] = diagonal[i, j, k, ..., n]</code> for <code>m == n</code>.</li> <li>\n<code>output[i, j, k, ..., m, n] = input[i, j, k, ..., m, n]</code> for <code>m != n</code>.</li> </ul>  <h5 id=\"args-44\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Rank <code>k+1</code>, where <code>k &gt;= 1</code>.</li> <li>\n<code>diagonal</code>: A <code>Tensor</code>. Must have the same type as <code>input</code>. Rank <code>k</code>, where <code>k &gt;= 1</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-44\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Rank <code>k+1</code>, with <code>output.shape = input.shape</code>.</p>  <h3 id=\"diag\"><code>tf.diag(diagonal, name=None)</code></h3> <p>Returns a diagonal tensor with a given diagonal values.</p> <p>Given a <code>diagonal</code>, this operation returns a tensor with the <code>diagonal</code> and everything else padded with zeros. The diagonal is computed as follows:</p> <p>Assume <code>diagonal</code> has dimensions [D1,..., Dk], then the output is a tensor of rank 2k with dimensions [D1,..., Dk, D1,..., Dk] where:</p> <p><code>output[i1,..., ik, i1,..., ik] = diagonal[i1, ..., ik]</code> and 0 everywhere else.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 'diagonal' is [1, 2, 3, 4]\ntf.diag(diagonal) ==&gt; [[1, 0, 0, 0]\n                       [0, 2, 0, 0]\n                       [0, 0, 3, 0]\n                       [0, 0, 0, 4]]\n</pre>  <h5 id=\"args-45\">Args:</h5> <ul> <li>\n<code>diagonal</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>. Rank k tensor where k is at most 3.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-45\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>diagonal</code>.</p>  <h3 id=\"diag_part\"><code>tf.diag_part(input, name=None)</code></h3> <p>Returns the diagonal part of the tensor.</p> <p>This operation returns a tensor with the <code>diagonal</code> part of the <code>input</code>. The <code>diagonal</code> part is computed as follows:</p> <p>Assume <code>input</code> has dimensions <code>[D1,..., Dk, D1,..., Dk]</code>, then the output is a tensor of rank <code>k</code> with dimensions <code>[D1,..., Dk]</code> where:</p> <p><code>diagonal[i1,..., ik] = input[i1, ..., ik, i1,..., ik]</code>.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 'input' is [[1, 0, 0, 0]\n              [0, 2, 0, 0]\n              [0, 0, 3, 0]\n              [0, 0, 0, 4]]\n\ntf.diag_part(input) ==&gt; [1, 2, 3, 4]\n</pre>  <h5 id=\"args-46\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>. Rank k tensor where k is 2, 4, or 6.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-46\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. The extracted diagonal.</p>  <h3 id=\"trace\"><code>tf.trace(x, name=None)</code></h3> <p>Compute the trace of a tensor <code>x</code>.</p> <p><code>trace(x)</code> returns the sum of along the diagonal.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'x' is [[1, 1],\n#         [1, 1]]\ntf.trace(x) ==&gt; 2\n\n# 'x' is [[1,2,3],\n#         [4,5,6],\n#         [7,8,9]]\ntf.trace(x) ==&gt; 15\n</pre>  <h5 id=\"args-47\">Args:</h5> <ul> <li>\n<code>x</code>: 2-D tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-47\">Returns:</h5> <p>The trace of input tensor.</p>  <h3 id=\"transpose\"><code>tf.transpose(a, perm=None, name='transpose')</code></h3> <p>Transposes <code>a</code>. Permutes the dimensions according to <code>perm</code>.</p> <p>The returned tensor's dimension i will correspond to the input dimension <code>perm[i]</code>. If <code>perm</code> is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on 2-D input Tensors.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'x' is [[1 2 3]\n#         [4 5 6]]\ntf.transpose(x) ==&gt; [[1 4]\n                     [2 5]\n                     [3 6]]\n\n# Equivalently\ntf.transpose(x, perm=[1, 0]) ==&gt; [[1 4]\n                                  [2 5]\n                                  [3 6]]\n\n# 'perm' is more useful for n-dimensional tensors, for n &gt; 2\n# 'x' is   [[[1  2  3]\n#            [4  5  6]]\n#           [[7  8  9]\n#            [10 11 12]]]\n# Take the transpose of the matrices in dimension-0\ntf.transpose(x, perm=[0, 2, 1]) ==&gt; [[[1  4]\n                                      [2  5]\n                                      [3  6]]\n\n                                     [[7 10]\n                                      [8 11]\n                                      [9 12]]]\n</pre>  <h5 id=\"args-48\">Args:</h5> <ul> <li>\n<code>a</code>: A <code>Tensor</code>.</li> <li>\n<code>perm</code>: A permutation of the dimensions of <code>a</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-48\">Returns:</h5> <p>A transposed <code>Tensor</code>.</p>  <h3 id=\"batch_matrix_transpose\"><code>tf.batch_matrix_transpose(a, name='batch_matrix_transpose')</code></h3> <p>Transposes last two dimensions of batch matrix <code>a</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Matrix with no batch dimension.\n# 'x' is [[1 2 3]\n#         [4 5 6]]\ntf.batch_matrixtranspose(x) ==&gt; [[1 4]\n                                 [2 5]\n                                 [3 6]]\n\n# Matrix with two batch dimensions.\n# x.shape is [1, 2, 3, 4]\n# tf.batch_matrix_transpose(x) is shape [1, 2, 4, 3]\n</pre>  <h5 id=\"args-49\">Args:</h5> <ul> <li>\n<code>a</code>: A <code>Tensor</code> with <code>rank &gt;= 2</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-49\">Returns:</h5> <p>A transposed batch matrix <code>Tensor</code>.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>a</code> is determined statically to have <code>rank &lt; 2</code>.</li> </ul>  <h3 id=\"matmul\"><code>tf.matmul(a, b, transpose_a=False, transpose_b=False, a_is_sparse=False, b_is_sparse=False, name=None)</code></h3> <p>Multiplies matrix <code>a</code> by matrix <code>b</code>, producing <code>a</code> * <code>b</code>.</p> <p>The inputs must be two-dimensional matrices, with matching inner dimensions, possibly after transposition.</p> <p>Both matrices must be of the same type. The supported types are: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>.</p> <p>Either matrix can be transposed on the fly by setting the corresponding flag to <code>True</code>. This is <code>False</code> by default.</p> <p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code>a_is_sparse</code> or <code>b_is_sparse</code> flag to <code>True</code>. These are <code>False</code> by default.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 2-D tensor `a`\na = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) =&gt; [[1. 2. 3.]\n                                                      [4. 5. 6.]]\n# 2-D tensor `b`\nb = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) =&gt; [[7. 8.]\n                                                         [9. 10.]\n                                                         [11. 12.]]\nc = tf.matmul(a, b) =&gt; [[58 64]\n                        [139 154]]\n</pre>  <h5 id=\"args-50\">Args:</h5> <ul> <li>\n<code>a</code>: <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code> or <code>complex64</code>.</li> <li>\n<code>b</code>: <code>Tensor</code> with same type as <code>a</code>.</li> <li>\n<code>transpose_a</code>: If <code>True</code>, <code>a</code> is transposed before multiplication.</li> <li>\n<code>transpose_b</code>: If <code>True</code>, <code>b</code> is transposed before multiplication.</li> <li>\n<code>a_is_sparse</code>: If <code>True</code>, <code>a</code> is treated as a sparse matrix.</li> <li>\n<code>b_is_sparse</code>: If <code>True</code>, <code>b</code> is treated as a sparse matrix.</li> <li>\n<code>name</code>: Name for the operation (optional).</li> </ul>  <h5 id=\"returns-50\">Returns:</h5> <p>A <code>Tensor</code> of the same type as <code>a</code>.</p>  <h3 id=\"batch_matmul\"><code>tf.batch_matmul(x, y, adj_x=None, adj_y=None, name=None)</code></h3> <p>Multiplies slices of two tensors in batches.</p> <p>Multiplies all slices of <code>Tensor</code> <code>x</code> and <code>y</code> (each slice can be viewed as an element of a batch), and arranges the individual results in a single output tensor of the same batch size. Each of the individual slices can optionally be adjointed (to adjoint a matrix means to transpose and conjugate it) before multiplication by setting the <code>adj_x</code> or <code>adj_y</code> flag to <code>True</code>, which are by default <code>False</code>.</p> <p>The input tensors <code>x</code> and <code>y</code> are 3-D or higher with shape <code>[..., r_x, c_x]</code> and <code>[..., r_y, c_y]</code>.</p> <p>The output tensor is 3-D or higher with shape <code>[..., r_o, c_o]</code>, where:</p> <pre class=\"\">r_o = c_x if adj_x else r_x\nc_o = r_y if adj_y else c_y\n</pre> <p>It is computed as:</p> <pre class=\"\">output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])\n</pre>  <h5 id=\"args-51\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code>. 3-D or higher with shape <code>[..., r_x, c_x]</code>.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>. 3-D or higher with shape <code>[..., r_y, c_y]</code>.</li> <li>\n<code>adj_x</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If <code>True</code>, adjoint the slices of <code>x</code>. Defaults to <code>False</code>.</li> <li>\n<code>adj_y</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If <code>True</code>, adjoint the slices of <code>y</code>. Defaults to <code>False</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-51\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>. 3-D or higher with shape <code>[..., r_o, c_o]</code></p>  <h3 id=\"matrix_determinant\"><code>tf.matrix_determinant(input, name=None)</code></h3> <p>Computes the determinant of a square matrix.</p>  <h5 id=\"args-52\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>. A tensor of shape <code>[M, M]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-52\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. A scalar, equal to the determinant of the input.</p>  <h3 id=\"batch_matrix_determinant\"><code>tf.batch_matrix_determinant(input, name=None)</code></h3> <p>Computes the determinants for a batch of square matrices.</p> <p>The input is a tensor of shape <code>[..., M, M]</code> whose inner-most 2 dimensions form square matrices. The output is a tensor containing the determinants for all input submatrices <code>[..., :, :]</code>.</p>  <h5 id=\"args-53\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>. Shape is <code>[..., M, M]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-53\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Shape is <code>[...]</code>.</p>  <h3 id=\"matrix_inverse\"><code>tf.matrix_inverse(input, adjoint=None, name=None)</code></h3> <p>Computes the inverse of a square invertible matrix or its adjoint (conjugate</p> <p>transpose).</p> <p>The op uses LU decomposition with partial pivoting to compute the inverse.</p> <p>If the matrix is not invertible there is no guarantee what the op does. It may detect the condition and raise an exception or it may simply return a garbage result.</p>  <h5 id=\"args-54\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float64</code>, <code>float32</code>. Shape is <code>[M, M]</code>.</li> <li>\n<code>adjoint</code>: An optional <code>bool</code>. Defaults to <code>False</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-54\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Shape is <code>[M, M]</code>. If <code>adjoint</code> is <code>False</code> then <code>output</code> contains the matrix inverse of <code>input</code>. If <code>adjoint</code> is <code>True</code> then <code>output</code> contains the matrix inverse of the adjoint of <code>input</code>.</p>  <h3 id=\"batch_matrix_inverse\"><code>tf.batch_matrix_inverse(input, adjoint=None, name=None)</code></h3> <p>Computes the inverse of square invertible matrices or their adjoints</p> <p>(conjugate transposes).</p> <p>The input is a tensor of shape <code>[..., M, M]</code> whose inner-most 2 dimensions form square matrices. The output is a tensor of the same shape as the input containing the inverse for all input submatrices <code>[..., :, :]</code>.</p> <p>The op uses LU decomposition with partial pivoting to compute the inverses.</p> <p>If a matrix is not invertible there is no guarantee what the op does. It may detect the condition and raise an exception or it may simply return a garbage result.</p>  <h5 id=\"args-55\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float64</code>, <code>float32</code>. Shape is <code>[..., M, M]</code>.</li> <li>\n<code>adjoint</code>: An optional <code>bool</code>. Defaults to <code>False</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-55\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Shape is <code>[..., M, M]</code>.</p>  <h3 id=\"cholesky\"><code>tf.cholesky(input, name=None)</code></h3> <p>Computes the Cholesky decomposition of a square matrix.</p> <p>The input has to be symmetric and positive definite. Only the lower-triangular part of the input will be used for this operation. The upper-triangular part will not be read.</p> <p>The result is the lower-triangular matrix of the Cholesky decomposition of the input, <code>L</code>, so that <code>input = L L^*</code>.</p>  <h5 id=\"args-56\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float64</code>, <code>float32</code>. Shape is <code>[M, M]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-56\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Shape is <code>[M, M]</code>.</p>  <h3 id=\"batch_cholesky\"><code>tf.batch_cholesky(input, name=None)</code></h3> <p>Computes the Cholesky decomposition of a batch of square matrices.</p> <p>The input is a tensor of shape <code>[..., M, M]</code> whose inner-most 2 dimensions form square matrices, with the same constraints as the single matrix Cholesky decomposition above. The output is a tensor of the same shape as the input containing the Cholesky decompositions for all input submatrices <code>[..., :, :]</code>.</p>  <h5 id=\"args-57\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float64</code>, <code>float32</code>. Shape is <code>[..., M, M]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-57\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. Shape is <code>[..., M, M]</code>.</p>  <h3 id=\"cholesky_solve\"><code>tf.cholesky_solve(chol, rhs, name=None)</code></h3> <p>Solve linear equations <code>A X = RHS</code>, given Cholesky factorization of <code>A</code>.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Solve one system of linear equations (K = 1).\nA = [[3, 1], [1, 3]]\nRHS = [[2], [22]]  # shape 2 x 1\nchol = tf.cholesky(A)\nX = tf.cholesky_solve(chol, RHS)\n# tf.matmul(A, X) ~ RHS\nX[:, 0]  # Solution to the linear system A x = RHS[:, 0]\n\n# Solve five systems of linear equations (K = 5).\nA = [[3, 1], [1, 3]]\nRHS = [[1, 2, 3, 4, 5], [11, 22, 33, 44, 55]]  # shape 2 x 5\n...\nX[:, 2]  # Solution to the linear system A x = RHS[:, 2]\n</pre>  <h5 id=\"args-58\">Args:</h5> <ul> <li>\n<code>chol</code>: A <code>Tensor</code>. Must be <code>float32</code> or <code>float64</code>, shape is <code>[M, M]</code>. Cholesky factorization of <code>A</code>, e.g. <code>chol = tf.cholesky(A)</code>. For that reason, only the lower triangular part (including the diagonal) of <code>chol</code> is used. The strictly upper part is assumed to be zero and not accessed.</li> <li>\n<code>rhs</code>: A <code>Tensor</code>, same type as <code>chol</code>, shape is <code>[M, K]</code>, designating <code>K</code> systems of linear equations.</li> <li>\n<code>name</code>: A name to give this <code>Op</code>. Defaults to <code>cholesky_solve</code>.</li> </ul>  <h5 id=\"returns-58\">Returns:</h5> <p>Solution to <code>A X = RHS</code>, shape <code>[M, K]</code>. The solutions to the <code>K</code> systems.</p>  <h3 id=\"batch_cholesky_solve\"><code>tf.batch_cholesky_solve(chol, rhs, name=None)</code></h3> <p>Solve batches of linear eqns <code>A X = RHS</code>, given Cholesky factorizations.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Solve one linear system (K = 1) for every member of the length 10 batch.\nA = ... # shape 10 x 2 x 2\nRHS = ... # shape 10 x 2 x 1\nchol = tf.batch_cholesky(A)  # shape 10 x 2 x 2\nX = tf.batch_cholesky_solve(chol, RHS)  # shape 10 x 2 x 1\n# tf.matmul(A, X) ~ RHS\nX[3, :, 0]  # Solution to the linear system A[3, :, :] x = RHS[3, :, 0]\n\n# Solve five linear systems (K = 5) for every member of the length 10 batch.\nA = ... # shape 10 x 2 x 2\nRHS = ... # shape 10 x 2 x 5\n...\nX[3, :, 2]  # Solution to the linear system A[3, :, :] x = RHS[3, :, 2]\n</pre>  <h5 id=\"args-59\">Args:</h5> <ul> <li>\n<code>chol</code>: A <code>Tensor</code>. Must be <code>float32</code> or <code>float64</code>, shape is <code>[..., M, M]</code>. Cholesky factorization of <code>A</code>, e.g. <code>chol = tf.batch_cholesky(A)</code>. For that reason, only the lower triangular parts (including the diagonal) of the last two dimensions of <code>chol</code> are used. The strictly upper part is assumed to be zero and not accessed.</li> <li>\n<code>rhs</code>: A <code>Tensor</code>, same type as <code>chol</code>, shape is <code>[..., M, K]</code>.</li> <li>\n<code>name</code>: A name to give this <code>Op</code>. Defaults to <code>batch_cholesky_solve</code>.</li> </ul>  <h5 id=\"returns-59\">Returns:</h5> <p>Solution to <code>A x = rhs</code>, shape <code>[..., M, K]</code>.</p>  <h3 id=\"matrix_solve\"><code>tf.matrix_solve(matrix, rhs, adjoint=None, name=None)</code></h3> <p>Solves a system of linear equations. Checks for invertibility.</p>  <h5 id=\"args-60\">Args:</h5> <ul> <li>\n<code>matrix</code>: A <code>Tensor</code>. Must be one of the following types: <code>float64</code>, <code>float32</code>. Shape is <code>[M, M]</code>.</li> <li>\n<code>rhs</code>: A <code>Tensor</code>. Must have the same type as <code>matrix</code>. Shape is <code>[M, K]</code>.</li> <li>\n<code>adjoint</code>: An optional <code>bool</code>. Defaults to <code>False</code>. Boolean indicating whether to solve with <code>matrix</code> or its adjoint.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-60\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>matrix</code>. Shape is <code>[M, K]</code>. If <code>adjoint</code> is <code>False</code> then <code>output</code> that solves <code>matrix</code> * <code>output</code> = <code>rhs</code>. If <code>adjoint</code> is <code>True</code> then <code>output</code> that solves <code>adjoint(matrix)</code> * <code>output</code> = <code>rhs</code>.</p>  <h3 id=\"batch_matrix_solve\"><code>tf.batch_matrix_solve(matrix, rhs, adjoint=None, name=None)</code></h3> <p>Solves systems of linear equations. Checks for invertibility.</p> <p>Matrix is a tensor of shape <code>[..., M, M]</code> whose inner-most 2 dimensions form square matrices. Rhs is a tensor of shape <code>[..., M, K]</code>. The output is a tensor shape <code>[..., M, K]</code>. If <code>adjoint</code> is <code>False</code> then each output matrix satisfies <code>matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]</code>. If <code>adjoint</code> is <code>True</code> then each output matrix satisfies <code>adjoint(matrix[..., :, :]) * output[..., :, :] = rhs[..., :, :]</code>.</p>  <h5 id=\"args-61\">Args:</h5> <ul> <li>\n<code>matrix</code>: A <code>Tensor</code>. Must be one of the following types: <code>float64</code>, <code>float32</code>. Shape is <code>[..., M, M]</code>.</li> <li>\n<code>rhs</code>: A <code>Tensor</code>. Must have the same type as <code>matrix</code>. Shape is <code>[..., M, K]</code>.</li> <li>\n<code>adjoint</code>: An optional <code>bool</code>. Defaults to <code>False</code>. Boolean indicating whether to solve with <code>matrix</code> or its (block-wise) adjoint.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-61\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>matrix</code>. Shape is <code>[..., M, K]</code>.</p>  <h3 id=\"matrix_triangular_solve\"><code>tf.matrix_triangular_solve(matrix, rhs, lower=None, adjoint=None, name=None)</code></h3> <p>Solves a system of linear equations with an upper or lower triangular matrix by</p> <p>backsubstitution.</p> <p><code>matrix</code> is a matrix of shape <code>[M, M]</code>. If <code>lower</code> is <code>True</code> then the strictly upper triangular part of <code>matrix</code> is assumed to be zero and not accessed. If <code>lower</code> is False then the strictly lower triangular part of <code>matrix</code> is assumed to be zero and not accessed. <code>rhs</code> is a matrix of shape [M, K]`.</p> <p>The output is a matrix of shape <code>[M, K]</code>. If <code>adjoint</code> is <code>False</code> the output satisfies the matrix equation <code>matrix</code> * <code>output</code> = <code>rhs</code>. If <code>adjoint</code> is <code>False</code> then <code>output</code> satisfies the matrix equation <code>matrix</code> * <code>output</code> = <code>rhs</code>. If <code>adjoint</code> is <code>True</code> then <code>output</code> satisfies the matrix equation <code>adjoint(matrix)</code> * <code>output</code> = <code>rhs</code>.</p>  <h5 id=\"args-62\">Args:</h5> <ul> <li>\n<code>matrix</code>: A <code>Tensor</code>. Must be one of the following types: <code>float64</code>, <code>float32</code>. Shape is <code>[M, M]</code>.</li> <li>\n<code>rhs</code>: A <code>Tensor</code>. Must have the same type as <code>matrix</code>. Shape is <code>[M, K]</code>.</li> <li>\n<code>lower</code>: An optional <code>bool</code>. Defaults to <code>True</code>. Boolean indicating whether <code>matrix</code> is lower or upper triangular</li> <li>\n<code>adjoint</code>: An optional <code>bool</code>. Defaults to <code>False</code>. Boolean indicating whether to solve with <code>matrix</code> or its adjoint.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-62\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>matrix</code>. Shape is <code>[M, K]</code>.</p>  <h3 id=\"batch_matrix_triangular_solve\"><code>tf.batch_matrix_triangular_solve(matrix, rhs, lower=None, adjoint=None, name=None)</code></h3> <p>Solves systems of linear equations with upper or lower triangular matrices by</p> <p>backsubstitution.</p> <p><code>matrix</code> is a tensor of shape <code>[..., M, M]</code> whose inner-most 2 dimensions form square matrices. If <code>lower</code> is <code>True</code> then the strictly upper triangular part of each inner-most matrix is assumed to be zero and not accessed. If <code>lower</code> is False then the strictly lower triangular part of each inner-most matrix is assumed to be zero and not accessed. <code>rhs</code> is a tensor of shape [..., M, K]`.</p> <p>The output is a tensor of shape <code>[..., M, K]</code>. If <code>adjoint</code> is <code>True</code> then the innermost matrices in output<code>satisfy matrix equations</code>matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]<code>.\nIf</code>adjoint<code>is</code>False<code>then the strictly then the  innermost matrices in</code>output<code>satisfy matrix equations</code>adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]`.</p>  <h5 id=\"args-63\">Args:</h5> <ul> <li>\n<code>matrix</code>: A <code>Tensor</code>. Must be one of the following types: <code>float64</code>, <code>float32</code>. Shape is <code>[..., M, M]</code>.</li> <li>\n<code>rhs</code>: A <code>Tensor</code>. Must have the same type as <code>matrix</code>. Shape is <code>[..., M, K]</code>.</li> <li>\n<code>lower</code>: An optional <code>bool</code>. Defaults to <code>True</code>. Boolean indicating whether the innermost matrices in <code>matrix</code> are lower or upper triangular.</li> <li>\n<code>adjoint</code>: An optional <code>bool</code>. Defaults to <code>False</code>. Boolean indicating whether to solve with <code>matrix</code> or its (block-wise) adjoint.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-63\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>matrix</code>. Shape is <code>[..., M, K]</code>.</p>  <h3 id=\"matrix_solve_ls\"><code>tf.matrix_solve_ls(matrix, rhs, l2_regularizer=0.0, fast=True, name=None)</code></h3> <p>Solves a linear least-squares problem.</p> <p>Below we will use the following notation <code>matrix</code>=\\(A \\in \\Re^{m \\times n}\\), <code>rhs</code>=\\(B \\in \\Re^{m \\times k}\\), <code>output</code>=\\(X \\in \\Re^{n \\times k}\\), <code>l2_regularizer</code>=\\(\\lambda\\).</p> <p>If <code>fast</code> is <code>True</code>, then the solution is computed by solving the normal equations using Cholesky decomposition. Specifically, if \\(m \\ge n\\) then \\(X = (A^T A + \\lambda I)^{-1} A^T B\\), which solves the regularized least-squares problem \\(X = \\mathrm{argmin}_{Z \\in \\Re^{n \\times k}} ||A Z - B||_F^2 + \\lambda ||Z||_F^2\\). If \\(m \\lt n\\) then <code>output</code> is computed as \\(X = A^T (A A^T + \\lambda I)^{-1} B\\), which (for \\(\\lambda = 0\\)) is the minimum-norm solution to the under-determined linear system, i.e. \\(X = \\mathrm{argmin}_{Z \\in \\Re^{n \\times k}} ||Z||_F^2 \\), subject to \\(A Z = B\\). Notice that the fast path is only numerically stable when \\(A\\) is numerically full rank and has a condition number \\(\\mathrm{cond}(A) \\lt \\frac{1}{\\sqrt{\\epsilon_{mach}}}\\) or \\(\\lambda\\) is sufficiently large.</p> <p>If <code>fast</code> is <code>False</code> then the solution is computed using the rank revealing QR decomposition with column pivoting. This will always compute a least-squares solution that minimizes the residual norm \\(||A X - B||_F^2 \\), even when \\(A\\) is rank deficient or ill-conditioned. Notice: The current version does not compute a minimum norm solution. If <code>fast</code> is <code>False</code> then <code>l2_regularizer</code> is ignored.</p>  <h5 id=\"args-64\">Args:</h5> <ul> <li>\n<code>matrix</code>: 2-D <code>Tensor</code> of shape <code>[M, N]</code>.</li> <li>\n<code>rhs</code>: 2-D <code>Tensor</code> of shape is <code>[M, K]</code>.</li> <li>\n<code>l2_regularizer</code>: 0-D <code>double</code> <code>Tensor</code>. Ignored if <code>fast=False</code>.</li> <li>\n<code>fast</code>: bool. Defaults to <code>True</code>.</li> <li>\n<code>name</code>: string, optional name of the operation.</li> </ul>  <h5 id=\"returns-64\">Returns:</h5> <ul> <li>\n<code>output</code>: Matrix of shape <code>[N, K]</code> containing the matrix that solves <code>matrix * output = rhs</code> in the least-squares sense.</li> </ul>  <h3 id=\"batch_matrix_solve_ls\"><code>tf.batch_matrix_solve_ls(matrix, rhs, l2_regularizer=0.0, fast=True, name=None)</code></h3> <p>Solves multiple linear least-squares problems.</p> <p><code>matrix</code> is a tensor of shape <code>[..., M, N]</code> whose inner-most 2 dimensions form <code>M</code>-by-<code>N</code> matrices. Rhs is a tensor of shape <code>[..., M, K]</code> whose inner-most 2 dimensions form <code>M</code>-by-<code>K</code> matrices. The computed output is a <code>Tensor</code> of shape <code>[..., N, K]</code> whose inner-most 2 dimensions form <code>M</code>-by-<code>K</code> matrices that solve the equations <code>matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]</code> in the least squares sense.</p> <p>Below we will use the following notation for each pair of matrix and right-hand sides in the batch:</p> <p><code>matrix</code>=\\(A \\in \\Re^{m \\times n}\\), <code>rhs</code>=\\(B \\in \\Re^{m \\times k}\\), <code>output</code>=\\(X \\in \\Re^{n \\times k}\\), <code>l2_regularizer</code>=\\(\\lambda\\).</p> <p>If <code>fast</code> is <code>True</code>, then the solution is computed by solving the normal equations using Cholesky decomposition. Specifically, if \\(m \\ge n\\) then \\(X = (A^T A + \\lambda I)^{-1} A^T B\\), which solves the least-squares problem \\(X = \\mathrm{argmin}_{Z \\in \\Re^{n \\times k}} ||A Z - B||_F^2 + \\lambda ||Z||_F^2\\). If \\(m \\lt n\\) then <code>output</code> is computed as \\(X = A^T (A A^T + \\lambda I)^{-1} B\\), which (for \\(\\lambda = 0\\)) is the minimum-norm solution to the under-determined linear system, i.e. \\(X = \\mathrm{argmin}_{Z \\in \\Re^{n \\times k}} ||Z||_F^2 \\), subject to \\(A Z = B\\). Notice that the fast path is only numerically stable when \\(A\\) is numerically full rank and has a condition number \\(\\mathrm{cond}(A) \\lt \\frac{1}{\\sqrt{\\epsilon_{mach}}}\\) or\\(\\lambda\\) is sufficiently large.</p> <p>If <code>fast</code> is <code>False</code> an algorithm based on the numerically robust complete orthogonal decomposition is used. This computes the minimum-norm least-squares solution, even when \\(A\\) is rank deficient. This path is typically 6-7 times slower than the fast path. If <code>fast</code> is <code>False</code> then <code>l2_regularizer</code> is ignored.</p>  <h5 id=\"args-65\">Args:</h5> <ul> <li>\n<code>matrix</code>: <code>Tensor</code> of shape <code>[..., M, N]</code>.</li> <li>\n<code>rhs</code>: <code>Tensor</code> of shape <code>[..., M, K]</code>.</li> <li>\n<code>l2_regularizer</code>: 0-D <code>double</code> <code>Tensor</code>. Ignored if <code>fast=False</code>.</li> <li>\n<code>fast</code>: bool. Defaults to <code>True</code>.</li> <li>\n<code>name</code>: string, optional name of the operation.</li> </ul>  <h5 id=\"returns-65\">Returns:</h5> <ul> <li>\n<code>output</code>: <code>Tensor</code> of shape <code>[..., N, K]</code> whose inner-most 2 dimensions form <code>M</code>-by-<code>K</code> matrices that solve the equations <code>matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]</code> in the least squares sense.</li> </ul>  <h3 id=\"self_adjoint_eig\"><code>tf.self_adjoint_eig(matrix, name=None)</code></h3> <p>Computes the eigen decomposition of a self-adjoint matrix.</p> <p>Computes the eigenvalues and eigenvectors of an N-by-N matrix <code>matrix</code> such that <code>matrix * v[:,i] = e(i) * v[:,i]</code>, for i=0...N-1.</p>  <h5 id=\"args-66\">Args:</h5> <ul> <li>\n<code>matrix</code>: <code>Tensor</code> of shape <code>[N, N]</code>.</li> <li>\n<code>name</code>: string, optional name of the operation.</li> </ul>  <h5 id=\"returns-66\">Returns:</h5> <ul> <li>\n<code>e</code>: Eigenvalues. Shape is <code>[N]</code>.</li> <li>\n<code>v</code>: Eigenvectors. Shape is <code>[N, N]</code>. The columns contain the eigenvectors of <code>matrix</code>.</li> </ul>  <h3 id=\"batch_self_adjoint_eig\"><code>tf.batch_self_adjoint_eig(tensor, name=None)</code></h3> <p>Computes the eigen decomposition of a batch of self-adjoint matrices.</p> <p>Computes the eigenvalues and eigenvectors of the innermost N-by-N matrices in <code>tensor</code> such that <code>tensor[...,:,:] * v[..., :,i] = e(..., i) * v[...,:,i]</code>, for i=0...N-1.</p>  <h5 id=\"args-67\">Args:</h5> <ul> <li>\n<code>tensor</code>: <code>Tensor</code> of shape <code>[..., N, N]</code>.</li> <li>\n<code>name</code>: string, optional name of the operation.</li> </ul>  <h5 id=\"returns-67\">Returns:</h5> <ul> <li>\n<code>e</code>: Eigenvalues. Shape is <code>[..., N]</code>.</li> <li>\n<code>v</code>: Eigenvectors. Shape is <code>[..., N, N]</code>. The columns of the inner most matrices contain eigenvectors of the corresponding matrices in <code>tensor</code>\n</li> </ul>  <h3 id=\"self_adjoint_eigvals\"><code>tf.self_adjoint_eigvals(matrix, name=None)</code></h3> <p>Computes the eigenvalues a self-adjoint matrix.</p>  <h5 id=\"args-68\">Args:</h5> <ul> <li>\n<code>matrix</code>: <code>Tensor</code> of shape <code>[N, N]</code>.</li> <li>\n<code>name</code>: string, optional name of the operation.</li> </ul>  <h5 id=\"returns-68\">Returns:</h5> <ul> <li>\n<code>e</code>: Eigenvalues of <code>matrix</code>. Shape is <code>[N]</code>.</li> </ul>  <h3 id=\"batch_self_adjoint_eigvals\"><code>tf.batch_self_adjoint_eigvals(tensor, name=None)</code></h3> <p>Computes the eigenvalues of a batch of self-adjoint matrices.</p>  <h5 id=\"args-69\">Args:</h5> <ul> <li>\n<code>tensor</code>: <code>Tensor</code> of shape <code>[..., N, N]</code>.</li> <li>\n<code>name</code>: string, optional name of the operation.</li> </ul>  <h5 id=\"returns-69\">Returns:</h5> <ul> <li>\n<code>e</code>: Eigenvalues. Shape is <code>[..., N]</code>. The vector <code>e[..., :]</code> contains the <code>N</code> eigenvalues of <code>tensor[..., :, :]</code>.</li> </ul>  <h3 id=\"svd\"><code>tf.svd(matrix, compute_uv=True, full_matrices=False, name=None)</code></h3> <p>Computes the singular value decomposition of a matrix.</p> <p>Computes the SVD of <code>matrix</code> such that <code>matrix = u * diag(s) *\ntranspose(v)</code></p> <pre class=\"lang-prettyprint no-auto-prettify\"># a is a matrix.\n# s is a vector of singular values.\n# u is the matrix of left singular vectors.\n# v is a matrix of right singular vectors.\ns, u, v = svd(a)\ns = svd(a, compute_uv=False)\n</pre>  <h5 id=\"args-70\">Args:</h5> <ul> <li>\n<code>matrix</code>: <code>Tensor</code> of shape <code>[M, N]</code>. Let <code>P</code> be the minimum of <code>M</code> and <code>N</code>.</li> <li>\n<code>compute_uv</code>: If <code>True</code> then left and right singular vectors will be computed and returned in <code>u</code> and <code>v</code>, respectively. Otherwise, only the singular values will be computed, which can be significantly faster.</li> <li>\n<code>full_matrices</code>: If true, compute full-sized <code>u</code> and <code>v</code>. If false (the default), compute only the leading <code>P</code> singular vectors. Ignored if <code>compute_uv</code> is <code>False</code>.</li> <li>\n<code>name</code>: string, optional name of the operation.</li> </ul>  <h5 id=\"returns-70\">Returns:</h5> <ul> <li>\n<code>s</code>: Singular values. Shape is <code>[P]</code>.</li> <li>\n<code>u</code>: Right singular vectors. If <code>full_matrices</code> is <code>False</code> (default) then shape is <code>[M, P]</code>; if <code>full_matrices</code> is <code>True</code> then shape is <code>[M, M]</code>. Not returned if <code>compute_uv</code> is <code>False</code>.</li> <li>\n<code>v</code>: Left singular vectors. If <code>full_matrices</code> is <code>False</code> (default) then shape is <code>[N, P]</code>. If <code>full_matrices</code> is <code>True</code> then shape is <code>[N, N]</code>. Not returned if <code>compute_uv</code> is <code>False</code>.</li> </ul>  <h3 id=\"batch_svd\"><code>tf.batch_svd(tensor, compute_uv=True, full_matrices=False, name=None)</code></h3> <p>Computes the singular value decompositions of a batch of matrices.</p> <p>Computes the SVD of each inner matrix in <code>tensor</code> such that <code>tensor[..., :, :] = u[..., :, :] * diag(s[..., :, :]) * transpose(v[..., :,\n:])</code></p> <pre class=\"lang-prettyprint no-auto-prettify\"># a is a tensor.\n# s is a tensor of singular values.\n# u is a tensor of left singular vectors.\n# v is a tensor of right singular vectors.\ns, u, v = batch_svd(a)\ns = batch_svd(a, compute_uv=False)\n</pre>  <h5 id=\"args-71\">Args:</h5> <ul> <li>\n<code>matrix</code>: <code>Tensor</code> of shape <code>[..., M, N]</code>. Let <code>P</code> be the minimum of <code>M</code> and <code>N</code>.</li> <li>\n<code>compute_uv</code>: If <code>True</code> then left and right singular vectors will be computed and returned in <code>u</code> and <code>v</code>, respectively. Otherwise, only the singular values will be computed, which can be significantly faster.</li> <li>\n<code>full_matrices</code>: If true, compute full-sized <code>u</code> and <code>v</code>. If false (the default), compute only the leading <code>P</code> singular vectors. Ignored if <code>compute_uv</code> is <code>False</code>.</li> <li>\n<code>name</code>: string, optional name of the operation.</li> </ul>  <h5 id=\"returns-71\">Returns:</h5> <ul> <li>\n<code>s</code>: Singular values. Shape is <code>[..., P]</code>.</li> <li>\n<code>u</code>: Right singular vectors. If <code>full_matrices</code> is <code>False</code> (default) then shape is <code>[..., M, P]</code>; if <code>full_matrices</code> is <code>True</code> then shape is <code>[..., M, M]</code>. Not returned if <code>compute_uv</code> is <code>False</code>.</li> <li>\n<code>v</code>: Left singular vectors. If <code>full_matrices</code> is <code>False</code> (default) then shape is <code>[..., N, P]</code>. If <code>full_matrices</code> is <code>True</code> then shape is <code>[..., N, N]</code>. Not returned if <code>compute_uv</code> is <code>False</code>.</li> </ul>  <h2 id=\"complex-number-functions\">Complex Number Functions</h2> <p>TensorFlow provides several operations that you can use to add complex number functions to your graph.</p>  <h3 id=\"complex\"><code>tf.complex(real, imag, name=None)</code></h3> <p>Converts two real numbers to a complex number.</p> <p>Given a tensor <code>real</code> representing the real part of a complex number, and a tensor <code>imag</code> representing the imaginary part of a complex number, this operation returns complex numbers elementwise of the form (a + bj), where <em>a</em> represents the <code>real</code> part and <em>b</em> represents the <code>imag</code> part.</p> <p>The input tensors <code>real</code> and <code>imag</code> must have the same shape.</p> <p>For example:</p> <pre class=\"\"># tensor 'real' is [2.25, 3.25]\n# tensor `imag` is [4.75, 5.75]\ntf.complex(real, imag) ==&gt; [[2.25 + 4.75j], [3.25 + 5.75j]]\n</pre>  <h5 id=\"args-72\">Args:</h5> <ul> <li>\n<code>real</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>imag</code>: A <code>Tensor</code>. Must have the same type as <code>real</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-72\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code> or <code>complex128</code>.</p>  <h3 id=\"complex_abs\"><code>tf.complex_abs(x, name=None)</code></h3> <p>Computes the complex absolute value of a tensor.</p> <p>Given a tensor <code>x</code> of complex numbers, this operation returns a tensor of type <code>float32</code> or <code>float64</code> that is the absolute value of each element in <code>x</code>. All elements in <code>x</code> must be complex numbers of the form \\(a + bj\\). The absolute value is computed as \\( \\sqrt{a^2 + b^2}\\).</p> <p>For example:</p> <pre class=\"\"># tensor 'x' is [[-2.25 + 4.75j], [-3.25 + 5.75j]]\ntf.complex_abs(x) ==&gt; [5.25594902, 6.60492229]\n</pre>  <h5 id=\"args-73\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> of type <code>complex64</code> or <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-73\">Returns:</h5> <p>A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</p>  <h3 id=\"conj\"><code>tf.conj(input, name=None)</code></h3> <p>Returns the complex conjugate of a complex number.</p> <p>Given a tensor <code>input</code> of complex numbers, this operation returns a tensor of complex numbers that are the complex conjugate of each element in <code>input</code>. The complex numbers in <code>input</code> must be of the form \\(a + bj\\), where <em>a</em> is the real part and <em>b</em> is the imaginary part.</p> <p>The complex conjugate returned by this operation is of the form \\(a - bj\\).</p> <p>For example:</p> <pre class=\"\"># tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.conj(input) ==&gt; [-2.25 - 4.75j, 3.25 - 5.75j]\n</pre>  <h5 id=\"args-74\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-74\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h3 id=\"imag\"><code>tf.imag(input, name=None)</code></h3> <p>Returns the imaginary part of a complex number.</p> <p>Given a tensor <code>input</code> of complex numbers, this operation returns a tensor of type <code>float32</code> or <code>float64</code> that is the imaginary part of each element in <code>input</code>. All elements in <code>input</code> must be complex numbers of the form (a + bj), where <em>a</em> is the real part and <em>b</em> is the imaginary part returned by this operation.</p> <p>For example:</p> <pre class=\"\"># tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.imag(input) ==&gt; [4.75, 5.75]\n</pre>  <h5 id=\"args-75\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-75\">Returns:</h5> <p>A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</p>  <h3 id=\"real\"><code>tf.real(input, name=None)</code></h3> <p>Returns the real part of a complex number.</p> <p>Given a tensor <code>input</code> of complex numbers, this operation returns a tensor of type <code>float32</code> or <code>float64</code> that is the real part of each element in <code>input</code>. All elements in <code>input</code> must be complex numbers of the form (a + bj), where <em>a</em> is the real part returned by this operation and <em>b</em> is the imaginary part.</p> <p>For example:</p> <pre class=\"\"># tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.real(input) ==&gt; [-2.25, 3.25]\n</pre>  <h5 id=\"args-76\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>complex64</code>, <code>complex128</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-76\">Returns:</h5> <p>A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</p>  <h3 id=\"fft\"><code>tf.fft(input, name=None)</code></h3> <p>Compute the 1-dimensional discrete Fourier Transform.</p>  <h5 id=\"args-77\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 vector.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-77\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. The 1D Fourier Transform of <code>input</code>.</p>  <h3 id=\"ifft\"><code>tf.ifft(input, name=None)</code></h3> <p>.Doc(R\"doc(</p> <p>Compute the inverse 1-dimensional discrete Fourier Transform.</p>  <h5 id=\"args-78\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 vector.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-78\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. The inverse 1D Fourier Transform of <code>input</code>.</p>  <h3 id=\"fft2d\"><code>tf.fft2d(input, name=None)</code></h3> <p>Compute the 2-dimensional discrete Fourier Transform.</p>  <h5 id=\"args-79\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 matrix.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-79\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. The 2D Fourier Transform of <code>input</code>.</p>  <h3 id=\"ifft2d\"><code>tf.ifft2d(input, name=None)</code></h3> <p>Compute the inverse 2-dimensional discrete Fourier Transform.</p>  <h5 id=\"args-80\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 matrix.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-80\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. The inverse 2D Fourier Transform of <code>input</code>.</p>  <h3 id=\"fft3d\"><code>tf.fft3d(input, name=None)</code></h3> <p>Compute the 3-dimensional discrete Fourier Transform.</p>  <h5 id=\"args-81\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 3-D tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-81\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. The 3D Fourier Transform of <code>input</code>.</p>  <h3 id=\"ifft3d\"><code>tf.ifft3d(input, name=None)</code></h3> <p>Compute the inverse 3-dimensional discrete Fourier Transform.</p>  <h5 id=\"args-82\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 3-D tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-82\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. The inverse 3D Fourier Transform of <code>input</code>.</p>  <h3 id=\"batch_fft\"><code>tf.batch_fft(input, name=None)</code></h3> <p>Compute the 1-dimensional discrete Fourier Transform over the inner-most</p> <p>dimension of <code>input</code>.</p>  <h5 id=\"args-83\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-83\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor of the same shape as <code>input</code>. The inner-most dimension of <code>input</code> is replaced with its 1D Fourier Transform.</p>  <h3 id=\"batch_ifft\"><code>tf.batch_ifft(input, name=None)</code></h3> <p>Compute the inverse 1-dimensional discrete Fourier Transform over the inner-most</p> <p>dimension of <code>input</code>.</p>  <h5 id=\"args-84\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-84\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor of the same shape as <code>input</code>. The inner-most dimension of <code>input</code> is replaced with its inverse 1D Fourier Transform.</p>  <h3 id=\"batch_fft2d\"><code>tf.batch_fft2d(input, name=None)</code></h3> <p>Compute the 2-dimensional discrete Fourier Transform over the inner-most</p> <p>2 dimensions of <code>input</code>.</p>  <h5 id=\"args-85\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-85\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor of the same shape as <code>input</code>. The inner-most 2 dimensions of <code>input</code> are replaced with their 2D Fourier Transform.</p>  <h3 id=\"batch_ifft2d\"><code>tf.batch_ifft2d(input, name=None)</code></h3> <p>Compute the inverse 2-dimensional discrete Fourier Transform over the inner-most</p> <p>2 dimensions of <code>input</code>.</p>  <h5 id=\"args-86\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-86\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor of the same shape as <code>input</code>. The inner-most 2 dimensions of <code>input</code> are replaced with their inverse 2D Fourier Transform.</p>  <h3 id=\"batch_fft3d\"><code>tf.batch_fft3d(input, name=None)</code></h3> <p>Compute the 3-dimensional discrete Fourier Transform over the inner-most 3</p> <p>dimensions of <code>input</code>.</p>  <h5 id=\"args-87\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-87\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor of the same shape as <code>input</code>. The inner-most 3 dimensions of <code>input</code> are replaced with their 3D Fourier Transform.</p>  <h3 id=\"batch_ifft3d\"><code>tf.batch_ifft3d(input, name=None)</code></h3> <p>Compute the inverse 3-dimensional discrete Fourier Transform over the inner-most</p> <p>3 dimensions of <code>input</code>.</p>  <h5 id=\"args-88\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-88\">Returns:</h5> <p>A <code>Tensor</code> of type <code>complex64</code>. A complex64 tensor of the same shape as <code>input</code>. The inner-most 3 dimensions of <code>input</code> are replaced with their inverse 3D Fourier Transform.</p> <h2 id=\"reduction\">Reduction</h2> <p>TensorFlow provides several operations that you can use to perform common math computations that reduce various dimensions of a tensor.</p>  <h3 id=\"reduce_sum\"><code>tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></h3> <p>Computes the sum of elements across dimensions of a tensor.</p> <p>Reduces <code>input_tensor</code> along the dimensions given in <code>reduction_indices</code>. Unless <code>keep_dims</code> is true, the rank of the tensor is reduced by 1 for each entry in <code>reduction_indices</code>. If <code>keep_dims</code> is true, the reduced dimensions are retained with length 1.</p> <p>If <code>reduction_indices</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'x' is [[1, 1, 1]\n#         [1, 1, 1]]\ntf.reduce_sum(x) ==&gt; 6\ntf.reduce_sum(x, 0) ==&gt; [2, 2, 2]\ntf.reduce_sum(x, 1) ==&gt; [3, 3]\ntf.reduce_sum(x, 1, keep_dims=True) ==&gt; [[3], [3]]\ntf.reduce_sum(x, [0, 1]) ==&gt; 6\n</pre>  <h5 id=\"args-89\">Args:</h5> <ul> <li>\n<code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li> <li>\n<code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions.</li> <li>\n<code>keep_dims</code>: If true, retains reduced dimensions with length 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-89\">Returns:</h5> <p>The reduced tensor.</p>  <h3 id=\"reduce_prod\"><code>tf.reduce_prod(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></h3> <p>Computes the product of elements across dimensions of a tensor.</p> <p>Reduces <code>input_tensor</code> along the dimensions given in <code>reduction_indices</code>. Unless <code>keep_dims</code> is true, the rank of the tensor is reduced by 1 for each entry in <code>reduction_indices</code>. If <code>keep_dims</code> is true, the reduced dimensions are retained with length 1.</p> <p>If <code>reduction_indices</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>  <h5 id=\"args-90\">Args:</h5> <ul> <li>\n<code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li> <li>\n<code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions.</li> <li>\n<code>keep_dims</code>: If true, retains reduced dimensions with length 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-90\">Returns:</h5> <p>The reduced tensor.</p>  <h3 id=\"reduce_min\"><code>tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></h3> <p>Computes the minimum of elements across dimensions of a tensor.</p> <p>Reduces <code>input_tensor</code> along the dimensions given in <code>reduction_indices</code>. Unless <code>keep_dims</code> is true, the rank of the tensor is reduced by 1 for each entry in <code>reduction_indices</code>. If <code>keep_dims</code> is true, the reduced dimensions are retained with length 1.</p> <p>If <code>reduction_indices</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>  <h5 id=\"args-91\">Args:</h5> <ul> <li>\n<code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li> <li>\n<code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions.</li> <li>\n<code>keep_dims</code>: If true, retains reduced dimensions with length 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-91\">Returns:</h5> <p>The reduced tensor.</p>  <h3 id=\"reduce_max\"><code>tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></h3> <p>Computes the maximum of elements across dimensions of a tensor.</p> <p>Reduces <code>input_tensor</code> along the dimensions given in <code>reduction_indices</code>. Unless <code>keep_dims</code> is true, the rank of the tensor is reduced by 1 for each entry in <code>reduction_indices</code>. If <code>keep_dims</code> is true, the reduced dimensions are retained with length 1.</p> <p>If <code>reduction_indices</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>  <h5 id=\"args-92\">Args:</h5> <ul> <li>\n<code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li> <li>\n<code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions.</li> <li>\n<code>keep_dims</code>: If true, retains reduced dimensions with length 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-92\">Returns:</h5> <p>The reduced tensor.</p>  <h3 id=\"reduce_mean\"><code>tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></h3> <p>Computes the mean of elements across dimensions of a tensor.</p> <p>Reduces <code>input_tensor</code> along the dimensions given in <code>reduction_indices</code>. Unless <code>keep_dims</code> is true, the rank of the tensor is reduced by 1 for each entry in <code>reduction_indices</code>. If <code>keep_dims</code> is true, the reduced dimensions are retained with length 1.</p> <p>If <code>reduction_indices</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'x' is [[1., 1.]\n#         [2., 2.]]\ntf.reduce_mean(x) ==&gt; 1.5\ntf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]\ntf.reduce_mean(x, 1) ==&gt; [1.,  2.]\n</pre>  <h5 id=\"args-93\">Args:</h5> <ul> <li>\n<code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li> <li>\n<code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions.</li> <li>\n<code>keep_dims</code>: If true, retains reduced dimensions with length 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-93\">Returns:</h5> <p>The reduced tensor.</p>  <h3 id=\"reduce_all\"><code>tf.reduce_all(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></h3> <p>Computes the \"logical and\" of elements across dimensions of a tensor.</p> <p>Reduces <code>input_tensor</code> along the dimensions given in <code>reduction_indices</code>. Unless <code>keep_dims</code> is true, the rank of the tensor is reduced by 1 for each entry in <code>reduction_indices</code>. If <code>keep_dims</code> is true, the reduced dimensions are retained with length 1.</p> <p>If <code>reduction_indices</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'x' is [[True,  True]\n#         [False, False]]\ntf.reduce_all(x) ==&gt; False\ntf.reduce_all(x, 0) ==&gt; [False, False]\ntf.reduce_all(x, 1) ==&gt; [True, False]\n</pre>  <h5 id=\"args-94\">Args:</h5> <ul> <li>\n<code>input_tensor</code>: The boolean tensor to reduce.</li> <li>\n<code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions.</li> <li>\n<code>keep_dims</code>: If true, retains reduced dimensions with length 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-94\">Returns:</h5> <p>The reduced tensor.</p>  <h3 id=\"reduce_any\"><code>tf.reduce_any(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></h3> <p>Computes the \"logical or\" of elements across dimensions of a tensor.</p> <p>Reduces <code>input_tensor</code> along the dimensions given in <code>reduction_indices</code>. Unless <code>keep_dims</code> is true, the rank of the tensor is reduced by 1 for each entry in <code>reduction_indices</code>. If <code>keep_dims</code> is true, the reduced dimensions are retained with length 1.</p> <p>If <code>reduction_indices</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'x' is [[True,  True]\n#         [False, False]]\ntf.reduce_any(x) ==&gt; True\ntf.reduce_any(x, 0) ==&gt; [True, True]\ntf.reduce_any(x, 1) ==&gt; [True, False]\n</pre>  <h5 id=\"args-95\">Args:</h5> <ul> <li>\n<code>input_tensor</code>: The boolean tensor to reduce.</li> <li>\n<code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions.</li> <li>\n<code>keep_dims</code>: If true, retains reduced dimensions with length 1.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-95\">Returns:</h5> <p>The reduced tensor.</p>  <h3 id=\"accumulate_n\"><code>tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None)</code></h3> <p>Returns the element-wise sum of a list of tensors.</p> <p>Optionally, pass <code>shape</code> and <code>tensor_dtype</code> for shape and type checking, otherwise, these are inferred.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># tensor 'a' is [[1, 2], [3, 4]]\n# tensor `b` is [[5, 0], [0, 6]]\ntf.accumulate_n([a, b, a]) ==&gt; [[7, 4], [6, 14]]\n\n# Explicitly pass shape and type\ntf.accumulate_n([a, b, a], shape=[2, 2], tensor_dtype=tf.int32)\n  ==&gt; [[7, 4], [6, 14]]\n</pre>  <h5 id=\"args-96\">Args:</h5> <ul> <li>\n<code>inputs</code>: A list of <code>Tensor</code> objects, each with same shape and type.</li> <li>\n<code>shape</code>: Shape of elements of <code>inputs</code>.</li> <li>\n<code>tensor_dtype</code>: The type of <code>inputs</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-96\">Returns:</h5> <p>A <code>Tensor</code> of same shape and type as the elements of <code>inputs</code>.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>inputs</code> don't all have same shape and dtype or the shape cannot be inferred.</li> </ul> <h2 id=\"scan\">Scan</h2> <p>TensorFlow provides several operations that you can use to perform scans (running totals) across one axis of a tensor.</p>  <h3 id=\"cumsum\"><code>tf.cumsum(x, axis=0, exclusive=False, reverse=False, name=None)</code></h3> <p>Compute the cumulative sum of the tensor <code>x</code> along <code>axis</code>.</p> <p>By default, this op performs an inclusive cumsum, which means that the first element of the input is identical to the first element of the output: <code>prettyprint\ntf.cumsum([a, b, c]) ==&gt; [a, a + b, a + b + c]</code></p> <p>By setting the <code>exclusive</code> kwarg to <code>True</code>, an exclusive cumsum is performed instead: <code>prettyprint\ntf.cumsum([a, b, c], exclusive=True) ==&gt; [0, a, a + b]</code></p> <p>By setting the <code>reverse</code> kwarg to <code>True</code>, the cumsum is performed in the opposite direction: <code>prettyprint\ntf.cumsum([a, b, c], reverse=True) ==&gt; [a + b + c, b + c, c]</code> This is more efficient than using separate <code>tf.reverse</code> ops.</p> <p>The <code>reverse</code> and <code>exclusive</code> kwargs can also be combined: <code>prettyprint\ntf.cumsum([a, b, c], exclusive=True, reverse=True) ==&gt; [b + c, c, 0]</code></p>  <h5 id=\"args-97\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.</li> <li>\n<code>axis</code>: A <code>Tensor</code> of type <code>int32</code> (default: 0).</li> <li>\n<code>reverse</code>: A <code>bool</code> (default: False).</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-97\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>  <h3 id=\"cumprod\"><code>tf.cumprod(x, axis=0, exclusive=False, reverse=False, name=None)</code></h3> <p>Compute the cumulative product of the tensor <code>x</code> along <code>axis</code>.</p> <p>By default, this op performs an inclusive cumprod, which means that the first element of the input is identical to the first element of the output: <code>prettyprint\ntf.cumprod([a, b, c]) ==&gt; [a, a * b, a * b * c]</code></p> <p>By setting the <code>exclusive</code> kwarg to <code>True</code>, an exclusive cumprod is performed instead: <code>prettyprint\ntf.cumprod([a, b, c], exclusive=True) ==&gt; [0, a, a * b]</code></p> <p>By setting the <code>reverse</code> kwarg to <code>True</code>, the cumprod is performed in the opposite direction: <code>prettyprint\ntf.cumprod([a, b, c], reverse=True) ==&gt; [a * b * c, b * c, c]</code> This is more efficient than using separate <code>tf.reverse</code> ops.</p> <p>The <code>reverse</code> and <code>exclusive</code> kwargs can also be combined: <code>prettyprint\ntf.cumprod([a, b, c], exclusive=True, reverse=True) ==&gt; [b * c, c, 0]</code></p>  <h5 id=\"args-98\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.</li> <li>\n<code>axis</code>: A <code>Tensor</code> of type <code>int32</code> (default: 0).</li> <li>\n<code>reverse</code>: A <code>bool</code> (default: False).</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-98\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p> <h2 id=\"segmentation\">Segmentation</h2> <p>TensorFlow provides several operations that you can use to perform common math computations on tensor segments. Here a segmentation is a partitioning of a tensor along the first dimension, i.e. it defines a mapping from the first dimension onto <code>segment_ids</code>. The <code>segment_ids</code> tensor should be the size of the first dimension, <code>d0</code>, with consecutive IDs in the range <code>0</code> to <code>k</code>, where <code>k&lt;d0</code>. In particular, a segmentation of a matrix tensor is a mapping of rows to segments.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])\ntf.segment_sum(c, tf.constant([0, 0, 1]))\n  ==&gt;  [[0 0 0 0]\n        [5 6 7 8]]\n</pre>  <h3 id=\"segment_sum\"><code>tf.segment_sum(data, segment_ids, name=None)</code></h3> <p>Computes the sum along segments of a tensor.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p> <p>Computes a tensor such that \\(output_i = \\sum_j data_j\\) where sum is over <code>j</code> such that <code>segment_ids[j] == i</code>.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/SegmentSum.png\" alt> </div>  <h5 id=\"args-99\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A 1-D tensor whose rank is equal to the rank of <code>data</code>'s first dimension. Values should be sorted and can be repeated.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-99\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for dimension 0 which has size <code>k</code>, the number of segments.</p>  <h3 id=\"segment_prod\"><code>tf.segment_prod(data, segment_ids, name=None)</code></h3> <p>Computes the product along segments of a tensor.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p> <p>Computes a tensor such that \\(output_i = \\prod_j data_j\\) where the product is over <code>j</code> such that <code>segment_ids[j] == i</code>.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/SegmentProd.png\" alt> </div>  <h5 id=\"args-100\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A 1-D tensor whose rank is equal to the rank of <code>data</code>'s first dimension. Values should be sorted and can be repeated.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-100\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for dimension 0 which has size <code>k</code>, the number of segments.</p>  <h3 id=\"segment_min\"><code>tf.segment_min(data, segment_ids, name=None)</code></h3> <p>Computes the minimum along segments of a tensor.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p> <p>Computes a tensor such that \\(output_i = \\min_j(data_j)\\) where <code>min</code> is over <code>j</code> such that <code>segment_ids[j] == i</code>.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/SegmentMin.png\" alt> </div>  <h5 id=\"args-101\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A 1-D tensor whose rank is equal to the rank of <code>data</code>'s first dimension. Values should be sorted and can be repeated.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-101\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for dimension 0 which has size <code>k</code>, the number of segments.</p>  <h3 id=\"segment_max\"><code>tf.segment_max(data, segment_ids, name=None)</code></h3> <p>Computes the maximum along segments of a tensor.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p> <p>Computes a tensor such that \\(output_i = \\max_j(data_j)\\) where <code>max</code> is over <code>j</code> such that <code>segment_ids[j] == i</code>.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/SegmentMax.png\" alt> </div>  <h5 id=\"args-102\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A 1-D tensor whose rank is equal to the rank of <code>data</code>'s first dimension. Values should be sorted and can be repeated.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-102\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for dimension 0 which has size <code>k</code>, the number of segments.</p>  <h3 id=\"segment_mean\"><code>tf.segment_mean(data, segment_ids, name=None)</code></h3> <p>Computes the mean along segments of a tensor.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p> <p>Computes a tensor such that \\(output_i = \\frac{\\sum_j data_j}{N}\\) where <code>mean</code> is over <code>j</code> such that <code>segment_ids[j] == i</code> and <code>N</code> is the total number of values summed.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/SegmentMean.png\" alt> </div>  <h5 id=\"args-103\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A 1-D tensor whose rank is equal to the rank of <code>data</code>'s first dimension. Values should be sorted and can be repeated.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-103\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for dimension 0 which has size <code>k</code>, the number of segments.</p>  <h3 id=\"unsorted_segment_sum\"><code>tf.unsorted_segment_sum(data, segment_ids, num_segments, name=None)</code></h3> <p>Computes the sum along segments of a tensor.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p> <p>Computes a tensor such that <code>(output[i] = sum_{j...} data[j...]</code> where the sum is over tuples <code>j...</code> such that <code>segment_ids[j...] == i</code>. Unlike <code>SegmentSum</code>, <code>segment_ids</code> need not be sorted and need not cover all values in the full range of valid values.</p> <p>If the sum is empty for a given segment ID <code>i</code>, <code>output[i] = 0</code>.</p> <p><code>num_segments</code> should equal the number of distinct segment IDs.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/UnsortedSegmentSum.png\" alt> </div>  <h5 id=\"args-104\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A tensor whose shape is a prefix of <code>data.shape</code>.</li> <li>\n<code>num_segments</code>: A <code>Tensor</code> of type <code>int32</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-104\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for the first <code>segment_ids.rank</code> dimensions, which are replaced with a single dimension which has size <code>num_segments</code>.</p>  <h3 id=\"sparse_segment_sum\"><code>tf.sparse_segment_sum(data, indices, segment_ids, name=None)</code></h3> <p>Computes the sum along sparse segments of a tensor.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p> <p>Like <code>SegmentSum</code>, but <code>segment_ids</code> can have rank less than <code>data</code>'s first dimension, selecting a subset of dimension 0, specified by <code>indices</code>.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\">c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])\n\n# Select two rows, one segment.\ntf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))\n  ==&gt; [[0 0 0 0]]\n\n# Select two rows, two segment.\ntf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))\n  ==&gt; [[ 1  2  3  4]\n       [-1 -2 -3 -4]]\n\n# Select all rows, two segments.\ntf.sparse_segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))\n  ==&gt; [[0 0 0 0]\n       [5 6 7 8]]\n\n# Which is equivalent to:\ntf.segment_sum(c, tf.constant([0, 0, 1]))\n</pre>  <h5 id=\"args-105\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>indices</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor. Has same rank as <code>segment_ids</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor. Values should be sorted and can be repeated.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-105\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for dimension 0 which has size <code>k</code>, the number of segments.</p>  <h3 id=\"sparse_segment_mean\"><code>tf.sparse_segment_mean(data, indices, segment_ids, name=None)</code></h3> <p>Computes the mean along sparse segments of a tensor.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p> <p>Like <code>SegmentMean</code>, but <code>segment_ids</code> can have rank less than <code>data</code>'s first dimension, selecting a subset of dimension 0, specified by <code>indices</code>.</p>  <h5 id=\"args-106\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>indices</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor. Has same rank as <code>segment_ids</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor. Values should be sorted and can be repeated.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-106\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for dimension 0 which has size <code>k</code>, the number of segments.</p>  <h3 id=\"sparse_segment_sqrt_n\"><code>tf.sparse_segment_sqrt_n(data, indices, segment_ids, name=None)</code></h3> <p>Computes the sum along sparse segments of a tensor divided by the sqrt of N.</p> <p>N is the size of the segment being reduced.</p> <p>Read <a href=\"math_ops#segmentation\">the section on Segmentation</a> for an explanation of segments.</p>  <h5 id=\"args-107\">Args:</h5> <ul> <li>\n<code>data</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>indices</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor. Has same rank as <code>segment_ids</code>.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code> of type <code>int32</code>. A 1-D tensor. Values should be sorted and can be repeated.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-107\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>data</code>. Has same shape as data, except for dimension 0 which has size <code>k</code>, the number of segments.</p>  <h2 id=\"sequence-comparison-and-indexing\">Sequence Comparison and Indexing</h2> <p>TensorFlow provides several operations that you can use to add sequence comparison and index extraction to your graph. You can use these operations to determine sequence differences and determine the indexes of specific values in a tensor.</p>  <h3 id=\"argmin\"><code>tf.argmin(input, dimension, name=None)</code></h3> <p>Returns the index with the smallest value across dimensions of a tensor.</p>  <h5 id=\"args-108\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.</li> <li>\n<code>dimension</code>: A <code>Tensor</code> of type <code>int32</code>. int32, 0 &lt;= dimension &lt; rank(input). Describes which dimension of the input Tensor to reduce across. For vectors, use dimension = 0.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-108\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int64</code>.</p>  <h3 id=\"argmax\"><code>tf.argmax(input, dimension, name=None)</code></h3> <p>Returns the index with the largest value across dimensions of a tensor.</p>  <h5 id=\"args-109\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.</li> <li>\n<code>dimension</code>: A <code>Tensor</code> of type <code>int32</code>. int32, 0 &lt;= dimension &lt; rank(input). Describes which dimension of the input Tensor to reduce across. For vectors, use dimension = 0.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-109\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int64</code>.</p>  <h3 id=\"listdiff\"><code>tf.listdiff(x, y, name=None)</code></h3> <p>Computes the difference between two lists of numbers or strings.</p> <p>Given a list <code>x</code> and a list <code>y</code>, this operation returns a list <code>out</code> that represents all values that are in <code>x</code> but not in <code>y</code>. The returned list <code>out</code> is sorted in the same order that the numbers appear in <code>x</code> (duplicates are preserved). This operation also returns a list <code>idx</code> that represents the position of each <code>out</code> element in <code>x</code>. In other words:</p> <p><code>out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1]</code></p> <p>For example, given this input:</p> <pre class=\"lang-prettyprint no-auto-prettify\">x = [1, 2, 3, 4, 5, 6]\ny = [1, 3, 5]\n</pre> <p>This operation would return:</p> <pre class=\"lang-prettyprint no-auto-prettify\">out ==&gt; [2, 4, 6]\nidx ==&gt; [1, 3, 5]\n</pre>  <h5 id=\"args-110\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. 1-D. Values to keep.</li> <li>\n<code>y</code>: A <code>Tensor</code>. Must have the same type as <code>x</code>. 1-D. Values to remove.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-110\">Returns:</h5> <p>A tuple of <code>Tensor</code> objects (out, idx).</p> <ul> <li>\n<code>out</code>: A <code>Tensor</code>. Has the same type as <code>x</code>. 1-D. Values present in <code>x</code> but not in <code>y</code>.</li> <li>\n<code>idx</code>: A <code>Tensor</code> of type <code>int32</code>. 1-D. Positions of <code>x</code> values preserved in <code>out</code>.</li> </ul>  <h3 id=\"where\"><code>tf.where(input, name=None)</code></h3> <p>Returns locations of true values in a boolean tensor.</p> <p>This operation returns the coordinates of true elements in <code>input</code>. The coordinates are returned in a 2-D tensor where the first dimension (rows) represents the number of true elements, and the second dimension (columns) represents the coordinates of the true elements. Keep in mind, the shape of the output tensor can vary depending on how many true values there are in <code>input</code>. Indices are output in row-major order.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># 'input' tensor is [[True, False]\n#                    [True, False]]\n# 'input' has two true values, so output has two coordinates.\n# 'input' has rank of 2, so coordinates have two indices.\nwhere(input) ==&gt; [[0, 0],\n                  [1, 0]]\n\n# `input` tensor is [[[True, False]\n#                     [True, False]]\n#                    [[False, True]\n#                     [False, True]]\n#                    [[False, False]\n#                     [False, True]]]\n# 'input' has 5 true values, so output has 5 coordinates.\n# 'input' has rank of 3, so coordinates have three indices.\nwhere(input) ==&gt; [[0, 0, 0],\n                  [0, 1, 0],\n                  [1, 0, 1],\n                  [1, 1, 1],\n                  [2, 1, 1]]\n</pre>  <h5 id=\"args-111\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-111\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int64</code>.</p>  <h3 id=\"unique\"><code>tf.unique(x, name=None)</code></h3> <p>Finds unique elements in a 1-D tensor.</p> <p>This operation returns a tensor <code>y</code> containing all of the unique elements of <code>x</code> sorted in the same order that they occur in <code>x</code>. This operation also returns a tensor <code>idx</code> the same size as <code>x</code> that contains the index of each value of <code>x</code> in the unique output <code>y</code>. In other words:</p> <p><code>y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]</code></p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx = unique(x)\ny ==&gt; [1, 2, 4, 7, 8]\nidx ==&gt; [0, 0, 1, 2, 2, 2, 3, 4, 4]\n</pre>  <h5 id=\"args-112\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>. 1-D.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-112\">Returns:</h5> <p>A tuple of <code>Tensor</code> objects (y, idx).</p> <ul> <li>\n<code>y</code>: A <code>Tensor</code>. Has the same type as <code>x</code>. 1-D.</li> <li>\n<code>idx</code>: A <code>Tensor</code> of type <code>int32</code>. 1-D.</li> </ul>  <h3 id=\"edit_distance\"><code>tf.edit_distance(hypothesis, truth, normalize=True, name='edit_distance')</code></h3> <p>Computes the Levenshtein distance between sequences.</p> <p>This operation takes variable-length sequences (<code>hypothesis</code> and <code>truth</code>), each provided as a <code>SparseTensor</code>, and computes the Levenshtein distance. You can normalize the edit distance by length of <code>truth</code> by setting <code>normalize</code> to true.</p> <p>For example, given the following input:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'hypothesis' is a tensor of shape `[2, 1]` with variable-length values:\n#   (0,0) = [\"a\"]\n#   (1,0) = [\"b\"]\nhypothesis = tf.SparseTensor(\n    [[0, 0, 0],\n     [1, 0, 0]],\n    [\"a\", \"b\"]\n    (2, 1, 1))\n\n# 'truth' is a tensor of shape `[2, 2]` with variable-length values:\n#   (0,0) = []\n#   (0,1) = [\"a\"]\n#   (1,0) = [\"b\", \"c\"]\n#   (1,1) = [\"a\"]\ntruth = tf.SparseTensor(\n    [[0, 1, 0],\n     [1, 0, 0],\n     [1, 0, 1],\n     [1, 1, 0]]\n    [\"a\", \"b\", \"c\", \"a\"],\n    (2, 2, 2))\n\nnormalize = True\n</pre> <p>This operation would return the following:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># 'output' is a tensor of shape `[2, 2]` with edit distances normalized\n# by 'truth' lengths.\noutput ==&gt; [[inf, 1.0],  # (0,0): no truth, (0,1): no hypothesis\n           [0.5, 1.0]]  # (1,0): addition, (1,1): no hypothesis\n</pre>  <h5 id=\"args-113\">Args:</h5> <ul> <li>\n<code>hypothesis</code>: A <code>SparseTensor</code> containing hypothesis sequences.</li> <li>\n<code>truth</code>: A <code>SparseTensor</code> containing truth sequences.</li> <li>\n<code>normalize</code>: A <code>bool</code>. If <code>True</code>, normalizes the Levenshtein distance by length of <code>truth.</code>\n</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-113\">Returns:</h5> <p>A dense <code>Tensor</code> with rank <code>R - 1</code>, where R is the rank of the <code>SparseTensor</code> inputs <code>hypothesis</code> and <code>truth</code>.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If either <code>hypothesis</code> or <code>truth</code> are not a <code>SparseTensor</code>.</li> </ul>  <h3 id=\"invert_permutation\"><code>tf.invert_permutation(x, name=None)</code></h3> <p>Computes the inverse permutation of a tensor.</p> <p>This operation computes the inverse of an index permutation. It takes a 1-D integer tensor <code>x</code>, which represents the indices of a zero-based array, and swaps each value with its index position. In other words, for an output tensor <code>y</code> and an input tensor <code>x</code>, this operation computes the following:</p> <p><code>y[x[i]] = i for i in [0, 1, ..., len(x) - 1]</code></p> <p>The values must include 0. There can be no duplicate values or negative values.</p> <p>For example:</p> <pre class=\"lang-prettyprint no-auto-prettify\"># tensor `x` is [3, 4, 0, 2, 1]\ninvert_permutation(x) ==&gt; [2, 4, 3, 0, 1]\n</pre>  <h5 id=\"args-114\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code> of type <code>int32</code>. 1-D.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-114\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int32</code>. 1-D.</p>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"scalar_mul\"><code>tf.scalar_mul(scalar, x)</code></h3> <p>Multiplies a scalar times a <code>Tensor</code> or <code>IndexedSlices</code> object.</p> <p>Intended for use in gradient code which might deal with <code>IndexedSlices</code> objects, which are easy to multiply by a scalar but more expensive to multiply with arbitrary tensors.</p>  <h5 id=\"args-115\">Args:</h5> <ul> <li>\n<code>scalar</code>: A 0-D scalar <code>Tensor</code>. Must have known shape.</li> <li>\n<code>x</code>: A <code>Tensor</code> or <code>IndexedSlices</code> to be scaled.</li> </ul>  <h5 id=\"returns-115\">Returns:</h5> <p><code>scalar * x</code> of the same type (<code>Tensor</code> or <code>IndexedSlices</code>) as <code>x</code>.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if scalar is not a 0-D <code>scalar</code>.</li> </ul>  <h3 id=\"sparse_segment_sqrt_n_grad\"><code>tf.sparse_segment_sqrt_n_grad(grad, indices, segment_ids, output_dim0, name=None)</code></h3> <p>Computes gradients for SparseSegmentSqrtN.</p> <p>Returns tensor \"output\" with same shape as grad, except for dimension 0 whose value is output_dim0.</p>  <h5 id=\"args-116\">Args:</h5> <ul> <li>\n<code>grad</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>. gradient propagated to the SparseSegmentSqrtN op.</li> <li>\n<code>indices</code>: A <code>Tensor</code> of type <code>int32</code>. indices passed to the corresponding SparseSegmentSqrtN op.</li> <li>\n<code>segment_ids</code>: A <code>Tensor</code> of type <code>int32</code>. segment_ids passed to the corresponding SparseSegmentSqrtN op.</li> <li>\n<code>output_dim0</code>: A <code>Tensor</code> of type <code>int32</code>. dimension 0 of \"data\" passed to SparseSegmentSqrtN op.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-116\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>grad</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/math_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/math_ops.html</a>\n  </p>\n</div>\n","string_ops":"<h1 id=\"strings\">Strings</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#strings\">Strings</a></li> <ul> <li><a href=\"#hashing\">Hashing</a></li> <ul> <li><a href=\"#string_to_hash_bucket_fast\"><code>tf.string_to_hash_bucket_fast(input, num_buckets, name=None)</code></a></li> <li><a href=\"#string_to_hash_bucket_strong\"><code>tf.string_to_hash_bucket_strong(input, num_buckets, key, name=None)</code></a></li> <li><a href=\"#string_to_hash_bucket\"><code>tf.string_to_hash_bucket(string_tensor, num_buckets, name=None)</code></a></li> </ul> <li><a href=\"#joining\">Joining</a></li> <ul> <li><a href=\"#reduce_join\"><code>tf.reduce_join(inputs, reduction_indices, keep_dims=None, separator=None, name=None)</code></a></li> </ul>\n</ul> <li><a href=\"#tensor-a-is-a-b-c-d\">tensor <code>a</code> is [[a, b], [c, d]]</a></li> <ul>\n<ul> <li><a href=\"#string_join\"><code>tf.string_join(inputs, separator=None, name=None)</code></a></li> </ul> <li><a href=\"#conversion\">Conversion</a></li> <ul> <li><a href=\"#as_string\"><code>tf.as_string(input, precision=None, scientific=None, shortest=None, width=None, fill=None, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div> <h2 id=\"hashing\">Hashing</h2> <p>String hashing ops take a string input tensor and map each element to an integer.</p>  <h3 id=\"string_to_hash_bucket_fast\"><code>tf.string_to_hash_bucket_fast(input, num_buckets, name=None)</code></h3> <p>Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p>The hash function is deterministic on the content of the string within the process and will never change. However, it is not suitable for cryptography. This function may be used when CPU time is scarce and inputs are trusted or unimportant. There is a risk of adversaries constructing inputs that all hash to the same bucket. To prevent this problem, use a strong hash function with <code>tf.string_to_hash_bucket_strong</code>.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>string</code>. The strings to assign a hash bucket.</li> <li>\n<code>num_buckets</code>: An <code>int</code> that is <code>&gt;= 1</code>. The number of buckets.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int64</code>. A Tensor of the same shape as the input <code>string_tensor</code>.</p>  <h3 id=\"string_to_hash_bucket_strong\"><code>tf.string_to_hash_bucket_strong(input, num_buckets, key, name=None)</code></h3> <p>Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p>The hash function is deterministic on the content of the string within the process. The hash function is a keyed hash function, where attribute <code>key</code> defines the key of the hash function. <code>key</code> is an array of 2 elements.</p> <p>A strong hash is important when inputs may be malicious, e.g. URLs with additional components. Adversaries could try to make their inputs hash to the same bucket for a denial-of-service attack or to skew the results. A strong hash prevents this by making it dificult, if not infeasible, to compute inputs that hash to the same bucket. This comes at a cost of roughly 4x higher compute time than tf.string_to_hash_bucket_fast.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code> of type <code>string</code>. The strings to assign a hash bucket.</li> <li>\n<code>num_buckets</code>: An <code>int</code> that is <code>&gt;= 1</code>. The number of buckets.</li> <li>\n<code>key</code>: A list of <code>ints</code>. The key for the keyed hash function passed as a list of two uint64 elements.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int64</code>. A Tensor of the same shape as the input <code>string_tensor</code>.</p>  <h3 id=\"string_to_hash_bucket\"><code>tf.string_to_hash_bucket(string_tensor, num_buckets, name=None)</code></h3> <p>Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p>The hash function is deterministic on the content of the string within the process.</p> <p>Note that the hash function may change from time to time. This functionality will be deprecated and it's recommended to use <code>tf.string_to_hash_bucket_fast()</code> or <code>tf.string_to_hash_bucket_strong()</code>.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>string_tensor</code>: A <code>Tensor</code> of type <code>string</code>.</li> <li>\n<code>num_buckets</code>: An <code>int</code> that is <code>&gt;= 1</code>. The number of buckets.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>Tensor</code> of type <code>int64</code>. A Tensor of the same shape as the input <code>string_tensor</code>.</p> <h2 id=\"joining\">Joining</h2> <p>String joining ops concatenate elements of input string tensors to produce a new string tensor.</p>  <h3 id=\"reduce_join\"><code>tf.reduce_join(inputs, reduction_indices, keep_dims=None, separator=None, name=None)</code></h3> <p>Joins a string Tensor across the given dimensions.</p> <p>Computes the string join across dimensions in the given string Tensor of shape <code>[d_0, d_1, ..., d_n-1]</code>. Returns a new Tensor created by joining the input strings with the given separator (default: empty string). Negative indices are counted backwards from the end, with <code>-1</code> being equivalent to <code>n - 1</code>. Passing an empty <code>reduction_indices</code> joins all strings in linear index order and outputs a scalar string.</p> <p>For example: ```</p> <h1 id=\"tensor-a-is-a-b-c-d\">tensor <code>a</code> is [[\"a\", \"b\"], [\"c\", \"d\"]]</h1> <p>tf.reduce_join(a, 0) ==&gt; [\"ac\", \"bd\"] tf.reduce_join(a, 1) ==&gt; [\"ab\", \"cd\"] tf.reduce_join(a, -2) = tf.reduce_join(a, 0) ==&gt; [\"ac\", \"bd\"] tf.reduce_join(a, -1) = tf.reduce_join(a, 1) ==&gt; [\"ab\", \"cd\"] tf.reduce_join(a, 0, keep_dims=True) ==&gt; [[\"ac\", \"bd\"]] tf.reduce_join(a, 1, keep_dims=True) ==&gt; [[\"ab\"], [\"cd\"]] tf.reduce_join(a, 0, separator=\".\") ==&gt; [\"a.c\", \"b.d\"] tf.reduce_join(a, [0, 1]) ==&gt; [\"acbd\"] tf.reduce_join(a, [1, 0]) ==&gt; [\"abcd\"] tf.reduce_join(a, []) ==&gt; [\"abcd\"] ```</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>inputs</code>: A <code>Tensor</code> of type <code>string</code>. The input to be joined. All reduced indices must have non-zero size.</li> <li>\n<code>reduction_indices</code>: A <code>Tensor</code> of type <code>int32</code>. The dimensions to reduce over. Dimensions are reduced in the order specified. If <code>reduction_indices</code> has higher rank than <code>1</code>, it is flattened. Omitting <code>reduction_indices</code> is equivalent to passing <code>[n-1, n-2, ..., 0]</code>. Negative indices from <code>-n</code> to <code>-1</code> are supported.</li> <li>\n<code>keep_dims</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If <code>True</code>, retain reduced dimensions with length <code>1</code>.</li> <li>\n<code>separator</code>: An optional <code>string</code>. Defaults to <code>\"\"</code>. The separator to use when joining.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>Tensor</code> of type <code>string</code>. Has shape equal to that of the input with reduced dimensions removed or set to <code>1</code> depending on <code>keep_dims</code>.</p>  <h3 id=\"string_join\"><code>tf.string_join(inputs, separator=None, name=None)</code></h3> <p>Joins the strings in the given list of string tensors into one tensor;</p> <p>with the given separator (default is an empty separator).</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>inputs</code>: A list of at least 1 <code>Tensor</code> objects of type <code>string</code>. A list of string tensors. The tensors must all have the same shape, or be scalars. Scalars may be mixed in; these will be broadcast to the shape of non-scalar inputs.</li> <li>\n<code>separator</code>: An optional <code>string</code>. Defaults to <code>\"\"</code>. string, an optional join separator.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A <code>Tensor</code> of type <code>string</code>.</p> <h2 id=\"conversion\">Conversion</h2>  <h3 id=\"as_string\"><code>tf.as_string(input, precision=None, scientific=None, shortest=None, width=None, fill=None, name=None)</code></h3> <p>Converts each entry in the given tensor to strings. Supports many numeric</p> <p>types and boolean.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>float32</code>, <code>float64</code>, <code>bool</code>, <code>int8</code>.</li> <li>\n<code>precision</code>: An optional <code>int</code>. Defaults to <code>-1</code>. The post-decimal precision to use for floating point numbers. Only used if precision &gt; -1.</li> <li>\n<code>scientific</code>: An optional <code>bool</code>. Defaults to <code>False</code>. Use scientific notation for floating point numbers.</li> <li>\n<code>shortest</code>: An optional <code>bool</code>. Defaults to <code>False</code>. Use shortest representation (either scientific or standard) for floating point numbers.</li> <li>\n<code>width</code>: An optional <code>int</code>. Defaults to <code>-1</code>. Pad pre-decimal numbers to this width. Applies to both floating point and integer numbers. Only used if width &gt; -1.</li> <li>\n<code>fill</code>: An optional <code>string</code>. Defaults to <code>\"\"</code>. The value to pad if width &gt; -1. If empty, pads with spaces. Another typical value is '0'. String cannot be longer than 1 character.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A <code>Tensor</code> of type <code>string</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/string_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/string_ops.html</a>\n  </p>\n</div>\n","nn":"<h1 id=\"neural-network\">Neural Network</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#neural-network\">Neural Network</a></li> <ul> <li><a href=\"#activation-functions\">Activation Functions.</a></li> <ul> <li><a href=\"#relu\"><code>tf.nn.relu(features, name=None)</code></a></li> <li><a href=\"#relu6\"><code>tf.nn.relu6(features, name=None)</code></a></li> <li><a href=\"#elu\"><code>tf.nn.elu(features, name=None)</code></a></li> <li><a href=\"#softplus\"><code>tf.nn.softplus(features, name=None)</code></a></li> <li><a href=\"#softsign\"><code>tf.nn.softsign(features, name=None)</code></a></li> <li><a href=\"#dropout\"><code>tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)</code></a></li> <li><a href=\"#bias_add\"><code>tf.nn.bias_add(value, bias, data_format=None, name=None)</code></a></li> <li><a href=\"#sigmoid\"><code>tf.sigmoid(x, name=None)</code></a></li> <li><a href=\"#tanh\"><code>tf.tanh(x, name=None)</code></a></li> </ul> <li><a href=\"#convolution\">Convolution</a></li> <ul> <li><a href=\"#conv2d\"><code>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)</code></a></li> <li><a href=\"#depthwise_conv2d\"><code>tf.nn.depthwise_conv2d(input, filter, strides, padding, name=None)</code></a></li> <li><a href=\"#separable_conv2d\"><code>tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=None)</code></a></li> <li><a href=\"#atrous_conv2d\"><code>tf.nn.atrous_conv2d(value, filters, rate, padding, name=None)</code></a></li> <li><a href=\"#conv2d_transpose\"><code>tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding=SAME, name=None)</code></a></li> <li><a href=\"#conv3d\"><code>tf.nn.conv3d(input, filter, strides, padding, name=None)</code></a></li> </ul> <li><a href=\"#pooling\">Pooling</a></li> <ul> <li><a href=\"#avg_pool\"><code>tf.nn.avg_pool(value, ksize, strides, padding, data_format=NHWC, name=None)</code></a></li> <li><a href=\"#max_pool\"><code>tf.nn.max_pool(value, ksize, strides, padding, data_format=NHWC, name=None)</code></a></li> <li><a href=\"#max_pool_with_argmax\"><code>tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax=None, name=None)</code></a></li> <li><a href=\"#avg_pool3d\"><code>tf.nn.avg_pool3d(input, ksize, strides, padding, name=None)</code></a></li> <li><a href=\"#max_pool3d\"><code>tf.nn.max_pool3d(input, ksize, strides, padding, name=None)</code></a></li> </ul> <li><a href=\"#morphological-filtering\">Morphological filtering</a></li> <ul> <li><a href=\"#dilation2d\"><code>tf.nn.dilation2d(input, filter, strides, rates, padding, name=None)</code></a></li> <li><a href=\"#erosion2d\"><code>tf.nn.erosion2d(value, kernel, strides, rates, padding, name=None)</code></a></li> </ul> <li><a href=\"#normalization\">Normalization</a></li> <ul> <li><a href=\"#l2_normalize\"><code>tf.nn.l2_normalize(x, dim, epsilon=1e-12, name=None)</code></a></li> <li><a href=\"#local_response_normalization\"><code>tf.nn.local_response_normalization(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None)</code></a></li> <li><a href=\"#sufficient_statistics\"><code>tf.nn.sufficient_statistics(x, axes, shift=None, keep_dims=False, name=None)</code></a></li> <li><a href=\"#normalize_moments\"><code>tf.nn.normalize_moments(counts, mean_ss, variance_ss, shift, name=None)</code></a></li> <li><a href=\"#moments\"><code>tf.nn.moments(x, axes, shift=None, name=None, keep_dims=False)</code></a></li> </ul> <li><a href=\"#losses\">Losses</a></li> <ul> <li><a href=\"#l2_loss\"><code>tf.nn.l2_loss(t, name=None)</code></a></li> <li><a href=\"#log_poisson_loss\"><code>tf.nn.log_poisson_loss(log_input, targets, compute_full_loss=False, name=None)</code></a></li> </ul> <li><a href=\"#classification\">Classification</a></li> <ul> <li><a href=\"#sigmoid_cross_entropy_with_logits\"><code>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)</code></a></li> <li><a href=\"#softmax\"><code>tf.nn.softmax(logits, name=None)</code></a></li> <li><a href=\"#log_softmax\"><code>tf.nn.log_softmax(logits, name=None)</code></a></li> <li><a href=\"#softmax_cross_entropy_with_logits\"><code>tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)</code></a></li> <li><a href=\"#sparse_softmax_cross_entropy_with_logits\"><code>tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)</code></a></li> <li><a href=\"#weighted_cross_entropy_with_logits\"><code>tf.nn.weighted_cross_entropy_with_logits(logits, targets, pos_weight, name=None)</code></a></li> </ul> <li><a href=\"#embeddings\">Embeddings</a></li> <ul> <li><a href=\"#embedding_lookup\"><code>tf.nn.embedding_lookup(params, ids, partition_strategy=mod, name=None, validate_indices=True)</code></a></li> <li><a href=\"#embedding_lookup_sparse\"><code>tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights, partition_strategy=mod, name=None, combiner=mean)</code></a></li> </ul> <li><a href=\"#recurrent-neural-networks\">Recurrent Neural Networks</a></li> <ul> <li><a href=\"#dynamic_rnn\"><code>tf.nn.dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)</code></a></li> <li><a href=\"#rnn\"><code>tf.nn.rnn(cell, inputs, initial_state=None, dtype=None, sequence_length=None, scope=None)</code></a></li> <li><a href=\"#state_saving_rnn\"><code>tf.nn.state_saving_rnn(cell, inputs, state_saver, state_name, sequence_length=None, scope=None)</code></a></li> <li><a href=\"#bidirectional_rnn\"><code>tf.nn.bidirectional_rnn(cell_fw, cell_bw, inputs, initial_state_fw=None, initial_state_bw=None, dtype=None, sequence_length=None, scope=None)</code></a></li> </ul> <li><a href=\"#conectionist-temporal-classification-ctc\">Conectionist Temporal Classification (CTC)</a></li> <ul> <li><a href=\"#ctc_loss\"><code>tf.nn.ctc_loss(inputs, labels, sequence_length, preprocess_collapse_repeated=False, ctc_merge_repeated=True)</code></a></li> <li><a href=\"#ctc_greedy_decoder\"><code>tf.nn.ctc_greedy_decoder(inputs, sequence_length, merge_repeated=True)</code></a></li> <li><a href=\"#ctc_beam_search_decoder\"><code>tf.nn.ctc_beam_search_decoder(inputs, sequence_length, beam_width=100, top_paths=1, merge_repeated=True)</code></a></li> </ul> <li><a href=\"#evaluation\">Evaluation</a></li> <ul> <li><a href=\"#top_k\"><code>tf.nn.top_k(input, k=1, sorted=True, name=None)</code></a></li> <li><a href=\"#in_top_k\"><code>tf.nn.in_top_k(predictions, targets, k, name=None)</code></a></li> </ul> <li><a href=\"#candidate-sampling\">Candidate Sampling</a></li> <ul> <li><a href=\"#sampled-loss-functions\">Sampled Loss Functions</a></li> <li><a href=\"#nce_loss\"><code>tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy=mod, name=nce_loss)</code></a></li> <li><a href=\"#sampled_softmax_loss\"><code>tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy=mod, name=sampled_softmax_loss)</code></a></li> <li><a href=\"#candidate-samplers\">Candidate Samplers</a></li> <li><a href=\"#uniform_candidate_sampler\"><code>tf.nn.uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)</code></a></li> <li><a href=\"#log_uniform_candidate_sampler\"><code>tf.nn.log_uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)</code></a></li> <li><a href=\"#learned_unigram_candidate_sampler\"><code>tf.nn.learned_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)</code></a></li> <li><a href=\"#fixed_unigram_candidate_sampler\"><code>tf.nn.fixed_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, vocab_file=, distortion=1.0, num_reserved_ids=0, num_shards=1, shard=0, unigrams=(), seed=None, name=None)</code></a></li> <li><a href=\"#miscellaneous-candidate-sampling-utilities\">Miscellaneous candidate sampling utilities</a></li> <li><a href=\"#compute_accidental_hits\"><code>tf.nn.compute_accidental_hits(true_classes, sampled_candidates, num_true, seed=None, name=None)</code></a></li> </ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#batch_normalization\"><code>tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None)</code></a></li> <li><a href=\"#depthwise_conv2d_native\"><code>tf.nn.depthwise_conv2d_native(input, filter, strides, padding, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"activation-functions\">Activation Functions.</h2> <p>The activation ops provide different types of nonlinearities for use in neural networks. These include smooth nonlinearities (<code>sigmoid</code>, <code>tanh</code>, <code>elu</code>, <code>softplus</code>, and <code>softsign</code>), continuous but not everywhere differentiable functions (<code>relu</code>, <code>relu6</code>, and <code>relu_x</code>), and random regularization (<code>dropout</code>).</p> <p>All activation ops apply componentwise, and produce a tensor of the same shape as the input tensor.</p>  <h3 id=\"relu\"><code>tf.nn.relu(features, name=None)</code></h3> <p>Computes rectified linear: <code>max(features, 0)</code>.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>features</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>features</code>.</p>  <h3 id=\"relu6\"><code>tf.nn.relu6(features, name=None)</code></h3> <p>Computes Rectified Linear 6: <code>min(max(features, 0), 6)</code>.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>features</code>: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, or <code>int8</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code> with the same type as <code>features</code>.</p>  <h3 id=\"elu\"><code>tf.nn.elu(features, name=None)</code></h3> <p>Computes exponential linear: <code>exp(features) - 1</code> if &lt; 0, <code>features</code> otherwise.</p> <p>See <a href=\"http://arxiv.org/abs/1511.07289\" rel=\"noreferrer\">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) </a></p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>features</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>features</code>.</p>  <h3 id=\"softplus\"><code>tf.nn.softplus(features, name=None)</code></h3> <p>Computes softplus: <code>log(exp(features) + 1)</code>.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>features</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>features</code>.</p>  <h3 id=\"softsign\"><code>tf.nn.softsign(features, name=None)</code></h3> <p>Computes softsign: <code>features / (abs(features) + 1)</code>.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>features</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>features</code>.</p>  <h3 id=\"dropout\"><code>tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)</code></h3> <p>Computes dropout.</p> <p>With probability <code>keep_prob</code>, outputs the input element scaled up by <code>1 / keep_prob</code>, otherwise outputs <code>0</code>. The scaling is so that the expected sum is unchanged.</p> <p>By default, each element is kept or dropped independently. If <code>noise_shape</code> is specified, it must be <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" rel=\"noreferrer\">broadcastable</a> to the shape of <code>x</code>, and only dimensions with <code>noise_shape[i] == shape(x)[i]</code> will make independent decisions. For example, if <code>shape(x) = [k, l, m, n]</code> and <code>noise_shape = [k, 1, 1, n]</code>, each batch and channel component will be kept independently and each row and column will be kept or not kept together.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>x</code>: A tensor.</li> <li>\n<code>keep_prob</code>: A scalar <code>Tensor</code> with the same type as x. The probability that each element is kept.</li> <li>\n<code>noise_shape</code>: A 1-D <code>Tensor</code> of type <code>int32</code>, representing the shape for randomly generated keep/drop flags.</li> <li>\n<code>seed</code>: A Python integer. Used to create random seeds. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A Tensor of the same shape of <code>x</code>.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>keep_prob</code> is not in <code>(0, 1]</code>.</li> </ul>  <h3 id=\"bias_add\"><code>tf.nn.bias_add(value, bias, data_format=None, name=None)</code></h3> <p>Adds <code>bias</code> to <code>value</code>.</p> <p>This is (mostly) a special case of <code>tf.add</code> where <code>bias</code> is restricted to 1-D. Broadcasting is supported, so <code>value</code> may have any number of dimensions. Unlike <code>tf.add</code>, the type of <code>bias</code> is allowed to differ from <code>value</code> in the case where both types are quantized.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>value</code>: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, or <code>complex128</code>.</li> <li>\n<code>bias</code>: A 1-D <code>Tensor</code> with size matching the last dimension of <code>value</code>. Must be the same type as <code>value</code> unless <code>value</code> is a quantized type, in which case a different quantized type may be used.</li> <li>\n<code>data_format</code>: A string. 'NHWC' and 'NCHW' are supported.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A <code>Tensor</code> with the same type as <code>value</code>.</p>  <h3 id=\"sigmoid\"><code>tf.sigmoid(x, name=None)</code></h3> <p>Computes sigmoid of <code>x</code> element-wise.</p> <p>Specifically, <code>y = 1 / (1 + exp(-x))</code>.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>x</code>: A Tensor with type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>int64</code>, or <code>qint32</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A Tensor with the same type as <code>x</code> if <code>x.dtype != qint32</code> otherwise the return type is <code>quint8</code>.</p>  <h3 id=\"tanh\"><code>tf.tanh(x, name=None)</code></h3> <p>Computes hyperbolic tangent of <code>x</code> element-wise.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>x</code>: A Tensor or SparseTensor with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>complex64</code>, <code>int64</code>, or <code>qint32</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A Tensor or SparseTensor respectively with the same type as <code>x</code> if <code>x.dtype != qint32</code> otherwise the return type is <code>quint8</code>.</p> <h2 id=\"convolution\">Convolution</h2> <p>The convolution ops sweep a 2-D filter over a batch of images, applying the filter to each window of each image of the appropriate size. The different ops trade off between generic vs. specific filters:</p> <ul> <li>\n<code>conv2d</code>: Arbitrary filters that can mix channels together.</li> <li>\n<code>depthwise_conv2d</code>: Filters that operate on each channel independently.</li> <li>\n<code>separable_conv2d</code>: A depthwise spatial filter followed by a pointwise filter.</li> </ul> <p>Note that although these ops are called \"convolution\", they are strictly speaking \"cross-correlation\" since the filter is combined with an input window without reversing the filter. For details, see <a href=\"https://en.wikipedia.org/wiki/Cross-correlation#Properties\" rel=\"noreferrer\">the properties of cross-correlation</a>.</p> <p>The filter is applied to image patches of the same size as the filter and strided according to the <code>strides</code> argument. <code>strides = [1, 1, 1, 1]</code> applies the filter to a patch at every offset, <code>strides = [1, 2, 2, 1]</code> applies the filter to every other image patch in each dimension, etc.</p> <p>Ignoring channels for the moment, and assume that the 4-D <code>input</code> has shape <code>[batch, in_height, in_width, ...]</code> and the 4-D <code>filter</code> has shape <code>[filter_height, filter_width, ...]</code>, then the spatial semantics of the convolution ops are as follows: first, according to the padding scheme chosen as <code>'SAME'</code> or <code>'VALID'</code>, the output size and the padding pixels are computed. For the <code>'SAME'</code> padding, the output height and width are computed as:</p> <pre class=\"\">out_height = ceil(float(in_height) / float(strides[1]))\nout_width  = ceil(float(in_width) / float(strides[2]))\n</pre> <p>and the padding on the top and left are computed as:</p> <pre class=\"\">pad_along_height = ((out_height - 1) * strides[1] +\n                    filter_height - in_height)\npad_along_width = ((out_width - 1) * strides[2] +\n                   filter_width - in_width)\npad_top = pad_along_height / 2\npad_left = pad_along_width / 2\n</pre> <p>Note that the division by 2 means that there might be cases when the padding on both sides (top vs bottom, right vs left) are off by one. In this case, the bottom and right sides always get the one additional padded pixel. For example, when <code>pad_along_height</code> is 5, we pad 2 pixels at the top and 3 pixels at the bottom. Note that this is different from existing libraries such as cuDNN and Caffe, which explicitly specify the number of padded pixels and always pad the same number of pixels on both sides.</p> <p>For the <code>'VALID</code>' padding, the output height and width are computed as:</p> <pre class=\"\">out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\nout_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n</pre> <p>and the padding values are always zero. The output is then computed as</p> <pre class=\"\">output[b, i, j, :] =\n    sum_{di, dj} input[b, strides[1] * i + di - pad_top,\n                       strides[2] * j + dj - pad_left, ...] *\n                 filter[di, dj, ...]\n</pre> <p>where any value outside the original input image region are considered zero ( i.e. we pad zero values around the border of the image).</p> <p>Since <code>input</code> is 4-D, each <code>input[b, i, j, :]</code> is a vector. For <code>conv2d</code>, these vectors are multiplied by the <code>filter[di, dj, :, :]</code> matrices to produce new vectors. For <code>depthwise_conv_2d</code>, each scalar component <code>input[b, i, j, k]</code> is multiplied by a vector <code>filter[di, dj, k]</code>, and all the vectors are concatenated.</p>  <h3 id=\"conv2d\"><code>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)</code></h3> <p>Computes a 2-D convolution given 4-D <code>input</code> and <code>filter</code> tensors.</p> <p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code> and a filter / kernel tensor of shape <code>[filter_height, filter_width, in_channels, out_channels]</code>, this op performs the following:</p> <ol> <li>Flattens the filter to a 2-D matrix with shape <code>[filter_height * filter_width * in_channels, output_channels]</code>.</li> <li>Extracts image patches from the input tensor to form a <em>virtual</em> tensor of shape <code>[batch, out_height, out_width,\nfilter_height * filter_width * in_channels]</code>.</li> <li>For each patch, right-multiplies the filter matrix and the image patch vector.</li> </ol> <p>In detail, with the default NHWC format,</p> <pre class=\"\">output[b, i, j, k] =\n    sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *\n                    filter[di, dj, q, k]\n</pre> <p>Must have <code>strides[0] = strides[3] = 1</code>. For the most common case of the same horizontal and vertices strides, <code>strides = [1, stride, stride, 1]</code>.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>\n<code>filter</code>: A <code>Tensor</code>. Must have the same type as <code>input</code>.</li> <li>\n<code>strides</code>: A list of <code>ints</code>. 1-D of length 4. The stride of the sliding window for each dimension of <code>input</code>. Must be in the same order as the dimension specified with format.</li> <li>\n<code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</li> <li>\n<code>use_cudnn_on_gpu</code>: An optional <code>bool</code>. Defaults to <code>True</code>.</li> <li>\n<code>data_format</code>: An optional <code>string</code> from: <code>\"NHWC\", \"NCHW\"</code>. Defaults to <code>\"NHWC\"</code>. Specify the data format of the input and output data. With the default format \"NHWC\", the data is stored in the order of: [batch, in_height, in_width, in_channels]. Alternatively, the format could be \"NCHW\", the data storage order of: [batch, in_channels, in_height, in_width].</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h3 id=\"depthwise_conv2d\"><code>tf.nn.depthwise_conv2d(input, filter, strides, padding, name=None)</code></h3> <p>Depthwise 2-D convolution.</p> <p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code> and a filter tensor of shape <code>[filter_height, filter_width, in_channels, channel_multiplier]</code> containing <code>in_channels</code> convolutional filters of depth 1, <code>depthwise_conv2d</code> applies a different filter to each input channel (expanding from 1 channel to <code>channel_multiplier</code> channels for each), then concatenates the results together. The output has <code>in_channels * channel_multiplier</code> channels.</p> <p>In detail,</p> <pre class=\"\">output[b, i, j, k * channel_multiplier + q] =\n    sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *\n                 filter[di, dj, k, q]\n</pre> <p>Must have <code>strides[0] = strides[3] = 1</code>. For the most common case of the same horizontal and vertical strides, <code>strides = [1, stride, stride, 1]</code>.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>input</code>: 4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.</li> <li>\n<code>filter</code>: 4-D with shape <code>[filter_height, filter_width, in_channels, channel_multiplier]</code>.</li> <li>\n<code>strides</code>: 1-D of size 4. The stride of the sliding window for each dimension of <code>input</code>.</li> <li>\n<code>padding</code>: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm. See the <a href=\"https://www.tensorflow.org/api_docs/python/nn.html#convolution\" rel=\"noreferrer\">comment here</a>\n</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A 4-D <code>Tensor</code> of shape <code>[batch, out_height, out_width, in_channels * channel_multiplier].</code></p>  <h3 id=\"separable_conv2d\"><code>tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=None)</code></h3> <p>2-D convolution with separable filters.</p> <p>Performs a depthwise convolution that acts separately on channels followed by a pointwise convolution that mixes channels. Note that this is separability between dimensions <code>[1, 2]</code> and <code>3</code>, not spatial separability between dimensions <code>1</code> and <code>2</code>.</p> <p>In detail,</p> <pre class=\"\">output[b, i, j, k] = sum_{di, dj, q, r]\n    input[b, strides[1] * i + di, strides[2] * j + dj, q] *\n    depthwise_filter[di, dj, q, r] *\n    pointwise_filter[0, 0, q * channel_multiplier + r, k]\n</pre> <p><code>strides</code> controls the strides for the depthwise convolution only, since the pointwise convolution has implicit strides of <code>[1, 1, 1, 1]</code>. Must have <code>strides[0] = strides[3] = 1</code>. For the most common case of the same horizontal and vertical strides, <code>strides = [1, stride, stride, 1]</code>.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>input</code>: 4-D <code>Tensor</code> with shape <code>[batch, in_height, in_width, in_channels]</code>.</li> <li>\n<code>depthwise_filter</code>: 4-D <code>Tensor</code> with shape <code>[filter_height, filter_width, in_channels, channel_multiplier]</code>. Contains <code>in_channels</code> convolutional filters of depth 1.</li> <li>\n<code>pointwise_filter</code>: 4-D <code>Tensor</code> with shape <code>[1, 1, channel_multiplier * in_channels, out_channels]</code>. Pointwise filter to mix channels after <code>depthwise_filter</code> has convolved spatially.</li> <li>\n<code>strides</code>: 1-D of size 4. The strides for the depthwise convolution for each dimension of <code>input</code>.</li> <li>\n<code>padding</code>: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm. See the <a href=\"https://www.tensorflow.org/api_docs/python/nn.html#convolution\" rel=\"noreferrer\">comment here</a>\n</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>A 4-D <code>Tensor</code> of shape <code>[batch, out_height, out_width, out_channels]</code>.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If channel_multiplier * in_channels &gt; out_channels, which means that the separable convolution is overparameterized.</li> </ul>  <h3 id=\"atrous_conv2d\"><code>tf.nn.atrous_conv2d(value, filters, rate, padding, name=None)</code></h3> <p>Atrous convolution (a.k.a. convolution with holes or dilated convolution).</p> <p>Computes a 2-D atrous convolution, also known as convolution with holes or dilated convolution, given 4-D <code>value</code> and <code>filters</code> tensors. If the <code>rate</code> parameter is equal to one, it performs regular 2-D convolution. If the <code>rate</code> parameter is greater than one, it performs convolution with holes, sampling the input values every <code>rate</code> pixels in the <code>height</code> and <code>width</code> dimensions. This is equivalent to convolving the input with a set of upsampled filters, produced by inserting <code>rate - 1</code> zeros between two consecutive values of the filters along the <code>height</code> and <code>width</code> dimensions, hence the name atrous convolution or convolution with holes (the French word trous means holes in English).</p> <p>More specifically:</p> <pre class=\"\">output[b, i, j, k] = sum_{di, dj, q} filters[di, dj, q, k] *\n      value[b, i + rate * di, j + rate * dj, q]\n</pre> <p>Atrous convolution allows us to explicitly control how densely to compute feature responses in fully convolutional networks. Used in conjunction with bilinear interpolation, it offers an alternative to <code>conv2d_transpose</code> in dense prediction tasks such as semantic image segmentation, optical flow computation, or depth estimation. It also allows us to effectively enlarge the field of view of filters without increasing the number of parameters or the amount of computation.</p> <p>For a description of atrous convolution and how it can be used for dense feature extraction, please see: <a href=\"http://arxiv.org/abs/1412.7062\" rel=\"noreferrer\">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a>. The same operation is investigated further in <a href=\"http://arxiv.org/abs/1511.07122\" rel=\"noreferrer\">Multi-Scale Context Aggregation by Dilated Convolutions</a>. Previous works that effectively use atrous convolution in different ways are, among others, <a href=\"http://arxiv.org/abs/1312.6229\" rel=\"noreferrer\">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</a> and <a href=\"http://arxiv.org/abs/1302.1700\" rel=\"noreferrer\">Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks</a>. Atrous convolution is also closely related to the so-called noble identities in multi-rate signal processing.</p> <p>There are many different ways to implement atrous convolution (see the refs above). The implementation here reduces</p> <pre class=\"\">atrous_conv2d(value, filters, rate, padding=padding)\n</pre> <p>to the following three operations:</p> <pre class=\"lang-matlab no-auto-prettify\">paddings = ...\nnet = space_to_batch(value, paddings, block_size=rate)\nnet = conv2d(net, filters, strides=[1, 1, 1, 1], padding=\"VALID\")\ncrops = ...\nnet = batch_to_space(net, crops, block_size=rate)\n</pre> <p>Advanced usage. Note the following optimization: A sequence of <code>atrous_conv2d</code> operations with identical <code>rate</code> parameters, 'SAME' <code>padding</code>, and filters with odd heights/ widths:</p> <pre class=\"\">net = atrous_conv2d(net, filters1, rate, padding=\"SAME\")\nnet = atrous_conv2d(net, filters2, rate, padding=\"SAME\")\n...\nnet = atrous_conv2d(net, filtersK, rate, padding=\"SAME\")\n</pre> <p>can be equivalently performed cheaper in terms of computation and memory as:</p> <pre class=\"lang-matlab no-auto-prettify\">pad = ...  # padding so that the input dims are multiples of rate\nnet = space_to_batch(net, paddings=pad, block_size=rate)\nnet = conv2d(net, filters1, strides=[1, 1, 1, 1], padding=\"SAME\")\nnet = conv2d(net, filters2, strides=[1, 1, 1, 1], padding=\"SAME\")\n...\nnet = conv2d(net, filtersK, strides=[1, 1, 1, 1], padding=\"SAME\")\nnet = batch_to_space(net, crops=pad, block_size=rate)\n</pre> <p>because a pair of consecutive <code>space_to_batch</code> and <code>batch_to_space</code> ops with the same <code>block_size</code> cancel out when their respective <code>paddings</code> and <code>crops</code> inputs are identical.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>value</code>: A 4-D <code>Tensor</code> of type <code>float</code>. It needs to be in the default \"NHWC\" format. Its shape is <code>[batch, in_height, in_width, in_channels]</code>.</li> <li>\n<code>filters</code>: A 4-D <code>Tensor</code> with the same type as <code>value</code> and shape <code>[filter_height, filter_width, in_channels, out_channels]</code>. <code>filters</code>' <code>in_channels</code> dimension must match that of <code>value</code>. Atrous convolution is equivalent to standard convolution with upsampled filters with effective height <code>filter_height + (filter_height - 1) * (rate - 1)</code> and effective width <code>filter_width + (filter_width - 1) * (rate - 1)</code>, produced by inserting <code>rate - 1</code> zeros along consecutive elements across the <code>filters</code>' spatial dimensions.</li> <li>\n<code>rate</code>: A positive int32. The stride with which we sample input values across the <code>height</code> and <code>width</code> dimensions. Equivalently, the rate by which we upsample the filter values by inserting zeros across the <code>height</code> and <code>width</code> dimensions. In the literature, the same parameter is sometimes called <code>input stride</code> or <code>dilation</code>.</li> <li>\n<code>padding</code>: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm.</li> <li>\n<code>name</code>: Optional name for the returned tensor.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A <code>Tensor</code> with the same type as <code>value</code>.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If input/output depth does not match <code>filters</code>' shape, or if padding is other than <code>'VALID'</code> or <code>'SAME'</code>.</li> </ul>  <h3 id=\"conv2d_transpose\"><code>tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding='SAME', name=None)</code></h3> <p>The transpose of <code>conv2d</code>.</p> <p>This operation is sometimes called \"deconvolution\" after <a href=\"http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf\" rel=\"noreferrer\">Deconvolutional Networks</a>, but is actually the transpose (gradient) of <code>conv2d</code> rather than an actual deconvolution.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>value</code>: A 4-D <code>Tensor</code> of type <code>float</code> and shape <code>[batch, height, width, in_channels]</code>.</li> <li>\n<code>filter</code>: A 4-D <code>Tensor</code> with the same type as <code>value</code> and shape <code>[height, width, output_channels, in_channels]</code>. <code>filter</code>'s <code>in_channels</code> dimension must match that of <code>value</code>.</li> <li>\n<code>output_shape</code>: A 1-D <code>Tensor</code> representing the output shape of the deconvolution op.</li> <li>\n<code>strides</code>: A list of ints. The stride of the sliding window for each dimension of the input tensor.</li> <li>\n<code>padding</code>: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm. See the <a href=\"https://www.tensorflow.org/api_docs/python/nn.html#convolution\" rel=\"noreferrer\">comment here</a>\n</li> <li>\n<code>name</code>: Optional name for the returned tensor.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A <code>Tensor</code> with the same type as <code>value</code>.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If input/output depth does not match <code>filter</code>'s shape, or if padding is other than <code>'VALID'</code> or <code>'SAME'</code>.</li> </ul>  <h3 id=\"conv3d\"><code>tf.nn.conv3d(input, filter, strides, padding, name=None)</code></h3> <p>Computes a 3-D convolution given 5-D <code>input</code> and <code>filter</code> tensors.</p> <p>In signal processing, cross-correlation is a measure of similarity of two waveforms as a function of a time-lag applied to one of them. This is also known as a sliding dot product or sliding inner-product.</p> <p>Our Conv3D implements a form of cross-correlation.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>. Shape <code>[batch, in_depth, in_height, in_width, in_channels]</code>.</li> <li>\n<code>filter</code>: A <code>Tensor</code>. Must have the same type as <code>input</code>. Shape <code>[filter_depth, filter_height, filter_width, in_channels,\nout_channels]</code>. <code>in_channels</code> must match between <code>input</code> and <code>filter</code>.</li> <li>\n<code>strides</code>: A list of <code>ints</code> that has length <code>&gt;= 5</code>. 1-D tensor of length 5. The stride of the sliding window for each dimension of <code>input</code>. Must have <code>strides[0] = strides[4] = 1</code>.</li> <li>\n<code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p> <h2 id=\"pooling\">Pooling</h2> <p>The pooling ops sweep a rectangular window over the input tensor, computing a reduction operation for each window (average, max, or max with argmax). Each pooling op uses rectangular windows of size <code>ksize</code> separated by offset <code>strides</code>. For example, if <code>strides</code> is all ones every window is used, if <code>strides</code> is all twos every other window is used in each dimension, etc.</p> <p>In detail, the output is</p> <pre class=\"\">output[i] = reduce(value[strides * i:strides * i + ksize])\n</pre> <p>where the indices also take into consideration the padding values. Please refer to the <code>Convolution</code> section for details about the padding calculation.</p>  <h3 id=\"avg_pool\"><code>tf.nn.avg_pool(value, ksize, strides, padding, data_format='NHWC', name=None)</code></h3> <p>Performs the average pooling on the input.</p> <p>Each entry in <code>output</code> is the mean of the corresponding size <code>ksize</code> window in <code>value</code>.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>value</code>: A 4-D <code>Tensor</code> of shape <code>[batch, height, width, channels]</code> and type <code>float32</code>, <code>float64</code>, <code>qint8</code>, <code>quint8</code>, or <code>qint32</code>.</li> <li>\n<code>ksize</code>: A list of ints that has length &gt;= 4. The size of the window for each dimension of the input tensor.</li> <li>\n<code>strides</code>: A list of ints that has length &gt;= 4. The stride of the sliding window for each dimension of the input tensor.</li> <li>\n<code>padding</code>: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm. See the <a href=\"https://www.tensorflow.org/api_docs/python/nn.html#convolution\" rel=\"noreferrer\">comment here</a>\n</li> <li>\n<code>data_format</code>: A string. 'NHWC' and 'NCHW' are supported.</li> <li>\n<code>name</code>: Optional name for the operation.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A <code>Tensor</code> with the same type as <code>value</code>. The average pooled output tensor.</p>  <h3 id=\"max_pool\"><code>tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)</code></h3> <p>Performs the max pooling on the input.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>value</code>: A 4-D <code>Tensor</code> with shape <code>[batch, height, width, channels]</code> and type <code>tf.float32</code>.</li> <li>\n<code>ksize</code>: A list of ints that has length &gt;= 4. The size of the window for each dimension of the input tensor.</li> <li>\n<code>strides</code>: A list of ints that has length &gt;= 4. The stride of the sliding window for each dimension of the input tensor.</li> <li>\n<code>padding</code>: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm. See the <a href=\"https://www.tensorflow.org/api_docs/python/nn.html#convolution\" rel=\"noreferrer\">comment here</a>\n</li> <li>\n<code>data_format</code>: A string. 'NHWC' and 'NCHW' are supported.</li> <li>\n<code>name</code>: Optional name for the operation.</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A <code>Tensor</code> with type <code>tf.float32</code>. The max pooled output tensor.</p>  <h3 id=\"max_pool_with_argmax\"><code>tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax=None, name=None)</code></h3> <p>Performs max pooling on the input and outputs both max values and indices.</p> <p>The indices in <code>argmax</code> are flattened, so that a maximum value at position <code>[b, y, x, c]</code> becomes flattened index <code>((b * height + y) * width + x) * channels + c</code>.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>half</code>. 4-D with shape <code>[batch, height, width, channels]</code>. Input to pool over.</li> <li>\n<code>ksize</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. The size of the window for each dimension of the input tensor.</li> <li>\n<code>strides</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. The stride of the sliding window for each dimension of the input tensor.</li> <li>\n<code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</li> <li>\n<code>Targmax</code>: An optional <code>tf.DType</code> from: <code>tf.int32, tf.int64</code>. Defaults to <code>tf.int64</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>A tuple of <code>Tensor</code> objects (output, argmax).</p> <ul> <li>\n<code>output</code>: A <code>Tensor</code>. Has the same type as <code>input</code>. The max pooled output tensor.</li> <li>\n<code>argmax</code>: A <code>Tensor</code> of type <code>Targmax</code>. 4-D. The flattened indices of the max values chosen for each output.</li> </ul>  <h3 id=\"avg_pool3d\"><code>tf.nn.avg_pool3d(input, ksize, strides, padding, name=None)</code></h3> <p>Performs 3D average pooling on the input.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>. Shape <code>[batch, depth, rows, cols, channels]</code> tensor to pool over.</li> <li>\n<code>ksize</code>: A list of <code>ints</code> that has length <code>&gt;= 5</code>. 1-D tensor of length 5. The size of the window for each dimension of the input tensor. Must have <code>ksize[0] = ksize[4] = 1</code>.</li> <li>\n<code>strides</code>: A list of <code>ints</code> that has length <code>&gt;= 5</code>. 1-D tensor of length 5. The stride of the sliding window for each dimension of <code>input</code>. Must have <code>strides[0] = strides[4] = 1</code>.</li> <li>\n<code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. The average pooled output tensor.</p>  <h3 id=\"max_pool3d\"><code>tf.nn.max_pool3d(input, ksize, strides, padding, name=None)</code></h3> <p>Performs 3D max pooling on the input.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>. Shape <code>[batch, depth, rows, cols, channels]</code> tensor to pool over.</li> <li>\n<code>ksize</code>: A list of <code>ints</code> that has length <code>&gt;= 5</code>. 1-D tensor of length 5. The size of the window for each dimension of the input tensor. Must have <code>ksize[0] = ksize[4] = 1</code>.</li> <li>\n<code>strides</code>: A list of <code>ints</code> that has length <code>&gt;= 5</code>. 1-D tensor of length 5. The stride of the sliding window for each dimension of <code>input</code>. Must have <code>strides[0] = strides[4] = 1</code>.</li> <li>\n<code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. The max pooled output tensor.</p>  <h2 id=\"morphological-filtering\">Morphological filtering</h2> <p>Morphological operators are non-linear filters used in image processing.</p> <p><a href=\"https://en.wikipedia.org/wiki/Dilation_(morphology)\" rel=\"noreferrer\">Greyscale morphological dilation</a> is the max-sum counterpart of standard sum-product convolution:</p> <pre class=\"\">output[b, y, x, c] =\n    max_{dy, dx} input[b,\n                       strides[1] * y + rates[1] * dy,\n                       strides[2] * x + rates[2] * dx,\n                       c] +\n                 filter[dy, dx, c]\n</pre> <p>The <code>filter</code> is usually called structuring function. Max-pooling is a special case of greyscale morphological dilation when the filter assumes all-zero values (a.k.a. flat structuring function).</p> <p><a href=\"https://en.wikipedia.org/wiki/Erosion_(morphology)\" rel=\"noreferrer\">Greyscale morphological erosion</a> is the min-sum counterpart of standard sum-product convolution:</p> <pre class=\"\">output[b, y, x, c] =\n    min_{dy, dx} input[b,\n                       strides[1] * y - rates[1] * dy,\n                       strides[2] * x - rates[2] * dx,\n                       c] -\n                 filter[dy, dx, c]\n</pre> <p>Dilation and erosion are dual to each other. The dilation of the input signal <code>f</code> by the structuring signal <code>g</code> is equal to the negation of the erosion of <code>-f</code> by the reflected <code>g</code>, and vice versa.</p> <p>Striding and padding is carried out in exactly the same way as in standard convolution. Please refer to the <code>Convolution</code> section for details.</p>  <h3 id=\"dilation2d\"><code>tf.nn.dilation2d(input, filter, strides, rates, padding, name=None)</code></h3> <p>Computes the grayscale dilation of 4-D <code>input</code> and 3-D <code>filter</code> tensors.</p> <p>The <code>input</code> tensor has shape <code>[batch, in_height, in_width, depth]</code> and the <code>filter</code> tensor has shape <code>[filter_height, filter_width, depth]</code>, i.e., each input channel is processed independently of the others with its own structuring function. The <code>output</code> tensor has shape <code>[batch, out_height, out_width, depth]</code>. The spatial dimensions of the output tensor depend on the <code>padding</code> algorithm. We currently only support the default \"NHWC\" <code>data_format</code>.</p> <p>In detail, the grayscale morphological 2-D dilation is the max-sum correlation (for consistency with <code>conv2d</code>, we use unmirrored filters):</p> <pre class=\"\">output[b, y, x, c] =\n   max_{dy, dx} input[b,\n                      strides[1] * y + rates[1] * dy,\n                      strides[2] * x + rates[2] * dx,\n                      c] +\n                filter[dy, dx, c]\n</pre> <p>Max-pooling is a special case when the filter has size equal to the pooling kernel size and contains all zeros.</p> <p>Note on duality: The dilation of <code>input</code> by the <code>filter</code> is equal to the negation of the erosion of <code>-input</code> by the reflected <code>filter</code>.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>. 4-D with shape <code>[batch, in_height, in_width, depth]</code>.</li> <li>\n<code>filter</code>: A <code>Tensor</code>. Must have the same type as <code>input</code>. 3-D with shape <code>[filter_height, filter_width, depth]</code>.</li> <li>\n<code>strides</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. The stride of the sliding window for each dimension of the input tensor. Must be: <code>[1, stride_height, stride_width, 1]</code>.</li> <li>\n<code>rates</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. The input stride for atrous morphological dilation. Must be: <code>[1, rate_height, rate_width, 1]</code>.</li> <li>\n<code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. 4-D with shape <code>[batch, out_height, out_width, depth]</code>.</p>  <h3 id=\"erosion2d\"><code>tf.nn.erosion2d(value, kernel, strides, rates, padding, name=None)</code></h3> <p>Computes the grayscale erosion of 4-D <code>value</code> and 3-D <code>kernel</code> tensors.</p> <p>The <code>value</code> tensor has shape <code>[batch, in_height, in_width, depth]</code> and the <code>kernel</code> tensor has shape <code>[kernel_height, kernel_width, depth]</code>, i.e., each input channel is processed independently of the others with its own structuring function. The <code>output</code> tensor has shape <code>[batch, out_height, out_width, depth]</code>. The spatial dimensions of the output tensor depend on the <code>padding</code> algorithm. We currently only support the default \"NHWC\" <code>data_format</code>.</p> <p>In detail, the grayscale morphological 2-D erosion is given by:</p> <pre class=\"\">output[b, y, x, c] =\n   min_{dy, dx} value[b,\n                      strides[1] * y - rates[1] * dy,\n                      strides[2] * x - rates[2] * dx,\n                      c] -\n                kernel[dy, dx, c]\n</pre> <p>Duality: The erosion of <code>value</code> by the <code>kernel</code> is equal to the negation of the dilation of <code>-value</code> by the reflected <code>kernel</code>.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>value</code>: A <code>Tensor</code>. 4-D with shape <code>[batch, in_height, in_width, depth]</code>.</li> <li>\n<code>kernel</code>: A <code>Tensor</code>. Must have the same type as <code>value</code>. 3-D with shape <code>[kernel_height, kernel_width, depth]</code>.</li> <li>\n<code>strides</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. 1-D of length 4. The stride of the sliding window for each dimension of the input tensor. Must be: <code>[1, stride_height, stride_width, 1]</code>.</li> <li>\n<code>rates</code>: A list of <code>ints</code> that has length <code>&gt;= 4</code>. 1-D of length 4. The input stride for atrous morphological dilation. Must be: <code>[1, rate_height, rate_width, 1]</code>.</li> <li>\n<code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</li> <li>\n<code>name</code>: A name for the operation (optional). If not specified \"erosion2d\" is used.</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>value</code>. 4-D with shape <code>[batch, out_height, out_width, depth]</code>.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the <code>value</code> depth does not match <code>kernel</code>' shape, or if padding is other than <code>'VALID'</code> or <code>'SAME'</code>.</li> </ul> <h2 id=\"normalization\">Normalization</h2> <p>Normalization is useful to prevent neurons from saturating when inputs may have varying scale, and to aid generalization.</p>  <h3 id=\"l2_normalize\"><code>tf.nn.l2_normalize(x, dim, epsilon=1e-12, name=None)</code></h3> <p>Normalizes along dimension <code>dim</code> using an L2 norm.</p> <p>For a 1-D tensor with <code>dim = 0</code>, computes</p> <pre class=\"\">output = x / sqrt(max(sum(x**2), epsilon))\n</pre> <p>For <code>x</code> with more dimensions, independently normalizes each 1-D slice along dimension <code>dim</code>.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>.</li> <li>\n<code>dim</code>: Dimension along which to normalize.</li> <li>\n<code>epsilon</code>: A lower bound value for the norm. Will use <code>sqrt(epsilon)</code> as the divisor if <code>norm &lt; sqrt(epsilon)</code>.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>A <code>Tensor</code> with the same shape as <code>x</code>.</p>  <h3 id=\"local_response_normalization\"><code>tf.nn.local_response_normalization(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None)</code></h3> <p>Local Response Normalization.</p> <p>The 4-D <code>input</code> tensor is treated as a 3-D array of 1-D vectors (along the last dimension), and each vector is normalized independently. Within a given vector, each component is divided by the weighted, squared sum of inputs within <code>depth_radius</code>. In detail,</p> <pre class=\"\">sqr_sum[a, b, c, d] =\n    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\noutput = input / (bias + alpha * sqr_sum) ** beta\n</pre> <p>For details, see <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\" rel=\"noreferrer\">Krizhevsky et al., ImageNet classification with deep convolutional neural networks (NIPS 2012)</a>.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>half</code>. 4-D.</li> <li>\n<code>depth_radius</code>: An optional <code>int</code>. Defaults to <code>5</code>. 0-D. Half-width of the 1-D normalization window.</li> <li>\n<code>bias</code>: An optional <code>float</code>. Defaults to <code>1</code>. An offset (usually positive to avoid dividing by 0).</li> <li>\n<code>alpha</code>: An optional <code>float</code>. Defaults to <code>1</code>. A scale factor, usually positive.</li> <li>\n<code>beta</code>: An optional <code>float</code>. Defaults to <code>0.5</code>. An exponent.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h3 id=\"sufficient_statistics\"><code>tf.nn.sufficient_statistics(x, axes, shift=None, keep_dims=False, name=None)</code></h3> <p>Calculate the sufficient statistics for the mean and variance of <code>x</code>.</p> <p>These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted. See: <a target=\"_blank\" href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance%23Computing_shifted_data&amp;usg=AFQjCNG5RoY7Xvpv4xg-Wy-UJvAPh2zDQw\">https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data</a></p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>.</li> <li>\n<code>axes</code>: Array of ints. Axes along which to compute mean and variance.</li> <li>\n<code>shift</code>: A <code>Tensor</code> containing the value by which to shift the data for numerical stability, or <code>None</code> if no shift is to be performed. A shift close to the true mean provides the most numerically stable results.</li> <li>\n<code>keep_dims</code>: produce statistics with the same dimensionality as the input.</li> <li>\n<code>name</code>: Name used to scope the operations that compute the sufficient stats.</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>Four <code>Tensor</code> objects of the same type as <code>x</code>: * the count (number of elements to average over). * the (possibly shifted) sum of the elements in the array. * the (possibly shifted) sum of squares of the elements in the array. * the shift by which the mean must be corrected or None if <code>shift</code> is None.</p>  <h3 id=\"normalize_moments\"><code>tf.nn.normalize_moments(counts, mean_ss, variance_ss, shift, name=None)</code></h3> <p>Calculate the mean and variance of based on the sufficient statistics.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>counts</code>: A <code>Tensor</code> containing a the total count of the data (one value).</li> <li>\n<code>mean_ss</code>: A <code>Tensor</code> containing the mean sufficient statistics: the (possibly shifted) sum of the elements to average over.</li> <li>\n<code>variance_ss</code>: A <code>Tensor</code> containing the variance sufficient statistics: the (possibly shifted) squared sum of the data to compute the variance over.</li> <li>\n<code>shift</code>: A <code>Tensor</code> containing the value by which the data is shifted for numerical stability, or <code>None</code> if no shift was performed.</li> <li>\n<code>name</code>: Name used to scope the operations that compute the moments.</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <p>Two <code>Tensor</code> objects: <code>mean</code> and <code>variance</code>.</p>  <h3 id=\"moments\"><code>tf.nn.moments(x, axes, shift=None, name=None, keep_dims=False)</code></h3> <p>Calculate the mean and variance of <code>x</code>.</p> <p>The mean and variance are calculated by aggregating the contents of <code>x</code> across <code>axes</code>. If <code>x</code> is 1-D and <code>axes = [0]</code> this is just the mean and variance of a vector.</p> <p>When using these moments for batch normalization (see <code>tf.nn.batch_normalization</code>): * for so-called \"global normalization\", used with convolutional filters with shape <code>[batch, height, width, depth]</code>, pass <code>axes=[0, 1, 2]</code>. * for simple batch normalization pass <code>axes=[0]</code> (batch only).</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>x</code>: A <code>Tensor</code>.</li> <li>\n<code>axes</code>: array of ints. Axes along which to compute mean and variance.</li> <li>\n<code>shift</code>: A <code>Tensor</code> containing the value by which to shift the data for numerical stability, or <code>None</code> if no shift is to be performed. A shift close to the true mean provides the most numerically stable results.</li> <li>\n<code>name</code>: Name used to scope the operations that compute the moments.</li> <li>\n<code>keep_dims</code>: produce moments with the same dimensionality as the input.</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>Two <code>Tensor</code> objects: <code>mean</code> and <code>variance</code>.</p> <h2 id=\"losses\">Losses</h2> <p>The loss ops measure error between two tensors, or between a tensor and zero. These can be used for measuring accuracy of a network in a regression task or for regularization purposes (weight decay).</p>  <h3 id=\"l2_loss\"><code>tf.nn.l2_loss(t, name=None)</code></h3> <p>L2 Loss.</p> <p>Computes half the L2 norm of a tensor without the <code>sqrt</code>:</p> <pre class=\"\">output = sum(t ** 2) / 2\n</pre>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>t</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>. Typically 2-D, but may have any dimensions.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>t</code>. 0-D.</p>  <h3 id=\"log_poisson_loss\"><code>tf.nn.log_poisson_loss(log_input, targets, compute_full_loss=False, name=None)</code></h3> <p>Computes log poisson loss given <code>log_input</code>.</p> <p>Gives the log-likelihood loss between the prediction and the target under the assumption that the target has a poisson distribution. Caveat: By default, this is not the exact loss, but the loss minus a constant term [log(z!)]. That has no effect for optimization, but does not play well with relative loss comparisons. To compute an approximation of the log factorial term, specify compute_full_loss=True to enable Stirling's Approximation.</p> <p>For brevity, let <code>c = log(x) = log_input</code>, <code>z = targets</code>. The log poisson loss is</p> <pre class=\"lang-m no-auto-prettify\">  -log(exp(-x) * (x^z) / z!)\n= -log(exp(-x) * (x^z)) + log(z!)\n~ -log(exp(-x)) - log(x^z) [+ z * log(z) - z + 0.5 * log(2 * pi * z)]\n    [ Note the second term is the Stirling's Approximation for log(z!).\n      It is invariant to x and does not affect optimization, though\n      important for correct relative loss comparisons. It is only\n      computed when compute_full_loss == True. ]\n= x - z * log(x) [+ z * log(z) - z + 0.5 * log(2 * pi * z)]\n= exp(c) - z * c [+ z * log(z) - z + 0.5 * log(2 * pi * z)]\n</pre>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>log_input</code>: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</li> <li>\n<code>targets</code>: A <code>Tensor</code> of the same type and shape as <code>log_input</code>.</li> <li>\n<code>compute_full_loss</code>: whether to compute the full loss. If false, a constant term is dropped in favor of more efficient optimization.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>A <code>Tensor</code> of the same shape as <code>log_input</code> with the componentwise logistic losses.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>log_input</code> and <code>targets</code> do not have the same shape.</li> </ul> <h2 id=\"classification\">Classification</h2> <p>TensorFlow provides several operations that help you perform classification.</p>  <h3 id=\"sigmoid_cross_entropy_with_logits\"><code>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)</code></h3> <p>Computes sigmoid cross entropy given <code>logits</code>.</p> <p>Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.</p> <p>For brevity, let <code>x = logits</code>, <code>z = targets</code>. The logistic loss is</p> <pre class=\"\">  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n= (1 - z) * x + log(1 + exp(-x))\n= x - x * z + log(1 + exp(-x))\n</pre> <p>For x &lt; 0, to avoid overflow in exp(-x), we reformulate the above</p> <pre class=\"\">  x - x * z + log(1 + exp(-x))\n= log(exp(x)) - x * z + log(1 + exp(-x))\n= - x * z + log(1 + exp(x))\n</pre> <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent formulation</p> <pre class=\"\">max(x, 0) - x * z + log(1 + exp(-abs(x)))\n</pre> <p><code>logits</code> and <code>targets</code> must have the same type and shape.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>logits</code>: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</li> <li>\n<code>targets</code>: A <code>Tensor</code> of the same type and shape as <code>logits</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>A <code>Tensor</code> of the same shape as <code>logits</code> with the componentwise logistic losses.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>logits</code> and <code>targets</code> do not have the same shape.</li> </ul>  <h3 id=\"softmax\"><code>tf.nn.softmax(logits, name=None)</code></h3> <p>Computes softmax activations.</p> <p>For each batch <code>i</code> and class <code>j</code> we have</p> <pre class=\"\">softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))\n</pre>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>logits</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>. 2-D with shape <code>[batch_size, num_classes]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>logits</code>. Same shape as <code>logits</code>.</p>  <h3 id=\"log_softmax\"><code>tf.nn.log_softmax(logits, name=None)</code></h3> <p>Computes log softmax activations.</p> <p>For each batch <code>i</code> and class <code>j</code> we have</p> <pre class=\"\">logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))\n</pre>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>logits</code>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>. 2-D with shape <code>[batch_size, num_classes]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>logits</code>. Same shape as <code>logits</code>.</p>  <h3 id=\"softmax_cross_entropy_with_logits\"><code>tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)</code></h3> <p>Computes softmax cross entropy between <code>logits</code> and <code>labels</code>.</p> <p>Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both.</p> <p><strong>NOTE:</strong> While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of <code>labels</code> is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.</p> <p>If using exclusive <code>labels</code> (wherein one and only one class is true at a time), see <code>sparse_softmax_cross_entropy_with_logits</code>.</p> <p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a <code>softmax</code> on <code>logits</code> internally for efficiency. Do not call this op with the output of <code>softmax</code>, as it will produce incorrect results.</p> <p><code>logits</code> and <code>labels</code> must have the same shape <code>[batch_size, num_classes]</code> and the same dtype (either <code>float16</code>, <code>float32</code>, or <code>float64</code>).</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>logits</code>: Unscaled log probabilities.</li> <li>\n<code>labels</code>: Each row <code>labels[i]</code> must be a valid probability distribution.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>A 1-D <code>Tensor</code> of length <code>batch_size</code> of the same type as <code>logits</code> with the softmax cross entropy loss.</p>  <h3 id=\"sparse_softmax_cross_entropy_with_logits\"><code>tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)</code></h3> <p>Computes sparse softmax cross entropy between <code>logits</code> and <code>labels</code>.</p> <p>Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both.</p> <p><strong>NOTE:</strong> For this operation, the probability of a given label is considered exclusive. That is, soft classes are not allowed, and the <code>labels</code> vector must provide a single specific index for the true class for each row of <code>logits</code> (each minibatch entry). For soft softmax classification with a probability distribution for each entry, see <code>softmax_cross_entropy_with_logits</code>.</p> <p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a softmax on <code>logits</code> internally for efficiency. Do not call this op with the output of <code>softmax</code>, as it will produce incorrect results.</p> <p>A common use case is to have logits of shape <code>[batch_size, num_classes]</code> and labels of shape <code>[batch_size]</code>. But higher dimensions are supported.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>logits</code>: Unscaled log probabilities of rank <code>r</code> and shape <code>[d_0, d_1, ..., d_{r-2}, num_classes]</code> and dtype <code>float32</code> or <code>float64</code>.</li> <li>\n<code>labels</code>: <code>Tensor</code> of shape <code>[d_0, d_1, ..., d_{r-2}]</code> and dtype <code>int32</code> or <code>int64</code>. Each entry in <code>labels</code> must be an index in <code>[0, num_classes)</code>. Other values will result in a loss of 0, but incorrect gradient computations.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-34\">Returns:</h5> <p>A <code>Tensor</code> of the same shape as <code>labels</code> and of the same type as <code>logits</code> with the softmax cross entropy loss.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If logits are scalars (need to have rank &gt;= 1) or if the rank of the labels is not equal to the rank of the labels minus one.</li> </ul>  <h3 id=\"weighted_cross_entropy_with_logits\"><code>tf.nn.weighted_cross_entropy_with_logits(logits, targets, pos_weight, name=None)</code></h3> <p>Computes a weighted cross entropy.</p> <p>This is like <code>sigmoid_cross_entropy_with_logits()</code> except that <code>pos_weight</code>, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.</p> <p>The usual cross-entropy cost is defined as:</p> <p>targets * -log(sigmoid(logits)) + (1 - targets) * -log(1 - sigmoid(logits))</p> <p>The argument <code>pos_weight</code> is used as a multiplier for the positive targets:</p> <p>targets * -log(sigmoid(logits)) * pos_weight + (1 - targets) * -log(1 - sigmoid(logits))</p> <p>For brevity, let <code>x = logits</code>, <code>z = targets</code>, <code>q = pos_weight</code>. The loss is:</p> <pre class=\"\">  qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n= qz * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n= qz * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n= qz * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n= (1 - z) * x + (qz +  1 - z) * log(1 + exp(-x))\n= (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))\n</pre> <p>Setting <code>l = (1 + (q - 1) * z)</code>, to ensure stability and avoid overflow, the implementation uses</p> <pre class=\"\">(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))\n</pre> <p><code>logits</code> and <code>targets</code> must have the same type and shape.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>logits</code>: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</li> <li>\n<code>targets</code>: A <code>Tensor</code> of the same type and shape as <code>logits</code>.</li> <li>\n<code>pos_weight</code>: A coefficient to use on the positive examples.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>A <code>Tensor</code> of the same shape as <code>logits</code> with the componentwise weightedlogistic losses.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>logits</code> and <code>targets</code> do not have the same shape.</li> </ul> <h2 id=\"embeddings\">Embeddings</h2> <p>TensorFlow provides library support for looking up values in embedding tensors.</p>  <h3 id=\"embedding_lookup\"><code>tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True)</code></h3> <p>Looks up <code>ids</code> in a list of embedding tensors.</p> <p>This function is used to perform parallel lookups on the list of tensors in <code>params</code>. It is a generalization of <a href=\"array_ops#gather\"><code>tf.gather()</code></a>, where <code>params</code> is interpreted as a partition of a larger embedding tensor.</p> <p>If <code>len(params) &gt; 1</code>, each element <code>id</code> of <code>ids</code> is partitioned between the elements of <code>params</code> according to the <code>partition_strategy</code>. In all strategies, if the id space does not evenly divide the number of partitions, each of the first <code>(max_id + 1) % len(params)</code> partitions will be assigned one more id.</p> <p>If <code>partition_strategy</code> is <code>\"mod\"</code>, we assign each id to partition <code>p = id % len(params)</code>. For instance, 13 ids are split across 5 partitions as: <code>[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]</code></p> <p>If <code>partition_strategy</code> is <code>\"div\"</code>, we assign ids to partitions in a contiguous manner. In this case, 13 ids are split across 5 partitions as: <code>[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</code></p> <p>The results of the lookup are concatenated into a dense tensor. The returned tensor has shape <code>shape(ids) + shape(params)[1:]</code>.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>params</code>: A list of tensors with the same type and which can be concatenated along dimension 0. Each <code>Tensor</code> must be appropriately sized for the given <code>partition_strategy</code>.</li> <li>\n<code>ids</code>: A <code>Tensor</code> with type <code>int32</code> or <code>int64</code> containing the ids to be looked up in <code>params</code>.</li> <li>\n<code>partition_strategy</code>: A string specifying the partitioning strategy, relevant if <code>len(params) &gt; 1</code>. Currently <code>\"div\"</code> and <code>\"mod\"</code> are supported. Default is <code>\"mod\"</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> <li>\n<code>validate_indices</code>: Whether or not to validate gather indices.</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p>A <code>Tensor</code> with the same type as the tensors in <code>params</code>.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>params</code> is empty.</li> </ul>  <h3 id=\"embedding_lookup_sparse\"><code>tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights, partition_strategy='mod', name=None, combiner='mean')</code></h3> <p>Computes embeddings for the given ids and weights.</p> <p>This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order.</p> <p>It also assumes that all id values lie in the range [0, p0), where p0 is the sum of the size of params along dimension 0.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>params</code>: A single tensor representing the complete embedding tensor, or a list of P tensors all of same shape except for the first dimension, representing sharded embedding tensors.</li> <li>\n<code>sp_ids</code>: N x M SparseTensor of int64 ids (typically from FeatureValueToId), where N is typically batch size and M is arbitrary.</li> <li>\n<code>sp_weights</code>: either a SparseTensor of float / double weights, or None to indicate all weights should be taken to be 1. If specified, sp_weights must have exactly the same shape and indices as sp_ids.</li> <li>\n<code>partition_strategy</code>: A string specifying the partitioning strategy, relevant if <code>len(params) &gt; 1</code>. Currently <code>\"div\"</code> and <code>\"mod\"</code> are supported. Default is <code>\"mod\"</code>. See <code>tf.nn.embedding_lookup</code> for more details.</li> <li>\n<code>name</code>: Optional name for the op.</li> <li>\n<code>combiner</code>: A string specifying the reduction op. Currently \"mean\", \"sqrtn\" and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding results for each row. \"mean\" is the weighted sum divided by the total weight. \"sqrtn\" is the weighted sum divided by the square root of the sum of the squares of the weights.</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <p>A dense tensor representing the combined embeddings for the sparse ids. For each row in the dense tensor represented by sp_ids, the op looks up the embeddings for all ids in that row, multiplies them by the corresponding weight, and combines these embeddings as specified.</p> <p>In other words, if shape(combined params) = [p0, p1, ..., pm] and shape(sp_ids) = shape(sp_weights) = [d0, d1, ..., dn] then shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].</p> <p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p> <pre class=\"\">[0, 0]: id 1, weight 2.0\n[0, 1]: id 3, weight 0.5\n[1, 0]: id 0, weight 1.0\n[2, 3]: id 1, weight 3.0\n</pre> <p>with combiner=\"mean\", then the output will be a 3x20 matrix where output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5) output[1, :] = params[0, :] * 1.0 output[2, :] = params[1, :] * 3.0</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If sp_ids is not a SparseTensor, or if sp_weights is neither None nor SparseTensor.</li> <li>\n<code>ValueError</code>: If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}.</li> </ul>  <h2 id=\"recurrent-neural-networks\">Recurrent Neural Networks</h2> <p>TensorFlow provides a number of methods for constructing Recurrent Neural Networks. Most accept an <code>RNNCell</code>-subclassed object (see the documentation for <code>tf.nn.rnn_cell</code>).</p>  <h3 id=\"dynamic_rnn\"><code>tf.nn.dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)</code></h3> <p>Creates a recurrent neural network specified by RNNCell <code>cell</code>.</p> <p>This function is functionally identical to the function <code>rnn</code> above, but performs fully dynamic unrolling of <code>inputs</code>.</p> <p>Unlike <code>rnn</code>, the input <code>inputs</code> is not a Python list of <code>Tensors</code>, one for each frame. Instead, <code>inputs</code> may be a single <code>Tensor</code> where the maximum time is either the first or second dimension (see the parameter <code>time_major</code>). Alternatively, it may be a (possibly nested) tuple of Tensors, each of them having matching batch and time dimensions. The corresponding output is either a single <code>Tensor</code> having the same number of time steps and batch size, or a (possibly nested) tuple of such tensors, matching the nested structure of <code>cell.output_size</code>.</p> <p>The parameter <code>sequence_length</code> is optional and is used to copy-through state and zero-out outputs when past a batch element's sequence length. So it's more for correctness than performance, unlike in rnn().</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>cell</code>: An instance of RNNCell.</li> <li>\n<p><code>inputs</code>: The RNN inputs.</p> <p>If <code>time_major == False</code> (default), this must be a <code>Tensor</code> of shape: <code>[batch_size, max_time, ...]</code>, or a nested tuple of such elements.</p> <p>If <code>time_major == True</code>, this must be a <code>Tensor</code> of shape: <code>[max_time, batch_size, ...]</code>, or a nested tuple of such elements.</p> <p>This may also be a (possibly nested) tuple of Tensors satisfying this property. The first two dimensions must match across all the inputs, but otherwise the ranks and other shape components may differ. In this case, input to <code>cell</code> at each time-step will replicate the structure of these tuples, except for the time dimension (from which the time is taken).</p> <p>The input to <code>cell</code> at each time step will be a <code>Tensor</code> or (possibly nested) tuple of Tensors each with dimensions <code>[batch_size, ...]</code>.</p>\n</li> <li><p><code>sequence_length</code>: (optional) An int32/int64 vector sized <code>[batch_size]</code>.</p></li> <li><p><code>initial_state</code>: (optional) An initial state for the RNN. If <code>cell.state_size</code> is an integer, this must be a <code>Tensor</code> of appropriate type and shape <code>[batch_size x cell.state_size]</code>. If <code>cell.state_size</code> is a tuple, this should be a tuple of tensors having shapes <code>[batch_size, s] for s in cell.state_size</code>.</p></li> <li><p><code>dtype</code>: (optional) The data type for the initial state and expected output. Required if initial_state is not provided or RNN state has a heterogeneous dtype.</p></li> <li><p><code>parallel_iterations</code>: (Default: 32). The number of iterations to run in parallel. Those operations which do not have any temporal dependency and can be run in parallel, will be. This parameter trades off time for space. Values &gt;&gt; 1 use more memory but take less time, while smaller values use less memory but computations take longer.</p></li> <li><p><code>swap_memory</code>: Transparently swap the tensors produced in forward inference but needed for back prop from GPU to CPU. This allows training RNNs which would typically not fit on a single GPU, with very minimal (or no) performance penalty.</p></li> <li><p><code>time_major</code>: The shape format of the <code>inputs</code> and <code>outputs</code> Tensors. If true, these <code>Tensors</code> must be shaped <code>[max_time, batch_size, depth]</code>. If false, these <code>Tensors</code> must be shaped <code>[batch_size, max_time, depth]</code>. Using <code>time_major = True</code> is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.</p></li> <li><p><code>scope</code>: VariableScope for the created subgraph; defaults to \"RNN\".</p></li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>A pair (outputs, state) where:</p> <ul> <li>\n<p><code>outputs</code>: The RNN output <code>Tensor</code>.</p> <p>If time_major == False (default), this will be a <code>Tensor</code> shaped: <code>[batch_size, max_time, cell.output_size]</code>.</p> <p>If time_major == True, this will be a <code>Tensor</code> shaped: <code>[max_time, batch_size, cell.output_size]</code>.</p> <p>Note, if <code>cell.output_size</code> is a (possibly nested) tuple of integers or <code>TensorShape</code> objects, then <code>outputs</code> will be a tuple having the same structure as <code>cell.output_size</code>, containing Tensors having shapes corresponding to the shape data in <code>cell.output_size</code>.</p>\n</li> <li><p><code>state</code>: The final state. If <code>cell.state_size</code> is an int, this will be shaped <code>[batch_size, cell.state_size]</code>. If it is a <code>TensorShape</code>, this will be shaped <code>[batch_size] + cell.state_size</code>. If it is a (possibly nested) tuple of ints or <code>TensorShape</code>, this will be a tuple having the corresponding shapes.</p></li> </ul>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>cell</code> is not an instance of RNNCell.</li> <li>\n<code>ValueError</code>: If inputs is None or an empty list.</li> </ul>  <h3 id=\"rnn\"><code>tf.nn.rnn(cell, inputs, initial_state=None, dtype=None, sequence_length=None, scope=None)</code></h3> <p>Creates a recurrent neural network specified by RNNCell <code>cell</code>.</p> <p>The simplest form of RNN network generated is: <code>py\n  state = cell.zero_state(...)\n  outputs = []\n  for input_ in inputs:\n    output, state = cell(input_, state)\n    outputs.append(output)\n  return (outputs, state)</code> However, a few other options are available:</p> <p>An initial state can be provided. If the sequence_length vector is provided, dynamic calculation is performed. This method of calculation does not compute the RNN steps past the maximum sequence length of the minibatch (thus saving computational time), and properly propagates the state at an example's sequence length to the final state output.</p> <p>The dynamic calculation performed is, at time t for batch row b, (output, state)(b, t) = (t &gt;= sequence_length(b)) ? (zeros(cell.output_size), states(b, sequence_length(b) - 1)) : cell(input(b, t), state(b, t - 1))</p>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>cell</code>: An instance of RNNCell.</li> <li>\n<code>inputs</code>: A length T list of inputs, each a <code>Tensor</code> of shape <code>[batch_size, input_size]</code>, or a nested tuple of such elements.</li> <li>\n<code>initial_state</code>: (optional) An initial state for the RNN. If <code>cell.state_size</code> is an integer, this must be a <code>Tensor</code> of appropriate type and shape <code>[batch_size, cell.state_size]</code>. If <code>cell.state_size</code> is a tuple, this should be a tuple of tensors having shapes <code>[batch_size, s] for s in cell.state_size</code>.</li> <li>\n<code>dtype</code>: (optional) The data type for the initial state and expected output. Required if initial_state is not provided or RNN state has a heterogeneous dtype.</li> <li>\n<code>sequence_length</code>: Specifies the length of each sequence in inputs. An int32 or int64 vector (tensor) size <code>[batch_size]</code>, values in <code>[0, T)</code>.</li> <li>\n<code>scope</code>: VariableScope for the created subgraph; defaults to \"RNN\".</li> </ul>  <h5 id=\"returns-39\">Returns:</h5> <p>A pair (outputs, state) where: - outputs is a length T list of outputs (one for each input), or a nested tuple of such elements. - state is the final state</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>cell</code> is not an instance of RNNCell.</li> <li>\n<code>ValueError</code>: If <code>inputs</code> is <code>None</code> or an empty list, or if the input depth (column size) cannot be inferred from inputs via shape inference.</li> </ul>  <h3 id=\"state_saving_rnn\"><code>tf.nn.state_saving_rnn(cell, inputs, state_saver, state_name, sequence_length=None, scope=None)</code></h3> <p>RNN that accepts a state saver for time-truncated RNN calculation.</p>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>cell</code>: An instance of <code>RNNCell</code>.</li> <li>\n<code>inputs</code>: A length T list of inputs, each a <code>Tensor</code> of shape <code>[batch_size, input_size]</code>.</li> <li>\n<code>state_saver</code>: A state saver object with methods <code>state</code> and <code>save_state</code>.</li> <li>\n<code>state_name</code>: Python string or tuple of strings. The name to use with the state_saver. If the cell returns tuples of states (i.e., <code>cell.state_size</code> is a tuple) then <code>state_name</code> should be a tuple of strings having the same length as <code>cell.state_size</code>. Otherwise it should be a single string.</li> <li>\n<code>sequence_length</code>: (optional) An int32/int64 vector size [batch_size]. See the documentation for rnn() for more details about sequence_length.</li> <li>\n<code>scope</code>: VariableScope for the created subgraph; defaults to \"RNN\".</li> </ul>  <h5 id=\"returns-40\">Returns:</h5> <p>A pair (outputs, state) where: outputs is a length T list of outputs (one for each input) states is the final state</p>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>cell</code> is not an instance of RNNCell.</li> <li>\n<code>ValueError</code>: If <code>inputs</code> is <code>None</code> or an empty list, or if the arity and type of <code>state_name</code> does not match that of <code>cell.state_size</code>.</li> </ul>  <h3 id=\"bidirectional_rnn\"><code>tf.nn.bidirectional_rnn(cell_fw, cell_bw, inputs, initial_state_fw=None, initial_state_bw=None, dtype=None, sequence_length=None, scope=None)</code></h3> <p>Creates a bidirectional recurrent neural network.</p> <p>Similar to the unidirectional case above (rnn) but takes input and builds independent forward and backward RNNs with the final forward and backward outputs depth-concatenated, such that the output will have the format [time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of forward and backward cell must match. The initial state for both directions is zero by default (but can be set optionally) and no intermediate states are ever returned -- the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given.</p>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>cell_fw</code>: An instance of RNNCell, to be used for forward direction.</li> <li>\n<code>cell_bw</code>: An instance of RNNCell, to be used for backward direction.</li> <li>\n<code>inputs</code>: A length T list of inputs, each a tensor of shape [batch_size, input_size], or a nested tuple of such elements.</li> <li>\n<code>initial_state_fw</code>: (optional) An initial state for the forward RNN. This must be a tensor of appropriate type and shape <code>[batch_size x cell_fw.state_size]</code>. If <code>cell_fw.state_size</code> is a tuple, this should be a tuple of tensors having shapes <code>[batch_size, s] for s in cell_fw.state_size</code>.</li> <li>\n<code>initial_state_bw</code>: (optional) Same as for <code>initial_state_fw</code>, but using the corresponding properties of <code>cell_bw</code>.</li> <li>\n<code>dtype</code>: (optional) The data type for the initial state. Required if either of the initial states are not provided.</li> <li>\n<code>sequence_length</code>: (optional) An int32/int64 vector, size <code>[batch_size]</code>, containing the actual lengths for each of the sequences.</li> <li>\n<code>scope</code>: VariableScope for the created subgraph; defaults to \"BiRNN\"</li> </ul>  <h5 id=\"returns-41\">Returns:</h5> <p>A tuple (outputs, output_state_fw, output_state_bw) where: outputs is a length <code>T</code> list of outputs (one for each input), which are depth-concatenated forward and backward outputs. output_state_fw is the final state of the forward rnn. output_state_bw is the final state of the backward rnn.</p>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>cell_fw</code> or <code>cell_bw</code> is not an instance of <code>RNNCell</code>.</li> <li>\n<code>ValueError</code>: If inputs is None or an empty list.</li> </ul>  <h2 id=\"conectionist-temporal-classification-ctc\">Conectionist Temporal Classification (CTC)</h2>  <h3 id=\"ctc_loss\"><code>tf.nn.ctc_loss(inputs, labels, sequence_length, preprocess_collapse_repeated=False, ctc_merge_repeated=True)</code></h3> <p>Computes the CTC (Connectionist Temporal Classification) Loss.</p> <p>This op implements the CTC loss as presented in the article:</p> <p>A. Graves, S. Fernandez, F. Gomez, J. Schmidhuber. Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. ICML 2006, Pittsburgh, USA, pp. 369-376.</p> <p><a target=\"_blank\" href=\"https://www.google.com/url?q=http://www.cs.toronto.edu/~graves/icml_2006.pdf&amp;usg=AFQjCNHtkorVHXLPKg-q9NGgrzY7YmWoag\">http://www.cs.toronto.edu/~graves/icml_2006.pdf</a></p> <p>Input requirements:</p> <pre class=\"\">sequence_length(b) &lt;= time for all b\n\nmax(labels.indices(labels.indices[:, 1] == b, 2))\n  &lt;= sequence_length(b) for all b.\n</pre> <p>Regarding the arguments <code>preprocess_collapse_repeated</code> and <code>ctc_merge_repeated</code>:</p> <p>If <code>preprocess_collapse_repeated</code> is True, then a preprocessing step runs before loss calculation, wherein repeated labels passed to the loss are merged into single labels. This is useful if the training labels come from, e.g., forced alignments and therefore have unnecessary repetitions.</p> <p>If <code>ctc_merge_repeated</code> is set False, then deep within the CTC calculation, repeated non-blank labels will not be merged and are interpreted as individual labels. This is a simplified (non-standard) version of CTC.</p> <p>Here is a table of the (roughly) expected first order behavior:</p> <ul> <li>\n<p><code>preprocess_collapse_repeated=False</code>, <code>ctc_merge_repeated=True</code></p> <p>Classical CTC behavior: Outputs true repeated classes with blanks in between, and can also output repeated classes with no blanks in between that need to be collapsed by the decoder.</p>\n</li> <li>\n<p><code>preprocess_collapse_repeated=True</code>, <code>ctc_merge_repeated=False</code></p> <p>Never learns to output repeated classes, as they are collapsed in the input labels before training.</p>\n</li> <li>\n<p><code>preprocess_collapse_repeated=False</code>, <code>ctc_merge_repeated=False</code></p> <p>Outputs repeated classes with blanks in between, but generally does not require the decoder to collapse/merge repeated classes.</p>\n</li> <li>\n<p><code>preprocess_collapse_repeated=True</code>, <code>ctc_merge_repeated=True</code></p> <p>Untested. Very likely will not learn to output repeated classes.</p>\n</li> </ul>  <h5 id=\"args-42\">Args:</h5> <ul> <li>\n<code>inputs</code>: 3-D <code>float</code> <code>Tensor</code> sized <code>[max_time x batch_size x num_classes]</code>. The logits.</li> <li>\n<code>labels</code>: An <code>int32</code> <code>SparseTensor</code>. <code>labels.indices[i, :] == [b, t]</code> means <code>labels.values[i]</code> stores the id for (batch b, time t). See <code>core/ops/ctc_ops.cc</code> for more details.</li> <li>\n<code>sequence_length</code>: 1-D <code>int32</code> vector, size <code>[batch_size]</code>. The sequence lengths.</li> <li>\n<code>preprocess_collapse_repeated</code>: Boolean. Default: False. If True, repeated labels are collapsed prior to the CTC calculation.</li> <li>\n<code>ctc_merge_repeated</code>: Boolean. Default: True.</li> </ul>  <h5 id=\"returns-42\">Returns:</h5> <p>A 1-D <code>float</code> <code>Tensor</code>, size <code>[batch]</code>, containing the negative log probabilities.</p>  <h5 id=\"raises-16\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if labels is not a <code>SparseTensor</code>.</li> </ul>  <h3 id=\"ctc_greedy_decoder\"><code>tf.nn.ctc_greedy_decoder(inputs, sequence_length, merge_repeated=True)</code></h3> <p>Performs greedy decoding on the logits given in input (best path).</p> <p>Note: Regardless of the value of merge_repeated, if the maximum index of a given time and batch corresponds to the blank index <code>(num_classes - 1)</code>, no new element is emitted.</p> <p>If <code>merge_repeated</code> is <code>True</code>, merge repeated classes in output. This means that if consecutive logits' maximum indices are the same, only the first of these is emitted. The sequence <code>A B B * B * B</code> (where '*' is the blank label) becomes</p> <ul> <li>\n<code>A B</code> if <code>merge_repeated=True</code>.</li> <li>\n<code>A B B B B B</code> if <code>merge_repeated=False</code>.</li> </ul>  <h5 id=\"args-43\">Args:</h5> <ul> <li>\n<code>inputs</code>: 3-D <code>float</code> <code>Tensor</code> sized <code>[max_time x batch_size x num_classes]</code>. The logits.</li> <li>\n<code>sequence_length</code>: 1-D <code>int32</code> vector containing sequence lengths, having size <code>[batch_size]</code>.</li> <li>\n<code>merge_repeated</code>: Boolean. Default: True.</li> </ul>  <h5 id=\"returns-43\">Returns:</h5> <p>A tuple <code>(decoded, log_probabilities)</code> where</p> <ul> <li>\n<code>decoded</code>: A single-element list. <code>decoded[0]</code> is an <code>SparseTensor</code> containing the decoded outputs s.t.: <code>decoded.indices</code>: Indices matrix <code>(total_decoded_outputs x 2)</code>. The rows store: <code>[batch, time]</code>. <code>decoded.values</code>: Values vector, size <code>(total_decoded_outputs)</code>. The vector stores the decoded classes. <code>decoded.shape</code>: Shape vector, size <code>(2)</code>. The shape values are: <code>[batch_size, max_decoded_length]</code>\n</li> <li>\n<code>log_probability</code>: A <code>float</code> matrix <code>(batch_size x 1)</code> containing sequence log-probabilities.</li> </ul>  <h3 id=\"ctc_beam_search_decoder\"><code>tf.nn.ctc_beam_search_decoder(inputs, sequence_length, beam_width=100, top_paths=1, merge_repeated=True)</code></h3> <p>Performs beam search decoding on the logits given in input.</p> <p><strong>Note</strong> The <code>ctc_greedy_decoder</code> is a special case of the <code>ctc_beam_search_decoder</code> with <code>top_paths=1</code> (but that decoder is faster for this special case).</p> <p>If <code>merge_repeated</code> is <code>True</code>, merge repeated classes in the output beams. This means that if consecutive entries in a beam are the same, only the first of these is emitted. That is, when the top path is <code>A B B B B</code>, the return value is:</p> <ul> <li>\n<code>A B</code> if <code>merge_repeated = True</code>.</li> <li>\n<code>A B B B B</code> if <code>merge_repeated = False</code>.</li> </ul>  <h5 id=\"args-44\">Args:</h5> <ul> <li>\n<code>inputs</code>: 3-D <code>float</code> <code>Tensor</code>, size <code>[max_time x batch_size x num_classes]</code>. The logits.</li> <li>\n<code>sequence_length</code>: 1-D <code>int32</code> vector containing sequence lengths, having size <code>[batch_size]</code>.</li> <li>\n<code>beam_width</code>: An int scalar &gt;= 0 (beam search beam width).</li> <li>\n<code>top_paths</code>: An int scalar &gt;= 0, &lt;= beam_width (controls output size).</li> <li>\n<code>merge_repeated</code>: Boolean. Default: True.</li> </ul>  <h5 id=\"returns-44\">Returns:</h5> <p>A tuple <code>(decoded, log_probabilities)</code> where</p> <ul> <li>\n<code>decoded</code>: A list of length top_paths, where <code>decoded[j]</code> is a <code>SparseTensor</code> containing the decoded outputs: <code>decoded[j].indices</code>: Indices matrix <code>(total_decoded_outputs[j] x 2)</code> The rows store: [batch, time]. <code>decoded[j].values</code>: Values vector, size <code>(total_decoded_outputs[j])</code>. The vector stores the decoded classes for beam j. <code>decoded[j].shape</code>: Shape vector, size <code>(2)</code>. The shape values are: <code>[batch_size, max_decoded_length[j]]</code>.</li> <li>\n<code>log_probability</code>: A <code>float</code> matrix <code>(batch_size x top_paths)</code> containing sequence log-probabilities.</li> </ul> <h2 id=\"evaluation\">Evaluation</h2> <p>The evaluation ops are useful for measuring the performance of a network. Since they are nondifferentiable, they are typically used at evaluation time.</p>  <h3 id=\"top_k\"><code>tf.nn.top_k(input, k=1, sorted=True, name=None)</code></h3> <p>Finds values and indices of the <code>k</code> largest entries for the last dimension.</p> <p>If the input is a vector (rank-1), finds the <code>k</code> largest entries in the vector and outputs their values and indices as vectors. Thus <code>values[j]</code> is the <code>j</code>-th largest entry in <code>input</code>, and its index is <code>indices[j]</code>.</p> <p>For matrices (resp. higher rank input), computes the top <code>k</code> entries in each row (resp. vector along the last dimension). Thus,</p> <pre class=\"lang-tcl no-auto-prettify\">values.shape = indices.shape = input.shape[:-1] + [k]\n</pre> <p>If two elements are equal, the lower-index element appears first.</p>  <h5 id=\"args-45\">Args:</h5> <ul> <li>\n<code>input</code>: 1-D or higher <code>Tensor</code> with last dimension at least <code>k</code>.</li> <li>\n<code>k</code>: 0-D <code>int32</code> <code>Tensor</code>. Number of top elements to look for along the last dimension (along each row for matrices).</li> <li>\n<code>sorted</code>: If true the resulting <code>k</code> elements will be sorted by the values in descending order.</li> <li>\n<code>name</code>: Optional name for the operation.</li> </ul>  <h5 id=\"returns-45\">Returns:</h5> <ul> <li>\n<code>values</code>: The <code>k</code> largest elements along each last dimensional slice.</li> <li>\n<code>indices</code>: The indices of <code>values</code> within the last dimension of <code>input</code>.</li> </ul>  <h3 id=\"in_top_k\"><code>tf.nn.in_top_k(predictions, targets, k, name=None)</code></h3> <p>Says whether the targets are in the top <code>K</code> predictions.</p> <p>This outputs a <code>batch_size</code> bool array, an entry <code>out[i]</code> is <code>true</code> if the prediction for the target class is among the top <code>k</code> predictions among all predictions for example <code>i</code>. Note that the behavior of <code>InTopK</code> differs from the <code>TopK</code> op in its handling of ties; if multiple classes have the same prediction value and straddle the top-<code>k</code> boundary, all of those classes are considered to be in the top <code>k</code>.</p> <p>More formally, let</p> <p>\\(predictions_i\\) be the predictions for all classes for example <code>i</code>, \\(targets_i\\) be the target class for example <code>i</code>, \\(out_i\\) be the output for example <code>i</code>,</p> <p>\\[out_i = predictions_{i, targets_i} \\in TopKIncludingTies(predictions_i)\\]</p>  <h5 id=\"args-46\">Args:</h5> <ul> <li>\n<code>predictions</code>: A <code>Tensor</code> of type <code>float32</code>. A <code>batch_size</code> x <code>classes</code> tensor.</li> <li>\n<code>targets</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A <code>batch_size</code> vector of class ids.</li> <li>\n<code>k</code>: An <code>int</code>. Number of top elements to look at for computing precision.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-46\">Returns:</h5> <p>A <code>Tensor</code> of type <code>bool</code>. Computed Precision at <code>k</code> as a <code>bool Tensor</code>.</p>  <h2 id=\"candidate-sampling\">Candidate Sampling</h2> <p>Do you want to train a multiclass or multilabel model with thousands or millions of output classes (for example, a language model with a large vocabulary)? Training with a full Softmax is slow in this case, since all of the classes are evaluated for every training example. Candidate Sampling training algorithms can speed up your step times by only considering a small randomly-chosen subset of contrastive classes (called candidates) for each batch of training examples.</p> <p>See our <a href=\"https://www.tensorflow.org/versions/r0.10/extras/candidate_sampling.pdf\">Candidate Sampling Algorithms Reference</a></p>  <h3 id=\"sampled-loss-functions\">Sampled Loss Functions</h3> <p>TensorFlow provides the following sampled loss functions for faster training.</p>  <h3 id=\"nce_loss\"><code>tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy='mod', name='nce_loss')</code></h3> <p>Computes and returns the noise-contrastive estimation training loss.</p> <p>See <a href=\"http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf\" rel=\"noreferrer\">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</a>. Also see our <a href=\"https://www.tensorflow.org/versions/r0.10/extras/candidate_sampling.pdf\">Candidate Sampling Algorithms Reference</a></p> <p>Note: In the case where <code>num_true</code> &gt; 1, we assign to each target class the target probability 1 / <code>num_true</code> so that the target probabilities sum to 1 per-example.</p> <p>Note: It would be useful to allow a variable number of target classes per example. We hope to provide this functionality in a future release. For now, if you have a variable number of target classes, you can pad them out to a constant number by either repeating them or by padding with an otherwise unused class.</p>  <h5 id=\"args-47\">Args:</h5> <ul> <li>\n<code>weights</code>: A <code>Tensor</code> of shape <code>[num_classes, dim]</code>, or a list of <code>Tensor</code> objects whose concatenation along dimension 0 has shape [num_classes, dim]. The (possibly-partitioned) class embeddings.</li> <li>\n<code>biases</code>: A <code>Tensor</code> of shape <code>[num_classes]</code>. The class biases.</li> <li>\n<code>inputs</code>: A <code>Tensor</code> of shape <code>[batch_size, dim]</code>. The forward activations of the input network.</li> <li>\n<code>labels</code>: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,\n  num_true]</code>. The target classes.</li> <li>\n<code>num_sampled</code>: An <code>int</code>. The number of classes to randomly sample per batch.</li> <li>\n<code>num_classes</code>: An <code>int</code>. The number of possible classes.</li> <li>\n<code>num_true</code>: An <code>int</code>. The number of target classes per training example.</li> <li>\n<code>sampled_values</code>: a tuple of (<code>sampled_candidates</code>, <code>true_expected_count</code>, <code>sampled_expected_count</code>) returned by a <code>*_candidate_sampler</code> function. (if None, we default to <code>log_uniform_candidate_sampler</code>)</li> <li>\n<code>remove_accidental_hits</code>: A <code>bool</code>. Whether to remove \"accidental hits\" where a sampled class equals one of the target classes. If set to <code>True</code>, this is a \"Sampled Logistic\" loss instead of NCE, and we are learning to generate log-odds instead of log probabilities. See our <a href=\"https://www.tensorflow.org/versions/r0.10/extras/candidate_sampling.pdf\">Candidate Sampling Algorithms Reference</a>. Default is False.</li> <li>\n<code>partition_strategy</code>: A string specifying the partitioning strategy, relevant if <code>len(weights) &gt; 1</code>. Currently <code>\"div\"</code> and <code>\"mod\"</code> are supported. Default is <code>\"mod\"</code>. See <code>tf.nn.embedding_lookup</code> for more details.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-47\">Returns:</h5> <p>A <code>batch_size</code> 1-D tensor of per-example NCE losses.</p>  <h3 id=\"sampled_softmax_loss\"><code>tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy='mod', name='sampled_softmax_loss')</code></h3> <p>Computes and returns the sampled softmax training loss.</p> <p>This is a faster way to train a softmax classifier over a huge number of classes.</p> <p>This operation is for training only. It is generally an underestimate of the full softmax loss.</p> <p>At inference time, you can compute full softmax probabilities with the expression <code>tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases)</code>.</p> <p>See our <a href=\"https://www.tensorflow.org/versions/r0.10/extras/candidate_sampling.pdf\">Candidate Sampling Algorithms Reference</a></p> <p>Also see Section 3 of <a href=\"http://arxiv.org/abs/1412.2007\" rel=\"noreferrer\">Jean et al., 2014</a> (<a href=\"http://arxiv.org/pdf/1412.2007.pdf\" rel=\"noreferrer\">pdf</a>) for the math.</p>  <h5 id=\"args-48\">Args:</h5> <ul> <li>\n<code>weights</code>: A <code>Tensor</code> of shape <code>[num_classes, dim]</code>, or a list of <code>Tensor</code> objects whose concatenation along dimension 0 has shape [num_classes, dim]. The (possibly-sharded) class embeddings.</li> <li>\n<code>biases</code>: A <code>Tensor</code> of shape <code>[num_classes]</code>. The class biases.</li> <li>\n<code>inputs</code>: A <code>Tensor</code> of shape <code>[batch_size, dim]</code>. The forward activations of the input network.</li> <li>\n<code>labels</code>: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,\n  num_true]</code>. The target classes. Note that this format differs from the <code>labels</code> argument of <code>nn.softmax_cross_entropy_with_logits</code>.</li> <li>\n<code>num_sampled</code>: An <code>int</code>. The number of classes to randomly sample per batch.</li> <li>\n<code>num_classes</code>: An <code>int</code>. The number of possible classes.</li> <li>\n<code>num_true</code>: An <code>int</code>. The number of target classes per training example.</li> <li>\n<code>sampled_values</code>: a tuple of (<code>sampled_candidates</code>, <code>true_expected_count</code>, <code>sampled_expected_count</code>) returned by a <code>*_candidate_sampler</code> function. (if None, we default to <code>log_uniform_candidate_sampler</code>)</li> <li>\n<code>remove_accidental_hits</code>: A <code>bool</code>. whether to remove \"accidental hits\" where a sampled class equals one of the target classes. Default is True.</li> <li>\n<code>partition_strategy</code>: A string specifying the partitioning strategy, relevant if <code>len(weights) &gt; 1</code>. Currently <code>\"div\"</code> and <code>\"mod\"</code> are supported. Default is <code>\"mod\"</code>. See <code>tf.nn.embedding_lookup</code> for more details.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-48\">Returns:</h5> <p>A <code>batch_size</code> 1-D tensor of per-example sampled softmax losses.</p>  <h3 id=\"candidate-samplers\">Candidate Samplers</h3> <p>TensorFlow provides the following samplers for randomly sampling candidate classes when using one of the sampled loss functions above.</p>  <h3 id=\"uniform_candidate_sampler\"><code>tf.nn.uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)</code></h3> <p>Samples a set of classes using a uniform base distribution.</p> <p>This operation randomly samples a tensor of sampled classes (<code>sampled_candidates</code>) from the range of integers <code>[0, range_max)</code>.</p> <p>The elements of <code>sampled_candidates</code> are drawn without replacement (if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from the base distribution.</p> <p>The base distribution for this operation is the uniform distribution over the range of integers <code>[0, range_max)</code>.</p> <p>In addition, this operation returns tensors <code>true_expected_count</code> and <code>sampled_expected_count</code> representing the number of times each of the target classes (<code>true_classes</code>) and the sampled classes (<code>sampled_candidates</code>) is expected to occur in an average tensor of sampled classes. These values correspond to <code>Q(y|x)</code> defined in <a href=\"http://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"noreferrer\">this document</a>. If <code>unique=True</code>, then these are post-rejection probabilities and we compute them approximately.</p>  <h5 id=\"args-49\">Args:</h5> <ul> <li>\n<code>true_classes</code>: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,\nnum_true]</code>. The target classes.</li> <li>\n<code>num_true</code>: An <code>int</code>. The number of target classes per training example.</li> <li>\n<code>num_sampled</code>: An <code>int</code>. The number of classes to randomly sample per batch.</li> <li>\n<code>unique</code>: A <code>bool</code>. Determines whether all sampled classes in a batch are unique.</li> <li>\n<code>range_max</code>: An <code>int</code>. The number of possible classes.</li> <li>\n<code>seed</code>: An <code>int</code>. An operation-specific seed. Default is 0.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-49\">Returns:</h5> <ul> <li>\n<code>sampled_candidates</code>: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>. The sampled classes.</li> <li>\n<code>true_expected_count</code>: A tensor of type <code>float</code>. Same shape as <code>true_classes</code>. The expected counts under the sampling distribution of each of <code>true_classes</code>.</li> <li>\n<code>sampled_expected_count</code>: A tensor of type <code>float</code>. Same shape as <code>sampled_candidates</code>. The expected counts under the sampling distribution of each of <code>sampled_candidates</code>.</li> </ul>  <h3 id=\"log_uniform_candidate_sampler\"><code>tf.nn.log_uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)</code></h3> <p>Samples a set of classes using a log-uniform (Zipfian) base distribution.</p> <p>This operation randomly samples a tensor of sampled classes (<code>sampled_candidates</code>) from the range of integers <code>[0, range_max)</code>.</p> <p>The elements of <code>sampled_candidates</code> are drawn without replacement (if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from the base distribution.</p> <p>The base distribution for this operation is an approximately log-uniform or Zipfian distribution:</p> <p><code>P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)</code></p> <p>This sampler is useful when the target classes approximately follow such a distribution - for example, if the classes represent words in a lexicon sorted in decreasing order of frequency. If your classes are not ordered by decreasing frequency, do not use this op.</p> <p>In addition, this operation returns tensors <code>true_expected_count</code> and <code>sampled_expected_count</code> representing the number of times each of the target classes (<code>true_classes</code>) and the sampled classes (<code>sampled_candidates</code>) is expected to occur in an average tensor of sampled classes. These values correspond to <code>Q(y|x)</code> defined in <a href=\"http://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"noreferrer\">this document</a>. If <code>unique=True</code>, then these are post-rejection probabilities and we compute them approximately.</p>  <h5 id=\"args-50\">Args:</h5> <ul> <li>\n<code>true_classes</code>: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,\nnum_true]</code>. The target classes.</li> <li>\n<code>num_true</code>: An <code>int</code>. The number of target classes per training example.</li> <li>\n<code>num_sampled</code>: An <code>int</code>. The number of classes to randomly sample per batch.</li> <li>\n<code>unique</code>: A <code>bool</code>. Determines whether all sampled classes in a batch are unique.</li> <li>\n<code>range_max</code>: An <code>int</code>. The number of possible classes.</li> <li>\n<code>seed</code>: An <code>int</code>. An operation-specific seed. Default is 0.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-50\">Returns:</h5> <ul> <li>\n<code>sampled_candidates</code>: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>. The sampled classes.</li> <li>\n<code>true_expected_count</code>: A tensor of type <code>float</code>. Same shape as <code>true_classes</code>. The expected counts under the sampling distribution of each of <code>true_classes</code>.</li> <li>\n<code>sampled_expected_count</code>: A tensor of type <code>float</code>. Same shape as <code>sampled_candidates</code>. The expected counts under the sampling distribution of each of <code>sampled_candidates</code>.</li> </ul>  <h3 id=\"learned_unigram_candidate_sampler\"><code>tf.nn.learned_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)</code></h3> <p>Samples a set of classes from a distribution learned during training.</p> <p>This operation randomly samples a tensor of sampled classes (<code>sampled_candidates</code>) from the range of integers <code>[0, range_max)</code>.</p> <p>The elements of <code>sampled_candidates</code> are drawn without replacement (if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from the base distribution.</p> <p>The base distribution for this operation is constructed on the fly during training. It is a unigram distribution over the target classes seen so far during training. Every integer in <code>[0, range_max)</code> begins with a weight of 1, and is incremented by 1 each time it is seen as a target class. The base distribution is not saved to checkpoints, so it is reset when the model is reloaded.</p> <p>In addition, this operation returns tensors <code>true_expected_count</code> and <code>sampled_expected_count</code> representing the number of times each of the target classes (<code>true_classes</code>) and the sampled classes (<code>sampled_candidates</code>) is expected to occur in an average tensor of sampled classes. These values correspond to <code>Q(y|x)</code> defined in <a href=\"http://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"noreferrer\">this document</a>. If <code>unique=True</code>, then these are post-rejection probabilities and we compute them approximately.</p>  <h5 id=\"args-51\">Args:</h5> <ul> <li>\n<code>true_classes</code>: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,\nnum_true]</code>. The target classes.</li> <li>\n<code>num_true</code>: An <code>int</code>. The number of target classes per training example.</li> <li>\n<code>num_sampled</code>: An <code>int</code>. The number of classes to randomly sample per batch.</li> <li>\n<code>unique</code>: A <code>bool</code>. Determines whether all sampled classes in a batch are unique.</li> <li>\n<code>range_max</code>: An <code>int</code>. The number of possible classes.</li> <li>\n<code>seed</code>: An <code>int</code>. An operation-specific seed. Default is 0.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-51\">Returns:</h5> <ul> <li>\n<code>sampled_candidates</code>: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>. The sampled classes.</li> <li>\n<code>true_expected_count</code>: A tensor of type <code>float</code>. Same shape as <code>true_classes</code>. The expected counts under the sampling distribution of each of <code>true_classes</code>.</li> <li>\n<code>sampled_expected_count</code>: A tensor of type <code>float</code>. Same shape as <code>sampled_candidates</code>. The expected counts under the sampling distribution of each of <code>sampled_candidates</code>.</li> </ul>  <h3 id=\"fixed_unigram_candidate_sampler\"><code>tf.nn.fixed_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, vocab_file='', distortion=1.0, num_reserved_ids=0, num_shards=1, shard=0, unigrams=(), seed=None, name=None)</code></h3> <p>Samples a set of classes using the provided (fixed) base distribution.</p> <p>This operation randomly samples a tensor of sampled classes (<code>sampled_candidates</code>) from the range of integers <code>[0, range_max)</code>.</p> <p>The elements of <code>sampled_candidates</code> are drawn without replacement (if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from the base distribution.</p> <p>The base distribution is read from a file or passed in as an in-memory array. There is also an option to skew the distribution by applying a distortion power to the weights.</p> <p>In addition, this operation returns tensors <code>true_expected_count</code> and <code>sampled_expected_count</code> representing the number of times each of the target classes (<code>true_classes</code>) and the sampled classes (<code>sampled_candidates</code>) is expected to occur in an average tensor of sampled classes. These values correspond to <code>Q(y|x)</code> defined in <a href=\"http://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"noreferrer\">this document</a>. If <code>unique=True</code>, then these are post-rejection probabilities and we compute them approximately.</p>  <h5 id=\"args-52\">Args:</h5> <ul> <li>\n<code>true_classes</code>: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,\nnum_true]</code>. The target classes.</li> <li>\n<code>num_true</code>: An <code>int</code>. The number of target classes per training example.</li> <li>\n<code>num_sampled</code>: An <code>int</code>. The number of classes to randomly sample per batch.</li> <li>\n<code>unique</code>: A <code>bool</code>. Determines whether all sampled classes in a batch are unique.</li> <li>\n<code>range_max</code>: An <code>int</code>. The number of possible classes.</li> <li>\n<code>vocab_file</code>: Each valid line in this file (which should have a CSV-like format) corresponds to a valid word ID. IDs are in sequential order, starting from num_reserved_ids. The last entry in each line is expected to be a value corresponding to the count or relative probability. Exactly one of <code>vocab_file</code> and <code>unigrams</code> needs to be passed to this operation.</li> <li>\n<code>distortion</code>: The distortion is used to skew the unigram probability distribution. Each weight is first raised to the distortion's power before adding to the internal unigram distribution. As a result, <code>distortion = 1.0</code> gives regular unigram sampling (as defined by the vocab file), and <code>distortion = 0.0</code> gives a uniform distribution.</li> <li>\n<code>num_reserved_ids</code>: Optionally some reserved IDs can be added in the range <code>[0, num_reserved_ids]</code> by the users. One use case is that a special unknown word token is used as ID 0. These IDs will have a sampling probability of 0.</li> <li>\n<code>num_shards</code>: A sampler can be used to sample from a subset of the original range in order to speed up the whole computation through parallelism. This parameter (together with <code>shard</code>) indicates the number of partitions that are being used in the overall computation.</li> <li>\n<code>shard</code>: A sampler can be used to sample from a subset of the original range in order to speed up the whole computation through parallelism. This parameter (together with <code>num_shards</code>) indicates the particular partition number of the operation, when partitioning is being used.</li> <li>\n<code>unigrams</code>: A list of unigram counts or probabilities, one per ID in sequential order. Exactly one of <code>vocab_file</code> and <code>unigrams</code> should be passed to this operation.</li> <li>\n<code>seed</code>: An <code>int</code>. An operation-specific seed. Default is 0.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-52\">Returns:</h5> <ul> <li>\n<code>sampled_candidates</code>: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>. The sampled classes.</li> <li>\n<code>true_expected_count</code>: A tensor of type <code>float</code>. Same shape as <code>true_classes</code>. The expected counts under the sampling distribution of each of <code>true_classes</code>.</li> <li>\n<code>sampled_expected_count</code>: A tensor of type <code>float</code>. Same shape as <code>sampled_candidates</code>. The expected counts under the sampling distribution of each of <code>sampled_candidates</code>.</li> </ul>  <h3 id=\"miscellaneous-candidate-sampling-utilities\">Miscellaneous candidate sampling utilities</h3>  <h3 id=\"compute_accidental_hits\"><code>tf.nn.compute_accidental_hits(true_classes, sampled_candidates, num_true, seed=None, name=None)</code></h3> <p>Compute the position ids in <code>sampled_candidates</code> matching <code>true_classes</code>.</p> <p>In Candidate Sampling, this operation facilitates virtually removing sampled classes which happen to match target classes. This is done in Sampled Softmax and Sampled Logistic.</p> <p>See our <a href=\"http://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"noreferrer\">Candidate Sampling Algorithms Reference</a>.</p> <p>We presuppose that the <code>sampled_candidates</code> are unique.</p> <p>We call it an 'accidental hit' when one of the target classes matches one of the sampled classes. This operation reports accidental hits as triples <code>(index, id, weight)</code>, where <code>index</code> represents the row number in <code>true_classes</code>, <code>id</code> represents the position in <code>sampled_candidates</code>, and weight is <code>-FLOAT_MAX</code>.</p> <p>The result of this op should be passed through a <code>sparse_to_dense</code> operation, then added to the logits of the sampled classes. This removes the contradictory effect of accidentally sampling the true target classes as noise classes for the same example.</p>  <h5 id=\"args-53\">Args:</h5> <ul> <li>\n<code>true_classes</code>: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,\nnum_true]</code>. The target classes.</li> <li>\n<code>sampled_candidates</code>: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>. The sampled_candidates output of CandidateSampler.</li> <li>\n<code>num_true</code>: An <code>int</code>. The number of target classes per training example.</li> <li>\n<code>seed</code>: An <code>int</code>. An operation-specific seed. Default is 0.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-53\">Returns:</h5> <ul> <li>\n<code>indices</code>: A <code>Tensor</code> of type <code>int32</code> and shape <code>[num_accidental_hits]</code>. Values indicate rows in <code>true_classes</code>.</li> <li>\n<code>ids</code>: A <code>Tensor</code> of type <code>int64</code> and shape <code>[num_accidental_hits]</code>. Values indicate positions in <code>sampled_candidates</code>.</li> <li>\n<code>weights</code>: A <code>Tensor</code> of type <code>float</code> and shape <code>[num_accidental_hits]</code>. Each value is <code>-FLOAT_MAX</code>.</li> </ul>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"batch_normalization\"><code>tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None)</code></h3> <p>Batch normalization.</p> <p>As described in <a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/abs/1502.03167&amp;usg=AFQjCNEfFbC155me911plnLz7i6FE9f-jg\">http://arxiv.org/abs/1502.03167</a>. Normalizes a tensor by <code>mean</code> and <code>variance</code>, and applies (optionally) a <code>scale</code> \\\\(\\gamma\\\\\\) to it, as well as an <code>offset</code> \\\\(\\\\beta\\\\\\):</p> <p>\\\\(\\\\frac{\\gamma(x-\\mu)}{\\sigma}+\\\\beta\\\\\\)</p> <p><code>mean</code>, <code>variance</code>, <code>offset</code> and <code>scale</code> are all expected to be of one of two shapes: * In all generality, they can have the same number of dimensions as the input <code>x</code>, with identical sizes as <code>x</code> for the dimensions that are not normalized over (the 'depth' dimension(s)), and dimension 1 for the others which are being normalized over. <code>mean</code> and <code>variance</code> in this case would typically be the outputs of <code>tf.nn.moments(..., keep_dims=True)</code> during training, or running averages thereof during inference. * In the common case where the 'depth' dimension is the last dimension in the input tensor <code>x</code>, they may be one dimensional tensors of the same size as the 'depth' dimension. This is the case for example for the common <code>[batch, depth]</code> layout of fully-connected layers, and <code>[batch, height, width, depth]</code> for convolutions. <code>mean</code> and <code>variance</code> in this case would typically be the outputs of <code>tf.nn.moments(..., keep_dims=False)</code> during training, or running averages thereof during inference.</p>  <h5 id=\"args-54\">Args:</h5> <ul> <li>\n<code>x</code>: Input <code>Tensor</code> of arbitrary dimensionality.</li> <li>\n<code>mean</code>: A mean <code>Tensor</code>.</li> <li>\n<code>variance</code>: A variance <code>Tensor</code>.</li> <li>\n<code>offset</code>: An offset <code>Tensor</code>, often denoted \\\\(\\\\beta\\\\\\) in equations, or None. If present, will be added to the normalized tensor.</li> <li>\n<code>scale</code>: A scale <code>Tensor</code>, often denoted \\\\(\\gamma\\\\\\) in equations, or <code>None</code>. If present, the scale is applied to the normalized tensor.</li> <li>\n<code>variance_epsilon</code>: A small float number to avoid dividing by 0.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-54\">Returns:</h5> <p>the normalized, scaled, offset tensor.</p>  <h3 id=\"depthwise_conv2d_native\"><code>tf.nn.depthwise_conv2d_native(input, filter, strides, padding, name=None)</code></h3> <p>Computes a 2-D depthwise convolution given 4-D <code>input</code> and <code>filter</code> tensors.</p> <p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code> and a filter / kernel tensor of shape <code>[filter_height, filter_width, in_channels, channel_multiplier]</code>, containing <code>in_channels</code> convolutional filters of depth 1, <code>depthwise_conv2d</code> applies a different filter to each input channel (expanding from 1 channel to <code>channel_multiplier</code> channels for each), then concatenates the results together. Thus, the output has <code>in_channels * channel_multiplier</code> channels.</p> <p>for k in 0..in_channels-1 for q in 0..channel_multiplier-1 output[b, i, j, k * channel_multiplier + q] = sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] * filter[di, dj, k, q]</p> <p>Must have <code>strides[0] = strides[3] = 1</code>. For the most common case of the same horizontal and vertices strides, <code>strides = [1, stride, stride, 1]</code>.</p>  <h5 id=\"args-55\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.</li> <li>\n<code>filter</code>: A <code>Tensor</code>. Must have the same type as <code>input</code>.</li> <li>\n<code>strides</code>: A list of <code>ints</code>. 1-D of length 4. The stride of the sliding window for each dimension of <code>input</code>.</li> <li>\n<code>padding</code>: A <code>string</code> from: <code>\"SAME\", \"VALID\"</code>. The type of padding algorithm to use.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-55\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html</a>\n  </p>\n</div>\n","summary":"<h1 id=\"summary-operations\">Summary Operations</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#summary-operations\">Summary Operations</a></li> <ul> <li><a href=\"#summary-ops\">Summary Ops</a></li> <ul> <li><a href=\"#tensor_summary\"><code>tf.summary.tensor_summary(display_name, tensor, description=, labels=None, collections=None, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>This module contains ops for generating summaries.</p>  <h2 id=\"summary-ops\">Summary Ops</h2>  <h3 id=\"tensor_summary\"><code>tf.summary.tensor_summary(display_name, tensor, description='', labels=None, collections=None, name=None)</code></h3> <p>Outputs a <code>Summary</code> protocol buffer with a serialized tensor.proto.</p> <p>The generated <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto\" rel=\"noreferrer\"><code>Summary</code></a> has one summary value containing input_tensor.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>display_name</code>: A name to associate with the data series. Will be used to organize output data and as a name in visualizers.</li> <li>\n<code>tensor</code>: A tensor of any type and shape to serialize.</li> <li>\n<code>description</code>: An optional long description of the data being output.</li> <li>\n<code>labels</code>: a list of strings used to specify how the data can be interpreted, for example: <ul> <li>\n<code>'encoding:image/jpg'</code> for a string tensor containing jpg images</li> <li>\n<code>'encoding:proto/X/Y/foo.proto'</code> for a string tensor containing Foos</li> <li>\n<code>'group:$groupName/$roleInGroup'</code> for a tensor that is related to other tensors that are all in a group. (e.g. bounding boxes and images)</li> </ul>\n</li> <li>\n<code>collections</code>: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to <code>[GraphKeys.SUMMARIES]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A scalar <code>Tensor</code> of type <code>string</code>. The serialized <code>Summary</code> protocol buffer.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/summary.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/summary.html</a>\n  </p>\n</div>\n","io_ops":"<h1 id=\"inputs-and-readers\">Inputs and Readers</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#inputs-and-readers\">Inputs and Readers</a></li> <ul> <li><a href=\"#placeholders\">Placeholders</a></li> <ul> <li><a href=\"#placeholder\"><code>tf.placeholder(dtype, shape=None, name=None)</code></a></li> <li><a href=\"#placeholder_with_default\"><code>tf.placeholder_with_default(input, shape, name=None)</code></a></li> <li><a href=\"#sparse_placeholder\"><code>tf.sparse_placeholder(dtype, shape=None, name=None)</code></a></li> </ul> <li><a href=\"#readers\">Readers</a></li> <ul> <li><a href=\"#ReaderBase\"><code>class tf.ReaderBase</code></a></li> <li><a href=\"#TextLineReader\"><code>class tf.TextLineReader</code></a></li> <li><a href=\"#WholeFileReader\"><code>class tf.WholeFileReader</code></a></li> <li><a href=\"#IdentityReader\"><code>class tf.IdentityReader</code></a></li> <li><a href=\"#TFRecordReader\"><code>class tf.TFRecordReader</code></a></li> <li><a href=\"#FixedLengthRecordReader\"><code>class tf.FixedLengthRecordReader</code></a></li> </ul> <li><a href=\"#converting\">Converting</a></li> <ul> <li><a href=\"#decode_csv\"><code>tf.decode_csv(records, record_defaults, field_delim=None, name=None)</code></a></li> <li><a href=\"#decode_raw\"><code>tf.decode_raw(bytes, out_type, little_endian=None, name=None)</code></a></li> <li><a href=\"#example-protocol-buffer\">Example protocol buffer</a></li> <li><a href=\"#VarLenFeature\"><code>class tf.VarLenFeature</code></a></li> <li><a href=\"#FixedLenFeature\"><code>class tf.FixedLenFeature</code></a></li> <li><a href=\"#FixedLenSequenceFeature\"><code>class tf.FixedLenSequenceFeature</code></a></li> <li><a href=\"#parse_example\"><code>tf.parse_example(serialized, features, name=None, example_names=None)</code></a></li> <li><a href=\"#parse_single_example\"><code>tf.parse_single_example(serialized, features, name=None, example_names=None)</code></a></li> <li><a href=\"#decode_json_example\"><code>tf.decode_json_example(json_examples, name=None)</code></a></li> </ul> <li><a href=\"#queues\">Queues</a></li> <ul> <li><a href=\"#QueueBase\"><code>class tf.QueueBase</code></a></li> <li><a href=\"#FIFOQueue\"><code>class tf.FIFOQueue</code></a></li> <li><a href=\"#PaddingFIFOQueue\"><code>class tf.PaddingFIFOQueue</code></a></li> <li><a href=\"#RandomShuffleQueue\"><code>class tf.RandomShuffleQueue</code></a></li> </ul> <li><a href=\"#dealing-with-the-filesystem\">Dealing with the filesystem</a></li> <ul> <li><a href=\"#matching_files\"><code>tf.matching_files(pattern, name=None)</code></a></li> <li><a href=\"#read_file\"><code>tf.read_file(filename, name=None)</code></a></li> </ul> <li><a href=\"#input-pipeline\">Input pipeline</a></li> <ul> <li><a href=\"#beginning-of-an-input-pipeline\">Beginning of an input pipeline</a></li> <li><a href=\"#match_filenames_once\"><code>tf.train.match_filenames_once(pattern, name=None)</code></a></li> <li><a href=\"#limit_epochs\"><code>tf.train.limit_epochs(tensor, num_epochs=None, name=None)</code></a></li> <li><a href=\"#input_producer\"><code>tf.train.input_producer(input_tensor, element_shape=None, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, summary_name=None, name=None)</code></a></li> <li><a href=\"#range_input_producer\"><code>tf.train.range_input_producer(limit, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code></a></li> <li><a href=\"#slice_input_producer\"><code>tf.train.slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code></a></li> <li><a href=\"#string_input_producer\"><code>tf.train.string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code></a></li> <li><a href=\"#batching-at-the-end-of-an-input-pipeline\">Batching at the end of an input pipeline</a></li> <li><a href=\"#batch\"><code>tf.train.batch(tensors, batch_size, num_threads=1, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)</code></a></li> <li><a href=\"#batch_join\"><code>tf.train.batch_join(tensors_list, batch_size, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)</code></a></li> <li><a href=\"#shuffle_batch\"><code>tf.train.shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)</code></a></li> <li><a href=\"#shuffle_batch_join\"><code>tf.train.shuffle_batch_join(tensors_list, batch_size, capacity, min_after_dequeue, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)</code></a></li> </ul>\n</ul>\n</ul> </div> <h2 id=\"placeholders\">Placeholders</h2> <p>TensorFlow provides a placeholder operation that must be fed with data on execution. For more info, see the section on <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#feeding\">Feeding data</a>.</p>  <h3 id=\"placeholder\"><code>tf.placeholder(dtype, shape=None, name=None)</code></h3> <p>Inserts a placeholder for a tensor that will be always fed.</p> <p><strong>Important</strong>: This tensor will produce an error if evaluated. Its value must be fed using the <code>feed_dict</code> optional argument to <code>Session.run()</code>, <code>Tensor.eval()</code>, or <code>Operation.run()</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.placeholder(tf.float32, shape=(1024, 1024))\ny = tf.matmul(x, x)\n\nwith tf.Session() as sess:\n  print(sess.run(y))  # ERROR: will fail because x was not fed.\n\n  rand_array = np.random.rand(1024, 1024)\n  print(sess.run(y, feed_dict={x: rand_array}))  # Will succeed.\n</pre> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>dtype</code>: The type of elements in the tensor to be fed.</li> <li>\n<code>shape</code>: The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a tensor of any shape.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code> that may be used as a handle for feeding a value, but not evaluated directly.</p>  <h3 id=\"placeholder_with_default\"><code>tf.placeholder_with_default(input, shape, name=None)</code></h3> <p>A placeholder op that passes though <code>input</code> when its output is not fed.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>. The default value to produce when <code>output</code> is not fed.</li> <li>\n<code>shape</code>: A <code>tf.TensorShape</code> or list of <code>ints</code>. The (possibly partial) shape of the tensor.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>. A placeholder tensor that defaults to <code>input</code> if it is not fed.</p> <p>For feeding <code>SparseTensor</code>s which are composite type, there is a convenience function:</p>  <h3 id=\"sparse_placeholder\"><code>tf.sparse_placeholder(dtype, shape=None, name=None)</code></h3> <p>Inserts a placeholder for a sparse tensor that will be always fed.</p> <p><strong>Important</strong>: This sparse tensor will produce an error if evaluated. Its value must be fed using the <code>feed_dict</code> optional argument to <code>Session.run()</code>, <code>Tensor.eval()</code>, or <code>Operation.run()</code>.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">x = tf.sparse_placeholder(tf.float32)\ny = tf.sparse_reduce_sum(x)\n\nwith tf.Session() as sess:\n  print(sess.run(y))  # ERROR: will fail because x was not fed.\n\n  indices = np.array([[3, 2, 0], [4, 5, 1]], dtype=np.int64)\n  values = np.array([1.0, 2.0], dtype=np.float32)\n  shape = np.array([7, 9, 2], dtype=np.int64)\n  print(sess.run(y, feed_dict={\n    x: tf.SparseTensorValue(indices, values, shape)}))  # Will succeed.\n  print(sess.run(y, feed_dict={\n    x: (indices, values, shape)}))  # Will succeed.\n\n  sp = tf.SparseTensor(indices=indices, values=values, shape=shape)\n  sp_value = sp.eval(session)\n  print(sess.run(y, feed_dict={x: sp_value}))  # Will succeed.\n</pre>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>dtype</code>: The type of <code>values</code> elements in the tensor to be fed.</li> <li>\n<code>shape</code>: The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a sparse tensor of any shape.</li> <li>\n<code>name</code>: A name for prefixing the operations (optional).</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>SparseTensor</code> that may be used as a handle for feeding a value, but not evaluated directly.</p> <h2 id=\"readers\">Readers</h2> <p>TensorFlow provides a set of Reader classes for reading data formats. For more information on inputs and readers, see <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html\">Reading data</a>.</p>  <h3 id=\"ReaderBase\"><code>class tf.ReaderBase</code></h3> <p>Base class for different Reader types, that produce a record every step.</p> <p>Conceptually, Readers convert string 'work units' into records (key, value pairs). Typically the 'work units' are filenames and the records are extracted from the contents of those files. We want a single record produced per step, but a work unit can correspond to many records.</p> <p>Therefore we introduce some decoupling using a queue. The queue contains the work units and the Reader dequeues from the queue when it is asked to produce a record (via Read()) but it has finished the last work unit.</p>  <h4 id=\"ReaderBase.__init__\"><code>tf.ReaderBase.__init__(reader_ref, supports_serialize=False)</code></h4> <p>Creates a new ReaderBase.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>reader_ref</code>: The operation that implements the reader.</li> <li>\n<code>supports_serialize</code>: True if the reader implementation can serialize its state.</li> </ul>  <h4 id=\"ReaderBase.num_records_produced\"><code>tf.ReaderBase.num_records_produced(name=None)</code></h4> <p>Returns the number of records this reader has produced.</p> <p>This is the same as the number of Read executions that have succeeded.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"ReaderBase.num_work_units_completed\"><code>tf.ReaderBase.num_work_units_completed(name=None)</code></h4> <p>Returns the number of work units this reader has finished processing.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"ReaderBase.read\"><code>tf.ReaderBase.read(queue, name=None)</code></h4> <p>Returns the next record (key, value pair) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A tuple of Tensors (key, value).</p> <ul> <li>\n<code>key</code>: A string scalar Tensor.</li> <li>\n<code>value</code>: A string scalar Tensor.</li> </ul>  <h4 id=\"ReaderBase.read_up_to\"><code>tf.ReaderBase.read_up_to(queue, num_records, name=None)</code></h4> <p>Returns up to num_records (key, value pairs) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>num_records</code>: Number of records to read.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A tuple of Tensors (keys, values).</p> <ul> <li>\n<code>keys</code>: A 1-D string Tensor.</li> <li>\n<code>values</code>: A 1-D string Tensor.</li> </ul>  <h4 id=\"ReaderBase.reader_ref\"><code>tf.ReaderBase.reader_ref</code></h4> <p>Op that implements the reader.</p>  <h4 id=\"ReaderBase.reset\"><code>tf.ReaderBase.reset(name=None)</code></h4> <p>Restore a reader to its initial clean state.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"ReaderBase.restore_state\"><code>tf.ReaderBase.restore_state(state, name=None)</code></h4> <p>Restore a reader to a previously saved state.</p> <p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>state</code>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"ReaderBase.serialize_state\"><code>tf.ReaderBase.serialize_state(name=None)</code></h4> <p>Produce a string tensor that encodes the state of a reader.</p> <p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A string Tensor.</p>  <h4 id=\"ReaderBase.supports_serialize\"><code>tf.ReaderBase.supports_serialize</code></h4> <p>Whether the Reader implementation can serialize its state.</p>  <h3 id=\"TextLineReader\"><code>class tf.TextLineReader</code></h3> <p>A Reader that outputs the lines of a file delimited by newlines.</p> <p>Newlines are stripped from the output. See ReaderBase for supported methods.</p>  <h4 id=\"TextLineReader.__init__\"><code>tf.TextLineReader.__init__(skip_header_lines=None, name=None)</code></h4> <p>Create a TextLineReader.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>skip_header_lines</code>: An optional int. Defaults to 0. Number of lines to skip from the beginning of every file.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h4 id=\"TextLineReader.num_records_produced\"><code>tf.TextLineReader.num_records_produced(name=None)</code></h4> <p>Returns the number of records this reader has produced.</p> <p>This is the same as the number of Read executions that have succeeded.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"TextLineReader.num_work_units_completed\"><code>tf.TextLineReader.num_work_units_completed(name=None)</code></h4> <p>Returns the number of work units this reader has finished processing.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"TextLineReader.read\"><code>tf.TextLineReader.read(queue, name=None)</code></h4> <p>Returns the next record (key, value pair) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A tuple of Tensors (key, value).</p> <ul> <li>\n<code>key</code>: A string scalar Tensor.</li> <li>\n<code>value</code>: A string scalar Tensor.</li> </ul>  <h4 id=\"TextLineReader.read_up_to\"><code>tf.TextLineReader.read_up_to(queue, num_records, name=None)</code></h4> <p>Returns up to num_records (key, value pairs) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>num_records</code>: Number of records to read.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A tuple of Tensors (keys, values).</p> <ul> <li>\n<code>keys</code>: A 1-D string Tensor.</li> <li>\n<code>values</code>: A 1-D string Tensor.</li> </ul>  <h4 id=\"TextLineReader.reader_ref\"><code>tf.TextLineReader.reader_ref</code></h4> <p>Op that implements the reader.</p>  <h4 id=\"TextLineReader.reset\"><code>tf.TextLineReader.reset(name=None)</code></h4> <p>Restore a reader to its initial clean state.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"TextLineReader.restore_state\"><code>tf.TextLineReader.restore_state(state, name=None)</code></h4> <p>Restore a reader to a previously saved state.</p> <p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>state</code>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"TextLineReader.serialize_state\"><code>tf.TextLineReader.serialize_state(name=None)</code></h4> <p>Produce a string tensor that encodes the state of a reader.</p> <p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A string Tensor.</p>  <h4 id=\"TextLineReader.supports_serialize\"><code>tf.TextLineReader.supports_serialize</code></h4> <p>Whether the Reader implementation can serialize its state.</p>  <h3 id=\"WholeFileReader\"><code>class tf.WholeFileReader</code></h3> <p>A Reader that outputs the entire contents of a file as a value.</p> <p>To use, enqueue filenames in a Queue. The output of Read will be a filename (key) and the contents of that file (value).</p> <p>See ReaderBase for supported methods.</p>  <h4 id=\"WholeFileReader.__init__\"><code>tf.WholeFileReader.__init__(name=None)</code></h4> <p>Create a WholeFileReader.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h4 id=\"WholeFileReader.num_records_produced\"><code>tf.WholeFileReader.num_records_produced(name=None)</code></h4> <p>Returns the number of records this reader has produced.</p> <p>This is the same as the number of Read executions that have succeeded.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"WholeFileReader.num_work_units_completed\"><code>tf.WholeFileReader.num_work_units_completed(name=None)</code></h4> <p>Returns the number of work units this reader has finished processing.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"WholeFileReader.read\"><code>tf.WholeFileReader.read(queue, name=None)</code></h4> <p>Returns the next record (key, value pair) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A tuple of Tensors (key, value).</p> <ul> <li>\n<code>key</code>: A string scalar Tensor.</li> <li>\n<code>value</code>: A string scalar Tensor.</li> </ul>  <h4 id=\"WholeFileReader.read_up_to\"><code>tf.WholeFileReader.read_up_to(queue, num_records, name=None)</code></h4> <p>Returns up to num_records (key, value pairs) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>num_records</code>: Number of records to read.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>A tuple of Tensors (keys, values).</p> <ul> <li>\n<code>keys</code>: A 1-D string Tensor.</li> <li>\n<code>values</code>: A 1-D string Tensor.</li> </ul>  <h4 id=\"WholeFileReader.reader_ref\"><code>tf.WholeFileReader.reader_ref</code></h4> <p>Op that implements the reader.</p>  <h4 id=\"WholeFileReader.reset\"><code>tf.WholeFileReader.reset(name=None)</code></h4> <p>Restore a reader to its initial clean state.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"WholeFileReader.restore_state\"><code>tf.WholeFileReader.restore_state(state, name=None)</code></h4> <p>Restore a reader to a previously saved state.</p> <p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>state</code>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"WholeFileReader.serialize_state\"><code>tf.WholeFileReader.serialize_state(name=None)</code></h4> <p>Produce a string tensor that encodes the state of a reader.</p> <p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A string Tensor.</p>  <h4 id=\"WholeFileReader.supports_serialize\"><code>tf.WholeFileReader.supports_serialize</code></h4> <p>Whether the Reader implementation can serialize its state.</p>  <h3 id=\"IdentityReader\"><code>class tf.IdentityReader</code></h3> <p>A Reader that outputs the queued work as both the key and value.</p> <p>To use, enqueue strings in a Queue. Read will take the front work string and output (work, work).</p> <p>See ReaderBase for supported methods.</p>  <h4 id=\"IdentityReader.__init__\"><code>tf.IdentityReader.__init__(name=None)</code></h4> <p>Create a IdentityReader.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h4 id=\"IdentityReader.num_records_produced\"><code>tf.IdentityReader.num_records_produced(name=None)</code></h4> <p>Returns the number of records this reader has produced.</p> <p>This is the same as the number of Read executions that have succeeded.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"IdentityReader.num_work_units_completed\"><code>tf.IdentityReader.num_work_units_completed(name=None)</code></h4> <p>Returns the number of work units this reader has finished processing.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"IdentityReader.read\"><code>tf.IdentityReader.read(queue, name=None)</code></h4> <p>Returns the next record (key, value pair) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>A tuple of Tensors (key, value).</p> <ul> <li>\n<code>key</code>: A string scalar Tensor.</li> <li>\n<code>value</code>: A string scalar Tensor.</li> </ul>  <h4 id=\"IdentityReader.read_up_to\"><code>tf.IdentityReader.read_up_to(queue, num_records, name=None)</code></h4> <p>Returns up to num_records (key, value pairs) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>num_records</code>: Number of records to read.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>A tuple of Tensors (keys, values).</p> <ul> <li>\n<code>keys</code>: A 1-D string Tensor.</li> <li>\n<code>values</code>: A 1-D string Tensor.</li> </ul>  <h4 id=\"IdentityReader.reader_ref\"><code>tf.IdentityReader.reader_ref</code></h4> <p>Op that implements the reader.</p>  <h4 id=\"IdentityReader.reset\"><code>tf.IdentityReader.reset(name=None)</code></h4> <p>Restore a reader to its initial clean state.</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"IdentityReader.restore_state\"><code>tf.IdentityReader.restore_state(state, name=None)</code></h4> <p>Restore a reader to a previously saved state.</p> <p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>state</code>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"IdentityReader.serialize_state\"><code>tf.IdentityReader.serialize_state(name=None)</code></h4> <p>Produce a string tensor that encodes the state of a reader.</p> <p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>A string Tensor.</p>  <h4 id=\"IdentityReader.supports_serialize\"><code>tf.IdentityReader.supports_serialize</code></h4> <p>Whether the Reader implementation can serialize its state.</p>  <h3 id=\"TFRecordReader\"><code>class tf.TFRecordReader</code></h3> <p>A Reader that outputs the records from a TFRecords file.</p> <p>See ReaderBase for supported methods.</p>  <h4 id=\"TFRecordReader.__init__\"><code>tf.TFRecordReader.__init__(name=None, options=None)</code></h4> <p>Create a TFRecordReader.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> <li>\n<code>options</code>: A TFRecordOptions object (optional).</li> </ul>  <h4 id=\"TFRecordReader.num_records_produced\"><code>tf.TFRecordReader.num_records_produced(name=None)</code></h4> <p>Returns the number of records this reader has produced.</p> <p>This is the same as the number of Read executions that have succeeded.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"TFRecordReader.num_work_units_completed\"><code>tf.TFRecordReader.num_work_units_completed(name=None)</code></h4> <p>Returns the number of work units this reader has finished processing.</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"TFRecordReader.read\"><code>tf.TFRecordReader.read(queue, name=None)</code></h4> <p>Returns the next record (key, value pair) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-34\">Returns:</h5> <p>A tuple of Tensors (key, value).</p> <ul> <li>\n<code>key</code>: A string scalar Tensor.</li> <li>\n<code>value</code>: A string scalar Tensor.</li> </ul>  <h4 id=\"TFRecordReader.read_up_to\"><code>tf.TFRecordReader.read_up_to(queue, num_records, name=None)</code></h4> <p>Returns up to num_records (key, value pairs) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>num_records</code>: Number of records to read.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>A tuple of Tensors (keys, values).</p> <ul> <li>\n<code>keys</code>: A 1-D string Tensor.</li> <li>\n<code>values</code>: A 1-D string Tensor.</li> </ul>  <h4 id=\"TFRecordReader.reader_ref\"><code>tf.TFRecordReader.reader_ref</code></h4> <p>Op that implements the reader.</p>  <h4 id=\"TFRecordReader.reset\"><code>tf.TFRecordReader.reset(name=None)</code></h4> <p>Restore a reader to its initial clean state.</p>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"TFRecordReader.restore_state\"><code>tf.TFRecordReader.restore_state(state, name=None)</code></h4> <p>Restore a reader to a previously saved state.</p> <p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>  <h5 id=\"args-42\">Args:</h5> <ul> <li>\n<code>state</code>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"TFRecordReader.serialize_state\"><code>tf.TFRecordReader.serialize_state(name=None)</code></h4> <p>Produce a string tensor that encodes the state of a reader.</p> <p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>  <h5 id=\"args-43\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>A string Tensor.</p>  <h4 id=\"TFRecordReader.supports_serialize\"><code>tf.TFRecordReader.supports_serialize</code></h4> <p>Whether the Reader implementation can serialize its state.</p>  <h3 id=\"FixedLengthRecordReader\"><code>class tf.FixedLengthRecordReader</code></h3> <p>A Reader that outputs fixed-length records from a file.</p> <p>See ReaderBase for supported methods.</p>  <h4 id=\"FixedLengthRecordReader.__init__\"><code>tf.FixedLengthRecordReader.__init__(record_bytes, header_bytes=None, footer_bytes=None, name=None)</code></h4> <p>Create a FixedLengthRecordReader.</p>  <h5 id=\"args-44\">Args:</h5> <ul> <li>\n<code>record_bytes</code>: An int.</li> <li>\n<code>header_bytes</code>: An optional int. Defaults to 0.</li> <li>\n<code>footer_bytes</code>: An optional int. Defaults to 0.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h4 id=\"FixedLengthRecordReader.num_records_produced\"><code>tf.FixedLengthRecordReader.num_records_produced(name=None)</code></h4> <p>Returns the number of records this reader has produced.</p> <p>This is the same as the number of Read executions that have succeeded.</p>  <h5 id=\"args-45\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-39\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"FixedLengthRecordReader.num_work_units_completed\"><code>tf.FixedLengthRecordReader.num_work_units_completed(name=None)</code></h4> <p>Returns the number of work units this reader has finished processing.</p>  <h5 id=\"args-46\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-40\">Returns:</h5> <p>An int64 Tensor.</p>  <h4 id=\"FixedLengthRecordReader.read\"><code>tf.FixedLengthRecordReader.read(queue, name=None)</code></h4> <p>Returns the next record (key, value pair) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>  <h5 id=\"args-47\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-41\">Returns:</h5> <p>A tuple of Tensors (key, value).</p> <ul> <li>\n<code>key</code>: A string scalar Tensor.</li> <li>\n<code>value</code>: A string scalar Tensor.</li> </ul>  <h4 id=\"FixedLengthRecordReader.read_up_to\"><code>tf.FixedLengthRecordReader.read_up_to(queue, num_records, name=None)</code></h4> <p>Returns up to num_records (key, value pairs) produced by a reader.</p> <p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>  <h5 id=\"args-48\">Args:</h5> <ul> <li>\n<code>queue</code>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li> <li>\n<code>num_records</code>: Number of records to read.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-42\">Returns:</h5> <p>A tuple of Tensors (keys, values).</p> <ul> <li>\n<code>keys</code>: A 1-D string Tensor.</li> <li>\n<code>values</code>: A 1-D string Tensor.</li> </ul>  <h4 id=\"FixedLengthRecordReader.reader_ref\"><code>tf.FixedLengthRecordReader.reader_ref</code></h4> <p>Op that implements the reader.</p>  <h4 id=\"FixedLengthRecordReader.reset\"><code>tf.FixedLengthRecordReader.reset(name=None)</code></h4> <p>Restore a reader to its initial clean state.</p>  <h5 id=\"args-49\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-43\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"FixedLengthRecordReader.restore_state\"><code>tf.FixedLengthRecordReader.restore_state(state, name=None)</code></h4> <p>Restore a reader to a previously saved state.</p> <p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>  <h5 id=\"args-50\">Args:</h5> <ul> <li>\n<code>state</code>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-44\">Returns:</h5> <p>The created Operation.</p>  <h4 id=\"FixedLengthRecordReader.serialize_state\"><code>tf.FixedLengthRecordReader.serialize_state(name=None)</code></h4> <p>Produce a string tensor that encodes the state of a reader.</p> <p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>  <h5 id=\"args-51\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-45\">Returns:</h5> <p>A string Tensor.</p>  <h4 id=\"FixedLengthRecordReader.supports_serialize\"><code>tf.FixedLengthRecordReader.supports_serialize</code></h4> <p>Whether the Reader implementation can serialize its state.</p> <h2 id=\"converting\">Converting</h2> <p>TensorFlow provides several operations that you can use to convert various data formats into tensors.</p>  <h3 id=\"decode_csv\"><code>tf.decode_csv(records, record_defaults, field_delim=None, name=None)</code></h3> <p>Convert CSV records to tensors. Each column maps to one tensor.</p> <p>RFC 4180 format is expected for the CSV records. (<a target=\"_blank\" href=\"https://www.google.com/url?q=https://tools.ietf.org/html/rfc4180&amp;usg=AFQjCNFnElM47j7H-JbuWRKAe49PFOkN8w\">https://tools.ietf.org/html/rfc4180</a>) Note that we allow leading and trailing spaces with int or float field.</p>  <h5 id=\"args-52\">Args:</h5> <ul> <li>\n<code>records</code>: A <code>Tensor</code> of type <code>string</code>. Each string is a record/row in the csv and all records should have the same format.</li> <li>\n<code>record_defaults</code>: A list of <code>Tensor</code> objects with types from: <code>float32</code>, <code>int32</code>, <code>int64</code>, <code>string</code>. One tensor per column of the input record, with either a scalar default value for that column or empty if the column is required.</li> <li>\n<code>field_delim</code>: An optional <code>string</code>. Defaults to <code>\",\"</code>. delimiter to separate fields in a record.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-46\">Returns:</h5> <p>A list of <code>Tensor</code> objects. Has the same type as <code>record_defaults</code>. Each tensor will have the same shape as records.</p>  <h3 id=\"decode_raw\"><code>tf.decode_raw(bytes, out_type, little_endian=None, name=None)</code></h3> <p>Reinterpret the bytes of a string as a vector of numbers.</p>  <h5 id=\"args-53\">Args:</h5> <ul> <li>\n<code>bytes</code>: A <code>Tensor</code> of type <code>string</code>. All the elements must have the same length.</li> <li>\n<code>out_type</code>: A <code>tf.DType</code> from: <code>tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.int64</code>.</li> <li>\n<code>little_endian</code>: An optional <code>bool</code>. Defaults to <code>True</code>. Whether the input <code>bytes</code> are in little-endian order. Ignored for <code>out_type</code> values that are stored in a single byte like <code>uint8</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-47\">Returns:</h5> <p>A <code>Tensor</code> of type <code>out_type</code>. A Tensor with one more dimension than the input <code>bytes</code>. The added dimension will have size equal to the length of the elements of <code>bytes</code> divided by the number of bytes to represent <code>out_type</code>.</p>   <h3 id=\"example-protocol-buffer\">Example protocol buffer</h3> <p>TensorFlow's <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#standard-tensorflow-format\">recommended format for training examples</a> is serialized <code>Example</code> protocol buffers, <a href=\"https://www.tensorflow.org/code/tensorflow/core/example/example.proto\" rel=\"noreferrer\">described here</a>. They contain <code>Features</code>, <a href=\"https://www.tensorflow.org/code/tensorflow/core/example/feature.proto\" rel=\"noreferrer\">described here</a>.</p>  <h3 id=\"VarLenFeature\"><code>class tf.VarLenFeature</code></h3> <p>Configuration for parsing a variable-length input feature.</p> <p>Fields: dtype: Data type of input.</p>  <h4 id=\"VarLenFeature.dtype\"><code>tf.VarLenFeature.dtype</code></h4> <p>Alias for field number 0</p>  <h3 id=\"FixedLenFeature\"><code>class tf.FixedLenFeature</code></h3> <p>Configuration for parsing a fixed-length input feature.</p> <p>To treat sparse input as dense, provide a <code>default_value</code>; otherwise, the parse functions will fail on any examples missing this feature.</p> <p>Fields: shape: Shape of input data. dtype: Data type of input. default_value: Value to be used if an example is missing this feature. It must be compatible with <code>dtype</code>.</p>  <h4 id=\"FixedLenFeature.default_value\"><code>tf.FixedLenFeature.default_value</code></h4> <p>Alias for field number 2</p>  <h4 id=\"FixedLenFeature.dtype\"><code>tf.FixedLenFeature.dtype</code></h4> <p>Alias for field number 1</p>  <h4 id=\"FixedLenFeature.shape\"><code>tf.FixedLenFeature.shape</code></h4> <p>Alias for field number 0</p>  <h3 id=\"FixedLenSequenceFeature\"><code>class tf.FixedLenSequenceFeature</code></h3> <p>Configuration for a dense input feature in a sequence item.</p> <p>To treat a sparse input as dense, provide <code>allow_missing=True</code>; otherwise, the parse functions will fail on any examples missing this feature.</p> <p>Fields: shape: Shape of input data. dtype: Data type of input. allow_missing: Whether to allow this feature to be missing from a feature list item.</p>  <h4 id=\"FixedLenSequenceFeature.allow_missing\"><code>tf.FixedLenSequenceFeature.allow_missing</code></h4> <p>Alias for field number 2</p>  <h4 id=\"FixedLenSequenceFeature.dtype\"><code>tf.FixedLenSequenceFeature.dtype</code></h4> <p>Alias for field number 1</p>  <h4 id=\"FixedLenSequenceFeature.shape\"><code>tf.FixedLenSequenceFeature.shape</code></h4> <p>Alias for field number 0</p>  <h3 id=\"parse_example\"><code>tf.parse_example(serialized, features, name=None, example_names=None)</code></h3> <p>Parses <code>Example</code> protos into a <code>dict</code> of tensors.</p> <p>Parses a number of serialized <a href=\"https://www.tensorflow.org/code/tensorflow/core/example/example.proto\" rel=\"noreferrer\"><code>Example</code></a> protos given in <code>serialized</code>.</p> <p><code>example_names</code> may contain descriptive names for the corresponding serialized protos. These may be useful for debugging purposes, but they have no effect on the output. If not <code>None</code>, <code>example_names</code> must be the same length as <code>serialized</code>.</p> <p>This op parses serialized examples into a dictionary mapping keys to <code>Tensor</code> and <code>SparseTensor</code> objects. <code>features</code> is a dict from keys to <code>VarLenFeature</code> and <code>FixedLenFeature</code> objects. Each <code>VarLenFeature</code> is mapped to a <code>SparseTensor</code>, and each <code>FixedLenFeature</code> is mapped to a <code>Tensor</code>.</p> <p>Each <code>VarLenFeature</code> maps to a <code>SparseTensor</code> of the specified type representing a ragged matrix. Its indices are <code>[batch, index]</code> where <code>batch</code> is the batch entry the value is from in <code>serialized</code>, and <code>index</code> is the value's index in the list of values associated with that feature and example.</p> <p>Each <code>FixedLenFeature</code> <code>df</code> maps to a <code>Tensor</code> of the specified type (or <code>tf.float32</code> if not specified) and shape <code>(serialized.size(),) + df.shape</code>.</p> <p><code>FixedLenFeature</code> entries with a <code>default_value</code> are optional. With no default value, we will fail if that <code>Feature</code> is missing from any example in <code>serialized</code>.</p> <p>Examples:</p> <p>For example, if one expects a <code>tf.float32</code> sparse feature <code>ft</code> and three serialized <code>Example</code>s are provided:</p> <pre class=\"\">serialized = [\n  features\n    { feature { key: \"ft\" value { float_list { value: [1.0, 2.0] } } } },\n  features\n    { feature []},\n  features\n    { feature { key: \"ft\" value { float_list { value: [3.0] } } }\n]\n</pre> <p>then the output will look like:</p> <pre class=\"\">{\"ft\": SparseTensor(indices=[[0, 0], [0, 1], [2, 0]],\n                    values=[1.0, 2.0, 3.0],\n                    shape=(3, 2)) }\n</pre> <p>Given two <code>Example</code> input protos in <code>serialized</code>:</p> <pre class=\"\">[\n  features {\n    feature { key: \"kw\" value { bytes_list { value: [ \"knit\", \"big\" ] } } }\n    feature { key: \"gps\" value { float_list { value: [] } } }\n  },\n  features {\n    feature { key: \"kw\" value { bytes_list { value: [ \"emmy\" ] } } }\n    feature { key: \"dank\" value { int64_list { value: [ 42 ] } } }\n    feature { key: \"gps\" value { } }\n  }\n]\n</pre> <p>And arguments</p> <pre class=\"lang-eiffel no-auto-prettify\">example_names: [\"input0\", \"input1\"],\nfeatures: {\n    \"kw\": VarLenFeature(tf.string),\n    \"dank\": VarLenFeature(tf.int64),\n    \"gps\": VarLenFeature(tf.float32),\n}\n</pre> <p>Then the output is a dictionary:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">{\n  \"kw\": SparseTensor(\n      indices=[[0, 0], [0, 1], [1, 0]],\n      values=[\"knit\", \"big\", \"emmy\"]\n      shape=[2, 2]),\n  \"dank\": SparseTensor(\n      indices=[[1, 0]],\n      values=[42],\n      shape=[2, 1]),\n  \"gps\": SparseTensor(\n      indices=[],\n      values=[],\n      shape=[2, 0]),\n}\n</pre> <p>For dense results in two serialized <code>Example</code>s:</p> <pre class=\"\">[\n  features {\n    feature { key: \"age\" value { int64_list { value: [ 0 ] } } }\n    feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n   },\n   features {\n    feature { key: \"age\" value { int64_list { value: [] } } }\n    feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n  }\n]\n</pre> <p>We can use arguments:</p> <pre class=\"lang-eiffel no-auto-prettify\">example_names: [\"input0\", \"input1\"],\nfeatures: {\n    \"age\": FixedLenFeature([], dtype=tf.int64, default_value=-1),\n    \"gender\": FixedLenFeature([], dtype=tf.string),\n}\n</pre> <p>And the expected output is:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">{\n  \"age\": [[0], [-1]],\n  \"gender\": [[\"f\"], [\"f\"]],\n}\n</pre>  <h5 id=\"args-54\">Args:</h5> <ul> <li>\n<code>serialized</code>: A vector (1-D Tensor) of strings, a batch of binary serialized <code>Example</code> protos.</li> <li>\n<code>features</code>: A <code>dict</code> mapping feature keys to <code>FixedLenFeature</code> or <code>VarLenFeature</code> values.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> <li>\n<code>example_names</code>: A vector (1-D Tensor) of strings (optional), the names of the serialized protos in the batch.</li> </ul>  <h5 id=\"returns-48\">Returns:</h5> <p>A <code>dict</code> mapping feature keys to <code>Tensor</code> and <code>SparseTensor</code> values.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if any feature is invalid.</li> </ul>  <h3 id=\"parse_single_example\"><code>tf.parse_single_example(serialized, features, name=None, example_names=None)</code></h3> <p>Parses a single <code>Example</code> proto.</p> <p>Similar to <code>parse_example</code>, except:</p> <p>For dense tensors, the returned <code>Tensor</code> is identical to the output of <code>parse_example</code>, except there is no batch dimension, the output shape is the same as the shape given in <code>dense_shape</code>.</p> <p>For <code>SparseTensor</code>s, the first (batch) column of the indices matrix is removed (the indices matrix is a column vector), the values vector is unchanged, and the first (<code>batch_size</code>) entry of the shape vector is removed (it is now a single element vector).</p>  <h5 id=\"args-55\">Args:</h5> <ul> <li>\n<code>serialized</code>: A scalar string Tensor, a single serialized Example. See <code>_parse_single_example_raw</code> documentation for more details.</li> <li>\n<code>features</code>: A <code>dict</code> mapping feature keys to <code>FixedLenFeature</code> or <code>VarLenFeature</code> values.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> <li>\n<code>example_names</code>: (Optional) A scalar string Tensor, the associated name. See <code>_parse_single_example_raw</code> documentation for more details.</li> </ul>  <h5 id=\"returns-49\">Returns:</h5> <p>A <code>dict</code> mapping feature keys to <code>Tensor</code> and <code>SparseTensor</code> values.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if any feature is invalid.</li> </ul>  <h3 id=\"decode_json_example\"><code>tf.decode_json_example(json_examples, name=None)</code></h3> <p>Convert JSON-encoded Example records to binary protocol buffer strings.</p> <p>This op translates a tensor containing Example records, encoded using the <a href=\"https://developers.google.com/protocol-buffers/docs/proto3#json\">standard JSON mapping</a>, into a tensor containing the same records encoded as binary protocol buffers. The resulting tensor can then be fed to any of the other Example-parsing ops.</p>  <h5 id=\"args-56\">Args:</h5> <ul> <li>\n<code>json_examples</code>: A <code>Tensor</code> of type <code>string</code>. Each string is a JSON object serialized according to the JSON mapping of the Example proto.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-50\">Returns:</h5> <p>A <code>Tensor</code> of type <code>string</code>. Each string is a binary Example protocol buffer corresponding to the respective element of <code>json_examples</code>.</p> <h2 id=\"queues\">Queues</h2> <p>TensorFlow provides several implementations of 'Queues', which are structures within the TensorFlow computation graph to stage pipelines of tensors together. The following describe the basic Queue interface and some implementations. To see an example use, see <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/threading_and_queues/index.html\">Threading and Queues</a>.</p>  <h3 id=\"QueueBase\"><code>class tf.QueueBase</code></h3> <p>Base class for queue implementations.</p> <p>A queue is a TensorFlow data structure that stores tensors across multiple steps, and exposes operations that enqueue and dequeue tensors.</p> <p>Each queue element is a tuple of one or more tensors, where each tuple component has a static dtype, and may have a static shape. The queue implementations support versions of enqueue and dequeue that handle single elements, versions that support enqueuing and dequeuing a batch of elements at once.</p> <p>See <a href=\"#FIFOQueue\"><code>tf.FIFOQueue</code></a> and <a href=\"#RandomShuffleQueue\"><code>tf.RandomShuffleQueue</code></a> for concrete implementations of this class, and instructions on how to create them.</p>  <h4 id=\"QueueBase.enqueue\"><code>tf.QueueBase.enqueue(vals, name=None)</code></h4> <p>Enqueues one element to this queue.</p> <p>If the queue is full when this operation executes, it will block until the element has been enqueued.</p> <p>At runtime, this operation may raise an error if the queue is <a href=\"#QueueBase.close\">closed</a> before or during its execution. If the queue is closed before this operation runs, <code>tf.errors.AbortedError</code> will be raised. If this operation is blocked, and either (i) the queue is closed by a close operation with <code>cancel_pending_enqueues=True</code>, or (ii) the session is <a href=\"client#Session.close\">closed</a>, <code>tf.errors.CancelledError</code> will be raised.</p>  <h5 id=\"args-57\">Args:</h5> <ul> <li>\n<code>vals</code>: A tensor, a list or tuple of tensors, or a dictionary containing the values to enqueue.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-51\">Returns:</h5> <p>The operation that enqueues a new tuple of tensors to the queue.</p>  <h4 id=\"QueueBase.enqueue_many\"><code>tf.QueueBase.enqueue_many(vals, name=None)</code></h4> <p>Enqueues zero or more elements to this queue.</p> <p>This operation slices each component tensor along the 0th dimension to make multiple queue elements. All of the tensors in <code>vals</code> must have the same size in the 0th dimension.</p> <p>If the queue is full when this operation executes, it will block until all of the elements have been enqueued.</p> <p>At runtime, this operation may raise an error if the queue is <a href=\"#QueueBase.close\">closed</a> before or during its execution. If the queue is closed before this operation runs, <code>tf.errors.AbortedError</code> will be raised. If this operation is blocked, and either (i) the queue is closed by a close operation with <code>cancel_pending_enqueues=True</code>, or (ii) the session is <a href=\"client#Session.close\">closed</a>, <code>tf.errors.CancelledError</code> will be raised.</p>  <h5 id=\"args-58\">Args:</h5> <ul> <li>\n<code>vals</code>: A tensor, a list or tuple of tensors, or a dictionary from which the queue elements are taken.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-52\">Returns:</h5> <p>The operation that enqueues a batch of tuples of tensors to the queue.</p>  <h4 id=\"QueueBase.dequeue\"><code>tf.QueueBase.dequeue(name=None)</code></h4> <p>Dequeues one element from this queue.</p> <p>If the queue is empty when this operation executes, it will block until there is an element to dequeue.</p> <p>At runtime, this operation may raise an error if the queue is <a href=\"#QueueBase.close\">closed</a> before or during its execution. If the queue is closed, the queue is empty, and there are no pending enqueue operations that can fulfil this request, <code>tf.errors.OutOfRangeError</code> will be raised. If the session is <a href=\"client#Session.close\">closed</a>, <code>tf.errors.CancelledError</code> will be raised.</p>  <h5 id=\"args-59\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-53\">Returns:</h5> <p>The tuple of tensors that was dequeued.</p>  <h4 id=\"QueueBase.dequeue_many\"><code>tf.QueueBase.dequeue_many(n, name=None)</code></h4> <p>Dequeues and concatenates <code>n</code> elements from this queue.</p> <p>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. All of the components in the dequeued tuple will have size <code>n</code> in the 0th dimension.</p> <p>If the queue is closed and there are less than <code>n</code> elements left, then an <code>OutOfRange</code> exception is raised.</p> <p>At runtime, this operation may raise an error if the queue is <a href=\"#QueueBase.close\">closed</a> before or during its execution. If the queue is closed, the queue contains fewer than <code>n</code> elements, and there are no pending enqueue operations that can fulfil this request, <code>tf.errors.OutOfRangeError</code> will be raised. If the session is <a href=\"client#Session.close\">closed</a>, <code>tf.errors.CancelledError</code> will be raised.</p>  <h5 id=\"args-60\">Args:</h5> <ul> <li>\n<code>n</code>: A scalar <code>Tensor</code> containing the number of elements to dequeue.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-54\">Returns:</h5> <p>The tuple of concatenated tensors that was dequeued.</p>  <h4 id=\"QueueBase.size\"><code>tf.QueueBase.size(name=None)</code></h4> <p>Compute the number of elements in this queue.</p>  <h5 id=\"args-61\">Args:</h5> <ul> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-55\">Returns:</h5> <p>A scalar tensor containing the number of elements in this queue.</p>  <h4 id=\"QueueBase.close\"><code>tf.QueueBase.close(cancel_pending_enqueues=False, name=None)</code></h4> <p>Closes this queue.</p> <p>This operation signals that no more elements will be enqueued in the given queue. Subsequent <code>enqueue</code> and <code>enqueue_many</code> operations will fail. Subsequent <code>dequeue</code> and <code>dequeue_many</code> operations will continue to succeed if sufficient elements remain in the queue. Subsequent <code>dequeue</code> and <code>dequeue_many</code> operations that would block will fail immediately.</p> <p>If <code>cancel_pending_enqueues</code> is <code>True</code>, all pending requests will also be cancelled.</p>  <h5 id=\"args-62\">Args:</h5> <ul> <li>\n<code>cancel_pending_enqueues</code>: (Optional.) A boolean, defaulting to <code>False</code> (described above).</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-56\">Returns:</h5> <p>The operation that closes the queue.</p>  <h4 id=\"other-methods\">Other Methods</h4>  <h4 id=\"QueueBase.__init__\"><code>tf.QueueBase.__init__(dtypes, shapes, names, queue_ref)</code></h4> <p>Constructs a queue object from a queue reference.</p> <p>The two optional lists, <code>shapes</code> and <code>names</code>, must be of the same length as <code>dtypes</code> if provided. The values at a given index <code>i</code> indicate the shape and name to use for the corresponding queue component in <code>dtypes</code>.</p>  <h5 id=\"args-63\">Args:</h5> <ul> <li>\n<code>dtypes</code>: A list of types. The length of dtypes must equal the number of tensors in each element.</li> <li>\n<code>shapes</code>: Constraints on the shapes of tensors in an element: A list of shape tuples or None. This list is the same length as dtypes. If the shape of any tensors in the element are constrained, all must be; shapes can be None if the shapes should not be constrained.</li> <li>\n<code>names</code>: Optional list of names. If provided, the <code>enqueue()</code> and <code>dequeue()</code> methods will use dictionaries with these names as keys. Must be None or a list or tuple of the same length as <code>dtypes</code>.</li> <li>\n<code>queue_ref</code>: The queue reference, i.e. the output of the queue op.</li> </ul>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If one of the arguments is invalid.</li> </ul>  <h4 id=\"QueueBase.dequeue_up_to\"><code>tf.QueueBase.dequeue_up_to(n, name=None)</code></h4> <p>Dequeues and concatenates <code>n</code> elements from this queue.</p> <p><strong>Note</strong> This operation is not supported by all queues. If a queue does not support DequeueUpTo, then a <code>tf.errors.UnimplementedError</code> is raised.</p> <p>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. If the queue has not been closed, all of the components in the dequeued tuple will have size <code>n</code> in the 0th dimension.</p> <p>If the queue is closed and there are more than <code>0</code> but fewer than <code>n</code> elements remaining, then instead of raising a <code>tf.errors.OutOfRangeError</code> like <a href=\"#QueueBase.dequeue_many\"><code>dequeue_many</code></a>, less than <code>n</code> elements are returned immediately. If the queue is closed and there are <code>0</code> elements left in the queue, then a <code>tf.errors.OutOfRangeError</code> is raised just like in <code>dequeue_many</code>. Otherwise the behavior is identical to <code>dequeue_many</code>.</p>  <h5 id=\"args-64\">Args:</h5> <ul> <li>\n<code>n</code>: A scalar <code>Tensor</code> containing the number of elements to dequeue.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-57\">Returns:</h5> <p>The tuple of concatenated tensors that was dequeued.</p>  <h4 id=\"QueueBase.dtypes\"><code>tf.QueueBase.dtypes</code></h4> <p>The list of dtypes for each component of a queue element.</p>  <h4 id=\"QueueBase.from_list\"><code>tf.QueueBase.from_list(index, queues)</code></h4> <p>Create a queue using the queue reference from <code>queues[index]</code>.</p>  <h5 id=\"args-65\">Args:</h5> <ul> <li>\n<code>index</code>: An integer scalar tensor that determines the input that gets selected.</li> <li>\n<code>queues</code>: A list of <code>QueueBase</code> objects.</li> </ul>  <h5 id=\"returns-58\">Returns:</h5> <p>A <code>QueueBase</code> object.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: When <code>queues</code> is not a list of <code>QueueBase</code> objects, or when the data types of <code>queues</code> are not all the same.</li> </ul>  <h4 id=\"QueueBase.name\"><code>tf.QueueBase.name</code></h4> <p>The name of the underlying queue.</p>  <h4 id=\"QueueBase.names\"><code>tf.QueueBase.names</code></h4> <p>The list of names for each component of a queue element.</p>  <h4 id=\"QueueBase.queue_ref\"><code>tf.QueueBase.queue_ref</code></h4> <p>The underlying queue reference.</p>  <h3 id=\"FIFOQueue\"><code>class tf.FIFOQueue</code></h3> <p>A queue implementation that dequeues elements in first-in-first out order.</p> <p>See <a href=\"#QueueBase\"><code>tf.QueueBase</code></a> for a description of the methods on this class.</p>  <h4 id=\"FIFOQueue.__init__\"><code>tf.FIFOQueue.__init__(capacity, dtypes, shapes=None, names=None, shared_name=None, name='fifo_queue')</code></h4> <p>Creates a queue that dequeues elements in a first-in first-out order.</p> <p>A <code>FIFOQueue</code> has bounded capacity; supports multiple concurrent producers and consumers; and provides exactly-once delivery.</p> <p>A <code>FIFOQueue</code> holds a list of up to <code>capacity</code> elements. Each element is a fixed-length tuple of tensors whose dtypes are described by <code>dtypes</code>, and whose shapes are optionally described by the <code>shapes</code> argument.</p> <p>If the <code>shapes</code> argument is specified, each component of a queue element must have the respective fixed shape. If it is unspecified, different queue elements may have different shapes, but the use of <code>dequeue_many</code> is disallowed.</p>  <h5 id=\"args-66\">Args:</h5> <ul> <li>\n<code>capacity</code>: An integer. The upper bound on the number of elements that may be stored in this queue.</li> <li>\n<code>dtypes</code>: A list of <code>DType</code> objects. The length of <code>dtypes</code> must equal the number of tensors in each queue element.</li> <li>\n<code>shapes</code>: (Optional.) A list of fully-defined <code>TensorShape</code> objects with the same length as <code>dtypes</code>, or <code>None</code>.</li> <li>\n<code>names</code>: (Optional.) A list of string naming the components in the queue with the same length as <code>dtypes</code>, or <code>None</code>. If specified the dequeue methods return a dictionary with the names as keys.</li> <li>\n<code>shared_name</code>: (Optional.) If non-empty, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: Optional name for the queue operation.</li> </ul>  <h3 id=\"PaddingFIFOQueue\"><code>class tf.PaddingFIFOQueue</code></h3> <p>A FIFOQueue that supports batching variable-sized tensors by padding.</p> <p>A <code>PaddingFIFOQueue</code> may contain components with dynamic shape, while also supporting <code>dequeue_many</code>. See the constructor for more details.</p> <p>See <a href=\"#QueueBase\"><code>tf.QueueBase</code></a> for a description of the methods on this class.</p>  <h4 id=\"PaddingFIFOQueue.__init__\"><code>tf.PaddingFIFOQueue.__init__(capacity, dtypes, shapes, names=None, shared_name=None, name='padding_fifo_queue')</code></h4> <p>Creates a queue that dequeues elements in a first-in first-out order.</p> <p>A <code>PaddingFIFOQueue</code> has bounded capacity; supports multiple concurrent producers and consumers; and provides exactly-once delivery.</p> <p>A <code>PaddingFIFOQueue</code> holds a list of up to <code>capacity</code> elements. Each element is a fixed-length tuple of tensors whose dtypes are described by <code>dtypes</code>, and whose shapes are described by the <code>shapes</code> argument.</p> <p>The <code>shapes</code> argument must be specified; each component of a queue element must have the respective shape. Shapes of fixed rank but variable size are allowed by setting any shape dimension to None. In this case, the inputs' shape may vary along the given dimension, and <code>dequeue_many</code> will pad the given dimension with zeros up to the maximum shape of all elements in the given batch.</p>  <h5 id=\"args-67\">Args:</h5> <ul> <li>\n<code>capacity</code>: An integer. The upper bound on the number of elements that may be stored in this queue.</li> <li>\n<code>dtypes</code>: A list of <code>DType</code> objects. The length of <code>dtypes</code> must equal the number of tensors in each queue element.</li> <li>\n<code>shapes</code>: A list of <code>TensorShape</code> objects, with the same length as <code>dtypes</code>. Any dimension in the <code>TensorShape</code> containing value <code>None</code> is dynamic and allows values to be enqueued with variable size in that dimension.</li> <li>\n<code>names</code>: (Optional.) A list of string naming the components in the queue with the same length as <code>dtypes</code>, or <code>None</code>. If specified the dequeue methods return a dictionary with the names as keys.</li> <li>\n<code>shared_name</code>: (Optional.) If non-empty, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: Optional name for the queue operation.</li> </ul>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If shapes is not a list of shapes, or the lengths of dtypes and shapes do not match, or if names is specified and the lengths of dtypes and names do not match.</li> </ul>  <h3 id=\"RandomShuffleQueue\"><code>class tf.RandomShuffleQueue</code></h3> <p>A queue implementation that dequeues elements in a random order.</p> <p>See <a href=\"#QueueBase\"><code>tf.QueueBase</code></a> for a description of the methods on this class.</p>  <h4 id=\"RandomShuffleQueue.__init__\"><code>tf.RandomShuffleQueue.__init__(capacity, min_after_dequeue, dtypes, shapes=None, names=None, seed=None, shared_name=None, name='random_shuffle_queue')</code></h4> <p>Create a queue that dequeues elements in a random order.</p> <p>A <code>RandomShuffleQueue</code> has bounded capacity; supports multiple concurrent producers and consumers; and provides exactly-once delivery.</p> <p>A <code>RandomShuffleQueue</code> holds a list of up to <code>capacity</code> elements. Each element is a fixed-length tuple of tensors whose dtypes are described by <code>dtypes</code>, and whose shapes are optionally described by the <code>shapes</code> argument.</p> <p>If the <code>shapes</code> argument is specified, each component of a queue element must have the respective fixed shape. If it is unspecified, different queue elements may have different shapes, but the use of <code>dequeue_many</code> is disallowed.</p> <p>The <code>min_after_dequeue</code> argument allows the caller to specify a minimum number of elements that will remain in the queue after a <code>dequeue</code> or <code>dequeue_many</code> operation completes, to ensure a minimum level of mixing of elements. This invariant is maintained by blocking those operations until sufficient elements have been enqueued. The <code>min_after_dequeue</code> argument is ignored after the queue has been closed.</p>  <h5 id=\"args-68\">Args:</h5> <ul> <li>\n<code>capacity</code>: An integer. The upper bound on the number of elements that may be stored in this queue.</li> <li>\n<code>min_after_dequeue</code>: An integer (described above).</li> <li>\n<code>dtypes</code>: A list of <code>DType</code> objects. The length of <code>dtypes</code> must equal the number of tensors in each queue element.</li> <li>\n<code>shapes</code>: (Optional.) A list of fully-defined <code>TensorShape</code> objects with the same length as <code>dtypes</code>, or <code>None</code>.</li> <li>\n<code>names</code>: (Optional.) A list of string naming the components in the queue with the same length as <code>dtypes</code>, or <code>None</code>. If specified the dequeue methods return a dictionary with the names as keys.</li> <li>\n<code>seed</code>: A Python integer. Used to create a random seed. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>shared_name</code>: (Optional.) If non-empty, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: Optional name for the queue operation.</li> </ul>  <h2 id=\"dealing-with-the-filesystem\">Dealing with the filesystem</h2>  <h3 id=\"matching_files\"><code>tf.matching_files(pattern, name=None)</code></h3> <p>Returns the set of files matching a pattern.</p> <p>Note that this routine only supports wildcard characters in the basename portion of the pattern, not in the directory portion.</p>  <h5 id=\"args-69\">Args:</h5> <ul> <li>\n<code>pattern</code>: A <code>Tensor</code> of type <code>string</code>. A (scalar) shell wildcard pattern.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-59\">Returns:</h5> <p>A <code>Tensor</code> of type <code>string</code>. A vector of matching filenames.</p>  <h3 id=\"read_file\"><code>tf.read_file(filename, name=None)</code></h3> <p>Reads and outputs the entire contents of the input filename.</p>  <h5 id=\"args-70\">Args:</h5> <ul> <li>\n<code>filename</code>: A <code>Tensor</code> of type <code>string</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-60\">Returns:</h5> <p>A <code>Tensor</code> of type <code>string</code>.</p>  <h2 id=\"input-pipeline\">Input pipeline</h2> <p>TensorFlow functions for setting up an input-prefetching pipeline. Please see the <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html\">reading data how-to</a> for context.</p>  <h3 id=\"beginning-of-an-input-pipeline\">Beginning of an input pipeline</h3> <p>The \"producer\" functions add a queue to the graph and a corresponding <code>QueueRunner</code> for running the subgraph that fills that queue.</p>  <h3 id=\"match_filenames_once\"><code>tf.train.match_filenames_once(pattern, name=None)</code></h3> <p>Save the list of files matching pattern, so it is only computed once.</p>  <h5 id=\"args-71\">Args:</h5> <ul> <li>\n<code>pattern</code>: A file pattern (glob).</li> <li>\n<code>name</code>: A name for the operations (optional).</li> </ul>  <h5 id=\"returns-61\">Returns:</h5> <p>A variable that is initialized to the list of files matching pattern.</p>  <h3 id=\"limit_epochs\"><code>tf.train.limit_epochs(tensor, num_epochs=None, name=None)</code></h3> <p>Returns tensor <code>num_epochs</code> times and then raises an <code>OutOfRange</code> error.</p>  <h5 id=\"args-72\">Args:</h5> <ul> <li>\n<code>tensor</code>: Any <code>Tensor</code>.</li> <li>\n<code>num_epochs</code>: A positive integer (optional). If specified, limits the number of steps the output tensor may be evaluated.</li> <li>\n<code>name</code>: A name for the operations (optional).</li> </ul>  <h5 id=\"returns-62\">Returns:</h5> <p>tensor or <code>OutOfRange</code>.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>num_epochs</code> is invalid.</li> </ul>  <h3 id=\"input_producer\"><code>tf.train.input_producer(input_tensor, element_shape=None, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, summary_name=None, name=None)</code></h3> <p>Output the rows of <code>input_tensor</code> to a queue for an input pipeline.</p>  <h5 id=\"args-73\">Args:</h5> <ul> <li>\n<code>input_tensor</code>: A tensor with the rows to produce. Must be at least one-dimensional. Must either have a fully-defined shape, or <code>element_shape</code> must be defined.</li> <li>\n<code>element_shape</code>: (Optional.) A <code>TensorShape</code> representing the shape of a row of <code>input_tensor</code>, if it cannot be inferred.</li> <li>\n<code>num_epochs</code>: (Optional.) An integer. If specified <code>input_producer</code> produces each row of <code>input_tensor</code> <code>num_epochs</code> times before generating an <code>OutOfRange</code> error. If not specified, <code>input_producer</code> can cycle through the rows of <code>input_tensor</code> an unlimited number of times.</li> <li>\n<code>shuffle</code>: (Optional.) A boolean. If true, the rows are randomly shuffled within each eopch.</li> <li>\n<code>seed</code>: (Optional.) An integer. The seed to use if <code>shuffle</code> is true.</li> <li>\n<code>capacity</code>: (Optional.) The capacity of the queue to be used for buffering the input.</li> <li>\n<code>shared_name</code>: (Optional.) If set, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>summary_name</code>: (Optional.) If set, a scalar summary for the current queue size will be generated, using this name as part of the tag.</li> <li>\n<code>name</code>: (Optional.) A name for queue.</li> </ul>  <h5 id=\"returns-63\">Returns:</h5> <p>A queue with the output rows. A <code>QueueRunner</code> for the queue is added to the current <code>QUEUE_RUNNER</code> collection of the current graph.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of the input cannot be inferred from the arguments.</li> </ul>  <h3 id=\"range_input_producer\"><code>tf.train.range_input_producer(limit, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code></h3> <p>Produces the integers from 0 to limit-1 in a queue.</p>  <h5 id=\"args-74\">Args:</h5> <ul> <li>\n<code>limit</code>: An int32 scalar tensor.</li> <li>\n<code>num_epochs</code>: An integer (optional). If specified, <code>range_input_producer</code> produces each integer <code>num_epochs</code> times before generating an OutOfRange error. If not specified, <code>range_input_producer</code> can cycle through the integers an unlimited number of times.</li> <li>\n<code>shuffle</code>: Boolean. If true, the integers are randomly shuffled within each epoch.</li> <li>\n<code>seed</code>: An integer (optional). Seed used if shuffle == True.</li> <li>\n<code>capacity</code>: An integer. Sets the queue capacity.</li> <li>\n<code>shared_name</code>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: A name for the operations (optional).</li> </ul>  <h5 id=\"returns-64\">Returns:</h5> <p>A Queue with the output integers. A <code>QueueRunner</code> for the Queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p>  <h3 id=\"slice_input_producer\"><code>tf.train.slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code></h3> <p>Produces a slice of each <code>Tensor</code> in <code>tensor_list</code>.</p> <p>Implemented using a Queue -- a <code>QueueRunner</code> for the Queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p>  <h5 id=\"args-75\">Args:</h5> <ul> <li>\n<code>tensor_list</code>: A list of <code>Tensor</code> objects. Every <code>Tensor</code> in <code>tensor_list</code> must have the same size in the first dimension.</li> <li>\n<code>num_epochs</code>: An integer (optional). If specified, <code>slice_input_producer</code> produces each slice <code>num_epochs</code> times before generating an <code>OutOfRange</code> error. If not specified, <code>slice_input_producer</code> can cycle through the slices an unlimited number of times.</li> <li>\n<code>shuffle</code>: Boolean. If true, the integers are randomly shuffled within each epoch.</li> <li>\n<code>seed</code>: An integer (optional). Seed used if shuffle == True.</li> <li>\n<code>capacity</code>: An integer. Sets the queue capacity.</li> <li>\n<code>shared_name</code>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: A name for the operations (optional).</li> </ul>  <h5 id=\"returns-65\">Returns:</h5> <p>A list of tensors, one for each element of <code>tensor_list</code>. If the tensor in <code>tensor_list</code> has shape <code>[N, a, b, .., z]</code>, then the corresponding output tensor will have shape <code>[a, b, ..., z]</code>.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>slice_input_producer</code> produces nothing from <code>tensor_list</code>.</li> </ul>  <h3 id=\"string_input_producer\"><code>tf.train.string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code></h3> <p>Output strings (e.g. filenames) to a queue for an input pipeline.</p>  <h5 id=\"args-76\">Args:</h5> <ul> <li>\n<code>string_tensor</code>: A 1-D string tensor with the strings to produce.</li> <li>\n<code>num_epochs</code>: An integer (optional). If specified, <code>string_input_producer</code> produces each string from <code>string_tensor</code> <code>num_epochs</code> times before generating an <code>OutOfRange</code> error. If not specified, <code>string_input_producer</code> can cycle through the strings in <code>string_tensor</code> an unlimited number of times.</li> <li>\n<code>shuffle</code>: Boolean. If true, the strings are randomly shuffled within each epoch.</li> <li>\n<code>seed</code>: An integer (optional). Seed used if shuffle == True.</li> <li>\n<code>capacity</code>: An integer. Sets the queue capacity.</li> <li>\n<code>shared_name</code>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: A name for the operations (optional).</li> </ul>  <h5 id=\"returns-66\">Returns:</h5> <p>A queue with the output strings. A <code>QueueRunner</code> for the Queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the string_tensor is a null Python list. At runtime, will fail with an assertion if string_tensor becomes a null tensor.</li> </ul>  <h3 id=\"batching-at-the-end-of-an-input-pipeline\">Batching at the end of an input pipeline</h3> <p>These functions add a queue to the graph to assemble a batch of examples, with possible shuffling. They also add a <code>QueueRunner</code> for running the subgraph that fills that queue.</p> <p>Use <a href=\"#batch\"><code>batch</code></a> or <a href=\"#batch_join\"><code>batch_join</code></a> for batching examples that have already been well shuffled. Use <a href=\"#shuffle_batch\"><code>shuffle_batch</code></a> or <a href=\"#shuffle_batch_join\"><code>shuffle_batch_join</code></a> for examples that would benefit from additional shuffling.</p> <p>Use <a href=\"#batch\"><code>batch</code></a> or <a href=\"#shuffle_batch\"><code>shuffle_batch</code></a> if you want a single thread producing examples to batch, or if you have a single subgraph producing examples but you want to run it in <em>N</em> threads (where you increase <em>N</em> until it can keep the queue full). Use <a href=\"#batch_join\"><code>batch_join</code></a> or <a href=\"#shuffle_batch_join\"><code>shuffle_batch_join</code></a> if you have <em>N</em> different subgraphs producing examples to batch and you want them run by <em>N</em> threads.</p>  <h3 id=\"batch\"><code>tf.train.batch(tensors, batch_size, num_threads=1, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3> <p>Creates batches of tensors in <code>tensors</code>.</p> <p>The argument <code>tensors</code> can be a list or a dictionary of tensors. The value returned by the function will be of the same type as <code>tensors</code>.</p> <p>This function is implemented using a queue. A <code>QueueRunner</code> for the queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p> <p>If <code>enqueue_many</code> is <code>False</code>, <code>tensors</code> is assumed to represent a single example. An input tensor with shape <code>[x, y, z]</code> will be output as a tensor with shape <code>[batch_size, x, y, z]</code>.</p> <p>If <code>enqueue_many</code> is <code>True</code>, <code>tensors</code> is assumed to represent a batch of examples, where the first dimension is indexed by example, and all members of <code>tensor_list</code> should have the same size in the first dimension. If an input tensor has shape <code>[*, x, y, z]</code>, the output will have shape <code>[batch_size, x,\ny, z]</code>. The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p> <p>The returned operation is a dequeue operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</p> <p><em>N.B.:</em> If <code>dynamic_pad</code> is <code>False</code>, you must ensure that either (i) the <code>shapes</code> argument is passed, or (ii) all of the tensors in <code>tensors</code> must have fully-defined shapes. <code>ValueError</code> will be raised if neither of these conditions holds.</p> <p>If <code>dynamic_pad</code> is <code>True</code>, it is sufficient that the <em>rank</em> of the tensors is known, but individual dimensions may have shape <code>None</code>. In this case, for each enqueue the dimensions with value <code>None</code> may have a variable length; upon dequeue, the output tensors will be padded on the right to the maximum shape of the tensors in the current minibatch. For numbers, this padding takes value 0. For strings, this padding is the empty string. See <code>PaddingFIFOQueue</code> for more info.</p> <p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded. In addition, all output tensors' static shapes, as accessed via the <code>get_shape</code> method will have a first <code>Dimension</code> value of <code>None</code>, and operations that depend on fixed batch_size would fail.</p>  <h5 id=\"args-77\">Args:</h5> <ul> <li>\n<code>tensors</code>: The list or dictionary of tensors to enqueue.</li> <li>\n<code>batch_size</code>: The new batch size pulled from the queue.</li> <li>\n<code>num_threads</code>: The number of threads enqueuing <code>tensor_list</code>.</li> <li>\n<code>capacity</code>: An integer. The maximum number of elements in the queue.</li> <li>\n<code>enqueue_many</code>: Whether each tensor in <code>tensor_list</code> is a single example.</li> <li>\n<code>shapes</code>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensor_list</code>.</li> <li>\n<code>dynamic_pad</code>: Boolean. Allow variable dimensions in input shapes. The given dimensions are padded upon dequeue so that tensors within a batch have the same shapes.</li> <li>\n<code>allow_smaller_final_batch</code>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li> <li>\n<code>shared_name</code>: (Optional). If set, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: (Optional) A name for the operations.</li> </ul>  <h5 id=\"returns-67\">Returns:</h5> <p>A list or dictionary of tensors with the same types as <code>tensors</code>.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors</code>.</li> </ul>  <h3 id=\"batch_join\"><code>tf.train.batch_join(tensors_list, batch_size, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3> <p>Runs a list of tensors to fill a queue to create batches of examples.</p> <p>The <code>tensors_list</code> argument is a list of tuples of tensors, or a list of dictionaries of tensors. Each element in the list is treated similarily to the <code>tensors</code> argument of <code>tf.train.batch()</code>.</p> <p>Enqueues a different list of tensors in different threads. Implemented using a queue -- a <code>QueueRunner</code> for the queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p> <p><code>len(tensors_list)</code> threads will be started, with thread <code>i</code> enqueuing the tensors from <code>tensors_list[i]</code>. <code>tensors_list[i1][j]</code> must match <code>tensors_list[i2][j]</code> in type and shape, except in the first dimension if <code>enqueue_many</code> is true.</p> <p>If <code>enqueue_many</code> is <code>False</code>, each <code>tensors_list[i]</code> is assumed to represent a single example. An input tensor <code>x</code> will be output as a tensor with shape <code>[batch_size] + x.shape</code>.</p> <p>If <code>enqueue_many</code> is <code>True</code>, <code>tensors_list[i]</code> is assumed to represent a batch of examples, where the first dimension is indexed by example, and all members of <code>tensors_list[i]</code> should have the same size in the first dimension. The slices of any input tensor <code>x</code> are treated as examples, and the output tensors will have shape <code>[batch_size] + x.shape[1:]</code>.</p> <p>The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p> <p>The returned operation is a dequeue operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</p> <p><em>N.B.:</em> If <code>dynamic_pad</code> is <code>False</code>, you must ensure that either (i) the <code>shapes</code> argument is passed, or (ii) all of the tensors in <code>tensors_list</code> must have fully-defined shapes. <code>ValueError</code> will be raised if neither of these conditions holds.</p> <p>If <code>dynamic_pad</code> is <code>True</code>, it is sufficient that the <em>rank</em> of the tensors is known, but individual dimensions may have value <code>None</code>. In this case, for each enqueue the dimensions with value <code>None</code> may have a variable length; upon dequeue, the output tensors will be padded on the right to the maximum shape of the tensors in the current minibatch. For numbers, this padding takes value 0. For strings, this padding is the empty string. See <code>PaddingFIFOQueue</code> for more info.</p> <p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded. In addition, all output tensors' static shapes, as accessed via the <code>get_shape</code> method will have a first <code>Dimension</code> value of <code>None</code>, and operations that depend on fixed batch_size would fail.</p>  <h5 id=\"args-78\">Args:</h5> <ul> <li>\n<code>tensors_list</code>: A list of tuples or dictionaries of tensors to enqueue.</li> <li>\n<code>batch_size</code>: An integer. The new batch size pulled from the queue.</li> <li>\n<code>capacity</code>: An integer. The maximum number of elements in the queue.</li> <li>\n<code>enqueue_many</code>: Whether each tensor in <code>tensor_list_list</code> is a single example.</li> <li>\n<code>shapes</code>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensor_list_list[i]</code>.</li> <li>\n<code>dynamic_pad</code>: Boolean. Allow variable dimensions in input shapes. The given dimensions are padded upon dequeue so that tensors within a batch have the same shapes.</li> <li>\n<code>allow_smaller_final_batch</code>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li> <li>\n<code>shared_name</code>: (Optional) If set, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: (Optional) A name for the operations.</li> </ul>  <h5 id=\"returns-68\">Returns:</h5> <p>A list or dictionary of tensors with the same number and types as <code>tensors_list[i]</code>.</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensor_list_list</code>.</li> </ul>  <h3 id=\"shuffle_batch\"><code>tf.train.shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3> <p>Creates batches by randomly shuffling tensors.</p> <p>This function adds the following to the current <code>Graph</code>:</p> <ul> <li>A shuffling queue into which tensors from <code>tensors</code> are enqueued.</li> <li>A <code>dequeue_many</code> operation to create batches from the queue.</li> <li>A <code>QueueRunner</code> to <code>QUEUE_RUNNER</code> collection, to enqueue the tensors from <code>tensors</code>.</li> </ul> <p>If <code>enqueue_many</code> is <code>False</code>, <code>tensors</code> is assumed to represent a single example. An input tensor with shape <code>[x, y, z]</code> will be output as a tensor with shape <code>[batch_size, x, y, z]</code>.</p> <p>If <code>enqueue_many</code> is <code>True</code>, <code>tensors</code> is assumed to represent a batch of examples, where the first dimension is indexed by example, and all members of <code>tensors</code> should have the same size in the first dimension. If an input tensor has shape <code>[*, x, y, z]</code>, the output will have shape <code>[batch_size, x, y, z]</code>.</p> <p>The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p> <p>The returned operation is a dequeue operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Creates batches of 32 images and 32 labels.\nimage_batch, label_batch = tf.train.shuffle_batch(\n      [single_image, single_label],\n      batch_size=32,\n      num_threads=4,\n      capacity=50000,\n      min_after_dequeue=10000)\n</pre> <p><em>N.B.:</em> You must ensure that either (i) the <code>shapes</code> argument is passed, or (ii) all of the tensors in <code>tensors</code> must have fully-defined shapes. <code>ValueError</code> will be raised if neither of these conditions holds.</p> <p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded. In addition, all output tensors' static shapes, as accessed via the <code>get_shape</code> method will have a first <code>Dimension</code> value of <code>None</code>, and operations that depend on fixed batch_size would fail.</p>  <h5 id=\"args-79\">Args:</h5> <ul> <li>\n<code>tensors</code>: The list or dictionary of tensors to enqueue.</li> <li>\n<code>batch_size</code>: The new batch size pulled from the queue.</li> <li>\n<code>capacity</code>: An integer. The maximum number of elements in the queue.</li> <li>\n<code>min_after_dequeue</code>: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.</li> <li>\n<code>num_threads</code>: The number of threads enqueuing <code>tensor_list</code>.</li> <li>\n<code>seed</code>: Seed for the random shuffling within the queue.</li> <li>\n<code>enqueue_many</code>: Whether each tensor in <code>tensor_list</code> is a single example.</li> <li>\n<code>shapes</code>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensor_list</code>.</li> <li>\n<code>allow_smaller_final_batch</code>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li> <li>\n<code>shared_name</code>: (Optional) If set, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: (Optional) A name for the operations.</li> </ul>  <h5 id=\"returns-69\">Returns:</h5> <p>A list or dictionary of tensors with the types as <code>tensors</code>.</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors</code>.</li> </ul>  <h3 id=\"shuffle_batch_join\"><code>tf.train.shuffle_batch_join(tensors_list, batch_size, capacity, min_after_dequeue, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3> <p>Create batches by randomly shuffling tensors.</p> <p>The <code>tensors_list</code> argument is a list of tuples of tensors, or a list of dictionaries of tensors. Each element in the list is treated similarily to the <code>tensors</code> argument of <code>tf.train.shuffle_batch()</code>.</p> <p>This version enqueues a different list of tensors in different threads. It adds the following to the current <code>Graph</code>:</p> <ul> <li>A shuffling queue into which tensors from <code>tensors_list</code> are enqueued.</li> <li>A <code>dequeue_many</code> operation to create batches from the queue.</li> <li>A <code>QueueRunner</code> to <code>QUEUE_RUNNER</code> collection, to enqueue the tensors from <code>tensors_list</code>.</li> </ul> <p><code>len(tensors_list)</code> threads will be started, with thread <code>i</code> enqueuing the tensors from <code>tensors_list[i]</code>. <code>tensors_list[i1][j]</code> must match <code>tensors_list[i2][j]</code> in type and shape, except in the first dimension if <code>enqueue_many</code> is true.</p> <p>If <code>enqueue_many</code> is <code>False</code>, each <code>tensors_list[i]</code> is assumed to represent a single example. An input tensor with shape <code>[x, y, z]</code> will be output as a tensor with shape <code>[batch_size, x, y, z]</code>.</p> <p>If <code>enqueue_many</code> is <code>True</code>, <code>tensors_list[i]</code> is assumed to represent a batch of examples, where the first dimension is indexed by example, and all members of <code>tensors_list[i]</code> should have the same size in the first dimension. If an input tensor has shape <code>[*, x,\ny, z]</code>, the output will have shape <code>[batch_size, x, y, z]</code>.</p> <p>The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p> <p>The returned operation is a dequeue operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</p> <p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded. In addition, all output tensors' static shapes, as accessed via the <code>get_shape</code> method will have a first <code>Dimension</code> value of <code>None</code>, and operations that depend on fixed batch_size would fail.</p>  <h5 id=\"args-80\">Args:</h5> <ul> <li>\n<code>tensors_list</code>: A list of tuples or dictionaries of tensors to enqueue.</li> <li>\n<code>batch_size</code>: An integer. The new batch size pulled from the queue.</li> <li>\n<code>capacity</code>: An integer. The maximum number of elements in the queue.</li> <li>\n<code>min_after_dequeue</code>: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.</li> <li>\n<code>seed</code>: Seed for the random shuffling within the queue.</li> <li>\n<code>enqueue_many</code>: Whether each tensor in <code>tensor_list_list</code> is a single example.</li> <li>\n<code>shapes</code>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensors_list[i]</code>.</li> <li>\n<code>allow_smaller_final_batch</code>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li> <li>\n<code>shared_name</code>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li> <li>\n<code>name</code>: (Optional) A name for the operations.</li> </ul>  <h5 id=\"returns-70\">Returns:</h5> <p>A list or dictionary of tensors with the same number and types as <code>tensors_list[i]</code>.</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors_list</code>.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html</a>\n  </p>\n</div>\n","test":"<h1 id=\"testing\">Testing</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#testing\">Testing</a></li> <ul> <li><a href=\"#unit-tests\">Unit tests</a></li> <ul> <li><a href=\"#main\"><code>tf.test.main()</code></a></li> </ul> <li><a href=\"#utilities\">Utilities</a></li> <ul> <li><a href=\"#assert_equal_graph_def\"><code>tf.test.assert_equal_graph_def(actual, expected)</code></a></li> <li><a href=\"#get_temp_dir\"><code>tf.test.get_temp_dir()</code></a></li> <li><a href=\"#is_built_with_cuda\"><code>tf.test.is_built_with_cuda()</code></a></li> </ul> <li><a href=\"#gradient-checking\">Gradient checking</a></li> <ul> <li><a href=\"#compute_gradient\"><code>tf.test.compute_gradient(x, x_shape, y, y_shape, x_init_value=None, delta=0.001, init_targets=None)</code></a></li> <li><a href=\"#compute_gradient_error\"><code>tf.test.compute_gradient_error(x, x_shape, y, y_shape, x_init_value=None, delta=0.001, init_targets=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"unit-tests\">Unit tests</h2> <p>TensorFlow provides a convenience class inheriting from <code>unittest.TestCase</code> which adds methods relevant to TensorFlow tests. Here is an example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">import tensorflow as tf\n\n\nclass SquareTest(tf.test.TestCase):\n\n  def testSquare(self):\n    with self.test_session():\n      x = tf.square([2, 3])\n      self.assertAllEqual(x.eval(), [4, 9])\n\n\nif __name__ == '__main__':\n  tf.test.main()\n</pre> <p><code>tf.test.TestCase</code> inherits from <code>unittest.TestCase</code> but adds a few additional methods. We will document these methods soon.</p>  <h3 id=\"main\"><code>tf.test.main()</code></h3> <p>Runs all unit tests.</p> <h2 id=\"utilities\">Utilities</h2>  <h3 id=\"assert_equal_graph_def\"><code>tf.test.assert_equal_graph_def(actual, expected)</code></h3> <p>Asserts that two <code>GraphDef</code>s are (mostly) the same.</p> <p>Compares two <code>GraphDef</code> protos for equality, ignoring versions and ordering of nodes, attrs, and control inputs. Node names are used to match up nodes between the graphs, so the naming of nodes must be consistent.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>actual</code>: The <code>GraphDef</code> we have.</li> <li>\n<code>expected</code>: The <code>GraphDef</code> we expected.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>AssertionError</code>: If the <code>GraphDef</code>s do not match.</li> <li>\n<code>TypeError</code>: If either argument is not a <code>GraphDef</code>.</li> </ul>  <h3 id=\"get_temp_dir\"><code>tf.test.get_temp_dir()</code></h3> <p>Returns a temporary directory for use during tests.</p> <p>There is no need to delete the directory after the test.</p> <h5 id=\"returns\">Returns:</h5> <p>The temporary directory.</p>  <h3 id=\"is_built_with_cuda\"><code>tf.test.is_built_with_cuda()</code></h3> <p>Returns whether TensorFlow was built with CUDA (GPU) support.</p>  <h2 id=\"gradient-checking\">Gradient checking</h2> <p><a href=\"#compute_gradient\"><code>compute_gradient</code></a> and <a href=\"#compute_gradient_error\"><code>compute_gradient_error</code></a> perform numerical differentiation of graphs for comparison against registered analytic gradients.</p>  <h3 id=\"compute_gradient\"><code>tf.test.compute_gradient(x, x_shape, y, y_shape, x_init_value=None, delta=0.001, init_targets=None)</code></h3> <p>Computes and returns the theoretical and numerical Jacobian.</p> <p>If <code>x</code> or <code>y</code> is complex, the Jacobian will still be real but the corresponding Jacobian dimension(s) will be twice as large. This is required even if both input and output is complex since TensorFlow graphs are not necessarily holomorphic, and may have gradients not expressible as complex numbers. For example, if <code>x</code> is complex with shape <code>[m]</code> and <code>y</code> is complex with shape <code>[n]</code>, each Jacobian <code>J</code> will have shape <code>[m * 2, n * 2]</code> with</p> <pre class=\"\">J[:m, :n] = d(Re y)/d(Re x)\nJ[:m, n:] = d(Im y)/d(Re x)\nJ[m:, :n] = d(Re y)/d(Im x)\nJ[m:, n:] = d(Im y)/d(Im x)\n</pre>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>x</code>: a tensor or list of tensors</li> <li><p><code>x_shape</code>: the dimensions of x as a tuple or an array of ints. If x is a list, then this is the list of shapes.</p></li> <li><p><code>y</code>: a tensor</p></li> <li><p><code>y_shape</code>: the dimensions of y as a tuple or an array of ints.</p></li> <li><p><code>x_init_value</code>: (optional) a numpy array of the same shape as \"x\" representing the initial value of x. If x is a list, this should be a list of numpy arrays. If this is none, the function will pick a random tensor as the initial value.</p></li> <li><p><code>delta</code>: (optional) the amount of perturbation.</p></li> <li><p><code>init_targets</code>: list of targets to run to initialize model params. TODO(<a target=\"_blank\" href=\"https://teams.googleplex.com/mrry\" class=\"di-hover\" data-ldap=\"mrry\">mrry</a>): remove this argument.</p></li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>Two 2-d numpy arrays representing the theoretical and numerical Jacobian for dy/dx. Each has \"x_size\" rows and \"y_size\" columns where \"x_size\" is the number of elements in x and \"y_size\" is the number of elements in y. If x is a list, returns a list of two numpy arrays.</p>  <h3 id=\"compute_gradient_error\"><code>tf.test.compute_gradient_error(x, x_shape, y, y_shape, x_init_value=None, delta=0.001, init_targets=None)</code></h3> <p>Computes the gradient error.</p> <p>Computes the maximum error for dy/dx between the computed Jacobian and the numerically estimated Jacobian.</p> <p>This function will modify the tensors passed in as it adds more operations and hence changing the consumers of the operations of the input tensors.</p> <p>This function adds operations to the current session. To compute the error using a particular device, such as a GPU, use the standard methods for setting a device (e.g. using with sess.graph.device() or setting a device function in the session constructor).</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>x</code>: a tensor or list of tensors</li> <li><p><code>x_shape</code>: the dimensions of x as a tuple or an array of ints. If x is a list, then this is the list of shapes.</p></li> <li><p><code>y</code>: a tensor</p></li> <li><p><code>y_shape</code>: the dimensions of y as a tuple or an array of ints.</p></li> <li><p><code>x_init_value</code>: (optional) a numpy array of the same shape as \"x\" representing the initial value of x. If x is a list, this should be a list of numpy arrays. If this is none, the function will pick a random tensor as the initial value.</p></li> <li><p><code>delta</code>: (optional) the amount of perturbation.</p></li> <li><p><code>init_targets</code>: list of targets to run to initialize model params. TODO(<a target=\"_blank\" href=\"https://teams.googleplex.com/mrry\" class=\"di-hover\" data-ldap=\"mrry\">mrry</a>): Remove this argument.</p></li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>The maximum error in between the two Jacobians.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/test.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/test.html</a>\n  </p>\n</div>\n","contrib.bayesflow.stochastic_graph":"<h1 id=\"bayesflow-stochastic-graph-contrib\">BayesFlow Stochastic Graph (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#bayesflow-stochastic-graph-contrib\">BayesFlow Stochastic Graph (contrib)</a></li> <ul> <li><a href=\"#stochastic-computation-graph-classes\">Stochastic Computation Graph Classes</a></li> <ul> <li><a href=\"#StochasticTensor\"><code>class tf.contrib.bayesflow.stochastic_graph.StochasticTensor</code></a></li> <li><a href=\"#DistributionTensor\"><code>class tf.contrib.bayesflow.stochastic_graph.DistributionTensor</code></a></li> </ul> <li><a href=\"#stochastic-computation-value-types\">Stochastic Computation Value Types</a></li> <ul> <li><a href=\"#MeanValue\"><code>class tf.contrib.bayesflow.stochastic_graph.MeanValue</code></a></li> <li><a href=\"#SampleValue\"><code>class tf.contrib.bayesflow.stochastic_graph.SampleValue</code></a></li> <li><a href=\"#SampleAndReshapeValue\"><code>class tf.contrib.bayesflow.stochastic_graph.SampleAndReshapeValue</code></a></li> <li><a href=\"#value_type\"><code>tf.contrib.bayesflow.stochastic_graph.value_type(dist_value_type)</code></a></li> <li><a href=\"#get_current_value_type\"><code>tf.contrib.bayesflow.stochastic_graph.get_current_value_type()</code></a></li> </ul> <li><a href=\"#stochastic-computation-surrogate-loss-functions\">Stochastic Computation Surrogate Loss Functions</a></li> <ul> <li><a href=\"#score_function\"><code>tf.contrib.bayesflow.stochastic_graph.score_function(dist_tensor, value, losses)</code></a></li> <li><a href=\"#get_score_function_with_baseline\"><code>tf.contrib.bayesflow.stochastic_graph.get_score_function_with_baseline(baseline)</code></a></li> </ul> <li><a href=\"#stochastic-computation-graph-helper-functions\">Stochastic Computation Graph Helper Functions</a></li> <ul> <li><a href=\"#surrogate_loss\"><code>tf.contrib.bayesflow.stochastic_graph.surrogate_loss(sample_losses, stochastic_tensors=None, name=SurrogateLoss)</code></a></li> </ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#NoValueTypeSetError\"><code>class tf.contrib.bayesflow.stochastic_graph.NoValueTypeSetError</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Classes and helper functions for Stochastic Computation Graphs.</p>  <h2 id=\"stochastic-computation-graph-classes\">Stochastic Computation Graph Classes</h2>  <h3 id=\"StochasticTensor\"><code>class tf.contrib.bayesflow.stochastic_graph.StochasticTensor</code></h3> <p>Base Class for Tensor-like objects that emit stochastic values.</p>  <h4 id=\"StochasticTensor.__init__\"><code>tf.contrib.bayesflow.stochastic_graph.StochasticTensor.__init__()</code></h4>  <h4 id=\"StochasticTensor.dtype\"><code>tf.contrib.bayesflow.stochastic_graph.StochasticTensor.dtype</code></h4>  <h4 id=\"StochasticTensor.graph\"><code>tf.contrib.bayesflow.stochastic_graph.StochasticTensor.graph</code></h4>  <h4 id=\"StochasticTensor.input_dict\"><code>tf.contrib.bayesflow.stochastic_graph.StochasticTensor.input_dict</code></h4>  <h4 id=\"StochasticTensor.loss\"><code>tf.contrib.bayesflow.stochastic_graph.StochasticTensor.loss(sample_losses)</code></h4> <p>Returns the term to add to the surrogate loss.</p> <p>This method is called by <code>surrogate_loss</code>. The input <code>sample_losses</code> should have already had <code>stop_gradient</code> applied to them. This is because the surrogate_loss usually provides a monte carlo sample term of the form <code>differentiable_surrogate * sum(sample_losses)</code> where <code>sample_losses</code> is considered constant with respect to the input for purposes of the gradient.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>sample_losses</code>: a list of Tensors, the sample losses downstream of this <code>StochasticTensor</code>.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>Either <code>None</code> or a <code>Tensor</code>.</p>  <h4 id=\"StochasticTensor.name\"><code>tf.contrib.bayesflow.stochastic_graph.StochasticTensor.name</code></h4>  <h4 id=\"StochasticTensor.value\"><code>tf.contrib.bayesflow.stochastic_graph.StochasticTensor.value(name=None)</code></h4>  <h3 id=\"DistributionTensor\"><code>class tf.contrib.bayesflow.stochastic_graph.DistributionTensor</code></h3> <p>DistributionTensor is a StochasticTensor backed by a distribution.</p>  <h4 id=\"DistributionTensor.__init__\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.__init__(dist_cls, name=None, dist_value_type=None, loss_fn=score_function, **dist_args)</code></h4> <p>Construct a <code>DistributionTensor</code>.</p> <p><code>DistributionTensor</code> will instantiate a distribution from <code>dist_cls</code> and <code>dist_args</code> and its <code>value</code> method will return the same value each time it is called. What <code>value</code> is returned is controlled by the <code>dist_value_type</code> (defaults to <code>SampleAndReshapeValue</code>).</p> <p>Some distributions' sample functions are not differentiable (e.g. a sample from a discrete distribution like a Bernoulli) and so to differentiate wrt parameters upstream of the sample requires a gradient estimator like the score function estimator. This is accomplished by passing a differentiable <code>loss_fn</code> to the <code>DistributionTensor</code>, which defaults to a function whose derivative is the score function estimator. Calling <code>stochastic_graph.surrogate_loss(final_losses)</code> will call <code>loss()</code> on every <code>DistributionTensor</code> upstream of final losses.</p> <p><code>loss()</code> will return None for <code>DistributionTensor</code>s backed by reparameterized distributions; it will also return None if the value type is <code>MeanValueType</code> or if <code>loss_fn=None</code>.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>dist_cls</code>: a class deriving from <code>BaseDistribution</code>.</li> <li>\n<code>name</code>: a name for this <code>DistributionTensor</code> and its ops.</li> <li>\n<code>dist_value_type</code>: a <code>_StochasticValueType</code>, which will determine what the <code>value</code> of this <code>DistributionTensor</code> will be. If not provided, the value type set with the <code>value_type</code> context manager will be used.</li> <li>\n<code>loss_fn</code>: callable that takes <code>(dt, dt.value(), influenced_losses)</code>, where <code>dt</code> is this <code>DistributionTensor</code>, and returns a <code>Tensor</code> loss.</li> <li>\n<code>**dist_args</code>: keyword arguments to be passed through to <code>dist_cls</code> on construction.</li> </ul>  <h4 id=\"DistributionTensor.clone\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.clone(name=None, **dist_args)</code></h4>  <h4 id=\"DistributionTensor.distribution\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.distribution</code></h4>  <h4 id=\"DistributionTensor.dtype\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.dtype</code></h4>  <h4 id=\"DistributionTensor.entropy\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.entropy(name='entropy')</code></h4>  <h4 id=\"DistributionTensor.graph\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.graph</code></h4>  <h4 id=\"DistributionTensor.input_dict\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.input_dict</code></h4>  <h4 id=\"DistributionTensor.loss\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.loss(final_losses, name='Loss')</code></h4>  <h4 id=\"DistributionTensor.mean\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.mean(name='mean')</code></h4>  <h4 id=\"DistributionTensor.name\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.name</code></h4>  <h4 id=\"DistributionTensor.value\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.value(name='value')</code></h4>  <h4 id=\"DistributionTensor.value_type\"><code>tf.contrib.bayesflow.stochastic_graph.DistributionTensor.value_type</code></h4>  <h2 id=\"stochastic-computation-value-types\">Stochastic Computation Value Types</h2>  <h3 id=\"MeanValue\"><code>class tf.contrib.bayesflow.stochastic_graph.MeanValue</code></h3>  <h4 id=\"MeanValue.__init__\"><code>tf.contrib.bayesflow.stochastic_graph.MeanValue.__init__(stop_gradient=False)</code></h4>  <h4 id=\"MeanValue.declare_inputs\"><code>tf.contrib.bayesflow.stochastic_graph.MeanValue.declare_inputs(unused_stochastic_tensor, unused_inputs_dict)</code></h4>  <h4 id=\"MeanValue.popped_above\"><code>tf.contrib.bayesflow.stochastic_graph.MeanValue.popped_above(unused_value_type)</code></h4>  <h4 id=\"MeanValue.pushed_above\"><code>tf.contrib.bayesflow.stochastic_graph.MeanValue.pushed_above(unused_value_type)</code></h4>  <h4 id=\"MeanValue.stop_gradient\"><code>tf.contrib.bayesflow.stochastic_graph.MeanValue.stop_gradient</code></h4>  <h3 id=\"SampleValue\"><code>class tf.contrib.bayesflow.stochastic_graph.SampleValue</code></h3> <p>Draw n samples along a new outer dimension.</p> <p>This ValueType draws <code>n</code> samples from StochasticTensors run within its context, increasing the rank by one along a new outer dimension.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">mu = tf.zeros((2,3))\nsigma = tf.ones((2, 3))\nwith sg.value_type(sg.SampleValue(n=4)):\n  dt = sg.DistributionTensor(\n    distributions.Normal, mu=mu, sigma=sigma)\n# draws 4 samples each with shape (2, 3) and concatenates\nassertEqual(dt.value().get_shape(), (4, 2, 3))\n</pre>  <h4 id=\"SampleValue.__init__\"><code>tf.contrib.bayesflow.stochastic_graph.SampleValue.__init__(n=1, stop_gradient=False)</code></h4> <p>Sample <code>n</code> times and concatenate along a new outer dimension.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>n</code>: A python integer or int32 tensor. The number of samples to take.</li> <li>\n<code>stop_gradient</code>: If <code>True</code>, StochasticTensors' values are wrapped in <code>stop_gradient</code>, to avoid backpropagation through.</li> </ul>  <h4 id=\"SampleValue.declare_inputs\"><code>tf.contrib.bayesflow.stochastic_graph.SampleValue.declare_inputs(unused_stochastic_tensor, unused_inputs_dict)</code></h4>  <h4 id=\"SampleValue.n\"><code>tf.contrib.bayesflow.stochastic_graph.SampleValue.n</code></h4>  <h4 id=\"SampleValue.popped_above\"><code>tf.contrib.bayesflow.stochastic_graph.SampleValue.popped_above(unused_value_type)</code></h4>  <h4 id=\"SampleValue.pushed_above\"><code>tf.contrib.bayesflow.stochastic_graph.SampleValue.pushed_above(unused_value_type)</code></h4>  <h4 id=\"SampleValue.stop_gradient\"><code>tf.contrib.bayesflow.stochastic_graph.SampleValue.stop_gradient</code></h4>  <h3 id=\"SampleAndReshapeValue\"><code>class tf.contrib.bayesflow.stochastic_graph.SampleAndReshapeValue</code></h3> <p>Ask the StochasticTensor for n samples and reshape the result.</p> <p>Sampling from a StochasticTensor increases the rank of the value by 1 (because each sample represents a new outer dimension).</p> <p>This ValueType requests <code>n</code> samples from StochasticTensors run within its context that the outer two dimensions are reshaped to intermix the samples with the outermost (usually batch) dimension.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># mu and sigma are both shaped (2, 3)\nmu = [[0.0, -1.0, 1.0], [0.0, -1.0, 1.0]]\nsigma = tf.constant([[1.1, 1.2, 1.3], [1.1, 1.2, 1.3]])\n\nwith sg.value_type(sg.SampleAndReshapeValue(n=2)):\n  dt = sg.DistributionTensor(\n      distributions.Normal, mu=mu, sigma=sigma)\n\n# sample(2) creates a (2, 2, 3) tensor, and the two outermost dimensions\n# are reshaped into one: the final value is a (4, 3) tensor.\ndt_value = dt.value()\nassertEqual(dt_value.get_shape(), (4, 3))\n\ndt_value_val = sess.run([dt_value])[0]  # or e.g. run([tf.identity(dt)])[0]\nassertEqual(dt_value_val.shape, (4, 3))\n</pre>  <h4 id=\"SampleAndReshapeValue.__init__\"><code>tf.contrib.bayesflow.stochastic_graph.SampleAndReshapeValue.__init__(n=1, stop_gradient=False)</code></h4> <p>Sample <code>n</code> times and reshape the outer 2 axes so rank does not change.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>n</code>: A python integer or int32 tensor. The number of samples to take.</li> <li>\n<code>stop_gradient</code>: If <code>True</code>, StochasticTensors' values are wrapped in <code>stop_gradient</code>, to avoid backpropagation through.</li> </ul>  <h4 id=\"SampleAndReshapeValue.declare_inputs\"><code>tf.contrib.bayesflow.stochastic_graph.SampleAndReshapeValue.declare_inputs(unused_stochastic_tensor, unused_inputs_dict)</code></h4>  <h4 id=\"SampleAndReshapeValue.n\"><code>tf.contrib.bayesflow.stochastic_graph.SampleAndReshapeValue.n</code></h4>  <h4 id=\"SampleAndReshapeValue.popped_above\"><code>tf.contrib.bayesflow.stochastic_graph.SampleAndReshapeValue.popped_above(unused_value_type)</code></h4>  <h4 id=\"SampleAndReshapeValue.pushed_above\"><code>tf.contrib.bayesflow.stochastic_graph.SampleAndReshapeValue.pushed_above(unused_value_type)</code></h4>  <h4 id=\"SampleAndReshapeValue.stop_gradient\"><code>tf.contrib.bayesflow.stochastic_graph.SampleAndReshapeValue.stop_gradient</code></h4>  <h3 id=\"value_type\"><code>tf.contrib.bayesflow.stochastic_graph.value_type(dist_value_type)</code></h3> <p>Creates a value type context for any StochasticTensor created within.</p> <p>Typical usage:</p> <pre class=\"\">with sg.value_type(sg.MeanValue(stop_gradients=True)):\n  dt = sg.DistributionTensor(distributions.Normal, mu=mu, sigma=sigma)\n</pre> <p>In the example above, <code>dt.value()</code> (or equivalently, <code>tf.identity(dt)</code>) will be the mean value of the Normal distribution, i.e., <code>mu</code> (possibly broadcasted to the shape of <code>sigma</code>). Furthermore, because the <code>MeanValue</code> was marked with <code>stop_gradients=True</code>, this value will have been wrapped in a <code>stop_gradients</code> call to disable any possible backpropagation.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>dist_value_type</code>: An instance of <code>MeanValue</code>, <code>SampleAndReshapeValue</code>, or any other stochastic value type.</li> </ul> <h5 id=\"yields\">Yields:</h5> <p>A context for <code>StochasticTensor</code> objects that controls the value created when they are initialized.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>dist_value_type</code> is not an instance of a stochastic value type.</li> </ul>  <h3 id=\"get_current_value_type\"><code>tf.contrib.bayesflow.stochastic_graph.get_current_value_type()</code></h3>  <h2 id=\"stochastic-computation-surrogate-loss-functions\">Stochastic Computation Surrogate Loss Functions</h2>  <h3 id=\"score_function\"><code>tf.contrib.bayesflow.stochastic_graph.score_function(dist_tensor, value, losses)</code></h3>  <h3 id=\"get_score_function_with_baseline\"><code>tf.contrib.bayesflow.stochastic_graph.get_score_function_with_baseline(baseline)</code></h3>  <h2 id=\"stochastic-computation-graph-helper-functions\">Stochastic Computation Graph Helper Functions</h2>  <h3 id=\"surrogate_loss\"><code>tf.contrib.bayesflow.stochastic_graph.surrogate_loss(sample_losses, stochastic_tensors=None, name='SurrogateLoss')</code></h3> <p>Surrogate loss for stochastic graphs.</p> <p>This function will call <code>loss_fn</code> on each <code>StochasticTensor</code> upstream of <code>sample_losses</code>, passing the losses that it influenced.</p> <p>Note that currently <code>surrogate_loss</code> does not work with <code>StochasticTensor</code>s instantiated in <code>while_loop</code>s or other control structures.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>sample_losses</code>: a list or tuple of final losses. Each loss should be per example in the batch (and possibly per sample); that is, it should have dimensionality of 1 or greater. All losses should have the same shape.</li> <li>\n<code>stochastic_tensors</code>: a list of <code>StochasticTensor</code>s to add loss terms for. If None, defaults to all <code>StochasticTensor</code>s in the graph upstream of the <code>Tensor</code>s in <code>sample_losses</code>.</li> <li>\n<code>name</code>: the name with which to prepend created ops.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p><code>Tensor</code> loss, which is the sum of <code>sample_losses</code> and the <code>loss_fn</code>s returned by the <code>StochasticTensor</code>s.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>sample_losses</code> is not a list or tuple, or if its elements are not <code>Tensor</code>s.</li> <li>\n<code>ValueError</code>: if any loss in <code>sample_losses</code> does not have dimensionality 1 or greater.</li> </ul>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"NoValueTypeSetError\"><code>class tf.contrib.bayesflow.stochastic_graph.NoValueTypeSetError</code></h3><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.bayesflow.stochastic_graph.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.bayesflow.stochastic_graph.html</a>\n  </p>\n</div>\n","contrib.ffmpeg":"<h1 id=\"ffmpeg-contrib\">FFmpeg (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#ffmpeg-contrib\">FFmpeg (contrib)</a></li> <ul> <li><a href=\"#encoding-and-decoding-audio-using-ffmpeg\">Encoding and decoding audio using FFmpeg</a></li> <ul> <li><a href=\"#decode_audio\"><code>tf.contrib.ffmpeg.decode_audio(contents, file_format=None, samples_per_second=None, channel_count=None)</code></a></li> <li><a href=\"#encode_audio\"><code>tf.contrib.ffmpeg.encode_audio(audio, file_format=None, samples_per_second=None)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"encoding-and-decoding-audio-using-ffmpeg\">Encoding and decoding audio using FFmpeg</h2> <p>TensorFlow provides Ops to decode and encode audio files using the <a href=\"https://www.ffmpeg.org/\" rel=\"noreferrer\">FFmpeg</a> library. FFmpeg must be locally <a href=\"https://ffmpeg.org/download.html\" rel=\"noreferrer\">installed</a> for these Ops to succeed.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">from tensorflow.contrib import ffmpeg\n\naudio_binary = tf.read_file('song.mp3')\nwaveform = ffmpeg.decode_audio(\n    audio_binary, file_format='mp3', samples_per_second=44100, channel_count=2)\nuncompressed_binary = ffmpeg.encode_audio(\n    waveform, file_format='wav', samples_per_second=44100)\n</pre>  <h3 id=\"decode_audio\"><code>tf.contrib.ffmpeg.decode_audio(contents, file_format=None, samples_per_second=None, channel_count=None)</code></h3> <p>Create an op that decodes the contents of an audio file.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>contents</code>: The binary contents of the audio file to decode. This is a scalar.</li> <li>\n<code>file_format</code>: A string specifying which format the contents will conform to. This can be mp3, ogg, or wav.</li> <li>\n<code>samples_per_second</code>: The number of samples per second that is assumed. In some cases, resampling will occur to generate the correct sample rate.</li> <li>\n<code>channel_count</code>: The number of channels that should be created from the audio contents. If the contents have more than this number, then some channels will be merged or dropped. If contents has fewer than this, then additional channels will be created from the existing ones.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A rank 2 tensor that has time along dimension 0 and channels along dimension 1. Dimension 0 will be <code>samples_per_second * length</code> wide, and dimension 1 will be <code>channel_count</code> wide. If ffmpeg fails to decode the audio then an empty tensor will be returned.</p>  <h3 id=\"encode_audio\"><code>tf.contrib.ffmpeg.encode_audio(audio, file_format=None, samples_per_second=None)</code></h3> <p>Creates an op that encodes an audio file using sampled audio from a tensor.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>audio</code>: A rank 2 tensor that has time along dimension 0 and channels along dimension 1. Dimension 0 is <code>samples_per_second * length</code> long in seconds.</li> <li>\n<code>file_format</code>: The type of file to encode. \"wav\" is the only supported format.</li> <li>\n<code>samples_per_second</code>: The number of samples in the audio tensor per second of audio.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A scalar tensor that contains the encoded audio in the specified file format.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.ffmpeg.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.ffmpeg.html</a>\n  </p>\n</div>\n","state_ops":"<h1 id=\"variables\">Variables</h1> <p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href=\"framework#convert_to_tensor\"><code>tf.convert_to_tensor</code></a>.</p> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#variables\">Variables</a></li> <ul> <li><a href=\"#variables-2\">Variables</a></li> <ul> <li><a href=\"#Variable\"><code>class tf.Variable</code></a></li> </ul> <li><a href=\"#variable-helper-functions\">Variable helper functions</a></li> <ul> <li><a href=\"#all_variables\"><code>tf.all_variables()</code></a></li> <li><a href=\"#trainable_variables\"><code>tf.trainable_variables()</code></a></li> <li><a href=\"#local_variables\"><code>tf.local_variables()</code></a></li> <li><a href=\"#moving_average_variables\"><code>tf.moving_average_variables()</code></a></li> <li><a href=\"#initialize_all_variables\"><code>tf.initialize_all_variables()</code></a></li> <li><a href=\"#initialize_variables\"><code>tf.initialize_variables(var_list, name=init)</code></a></li> <li><a href=\"#initialize_local_variables\"><code>tf.initialize_local_variables()</code></a></li> <li><a href=\"#is_variable_initialized\"><code>tf.is_variable_initialized(variable)</code></a></li> <li><a href=\"#report_uninitialized_variables\"><code>tf.report_uninitialized_variables(var_list=None, name=report_uninitialized_variables)</code></a></li> <li><a href=\"#assert_variables_initialized\"><code>tf.assert_variables_initialized(var_list=None)</code></a></li> </ul> <li><a href=\"#saving-and-restoring-variables\">Saving and Restoring Variables</a></li> <ul> <li><a href=\"#Saver\"><code>class tf.train.Saver</code></a></li> <li><a href=\"#latest_checkpoint\"><code>tf.train.latest_checkpoint(checkpoint_dir, latest_filename=None)</code></a></li> <li><a href=\"#get_checkpoint_state\"><code>tf.train.get_checkpoint_state(checkpoint_dir, latest_filename=None)</code></a></li> <li><a href=\"#update_checkpoint_state\"><code>tf.train.update_checkpoint_state(save_dir, model_checkpoint_path, all_model_checkpoint_paths=None, latest_filename=None)</code></a></li> </ul> <li><a href=\"#sharing-variables\">Sharing Variables</a></li> <ul> <li><a href=\"#get_variable\"><code>tf.get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, custom_getter=None)</code></a></li> <li><a href=\"#VariableScope\"><code>class tf.VariableScope</code></a></li> <li><a href=\"#variable_scope\"><code>tf.variable_scope(name_or_scope, reuse=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, dtype=None)</code></a></li> <li><a href=\"#variable_op_scope\"><code>tf.variable_op_scope(values, name_or_scope, default_name=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None)</code></a></li> <li><a href=\"#get_variable_scope\"><code>tf.get_variable_scope()</code></a></li> <li><a href=\"#make_template\"><code>tf.make_template(name_, func_, create_scope_now_=False, unique_name_=None, **kwargs)</code></a></li> <li><a href=\"#no_regularizer\"><code>tf.no_regularizer(_)</code></a></li> <li><a href=\"#constant_initializer\"><code>tf.constant_initializer(value=0.0, dtype=tf.float32)</code></a></li> <li><a href=\"#random_normal_initializer\"><code>tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)</code></a></li> <li><a href=\"#truncated_normal_initializer\"><code>tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)</code></a></li> <li><a href=\"#random_uniform_initializer\"><code>tf.random_uniform_initializer(minval=0.0, maxval=1.0, seed=None, dtype=tf.float32)</code></a></li> <li><a href=\"#uniform_unit_scaling_initializer\"><code>tf.uniform_unit_scaling_initializer(factor=1.0, seed=None, dtype=tf.float32, full_shape=None)</code></a></li> <li><a href=\"#zeros_initializer\"><code>tf.zeros_initializer(shape, dtype=tf.float32)</code></a></li> <li><a href=\"#ones_initializer\"><code>tf.ones_initializer(shape, dtype=tf.float32)</code></a></li> </ul> <li><a href=\"#variable-partitioners-for-sharding\">Variable Partitioners for Sharding</a></li> <ul> <li><a href=\"#variable_axis_size_partitioner\"><code>tf.variable_axis_size_partitioner(max_shard_bytes, axis=0, bytes_per_string_element=16, max_shards=None)</code></a></li> <li><a href=\"#min_max_variable_partitioner\"><code>tf.min_max_variable_partitioner(max_partitions=1, axis=0, min_slice_size=262144, bytes_per_string_element=16)</code></a></li> </ul> <li><a href=\"#sparse-variable-updates\">Sparse Variable Updates</a></li> <ul> <li><a href=\"#scatter_update\"><code>tf.scatter_update(ref, indices, updates, use_locking=None, name=None)</code></a></li> <li><a href=\"#scatter_add\"><code>tf.scatter_add(ref, indices, updates, use_locking=None, name=None)</code></a></li> <li><a href=\"#scatter_sub\"><code>tf.scatter_sub(ref, indices, updates, use_locking=None, name=None)</code></a></li> <li><a href=\"#sparse_mask\"><code>tf.sparse_mask(a, mask_indices, name=None)</code></a></li> <li><a href=\"#IndexedSlices\"><code>class tf.IndexedSlices</code></a></li> </ul> <li><a href=\"#exporting-and-importing-meta-graphs\">Exporting and Importing Meta Graphs</a></li> <ul> <li><a href=\"#export_meta_graph\"><code>tf.train.export_meta_graph(filename=None, meta_info_def=None, graph_def=None, saver_def=None, collection_list=None, as_text=False)</code></a></li> <li><a href=\"#import_meta_graph\"><code>tf.train.import_meta_graph(meta_graph_or_file)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"variables-2\">Variables</h2>  <h3 id=\"Variable\"><code>class tf.Variable</code></h3> <p>See the <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html\">Variables How To</a> for a high level overview.</p> <p>A variable maintains state in the graph across calls to <code>run()</code>. You add a variable to the graph by constructing an instance of the class <code>Variable</code>.</p> <p>The <code>Variable()</code> constructor requires an initial value for the variable, which can be a <code>Tensor</code> of any type and shape. The initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.</p> <p>If you want to change the shape of a variable later you have to use an <code>assign</code> Op with <code>validate_shape=False</code>.</p> <p>Just like any <code>Tensor</code>, variables created with <code>Variable()</code> can be used as inputs for other Ops in the graph. Additionally, all the operators overloaded for the <code>Tensor</code> class are carried over to variables, so you can also add nodes to the graph by just doing arithmetic on variables.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">import tensorflow as tf\n\n# Create a variable.\nw = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)\n\n# Use the variable in the graph like any Tensor.\ny = tf.matmul(w, ...another variable or tensor...)\n\n# The overloaded operators are available too.\nz = tf.sigmoid(w + y)\n\n# Assign a new value to the variable with `assign()` or a related method.\nw.assign(w + 1.0)\nw.assign_add(1.0)\n</pre> <p>When you launch the graph, variables have to be explicitly initialized before you can run Ops that use their value. You can initialize a variable by running its <em>initializer op</em>, restoring the variable from a save file, or simply running an <code>assign</code> Op that assigns a value to the variable. In fact, the variable <em>initializer op</em> is just an <code>assign</code> Op that assigns the variable's initial value to the variable itself.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Launch the graph in a session.\nwith tf.Session() as sess:\n    # Run the variable initializer.\n    sess.run(w.initializer)\n    # ...you now can run ops that use the value of 'w'...\n</pre> <p>The most common initialization pattern is to use the convenience function <code>initialize_all_variables()</code> to add an Op to the graph that initializes all the variables. You then run that Op after launching the graph.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Add an Op to initialize all variables.\ninit_op = tf.initialize_all_variables()\n\n# Launch the graph in a session.\nwith tf.Session() as sess:\n    # Run the Op that initializes all variables.\n    sess.run(init_op)\n    # ...you can now run any Op that uses variable values...\n</pre> <p>If you need to create a variable with an initial value dependent on another variable, use the other variable's <code>initialized_value()</code>. This ensures that variables are initialized in the right order.</p> <p>All variables are automatically collected in the graph where they are created. By default, the constructor adds the new variable to the graph collection <code>GraphKeys.VARIABLES</code>. The convenience function <code>all_variables()</code> returns the contents of that collection.</p> <p>When building a machine learning model it is often convenient to distinguish betwen variables holding the trainable model parameters and other variables such as a <code>global step</code> variable used to count training steps. To make this easier, the variable constructor supports a <code>trainable=&lt;bool&gt;</code> parameter. If <code>True</code>, the new variable is also added to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code>. The convenience function <code>trainable_variables()</code> returns the contents of this collection. The various <code>Optimizer</code> classes use this collection as the default list of variables to optimize.</p> <p>Creating a variable.</p>  <h4 id=\"Variable.__init__\"><code>tf.Variable.__init__(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None)</code></h4> <p>Creates a new variable with value <code>initial_value</code>.</p> <p>The new variable is added to the graph collections listed in <code>collections</code>, which defaults to <code>[GraphKeys.VARIABLES]</code>.</p> <p>If <code>trainable</code> is <code>True</code> the variable is also added to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code>.</p> <p>This constructor creates both a <code>variable</code> Op and an <code>assign</code> Op to set the variable to its initial value.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>initial_value</code>: A <code>Tensor</code>, or Python object convertible to a <code>Tensor</code>, which is the initial value for the Variable. The initial value must have a shape specified unless <code>validate_shape</code> is set to False. Can also be a callable with no argument that returns the initial value when called. In that case, <code>dtype</code> must be specified. (Note that initializer functions from init_ops.py must first be bound to a shape before being used here.)</li> <li>\n<code>trainable</code>: If <code>True</code>, the default, also adds the variable to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code>. This collection is used as the default list of variables to use by the <code>Optimizer</code> classes.</li> <li>\n<code>collections</code>: List of graph collections keys. The new variable is added to these collections. Defaults to <code>[GraphKeys.VARIABLES]</code>.</li> <li>\n<code>validate_shape</code>: If <code>False</code>, allows the variable to be initialized with a value of unknown shape. If <code>True</code>, the default, the shape of <code>initial_value</code> must be known.</li> <li>\n<code>caching_device</code>: Optional device string describing where the Variable should be cached for reading. Defaults to the Variable's device. If not <code>None</code>, caches on another device. Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through <code>Switch</code> and other conditional statements.</li> <li>\n<code>name</code>: Optional name for the variable. Defaults to <code>'Variable'</code> and gets uniquified automatically.</li> <li>\n<code>variable_def</code>: <code>VariableDef</code> protocol buffer. If not <code>None</code>, recreates the Variable object with its contents. <code>variable_def</code> and the other arguments are mutually exclusive.</li> <li>\n<code>dtype</code>: If set, initial_value will be converted to the given type. If <code>None</code>, either the datatype will be kept (if <code>initial_value</code> is a Tensor), or <code>convert_to_tensor</code> will decide.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A Variable.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If both <code>variable_def</code> and initial_value are specified.</li> <li>\n<code>ValueError</code>: If the initial value is not specified, or does not have a shape and <code>validate_shape</code> is <code>True</code>.</li> </ul>  <h4 id=\"Variable.initialized_value\"><code>tf.Variable.initialized_value()</code></h4> <p>Returns the value of the initialized variable.</p> <p>You should use this instead of the variable itself to initialize another variable with a value that depends on the value of this variable.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Initialize 'v' with a random tensor.\nv = tf.Variable(tf.truncated_normal([10, 40]))\n# Use `initialized_value` to guarantee that `v` has been\n# initialized before its value is used to initialize `w`.\n# The random values are picked only once.\nw = tf.Variable(v.initialized_value() * 2.0)\n</pre>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code> holding the value of this variable after its initializer has run.</p> <p>Changing a variable value.</p>  <h4 id=\"Variable.assign\"><code>tf.Variable.assign(value, use_locking=False)</code></h4> <p>Assigns a new value to the variable.</p> <p>This is essentially a shortcut for <code>assign(self, value)</code>.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>value</code>: A <code>Tensor</code>. The new value for this variable.</li> <li>\n<code>use_locking</code>: If <code>True</code>, use locking during the assignment.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>Tensor</code> that will hold the new value of this variable after the assignment has completed.</p>  <h4 id=\"Variable.assign_add\"><code>tf.Variable.assign_add(delta, use_locking=False)</code></h4> <p>Adds a value to this variable.</p> <p>This is essentially a shortcut for <code>assign_add(self, delta)</code>.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>delta</code>: A <code>Tensor</code>. The value to add to this variable.</li> <li>\n<code>use_locking</code>: If <code>True</code>, use locking during the operation.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>Tensor</code> that will hold the new value of this variable after the addition has completed.</p>  <h4 id=\"Variable.assign_sub\"><code>tf.Variable.assign_sub(delta, use_locking=False)</code></h4> <p>Subtracts a value from this variable.</p> <p>This is essentially a shortcut for <code>assign_sub(self, delta)</code>.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>delta</code>: A <code>Tensor</code>. The value to subtract from this variable.</li> <li>\n<code>use_locking</code>: If <code>True</code>, use locking during the operation.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A <code>Tensor</code> that will hold the new value of this variable after the subtraction has completed.</p>  <h4 id=\"Variable.scatter_sub\"><code>tf.Variable.scatter_sub(sparse_delta, use_locking=False)</code></h4> <p>Subtracts <code>IndexedSlices</code> from this variable.</p> <p>This is essentially a shortcut for <code>scatter_sub(self, sparse_delta.indices,\nsparse_delta.values)</code>.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>sparse_delta</code>: <code>IndexedSlices</code> to be subtracted from this variable.</li> <li>\n<code>use_locking</code>: If <code>True</code>, use locking during the operation.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A <code>Tensor</code> that will hold the new value of this variable after the scattered subtraction has completed.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li> </ul>  <h4 id=\"Variable.count_up_to\"><code>tf.Variable.count_up_to(limit)</code></h4> <p>Increments this variable until it reaches <code>limit</code>.</p> <p>When that Op is run it tries to increment the variable by <code>1</code>. If incrementing the variable would bring it above <code>limit</code> then the Op raises the exception <code>OutOfRangeError</code>.</p> <p>If no error is raised, the Op outputs the value of the variable before the increment.</p> <p>This is essentially a shortcut for <code>count_up_to(self, limit)</code>.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>limit</code>: value at which incrementing the variable raises an error.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A <code>Tensor</code> that will hold the variable value before the increment. If no other Op modifies this variable, the values produced will all be distinct.</p>  <h4 id=\"Variable.eval\"><code>tf.Variable.eval(session=None)</code></h4> <p>In a session, computes and returns the value of this variable.</p> <p>This is not a graph construction method, it does not add ops to the graph.</p> <p>This convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used. See the <a href=\"client#Session\">Session class</a> for more information on launching a graph and on sessions.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">v = tf.Variable([1, 2])\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    # Usage passing the session explicitly.\n    print(v.eval(sess))\n    # Usage with the default session.  The 'with' block\n    # above makes 'sess' the default session.\n    print(v.eval())\n</pre>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>session</code>: The session to use to evaluate this variable. If none, the default session is used.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A numpy <code>ndarray</code> with a copy of the value of this variable.</p> <p>Properties.</p>  <h4 id=\"Variable.name\"><code>tf.Variable.name</code></h4> <p>The name of this variable.</p>  <h4 id=\"Variable.dtype\"><code>tf.Variable.dtype</code></h4> <p>The <code>DType</code> of this variable.</p>  <h4 id=\"Variable.get_shape\"><code>tf.Variable.get_shape()</code></h4> <p>The <code>TensorShape</code> of this variable.</p>  <h5 id=\"returns-9\">Returns:</h5> <p>A <code>TensorShape</code>.</p>  <h4 id=\"Variable.device\"><code>tf.Variable.device</code></h4> <p>The device of this variable.</p>  <h4 id=\"Variable.initializer\"><code>tf.Variable.initializer</code></h4> <p>The initializer operation for this variable.</p>  <h4 id=\"Variable.graph\"><code>tf.Variable.graph</code></h4> <p>The <code>Graph</code> of this variable.</p>  <h4 id=\"Variable.op\"><code>tf.Variable.op</code></h4> <p>The <code>Operation</code> of this variable.</p>  <h4 id=\"other-methods\">Other Methods</h4>  <h4 id=\"Variable.from_proto\"><code>tf.Variable.from_proto(variable_def)</code></h4> <p>Returns a <code>Variable</code> object created from <code>variable_def</code>.</p>  <h4 id=\"Variable.initial_value\"><code>tf.Variable.initial_value</code></h4> <p>Returns the Tensor used as the initial value for the variable.</p> <p>Note that this is different from <code>initialized_value()</code> which runs the op that initializes the variable before returning its value. This method returns the tensor that is used by the op that initializes the variable.</p>  <h5 id=\"returns-10\">Returns:</h5> <p>A <code>Tensor</code>.</p>  <h4 id=\"Variable.ref\"><code>tf.Variable.ref()</code></h4> <p>Returns a reference to this variable.</p> <p>You usually do not need to call this method as all ops that need a reference to the variable call it automatically.</p> <p>Returns is a <code>Tensor</code> which holds a reference to the variable. You can assign a new value to the variable by passing the tensor to an assign op. See <a href=\"#Variable.value\"><code>value()</code></a> if you want to get the value of the variable.</p>  <h5 id=\"returns-11\">Returns:</h5> <p>A <code>Tensor</code> that is a reference to the variable.</p>  <h4 id=\"Variable.to_proto\"><code>tf.Variable.to_proto()</code></h4> <p>Converts a <code>Variable</code> to a <code>VariableDef</code> protocol buffer.</p>  <h5 id=\"returns-12\">Returns:</h5> <p>A <code>VariableDef</code> protocol buffer.</p>  <h4 id=\"Variable.value\"><code>tf.Variable.value()</code></h4> <p>Returns the last snapshot of this variable.</p> <p>You usually do not need to call this method as all ops that need the value of the variable call it automatically through a <code>convert_to_tensor()</code> call.</p> <p>Returns a <code>Tensor</code> which holds the value of the variable. You can not assign a new value to this tensor as it is not a reference to the variable. See <a href=\"#Variable.ref\"><code>ref()</code></a> if you want to get a reference to the variable.</p> <p>To avoid copies, if the consumer of the returned value is on the same device as the variable, this actually returns the live value of the variable, not a copy. Updates to the variable are seen by the consumer. If the consumer is on a different device it will get a copy of the variable.</p>  <h5 id=\"returns-13\">Returns:</h5> <p>A <code>Tensor</code> containing the value of the variable.</p>  <h2 id=\"variable-helper-functions\">Variable helper functions</h2> <p>TensorFlow provides a set of functions to help manage the set of variables collected in the graph.</p>  <h3 id=\"all_variables\"><code>tf.all_variables()</code></h3> <p>Returns all variables that must be saved/restored.</p> <p>The <code>Variable()</code> constructor automatically adds new variables to the graph collection <code>GraphKeys.VARIABLES</code>. This convenience function returns the contents of that collection.</p>  <h5 id=\"returns-14\">Returns:</h5> <p>A list of <code>Variable</code> objects.</p>  <h3 id=\"trainable_variables\"><code>tf.trainable_variables()</code></h3> <p>Returns all variables created with <code>trainable=True</code>.</p> <p>When passed <code>trainable=True</code>, the <code>Variable()</code> constructor automatically adds new variables to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code>. This convenience function returns the contents of that collection.</p>  <h5 id=\"returns-15\">Returns:</h5> <p>A list of Variable objects.</p>  <h3 id=\"local_variables\"><code>tf.local_variables()</code></h3> <p>Returns all variables created with collection=[LOCAL_VARIABLES].</p>  <h5 id=\"returns-16\">Returns:</h5> <p>A list of local Variable objects.</p>  <h3 id=\"moving_average_variables\"><code>tf.moving_average_variables()</code></h3> <p>Returns all variables that maintain their moving averages.</p> <p>If an <code>ExponentialMovingAverage</code> object is created and the <code>apply()</code> method is called on a list of variables, these variables will be added to the <code>GraphKeys.MOVING_AVERAGE_VARIABLES</code> collection. This convenience function returns the contents of that collection.</p>  <h5 id=\"returns-17\">Returns:</h5> <p>A list of Variable objects.</p>  <h3 id=\"initialize_all_variables\"><code>tf.initialize_all_variables()</code></h3> <p>Returns an Op that initializes all variables.</p> <p>This is just a shortcut for <code>initialize_variables(all_variables())</code></p>  <h5 id=\"returns-18\">Returns:</h5> <p>An Op that initializes all variables in the graph.</p>  <h3 id=\"initialize_variables\"><code>tf.initialize_variables(var_list, name='init')</code></h3> <p>Returns an Op that initializes a list of variables.</p> <p>After you launch the graph in a session, you can run the returned Op to initialize all the variables in <code>var_list</code>. This Op runs all the initializers of the variables in <code>var_list</code> in parallel.</p> <p>Calling <code>initialize_variables()</code> is equivalent to passing the list of initializers to <code>Group()</code>.</p> <p>If <code>var_list</code> is empty, however, the function still returns an Op that can be run. That Op just has no effect.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>var_list</code>: List of <code>Variable</code> objects to initialize.</li> <li>\n<code>name</code>: Optional name for the returned operation.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>An Op that run the initializers of all the specified variables.</p>  <h3 id=\"initialize_local_variables\"><code>tf.initialize_local_variables()</code></h3> <p>Returns an Op that initializes all local variables.</p> <p>This is just a shortcut for <code>initialize_variables(local_variables())</code></p>  <h5 id=\"returns-20\">Returns:</h5> <p>An Op that initializes all local variables in the graph.</p>  <h3 id=\"is_variable_initialized\"><code>tf.is_variable_initialized(variable)</code></h3> <p>Tests if a variable has been initialized.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>variable</code>: A <code>Variable</code>.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>Returns a scalar boolean Tensor, <code>True</code> if the variable has been initialized, <code>False</code> otherwise.</p>  <h3 id=\"report_uninitialized_variables\"><code>tf.report_uninitialized_variables(var_list=None, name='report_uninitialized_variables')</code></h3> <p>Adds ops to list the names of uninitialized variables.</p> <p>When run, it returns a 1-D tensor containing the names of uninitialized variables if there are any, or an empty array if there are none.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>var_list</code>: List of <code>Variable</code> objects to check. Defaults to the value of <code>all_variables() + local_variables()</code>\n</li> <li>\n<code>name</code>: Optional name of the <code>Operation</code>.</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>A 1-D tensor containing names of the unintialized variables, or an empty 1-D tensor if there are no variables or no uninitialized variables.</p>  <h3 id=\"assert_variables_initialized\"><code>tf.assert_variables_initialized(var_list=None)</code></h3> <p>Returns an Op to check if variables are initialized.</p> <p>NOTE: This function is obsolete and will be removed in 6 months. Please change your implementation to use <code>report_uninitialized_variables()</code>.</p> <p>When run, the returned Op will raise the exception <code>FailedPreconditionError</code> if any of the variables has not yet been initialized.</p> <p>Note: This function is implemented by trying to fetch the values of the variables. If one of the variables is not initialized a message may be logged by the C++ runtime. This is expected.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>var_list</code>: List of <code>Variable</code> objects to check. Defaults to the value of <code>all_variables().</code>\n</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>An Op, or None if there are no variables.</p>  <h2 id=\"saving-and-restoring-variables\">Saving and Restoring Variables</h2>  <h3 id=\"Saver\"><code>class tf.train.Saver</code></h3> <p>Saves and restores variables.</p> <p>See <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html\">Variables</a> for an overview of variables, saving and restoring.</p> <p>The <code>Saver</code> class adds ops to save and restore variables to and from <em>checkpoints</em>. It also provides convenience methods to run these ops.</p> <p>Checkpoints are binary files in a proprietary format which map variable names to tensor values. The best way to examine the contents of a checkpoint is to load it using a <code>Saver</code>.</p> <p>Savers can automatically number checkpoint filenames with a provided counter. This lets you keep multiple checkpoints at different steps while training a model. For example you can number the checkpoint filenames with the training step number. To avoid filling up disks, savers manage checkpoint files automatically. For example, they can keep only the N most recent files, or one checkpoint for every N hours of training.</p> <p>You number checkpoint filenames by passing a value to the optional <code>global_step</code> argument to <code>save()</code>:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">saver.save(sess, 'my-model', global_step=0) ==&gt; filename: 'my-model-0'\n...\nsaver.save(sess, 'my-model', global_step=1000) ==&gt; filename: 'my-model-1000'\n</pre> <p>Additionally, optional arguments to the <code>Saver()</code> constructor let you control the proliferation of checkpoint files on disk:</p> <ul> <li><p><code>max_to_keep</code> indicates the maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept. Defaults to 5 (that is, the 5 most recent checkpoint files are kept.)</p></li> <li><p><code>keep_checkpoint_every_n_hours</code>: In addition to keeping the most recent <code>max_to_keep</code> checkpoint files, you might want to keep one checkpoint file for every N hours of training. This can be useful if you want to later analyze how a model progressed during a long training session. For example, passing <code>keep_checkpoint_every_n_hours=2</code> ensures that you keep one checkpoint file for every 2 hours of training. The default value of 10,000 hours effectively disables the feature.</p></li> </ul> <p>Note that you still have to call the <code>save()</code> method to save the model. Passing these arguments to the constructor will not save variables automatically for you.</p> <p>A training program that saves regularly looks like:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">...\n# Create a saver.\nsaver = tf.train.Saver(...variables...)\n# Launch the graph and train, saving the model every 1,000 steps.\nsess = tf.Session()\nfor step in xrange(1000000):\n    sess.run(..training_op..)\n    if step % 1000 == 0:\n        # Append the step number to the checkpoint name:\n        saver.save(sess, 'my-model', global_step=step)\n</pre> <p>In addition to checkpoint files, savers keep a protocol buffer on disk with the list of recent checkpoints. This is used to manage numbered checkpoint files and by <code>latest_checkpoint()</code>, which makes it easy to discover the path to the most recent checkpoint. That protocol buffer is stored in a file named 'checkpoint' next to the checkpoint files.</p> <p>If you create several savers, you can specify a different filename for the protocol buffer file in the call to <code>save()</code>.</p>  <h4 id=\"Saver.__init__\"><code>tf.train.Saver.__init__(var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False, saver_def=None, builder=None)</code></h4> <p>Creates a <code>Saver</code>.</p> <p>The constructor adds ops to save and restore variables.</p> <p><code>var_list</code> specifies the variables that will be saved and restored. It can be passed as a <code>dict</code> or a list:</p> <ul> <li>A <code>dict</code> of names to variables: The keys are the names that will be used to save or restore the variables in the checkpoint files.</li> <li>A list of variables: The variables will be keyed with their op name in the checkpoint files.</li> </ul> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">v1 = tf.Variable(..., name='v1')\nv2 = tf.Variable(..., name='v2')\n\n# Pass the variables as a dict:\nsaver = tf.train.Saver({'v1': v1, 'v2': v2})\n\n# Or pass them as a list.\nsaver = tf.train.Saver([v1, v2])\n# Passing a list is equivalent to passing a dict with the variable op names\n# as keys:\nsaver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n</pre> <p>The optional <code>reshape</code> argument, if <code>True</code>, allows restoring a variable from a save file where the variable had a different shape, but the same number of elements and type. This is useful if you have reshaped a variable and want to reload it from an older checkpoint.</p> <p>The optional <code>sharded</code> argument, if <code>True</code>, instructs the saver to shard checkpoints per device.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>var_list</code>: A list of <code>Variable</code> objects or a dictionary mapping names to variables. If <code>None</code>, defaults to the list of all variables.</li> <li>\n<code>reshape</code>: If <code>True</code>, allows restoring parameters from a checkpoint where the variables have a different shape.</li> <li>\n<code>sharded</code>: If <code>True</code>, shard the checkpoints, one per device.</li> <li>\n<code>max_to_keep</code>: Maximum number of recent checkpoints to keep. Defaults to 5.</li> <li>\n<code>keep_checkpoint_every_n_hours</code>: How often to keep checkpoints. Defaults to 10,000 hours.</li> <li>\n<code>name</code>: String. Optional name to use as a prefix when adding operations.</li> <li>\n<code>restore_sequentially</code>: A <code>Bool</code>, which if true, causes restore of different variables to happen sequentially within each device. This can lower memory usage when restoring very large models.</li> <li>\n<code>saver_def</code>: Optional <code>SaverDef</code> proto to use instead of running the builder. This is only useful for specialty code that wants to recreate a <code>Saver</code> object for a previously built <code>Graph</code> that had a <code>Saver</code>. The <code>saver_def</code> proto should be the one returned by the <code>as_saver_def()</code> call of the <code>Saver</code> that was created for that <code>Graph</code>.</li> <li>\n<code>builder</code>: Optional <code>SaverBuilder</code> to use if a <code>saver_def</code> was not provided. Defaults to <code>BaseSaverBuilder()</code>.</li> </ul>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>var_list</code> is invalid.</li> <li>\n<code>ValueError</code>: If any of the keys or values in <code>var_list</code> are not unique.</li> </ul>  <h4 id=\"Saver.save\"><code>tf.train.Saver.save(sess, save_path, global_step=None, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True)</code></h4> <p>Saves variables.</p> <p>This method runs the ops added by the constructor for saving variables. It requires a session in which the graph was launched. The variables to save must also have been initialized.</p> <p>The method returns the path of the newly created checkpoint file. This path can be passed directly to a call to <code>restore()</code>.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>sess</code>: A Session to use to save the variables.</li> <li>\n<code>save_path</code>: String. Path to the checkpoint filename. If the saver is <code>sharded</code>, this is the prefix of the sharded checkpoint filename.</li> <li>\n<code>global_step</code>: If provided the global step number is appended to <code>save_path</code> to create the checkpoint filename. The optional argument can be a <code>Tensor</code>, a <code>Tensor</code> name or an integer.</li> <li>\n<code>latest_filename</code>: Optional name for the protocol buffer file that will contains the list of most recent checkpoint filenames. That file, kept in the same directory as the checkpoint files, is automatically managed by the saver to keep track of recent checkpoints. Defaults to 'checkpoint'.</li> <li>\n<code>meta_graph_suffix</code>: Suffix for <code>MetaGraphDef</code> file. Defaults to 'meta'.</li> <li>\n<code>write_meta_graph</code>: <code>Boolean</code> indicating whether or not to write the meta graph file.</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A string: path at which the variables were saved. If the saver is sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn' is the number of shards created.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>sess</code> is not a <code>Session</code>.</li> <li>\n<code>ValueError</code>: If <code>latest_filename</code> contains path components, or if it collides with <code>save_path</code>.</li> </ul>  <h4 id=\"Saver.restore\"><code>tf.train.Saver.restore(sess, save_path)</code></h4> <p>Restores previously saved variables.</p> <p>This method runs the ops added by the constructor for restoring variables. It requires a session in which the graph was launched. The variables to restore do not have to have been initialized, as restoring is itself a way to initialize variables.</p> <p>The <code>save_path</code> argument is typically a value previously returned from a <code>save()</code> call, or a call to <code>latest_checkpoint()</code>.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>sess</code>: A <code>Session</code> to use to restore the parameters.</li> <li>\n<code>save_path</code>: Path where parameters were previously saved.</li> </ul>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the given <code>save_path</code> does not point to a file.</li> </ul> <p>Other utility methods.</p>  <h4 id=\"Saver.last_checkpoints\"><code>tf.train.Saver.last_checkpoints</code></h4> <p>List of not-yet-deleted checkpoint filenames.</p> <p>You can pass any of the returned values to <code>restore()</code>.</p>  <h5 id=\"returns-25\">Returns:</h5> <p>A list of checkpoint filenames, sorted from oldest to newest.</p>  <h4 id=\"Saver.set_last_checkpoints\"><code>tf.train.Saver.set_last_checkpoints(last_checkpoints)</code></h4> <p>DEPRECATED: Use set_last_checkpoints_with_time.</p> <p>Sets the list of old checkpoint filenames.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>last_checkpoints</code>: A list of checkpoint filenames.</li> </ul>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>AssertionError</code>: If last_checkpoints is not a list.</li> </ul>  <h4 id=\"Saver.as_saver_def\"><code>tf.train.Saver.as_saver_def()</code></h4> <p>Generates a <code>SaverDef</code> representation of this saver.</p>  <h5 id=\"returns-26\">Returns:</h5> <p>A <code>SaverDef</code> proto.</p>  <h4 id=\"other-methods-2\">Other Methods</h4>  <h4 id=\"Saver.export_meta_graph\"><code>tf.train.Saver.export_meta_graph(filename=None, collection_list=None, as_text=False)</code></h4> <p>Writes <code>MetaGraphDef</code> to save_path/filename.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>filename</code>: Optional meta_graph filename including the path.</li> <li>\n<code>collection_list</code>: List of string keys to collect.</li> <li>\n<code>as_text</code>: If <code>True</code>, writes the meta_graph as an ASCII proto.</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>A <code>MetaGraphDef</code> proto.</p>  <h4 id=\"Saver.from_proto\"><code>tf.train.Saver.from_proto(saver_def)</code></h4> <p>Returns a <code>Saver</code> object created from <code>saver_def</code>.</p>  <h4 id=\"Saver.set_last_checkpoints_with_time\"><code>tf.train.Saver.set_last_checkpoints_with_time(last_checkpoints_with_time)</code></h4> <p>Sets the list of old checkpoint filenames and timestamps.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>last_checkpoints_with_time</code>: A list of tuples of checkpoint filenames and timestamps.</li> </ul>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>AssertionError</code>: If last_checkpoints_with_time is not a list.</li> </ul>  <h4 id=\"Saver.to_proto\"><code>tf.train.Saver.to_proto()</code></h4> <p>Converts this <code>Saver</code> to a <code>SaverDef</code> protocol buffer.</p>  <h5 id=\"returns-28\">Returns:</h5> <p>A <code>SaverDef</code> protocol buffer.</p>  <h3 id=\"latest_checkpoint\"><code>tf.train.latest_checkpoint(checkpoint_dir, latest_filename=None)</code></h3> <p>Finds the filename of latest saved checkpoint file.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>checkpoint_dir</code>: Directory where the variables were saved.</li> <li>\n<code>latest_filename</code>: Optional name for the protocol buffer file that contains the list of most recent checkpoint filenames. See the corresponding argument to <code>Saver.save()</code>.</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>The full path to the latest checkpoint or <code>None</code> if no checkpoint was found.</p>  <h3 id=\"get_checkpoint_state\"><code>tf.train.get_checkpoint_state(checkpoint_dir, latest_filename=None)</code></h3> <p>Returns CheckpointState proto from the \"checkpoint\" file.</p> <p>If the \"checkpoint\" file contains a valid CheckpointState proto, returns it.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>checkpoint_dir</code>: The directory of checkpoints.</li> <li>\n<code>latest_filename</code>: Optional name of the checkpoint file. Default to 'checkpoint'.</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>A CheckpointState if the state was available, None otherwise.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the checkpoint read doesn't have model_checkpoint_path set.</li> </ul>  <h3 id=\"update_checkpoint_state\"><code>tf.train.update_checkpoint_state(save_dir, model_checkpoint_path, all_model_checkpoint_paths=None, latest_filename=None)</code></h3> <p>Updates the content of the 'checkpoint' file.</p> <p>This updates the checkpoint file containing a CheckpointState proto.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>save_dir</code>: Directory where the model was saved.</li> <li>\n<code>model_checkpoint_path</code>: The checkpoint file.</li> <li>\n<code>all_model_checkpoint_paths</code>: List of strings. Paths to all not-yet-deleted checkpoints, sorted from oldest to newest. If this is a non-empty list, the last element must be equal to model_checkpoint_path. These paths are also saved in the CheckpointState proto.</li> <li>\n<code>latest_filename</code>: Optional name of the checkpoint file. Default to 'checkpoint'.</li> </ul>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: If the save paths conflict.</li> </ul>  <h2 id=\"sharing-variables\">Sharing Variables</h2> <p>TensorFlow provides several classes and operations that you can use to create variables contingent on certain conditions.</p>  <h3 id=\"get_variable\"><code>tf.get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, custom_getter=None)</code></h3> <p>Gets an existing variable with these parameters or create a new one.</p> <p>This function prefixes the name with the current variable scope and performs reuse checks. See the <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/variable_scope/index.html\">Variable Scope How To</a> for an extensive description of how reusing works. Here is a basic example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.variable_scope(\"foo\"):\n    v = tf.get_variable(\"v\", [1])  # v.name == \"foo/v:0\"\n    w = tf.get_variable(\"w\", [1])  # w.name == \"foo/w:0\"\nwith tf.variable_scope(\"foo\", reuse=True)\n    v1 = tf.get_variable(\"v\")  # The same as v above.\n</pre> <p>If initializer is <code>None</code> (the default), the default initializer passed in the variable scope will be used. If that one is <code>None</code> too, a <code>uniform_unit_scaling_initializer</code> will be used. The initializer can also be a Tensor, in which case the variable is initialized to this value and shape.</p> <p>Similarly, if the regularizer is <code>None</code> (the default), the default regularizer passed in the variable scope will be used (if that is <code>None</code> too, then by default no regularization is performed).</p> <p>If a partitioner is provided, first a sharded <code>Variable</code> is created via <code>_get_partitioned_variable</code>, and the return value is a <code>Tensor</code> composed of the shards concatenated along the partition axis.</p> <p>Some useful partitioners are available. See, e.g., <code>variable_axis_size_partitioner</code> and <code>min_max_variable_partitioner</code>.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>name</code>: The name of the new or existing variable.</li> <li>\n<code>shape</code>: Shape of the new or existing variable.</li> <li>\n<code>dtype</code>: Type of the new or existing variable (defaults to <code>DT_FLOAT</code>).</li> <li>\n<code>initializer</code>: Initializer for the variable if one is created.</li> <li>\n<code>regularizer</code>: A (Tensor -&gt; Tensor or None) function; the result of applying it on a newly created variable will be added to the collection GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.</li> <li>\n<code>trainable</code>: If <code>True</code> also add the variable to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li> <li>\n<code>collections</code>: List of graph collections keys to add the Variable to. Defaults to <code>[GraphKeys.VARIABLES]</code> (see tf.Variable).</li> <li>\n<code>caching_device</code>: Optional device string or function describing where the Variable should be cached for reading. Defaults to the Variable's device. If not <code>None</code>, caches on another device. Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through <code>Switch</code> and other conditional statements.</li> <li>\n<code>partitioner</code>: Optional callable that accepts a fully defined <code>TensorShape</code> and <code>dtype</code> of the Variable to be created, and returns a list of partitions for each axis (currently only one axis can be partitioned).</li> <li>\n<code>validate_shape</code>: If False, allows the variable to be initialized with a value of unknown shape. If True, the default, the shape of initial_value must be known.</li> <li>\n<code>custom_getter</code>: Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. The signature of <code>custom_getter</code> should match that of this method, but the most future-proof version will allow for changes: <code>def custom_getter(getter, *args, **kwargs)</code>. Direct access to all <code>get_variable</code> parameters is also allowed: <code>def custom_getter(getter, name, *args, **kwargs)</code>. A simple identity custom getter that simply creates variables with modified names is: <code>python\ndef custom_getter(getter, name, *args, **kwargs):\n  return getter(name + '_suffix', *args, **kwargs)</code>\n</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>The created or existing variable.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: when creating a new variable and shape is not declared, or when violating reuse during variable creation. Reuse is set inside <code>variable_scope</code>.</li> </ul>  <h3 id=\"VariableScope\"><code>class tf.VariableScope</code></h3> <p>Variable scope object to carry defaults to provide to get_variable.</p> <p>Many of the arguments we need for get_variable in a variable store are most easily handled with a context. This object is used for the defaults.</p> <p>Attributes: name: name of the current scope, used as prefix in get_variable. initializer: default initializer passed to get_variable. regularizer: default regularizer passed to get_variable. reuse: Boolean or None, setting the reuse in get_variable. caching_device: string, callable, or None: the caching device passed to get_variable. partitioner: callable or <code>None</code>: the partitioner passed to <code>get_variable</code>. custom_getter: default custom getter passed to get_variable. name_scope: The name passed to <code>tf.name_scope</code>. dtype: default type passed to get_variable (defaults to DT_FLOAT).</p>  <h4 id=\"VariableScope.__init__\"><code>tf.VariableScope.__init__(reuse, name='', initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, name_scope='', dtype=tf.float32)</code></h4> <p>Creates a new VariableScope with the given properties.</p>  <h4 id=\"VariableScope.caching_device\"><code>tf.VariableScope.caching_device</code></h4>  <h4 id=\"VariableScope.custom_getter\"><code>tf.VariableScope.custom_getter</code></h4>  <h4 id=\"VariableScope.dtype\"><code>tf.VariableScope.dtype</code></h4>  <h4 id=\"VariableScope.get_variable\"><code>tf.VariableScope.get_variable(var_store, name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, custom_getter=None)</code></h4> <p>Gets an existing variable with this name or create a new one.</p>  <h4 id=\"VariableScope.initializer\"><code>tf.VariableScope.initializer</code></h4>  <h4 id=\"VariableScope.name\"><code>tf.VariableScope.name</code></h4>  <h4 id=\"VariableScope.original_name_scope\"><code>tf.VariableScope.original_name_scope</code></h4>  <h4 id=\"VariableScope.partitioner\"><code>tf.VariableScope.partitioner</code></h4>  <h4 id=\"VariableScope.regularizer\"><code>tf.VariableScope.regularizer</code></h4>  <h4 id=\"VariableScope.reuse\"><code>tf.VariableScope.reuse</code></h4>  <h4 id=\"VariableScope.reuse_variables\"><code>tf.VariableScope.reuse_variables()</code></h4> <p>Reuse variables in this scope.</p>  <h4 id=\"VariableScope.set_caching_device\"><code>tf.VariableScope.set_caching_device(caching_device)</code></h4> <p>Set caching_device for this scope.</p>  <h4 id=\"VariableScope.set_custom_getter\"><code>tf.VariableScope.set_custom_getter(custom_getter)</code></h4> <p>Set custom getter for this scope.</p>  <h4 id=\"VariableScope.set_dtype\"><code>tf.VariableScope.set_dtype(dtype)</code></h4> <p>Set data type for this scope.</p>  <h4 id=\"VariableScope.set_initializer\"><code>tf.VariableScope.set_initializer(initializer)</code></h4> <p>Set initializer for this scope.</p>  <h4 id=\"VariableScope.set_partitioner\"><code>tf.VariableScope.set_partitioner(partitioner)</code></h4> <p>Set partitioner for this scope.</p>  <h4 id=\"VariableScope.set_regularizer\"><code>tf.VariableScope.set_regularizer(regularizer)</code></h4> <p>Set regularizer for this scope.</p>  <h3 id=\"variable_scope\"><code>tf.variable_scope(name_or_scope, reuse=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, dtype=None)</code></h3> <p>Returns a context for variable scope.</p> <p>Variable scope allows to create new variables and to share already created ones while providing checks to not create or share by accident. For details, see the <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/variable_scope/index.html\">Variable Scope How To</a>, here we present only a few basic examples.</p> <p>Simple example of how to create a new variable:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.variable_scope(\"foo\"):\n    with tf.variable_scope(\"bar\"):\n        v = tf.get_variable(\"v\", [1])\n        assert v.name == \"foo/bar/v:0\"\n</pre> <p>Basic example of sharing a variable:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.variable_scope(\"foo\"):\n    v = tf.get_variable(\"v\", [1])\nwith tf.variable_scope(\"foo\", reuse=True):\n    v1 = tf.get_variable(\"v\", [1])\nassert v1 == v\n</pre> <p>Sharing a variable by capturing a scope and setting reuse:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.variable_scope(\"foo\") as scope:\n    v = tf.get_variable(\"v\", [1])\n    scope.reuse_variables()\n    v1 = tf.get_variable(\"v\", [1])\nassert v1 == v\n</pre> <p>To prevent accidental sharing of variables, we raise an exception when getting an existing variable in a non-reusing scope.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.variable_scope(\"foo\"):\n    v = tf.get_variable(\"v\", [1])\n    v1 = tf.get_variable(\"v\", [1])\n    #  Raises ValueError(\"... v already exists ...\").\n</pre> <p>Similarly, we raise an exception when trying to get a variable that does not exist in reuse mode.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.variable_scope(\"foo\", reuse=True):\n    v = tf.get_variable(\"v\", [1])\n    #  Raises ValueError(\"... v does not exists ...\").\n</pre> <p>Note that the <code>reuse</code> flag is inherited: if we open a reusing scope, then all its sub-scopes become reusing as well.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>name_or_scope</code>: <code>string</code> or <code>VariableScope</code>: the scope to open.</li> <li>\n<code>reuse</code>: <code>True</code> or <code>None</code>; if <code>True</code>, we go into reuse mode for this scope as well as all sub-scopes; if <code>None</code>, we just inherit the parent scope reuse.</li> <li>\n<code>initializer</code>: default initializer for variables within this scope.</li> <li>\n<code>regularizer</code>: default regularizer for variables within this scope.</li> <li>\n<code>caching_device</code>: default caching device for variables within this scope.</li> <li>\n<code>partitioner</code>: default partitioner for variables within this scope.</li> <li>\n<code>custom_getter</code>: default custom getter for variables within this scope.</li> <li>\n<code>dtype</code>: type of variables created in this scope (defaults to the type in the passed scope, or inherited from parent scope).</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>A scope that can be to captured and reused.</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: when trying to reuse within a create scope, or create within a reuse scope, or if reuse is not <code>None</code> or <code>True</code>.</li> <li>\n<code>TypeError</code>: when the types of some arguments are not appropriate.</li> </ul>  <h3 id=\"variable_op_scope\"><code>tf.variable_op_scope(values, name_or_scope, default_name=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None)</code></h3> <p>Returns a context manager for defining an op that creates variables.</p> <p>This context manager validates that the given <code>values</code> are from the same graph, ensures that graph is the default graph, and pushes a name scope and a variable scope.</p> <p>If <code>name_or_scope</code> is not None, it is used as is in the variable scope. If <code>scope</code> is None, then <code>default_name</code> is used. In that case, if the same name has been previously used in the same scope, it will made unique be appending <code>_N</code> to it.</p> <p>This is intended to be used when defining generic ops and so reuse is always inherited.</p> <p>For example, to define a new Python op called <code>my_op_with_vars</code>:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def my_op_with_vars(a, b, scope=None):\n  with tf.variable_op_scope([a, b], scope, \"MyOp\") as scope:\n    a = tf.convert_to_tensor(a, name=\"a\")\n    b = tf.convert_to_tensor(b, name=\"b\")\n    c = tf.get_variable('c')\n    # Define some computation that uses `a`, `b`, and `c`.\n    return foo_op(..., name=scope)\n</pre>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>values</code>: The list of <code>Tensor</code> arguments that are passed to the op function.</li> <li>\n<code>name_or_scope</code>: The name argument that is passed to the op function, this name_or_scope is not uniquified in the variable scope.</li> <li>\n<code>default_name</code>: The default name to use if the <code>name_or_scope</code> argument is <code>None</code>, this name will be uniquified. If name_or_scope is provided it won't be used and therefore it is not required and can be None.</li> <li>\n<code>initializer</code>: The default initializer to pass to variable scope.</li> <li>\n<code>regularizer</code>: The default regularizer for variables within this scope.</li> <li>\n<code>caching_device</code>: The default caching device for variables within this scope.</li> <li>\n<code>partitioner</code>: The default partitioner for variables within this scope.</li> <li>\n<code>custom_getter</code>: The default custom getter for variables within this scope.</li> <li>\n<code>reuse</code>: <code>True</code> or <code>None</code>; if <code>True</code>, we go into reuse mode for this scope as well as all sub-scopes; if <code>None</code>, we just inherit the parent scope reuse.</li> <li>\n<code>dtype</code>: The default type of variables created in this scope, defaults to the type of the parent scope.</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>A context manager for use in defining a Python op.</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: when trying to reuse within a create scope, or create within a reuse scope, or if reuse is not <code>None</code> or <code>True</code>.</li> <li>\n<code>TypeError</code>: when the types of some arguments are not appropriate.</li> </ul>  <h3 id=\"get_variable_scope\"><code>tf.get_variable_scope()</code></h3> <p>Returns the current variable scope.</p>  <h3 id=\"make_template\"><code>tf.make_template(name_, func_, create_scope_now_=False, unique_name_=None, **kwargs)</code></h3> <p>Given an arbitrary function, wrap it so that it does variable sharing.</p> <p>This wraps <code>func_</code> in a Template and partially evaluates it. Templates are functions that create variables the first time they are called and reuse them thereafter. In order for <code>func_</code> to be compatible with a <code>Template</code> it must have the following properties:</p> <ul> <li>The function should create all trainable variables and any variables that should be reused by calling <code>tf.get_variable</code>. If a trainable variable is created using <code>tf.Variable</code>, then a ValueError will be thrown. Variables that are intended to be locals can be created by specifying <code>tf.Variable(..., trainable=false)</code>.</li> <li>The function may use variable scopes and other templates internally to create and reuse variables, but it shouldn't use <code>tf.get_variables</code> to capture variables that are defined outside of the scope of the function.</li> <li>Internal scopes and variable names should not depend on any arguments that are not supplied to <code>make_template</code>. In general you will get a ValueError telling you that you are trying to reuse a variable that doesn't exist if you make a mistake.</li> </ul> <p>In the following example, both <code>z</code> and <code>w</code> will be scaled by the same <code>y</code>. It is important to note that if we didn't assign <code>scalar_name</code> and used a different name for z and w that a <code>ValueError</code> would be thrown because it couldn't reuse the variable.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def my_op(x, scalar_name):\n  var1 = tf.get_variable(scalar_name,\n                         shape=[],\n                         initializer=tf.constant_initializer(1))\n  return x * var1\n\nscale_by_y = tf.make_template('scale_by_y', my_op, scalar_name='y')\n\nz = scale_by_y(input1)\nw = scale_by_y(input2)\n</pre> <p>As a safe-guard, the returned function will raise a <code>ValueError</code> after the first call if trainable variables are created by calling <code>tf.Variable</code>.</p> <p>If all of these are true, then 2 properties are enforced by the template:</p> <ol> <li>Calling the same template multiple times will share all non-local variables.</li> <li>Two different templates are guaranteed to be unique, unless you reenter the same variable scope as the initial definition of a template and redefine it. An examples of this exception:</li> </ol> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def my_op(x, scalar_name):\n  var1 = tf.get_variable(scalar_name,\n                         shape=[],\n                         initializer=tf.constant_initializer(1))\n  return x * var1\n\nwith tf.variable_scope('scope') as vs:\n  scale_by_y = tf.make_template('scale_by_y', my_op, scalar_name='y')\n  z = scale_by_y(input1)\n  w = scale_by_y(input2)\n\n# Creates a template that reuses the variables above.\nwith tf.variable_scope(vs, reuse=True):\n  scale_by_y2 = tf.make_template('scale_by_y', my_op, scalar_name='y')\n  z2 = scale_by_y2(input1)\n  w2 = scale_by_y2(input2)\n</pre> <p>Depending on the value of <code>create_scope_now_</code>, the full variable scope may be captured either at the time of first call or at the time of construction. If this option is set to True, then all Tensors created by repeated calls to the template will have an extra trailing _N+1 to their name, as the first time the scope is entered in the Template constructor no Tensors are created.</p> <p>Note: <code>name_</code>, <code>func_</code> and <code>create_scope_now_</code> have a trailing underscore to reduce the likelihood of collisions with kwargs.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>name_</code>: A name for the scope created by this template. If necessary, the name will be made unique by appending <code>_N</code> to the name.</li> <li>\n<code>func_</code>: The function to wrap.</li> <li>\n<code>create_scope_now_</code>: Boolean controlling whether the scope should be created when the template is constructed or when the template is called. Default is False, meaning the scope is created when the template is called.</li> <li>\n<code>unique_name_</code>: When used, it overrides name_ and is not made unique. If a template of the same scope/unique_name already exists and reuse is false, an error is raised. Defaults to None.</li> <li>\n<code>**kwargs</code>: Keyword arguments to apply to <code>func_</code>.</li> </ul>  <h5 id=\"returns-34\">Returns:</h5> <p>A function to encapsulate a set of variables which should be created once and reused. An enclosing scope will created, either where <code>make_template</code> is called, or wherever the result is called, depending on the value of <code>create_scope_now_</code>. Regardless of the value, the first time the template is called it will enter the scope with no reuse, and call <code>func_</code> to create variables, which are guaranteed to be unique. All subsequent calls will re-enter the scope and reuse those variables.</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the name is None.</li> </ul>  <h3 id=\"no_regularizer\"><code>tf.no_regularizer(_)</code></h3> <p>Use this function to prevent regularization of variables.</p>  <h3 id=\"constant_initializer\"><code>tf.constant_initializer(value=0.0, dtype=tf.float32)</code></h3> <p>Returns an initializer that generates tensors with a single value.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>value</code>: A Python scalar. All elements of the initialized variable will be set to this value.</li> <li>\n<code>dtype</code>: The data type. Only floating point types are supported.</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>An initializer that generates tensors with a single value.</p>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>dtype</code> is not a floating point type.</li> </ul>  <h3 id=\"random_normal_initializer\"><code>tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)</code></h3> <p>Returns an initializer that generates tensors with a normal distribution.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>mean</code>: a python scalar or a scalar tensor. Mean of the random values to generate.</li> <li>\n<code>stddev</code>: a python scalar or a scalar tensor. Standard deviation of the random values to generate.</li> <li>\n<code>seed</code>: A Python integer. Used to create random seeds. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>dtype</code>: The data type. Only floating point types are supported.</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p>An initializer that generates tensors with a normal distribution.</p>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>dtype</code> is not a floating point type.</li> </ul>  <h3 id=\"truncated_normal_initializer\"><code>tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)</code></h3> <p>Returns an initializer that generates a truncated normal distribution.</p> <p>These values are similar to values from a <code>random_normal_initializer</code> except that values more than two standard deviations from the mean are discarded and re-drawn. This is the recommended initializer for neural network weights and filters.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>mean</code>: a python scalar or a scalar tensor. Mean of the random values to generate.</li> <li>\n<code>stddev</code>: a python scalar or a scalar tensor. Standard deviation of the random values to generate.</li> <li>\n<code>seed</code>: A Python integer. Used to create random seeds. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>dtype</code>: The data type. Only floating point types are supported.</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <p>An initializer that generates tensors with a truncated normal distribution.</p>  <h5 id=\"raises-16\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>dtype</code> is not a floating point type.</li> </ul>  <h3 id=\"random_uniform_initializer\"><code>tf.random_uniform_initializer(minval=0.0, maxval=1.0, seed=None, dtype=tf.float32)</code></h3> <p>Returns an initializer that generates tensors with a uniform distribution.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>minval</code>: a python scalar or a scalar tensor. lower bound of the range of random values to generate.</li> <li>\n<code>maxval</code>: a python scalar or a scalar tensor. upper bound of the range of random values to generate.</li> <li>\n<code>seed</code>: A Python integer. Used to create random seeds. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>dtype</code>: The data type. Only floating point types are supported.</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>An initializer that generates tensors with a uniform distribution.</p>  <h5 id=\"raises-17\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>dtype</code> is not a floating point type.</li> </ul>  <h3 id=\"uniform_unit_scaling_initializer\"><code>tf.uniform_unit_scaling_initializer(factor=1.0, seed=None, dtype=tf.float32, full_shape=None)</code></h3> <p>Returns an initializer that generates tensors without scaling variance.</p> <p>When initializing a deep network, it is in principle advantageous to keep the scale of the input variance constant, so it does not explode or diminish by reaching the final layer. If the input is <code>x</code> and the operation <code>x * W</code>, and we want to initialize <code>W</code> uniformly at random, we need to pick <code>W</code> from</p> <pre class=\"\">[-sqrt(3) / sqrt(dim), sqrt(3) / sqrt(dim)]\n</pre> <p>to keep the scale intact, where <code>dim = W.shape[0]</code> (the size of the input). A similar calculation for convolutional networks gives an analogous result with <code>dim</code> equal to the product of the first 3 dimensions. When nonlinearities are present, we need to multiply this by a constant <code>factor</code>. See <a href=\"https://arxiv.org/abs/1412.6558\" rel=\"noreferrer\">Sussillo et al., 2014</a> (<a href=\"http://arxiv.org/pdf/1412.6558.pdf\" rel=\"noreferrer\">pdf</a>) for deeper motivation, experiments and the calculation of constants. In section 2.3 there, the constants were numerically computed: for a linear layer it's 1.0, relu: ~1.43, tanh: ~1.15.</p> <p>If the shape tuple <code>full_shape</code> is provided, the scale will be calculated from this predefined shape. This is useful when a <code>Variable</code> is being partitioned across several shards, and each shard has a smaller shape than the whole. Since the shards are usually concatenated when used, the scale should be based on the shape of the whole.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>factor</code>: Float. A multiplicative factor by which the values will be scaled.</li> <li>\n<code>seed</code>: A Python integer. Used to create random seeds. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>dtype</code>: The data type. Only floating point types are supported.</li> <li>\n<code>full_shape</code>: Tuple or list of integers. The shape used for calculating scale normalization (instead of the shape passed at creation time). Useful when creating sharded variables via partitioning.</li> </ul>  <h5 id=\"returns-39\">Returns:</h5> <p>An initializer that generates tensors with unit variance.</p>  <h5 id=\"raises-18\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>dtype</code> is not a floating point type.</li> </ul>  <h3 id=\"zeros_initializer\"><code>tf.zeros_initializer(shape, dtype=tf.float32)</code></h3> <p>An adaptor for zeros() to match the Initializer spec.</p>  <h3 id=\"ones_initializer\"><code>tf.ones_initializer(shape, dtype=tf.float32)</code></h3> <p>An adaptor for ones() to match the Initializer spec.</p>  <h2 id=\"variable-partitioners-for-sharding\">Variable Partitioners for Sharding</h2>  <h3 id=\"variable_axis_size_partitioner\"><code>tf.variable_axis_size_partitioner(max_shard_bytes, axis=0, bytes_per_string_element=16, max_shards=None)</code></h3> <p>Get a partitioner for VariableScope to keep shards below <code>max_shard_bytes</code>.</p> <p>This partitioner will shard a Variable along one axis, attempting to keep the maximum shard size below <code>max_shard_bytes</code>. In practice, this is not always possible when sharding along only one axis. When this happens, this axis is sharded as much as possible (i.e., every dimension becomes a separate shard).</p> <p>If the partitioner hits the <code>max_shards</code> limit, then each shard may end up larger than <code>max_shard_bytes</code>. By default <code>max_shards</code> equals <code>None</code> and no limit on the number of shards is enforced.</p> <p>One reasonable value for <code>max_shard_bytes</code> is <code>(64 &lt;&lt; 20) - 1</code>, or almost <code>64MB</code>, to keep below the protobuf byte limit.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>max_shard_bytes</code>: The maximum size any given shard is allowed to be.</li> <li>\n<code>axis</code>: The axis to partition along. Default: outermost axis.</li> <li>\n<code>bytes_per_string_element</code>: If the <code>Variable</code> is of type string, this provides an estimate of how large each scalar in the <code>Variable</code> is.</li> <li>\n<code>max_shards</code>: The maximum number of shards in int created taking precedence over <code>max_shard_bytes</code>.</li> </ul>  <h5 id=\"returns-40\">Returns:</h5> <p>A partition function usable as the <code>partitioner</code> argument to <code>variable_scope</code>, <code>get_variable</code>, and <code>get_partitioned_variable_list</code>.</p>  <h5 id=\"raises-19\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If any of the byte counts are non-positive.</li> </ul>  <h3 id=\"min_max_variable_partitioner\"><code>tf.min_max_variable_partitioner(max_partitions=1, axis=0, min_slice_size=262144, bytes_per_string_element=16)</code></h3> <p>Partitioner to allocate minimum size per slice.</p> <p>Returns a partitioner that partitions the variable of given shape and dtype such that each partition has a minimum of <code>min_slice_size</code> slice of the variable. The maximum number of such partitions (upper bound) is given by <code>max_partitions</code>.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>max_partitions</code>: Upper bound on the number of partitions. Defaults to 1.</li> <li>\n<code>axis</code>: Axis along which to partition the variable. Defaults to 0.</li> <li>\n<code>min_slice_size</code>: Minimum size of the variable slice per partition. Defaults to 256K.</li> <li>\n<code>bytes_per_string_element</code>: If the <code>Variable</code> is of type string, this provides an estimate of how large each scalar in the <code>Variable</code> is.</li> </ul>  <h5 id=\"returns-41\">Returns:</h5> <p>A partition function usable as the <code>partitioner</code> argument to <code>variable_scope</code>, <code>get_variable</code>, and <code>get_partitioned_variable_list</code>.</p>  <h2 id=\"sparse-variable-updates\">Sparse Variable Updates</h2> <p>The sparse update ops modify a subset of the entries in a dense <code>Variable</code>, either overwriting the entries or adding / subtracting a delta. These are useful for training embedding models and similar lookup-based networks, since only a small subset of embedding vectors change in any given step.</p> <p>Since a sparse update of a large tensor may be generated automatically during gradient computation (as in the gradient of <a href=\"array_ops#gather\"><code>tf.gather</code></a>), an <a href=\"#IndexedSlices\"><code>IndexedSlices</code></a> class is provided that encapsulates a set of sparse indices and values. <code>IndexedSlices</code> objects are detected and handled automatically by the optimizers in most cases.</p>  <h3 id=\"scatter_update\"><code>tf.scatter_update(ref, indices, updates, use_locking=None, name=None)</code></h3> <p>Applies sparse updates to a variable reference.</p> <p>This operation computes</p> <pre class=\"\"># Scalar indices\nref[indices, ...] = updates[...]\n\n# Vector indices (for each i)\nref[indices[i], ...] = updates[i, ...]\n\n# High rank indices (for each i, ..., j)\nref[indices[i, ..., j], ...] = updates[i, ..., j, ...]\n</pre> <p>This operation outputs <code>ref</code> after the update is done. This makes it easier to chain operations that need to use the reset value.</p> <p>If values in <code>ref</code> is to be updated more than once, because there are duplicate entires in <code>indices</code>, the order at which the updates happen for each value is undefined.</p> <p>Requires <code>updates.shape = indices.shape + ref.shape[1:]</code>.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/ScatterUpdate.png\" alt> </div>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>ref</code>: A mutable <code>Tensor</code>. Should be from a <code>Variable</code> node.</li> <li>\n<code>indices</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A tensor of indices into the first dimension of <code>ref</code>.</li> <li>\n<code>updates</code>: A <code>Tensor</code>. Must have the same type as <code>ref</code>. A tensor of updated values to store in <code>ref</code>.</li> <li>\n<code>use_locking</code>: An optional <code>bool</code>. Defaults to <code>True</code>. If True, the assignment will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-42\">Returns:</h5> <p>Same as <code>ref</code>. Returned as a convenience for operations that want to use the updated values after the update is done.</p>  <h3 id=\"scatter_add\"><code>tf.scatter_add(ref, indices, updates, use_locking=None, name=None)</code></h3> <p>Adds sparse updates to a variable reference.</p> <p>This operation computes</p> <pre class=\"\"># Scalar indices\nref[indices, ...] += updates[...]\n\n# Vector indices (for each i)\nref[indices[i], ...] += updates[i, ...]\n\n# High rank indices (for each i, ..., j)\nref[indices[i, ..., j], ...] += updates[i, ..., j, ...]\n</pre> <p>This operation outputs <code>ref</code> after the update is done. This makes it easier to chain operations that need to use the reset value.</p> <p>Duplicate entries are handled correctly: if multiple <code>indices</code> reference the same location, their contributions add.</p> <p>Requires <code>updates.shape = indices.shape + ref.shape[1:]</code>.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/ScatterAdd.png\" alt> </div>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>ref</code>: A mutable <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>. Should be from a <code>Variable</code> node.</li> <li>\n<code>indices</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A tensor of indices into the first dimension of <code>ref</code>.</li> <li>\n<code>updates</code>: A <code>Tensor</code>. Must have the same type as <code>ref</code>. A tensor of updated values to add to <code>ref</code>.</li> <li>\n<code>use_locking</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If True, the addition will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-43\">Returns:</h5> <p>Same as <code>ref</code>. Returned as a convenience for operations that want to use the updated values after the update is done.</p>  <h3 id=\"scatter_sub\"><code>tf.scatter_sub(ref, indices, updates, use_locking=None, name=None)</code></h3> <p>Subtracts sparse updates to a variable reference.</p> <pre class=\"\"># Scalar indices\nref[indices, ...] -= updates[...]\n\n# Vector indices (for each i)\nref[indices[i], ...] -= updates[i, ...]\n\n# High rank indices (for each i, ..., j)\nref[indices[i, ..., j], ...] -= updates[i, ..., j, ...]\n</pre> <p>This operation outputs <code>ref</code> after the update is done. This makes it easier to chain operations that need to use the reset value.</p> <p>Duplicate entries are handled correctly: if multiple <code>indices</code> reference the same location, their (negated) contributions add.</p> <p>Requires <code>updates.shape = indices.shape + ref.shape[1:]</code>.</p> <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\"> <img style=\"width:100%\" src=\"https://www.tensorflow.org/versions/r0.10/images/ScatterSub.png\" alt> </div>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>ref</code>: A mutable <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>. Should be from a <code>Variable</code> node.</li> <li>\n<code>indices</code>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>. A tensor of indices into the first dimension of <code>ref</code>.</li> <li>\n<code>updates</code>: A <code>Tensor</code>. Must have the same type as <code>ref</code>. A tensor of updated values to subtract from <code>ref</code>.</li> <li>\n<code>use_locking</code>: An optional <code>bool</code>. Defaults to <code>False</code>. If True, the subtraction will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-44\">Returns:</h5> <p>Same as <code>ref</code>. Returned as a convenience for operations that want to use the updated values after the update is done.</p>  <h3 id=\"sparse_mask\"><code>tf.sparse_mask(a, mask_indices, name=None)</code></h3> <p>Masks elements of <code>IndexedSlices</code>.</p> <p>Given an <code>IndexedSlices</code> instance <code>a</code>, returns another <code>IndexedSlices</code> that contains a subset of the slices of <code>a</code>. Only the slices at indices not specified in <code>mask_indices</code> are returned.</p> <p>This is useful when you need to extract a subset of slices in an <code>IndexedSlices</code> object.</p> <p>For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># `a` contains slices at indices [12, 26, 37, 45] from a large tensor\n# with shape [1000, 10]\na.indices =&gt; [12, 26, 37, 45]\ntf.shape(a.values) =&gt; [4, 10]\n\n# `b` will be the subset of `a` slices at its second and third indices, so\n# we want to mask its first and last indices (which are at absolute\n# indices 12, 45)\nb = tf.sparse_mask(a, [12, 45])\n\nb.indices =&gt; [26, 37]\ntf.shape(b.values) =&gt; [2, 10]\n\n</pre>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>a</code>: An <code>IndexedSlices</code> instance.</li> <li>\n<code>mask_indices</code>: Indices of elements to mask.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-45\">Returns:</h5> <p>The masked <code>IndexedSlices</code> instance.</p>  <h3 id=\"IndexedSlices\"><code>class tf.IndexedSlices</code></h3> <p>A sparse representation of a set of tensor slices at given indices.</p> <p>This class is a simple wrapper for a pair of <code>Tensor</code> objects:</p> <ul> <li>\n<code>values</code>: A <code>Tensor</code> of any dtype with shape <code>[D0, D1, ..., Dn]</code>.</li> <li>\n<code>indices</code>: A 1-D integer <code>Tensor</code> with shape <code>[D0]</code>.</li> </ul> <p>An <code>IndexedSlices</code> is typically used to represent a subset of a larger tensor <code>dense</code> of shape <code>[LARGE0, D1, .. , DN]</code> where <code>LARGE0 &gt;&gt; D0</code>. The values in <code>indices</code> are the indices in the first dimension of the slices that have been extracted from the larger tensor.</p> <p>The dense tensor <code>dense</code> represented by an <code>IndexedSlices</code> <code>slices</code> has</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">dense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]\n</pre> <p>The <code>IndexedSlices</code> class is used principally in the definition of gradients for operations that have sparse gradients (e.g. <a href=\"array_ops#gather\"><code>tf.gather</code></a>).</p> <p>Contrast this representation with <a href=\"sparse_ops#SparseTensor\"><code>SparseTensor</code></a>, which uses multi-dimensional indices and scalar values.</p>  <h4 id=\"IndexedSlices.__init__\"><code>tf.IndexedSlices.__init__(values, indices, dense_shape=None)</code></h4> <p>Creates an <code>IndexedSlices</code>.</p>  <h4 id=\"IndexedSlices.values\"><code>tf.IndexedSlices.values</code></h4> <p>A <code>Tensor</code> containing the values of the slices.</p>  <h4 id=\"IndexedSlices.indices\"><code>tf.IndexedSlices.indices</code></h4> <p>A 1-D <code>Tensor</code> containing the indices of the slices.</p>  <h4 id=\"IndexedSlices.dense_shape\"><code>tf.IndexedSlices.dense_shape</code></h4> <p>A 1-D <code>Tensor</code> containing the shape of the corresponding dense tensor.</p>  <h4 id=\"IndexedSlices.name\"><code>tf.IndexedSlices.name</code></h4> <p>The name of this <code>IndexedSlices</code>.</p>  <h4 id=\"IndexedSlices.dtype\"><code>tf.IndexedSlices.dtype</code></h4> <p>The <code>DType</code> of elements in this tensor.</p>  <h4 id=\"IndexedSlices.device\"><code>tf.IndexedSlices.device</code></h4> <p>The name of the device on which <code>values</code> will be produced, or <code>None</code>.</p>  <h4 id=\"IndexedSlices.op\"><code>tf.IndexedSlices.op</code></h4> <p>The <code>Operation</code> that produces <code>values</code> as an output.</p>  <h4 id=\"other-methods-3\">Other Methods</h4>  <h4 id=\"IndexedSlices.graph\"><code>tf.IndexedSlices.graph</code></h4> <p>The <code>Graph</code> that contains the values, indices, and shape tensors.</p>  <h2 id=\"exporting-and-importing-meta-graphs\">Exporting and Importing Meta Graphs</h2>  <h3 id=\"export_meta_graph\"><code>tf.train.export_meta_graph(filename=None, meta_info_def=None, graph_def=None, saver_def=None, collection_list=None, as_text=False)</code></h3> <p>Returns <code>MetaGraphDef</code> proto. Optionally writes it to filename.</p> <p>This function exports the graph, saver, and collection objects into <code>MetaGraphDef</code> protocol buffer with the intension of it being imported at a later time or location to restart training, run inference, or be a subgraph.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>filename</code>: Optional filename including the path for writing the generated <code>MetaGraphDef</code> protocol buffer.</li> <li>\n<code>meta_info_def</code>: <code>MetaInfoDef</code> protocol buffer.</li> <li>\n<code>graph_def</code>: <code>GraphDef</code> protocol buffer.</li> <li>\n<code>saver_def</code>: <code>SaverDef</code> protocol buffer.</li> <li>\n<code>collection_list</code>: List of string keys to collect.</li> <li>\n<code>as_text</code>: If <code>True</code>, writes the <code>MetaGraphDef</code> as an ASCII proto.</li> </ul>  <h5 id=\"returns-46\">Returns:</h5> <p>A <code>MetaGraphDef</code> proto.</p>  <h3 id=\"import_meta_graph\"><code>tf.train.import_meta_graph(meta_graph_or_file)</code></h3> <p>Recreates a Graph saved in a <code>MetaGraphDef</code> proto.</p> <p>This function takes a <code>MetaGraphDef</code> protocol buffer as input. If the argument is a file containing a <code>MetaGraphDef</code> protocol buffer , it constructs a protocol buffer from the file content. The function then adds all the nodes from the <code>graph_def</code> field to the current graph, recreates all the collections, and returns a saver constructed from the <code>saver_def</code> field.</p> <p>In combination with <code>export_meta_graph()</code>, this function can be used to</p> <ul> <li><p>Serialize a graph along with other Python objects such as <code>QueueRunner</code>, <code>Variable</code> into a <code>MetaGraphDef</code>.</p></li> <li><p>Restart training from a saved graph and checkpoints.</p></li> <li><p>Run inference from a saved graph and checkpoints.</p></li> </ul> <pre class=\"lang-Python no-auto-prettify\">...\n# Create a saver.\nsaver = tf.train.Saver(...variables...)\n# Remember the training_op we want to run by adding it to a collection.\ntf.add_to_collection('train_op', train_op)\nsess = tf.Session()\nfor step in xrange(1000000):\n    sess.run(train_op)\n    if step % 1000 == 0:\n        # Saves checkpoint, which by default also exports a meta_graph\n        # named 'my-model-global_step.meta'.\n        saver.save(sess, 'my-model', global_step=step)\n</pre> <p>Later we can continue training from this saved <code>meta_graph</code> without building the model from scratch.</p> <pre class=\"lang-Python no-auto-prettify\">with tf.Session() as sess:\n  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\n  new_saver.restore(sess, 'my-save-dir/my-model-10000')\n  # tf.get_collection() returns a list. In this example we only want the\n  # first one.\n  train_op = tf.get_collection('train_op')[0]\n  for step in xrange(1000000):\n    sess.run(train_op)\n</pre> <p>NOTE: Restarting training from saved <code>meta_graph</code> only works if the device assignments have not changed.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>meta_graph_or_file</code>: <code>MetaGraphDef</code> protocol buffer or filename (including the path) containing a <code>MetaGraphDef</code>.</li> </ul>  <h5 id=\"returns-47\">Returns:</h5> <p>A saver constructed from <code>saver_def</code> in <code>MetaGraphDef</code> or None.</p> <p>A None value is returned if no variables exist in the <code>MetaGraphDef</code> (i.e., there are no variables to restore).</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/state_ops.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/state_ops.html</a>\n  </p>\n</div>\n","train":"<h1 id=\"training\">Training</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#training\">Training</a></li> <ul> <li><a href=\"#optimizers\">Optimizers</a></li> <ul> <li><a href=\"#Optimizer\"><code>class tf.train.Optimizer</code></a></li> <li><a href=\"#usage\">Usage</a></li> <li><a href=\"#processing-gradients-before-applying-them\">Processing gradients before applying them.</a></li> <li><a href=\"#gating-gradients\">Gating Gradients</a></li> <li><a href=\"#slots\">Slots</a></li> <li><a href=\"#GradientDescentOptimizer\"><code>class tf.train.GradientDescentOptimizer</code></a></li> <li><a href=\"#AdadeltaOptimizer\"><code>class tf.train.AdadeltaOptimizer</code></a></li> <li><a href=\"#AdagradOptimizer\"><code>class tf.train.AdagradOptimizer</code></a></li> <li><a href=\"#MomentumOptimizer\"><code>class tf.train.MomentumOptimizer</code></a></li> <li><a href=\"#AdamOptimizer\"><code>class tf.train.AdamOptimizer</code></a></li> <li><a href=\"#FtrlOptimizer\"><code>class tf.train.FtrlOptimizer</code></a></li> <li><a href=\"#RMSPropOptimizer\"><code>class tf.train.RMSPropOptimizer</code></a></li> </ul> <li><a href=\"#gradient-computation\">Gradient Computation</a></li> <ul> <li><a href=\"#gradients\"><code>tf.gradients(ys, xs, grad_ys=None, name=gradients, colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)</code></a></li> <li><a href=\"#AggregationMethod\"><code>class tf.AggregationMethod</code></a></li> <li><a href=\"#stop_gradient\"><code>tf.stop_gradient(input, name=None)</code></a></li> </ul> <li><a href=\"#gradient-clipping\">Gradient Clipping</a></li> <ul> <li><a href=\"#clip_by_value\"><code>tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)</code></a></li> <li><a href=\"#clip_by_norm\"><code>tf.clip_by_norm(t, clip_norm, axes=None, name=None)</code></a></li> <li><a href=\"#clip_by_average_norm\"><code>tf.clip_by_average_norm(t, clip_norm, name=None)</code></a></li> <li><a href=\"#clip_by_global_norm\"><code>tf.clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None)</code></a></li> <li><a href=\"#global_norm\"><code>tf.global_norm(t_list, name=None)</code></a></li> </ul> <li><a href=\"#decaying-the-learning-rate\">Decaying the learning rate</a></li> <ul> <li><a href=\"#exponential_decay\"><code>tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)</code></a></li> </ul> <li><a href=\"#moving-averages\">Moving Averages</a></li> <ul> <li><a href=\"#ExponentialMovingAverage\"><code>class tf.train.ExponentialMovingAverage</code></a></li> </ul> <li><a href=\"#coordinator-and-queuerunner\">Coordinator and QueueRunner</a></li> <ul> <li><a href=\"#Coordinator\"><code>class tf.train.Coordinator</code></a></li> <li><a href=\"#QueueRunner\"><code>class tf.train.QueueRunner</code></a></li> <li><a href=\"#add_queue_runner\"><code>tf.train.add_queue_runner(qr, collection=queue_runners)</code></a></li> <li><a href=\"#start_queue_runners\"><code>tf.train.start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection=queue_runners)</code></a></li> </ul> <li><a href=\"#distributed-execution\">Distributed execution</a></li> <ul> <li><a href=\"#Server\"><code>class tf.train.Server</code></a></li> <li><a href=\"#Supervisor\"><code>class tf.train.Supervisor</code></a></li> <li><a href=\"#SessionManager\"><code>class tf.train.SessionManager</code></a></li> <li><a href=\"#usage-2\">Usage:</a></li> <li><a href=\"#ClusterSpec\"><code>class tf.train.ClusterSpec</code></a></li> <li><a href=\"#replica_device_setter\"><code>tf.train.replica_device_setter(ps_tasks=0, ps_device=/job:ps, worker_device=/job:worker, merge_devices=True, cluster=None, ps_ops=None)</code></a></li> </ul> <li><a href=\"#summary-operations\">Summary Operations</a></li> <ul> <li><a href=\"#scalar_summary\"><code>tf.scalar_summary(tags, values, collections=None, name=None)</code></a></li> <li><a href=\"#image_summary\"><code>tf.image_summary(tag, tensor, max_images=3, collections=None, name=None)</code></a></li> <li><a href=\"#audio_summary\"><code>tf.audio_summary(tag, tensor, sample_rate, max_outputs=3, collections=None, name=None)</code></a></li> <li><a href=\"#histogram_summary\"><code>tf.histogram_summary(tag, values, collections=None, name=None)</code></a></li> <li><a href=\"#zero_fraction\"><code>tf.nn.zero_fraction(value, name=None)</code></a></li> <li><a href=\"#merge_summary\"><code>tf.merge_summary(inputs, collections=None, name=None)</code></a></li> <li><a href=\"#merge_all_summaries\"><code>tf.merge_all_summaries(key=summaries)</code></a></li> </ul> <li><a href=\"#adding-summaries-to-event-files\">Adding Summaries to Event Files</a></li> <ul> <li><a href=\"#SummaryWriter\"><code>class tf.train.SummaryWriter</code></a></li> <li><a href=\"#summary_iterator\"><code>tf.train.summary_iterator(path)</code></a></li> </ul> <li><a href=\"#training-utilities\">Training utilities</a></li> <ul> <li><a href=\"#global_step\"><code>tf.train.global_step(sess, global_step_tensor)</code></a></li> <li><a href=\"#write_graph\"><code>tf.train.write_graph(graph_def, logdir, name, as_text=True)</code></a></li> </ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#LooperThread\"><code>class tf.train.LooperThread</code></a></li> <li><a href=\"#do_quantize_training_on_graphdef\"><code>tf.train.do_quantize_training_on_graphdef(input_graph, num_bits)</code></a></li> <li><a href=\"#generate_checkpoint_state_proto\"><code>tf.train.generate_checkpoint_state_proto(save_dir, model_checkpoint_path, all_model_checkpoint_paths=None)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>This library provides a set of classes and functions that helps train models.</p> <h2 id=\"optimizers\">Optimizers</h2> <p>The Optimizer base class provides methods to compute gradients for a loss and apply gradients to variables. A collection of subclasses implement classic optimization algorithms such as GradientDescent and Adagrad.</p> <p>You never instantiate the Optimizer class itself, but instead instantiate one of the subclasses.</p>  <h3 id=\"Optimizer\"><code>class tf.train.Optimizer</code></h3> <p>Base class for optimizers.</p> <p>This class defines the API to add Ops to train a model. You never use this class directly, but instead instantiate one of its subclasses such as <code>GradientDescentOptimizer</code>, <code>AdagradOptimizer</code>, or <code>MomentumOptimizer</code>.</p> <h3 id=\"usage\">Usage</h3> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Create an optimizer with the desired parameters.\nopt = GradientDescentOptimizer(learning_rate=0.1)\n# Add Ops to the graph to minimize a cost by updating a list of variables.\n# \"cost\" is a Tensor, and the list of variables contains tf.Variable\n# objects.\nopt_op = opt.minimize(cost, var_list=&lt;list of variables&gt;)\n</pre> <p>In the training program you will just have to run the returned Op.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Execute opt_op to do one step of training:\nopt_op.run()\n</pre>  <h3 id=\"processing-gradients-before-applying-them\">Processing gradients before applying them.</h3> <p>Calling <code>minimize()</code> takes care of both computing the gradients and applying them to the variables. If you want to process the gradients before applying them you can instead use the optimizer in three steps:</p> <ol> <li>Compute the gradients with <code>compute_gradients()</code>.</li> <li>Process the gradients as you wish.</li> <li>Apply the processed gradients with <code>apply_gradients()</code>.</li> </ol> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Create an optimizer.\nopt = GradientDescentOptimizer(learning_rate=0.1)\n\n# Compute the gradients for a list of variables.\ngrads_and_vars = opt.compute_gradients(loss, &lt;list of variables&gt;)\n\n# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n# need to the 'gradient' part, for example cap them, etc.\ncapped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n\n# Ask the optimizer to apply the capped gradients.\nopt.apply_gradients(capped_grads_and_vars)\n</pre>  <h4 id=\"Optimizer.__init__\"><code>tf.train.Optimizer.__init__(use_locking, name)</code></h4> <p>Create a new Optimizer.</p> <p>This must be called by the constructors of subclasses.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>use_locking</code>: Bool. If True apply use locks to prevent concurrent updates to variables.</li> <li>\n<code>name</code>: A non-empty string. The name to use for accumulators created for the optimizer.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If name is malformed.</li> </ul>  <h4 id=\"Optimizer.minimize\"><code>tf.train.Optimizer.minimize(loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)</code></h4> <p>Add operations to minimize <code>loss</code> by updating <code>var_list</code>.</p> <p>This method simply combines calls <code>compute_gradients()</code> and <code>apply_gradients()</code>. If you want to process the gradient before applying them call <code>compute_gradients()</code> and <code>apply_gradients()</code> explicitly instead of using this function.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>loss</code>: A <code>Tensor</code> containing the value to minimize.</li> <li>\n<code>global_step</code>: Optional <code>Variable</code> to increment by one after the variables have been updated.</li> <li>\n<code>var_list</code>: Optional list of <code>Variable</code> objects to update to minimize <code>loss</code>. Defaults to the list of variables collected in the graph under the key <code>GraphKeys.TRAINABLE_VARIABLES</code>.</li> <li>\n<code>gate_gradients</code>: How to gate the computation of gradients. Can be <code>GATE_NONE</code>, <code>GATE_OP</code>, or <code>GATE_GRAPH</code>.</li> <li>\n<code>aggregation_method</code>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code>AggregationMethod</code>.</li> <li>\n<code>colocate_gradients_with_ops</code>: If True, try colocating gradients with the corresponding op.</li> <li>\n<code>name</code>: Optional name for the returned operation.</li> <li>\n<code>grad_loss</code>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>An Operation that updates the variables in <code>var_list</code>. If <code>global_step</code> was not <code>None</code>, that operation also increments <code>global_step</code>.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If some of the variables are not <code>Variable</code> objects.</li> </ul>  <h4 id=\"Optimizer.compute_gradients\"><code>tf.train.Optimizer.compute_gradients(loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)</code></h4> <p>Compute gradients of <code>loss</code> for the variables in <code>var_list</code>.</p> <p>This is the first part of <code>minimize()</code>. It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a <code>Tensor</code>, an <code>IndexedSlices</code>, or <code>None</code> if there is no gradient for the given variable.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>loss</code>: A Tensor containing the value to minimize.</li> <li>\n<code>var_list</code>: Optional list of tf.Variable to update to minimize <code>loss</code>. Defaults to the list of variables collected in the graph under the key <code>GraphKey.TRAINABLE_VARIABLES</code>.</li> <li>\n<code>gate_gradients</code>: How to gate the computation of gradients. Can be <code>GATE_NONE</code>, <code>GATE_OP</code>, or <code>GATE_GRAPH</code>.</li> <li>\n<code>aggregation_method</code>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code>AggregationMethod</code>.</li> <li>\n<code>colocate_gradients_with_ops</code>: If True, try colocating gradients with the corresponding op.</li> <li>\n<code>grad_loss</code>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A list of (gradient, variable) pairs.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>var_list</code> contains anything else than <code>Variable</code> objects.</li> <li>\n<code>ValueError</code>: If some arguments are invalid.</li> </ul>  <h4 id=\"Optimizer.apply_gradients\"><code>tf.train.Optimizer.apply_gradients(grads_and_vars, global_step=None, name=None)</code></h4> <p>Apply gradients to variables.</p> <p>This is the second part of <code>minimize()</code>. It returns an <code>Operation</code> that applies gradients.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>grads_and_vars</code>: List of (gradient, variable) pairs as returned by <code>compute_gradients()</code>.</li> <li>\n<code>global_step</code>: Optional <code>Variable</code> to increment by one after the variables have been updated.</li> <li>\n<code>name</code>: Optional name for the returned operation. Default to the name passed to the <code>Optimizer</code> constructor.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>An <code>Operation</code> that applies the specified gradients. If <code>global_step</code> was not None, that operation also increments <code>global_step</code>.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>grads_and_vars</code> is malformed.</li> <li>\n<code>ValueError</code>: If none of the variables have gradients.</li> </ul>  <h3 id=\"gating-gradients\">Gating Gradients</h3> <p>Both <code>minimize()</code> and <code>compute_gradients()</code> accept a <code>gate_gradient</code> argument that controls the degree of parallelism during the application of the gradients.</p> <p>The possible values are: <code>GATE_NONE</code>, <code>GATE_OP</code>, and <code>GATE_GRAPH</code>.</p> <p><code>GATE_NONE</code>: Compute and apply gradients in parallel. This provides the maximum parallelism in execution, at the cost of some non-reproducibility in the results. For example the two gradients of <code>matmul</code> depend on the input values: With <code>GATE_NONE</code> one of the gradients could be applied to one of the inputs <em>before</em> the other gradient is computed resulting in non-reproducible results.</p> <p><code>GATE_OP</code>: For each Op, make sure all gradients are computed before they are used. This prevents race conditions for Ops that generate gradients for multiple inputs where the gradients depend on the inputs.</p> <p><code>GATE_GRAPH</code>: Make sure all gradients for all variables are computed before any one of them is used. This provides the least parallelism but can be useful if you want to process all gradients before applying any of them.</p> <h3 id=\"slots\">Slots</h3> <p>Some optimizer subclasses, such as <code>MomentumOptimizer</code> and <code>AdagradOptimizer</code> allocate and manage additional variables associated with the variables to train. These are called <i>Slots</i>. Slots have names and you can ask the optimizer for the names of the slots that it uses. Once you have a slot name you can ask the optimizer for the variable it created to hold the slot value.</p> <p>This can be useful if you want to log debug a training algorithm, report stats about the slots, etc.</p>  <h4 id=\"Optimizer.get_slot_names\"><code>tf.train.Optimizer.get_slot_names()</code></h4> <p>Return a list of the names of slots created by the <code>Optimizer</code>.</p> <p>See <code>get_slot()</code>.</p>  <h5 id=\"returns-4\">Returns:</h5> <p>A list of strings.</p>  <h4 id=\"Optimizer.get_slot\"><code>tf.train.Optimizer.get_slot(var, name)</code></h4> <p>Return a slot named <code>name</code> created for <code>var</code> by the Optimizer.</p> <p>Some <code>Optimizer</code> subclasses use additional variables. For example <code>Momentum</code> and <code>Adagrad</code> use variables to accumulate updates. This method gives access to these <code>Variable</code> objects if for some reason you need them.</p> <p>Use <code>get_slot_names()</code> to get the list of slot names created by the <code>Optimizer</code>.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>var</code>: A variable passed to <code>minimize()</code> or <code>apply_gradients()</code>.</li> <li>\n<code>name</code>: A string.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>The <code>Variable</code> for the slot if it was created, <code>None</code> otherwise.</p>  <h4 id=\"other-methods\">Other Methods</h4>  <h4 id=\"Optimizer.get_name\"><code>tf.train.Optimizer.get_name()</code></h4>  <h3 id=\"GradientDescentOptimizer\"><code>class tf.train.GradientDescentOptimizer</code></h3> <p>Optimizer that implements the gradient descent algorithm.</p>  <h4 id=\"GradientDescentOptimizer.__init__\"><code>tf.train.GradientDescentOptimizer.__init__(learning_rate, use_locking=False, name='GradientDescent')</code></h4> <p>Construct a new gradient descent optimizer.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>learning_rate</code>: A Tensor or a floating point value. The learning rate to use.</li> <li>\n<code>use_locking</code>: If True use locks for update operations.</li> <li>\n<code>name</code>: Optional name prefix for the operations created when applying gradients. Defaults to \"GradientDescent\".</li> </ul>  <h3 id=\"AdadeltaOptimizer\"><code>class tf.train.AdadeltaOptimizer</code></h3> <p>Optimizer that implements the Adadelta algorithm. </p> <p>See <a href=\"http://arxiv.org/abs/1212.5701\" rel=\"noreferrer\">M. D. Zeiler</a> (<a href=\"http://arxiv.org/pdf/1212.5701v1.pdf\" rel=\"noreferrer\">pdf</a>)</p>  <h4 id=\"AdadeltaOptimizer.__init__\"><code>tf.train.AdadeltaOptimizer.__init__(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name='Adadelta')</code></h4> <p>Construct a new Adadelta optimizer.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>learning_rate</code>: A <code>Tensor</code> or a floating point value. The learning rate.</li> <li>\n<code>rho</code>: A <code>Tensor</code> or a floating point value. The decay rate.</li> <li>\n<code>epsilon</code>: A <code>Tensor</code> or a floating point value. A constant epsilon used to better conditioning the grad update.</li> <li>\n<code>use_locking</code>: If <code>True</code> use locks for update operations.</li> <li>\n<code>name</code>: Optional name prefix for the operations created when applying gradients. Defaults to \"Adadelta\".</li> </ul>  <h3 id=\"AdagradOptimizer\"><code>class tf.train.AdagradOptimizer</code></h3> <p>Optimizer that implements the Adagrad algorithm.</p> <p>See this <a href=\"http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\" rel=\"noreferrer\">paper</a>.</p>  <h4 id=\"AdagradOptimizer.__init__\"><code>tf.train.AdagradOptimizer.__init__(learning_rate, initial_accumulator_value=0.1, use_locking=False, name='Adagrad')</code></h4> <p>Construct a new Adagrad optimizer.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>learning_rate</code>: A <code>Tensor</code> or a floating point value. The learning rate.</li> <li>\n<code>initial_accumulator_value</code>: A floating point value. Starting value for the accumulators, must be positive.</li> <li>\n<code>use_locking</code>: If <code>True</code> use locks for update operations.</li> <li>\n<code>name</code>: Optional name prefix for the operations created when applying gradients. Defaults to \"Adagrad\".</li> </ul>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the <code>initial_accumulator_value</code> is invalid.</li> </ul>  <h3 id=\"MomentumOptimizer\"><code>class tf.train.MomentumOptimizer</code></h3> <p>Optimizer that implements the Momentum algorithm.</p>  <h4 id=\"MomentumOptimizer.__init__\"><code>tf.train.MomentumOptimizer.__init__(learning_rate, momentum, use_locking=False, name='Momentum', use_nesterov=False)</code></h4> <p>Construct a new Momentum optimizer.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>learning_rate</code>: A <code>Tensor</code> or a floating point value. The learning rate.</li> <li>\n<code>momentum</code>: A <code>Tensor</code> or a floating point value. The momentum.</li> <li>\n<code>use_locking</code>: If <code>True</code> use locks for update operations.</li> <li>\n<code>name</code>: Optional name prefix for the operations created when applying gradients. Defaults to \"Momentum\".</li> </ul>  <h3 id=\"AdamOptimizer\"><code>class tf.train.AdamOptimizer</code></h3> <p>Optimizer that implements the Adam algorithm.</p> <p>See <a href=\"http://arxiv.org/abs/1412.6980\" rel=\"noreferrer\">Kingma et. al., 2014</a> (<a href=\"http://arxiv.org/pdf/1412.6980.pdf\" rel=\"noreferrer\">pdf</a>).</p>  <h4 id=\"AdamOptimizer.__init__\"><code>tf.train.AdamOptimizer.__init__(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')</code></h4> <p>Construct a new Adam optimizer.</p> <p>Initialization:</p> <pre class=\"\">m_0 &lt;- 0 (Initialize initial 1st moment vector)\nv_0 &lt;- 0 (Initialize initial 2nd moment vector)\nt &lt;- 0 (Initialize timestep)\n</pre> <p>The update rule for <code>variable</code> with gradient <code>g</code> uses an optimization described at the end of section2 of the paper:</p> <pre class=\"\">t &lt;- t + 1\nlr_t &lt;- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n\nm_t &lt;- beta1 * m_{t-1} + (1 - beta1) * g\nv_t &lt;- beta2 * v_{t-1} + (1 - beta2) * g * g\nvariable &lt;- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n</pre> <p>The default value of 1e-8 for epsilon might not be a good default in general. For example, when training an Inception network on ImageNet a current good choice is 1.0 or 0.1.</p> <p>Note that in dense implement of this algorithm, m_t, v_t and variable will update even if g is zero, but in sparse implement, m_t, v_t and variable will not update in iterations g is zero.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>learning_rate</code>: A Tensor or a floating point value. The learning rate.</li> <li>\n<code>beta1</code>: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.</li> <li>\n<code>beta2</code>: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates.</li> <li>\n<code>epsilon</code>: A small constant for numerical stability.</li> <li>\n<code>use_locking</code>: If True use locks for update operations.</li> <li>\n<code>name</code>: Optional name for the operations created when applying gradients. Defaults to \"Adam\".</li> </ul>  <h3 id=\"FtrlOptimizer\"><code>class tf.train.FtrlOptimizer</code></h3> <p>Optimizer that implements the FTRL algorithm.</p> <p>See this <a href=\"https://www.eecs.tufts.edu/%7Edsculley/papers/ad-click-prediction.pdf\" rel=\"noreferrer\">paper</a>.</p>  <h4 id=\"FtrlOptimizer.__init__\"><code>tf.train.FtrlOptimizer.__init__(learning_rate, learning_rate_power=-0.5, initial_accumulator_value=0.1, l1_regularization_strength=0.0, l2_regularization_strength=0.0, use_locking=False, name='Ftrl')</code></h4> <p>Construct a new FTRL optimizer.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>learning_rate</code>: A float value or a constant float <code>Tensor</code>.</li> <li>\n<code>learning_rate_power</code>: A float value, must be less or equal to zero.</li> <li>\n<code>initial_accumulator_value</code>: The starting value for accumulators. Only positive values are allowed.</li> <li>\n<code>l1_regularization_strength</code>: A float value, must be greater than or equal to zero.</li> <li>\n<code>l2_regularization_strength</code>: A float value, must be greater than or equal to zero.</li> <li>\n<code>use_locking</code>: If <code>True</code> use locks for update operations.</li> <li>\n<code>name</code>: Optional name prefix for the operations created when applying gradients. Defaults to \"Ftrl\".</li> </ul>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If one of the arguments is invalid.</li> </ul>  <h3 id=\"RMSPropOptimizer\"><code>class tf.train.RMSPropOptimizer</code></h3> <p>Optimizer that implements the RMSProp algorithm.</p> <p>See the <a href=\"http://www.cs.toronto.edu/%7Etijmen/csc321/slides/lecture_slides_lec6.pdf\" rel=\"noreferrer\">paper</a>.</p>  <h4 id=\"RMSPropOptimizer.__init__\"><code>tf.train.RMSPropOptimizer.__init__(learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False, name='RMSProp')</code></h4> <p>Construct a new RMSProp optimizer.</p> <p>Note that in dense implement of this algorithm, m_t and v_t will update even if g is zero, but in sparse implement, m_t and v_t will not update in iterations g is zero.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>learning_rate</code>: A Tensor or a floating point value. The learning rate.</li> <li>\n<code>decay</code>: Discounting factor for the history/coming gradient</li> <li>\n<code>momentum</code>: A scalar tensor.</li> <li>\n<code>epsilon</code>: Small value to avoid zero denominator.</li> <li>\n<code>use_locking</code>: If True use locks for update operation.</li> <li>\n<code>name</code>: Optional name prefix for the operations created when applying gradients. Defaults to \"RMSProp\".</li> </ul>  <h2 id=\"gradient-computation\">Gradient Computation</h2> <p>TensorFlow provides functions to compute the derivatives for a given TensorFlow computation graph, adding operations to the graph. The optimizer classes automatically compute derivatives on your graph, but creators of new Optimizers or expert users can call the lower-level functions below.</p>  <h3 id=\"gradients\"><code>tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)</code></h3> <p>Constructs symbolic partial derivatives of sum of <code>ys</code> w.r.t. x in <code>xs</code>.</p> <p><code>ys</code> and <code>xs</code> are each a <code>Tensor</code> or a list of tensors. <code>grad_ys</code> is a list of <code>Tensor</code>, holding the gradients received by the <code>ys</code>. The list must be the same length as <code>ys</code>.</p> <p><code>gradients()</code> adds ops to the graph to output the partial derivatives of <code>ys</code> with respect to <code>xs</code>. It returns a list of <code>Tensor</code> of length <code>len(xs)</code> where each tensor is the <code>sum(dy/dx)</code> for y in <code>ys</code>.</p> <p><code>grad_ys</code> is a list of tensors of the same length as <code>ys</code> that holds the initial gradients for each y in <code>ys</code>. When <code>grad_ys</code> is None, we fill in a tensor of '1's of the shape of y for each y in <code>ys</code>. A user can provide their own initial <code>grad_ys</code> to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>ys</code>: A <code>Tensor</code> or list of tensors to be differentiated.</li> <li>\n<code>xs</code>: A <code>Tensor</code> or list of tensors to be used for differentiation.</li> <li>\n<code>grad_ys</code>: Optional. A <code>Tensor</code> or list of tensors the same size as <code>ys</code> and holding the gradients computed for each y in <code>ys</code>.</li> <li>\n<code>name</code>: Optional name to use for grouping all the gradient ops together. defaults to 'gradients'.</li> <li>\n<code>colocate_gradients_with_ops</code>: If True, try colocating gradients with the corresponding op.</li> <li>\n<code>gate_gradients</code>: If True, add a tuple around the gradients returned for an operations. This avoids some race conditions.</li> <li>\n<code>aggregation_method</code>: Specifies the method used to combine gradient terms. Accepted values are constants defined in the class <code>AggregationMethod</code>.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A list of <code>sum(dy/dx)</code> for each x in <code>xs</code>.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>LookupError</code>: if one of the operations between <code>x</code> and <code>y</code> does not have a registered gradient function.</li> <li>\n<code>ValueError</code>: if the arguments are invalid.</li> </ul>  <h3 id=\"AggregationMethod\"><code>class tf.AggregationMethod</code></h3> <p>A class listing aggregation methods used to combine gradients.</p> <p>Computing partial derivatives can require aggregating gradient contributions. This class lists the various methods that can be used to combine gradients in the graph:</p> <ul> <li>\n<code>ADD_N</code>: All of the gradient terms are summed as part of one operation using the \"AddN\" op. It has the property that all gradients must be ready before any aggregation is performed.</li> <li>\n<code>DEFAULT</code>: The system-chosen default aggregation method.</li> </ul>  <h3 id=\"stop_gradient\"><code>tf.stop_gradient(input, name=None)</code></h3> <p>Stops gradient computation.</p> <p>When executed in a graph, this op outputs its input tensor as-is.</p> <p>When building ops to compute gradients, this op prevents the contribution of its inputs to be taken into account. Normally, the gradient generator adds ops to a graph to compute the derivatives of a specified 'loss' by recursively finding out inputs that contributed to its computation. If you insert this op in the graph it inputs are masked from the gradient generator. They are not taken into account for computing gradients.</p> <p>This is useful any time you want to compute a value with TensorFlow but need to pretend that the value was a constant. Some examples include:</p> <ul> <li>The <em>EM</em> algorithm where the <em>M-step</em> should not involve backpropagation through the output of the <em>E-step</em>.</li> <li>Contrastive divergence training of Boltzmann machines where, when differentiating the energy function, the training must not backpropagate through the graph that generated the samples from the model.</li> <li>Adversarial training, where no backprop should happen through the adversarial example generation process.</li> </ul>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>input</code>: A <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A <code>Tensor</code>. Has the same type as <code>input</code>.</p>  <h2 id=\"gradient-clipping\">Gradient Clipping</h2> <p>TensorFlow provides several operations that you can use to add clipping functions to your graph. You can use these functions to perform general data clipping, but they're particularly useful for handling exploding or vanishing gradients.</p>  <h3 id=\"clip_by_value\"><code>tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)</code></h3> <p>Clips tensor values to a specified min and max.</p> <p>Given a tensor <code>t</code>, this operation returns a tensor of the same type and shape as <code>t</code> with its values clipped to <code>clip_value_min</code> and <code>clip_value_max</code>. Any values less than <code>clip_value_min</code> are set to <code>clip_value_min</code>. Any values greater than <code>clip_value_max</code> are set to <code>clip_value_max</code>.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>t</code>: A <code>Tensor</code>.</li> <li>\n<code>clip_value_min</code>: A 0-D (scalar) <code>Tensor</code>. The minimum value to clip by.</li> <li>\n<code>clip_value_max</code>: A 0-D (scalar) <code>Tensor</code>. The maximum value to clip by.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A clipped <code>Tensor</code>.</p>  <h3 id=\"clip_by_norm\"><code>tf.clip_by_norm(t, clip_norm, axes=None, name=None)</code></h3> <p>Clips tensor values to a maximum L2-norm.</p> <p>Given a tensor <code>t</code>, and a maximum clip value <code>clip_norm</code>, this operation normalizes <code>t</code> so that its L2-norm is less than or equal to <code>clip_norm</code>, along the dimensions given in <code>axes</code>. Specifically, in the default case where all dimensions are used for calculation, if the L2-norm of <code>t</code> is already less than or equal to <code>clip_norm</code>, then <code>t</code> is not modified. If the L2-norm is greater than <code>clip_norm</code>, then this operation returns a tensor of the same type and shape as <code>t</code> with its values set to:</p> <p><code>t * clip_norm / l2norm(t)</code></p> <p>In this case, the L2-norm of the output tensor is <code>clip_norm</code>.</p> <p>As another example, if <code>t</code> is a matrix and <code>axes == [1]</code>, then each row of the output will have L2-norm equal to <code>clip_norm</code>. If <code>axes == [0]</code> instead, each column of the output will be clipped.</p> <p>This operation is typically used to clip gradients before applying them with an optimizer.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>t</code>: A <code>Tensor</code>.</li> <li>\n<code>clip_norm</code>: A 0-D (scalar) <code>Tensor</code> &gt; 0. A maximum clipping value.</li> <li>\n<code>axes</code>: A 1-D (vector) <code>Tensor</code> of type int32 containing the dimensions to use for computing the L2-norm. If <code>None</code> (the default), uses all dimensions.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A clipped <code>Tensor</code>.</p>  <h3 id=\"clip_by_average_norm\"><code>tf.clip_by_average_norm(t, clip_norm, name=None)</code></h3> <p>Clips tensor values to a maximum average L2-norm.</p> <p>Given a tensor <code>t</code>, and a maximum clip value <code>clip_norm</code>, this operation normalizes <code>t</code> so that its average L2-norm is less than or equal to <code>clip_norm</code>. Specifically, if the average L2-norm is already less than or equal to <code>clip_norm</code>, then <code>t</code> is not modified. If the average L2-norm is greater than <code>clip_norm</code>, then this operation returns a tensor of the same type and shape as <code>t</code> with its values set to:</p> <p><code>t * clip_norm / l2norm_avg(t)</code></p> <p>In this case, the average L2-norm of the output tensor is <code>clip_norm</code>.</p> <p>This operation is typically used to clip gradients before applying them with an optimizer.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>t</code>: A <code>Tensor</code>.</li> <li>\n<code>clip_norm</code>: A 0-D (scalar) <code>Tensor</code> &gt; 0. A maximum clipping value.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A clipped <code>Tensor</code>.</p>  <h3 id=\"clip_by_global_norm\"><code>tf.clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None)</code></h3> <p>Clips values of multiple tensors by the ratio of the sum of their norms.</p> <p>Given a tuple or list of tensors <code>t_list</code>, and a clipping ratio <code>clip_norm</code>, this operation returns a list of clipped tensors <code>list_clipped</code> and the global norm (<code>global_norm</code>) of all tensors in <code>t_list</code>. Optionally, if you've already computed the global norm for <code>t_list</code>, you can specify the global norm with <code>use_norm</code>.</p> <p>To perform the clipping, the values <code>t_list[i]</code> are set to:</p> <pre class=\"\">t_list[i] * clip_norm / max(global_norm, clip_norm)\n</pre> <p>where:</p> <pre class=\"\">global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n</pre> <p>If <code>clip_norm &gt; global_norm</code> then the entries in <code>t_list</code> remain as they are, otherwise they're all shrunk by the global ratio.</p> <p>Any of the entries of <code>t_list</code> that are of type <code>None</code> are ignored.</p> <p>This is the correct way to perform gradient clipping (for example, see <a href=\"http://arxiv.org/abs/1211.5063\" rel=\"noreferrer\">Pascanu et al., 2012</a> (<a href=\"http://arxiv.org/pdf/1211.5063.pdf\" rel=\"noreferrer\">pdf</a>)).</p> <p>However, it is slower than <code>clip_by_norm()</code> because all the parameters must be ready before the clipping operation can be performed.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>t_list</code>: A tuple or list of mixed <code>Tensors</code>, <code>IndexedSlices</code>, or None.</li> <li>\n<code>clip_norm</code>: A 0-D (scalar) <code>Tensor</code> &gt; 0. The clipping ratio.</li> <li>\n<code>use_norm</code>: A 0-D (scalar) <code>Tensor</code> of type <code>float</code> (optional). The global norm to use. If not provided, <code>global_norm()</code> is used to compute the norm.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <ul> <li>\n<code>list_clipped</code>: A list of <code>Tensors</code> of the same type as <code>list_t</code>.</li> <li>\n<code>global_norm</code>: A 0-D (scalar) <code>Tensor</code> representing the global norm.</li> </ul>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>t_list</code> is not a sequence.</li> </ul>  <h3 id=\"global_norm\"><code>tf.global_norm(t_list, name=None)</code></h3> <p>Computes the global norm of multiple tensors.</p> <p>Given a tuple or list of tensors <code>t_list</code>, this operation returns the global norm of the elements in all tensors in <code>t_list</code>. The global norm is computed as:</p> <p><code>global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))</code></p> <p>Any entries in <code>t_list</code> that are of type None are ignored.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>t_list</code>: A tuple or list of mixed <code>Tensors</code>, <code>IndexedSlices</code>, or None.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>A 0-D (scalar) <code>Tensor</code> of type <code>float</code>.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>t_list</code> is not a sequence.</li> </ul>  <h2 id=\"decaying-the-learning-rate\">Decaying the learning rate</h2>  <h3 id=\"exponential_decay\"><code>tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)</code></h3> <p>Applies exponential decay to the learning rate.</p> <p>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies an exponential decay function to a provided initial learning rate. It requires a <code>global_step</code> value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</p> <p>The function returns the decayed learning rate. It is computed as:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">decayed_learning_rate = learning_rate *\n                        decay_rate ^ (global_step / decay_steps)\n</pre> <p>If the argument <code>staircase</code> is <code>True</code>, then <code>global_step / decay_steps</code> is an integer division and the decayed learning rate follows a staircase function.</p> <p>Example: decay every 100000 steps with a base of 0.96:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">...\nglobal_step = tf.Variable(0, trainable=False)\nstarter_learning_rate = 0.1\nlearning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n                                           100000, 0.96, staircase=True)\n# Passing global_step to minimize() will increment it at each step.\nlearning_step = (\n    tf.train.GradientDescentOptimizer(learning_rate)\n    .minimize(...my loss..., global_step=global_step)\n)\n</pre>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>learning_rate</code>: A scalar <code>float32</code> or <code>float64</code> <code>Tensor</code> or a Python number. The initial learning rate.</li> <li>\n<code>global_step</code>: A scalar <code>int32</code> or <code>int64</code> <code>Tensor</code> or a Python number. Global step to use for the decay computation. Must not be negative.</li> <li>\n<code>decay_steps</code>: A scalar <code>int32</code> or <code>int64</code> <code>Tensor</code> or a Python number. Must be positive. See the decay computation above.</li> <li>\n<code>decay_rate</code>: A scalar <code>float32</code> or <code>float64</code> <code>Tensor</code> or a Python number. The decay rate.</li> <li>\n<code>staircase</code>: Boolean. It <code>True</code> decay the learning rate at discrete intervals</li> <li>\n<code>name</code>: String. Optional name of the operation. Defaults to 'ExponentialDecay'</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A scalar <code>Tensor</code> of the same type as <code>learning_rate</code>. The decayed learning rate.</p>  <h2 id=\"moving-averages\">Moving Averages</h2> <p>Some training algorithms, such as GradientDescent and Momentum often benefit from maintaining a moving average of variables during optimization. Using the moving averages for evaluations often improve results significantly.</p>  <h3 id=\"ExponentialMovingAverage\"><code>class tf.train.ExponentialMovingAverage</code></h3> <p>Maintains moving averages of variables by employing an exponential decay.</p> <p>When training a model, it is often beneficial to maintain moving averages of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.</p> <p>The <code>apply()</code> method adds shadow copies of trained variables and add ops that maintain a moving average of the trained variables in their shadow copies. It is used when building the training model. The ops that maintain moving averages are typically run after each training step. The <code>average()</code> and <code>average_name()</code> methods give access to the shadow variables and their names. They are useful when building an evaluation model, or when restoring a model from a checkpoint file. They help use the moving averages in place of the last trained values for evaluations.</p> <p>The moving averages are computed using exponential decay. You specify the decay value when creating the <code>ExponentialMovingAverage</code> object. The shadow variables are initialized with the same initial values as the trained variables. When you run the ops to maintain the moving averages, each shadow variable is updated with the formula:</p> <p><code>shadow_variable -= (1 - decay) * (shadow_variable - variable)</code></p> <p>This is mathematically equivalent to the classic formula below, but the use of an <code>assign_sub</code> op (the <code>\"-=\"</code> in the formula) allows concurrent lockless updates to the variables:</p> <p><code>shadow_variable = decay * shadow_variable + (1 - decay) * variable</code></p> <p>Reasonable values for <code>decay</code> are close to 1.0, typically in the multiple-nines range: 0.999, 0.9999, etc.</p> <p>Example usage when creating a training model:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Create variables.\nvar0 = tf.Variable(...)\nvar1 = tf.Variable(...)\n# ... use the variables to build a training model...\n...\n# Create an op that applies the optimizer.  This is what we usually\n# would use as a training op.\nopt_op = opt.minimize(my_loss, [var0, var1])\n\n# Create an ExponentialMovingAverage object\nema = tf.train.ExponentialMovingAverage(decay=0.9999)\n\n# Create the shadow variables, and add ops to maintain moving averages\n# of var0 and var1.\nmaintain_averages_op = ema.apply([var0, var1])\n\n# Create an op that will update the moving averages after each training\n# step.  This is what we will use in place of the usual training op.\nwith tf.control_dependencies([opt_op]):\n    training_op = tf.group(maintain_averages_op)\n\n...train the model by running training_op...\n</pre> <p>There are two ways to use the moving averages for evaluations:</p> <ul> <li>Build a model that uses the shadow variables instead of the variables. For this, use the <code>average()</code> method which returns the shadow variable for a given variable.</li> <li>Build a model normally but load the checkpoint files to evaluate by using the shadow variable names. For this use the <code>average_name()</code> method. See the <a href=\"train#Saver\">Saver class</a> for more information on restoring saved variables.</li> </ul> <p>Example of restoring the shadow variable values:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Create a Saver that loads variables from their saved shadow values.\nshadow_var0_name = ema.average_name(var0)\nshadow_var1_name = ema.average_name(var1)\nsaver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})\nsaver.restore(...checkpoint filename...)\n# var0 and var1 now hold the moving average values\n</pre>  <h4 id=\"ExponentialMovingAverage.__init__\"><code>tf.train.ExponentialMovingAverage.__init__(decay, num_updates=None, name='ExponentialMovingAverage')</code></h4> <p>Creates a new ExponentialMovingAverage object.</p> <p>The <code>apply()</code> method has to be called to create shadow variables and add ops to maintain moving averages.</p> <p>The optional <code>num_updates</code> parameter allows one to tweak the decay rate dynamically. . It is typical to pass the count of training steps, usually kept in a variable that is incremented at each step, in which case the decay rate is lower at the start of training. This makes moving averages move faster. If passed, the actual decay rate used is:</p> <p><code>min(decay, (1 + num_updates) / (10 + num_updates))</code></p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>decay</code>: Float. The decay to use.</li> <li>\n<code>num_updates</code>: Optional count of number of updates applied to variables.</li> <li>\n<code>name</code>: String. Optional prefix name to use for the name of ops added in <code>apply()</code>.</li> </ul>  <h4 id=\"ExponentialMovingAverage.apply\"><code>tf.train.ExponentialMovingAverage.apply(var_list=None)</code></h4> <p>Maintains moving averages of variables.</p> <p><code>var_list</code> must be a list of <code>Variable</code> or <code>Tensor</code> objects. This method creates shadow variables for all elements of <code>var_list</code>. Shadow variables for <code>Variable</code> objects are initialized to the variable's initial value. They will be added to the <code>GraphKeys.MOVING_AVERAGE_VARIABLES</code> collection. For <code>Tensor</code> objects, the shadow variables are initialized to 0.</p> <p>shadow variables are created with <code>trainable=False</code> and added to the <code>GraphKeys.ALL_VARIABLES</code> collection. They will be returned by calls to <code>tf.all_variables()</code>.</p> <p>Returns an op that updates all shadow variables as described above.</p> <p>Note that <code>apply()</code> can be called multiple times with different lists of variables.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>var_list</code>: A list of Variable or Tensor objects. The variables and Tensors must be of types float16, float32, or float64.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>An Operation that updates the moving averages.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If the arguments are not all float16, float32, or float64.</li> <li>\n<code>ValueError</code>: If the moving average of one of the variables is already being computed.</li> </ul>  <h4 id=\"ExponentialMovingAverage.average_name\"><code>tf.train.ExponentialMovingAverage.average_name(var)</code></h4> <p>Returns the name of the <code>Variable</code> holding the average for <code>var</code>.</p> <p>The typical scenario for <code>ExponentialMovingAverage</code> is to compute moving averages of variables during training, and restore the variables from the computed moving averages during evaluations.</p> <p>To restore variables, you have to know the name of the shadow variables. That name and the original variable can then be passed to a <code>Saver()</code> object to restore the variable from the moving average value with: <code>saver = tf.train.Saver({ema.average_name(var): var})</code></p> <p><code>average_name()</code> can be called whether or not <code>apply()</code> has been called.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>var</code>: A <code>Variable</code> object.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>A string: The name of the variable that will be used or was used by the <code>ExponentialMovingAverage class</code> to hold the moving average of <code>var</code>.</p>  <h4 id=\"ExponentialMovingAverage.average\"><code>tf.train.ExponentialMovingAverage.average(var)</code></h4> <p>Returns the <code>Variable</code> holding the average of <code>var</code>.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>var</code>: A <code>Variable</code> object.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A <code>Variable</code> object or <code>None</code> if the moving average of <code>var</code> is not maintained..</p>  <h4 id=\"ExponentialMovingAverage.variables_to_restore\"><code>tf.train.ExponentialMovingAverage.variables_to_restore(moving_avg_variables=None)</code></h4> <p>Returns a map of names to <code>Variables</code> to restore.</p> <p>If a variable has a moving average, use the moving average variable name as the restore name; otherwise, use the variable name.</p> <p>For example,</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">variables_to_restore = ema.variables_to_restore()\nsaver = tf.train.Saver(variables_to_restore)\n</pre> <p>Below is an example of such mapping:</p> <pre class=\"\">conv/batchnorm/gamma/ExponentialMovingAverage: conv/batchnorm/gamma,\nconv_4/conv2d_params/ExponentialMovingAverage: conv_4/conv2d_params,\nglobal_step: global_step\n</pre>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>moving_avg_variables</code>: a list of variables that require to use of the moving variable name to be restored. If None, it will default to variables.moving_average_variables() + variables.trainable_variables()</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A map from restore_names to variables. The restore_name can be the moving_average version of the variable name if it exist, or the original variable name.</p>  <h2 id=\"coordinator-and-queuerunner\">Coordinator and QueueRunner</h2> <p>See <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/threading_and_queues/index.html\">Threading and Queues</a> for how to use threads and queues. For documentation on the Queue API, see <a href=\"io_ops#queues\">Queues</a>.</p>  <h3 id=\"Coordinator\"><code>class tf.train.Coordinator</code></h3> <p>A coordinator for threads.</p> <p>This class implements a simple mechanism to coordinate the termination of a set of threads.</p> <h4 id=\"usage\">Usage:</h4> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Create a coordinator.\ncoord = Coordinator()\n# Start a number of threads, passing the coordinator to each of them.\n...start thread 1...(coord, ...)\n...start thread N...(coord, ...)\n# Wait for all the threads to terminate.\ncoord.join(threads)\n</pre> <p>Any of the threads can call <code>coord.request_stop()</code> to ask for all the threads to stop. To cooperate with the requests, each thread must check for <code>coord.should_stop()</code> on a regular basis. <code>coord.should_stop()</code> returns <code>True</code> as soon as <code>coord.request_stop()</code> has been called.</p> <p>A typical thread running with a coordinator will do something like:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">while not coord.should_stop():\n  ...do some work...\n</pre>  <h4 id=\"exception-handling\">Exception handling:</h4> <p>A thread can report an exception to the coordinator as part of the <code>should_stop()</code> call. The exception will be re-raised from the <code>coord.join()</code> call.</p> <p>Thread code:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">try:\n  while not coord.should_stop():\n    ...do some work...\nexcept Exception as e:\n  coord.request_stop(e)\n</pre> <p>Main code:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">try:\n  ...\n  coord = Coordinator()\n  # Start a number of threads, passing the coordinator to each of them.\n  ...start thread 1...(coord, ...)\n  ...start thread N...(coord, ...)\n  # Wait for all the threads to terminate.\n  coord.join(threads)\nexcept Exception as e:\n  ...exception that was passed to coord.request_stop()\n</pre> <p>To simplify the thread implementation, the Coordinator provides a context handler <code>stop_on_exception()</code> that automatically requests a stop if an exception is raised. Using the context handler the thread code above can be written as:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with coord.stop_on_exception():\n  while not coord.should_stop():\n    ...do some work...\n</pre>  <h4 id=\"grace-period-for-stopping\">Grace period for stopping:</h4> <p>After a thread has called <code>coord.request_stop()</code> the other threads have a fixed time to stop, this is called the 'stop grace period' and defaults to 2 minutes. If any of the threads is still alive after the grace period expires <code>coord.join()</code> raises a RuntimeException reporting the laggards.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">try:\n  ...\n  coord = Coordinator()\n  # Start a number of threads, passing the coordinator to each of them.\n  ...start thread 1...(coord, ...)\n  ...start thread N...(coord, ...)\n  # Wait for all the threads to terminate, give them 10s grace period\n  coord.join(threads, stop_grace_period_secs=10)\nexcept RuntimeException:\n  ...one of the threads took more than 10s to stop after request_stop()\n  ...was called.\nexcept Exception:\n  ...exception that was passed to coord.request_stop()\n</pre>  <h4 id=\"Coordinator.__init__\"><code>tf.train.Coordinator.__init__(clean_stop_exception_types=None)</code></h4> <p>Create a new Coordinator.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>clean_stop_exception_types</code>: Optional tuple of Exception types that should cause a clean stop of the coordinator. If an exception of one of these types is reported to <code>request_stop(ex)</code> the coordinator will behave as if <code>request_stop(None)</code> was called. Defaults to <code>(tf.errors.OutOfRangeError,)</code> which is used by input queues to signal the end of input. When feeding training data from a Python iterator it is common to add <code>StopIteration</code> to this list.</li> </ul>  <h4 id=\"Coordinator.clear_stop\"><code>tf.train.Coordinator.clear_stop()</code></h4> <p>Clears the stop flag.</p> <p>After this is called, calls to <code>should_stop()</code> will return <code>False</code>.</p>  <h4 id=\"Coordinator.join\"><code>tf.train.Coordinator.join(threads=None, stop_grace_period_secs=120)</code></h4> <p>Wait for threads to terminate.</p> <p>This call blocks until a set of threads have terminated. The set of thread is the union of the threads passed in the <code>threads</code> argument and the list of threads that registered with the coordinator by calling <code>Coordinator.register_thread()</code>.</p> <p>After the threads stop, if an <code>exc_info</code> was passed to <code>request_stop</code>, that exception is re-raised.</p> <p>Grace period handling: When <code>request_stop()</code> is called, threads are given 'stop_grace_period_secs' seconds to terminate. If any of them is still alive after that period expires, a <code>RuntimeError</code> is raised. Note that if an <code>exc_info</code> was passed to <code>request_stop()</code> then it is raised instead of that <code>RuntimeError</code>.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>threads</code>: List of <code>threading.Threads</code>. The started threads to join in addition to the registered threads.</li> <li>\n<code>stop_grace_period_secs</code>: Number of seconds given to threads to stop after <code>request_stop()</code> has been called.</li> </ul>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: If any thread is still alive after <code>request_stop()</code> is called and the grace period expires.</li> </ul>  <h4 id=\"Coordinator.joined\"><code>tf.train.Coordinator.joined</code></h4>  <h4 id=\"Coordinator.register_thread\"><code>tf.train.Coordinator.register_thread(thread)</code></h4> <p>Register a thread to join.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>thread</code>: A Python thread to join.</li> </ul>  <h4 id=\"Coordinator.request_stop\"><code>tf.train.Coordinator.request_stop(ex=None)</code></h4> <p>Request that the threads stop.</p> <p>After this is called, calls to <code>should_stop()</code> will return <code>True</code>.</p> <p>Note: If an exception is being passed in, in must be in the context of handling the exception (i.e. <code>try: ... except Exception as ex: ...</code>) and not a newly created one.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>ex</code>: Optional <code>Exception</code>, or Python <code>exc_info</code> tuple as returned by <code>sys.exc_info()</code>. If this is the first call to <code>request_stop()</code> the corresponding exception is recorded and re-raised from <code>join()</code>.</li> </ul>  <h4 id=\"Coordinator.should_stop\"><code>tf.train.Coordinator.should_stop()</code></h4> <p>Check if stop was requested.</p>  <h5 id=\"returns-18\">Returns:</h5> <p>True if a stop was requested.</p>  <h4 id=\"Coordinator.stop_on_exception\"><code>tf.train.Coordinator.stop_on_exception()</code></h4> <p>Context manager to request stop when an Exception is raised.</p> <p>Code that uses a coordinator must catch exceptions and pass them to the <code>request_stop()</code> method to stop the other threads managed by the coordinator.</p> <p>This context handler simplifies the exception handling. Use it as follows:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with coord.stop_on_exception():\n  # Any exception raised in the body of the with\n  # clause is reported to the coordinator before terminating\n  # the execution of the body.\n  ...body...\n</pre> <p>This is completely equivalent to the slightly longer code:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">try:\n  ...body...\nexception Exception as ex:\n  coord.request_stop(ex)\n</pre> <h5 id=\"yields\">Yields:</h5> <p>nothing.</p>  <h4 id=\"Coordinator.wait_for_stop\"><code>tf.train.Coordinator.wait_for_stop(timeout=None)</code></h4> <p>Wait till the Coordinator is told to stop.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>timeout</code>: Float. Sleep for up to that many seconds waiting for should_stop() to become True.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>True if the Coordinator is told stop, False if the timeout expired.</p>  <h3 id=\"QueueRunner\"><code>class tf.train.QueueRunner</code></h3> <p>Holds a list of enqueue operations for a queue, each to be run in a thread.</p> <p>Queues are a convenient TensorFlow mechanism to compute tensors asynchronously using multiple threads. For example in the canonical 'Input Reader' setup one set of threads generates filenames in a queue; a second set of threads read records from the files, processes them, and enqueues tensors on a second queue; a third set of threads dequeues these input records to construct batches and runs them through training operations.</p> <p>There are several delicate issues when running multiple threads that way: closing the queues in sequence as the input is exhausted, correctly catching and reporting exceptions, etc.</p> <p>The <code>QueueRunner</code>, combined with the <code>Coordinator</code>, helps handle these issues.</p>  <h4 id=\"QueueRunner.__init__\"><code>tf.train.QueueRunner.__init__(queue=None, enqueue_ops=None, close_op=None, cancel_op=None, queue_runner_def=None)</code></h4> <p>Create a QueueRunner.</p> <p>On construction the <code>QueueRunner</code> adds an op to close the queue. That op will be run if the enqueue ops raise exceptions.</p> <p>When you later call the <code>create_threads()</code> method, the <code>QueueRunner</code> will create one thread for each op in <code>enqueue_ops</code>. Each thread will run its enqueue op in parallel with the other threads. The enqueue ops do not have to all be the same op, but it is expected that they all enqueue tensors in <code>queue</code>.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>queue</code>: A <code>Queue</code>.</li> <li>\n<code>enqueue_ops</code>: List of enqueue ops to run in threads later.</li> <li>\n<code>close_op</code>: Op to close the queue. Pending enqueue ops are preserved.</li> <li>\n<code>cancel_op</code>: Op to close the queue and cancel pending enqueue ops.</li> <li>\n<code>queue_runner_def</code>: Optional <code>QueueRunnerDef</code> protocol buffer. If specified, recreates the QueueRunner from its contents. <code>queue_runner_def</code> and the other arguments are mutually exclusive.</li> </ul>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If both <code>queue_runner_def</code> and <code>queue</code> are both specified.</li> <li>\n<code>ValueError</code>: If <code>queue</code> or <code>enqueue_ops</code> are not provided when not restoring from <code>queue_runner_def</code>.</li> </ul>  <h4 id=\"QueueRunner.cancel_op\"><code>tf.train.QueueRunner.cancel_op</code></h4>  <h4 id=\"QueueRunner.close_op\"><code>tf.train.QueueRunner.close_op</code></h4>  <h4 id=\"QueueRunner.create_threads\"><code>tf.train.QueueRunner.create_threads(sess, coord=None, daemon=False, start=False)</code></h4> <p>Create threads to run the enqueue ops.</p> <p>This method requires a session in which the graph was launched. It creates a list of threads, optionally starting them. There is one thread for each op passed in <code>enqueue_ops</code>.</p> <p>The <code>coord</code> argument is an optional coordinator, that the threads will use to terminate together and report exceptions. If a coordinator is given, this method starts an additional thread to close the queue when the coordinator requests a stop.</p> <p>This method may be called again as long as all threads from a previous call have stopped.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>sess</code>: A <code>Session</code>.</li> <li>\n<code>coord</code>: Optional <code>Coordinator</code> object for reporting errors and checking stop conditions.</li> <li>\n<code>daemon</code>: Boolean. If <code>True</code> make the threads daemon threads.</li> <li>\n<code>start</code>: Boolean. If <code>True</code> starts the threads. If <code>False</code> the caller must call the <code>start()</code> method of the returned threads.</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A list of threads.</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: If threads from a previous call to <code>create_threads()</code> are still running.</li> </ul>  <h4 id=\"QueueRunner.enqueue_ops\"><code>tf.train.QueueRunner.enqueue_ops</code></h4>  <h4 id=\"QueueRunner.exceptions_raised\"><code>tf.train.QueueRunner.exceptions_raised</code></h4> <p>Exceptions raised but not handled by the <code>QueueRunner</code> threads.</p> <p>Exceptions raised in queue runner threads are handled in one of two ways depending on whether or not a <code>Coordinator</code> was passed to <code>create_threads()</code>:</p> <ul> <li>With a <code>Coordinator</code>, exceptions are reported to the coordinator and forgotten by the <code>QueueRunner</code>.</li> <li>Without a <code>Coordinator</code>, exceptions are captured by the <code>QueueRunner</code> and made available in this <code>exceptions_raised</code> property.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>A list of Python <code>Exception</code> objects. The list is empty if no exception was captured. (No exceptions are captured when using a Coordinator.)</p>  <h4 id=\"QueueRunner.from_proto\"><code>tf.train.QueueRunner.from_proto(queue_runner_def)</code></h4> <p>Returns a <code>QueueRunner</code> object created from <code>queue_runner_def</code>.</p>  <h4 id=\"QueueRunner.name\"><code>tf.train.QueueRunner.name</code></h4> <p>The string name of the underlying Queue.</p>  <h4 id=\"QueueRunner.queue\"><code>tf.train.QueueRunner.queue</code></h4>  <h4 id=\"QueueRunner.to_proto\"><code>tf.train.QueueRunner.to_proto()</code></h4> <p>Converts this <code>QueueRunner</code> to a <code>QueueRunnerDef</code> protocol buffer.</p>  <h5 id=\"returns-22\">Returns:</h5> <p>A <code>QueueRunnerDef</code> protocol buffer.</p>  <h3 id=\"add_queue_runner\"><code>tf.train.add_queue_runner(qr, collection='queue_runners')</code></h3> <p>Adds a <code>QueueRunner</code> to a collection in the graph.</p> <p>When building a complex model that uses many queues it is often difficult to gather all the queue runners that need to be run. This convenience function allows you to add a queue runner to a well known collection in the graph.</p> <p>The companion method <code>start_queue_runners()</code> can be used to start threads for all the collected queue runners.</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>qr</code>: A <code>QueueRunner</code>.</li> <li>\n<code>collection</code>: A <code>GraphKey</code> specifying the graph collection to add the queue runner to. Defaults to <code>GraphKeys.QUEUE_RUNNERS</code>.</li> </ul>  <h3 id=\"start_queue_runners\"><code>tf.train.start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection='queue_runners')</code></h3> <p>Starts all queue runners collected in the graph.</p> <p>This is a companion method to <code>add_queue_runner()</code>. It just starts threads for all queue runners collected in the graph. It returns the list of all threads.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>sess</code>: <code>Session</code> used to run the queue ops. Defaults to the default session.</li> <li>\n<code>coord</code>: Optional <code>Coordinator</code> for coordinating the started threads.</li> <li>\n<code>daemon</code>: Whether the threads should be marked as <code>daemons</code>, meaning they don't block program exit.</li> <li>\n<code>start</code>: Set to <code>False</code> to only create the threads, not start them.</li> <li>\n<code>collection</code>: A <code>GraphKey</code> specifying the graph collection to get the queue runners from. Defaults to <code>GraphKeys.QUEUE_RUNNERS</code>.</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>A list of threads.</p>  <h2 id=\"distributed-execution\">Distributed execution</h2> <p>See <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html\">Distributed TensorFlow</a> for more information about how to configure a distributed TensorFlow program.</p>  <h3 id=\"Server\"><code>class tf.train.Server</code></h3> <p>An in-process TensorFlow server, for use in distributed training.</p> <p>A <code>tf.train.Server</code> instance encapsulates a set of devices and a <a href=\"client#Session\"><code>tf.Session</code></a> target that can participate in distributed training. A server belongs to a cluster (specified by a <a href=\"#ClusterSpec\"><code>tf.train.ClusterSpec</code></a>), and corresponds to a particular task in a named job. The server can communicate with any other server in the same cluster.</p>  <h4 id=\"Server.__init__\"><code>tf.train.Server.__init__(server_or_cluster_def, job_name=None, task_index=None, protocol=None, config=None, start=True)</code></h4> <p>Creates a new server with the given definition.</p> <p>The <code>job_name</code>, <code>task_index</code>, and <code>protocol</code> arguments are optional, and override any information provided in <code>server_or_cluster_def</code>.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>server_or_cluster_def</code>: A <code>tf.train.ServerDef</code> or <code>tf.train.ClusterDef</code> protocol buffer, or a <code>tf.train.ClusterSpec</code> object, describing the server to be created and/or the cluster of which it is a member.</li> <li>\n<code>job_name</code>: (Optional.) Specifies the name of the job of which the server is a member. Defaults to the value in <code>server_or_cluster_def</code>, if specified.</li> <li>\n<code>task_index</code>: (Optional.) Specifies the task index of the server in its job. Defaults to the value in <code>server_or_cluster_def</code>, if specified. Otherwise defaults to 0 if the server's job has only one task.</li> <li>\n<code>protocol</code>: (Optional.) Specifies the protocol to be used by the server. Acceptable values include <code>\"grpc\"</code>. Defaults to the value in <code>server_or_cluster_def</code>, if specified. Otherwise defaults to <code>\"grpc\"</code>.</li> <li>\n<code>config</code>: (Options.) A <code>tf.ConfigProto</code> that specifies default configuration options for all sessions that run on this server.</li> <li>\n<code>start</code>: (Optional.) Boolean, indicating whether to start the server after creating it. Defaults to <code>True</code>.</li> </ul>  <h5 id=\"raises-14\">Raises:</h5> <p>tf.errors.OpError: Or one of its subclasses if an error occurs while creating the TensorFlow server.</p>  <h4 id=\"Server.create_local_server\"><code>tf.train.Server.create_local_server(config=None, start=True)</code></h4> <p>Creates a new single-process cluster running on the local host.</p> <p>This method is a convenience wrapper for creating a <code>tf.train.Server</code> with a <code>tf.train.ServerDef</code> that specifies a single-process cluster containing a single task in a job called <code>\"local\"</code>.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>config</code>: (Options.) A <code>tf.ConfigProto</code> that specifies default configuration options for all sessions that run on this server.</li> <li>\n<code>start</code>: (Optional.) Boolean, indicating whether to start the server after creating it. Defaults to <code>True</code>.</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A local <code>tf.train.Server</code>.</p>  <h4 id=\"Server.target\"><code>tf.train.Server.target</code></h4> <p>Returns the target for a <code>tf.Session</code> to connect to this server.</p> <p>To create a <a href=\"client#Session\"><code>tf.Session</code></a> that connects to this server, use the following snippet:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">server = tf.train.Server(...)\nwith tf.Session(server.target):\n  # ...\n</pre>  <h5 id=\"returns-25\">Returns:</h5> <p>A string containing a session target for this server.</p>  <h4 id=\"Server.server_def\"><code>tf.train.Server.server_def</code></h4> <p>Returns the <code>tf.train.ServerDef</code> for this server.</p>  <h5 id=\"returns-26\">Returns:</h5> <p>A <code>tf.train.ServerDef</code> prototocol buffer that describes the configuration of this server.</p>  <h4 id=\"Server.start\"><code>tf.train.Server.start()</code></h4> <p>Starts this server.</p>  <h5 id=\"raises-15\">Raises:</h5> <p>tf.errors.OpError: Or one of its subclasses if an error occurs while starting the TensorFlow server.</p>  <h4 id=\"Server.join\"><code>tf.train.Server.join()</code></h4> <p>Blocks until the server has shut down.</p> <p>This method currently blocks forever.</p>  <h5 id=\"raises-16\">Raises:</h5> <p>tf.errors.OpError: Or one of its subclasses if an error occurs while joining the TensorFlow server.</p>  <h3 id=\"Supervisor\"><code>class tf.train.Supervisor</code></h3> <p>A training helper that checkpoints models and computes summaries.</p> <p>The Supervisor is a small wrapper around a <code>Coordinator</code>, a <code>Saver</code>, and a <code>SessionManager</code> that takes care of common needs of TensorFlow training programs.</p>  <h4 id=\"use-for-a-single-program\">Use for a single program</h4> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.Graph().as_default():\n  ...add operations to the graph...\n  # Create a Supervisor that will checkpoint the model in '/tmp/mydir'.\n  sv = Supervisor(logdir='/tmp/mydir')\n  # Get a TensorFlow session managed by the supervisor.\n  with sv.managed_session(FLAGS.master) as sess:\n    # Use the session to train the graph.\n    while not sv.should_stop():\n      sess.run(&lt;my_train_op&gt;)\n</pre> <p>Within the <code>with sv.managed_session()</code> block all variables in the graph have been initialized. In addition, a few services have been started to checkpoint the model and add summaries to the event log.</p> <p>If the program crashes and is restarted, the managed session automatically reinitialize variables from the most recent checkpoint.</p> <p>The supervisor is notified of any exception raised by one of the services. After an exception is raised, <code>should_stop()</code> returns <code>True</code>. In that case the training loop should also stop. This is why the training loop has to check for <code>sv.should_stop()</code>.</p> <p>Exceptions that indicate that the training inputs have been exhausted, <code>tf.errors.OutOfRangeError</code>, also cause <code>sv.should_stop()</code> to return <code>True</code> but are not re-raised from the <code>with</code> block: they indicate a normal termination.</p>  <h4 id=\"use-for-multiple-replicas\">Use for multiple replicas</h4> <p>To train with replicas you deploy the same program in a <code>Cluster</code>. One of the tasks must be identified as the <em>chief</em>: the task that handles initialization, checkpoints, summaries, and recovery. The other tasks depend on the <em>chief</em> for these services.</p> <p>The only change you have to do to the single program code is to indicate if the program is running as the <em>chief</em>.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Choose a task as the chief. This could be based on server_def.task_index,\n# or job_def.name, or job_def.tasks. It's entirely up to the end user.\n# But there can be only one *chief*.\nis_chief = (server_def.task_index == 0)\nserver = tf.train.Server(server_def)\n\nwith tf.Graph().as_default():\n  ...add operations to the graph...\n  # Create a Supervisor that uses log directory on a shared file system.\n  # Indicate if you are the 'chief'\n  sv = Supervisor(logdir='/shared_directory/...', is_chief=is_chief)\n  # Get a Session in a TensorFlow server on the cluster.\n  with sv.managed_session(server.target) as sess:\n    # Use the session to train the graph.\n    while not sv.should_stop():\n      sess.run(&lt;my_train_op&gt;)\n</pre> <p>In the <em>chief</em> task, the <code>Supervisor</code> works exactly as in the first example above. In the other tasks <code>sv.managed_session()</code> waits for the Model to have been intialized before returning a session to the training code. The non-chief tasks depend on the chief taks for initializing the model.</p> <p>If one of the tasks crashes and restarts, <code>managed_session()</code> checks if the Model is initialized. If yes, it just creates a session and returns it to the training code that proceeds normally. If the model needs to be initialized, the chief task takes care of reinitializing it; the other tasks just wait for the model to have been initialized.</p> <p>NOTE: This modified program still works fine as a single program. The single program marks itself as the chief.</p>  <h4 id=\"what-master-string-to-use\">What <code>master</code> string to use</h4> <p>Whether you are running on your machine or in the cluster you can use the following values for the --master flag:</p> <ul> <li><p>Specifying <code>''</code> requests an in-process session that does not use RPC.</p></li> <li><p>Specifying <code>'local'</code> requests a session that uses the RPC-based \"Master interface\" to run TensorFlow programs. See <a href=\"#Server.create_local_server\"><code>tf.train.Server.create_local_server()</code></a> for details.</p></li> <li><p>Specifying <code>'grpc://hostname:port'</code> requests a session that uses the RPC interface to a specific , and also allows the in-process master to access remote tensorflow workers. Often, it is appropriate to pass <code>server.target</code> (for some <code>tf.train.Server</code> named `server).</p></li> </ul>  <h4 id=\"advanced-use\">Advanced use</h4>  <h5 id=\"launching-additional-services\">Launching additional services</h5> <p><code>managed_session()</code> launches the Checkpoint and Summary services (threads). If you need more services to run you can simply launch them in the block controlled by <code>managed_session()</code>.</p> <p>Example: Start a thread to print losses. We want this thread to run every 60 seconds, so we launch it with <code>sv.loop()</code>.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">...\nsv = Supervisor(logdir='/tmp/mydir')\nwith sv.managed_session(FLAGS.master) as sess:\n  sv.loop(60, print_loss, (sess))\n  while not sv.should_stop():\n    sess.run(my_train_op)\n</pre>  <h5 id=\"launching-fewer-services\">Launching fewer services</h5> <p><code>managed_session()</code> launches the \"summary\" and \"checkpoint\" threads which use either the optionally <code>summary_op</code> and <code>saver</code> passed to the constructor, or default ones created automatically by the supervisor. If you want to run your own summary and checkpointing logic, disable these services by passing <code>None</code> to the <code>summary_op</code> and <code>saver</code> parameters.</p> <p>Example: Create summaries manually every 100 steps in the chief.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Create a Supervisor with no automatic summaries.\nsv = Supervisor(logdir='/tmp/mydir', is_chief=is_chief, summary_op=None)\n# As summary_op was None, managed_session() does not start the\n# summary thread.\nwith sv.managed_session(FLAGS.master) as sess:\n  for step in xrange(1000000):\n    if sv.should_stop():\n      break\n    if is_chief and step % 100 == 0:\n      # Create the summary every 100 chief steps.\n      sv.summary_computed(sess, sess.run(my_summary_op))\n    else:\n      # Train normally\n      sess.run(my_train_op)\n</pre>  <h5 id=\"custom-model-initialization\">Custom model initialization</h5> <p><code>managed_session()</code> only supports initializing the model by running an <code>init_op</code> or restoring from the latest checkpoint. If you have special initialization needs, see how to specify a <code>local_init_op</code> when creating the supervisor. You can also use the <code>SessionManager</code> directly to create a session and check if it could be initialized automatically.</p>  <h4 id=\"Supervisor.__init__\"><code>tf.train.Supervisor.__init__(graph=None, ready_op=0, is_chief=True, init_op=0, init_feed_dict=None, local_init_op=0, logdir=None, summary_op=0, saver=0, global_step=0, save_summaries_secs=120, save_model_secs=600, recovery_wait_secs=30, stop_grace_secs=120, checkpoint_basename='model.ckpt', session_manager=None, summary_writer=0, init_fn=None)</code></h4> <p>Create a <code>Supervisor</code>.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>graph</code>: A <code>Graph</code>. The graph that the model will use. Defaults to the default <code>Graph</code>. The supervisor may add operations to the graph before creating a session, but the graph should not be modified by the caller after passing it to the supervisor.</li> <li>\n<code>ready_op</code>: 1-D string <code>Tensor</code>. This tensor is evaluated by supervisors in <code>prepare_or_wait_for_session()</code> to check if the model is ready to use. The model is considered ready if it returns an empty array. Defaults to the tensor returned from <code>tf.report_uninitialized_variables()</code> If <code>None</code>, the model is not checked for readiness.</li> <li>\n<code>is_chief</code>: If True, create a chief supervisor in charge of initializing and restoring the model. If False, create a supervisor that relies on a chief supervisor for inits and restore.</li> <li>\n<code>init_op</code>: <code>Operation</code>. Used by chief supervisors to initialize the model when it can not be recovered. Defaults to an <code>Operation</code> that initializes all variables. If <code>None</code>, no initialization is done automatically unless you pass a value for <code>init_fn</code>, see below.</li> <li>\n<code>init_feed_dict</code>: A dictionary that maps <code>Tensor</code> objects to feed values. This feed dictionary will be used when <code>init_op</code> is evaluated.</li> <li>\n<code>local_init_op</code>: <code>Operation</code>. Used by all supervisors to run initializations that should run for every new supervisor instance. By default these are table initializers and initializers for local variables. If <code>None</code>, no further per supervisor-instance initialization is done automatically.</li> <li>\n<code>logdir</code>: A string. Optional path to a directory where to checkpoint the model and log events for the visualizer. Used by chief supervisors. The directory will be created if it does not exist.</li> <li>\n<code>summary_op</code>: An <code>Operation</code> that returns a Summary for the event logs. Used by chief supervisors if a <code>logdir</code> was specified. Defaults to the operation returned from merge_all_summaries(). If <code>None</code>, summaries are not computed automatically.</li> <li>\n<code>saver</code>: A Saver object. Used by chief supervisors if a <code>logdir</code> was specified. Defaults to the saved returned by Saver(). If <code>None</code>, the model is not saved automatically.</li> <li>\n<code>global_step</code>: An integer Tensor of size 1 that counts steps. The value from 'global_step' is used in summaries and checkpoint filenames. Default to the op named 'global_step' in the graph if it exists, is of rank 1, size 1, and of type tf.int32 ot tf.int64. If <code>None</code> the global step is not recorded in summaries and checkpoint files. Used by chief supervisors if a <code>logdir</code> was specified.</li> <li>\n<code>save_summaries_secs</code>: Number of seconds between the computation of summaries for the event log. Defaults to 120 seconds. Pass 0 to disable summaries.</li> <li>\n<code>save_model_secs</code>: Number of seconds between the creation of model checkpoints. Defaults to 600 seconds. Pass 0 to disable checkpoints.</li> <li>\n<code>recovery_wait_secs</code>: Number of seconds between checks that the model is ready. Used by supervisors when waiting for a chief supervisor to initialize or restore the model. Defaults to 30 seconds.</li> <li>\n<code>stop_grace_secs</code>: Grace period, in seconds, given to running threads to stop when <code>stop()</code> is called. Defaults to 120 seconds.</li> <li>\n<code>checkpoint_basename</code>: The basename for checkpoint saving.</li> <li>\n<code>session_manager</code>: <code>SessionManager</code>, which manages Session creation and recovery. If it is <code>None</code>, a default <code>SessionManager</code> will be created with the set of arguments passed in for backwards compatibility.</li> <li>\n<code>summary_writer</code>: <code>SummaryWriter</code> to use or <code>USE_DEFAULT</code>. Can be <code>None</code> to indicate that no summaries should be written.</li> <li>\n<code>init_fn</code>: Optional callable used to initialize the model. Called after the optional <code>init_op</code> is called. The callable must accept one argument, the session being initialized.</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>A <code>Supervisor</code>.</p>  <h4 id=\"Supervisor.managed_session\"><code>tf.train.Supervisor.managed_session(master='', config=None, start_standard_services=True, close_summary_writer=True)</code></h4> <p>Returns a context manager for a managed session.</p> <p>This context manager creates and automatically recovers a session. It optionally starts the standard services that handle checkpoints and summaries. It monitors exceptions raised from the <code>with</code> block or from the services and stops the supervisor as needed.</p> <p>The context manager is typically used as follows:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def train():\n  sv = tf.train.Supervisor(...)\n  with sv.managed_session(&lt;master&gt;) as sess:\n    for step in xrange(..):\n      if sv.should_stop():\n        break\n      sess.run(&lt;my training op&gt;)\n      ...do other things needed at each training step...\n</pre> <p>An exception raised from the <code>with</code> block or one of the service threads is raised again when the block exits. This is done after stopping all threads and closing the session. For example, an <code>AbortedError</code> exception, raised in case of preemption of one of the workers in a distributed model, is raised again when the block exits.</p> <p>If you want to retry the training loop in case of preemption you can do it as follows:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def main(...):\n  while True\n    try:\n      train()\n    except tf.errors.Aborted:\n      pass\n</pre> <p>As a special case, exceptions used for control flow, such as <code>OutOfRangeError</code> which reports that input queues are exhausted, are not raised again from the <code>with</code> block: they indicate a clean termination of the training loop and are considered normal termination.</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>master</code>: name of the TensorFlow master to use. See the <code>tf.Session</code> constructor for how this is interpreted.</li> <li>\n<code>config</code>: Optional <code>ConfigProto</code> proto used to configure the session. Passed as-is to create the session.</li> <li>\n<code>start_standard_services</code>: Whether to start the standard services, such as checkpoint, summary and step counter.</li> <li>\n<code>close_summary_writer</code>: Whether to close the summary writer when closing the session. Defaults to True.</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>A context manager that yields a <code>Session</code> restored from the latest checkpoint or initialized from scratch if not checkpoint exists. The session is closed when the <code>with</code> block exits.</p>  <h4 id=\"Supervisor.prepare_or_wait_for_session\"><code>tf.train.Supervisor.prepare_or_wait_for_session(master='', config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True)</code></h4> <p>Make sure the model is ready to be used.</p> <p>Create a session on 'master', recovering or initializing the model as needed, or wait for a session to be ready. If running as the chief and <code>start_standard_service</code> is set to True, also call the session manager to start the standard services.</p>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>master</code>: name of the TensorFlow master to use. See the <code>tf.Session</code> constructor for how this is interpreted.</li> <li>\n<code>config</code>: Optional ConfigProto proto used to configure the session, which is passed as-is to create the session.</li> <li>\n<code>wait_for_checkpoint</code>: Whether we should wait for the availability of a checkpoint before creating Session. Defaults to False.</li> <li>\n<code>max_wait_secs</code>: Maximum time to wait for the session to become available.</li> <li>\n<code>start_standard_services</code>: Whether to start the standard services and the queue runners.</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>A Session object that can be used to drive the model.</p>  <h4 id=\"Supervisor.start_standard_services\"><code>tf.train.Supervisor.start_standard_services(sess)</code></h4> <p>Start the standard services for 'sess'.</p> <p>This starts services in the background. The services started depend on the parameters to the constructor and may include:</p> <ul> <li>A Summary thread computing summaries every save_summaries_secs.</li> <li>A Checkpoint thread saving the model every save_model_secs.</li> <li>A StepCounter thread measure step time.</li> </ul>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>sess</code>: A Session.</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>A list of threads that are running the standard services. You can use the Supervisor's Coordinator to join these threads with: sv.coord.Join(<list of threads>)</list></p>  <h5 id=\"raises-17\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: If called with a non-chief Supervisor.</li> <li>\n<code>ValueError</code>: If not <code>logdir</code> was passed to the constructor as the services need a log directory.</li> </ul>  <h4 id=\"Supervisor.start_queue_runners\"><code>tf.train.Supervisor.start_queue_runners(sess, queue_runners=None)</code></h4> <p>Start threads for <code>QueueRunners</code>.</p> <p>Note that the queue runners collected in the graph key <code>QUEUE_RUNNERS</code> are already started automatically when you create a session with the supervisor, so unless you have non-collected queue runners to start you do not need to call this explicitely.</p>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>sess</code>: A <code>Session</code>.</li> <li>\n<code>queue_runners</code>: A list of <code>QueueRunners</code>. If not specified, we'll use the list of queue runners gathered in the graph under the key <code>GraphKeys.QUEUE_RUNNERS</code>.</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>The list of threads started for the <code>QueueRunners</code>.</p>  <h4 id=\"Supervisor.summary_computed\"><code>tf.train.Supervisor.summary_computed(sess, summary, global_step=None)</code></h4> <p>Indicate that a summary was computed.</p>  <h5 id=\"args-42\">Args:</h5> <ul> <li>\n<code>sess</code>: A <code>Session</code> object.</li> <li>\n<code>summary</code>: A Summary proto, or a string holding a serialized summary proto.</li> <li>\n<code>global_step</code>: Int. global step this summary is associated with. If <code>None</code>, it will try to fetch the current step.</li> </ul>  <h5 id=\"raises-18\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if 'summary' is not a Summary proto or a string.</li> <li>\n<code>RuntimeError</code>: if the Supervisor was created without a <code>logdir</code>.</li> </ul>  <h4 id=\"Supervisor.stop\"><code>tf.train.Supervisor.stop(threads=None, close_summary_writer=True)</code></h4> <p>Stop the services and the coordinator.</p> <p>This does not close the session.</p>  <h5 id=\"args-43\">Args:</h5> <ul> <li>\n<code>threads</code>: Optional list of threads to join with the coordinator. If <code>None</code>, defaults to the threads running the standard services, the threads started for <code>QueueRunners</code>, and the threads started by the <code>loop()</code> method. To wait on additional threads, pass the list in this parameter.</li> <li>\n<code>close_summary_writer</code>: Whether to close the <code>summary_writer</code>. Defaults to <code>True</code> if the summary writer was created by the supervisor, <code>False</code> otherwise.</li> </ul>  <h4 id=\"Supervisor.request_stop\"><code>tf.train.Supervisor.request_stop(ex=None)</code></h4> <p>Request that the coordinator stop the threads.</p> <p>See <code>Coordinator.request_stop()</code>.</p>  <h5 id=\"args-44\">Args:</h5> <ul> <li>\n<code>ex</code>: Optional <code>Exception</code>, or Python <code>exc_info</code> tuple as returned by <code>sys.exc_info()</code>. If this is the first call to <code>request_stop()</code> the corresponding exception is recorded and re-raised from <code>join()</code>.</li> </ul>  <h4 id=\"Supervisor.should_stop\"><code>tf.train.Supervisor.should_stop()</code></h4> <p>Check if the coordinator was told to stop.</p> <p>See <code>Coordinator.should_stop()</code>.</p>  <h5 id=\"returns-32\">Returns:</h5> <p>True if the coordinator was told to stop, False otherwise.</p>  <h4 id=\"Supervisor.stop_on_exception\"><code>tf.train.Supervisor.stop_on_exception()</code></h4> <p>Context handler to stop the supervisor when an exception is raised.</p> <p>See <code>Coordinator.stop_on_exception()</code>.</p>  <h5 id=\"returns-33\">Returns:</h5> <p>A context handler.</p>  <h4 id=\"Supervisor.wait_for_stop\"><code>tf.train.Supervisor.wait_for_stop()</code></h4> <p>Block waiting for the coordinator to stop.</p>  <h4 id=\"other-methods-2\">Other Methods</h4>  <h4 id=\"Supervisor.Loop\"><code>tf.train.Supervisor.Loop(timer_interval_secs, target, args=None, kwargs=None)</code></h4> <p>Start a LooperThread that calls a function periodically.</p> <p>If <code>timer_interval_secs</code> is None the thread calls <code>target(*args, **kwargs)</code> repeatedly. Otherwise it calls it every <code>timer_interval_secs</code> seconds. The thread terminates when a stop is requested.</p> <p>The started thread is added to the list of threads managed by the supervisor so it does not need to be passed to the <code>stop()</code> method.</p>  <h5 id=\"args-45\">Args:</h5> <ul> <li>\n<code>timer_interval_secs</code>: Number. Time boundaries at which to call <code>target</code>.</li> <li>\n<code>target</code>: A callable object.</li> <li>\n<code>args</code>: Optional arguments to pass to <code>target</code> when calling it.</li> <li>\n<code>kwargs</code>: Optional keyword arguments to pass to <code>target</code> when calling it.</li> </ul>  <h5 id=\"returns-34\">Returns:</h5> <p>The started thread.</p>  <h4 id=\"Supervisor.PrepareSession\"><code>tf.train.Supervisor.PrepareSession(master='', config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True)</code></h4> <p>Make sure the model is ready to be used.</p> <p>Create a session on 'master', recovering or initializing the model as needed, or wait for a session to be ready. If running as the chief and <code>start_standard_service</code> is set to True, also call the session manager to start the standard services.</p>  <h5 id=\"args-46\">Args:</h5> <ul> <li>\n<code>master</code>: name of the TensorFlow master to use. See the <code>tf.Session</code> constructor for how this is interpreted.</li> <li>\n<code>config</code>: Optional ConfigProto proto used to configure the session, which is passed as-is to create the session.</li> <li>\n<code>wait_for_checkpoint</code>: Whether we should wait for the availability of a checkpoint before creating Session. Defaults to False.</li> <li>\n<code>max_wait_secs</code>: Maximum time to wait for the session to become available.</li> <li>\n<code>start_standard_services</code>: Whether to start the standard services and the queue runners.</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>A Session object that can be used to drive the model.</p>  <h4 id=\"Supervisor.RequestStop\"><code>tf.train.Supervisor.RequestStop(ex=None)</code></h4> <p>Request that the coordinator stop the threads.</p> <p>See <code>Coordinator.request_stop()</code>.</p>  <h5 id=\"args-47\">Args:</h5> <ul> <li>\n<code>ex</code>: Optional <code>Exception</code>, or Python <code>exc_info</code> tuple as returned by <code>sys.exc_info()</code>. If this is the first call to <code>request_stop()</code> the corresponding exception is recorded and re-raised from <code>join()</code>.</li> </ul>  <h4 id=\"Supervisor.ShouldStop\"><code>tf.train.Supervisor.ShouldStop()</code></h4> <p>Check if the coordinator was told to stop.</p> <p>See <code>Coordinator.should_stop()</code>.</p>  <h5 id=\"returns-36\">Returns:</h5> <p>True if the coordinator was told to stop, False otherwise.</p>  <h4 id=\"Supervisor.StartQueueRunners\"><code>tf.train.Supervisor.StartQueueRunners(sess, queue_runners=None)</code></h4> <p>Start threads for <code>QueueRunners</code>.</p> <p>Note that the queue runners collected in the graph key <code>QUEUE_RUNNERS</code> are already started automatically when you create a session with the supervisor, so unless you have non-collected queue runners to start you do not need to call this explicitely.</p>  <h5 id=\"args-48\">Args:</h5> <ul> <li>\n<code>sess</code>: A <code>Session</code>.</li> <li>\n<code>queue_runners</code>: A list of <code>QueueRunners</code>. If not specified, we'll use the list of queue runners gathered in the graph under the key <code>GraphKeys.QUEUE_RUNNERS</code>.</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <p>The list of threads started for the <code>QueueRunners</code>.</p>  <h4 id=\"Supervisor.StartStandardServices\"><code>tf.train.Supervisor.StartStandardServices(sess)</code></h4> <p>Start the standard services for 'sess'.</p> <p>This starts services in the background. The services started depend on the parameters to the constructor and may include:</p> <ul> <li>A Summary thread computing summaries every save_summaries_secs.</li> <li>A Checkpoint thread saving the model every save_model_secs.</li> <li>A StepCounter thread measure step time.</li> </ul>  <h5 id=\"args-49\">Args:</h5> <ul> <li>\n<code>sess</code>: A Session.</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>A list of threads that are running the standard services. You can use the Supervisor's Coordinator to join these threads with: sv.coord.Join(<list of threads>)</list></p>  <h5 id=\"raises-19\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: If called with a non-chief Supervisor.</li> <li>\n<code>ValueError</code>: If not <code>logdir</code> was passed to the constructor as the services need a log directory.</li> </ul>  <h4 id=\"Supervisor.Stop\"><code>tf.train.Supervisor.Stop(threads=None, close_summary_writer=True)</code></h4> <p>Stop the services and the coordinator.</p> <p>This does not close the session.</p>  <h5 id=\"args-50\">Args:</h5> <ul> <li>\n<code>threads</code>: Optional list of threads to join with the coordinator. If <code>None</code>, defaults to the threads running the standard services, the threads started for <code>QueueRunners</code>, and the threads started by the <code>loop()</code> method. To wait on additional threads, pass the list in this parameter.</li> <li>\n<code>close_summary_writer</code>: Whether to close the <code>summary_writer</code>. Defaults to <code>True</code> if the summary writer was created by the supervisor, <code>False</code> otherwise.</li> </ul>  <h4 id=\"Supervisor.StopOnException\"><code>tf.train.Supervisor.StopOnException()</code></h4> <p>Context handler to stop the supervisor when an exception is raised.</p> <p>See <code>Coordinator.stop_on_exception()</code>.</p>  <h5 id=\"returns-39\">Returns:</h5> <p>A context handler.</p>  <h4 id=\"Supervisor.SummaryComputed\"><code>tf.train.Supervisor.SummaryComputed(sess, summary, global_step=None)</code></h4> <p>Indicate that a summary was computed.</p>  <h5 id=\"args-51\">Args:</h5> <ul> <li>\n<code>sess</code>: A <code>Session</code> object.</li> <li>\n<code>summary</code>: A Summary proto, or a string holding a serialized summary proto.</li> <li>\n<code>global_step</code>: Int. global step this summary is associated with. If <code>None</code>, it will try to fetch the current step.</li> </ul>  <h5 id=\"raises-20\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if 'summary' is not a Summary proto or a string.</li> <li>\n<code>RuntimeError</code>: if the Supervisor was created without a <code>logdir</code>.</li> </ul>  <h4 id=\"Supervisor.WaitForStop\"><code>tf.train.Supervisor.WaitForStop()</code></h4> <p>Block waiting for the coordinator to stop.</p>  <h4 id=\"Supervisor.coord\"><code>tf.train.Supervisor.coord</code></h4> <p>Return the Coordinator used by the Supervisor.</p> <p>The Coordinator can be useful if you want to run multiple threads during your training.</p>  <h5 id=\"returns-40\">Returns:</h5> <p>A Coordinator object.</p>  <h4 id=\"Supervisor.global_step\"><code>tf.train.Supervisor.global_step</code></h4> <p>Return the global_step Tensor used by the supervisor.</p>  <h5 id=\"returns-41\">Returns:</h5> <p>An integer Tensor for the global_step.</p>  <h4 id=\"Supervisor.init_feed_dict\"><code>tf.train.Supervisor.init_feed_dict</code></h4> <p>Return the feed dictionary used when evaluating the <code>init_op</code>.</p>  <h5 id=\"returns-42\">Returns:</h5> <p>A feed dictionary or <code>None</code>.</p>  <h4 id=\"Supervisor.init_op\"><code>tf.train.Supervisor.init_op</code></h4> <p>Return the Init Op used by the supervisor.</p>  <h5 id=\"returns-43\">Returns:</h5> <p>An Op or <code>None</code>.</p>  <h4 id=\"Supervisor.is_chief\"><code>tf.train.Supervisor.is_chief</code></h4> <p>Return True if this is a chief supervisor.</p>  <h5 id=\"returns-44\">Returns:</h5> <p>A bool.</p>  <h4 id=\"Supervisor.loop\"><code>tf.train.Supervisor.loop(timer_interval_secs, target, args=None, kwargs=None)</code></h4> <p>Start a LooperThread that calls a function periodically.</p> <p>If <code>timer_interval_secs</code> is None the thread calls <code>target(*args, **kwargs)</code> repeatedly. Otherwise it calls it every <code>timer_interval_secs</code> seconds. The thread terminates when a stop is requested.</p> <p>The started thread is added to the list of threads managed by the supervisor so it does not need to be passed to the <code>stop()</code> method.</p>  <h5 id=\"args-52\">Args:</h5> <ul> <li>\n<code>timer_interval_secs</code>: Number. Time boundaries at which to call <code>target</code>.</li> <li>\n<code>target</code>: A callable object.</li> <li>\n<code>args</code>: Optional arguments to pass to <code>target</code> when calling it.</li> <li>\n<code>kwargs</code>: Optional keyword arguments to pass to <code>target</code> when calling it.</li> </ul>  <h5 id=\"returns-45\">Returns:</h5> <p>The started thread.</p>  <h4 id=\"Supervisor.ready_op\"><code>tf.train.Supervisor.ready_op</code></h4> <p>Return the Ready Op used by the supervisor.</p>  <h5 id=\"returns-46\">Returns:</h5> <p>An Op or <code>None</code>.</p>  <h4 id=\"Supervisor.save_model_secs\"><code>tf.train.Supervisor.save_model_secs</code></h4> <p>Return the delay between checkpoints.</p>  <h5 id=\"returns-47\">Returns:</h5> <p>A timestamp.</p>  <h4 id=\"Supervisor.save_path\"><code>tf.train.Supervisor.save_path</code></h4> <p>Return the save path used by the supervisor.</p>  <h5 id=\"returns-48\">Returns:</h5> <p>A string.</p>  <h4 id=\"Supervisor.save_summaries_secs\"><code>tf.train.Supervisor.save_summaries_secs</code></h4> <p>Return the delay between summary computations.</p>  <h5 id=\"returns-49\">Returns:</h5> <p>A timestamp.</p>  <h4 id=\"Supervisor.saver\"><code>tf.train.Supervisor.saver</code></h4> <p>Return the Saver used by the supervisor.</p>  <h5 id=\"returns-50\">Returns:</h5> <p>A Saver object.</p>  <h4 id=\"Supervisor.session_manager\"><code>tf.train.Supervisor.session_manager</code></h4> <p>Return the SessionManager used by the Supervisor.</p>  <h5 id=\"returns-51\">Returns:</h5> <p>A SessionManager object.</p>  <h4 id=\"Supervisor.summary_op\"><code>tf.train.Supervisor.summary_op</code></h4> <p>Return the Summary Tensor used by the chief supervisor.</p>  <h5 id=\"returns-52\">Returns:</h5> <p>A string Tensor for the summary or <code>None</code>.</p>  <h4 id=\"Supervisor.summary_writer\"><code>tf.train.Supervisor.summary_writer</code></h4> <p>Return the SummaryWriter used by the chief supervisor.</p>  <h5 id=\"returns-53\">Returns:</h5> <p>A SummaryWriter.</p>  <h3 id=\"SessionManager\"><code>class tf.train.SessionManager</code></h3> <p>Training helper that restores from checkpoint and creates session.</p> <p>This class is a small wrapper that takes care of session creation and checkpoint recovery. It also provides functions that to facilitate coordination among multiple training threads or processes.</p> <ul> <li>Checkpointing trained variables as the training progresses.</li> <li>Initializing variables on startup, restoring them from the most recent checkpoint after a crash, or wait for checkpoints to become available.</li> </ul>  <h3 id=\"usage-2\">Usage:</h3> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.Graph().as_default():\n   ...add operations to the graph...\n  # Create a SessionManager that will checkpoint the model in '/tmp/mydir'.\n  sm = SessionManager()\n  sess = sm.prepare_session(master, init_op, saver, checkpoint_dir)\n  # Use the session to train the graph.\n  while True:\n    sess.run(&lt;my_train_op&gt;)\n</pre> <p><code>prepare_session()</code> initializes or restores a model. It requires <code>init_op</code> and <code>saver</code> as an argument.</p> <p>A second process could wait for the model to be ready by doing the following:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">with tf.Graph().as_default():\n   ...add operations to the graph...\n  # Create a SessionManager that will wait for the model to become ready.\n  sm = SessionManager()\n  sess = sm.wait_for_session(master)\n  # Use the session to train the graph.\n  while True:\n    sess.run(&lt;my_train_op&gt;)\n</pre> <p><code>wait_for_session()</code> waits for a model to be initialized by other processes.</p>  <h4 id=\"SessionManager.__init__\"><code>tf.train.SessionManager.__init__(local_init_op=None, ready_op=None, graph=None, recovery_wait_secs=30)</code></h4> <p>Creates a SessionManager.</p> <p>The <code>local_init_op</code> is an <code>Operation</code> that is run always after a new session was created. If <code>None</code>, this step is skipped.</p> <p>The <code>ready_op</code> is an <code>Operation</code> used to check if the model is ready. The model is considered ready if that operation returns an empty string tensor. If the operation returns non empty string tensor, the elements are concatenated and used to indicate to the user why the model is not ready.</p> <p>If <code>ready_op</code> is <code>None</code>, the model is not checked for readiness.</p> <p><code>recovery_wait_secs</code> is the number of seconds between checks that the model is ready. It is used by processes to wait for a model to be initialized or restored. Defaults to 30 seconds.</p>  <h5 id=\"args-53\">Args:</h5> <ul> <li>\n<code>local_init_op</code>: An <code>Operation</code> run immediately after session creation. Usually used to initialize tables and local variables.</li> <li>\n<code>ready_op</code>: An <code>Operation</code> to check if the model is initialized.</li> <li>\n<code>graph</code>: The <code>Graph</code> that the model will use.</li> <li>\n<code>recovery_wait_secs</code>: Seconds between checks for the model to be ready.</li> </ul>  <h4 id=\"SessionManager.prepare_session\"><code>tf.train.SessionManager.prepare_session(master, init_op=None, saver=None, checkpoint_dir=None, wait_for_checkpoint=False, max_wait_secs=7200, config=None, init_feed_dict=None, init_fn=None)</code></h4> <p>Creates a <code>Session</code>. Makes sure the model is ready to be used.</p> <p>Creates a <code>Session</code> on 'master'. If a <code>saver</code> object is passed in, and <code>checkpoint_dir</code> points to a directory containing valid checkpoint files, then it will try to recover the model from checkpoint. If no checkpoint files are available, and <code>wait_for_checkpoint</code> is <code>True</code>, then the process would check every <code>recovery_wait_secs</code>, up to <code>max_wait_secs</code>, for recovery to succeed.</p> <p>If the model cannot be recovered successfully then it is initialized by either running the provided <code>init_op</code>, or calling the provided <code>init_fn</code>. It is an error if the model cannot be recovered and neither an <code>init_op</code> or an <code>init_fn</code> are passed.</p> <p>This is a convenient function for the following, with a few error checks added:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">sess, initialized = self.recover_session(master)\nif not initialized:\n  if init_op:\n    sess.run(init_op, feed_dict=init_feed_dict)\n  if init_fn;\n    init_fn(sess)\nreturn sess\n</pre>  <h5 id=\"args-54\">Args:</h5> <ul> <li>\n<code>master</code>: <code>String</code> representation of the TensorFlow master to use.</li> <li>\n<code>init_op</code>: Optional <code>Operation</code> used to initialize the model.</li> <li>\n<code>saver</code>: A <code>Saver</code> object used to restore a model.</li> <li>\n<code>checkpoint_dir</code>: Path to the checkpoint files.</li> <li>\n<code>wait_for_checkpoint</code>: Whether to wait for checkpoint to become available.</li> <li>\n<code>max_wait_secs</code>: Maximum time to wait for checkpoints to become available.</li> <li>\n<code>config</code>: Optional <code>ConfigProto</code> proto used to configure the session.</li> <li>\n<code>init_feed_dict</code>: Optional dictionary that maps <code>Tensor</code> objects to feed values. This feed dictionary is passed to the session <code>run()</code> call when running the init op.</li> <li>\n<code>init_fn</code>: Optional callable used to initialize the model. Called after the optional <code>init_op</code> is called. The callable must accept one argument, the session being initialized.</li> </ul>  <h5 id=\"returns-54\">Returns:</h5> <p>A <code>Session</code> object that can be used to drive the model.</p>  <h5 id=\"raises-21\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: If the model cannot be initialized or recovered.</li> </ul>  <h4 id=\"SessionManager.recover_session\"><code>tf.train.SessionManager.recover_session(master, saver=None, checkpoint_dir=None, wait_for_checkpoint=False, max_wait_secs=7200, config=None)</code></h4> <p>Creates a <code>Session</code>, recovering if possible.</p> <p>Creates a new session on 'master'. If the session is not initialized and can be recovered from a checkpoint, recover it.</p>  <h5 id=\"args-55\">Args:</h5> <ul> <li>\n<code>master</code>: <code>String</code> representation of the TensorFlow master to use.</li> <li>\n<code>saver</code>: A <code>Saver</code> object used to restore a model.</li> <li>\n<code>checkpoint_dir</code>: Path to the checkpoint files.</li> <li>\n<code>wait_for_checkpoint</code>: Whether to wait for checkpoint to become available.</li> <li>\n<code>max_wait_secs</code>: Maximum time to wait for checkpoints to become available.</li> <li>\n<code>config</code>: Optional <code>ConfigProto</code> proto used to configure the session.</li> </ul>  <h5 id=\"returns-55\">Returns:</h5> <p>A pair (sess, initialized) where 'initialized' is <code>True</code> if the session could be recovered, <code>False</code> otherwise.</p>  <h4 id=\"SessionManager.wait_for_session\"><code>tf.train.SessionManager.wait_for_session(master, config=None, max_wait_secs=inf)</code></h4> <p>Creates a new <code>Session</code> and waits for model to be ready.</p> <p>Creates a new <code>Session</code> on 'master'. Waits for the model to be initialized or recovered from a checkpoint. It's expected that another thread or process will make the model ready, and that this is intended to be used by threads/processes that participate in a distributed training configuration where a different thread/process is responsible for initializing or recovering the model being trained.</p> <p>NB: The amount of time this method waits for the session is bounded by max_wait_secs. By default, this function will wait indefinitely.</p>  <h5 id=\"args-56\">Args:</h5> <ul> <li>\n<code>master</code>: <code>String</code> representation of the TensorFlow master to use.</li> <li>\n<code>config</code>: Optional ConfigProto proto used to configure the session.</li> <li>\n<code>max_wait_secs</code>: Maximum time to wait for the session to become available.</li> </ul>  <h5 id=\"returns-56\">Returns:</h5> <p>A <code>Session</code>. May be None if the operation exceeds the timeout specified by config.operation_timeout_in_ms.</p>  <h5 id=\"raises-22\">Raises:</h5> <p>tf.DeadlineExceededError: if the session is not available after max_wait_secs.</p>  <h3 id=\"ClusterSpec\"><code>class tf.train.ClusterSpec</code></h3> <p>Represents a cluster as a set of \"tasks\", organized into \"jobs\".</p> <p>A <code>tf.train.ClusterSpec</code> represents the set of processes that participate in a distributed TensorFlow computation. Every <a href=\"#Server\"><code>tf.train.Server</code></a> is constructed in a particular cluster.</p> <p>To create a cluster with two jobs and five tasks, you specify the mapping from job names to lists of network addresses (typically hostname-port pairs).</p> <pre class=\"\">cluster = tf.train.ClusterSpec({\"worker\": [\"worker0.example.com:2222\",\n                                           \"worker1.example.com:2222\",\n                                           \"worker2.example.com:2222\"],\n                                \"ps\": [\"ps0.example.com:2222\",\n                                       \"ps1.example.com:2222\"]})\n</pre>  <h4 id=\"ClusterSpec.as_cluster_def\"><code>tf.train.ClusterSpec.as_cluster_def()</code></h4> <p>Returns a <code>tf.train.ClusterDef</code> protocol buffer based on this cluster.</p>  <h4 id=\"ClusterSpec.as_dict\"><code>tf.train.ClusterSpec.as_dict()</code></h4> <p>Returns a dictionary from job names to lists of network addresses.</p>  <h4 id=\"other-methods-3\">Other Methods</h4>  <h4 id=\"ClusterSpec.__init__\"><code>tf.train.ClusterSpec.__init__(cluster)</code></h4> <p>Creates a <code>ClusterSpec</code>.</p>  <h5 id=\"args-57\">Args:</h5> <ul> <li>\n<code>cluster</code>: A dictionary mapping one or more job names to lists of network addresses, or a <code>tf.train.ClusterDef</code> protocol buffer.</li> </ul>  <h5 id=\"raises-23\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>cluster</code> is not a dictionary mapping strings to lists of strings, and not a <code>tf.train.ClusterDef</code> protobuf.</li> </ul>  <h4 id=\"ClusterSpec.job_tasks\"><code>tf.train.ClusterSpec.job_tasks(job_name)</code></h4> <p>Returns a list of tasks in the given job.</p>  <h5 id=\"args-58\">Args:</h5> <ul> <li>\n<code>job_name</code>: The string name of a job in this cluster.</li> </ul>  <h5 id=\"returns-57\">Returns:</h5> <p>A list of strings, corresponding to the network addresses of tasks in the given job, ordered by task index.</p>  <h5 id=\"raises-24\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>job_name</code> does not name a job in this cluster.</li> </ul>  <h4 id=\"ClusterSpec.jobs\"><code>tf.train.ClusterSpec.jobs</code></h4> <p>Returns a list of job names in this cluster.</p>  <h5 id=\"returns-58\">Returns:</h5> <p>A list of strings, corresponding to the names of jobs in this cluster.</p>  <h3 id=\"replica_device_setter\"><code>tf.train.replica_device_setter(ps_tasks=0, ps_device='/job:ps', worker_device='/job:worker', merge_devices=True, cluster=None, ps_ops=None)</code></h3> <p>Return a <code>device function</code> to use when building a Graph for replicas.</p> <p>Device Functions are used in <code>with tf.device(device_function):</code> statement to automatically assign devices to <code>Operation</code> objects as they are constructed, Device constraints are added from the inner-most context first, working outwards. The merging behavior adds constraints to fields that are yet unset by a more inner context. Currently the fields are (job, task, cpu/gpu).</p> <p>If <code>cluster</code> is <code>None</code>, and <code>ps_tasks</code> is 0, the returned function is a no-op.</p> <p>For example,</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker\n# jobs on hosts worker0, worker1 and worker2.\ncluster_spec = {\n    \"ps\": [\"ps0:2222\", \"ps1:2222\"],\n    \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}\nwith tf.device(tf.replica_device_setter(cluster=cluster_spec)):\n  # Build your graph\n  v1 = tf.Variable(...)  # assigned to /job:ps/task:0\n  v2 = tf.Variable(...)  # assigned to /job:ps/task:1\n  v3 = tf.Variable(...)  # assigned to /job:ps/task:0\n# Run compute\n</pre>  <h5 id=\"args-59\">Args:</h5> <ul> <li>\n<code>ps_tasks</code>: Number of tasks in the <code>ps</code> job.</li> <li>\n<code>ps_device</code>: String. Device of the <code>ps</code> job. If empty no <code>ps</code> job is used. Defaults to <code>ps</code>.</li> <li>\n<code>worker_device</code>: String. Device of the <code>worker</code> job. If empty no <code>worker</code> job is used.</li> <li>\n<code>merge_devices</code>: <code>Boolean</code>. If <code>True</code>, merges or only sets a device if the device constraint is completely unset. merges device specification rather than overriding them.</li> <li>\n<code>cluster</code>: <code>ClusterDef</code> proto or <code>ClusterSpec</code>.</li> <li>\n<code>ps_ops</code>: List of <code>Operation</code> objects that need to be placed on <code>ps</code> devices.</li> </ul>  <h5 id=\"returns-59\">Returns:</h5> <p>A function to pass to <code>tf.device()</code>.</p>  <h5 id=\"raises-25\">Raises:</h5> <p>TypeError if <code>cluster</code> is not a dictionary or <code>ClusterDef</code> protocol buffer.</p>  <h2 id=\"summary-operations\">Summary Operations</h2> <p>The following ops output <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto\" rel=\"noreferrer\"><code>Summary</code></a> protocol buffers as serialized string tensors.</p> <p>You can fetch the output of a summary op in a session, and pass it to a <a href=\"train#SummaryWriter\">SummaryWriter</a> to append it to an event file. Event files contain <a href=\"https://www.tensorflow.org/code/tensorflow/core/util/event.proto\" rel=\"noreferrer\"><code>Event</code></a> protos that can contain <code>Summary</code> protos along with the timestamp and step. You can then use TensorBoard to visualize the contents of the event files. See <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html\">TensorBoard and Summaries</a> for more details.</p>  <h3 id=\"scalar_summary\"><code>tf.scalar_summary(tags, values, collections=None, name=None)</code></h3> <p>Outputs a <code>Summary</code> protocol buffer with scalar values.</p> <p>The input <code>tags</code> and <code>values</code> must have the same shape. The generated summary has a summary value for each tag-value pair in <code>tags</code> and <code>values</code>.</p>  <h5 id=\"args-60\">Args:</h5> <ul> <li>\n<code>tags</code>: A <code>string</code> <code>Tensor</code>. Tags for the summaries.</li> <li>\n<code>values</code>: A real numeric Tensor. Values for the summaries.</li> <li>\n<code>collections</code>: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to <code>[GraphKeys.SUMMARIES]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-60\">Returns:</h5> <p>A scalar <code>Tensor</code> of type <code>string</code>. The serialized <code>Summary</code> protocol buffer.</p>  <h3 id=\"image_summary\"><code>tf.image_summary(tag, tensor, max_images=3, collections=None, name=None)</code></h3> <p>Outputs a <code>Summary</code> protocol buffer with images.</p> <p>The summary has up to <code>max_images</code> summary values containing images. The images are built from <code>tensor</code> which must be 4-D with shape <code>[batch_size,\nheight, width, channels]</code> and where <code>channels</code> can be:</p> <ul> <li>1: <code>tensor</code> is interpreted as Grayscale.</li> <li>3: <code>tensor</code> is interpreted as RGB.</li> <li>4: <code>tensor</code> is interpreted as RGBA.</li> </ul> <p>The images have the same number of channels as the input tensor. For float input, the values are normalized one image at a time to fit in the range <code>[0, 255]</code>. <code>uint8</code> values are unchanged. The op uses two different normalization algorithms:</p> <ul> <li><p>If the input values are all positive, they are rescaled so the largest one is 255.</p></li> <li><p>If any input value is negative, the values are shifted so input value 0.0 is at 127. They are then rescaled so that either the smallest value is 0, or the largest one is 255.</p></li> </ul> <p>The <code>tag</code> argument is a scalar <code>Tensor</code> of type <code>string</code>. It is used to build the <code>tag</code> of the summary values:</p> <ul> <li>If <code>max_images</code> is 1, the summary value tag is '*tag*/image'.</li> <li>If <code>max_images</code> is greater than 1, the summary value tags are generated sequentially as '*tag*/image/0', '*tag*/image/1', etc.</li> </ul>  <h5 id=\"args-61\">Args:</h5> <ul> <li>\n<code>tag</code>: A scalar <code>Tensor</code> of type <code>string</code>. Used to build the <code>tag</code> of the summary values.</li> <li>\n<code>tensor</code>: A 4-D <code>uint8</code> or <code>float32</code> <code>Tensor</code> of shape <code>[batch_size, height,\nwidth, channels]</code> where <code>channels</code> is 1, 3, or 4.</li> <li>\n<code>max_images</code>: Max number of batch elements to generate images for.</li> <li>\n<code>collections</code>: Optional list of ops.GraphKeys. The collections to add the summary to. Defaults to [ops.GraphKeys.SUMMARIES]</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-61\">Returns:</h5> <p>A scalar <code>Tensor</code> of type <code>string</code>. The serialized <code>Summary</code> protocol buffer.</p>  <h3 id=\"audio_summary\"><code>tf.audio_summary(tag, tensor, sample_rate, max_outputs=3, collections=None, name=None)</code></h3> <p>Outputs a <code>Summary</code> protocol buffer with audio.</p> <p>The summary has up to <code>max_outputs</code> summary values containing audio. The audio is built from <code>tensor</code> which must be 3-D with shape <code>[batch_size,\nframes, channels]</code> or 2-D with shape <code>[batch_size, frames]</code>. The values are assumed to be in the range of <code>[-1.0, 1.0]</code> with a sample rate of <code>sample_rate</code>.</p> <p>The <code>tag</code> argument is a scalar <code>Tensor</code> of type <code>string</code>. It is used to build the <code>tag</code> of the summary values:</p> <ul> <li>If <code>max_outputs</code> is 1, the summary value tag is '*tag*/audio'.</li> <li>If <code>max_outputs</code> is greater than 1, the summary value tags are generated sequentially as '*tag*/audio/0', '*tag*/audio/1', etc.</li> </ul>  <h5 id=\"args-62\">Args:</h5> <ul> <li>\n<code>tag</code>: A scalar <code>Tensor</code> of type <code>string</code>. Used to build the <code>tag</code> of the summary values.</li> <li>\n<code>tensor</code>: A 3-D <code>float32</code> <code>Tensor</code> of shape <code>[batch_size, frames, channels]</code> or a 2-D <code>float32</code> <code>Tensor</code> of shape <code>[batch_size, frames]</code>.</li> <li>\n<code>sample_rate</code>: The sample rate of the signal in hertz.</li> <li>\n<code>max_outputs</code>: Max number of batch elements to generate audio for.</li> <li>\n<code>collections</code>: Optional list of ops.GraphKeys. The collections to add the summary to. Defaults to [ops.GraphKeys.SUMMARIES]</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-62\">Returns:</h5> <p>A scalar <code>Tensor</code> of type <code>string</code>. The serialized <code>Summary</code> protocol buffer.</p>  <h3 id=\"histogram_summary\"><code>tf.histogram_summary(tag, values, collections=None, name=None)</code></h3> <p>Outputs a <code>Summary</code> protocol buffer with a histogram.</p> <p>The generated <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto\" rel=\"noreferrer\"><code>Summary</code></a> has one summary value containing a histogram for <code>values</code>.</p> <p>This op reports an <code>InvalidArgument</code> error if any value is not finite.</p>  <h5 id=\"args-63\">Args:</h5> <ul> <li>\n<code>tag</code>: A <code>string</code> <code>Tensor</code>. 0-D. Tag to use for the summary value.</li> <li>\n<code>values</code>: A real numeric <code>Tensor</code>. Any shape. Values to use to build the histogram.</li> <li>\n<code>collections</code>: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to <code>[GraphKeys.SUMMARIES]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-63\">Returns:</h5> <p>A scalar <code>Tensor</code> of type <code>string</code>. The serialized <code>Summary</code> protocol buffer.</p>  <h3 id=\"zero_fraction\"><code>tf.nn.zero_fraction(value, name=None)</code></h3> <p>Returns the fraction of zeros in <code>value</code>.</p> <p>If <code>value</code> is empty, the result is <code>nan</code>.</p> <p>This is useful in summaries to measure and report sparsity. For example,</p> <pre class=\"\">z = tf.Relu(...)\nsumm = tf.scalar_summary('sparsity', tf.nn.zero_fraction(z))\n</pre>  <h5 id=\"args-64\">Args:</h5> <ul> <li>\n<code>value</code>: A tensor of numeric type.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-64\">Returns:</h5> <p>The fraction of zeros in <code>value</code>, with type <code>float32</code>.</p>  <h3 id=\"merge_summary\"><code>tf.merge_summary(inputs, collections=None, name=None)</code></h3> <p>Merges summaries.</p> <p>This op creates a <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto\" rel=\"noreferrer\"><code>Summary</code></a> protocol buffer that contains the union of all the values in the input summaries.</p> <p>When the Op is run, it reports an <code>InvalidArgument</code> error if multiple values in the summaries to merge use the same tag.</p>  <h5 id=\"args-65\">Args:</h5> <ul> <li>\n<code>inputs</code>: A list of <code>string</code> <code>Tensor</code> objects containing serialized <code>Summary</code> protocol buffers.</li> <li>\n<code>collections</code>: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to <code>[GraphKeys.SUMMARIES]</code>.</li> <li>\n<code>name</code>: A name for the operation (optional).</li> </ul>  <h5 id=\"returns-65\">Returns:</h5> <p>A scalar <code>Tensor</code> of type <code>string</code>. The serialized <code>Summary</code> protocol buffer resulting from the merging.</p>  <h3 id=\"merge_all_summaries\"><code>tf.merge_all_summaries(key='summaries')</code></h3> <p>Merges all summaries collected in the default graph.</p>  <h5 id=\"args-66\">Args:</h5> <ul> <li>\n<code>key</code>: <code>GraphKey</code> used to collect the summaries. Defaults to <code>GraphKeys.SUMMARIES</code>.</li> </ul>  <h5 id=\"returns-66\">Returns:</h5> <p>If no summaries were collected, returns None. Otherwise returns a scalar <code>Tensor</code> of type <code>string</code> containing the serialized <code>Summary</code> protocol buffer resulting from the merging.</p>  <h2 id=\"adding-summaries-to-event-files\">Adding Summaries to Event Files</h2> <p>See <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html\">Summaries and TensorBoard</a> for an overview of summaries, event files, and visualization in TensorBoard.</p>  <h3 id=\"SummaryWriter\"><code>class tf.train.SummaryWriter</code></h3> <p>Writes <code>Summary</code> protocol buffers to event files.</p> <p>The <code>SummaryWriter</code> class provides a mechanism to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.</p>  <h4 id=\"SummaryWriter.__init__\"><code>tf.train.SummaryWriter.__init__(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None)</code></h4> <p>Creates a <code>SummaryWriter</code> and an event file.</p> <p>On construction the summary writer creates a new event file in <code>logdir</code>. This event file will contain <code>Event</code> protocol buffers constructed when you call one of the following functions: <code>add_summary()</code>, <code>add_session_log()</code>, <code>add_event()</code>, or <code>add_graph()</code>.</p> <p>If you pass a <code>Graph</code> to the constructor it is added to the event file. (This is equivalent to calling <code>add_graph()</code> later).</p> <p>TensorBoard will pick the graph from the file and display it graphically so you can interactively explore the graph you built. You will usually pass the graph from the session in which you launched it:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">...create a graph...\n# Launch the graph in a session.\nsess = tf.Session()\n# Create a summary writer, add the 'graph' to the event file.\nwriter = tf.train.SummaryWriter(&lt;some-directory&gt;, sess.graph)\n</pre> <p>The other arguments to the constructor control the asynchronous writes to the event file:</p> <ul> <li>\n<code>flush_secs</code>: How often, in seconds, to flush the added summaries and events to disk.</li> <li>\n<code>max_queue</code>: Maximum number of summaries or events pending to be written to disk before one of the 'add' calls block.</li> </ul>  <h5 id=\"args-67\">Args:</h5> <ul> <li>\n<code>logdir</code>: A string. Directory where event file will be written.</li> <li>\n<code>graph</code>: A <code>Graph</code> object, such as <code>sess.graph</code>.</li> <li>\n<code>max_queue</code>: Integer. Size of the queue for pending events and summaries.</li> <li>\n<code>flush_secs</code>: Number. How often, in seconds, to flush the pending events and summaries to disk.</li> <li>\n<code>graph_def</code>: DEPRECATED: Use the <code>graph</code> argument instead.</li> </ul>  <h4 id=\"SummaryWriter.add_summary\"><code>tf.train.SummaryWriter.add_summary(summary, global_step=None)</code></h4> <p>Adds a <code>Summary</code> protocol buffer to the event file.</p> <p>This method wraps the provided summary in an <code>Event</code> protocol buffer and adds it to the event file.</p> <p>You can pass the result of evaluating any summary op, using <a href=\"client#Session.run\"><code>Session.run()</code></a> or <a href=\"framework#Tensor.eval\"><code>Tensor.eval()</code></a>, to this function. Alternatively, you can pass a <code>tf.Summary</code> protocol buffer that you populate with your own data. The latter is commonly done to report evaluation results in event files.</p>  <h5 id=\"args-68\">Args:</h5> <ul> <li>\n<code>summary</code>: A <code>Summary</code> protocol buffer, optionally serialized as a string.</li> <li>\n<code>global_step</code>: Number. Optional global step value to record with the summary.</li> </ul>  <h4 id=\"SummaryWriter.add_session_log\"><code>tf.train.SummaryWriter.add_session_log(session_log, global_step=None)</code></h4> <p>Adds a <code>SessionLog</code> protocol buffer to the event file.</p> <p>This method wraps the provided session in an <code>Event</code> procotol buffer and adds it to the event file.</p>  <h5 id=\"args-69\">Args:</h5> <ul> <li>\n<code>session_log</code>: A <code>SessionLog</code> protocol buffer.</li> <li>\n<code>global_step</code>: Number. Optional global step value to record with the summary.</li> </ul>  <h4 id=\"SummaryWriter.add_event\"><code>tf.train.SummaryWriter.add_event(event)</code></h4> <p>Adds an event to the event file.</p>  <h5 id=\"args-70\">Args:</h5> <ul> <li>\n<code>event</code>: An <code>Event</code> protocol buffer.</li> </ul>  <h4 id=\"SummaryWriter.add_graph\"><code>tf.train.SummaryWriter.add_graph(graph, global_step=None, graph_def=None)</code></h4> <p>Adds a <code>Graph</code> to the event file.</p> <p>The graph described by the protocol buffer will be displayed by TensorBoard. Most users pass a graph in the constructor instead.</p>  <h5 id=\"args-71\">Args:</h5> <ul> <li>\n<code>graph</code>: A <code>Graph</code> object, such as <code>sess.graph</code>.</li> <li>\n<code>global_step</code>: Number. Optional global step counter to record with the graph.</li> <li>\n<code>graph_def</code>: DEPRECATED. Use the <code>graph</code> parameter instead.</li> </ul>  <h5 id=\"raises-26\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If both graph and graph_def are passed to the method.</li> </ul>  <h4 id=\"SummaryWriter.add_run_metadata\"><code>tf.train.SummaryWriter.add_run_metadata(run_metadata, tag, global_step=None)</code></h4> <p>Adds a metadata information for a single session.run() call.</p>  <h5 id=\"args-72\">Args:</h5> <ul> <li>\n<code>run_metadata</code>: A <code>RunMetadata</code> protobuf object.</li> <li>\n<code>tag</code>: The tag name for this metadata.</li> <li>\n<code>global_step</code>: Number. Optional global step counter to record with the StepStats.</li> </ul>  <h5 id=\"raises-27\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the provided tag was already used for this type of event.</li> </ul>  <h4 id=\"SummaryWriter.flush\"><code>tf.train.SummaryWriter.flush()</code></h4> <p>Flushes the event file to disk.</p> <p>Call this method to make sure that all pending events have been written to disk.</p>  <h4 id=\"SummaryWriter.close\"><code>tf.train.SummaryWriter.close()</code></h4> <p>Flushes the event file to disk and close the file.</p> <p>Call this method when you do not need the summary writer anymore.</p>  <h4 id=\"other-methods-4\">Other Methods</h4>  <h4 id=\"SummaryWriter.reopen\"><code>tf.train.SummaryWriter.reopen()</code></h4> <p>Reopens the summary writer.</p> <p>Can be called after <code>close()</code> to add more events in the same directory. The events will go into a new events file.</p> <p>Does nothing if the summary writer was not closed.</p>  <h3 id=\"summary_iterator\"><code>tf.train.summary_iterator(path)</code></h3> <p>An iterator for reading <code>Event</code> protocol buffers from an event file.</p> <p>You can use this function to read events written to an event file. It returns a Python iterator that yields <code>Event</code> protocol buffers.</p> <p>Example: Print the contents of an events file.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">for e in tf.train.summary_iterator(path to events file):\n    print(e)\n</pre> <p>Example: Print selected summary values.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># This example supposes that the events file contains summaries with a\n# summary value tag 'loss'.  These could have been added by calling\n# `add_summary()`, passing the output of a scalar summary op created with\n# with: `tf.scalar_summary(['loss'], loss_tensor)`.\nfor e in tf.train.summary_iterator(path to events file):\n    for v in e.summary.value:\n        if v.tag == 'loss':\n            print(v.simple_value)\n</pre> <p>See the protocol buffer definitions of <a href=\"https://www.tensorflow.org/code/tensorflow/core/util/event.proto\" rel=\"noreferrer\">Event</a> and <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto\" rel=\"noreferrer\">Summary</a> for more information about their attributes.</p>  <h5 id=\"args-73\">Args:</h5> <ul> <li>\n<code>path</code>: The path to an event file created by a <code>SummaryWriter</code>.</li> </ul>  <h5 id=\"yields-2\">Yields:</h5> <p><code>Event</code> protocol buffers.</p>  <h2 id=\"training-utilities\">Training utilities</h2>  <h3 id=\"global_step\"><code>tf.train.global_step(sess, global_step_tensor)</code></h3> <p>Small helper to get the global step.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Creates a variable to hold the global_step.\nglobal_step_tensor = tf.Variable(10, trainable=False, name='global_step')\n# Creates a session.\nsess = tf.Session()\n# Initializes the variable.\nsess.run(global_step_tensor.initializer)\nprint('global_step: %s' % tf.train.global_step(sess, global_step_tensor))\n\nglobal_step: 10\n</pre>  <h5 id=\"args-74\">Args:</h5> <ul> <li>\n<code>sess</code>: A TensorFlow <code>Session</code> object.</li> <li>\n<code>global_step_tensor</code>: <code>Tensor</code> or the <code>name</code> of the operation that contains the global step.</li> </ul>  <h5 id=\"returns-67\">Returns:</h5> <p>The global step value.</p>  <h3 id=\"write_graph\"><code>tf.train.write_graph(graph_def, logdir, name, as_text=True)</code></h3> <p>Writes a graph proto to a file.</p> <p>The graph is written as a binary proto unless <code>as_text</code> is <code>True</code>.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">v = tf.Variable(0, name='my_variable')\nsess = tf.Session()\ntf.train.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt')\n</pre>  <h5 id=\"args-75\">Args:</h5> <ul> <li>\n<code>graph_def</code>: A <code>GraphDef</code> protocol buffer.</li> <li>\n<code>logdir</code>: Directory where to write the graph. This can refer to remote filesystems, such as Google Cloud Storage (GCS).</li> <li>\n<code>name</code>: Filename for the graph.</li> <li>\n<code>as_text</code>: If <code>True</code>, writes the graph as an ASCII proto.</li> </ul>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"LooperThread\"><code>class tf.train.LooperThread</code></h3> <p>A thread that runs code repeatedly, optionally on a timer.</p> <p>This thread class is intended to be used with a <code>Coordinator</code>. It repeatedly runs code specified either as <code>target</code> and <code>args</code> or by the <code>run_loop()</code> method.</p> <p>Before each run the thread checks if the coordinator has requested stop. In that case the looper thread terminates immediately.</p> <p>If the code being run raises an exception, that exception is reported to the coordinator and the thread terminates. The coordinator will then request all the other threads it coordinates to stop.</p> <p>You typically pass looper threads to the supervisor <code>Join()</code> method.</p>  <h4 id=\"LooperThread.__init__\"><code>tf.train.LooperThread.__init__(coord, timer_interval_secs, target=None, args=None, kwargs=None)</code></h4> <p>Create a LooperThread.</p>  <h5 id=\"args-76\">Args:</h5> <ul> <li>\n<code>coord</code>: A Coordinator.</li> <li>\n<code>timer_interval_secs</code>: Time boundaries at which to call Run(), or None if it should be called back to back.</li> <li>\n<code>target</code>: Optional callable object that will be executed in the thread.</li> <li>\n<code>args</code>: Optional arguments to pass to <code>target</code> when calling it.</li> <li>\n<code>kwargs</code>: Optional keyword arguments to pass to <code>target</code> when calling it.</li> </ul>  <h5 id=\"raises-28\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If one of the arguments is invalid.</li> </ul>  <h4 id=\"LooperThread.daemon\"><code>tf.train.LooperThread.daemon</code></h4> <p>A boolean value indicating whether this thread is a daemon thread (True) or not (False).</p> <p>This must be set before start() is called, otherwise RuntimeError is raised. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to daemon = False.</p> <p>The entire Python program exits when no alive non-daemon threads are left.</p>  <h4 id=\"LooperThread.getName\"><code>tf.train.LooperThread.getName()</code></h4>  <h4 id=\"LooperThread.ident\"><code>tf.train.LooperThread.ident</code></h4> <p>Thread identifier of this thread or None if it has not been started.</p> <p>This is a nonzero integer. See the thread.get_ident() function. Thread identifiers may be recycled when a thread exits and another thread is created. The identifier is available even after the thread has exited.</p>  <h4 id=\"LooperThread.isAlive\"><code>tf.train.LooperThread.isAlive()</code></h4> <p>Return whether the thread is alive.</p> <p>This method returns True just before the run() method starts until just after the run() method terminates. The module function enumerate() returns a list of all alive threads.</p>  <h4 id=\"LooperThread.isDaemon\"><code>tf.train.LooperThread.isDaemon()</code></h4>  <h4 id=\"LooperThread.is_alive\"><code>tf.train.LooperThread.is_alive()</code></h4> <p>Return whether the thread is alive.</p> <p>This method returns True just before the run() method starts until just after the run() method terminates. The module function enumerate() returns a list of all alive threads.</p>  <h4 id=\"LooperThread.join\"><code>tf.train.LooperThread.join(timeout=None)</code></h4> <p>Wait until the thread terminates.</p> <p>This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs.</p> <p>When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call isAlive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out.</p> <p>When the timeout argument is not present or None, the operation will block until the thread terminates.</p> <p>A thread can be join()ed many times.</p> <p>join() raises a RuntimeError if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to join() a thread before it has been started and attempts to do so raises the same exception.</p>  <h4 id=\"LooperThread.loop\"><code>tf.train.LooperThread.loop(coord, timer_interval_secs, target, args=None, kwargs=None)</code></h4> <p>Start a LooperThread that calls a function periodically.</p> <p>If <code>timer_interval_secs</code> is None the thread calls <code>target(args)</code> repeatedly. Otherwise <code>target(args)</code> is called every <code>timer_interval_secs</code> seconds. The thread terminates when a stop of the coordinator is requested.</p>  <h5 id=\"args-77\">Args:</h5> <ul> <li>\n<code>coord</code>: A Coordinator.</li> <li>\n<code>timer_interval_secs</code>: Number. Time boundaries at which to call <code>target</code>.</li> <li>\n<code>target</code>: A callable object.</li> <li>\n<code>args</code>: Optional arguments to pass to <code>target</code> when calling it.</li> <li>\n<code>kwargs</code>: Optional keyword arguments to pass to <code>target</code> when calling it.</li> </ul>  <h5 id=\"returns-68\">Returns:</h5> <p>The started thread.</p>  <h4 id=\"LooperThread.name\"><code>tf.train.LooperThread.name</code></h4> <p>A string used for identification purposes only.</p> <p>It has no semantics. Multiple threads may be given the same name. The initial name is set by the constructor.</p>  <h4 id=\"LooperThread.run\"><code>tf.train.LooperThread.run()</code></h4>  <h4 id=\"LooperThread.run_loop\"><code>tf.train.LooperThread.run_loop()</code></h4> <p>Called at 'timer_interval_secs' boundaries.</p>  <h4 id=\"LooperThread.setDaemon\"><code>tf.train.LooperThread.setDaemon(daemonic)</code></h4>  <h4 id=\"LooperThread.setName\"><code>tf.train.LooperThread.setName(name)</code></h4>  <h4 id=\"LooperThread.start\"><code>tf.train.LooperThread.start()</code></h4> <p>Start the thread's activity.</p> <p>It must be called at most once per thread object. It arranges for the object's run() method to be invoked in a separate thread of control.</p> <p>This method will raise a RuntimeError if called more than once on the same thread object.</p>  <h4 id=\"LooperThread.start_loop\"><code>tf.train.LooperThread.start_loop()</code></h4> <p>Called when the thread starts.</p>  <h4 id=\"LooperThread.stop_loop\"><code>tf.train.LooperThread.stop_loop()</code></h4> <p>Called when the thread stops.</p>  <h3 id=\"do_quantize_training_on_graphdef\"><code>tf.train.do_quantize_training_on_graphdef(input_graph, num_bits)</code></h3>  <h3 id=\"generate_checkpoint_state_proto\"><code>tf.train.generate_checkpoint_state_proto(save_dir, model_checkpoint_path, all_model_checkpoint_paths=None)</code></h3> <p>Generates a checkpoint state proto.</p>  <h5 id=\"args-78\">Args:</h5> <ul> <li>\n<code>save_dir</code>: Directory where the model was saved.</li> <li>\n<code>model_checkpoint_path</code>: The checkpoint file.</li> <li>\n<code>all_model_checkpoint_paths</code>: List of strings. Paths to all not-yet-deleted checkpoints, sorted from oldest to newest. If this is a non-empty list, the last element must be equal to model_checkpoint_path. These paths are also saved in the CheckpointState proto.</li> </ul>  <h5 id=\"returns-69\">Returns:</h5> <p>CheckpointState proto with model_checkpoint_path and all_model_checkpoint_paths updated to either absolute paths or relative paths to the current save_dir.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html</a>\n  </p>\n</div>\n","contrib.framework":"<h1 id=\"framework-contrib\">Framework (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#framework-contrib\">Framework (contrib)</a></li> <ul>\n<ul> <li><a href=\"#assert_same_float_dtype\"><code>tf.contrib.framework.assert_same_float_dtype(tensors=None, dtype=None)</code></a></li> <li><a href=\"#assert_scalar_int\"><code>tf.contrib.framework.assert_scalar_int(tensor)</code></a></li> <li><a href=\"#convert_to_tensor_or_sparse_tensor\"><code>tf.contrib.framework.convert_to_tensor_or_sparse_tensor(value, dtype=None, name=None, as_ref=False)</code></a></li> <li><a href=\"#get_graph_from_inputs\"><code>tf.contrib.framework.get_graph_from_inputs(op_input_list, graph=None)</code></a></li> <li><a href=\"#is_numeric_tensor\"><code>tf.is_numeric_tensor(tensor)</code></a></li> <li><a href=\"#is_non_decreasing\"><code>tf.is_non_decreasing(x, name=None)</code></a></li> <li><a href=\"#is_strictly_increasing\"><code>tf.is_strictly_increasing(x, name=None)</code></a></li> <li><a href=\"#is_tensor\"><code>tf.contrib.framework.is_tensor(x)</code></a></li> <li><a href=\"#reduce_sum_n\"><code>tf.contrib.framework.reduce_sum_n(tensors, name=None)</code></a></li> <li><a href=\"#safe_embedding_lookup_sparse\"><code>tf.contrib.framework.safe_embedding_lookup_sparse(*args, **kwargs)</code></a></li> <li><a href=\"#with_shape\"><code>tf.contrib.framework.with_shape(expected_shape, tensor)</code></a></li> <li><a href=\"#with_same_shape\"><code>tf.contrib.framework.with_same_shape(expected_tensor, tensor)</code></a></li> </ul> <li><a href=\"#deprecation\">Deprecation</a></li> <ul> <li><a href=\"#deprecated\"><code>tf.contrib.framework.deprecated(date, instructions)</code></a></li> <li><a href=\"#deprecated_arg_values\"><code>tf.contrib.framework.deprecated_arg_values(date, instructions, **deprecated_kwargs)</code></a></li> </ul> <li><a href=\"#arg-scope\">Arg_Scope</a></li> <ul> <li><a href=\"#arg_scope\"><code>tf.contrib.framework.arg_scope(list_ops_or_scope, **kwargs)</code></a></li> <li><a href=\"#add_arg_scope\"><code>tf.contrib.framework.add_arg_scope(func)</code></a></li> <li><a href=\"#has_arg_scope\"><code>tf.contrib.framework.has_arg_scope(func)</code></a></li> <li><a href=\"#arg_scoped_arguments\"><code>tf.contrib.framework.arg_scoped_arguments(func)</code></a></li> </ul> <li><a href=\"#variables\">Variables</a></li> <ul> <li><a href=\"#add_model_variable\"><code>tf.contrib.framework.add_model_variable(var)</code></a></li> <li><a href=\"#assert_global_step\"><code>tf.contrib.framework.assert_global_step(global_step_tensor)</code></a></li> <li><a href=\"#assert_or_get_global_step\"><code>tf.contrib.framework.assert_or_get_global_step(graph=None, global_step_tensor=None)</code></a></li> <li><a href=\"#create_global_step\"><code>tf.contrib.framework.create_global_step(graph=None)</code></a></li> <li><a href=\"#get_global_step\"><code>tf.contrib.framework.get_global_step(graph=None)</code></a></li> <li><a href=\"#get_or_create_global_step\"><code>tf.contrib.framework.get_or_create_global_step(graph=None)</code></a></li> <li><a href=\"#get_local_variables\"><code>tf.contrib.framework.get_local_variables(scope=None, suffix=None)</code></a></li> <li><a href=\"#get_model_variables\"><code>tf.contrib.framework.get_model_variables(scope=None, suffix=None)</code></a></li> <li><a href=\"#get_unique_variable\"><code>tf.contrib.framework.get_unique_variable(var_op_name)</code></a></li> <li><a href=\"#get_variables_by_name\"><code>tf.contrib.framework.get_variables_by_name(given_name, scope=None)</code></a></li> <li><a href=\"#get_variables_by_suffix\"><code>tf.contrib.framework.get_variables_by_suffix(suffix, scope=None)</code></a></li> <li><a href=\"#get_variables_to_restore\"><code>tf.contrib.framework.get_variables_to_restore(include=None, exclude=None)</code></a></li> <li><a href=\"#get_variables\"><code>tf.contrib.framework.get_variables(scope=None, suffix=None, collection=variables)</code></a></li> <li><a href=\"#local_variable\"><code>tf.contrib.framework.local_variable(initial_value, validate_shape=True, name=None)</code></a></li> <li><a href=\"#model_variable\"><code>tf.contrib.framework.model_variable(*args, **kwargs)</code></a></li> <li><a href=\"#variable\"><code>tf.contrib.framework.variable(*args, **kwargs)</code></a></li> <li><a href=\"#VariableDeviceChooser\"><code>class tf.contrib.framework.VariableDeviceChooser</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Framework utilities.</p>  <h3 id=\"assert_same_float_dtype\"><code>tf.contrib.framework.assert_same_float_dtype(tensors=None, dtype=None)</code></h3> <p>Validate and return float type based on <code>tensors</code> and <code>dtype</code>.</p> <p>For ops such as matrix multiplication, inputs and weights must be of the same float type. This function validates that all <code>tensors</code> are the same type, validates that type is <code>dtype</code> (if supplied), and returns the type. Type must be <code>dtypes.float32</code> or <code>dtypes.float64</code>. If neither <code>tensors</code> nor <code>dtype</code> is supplied, default to <code>dtypes.float32</code>.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>tensors</code>: Tensors of input values. Can include <code>None</code> elements, which will be ignored.</li> <li>\n<code>dtype</code>: Expected type.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>Validated type.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if neither <code>tensors</code> nor <code>dtype</code> is supplied, or result is not float.</li> </ul>  <h3 id=\"assert_scalar_int\"><code>tf.contrib.framework.assert_scalar_int(tensor)</code></h3> <p>Assert <code>tensor</code> is 0-D, of type <code>tf.int32</code> or <code>tf.int64</code>.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>tensor</code>: Tensor to test.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p><code>tensor</code>, for chaining.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>tensor</code> is not 0-D, of type <code>tf.int32</code> or <code>tf.int64</code>.</li> </ul>  <h3 id=\"convert_to_tensor_or_sparse_tensor\"><code>tf.contrib.framework.convert_to_tensor_or_sparse_tensor(value, dtype=None, name=None, as_ref=False)</code></h3> <p>Converts value to a <code>SparseTensor</code> or <code>Tensor</code>.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>value</code>: A <code>SparseTensor</code>, <code>SparseTensorValue</code>, or an object whose type has a registered <code>Tensor</code> conversion function.</li> <li>\n<code>dtype</code>: Optional element type for the returned tensor. If missing, the type is inferred from the type of <code>value</code>.</li> <li>\n<code>name</code>: Optional name to use if a new <code>Tensor</code> is created.</li> <li>\n<code>as_ref</code>: True if we want the result as a ref tensor. Only used if a new <code>Tensor</code> is created.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A <code>SparseTensor</code> or <code>Tensor</code> based on <code>value</code>.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>RuntimeError</code>: If result type is incompatible with <code>dtype</code>.</li> </ul>  <h3 id=\"get_graph_from_inputs\"><code>tf.contrib.framework.get_graph_from_inputs(op_input_list, graph=None)</code></h3> <p>Returns the appropriate graph to use for the given inputs.</p> <ol> <li>If <code>graph</code> is provided, we validate that all inputs in <code>op_input_list</code> are from the same graph.</li> <li>Otherwise, we attempt to select a graph from the first Operation- or Tensor-valued input in <code>op_input_list</code>, and validate that all other such inputs are in the same graph.</li> <li>If the graph was not specified and it could not be inferred from <code>op_input_list</code>, we attempt to use the default graph.</li> </ol>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>op_input_list</code>: A list of inputs to an operation, which may include <code>Tensor</code>, <code>Operation</code>, and other objects that may be converted to a graph element.</li> <li>\n<code>graph</code>: (Optional) The explicit graph to use.</li> </ul>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>op_input_list</code> is not a list or tuple, or if graph is not a Graph.</li> <li>\n<code>ValueError</code>: If a graph is explicitly passed and not all inputs are from it, or if the inputs are from multiple graphs, or we could not find a graph and there was no default graph.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>The appropriate graph to use for the given inputs.</p>  <h3 id=\"is_numeric_tensor\"><code>tf.is_numeric_tensor(tensor)</code></h3>  <h3 id=\"is_non_decreasing\"><code>tf.is_non_decreasing(x, name=None)</code></h3> <p>Returns <code>True</code> if <code>x</code> is non-decreasing.</p> <p>Elements of <code>x</code> are compared in row-major order. The tensor <code>[x[0],...]</code> is non-decreasing if for every adjacent pair we have <code>x[i] &lt;= x[i+1]</code>. If <code>x</code> has less than two elements, it is trivially non-decreasing.</p> <p>See also: <code>is_strictly_increasing</code></p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"is_non_decreasing\"</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>Boolean <code>Tensor</code>, equal to <code>True</code> iff <code>x</code> is non-decreasing.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> is not a numeric tensor.</li> </ul>  <h3 id=\"is_strictly_increasing\"><code>tf.is_strictly_increasing(x, name=None)</code></h3> <p>Returns <code>True</code> if <code>x</code> is strictly increasing.</p> <p>Elements of <code>x</code> are compared in row-major order. The tensor <code>[x[0],...]</code> is strictly increasing if for every adjacent pair we have <code>x[i] &lt; x[i+1]</code>. If <code>x</code> has less than two elements, it is trivially strictly increasing.</p> <p>See also: <code>is_non_decreasing</code></p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>x</code>: Numeric <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for this operation (optional). Defaults to \"is_strictly_increasing\"</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>Boolean <code>Tensor</code>, equal to <code>True</code> iff <code>x</code> is strictly increasing.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> is not a numeric tensor.</li> </ul>  <h3 id=\"is_tensor\"><code>tf.contrib.framework.is_tensor(x)</code></h3> <p>Check for tensor types. Check whether an object is a tensor. Equivalent to <code>isinstance(x, [tf.Tensor, tf.SparseTensor, tf.Variable])</code>.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>x</code>: An python object to check.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p><code>True</code> if <code>x</code> is a tensor, <code>False</code> if not.</p>  <h3 id=\"reduce_sum_n\"><code>tf.contrib.framework.reduce_sum_n(tensors, name=None)</code></h3> <p>Reduce tensors to a scalar sum.</p> <p>This reduces each tensor in <code>tensors</code> to a scalar via <code>tf.reduce_sum</code>, then adds them via <code>tf.add_n</code>.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>tensors</code>: List of tensors, all of the same numeric type.</li> <li>\n<code>name</code>: Tensor name, and scope for all other ops.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>Total loss tensor, or None if no losses have been configured.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>losses</code> is missing or empty.</li> </ul>  <h3 id=\"safe_embedding_lookup_sparse\"><code>tf.contrib.framework.safe_embedding_lookup_sparse(*args, **kwargs)</code></h3> <p>Lookup embedding results, accounting for invalid IDs and empty features. (deprecated)</p> <p>THIS FUNCTION IS DEPRECATED. It will be removed after 2016-09-01. Instructions for updating: Please use tf.contrib.layers.safe_embedding_lookup_sparse.</p> <p>The partitioned embedding in <code>embedding_weights</code> must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of <code>P</code>.</p> <p>Invalid IDs (&lt; 0) are pruned from input IDs and weights, as well as any IDs with non-positive weight. For an entry with no features, the embedding vector for <code>default_id</code> is returned, or the 0-vector if <code>default_id</code> is not supplied.</p> <p>The ids and weights may be multi-dimensional. Embeddings are always aggregated along the last dimension.</p> <p>Args: embedding_weights: A list of <code>P</code> float tensors or values representing partitioned embedding tensors. The total unpartitioned shape should be <code>[e_0, e_1, ..., e_m]</code>, where <code>e_0</code> represents the vocab size and <code>e_1, ..., e_m</code> are the embedding dimensions. sparse_ids: <code>SparseTensor</code> of shape <code>[d_0, d_1, ..., d_n]</code> containing the ids. <code>d_0</code> is typically batch size. sparse_weights: <code>SparseTensor</code> of same shape as <code>sparse_ids</code>, containing float weights corresponding to <code>sparse_ids</code>, or <code>None</code> if all weights are be assumed to be 1.0. combiner: A string specifying how to combine embedding results for each entry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the default. default_id: The id to use for an entry with no features. name: A name for this operation (optional). partition_strategy: A string specifying the partitioning strategy. Currently <code>\"div\"</code> and <code>\"mod\"</code> are supported. Default is <code>\"div\"</code>.</p> <p>Returns: Dense tensor of shape <code>[d_0, d_1, ..., d_{n-1}, e_1, ..., e_m]</code>.</p> <p>Raises: ValueError: if <code>embedding_weights</code> is empty.</p>  <h3 id=\"with_shape\"><code>tf.contrib.framework.with_shape(expected_shape, tensor)</code></h3> <p>Asserts tensor has expected shape.</p> <p>If tensor shape and expected_shape, are fully defined, assert they match. Otherwise, add assert op that will validate the shape when tensor is evaluated, and set shape on tensor.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>expected_shape</code>: Expected shape to assert, as a 1D array of ints, or tensor of same.</li> <li>\n<code>tensor</code>: Tensor whose shape we're validating.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>tensor, perhaps with a dependent assert operation.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if tensor has an invalid shape.</li> </ul>  <h3 id=\"with_same_shape\"><code>tf.contrib.framework.with_same_shape(expected_tensor, tensor)</code></h3> <p>Assert tensors are the same shape, from the same graph.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>expected_tensor</code>: Tensor with expected shape.</li> <li>\n<code>tensor</code>: Tensor of actual values.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>Tuple of (actual_tensor, label_tensor), possibly with assert ops added.</p> <h2 id=\"deprecation\">Deprecation</h2>  <h3 id=\"deprecated\"><code>tf.contrib.framework.deprecated(date, instructions)</code></h3> <p>Decorator for marking functions or methods deprecated.</p> <p>This decorator logs a deprecation warning whenever the decorated function is called. It has the following format:</p> <p><function> (from <module>) is deprecated and will be removed after <date>. Instructions for updating: </date></module></function></p> <p><function> will include the class name if it is a method.</function></p> <p>It also edits the docstring of the function: ' (deprecated)' is appended to the first line of the docstring and a deprecation notice is prepended to the rest of the docstring.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>date</code>: String. The date the function is scheduled to be removed. Must be ISO 8601 (YYYY-MM-DD).</li> <li>\n<code>instructions</code>: String. Instructions on how to update code using the deprecated function.</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>Decorated function or method.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If date is not in ISO 8601 format, or instructions are empty.</li> </ul>  <h3 id=\"deprecated_arg_values\"><code>tf.contrib.framework.deprecated_arg_values(date, instructions, **deprecated_kwargs)</code></h3> <p>Decorator for marking specific function argument values as deprecated.</p> <p>This decorator logs a deprecation warning whenever the decorated function is called with the deprecated argument values. It has the following format:</p> <p>Calling <function> (from <module>) with <arg>=<value> is deprecated and will be removed after <date>. Instructions for updating: </date></value></arg></module></function></p> <p><function> will include the class name if it is a method.</function></p> <p>It also edits the docstring of the function: ' (deprecated arguments)' is appended to the first line of the docstring and a deprecation notice is prepended to the rest of the docstring.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>date</code>: String. The date the function is scheduled to be removed. Must be ISO 8601 (YYYY-MM-DD).</li> <li>\n<code>instructions</code>: String. Instructions on how to update code using the deprecated function.</li> <li>\n<code>**deprecated_kwargs</code>: The deprecated argument values.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>Decorated function or method.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If date is not in ISO 8601 format, or instructions are empty.</li> </ul>  <h2 id=\"arg-scope\">Arg_Scope</h2>  <h3 id=\"arg_scope\"><code>tf.contrib.framework.arg_scope(list_ops_or_scope, **kwargs)</code></h3> <p>Stores the default arguments for the given set of list_ops.</p> <p>For usage, please see examples at top of the file.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>list_ops_or_scope</code>: List or tuple of operations to set argument scope for or a dictionary containg the current scope. When list_ops_or_scope is a dict, kwargs must be empty. When list_ops_or_scope is a list or tuple, then every op in it need to be decorated with @add_arg_scope to work.</li> <li>\n<code>**kwargs</code>: keyword=value that will define the defaults for each op in list_ops. All the ops need to accept the given set of arguments.</li> </ul> <h5 id=\"yields\">Yields:</h5> <p>the current_scope, which is a dictionary of {op: {arg: value}}</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if list_ops is not a list or a tuple.</li> <li>\n<code>ValueError</code>: if any op in list_ops has not be decorated with @add_arg_scope.</li> </ul>  <h3 id=\"add_arg_scope\"><code>tf.contrib.framework.add_arg_scope(func)</code></h3> <p>Decorates a function with args so it can be used within an arg_scope.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>func</code>: function to decorate.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A tuple with the decorated function func_with_args().</p>  <h3 id=\"has_arg_scope\"><code>tf.contrib.framework.has_arg_scope(func)</code></h3> <p>Checks whether a func has been decorated with @add_arg_scope or not.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>func</code>: function to check.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>a boolean.</p>  <h3 id=\"arg_scoped_arguments\"><code>tf.contrib.framework.arg_scoped_arguments(func)</code></h3> <p>Returns the list kwargs that arg_scope can set for a func.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>func</code>: function which has been decorated with @add_arg_scope.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>a list of kwargs names.</p> <h2 id=\"variables\">Variables</h2>  <h3 id=\"add_model_variable\"><code>tf.contrib.framework.add_model_variable(var)</code></h3> <p>Adds a variable to the <code>GraphKeys.MODEL_VARIABLES</code> collection.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>var</code>: a variable.</li> </ul>  <h3 id=\"assert_global_step\"><code>tf.contrib.framework.assert_global_step(global_step_tensor)</code></h3> <p>Asserts <code>global_step_tensor</code> is a scalar int <code>Variable</code> or <code>Tensor</code>.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>global_step_tensor</code>: <code>Tensor</code> to test.</li> </ul>  <h3 id=\"assert_or_get_global_step\"><code>tf.contrib.framework.assert_or_get_global_step(graph=None, global_step_tensor=None)</code></h3> <p>Verifies that a global step tensor is valid or gets one if None is given.</p> <p>If <code>global_step_tensor</code> is not None, check that it is a valid global step tensor (using <code>assert_global_step</code>). Otherwise find a global step tensor using <code>get_global_step</code> and return it.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>graph</code>: The graph to find the global step tensor for.</li> <li>\n<code>global_step_tensor</code>: The tensor to check for suitability as a global step. If None is given (the default), find a global step tensor.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A tensor suitable as a global step, or <code>None</code> if none was provided and none was found.</p>  <h3 id=\"create_global_step\"><code>tf.contrib.framework.create_global_step(graph=None)</code></h3> <p>Create global step tensor in graph.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>graph</code>: The graph in which to create the global step. If missing, use default graph.</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>Global step tensor.</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if global step key is already defined.</li> </ul>  <h3 id=\"get_global_step\"><code>tf.contrib.framework.get_global_step(graph=None)</code></h3> <p>Get the global step tensor.</p> <p>The global step tensor must be an integer variable. We first try to find it in the collection <code>GLOBAL_STEP</code>, or by name <code>global_step:0</code>.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>graph</code>: The graph to find the global step in. If missing, use default graph.</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>The global step variable, or <code>None</code> if none was found.</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If the global step tensor has a non-integer type, or if it is not a <code>Variable</code>.</li> </ul>  <h3 id=\"get_or_create_global_step\"><code>tf.contrib.framework.get_or_create_global_step(graph=None)</code></h3> <p>Returns and create (if necessary) the global step variable.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>graph</code>: The graph in which to create the global step. If missing, use default graph.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>the tensor representing the global step variable.</p>  <h3 id=\"get_local_variables\"><code>tf.contrib.framework.get_local_variables(scope=None, suffix=None)</code></h3> <p>Gets the list of model variables, filtered by scope and/or suffix.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>scope</code>: an optional scope for filtering the variables to return.</li> <li>\n<code>suffix</code>: an optional suffix for filtering the variables to return.</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>a list of variables in colelction with scope and suffix.</p>  <h3 id=\"get_model_variables\"><code>tf.contrib.framework.get_model_variables(scope=None, suffix=None)</code></h3> <p>Gets the list of model variables, filtered by scope and/or suffix.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>scope</code>: an optional scope for filtering the variables to return.</li> <li>\n<code>suffix</code>: an optional suffix for filtering the variables to return.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>a list of variables in colelction with scope and suffix.</p>  <h3 id=\"get_unique_variable\"><code>tf.contrib.framework.get_unique_variable(var_op_name)</code></h3> <p>Gets the variable uniquely identified by that var_op_name.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>var_op_name</code>: the full name of the variable op, including the scope.</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>a tensorflow variable.</p>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if no variable uniquely identified by the name exists.</li> </ul>  <h3 id=\"get_variables_by_name\"><code>tf.contrib.framework.get_variables_by_name(given_name, scope=None)</code></h3> <p>Gets the list of variables that were given that name.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>given_name</code>: name given to the variable without any scope.</li> <li>\n<code>scope</code>: an optional scope for filtering the variables to return.</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>a copied list of variables with the given name and scope.</p>  <h3 id=\"get_variables_by_suffix\"><code>tf.contrib.framework.get_variables_by_suffix(suffix, scope=None)</code></h3> <p>Gets the list of variables that end with the given suffix.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>suffix</code>: suffix for filtering the variables to return.</li> <li>\n<code>scope</code>: an optional scope for filtering the variables to return.</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>a copied list of variables with the given name and prefix.</p>  <h3 id=\"get_variables_to_restore\"><code>tf.contrib.framework.get_variables_to_restore(include=None, exclude=None)</code></h3> <p>Gets the list of the variables to restore.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>include</code>: an optional list/tuple of scope strings for filtering which variables from the VARIABLES collection to include. None would include all the variables.</li> <li>\n<code>exclude</code>: an optional list/tuple of scope strings for filtering which variables from the VARIABLES collection to exclude. None it would not exclude any.</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>a list of variables to restore.</p>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: include or exclude is provided but is not a list or a tuple.</li> </ul>  <h3 id=\"get_variables\"><code>tf.contrib.framework.get_variables(scope=None, suffix=None, collection='variables')</code></h3> <p>Gets the list of variables, filtered by scope and/or suffix.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>scope</code>: an optional scope for filtering the variables to return.</li> <li>\n<code>suffix</code>: an optional suffix for filtering the variables to return.</li> <li>\n<code>collection</code>: in which collection search for. Defaults to GraphKeys.VARIABLES.</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <p>a list of variables in colelction with scope and suffix.</p>  <h3 id=\"local_variable\"><code>tf.contrib.framework.local_variable(initial_value, validate_shape=True, name=None)</code></h3> <p>Create variable and add it to <code>GraphKeys.LOCAL_VARIABLES</code> collection.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>initial_value</code>: See variables.Variable.__init__.</li> <li>\n<code>validate_shape</code>: See variables.Variable.__init__.</li> <li>\n<code>name</code>: See variables.Variable.__init__.</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>New variable.</p>  <h3 id=\"model_variable\"><code>tf.contrib.framework.model_variable(*args, **kwargs)</code></h3> <p>Gets an existing model variable with these parameters or creates a new one.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>name</code>: the name of the new or existing variable.</li> <li>\n<code>shape</code>: shape of the new or existing variable.</li> <li>\n<code>dtype</code>: type of the new or existing variable (defaults to <code>DT_FLOAT</code>).</li> <li>\n<code>initializer</code>: initializer for the variable if one is created.</li> <li>\n<code>regularizer</code>: a (Tensor -&gt; Tensor or None) function; the result of applying it on a newly created variable will be added to the collection GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.</li> <li>\n<code>trainable</code>: If <code>True</code> also add the variable to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li> <li>\n<code>collections</code>: A list of collection names to which the Variable will be added. Note that the variable is always also added to the <code>GraphKeys.VARIABLES</code> and <code>GraphKeys.MODEL_VARIABLES</code> collections.</li> <li>\n<code>caching_device</code>: Optional device string or function describing where the Variable should be cached for reading. Defaults to the Variable's device.</li> <li>\n<code>device</code>: Optional device to place the variable. It can be an string or a function that is called to get the device for the variable.</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>The created or existing variable.</p>  <h3 id=\"variable\"><code>tf.contrib.framework.variable(*args, **kwargs)</code></h3> <p>Gets an existing variable with these parameters or creates a new one.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>name</code>: the name of the new or existing variable.</li> <li>\n<code>shape</code>: shape of the new or existing variable.</li> <li>\n<code>dtype</code>: type of the new or existing variable (defaults to <code>DT_FLOAT</code>).</li> <li>\n<code>initializer</code>: initializer for the variable if one is created.</li> <li>\n<code>regularizer</code>: a (Tensor -&gt; Tensor or None) function; the result of applying it on a newly created variable will be added to the collection GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.</li> <li>\n<code>trainable</code>: If <code>True</code> also add the variable to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li> <li>\n<code>collections</code>: A list of collection names to which the Variable will be added. If None it would default to tf.GraphKeys.VARIABLES.</li> <li>\n<code>caching_device</code>: Optional device string or function describing where the Variable should be cached for reading. Defaults to the Variable's device.</li> <li>\n<code>device</code>: Optional device to place the variable. It can be an string or a function that is called to get the device for the variable.</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>The created or existing variable.</p>  <h3 id=\"VariableDeviceChooser\"><code>class tf.contrib.framework.VariableDeviceChooser</code></h3> <p>Device chooser for variables.</p> <p>When using a parameter server it will assign them in a round-robin fashion. When not using a parameter server it allows GPU or CPU placement.</p>  <h4 id=\"VariableDeviceChooser.__init__\"><code>tf.contrib.framework.VariableDeviceChooser.__init__(num_tasks=0, job_name='ps', device_type='CPU', device_index=0)</code></h4> <p>Initialize VariableDeviceChooser.</p> <h5 id=\"usage\">Usage:</h5> <p>To use with 2 parameter servers: VariableDeviceChooser(2)</p> <p>To use without parameter servers: VariableDeviceChooser() VariableDeviceChooser(device_type='GPU') # For GPU placement</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>num_tasks</code>: number of tasks.</li> <li>\n<code>job_name</code>: String, a name for the parameter server job.</li> <li>\n<code>device_type</code>: Optional device type string (e.g. \"CPU\" or \"GPU\")</li> <li>\n<code>device_index</code>: int. Optional device index. If left unspecified, device represents 'any' device_index.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.framework.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.framework.html</a>\n  </p>\n</div>\n","contrib.graph_editor":"<h1 id=\"graph-editor-contrib\">Graph Editor (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#graph-editor-contrib\">Graph Editor (contrib)</a></li> <ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#SubGraphView\"><code>class tf.contrib.graph_editor.SubGraphView</code></a></li> <li><a href=\"#Transformer\"><code>class tf.contrib.graph_editor.Transformer</code></a></li> <li><a href=\"#bypass\"><code>tf.contrib.graph_editor.bypass(sgv)</code></a></li> <li><a href=\"#connect\"><code>tf.contrib.graph_editor.connect(sgv0, sgv1, disconnect_first=False)</code></a></li> <li><a href=\"#detach\"><code>tf.contrib.graph_editor.detach(sgv, control_inputs=False, control_outputs=None, control_ios=None)</code></a></li> <li><a href=\"#detach_inputs\"><code>tf.contrib.graph_editor.detach_inputs(sgv, control_inputs=False)</code></a></li> <li><a href=\"#detach_outputs\"><code>tf.contrib.graph_editor.detach_outputs(sgv, control_outputs=None)</code></a></li> <li><a href=\"#matcher\"><code>class tf.contrib.graph_editor.matcher</code></a></li> <li><a href=\"#ph\"><code>tf.contrib.graph_editor.ph(dtype, shape=None, scope=None)</code></a></li> <li><a href=\"#reroute_a2b\"><code>tf.contrib.graph_editor.reroute_a2b(sgv0, sgv1)</code></a></li> <li><a href=\"#reroute_a2b_inputs\"><code>tf.contrib.graph_editor.reroute_a2b_inputs(sgv0, sgv1)</code></a></li> <li><a href=\"#reroute_a2b_outputs\"><code>tf.contrib.graph_editor.reroute_a2b_outputs(sgv0, sgv1)</code></a></li> <li><a href=\"#reroute_b2a\"><code>tf.contrib.graph_editor.reroute_b2a(sgv0, sgv1)</code></a></li> <li><a href=\"#reroute_b2a_inputs\"><code>tf.contrib.graph_editor.reroute_b2a_inputs(sgv0, sgv1)</code></a></li> <li><a href=\"#reroute_b2a_outputs\"><code>tf.contrib.graph_editor.reroute_b2a_outputs(sgv0, sgv1)</code></a></li> <li><a href=\"#select_ops\"><code>tf.contrib.graph_editor.select_ops(*args, **kwargs)</code></a></li> <li><a href=\"#select_ts\"><code>tf.contrib.graph_editor.select_ts(*args, **kwargs)</code></a></li> <li><a href=\"#sgv\"><code>tf.contrib.graph_editor.sgv(*args, **kwargs)</code></a></li> <li><a href=\"#sgv_scope\"><code>tf.contrib.graph_editor.sgv_scope(scope, graph)</code></a></li> <li><a href=\"#swap\"><code>tf.contrib.graph_editor.swap(sgv0, sgv1)</code></a></li> <li><a href=\"#swap_inputs\"><code>tf.contrib.graph_editor.swap_inputs(sgv0, sgv1)</code></a></li> <li><a href=\"#swap_outputs\"><code>tf.contrib.graph_editor.swap_outputs(sgv0, sgv1)</code></a></li> <li><a href=\"#ts\"><code>tf.contrib.graph_editor.ts(*args, **kwargs)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Graph editor module allows to modify an existing graph in place.</p>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"SubGraphView\"><code>class tf.contrib.graph_editor.SubGraphView</code></h3> <p>A subgraph view on an existing tf.Graph.</p> <p>An instance of this class is a subgraph view on an existing tf.Graph. \"subgraph\" means that it can represent part of the whole tf.Graph. \"view\" means that it only provides a passive observation and do not to act on the tf.Graph. Note that in this documentation, the term \"subgraph\" is often used as substitute to \"subgraph view\".</p> <p>A subgraph contains: - a list of input tensors, accessible via the \"inputs\" property. - a list of output tensors, accessible via the \"outputs\" property. - and the operations in between, accessible via the \"ops\" property.</p> <p>An subgraph can be seen as a function F(i0, i1, ...) -&gt; o0, o1, ... It is a function which takes as input some input tensors and returns as output some output tensors. The computation that the function performs is encoded in the operations of the subgraph.</p> <p>The tensors (input or output) can be of two kinds: - connected: a connected tensor connects to at least one operation contained in the subgraph. One example is a subgraph representing a single operation and its inputs and outputs: all the input and output tensors of the op are \"connected\". - passthrough: a passthrough tensor does not connect to any operation contained in the subgraph. One example is a subgraph representing a single tensor: this tensor is passthrough. By default a passthrough tensor is present both in the input and output tensors of the subgraph. It can however be remapped to only appear as an input (or output) only.</p> <p>The input and output tensors can be remapped. For instance, some input tensor can be ommited. For instance, a subgraph representing an operation with two inputs can be remapped to only take one input. Note that this does not change at all the underlying tf.Graph (remember, it is a view). It means that the other input is being ignored, or is being treated as \"given\". The analogy with functions can be extended like this: F(x,y) is the original function. Remapping the inputs from [x, y] to just [x] means that the subgraph now represent the function F_y(x) (y is \"given\").</p> <p>The output tensors can also be remapped. For instance, some output tensor can be ommited. Other output tensor can be duplicated as well. As mentioned before, this does not change at all the underlying tf.Graph. The analogy with functions can be extended like this: F(...)-&gt;x,y is the original function. Remapping the outputs from [x, y] to just [y,y] means that the subgraph now represent the function M(F(...)) where M is the function M(a,b)-&gt;b,b.</p> <p>It is useful to describe three other kind of tensors: - internal: an internal tensor is a tensor connecting operations contained in the subgraph. One example in the subgraph representing the two operations A and B connected sequentially: -&gt; A -&gt; B -&gt;. The middle arrow is an internal tensor. - actual input: an input tensor of the subgraph, regardless of whether it is listed in \"inputs\" or not (masked-out). - actual output: an output tensor of the subgraph, regardless of whether it is listed in \"outputs\" or not (masked-out). - hidden input: an actual input which has been masked-out using an input remapping. In other word, a hidden input is a non-internal tensor not listed as a input tensor and one of whose consumers belongs to the subgraph. - hidden output: a actual output which has been masked-out using an output remapping. In other word, a hidden output is a non-internal tensor not listed as an output and one of whose generating operations belongs to the subgraph.</p> <p>Here are some usefull guarantees about an instance of a SubGraphView: - the input (or output) tensors are not internal. - the input (or output) tensors are either \"connected\" or \"passthrough\". - the passthrough tensors are not connected to any of the operation of the subgraph.</p> <p>Note that there is no guarantee that an operation in a subgraph contributes at all to its inputs or outputs. For instance, remapping both the inputs and outputs to empty lists will produce a subgraph which still contains all the original operations. However, the remove_unused_ops function can be used to make a new subgraph view whose operations are connected to at least one of the input or output tensors.</p> <p>An instance of this class is meant to be a lightweight object which is not modified in-place by the user. Rather, the user can create new modified instances of a given subgraph. In that sense, the class SubGraphView is meant to be used like an immutable python object.</p> <p>A common problem when using views is that they can get out-of-sync with the data they observe (in this case, a tf.Graph). This is up to the user to insure that this doesn't happen. To keep on the safe sife, it is recommended that the life time of subgraph views are kept very short. One way to achieve this is to use subgraphs within a \"with make_sgv(...) as sgv:\" Python context.</p> <p>To alleviate the out-of-sync problem, some functions are granted the right to modified subgraph in place. This is typically the case of graph manipulation functions which, given some subgraphs as arguments, can modify the underlying tf.Graph. Since this modification is likely to render the subgraph view invalid, those functions can modify the argument in place to reflect the change. For instance, calling the function swap_inputs(svg0, svg1) will modify svg0 and svg1 in place to reflect the fact that their inputs have now being swapped.</p>  <h4 id=\"SubGraphView.__init__\"><code>tf.contrib.graph_editor.SubGraphView.__init__(inside_ops=(), passthrough_ts=())</code></h4> <p>Create a subgraph containing the given ops and the \"passthrough\" tensors.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>inside_ops</code>: an object convertible to a list of tf.Operation. This list defines all the operations in the subgraph.</li> <li>\n<code>passthrough_ts</code>: an object convertible to a list of tf.Tensor. This list define all the \"passthrough\" tensors. A passthrough tensor is a tensor which goes directly from the input of the subgraph to it output, without any intermediate operations. All the non passthrough tensors are silently ignored.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if inside_ops cannot be converted to a list of tf.Operation or if passthrough_ts cannot be converted to a list of tf.Tensor.</li> </ul>  <h4 id=\"SubGraphView.connected_inputs\"><code>tf.contrib.graph_editor.SubGraphView.connected_inputs</code></h4> <p>The connected input tensors of this subgraph view.</p>  <h4 id=\"SubGraphView.connected_outputs\"><code>tf.contrib.graph_editor.SubGraphView.connected_outputs</code></h4> <p>The connected output tensors of this subgraph view.</p>  <h4 id=\"SubGraphView.consumers\"><code>tf.contrib.graph_editor.SubGraphView.consumers()</code></h4> <p>Return a Python set of all the consumers of this subgraph view.</p>  <h4 id=\"SubGraphView.copy\"><code>tf.contrib.graph_editor.SubGraphView.copy()</code></h4> <p>Return a copy of itself.</p> <p>Note that this class is a \"view\", copying it only create another view and does not copy the underlying part of the tf.Graph.</p> <h5 id=\"returns\">Returns:</h5> <p>a new instance identical to the original one.</p>  <h4 id=\"SubGraphView.find_op_by_name\"><code>tf.contrib.graph_editor.SubGraphView.find_op_by_name(op_name)</code></h4> <p>Return the op named op_name.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>op_name</code>: the name to search for</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>The op named op_name.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the op_name could not be found.</li> <li>\n<code>AssertionError</code>: if the name was found multiple time.</li> </ul>  <h4 id=\"SubGraphView.graph\"><code>tf.contrib.graph_editor.SubGraphView.graph</code></h4> <p>The underlying tf.Graph.</p>  <h4 id=\"SubGraphView.input_index\"><code>tf.contrib.graph_editor.SubGraphView.input_index(t)</code></h4> <p>Find the input index corresponding to the given input tensor t.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>t</code>: the input tensor of this subgraph view.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>the index in the self.inputs list.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>Error</code>: if t in not an input tensor.</li> </ul>  <h4 id=\"SubGraphView.inputs\"><code>tf.contrib.graph_editor.SubGraphView.inputs</code></h4> <p>The input tensors of this subgraph view.</p>  <h4 id=\"SubGraphView.is_passthrough\"><code>tf.contrib.graph_editor.SubGraphView.is_passthrough(t)</code></h4> <p>Check whether a tensor is passthrough.</p>  <h4 id=\"SubGraphView.op\"><code>tf.contrib.graph_editor.SubGraphView.op(op_id)</code></h4> <p>Get an op by its index.</p>  <h4 id=\"SubGraphView.ops\"><code>tf.contrib.graph_editor.SubGraphView.ops</code></h4> <p>The operations in this subgraph view.</p>  <h4 id=\"SubGraphView.output_index\"><code>tf.contrib.graph_editor.SubGraphView.output_index(t)</code></h4> <p>Find the output index corresponding to given output tensor t.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>t</code>: the output tensor of this subgraph view.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>the index in the self.outputs list.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>Error</code>: if t in not an output tensor.</li> </ul>  <h4 id=\"SubGraphView.outputs\"><code>tf.contrib.graph_editor.SubGraphView.outputs</code></h4> <p>The output tensors of this subgraph view.</p>  <h4 id=\"SubGraphView.passthroughs\"><code>tf.contrib.graph_editor.SubGraphView.passthroughs</code></h4> <p>The passthrough tensors, going straight from input to output.</p>  <h4 id=\"SubGraphView.remap\"><code>tf.contrib.graph_editor.SubGraphView.remap(new_input_indices=None, new_output_indices=None)</code></h4> <p>Remap the inputs and outputs of the subgraph.</p> <p>Note that this is only modifying the view: the underlying tf.Graph is not affected.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>new_input_indices</code>: an iterable of integers representing a mapping between the old inputs and the new ones. This mapping can be under-complete and must be without repetitions.</li> <li>\n<code>new_output_indices</code>: an iterable of integers representing a mapping between the old outputs and the new ones. This mapping can be under-complete and can have repetitions.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A new modified instance of the original subgraph view with remapped inputs and outputs.</p>  <h4 id=\"SubGraphView.remap_default\"><code>tf.contrib.graph_editor.SubGraphView.remap_default(remove_input_map=True, remove_output_map=True)</code></h4> <p>Remap the inputs and/or outputs to the default mapping.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>remove_input_map</code>: if True the input map is reset to the default one.</li> <li>\n<code>remove_output_map</code>: if True the output map is reset to the default one.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A new modified instance of the original subgraph view with its input and/or output mapping reset to the default one.</p>  <h4 id=\"SubGraphView.remap_inputs\"><code>tf.contrib.graph_editor.SubGraphView.remap_inputs(new_input_indices)</code></h4> <p>Remap the inputs of the subgraph.</p> <p>If the inputs of the original subgraph are [t0, t1, t2], remapping to [2,0] will create a new instance whose inputs is [t2, t0].</p> <p>Note that this is only modifying the view: the underlying tf.Graph is not affected.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>new_input_indices</code>: an iterable of integers representing a mapping between the old inputs and the new ones. This mapping can be under-complete and must be without repetitions.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A new modified instance of the original subgraph view with remapped inputs.</p>  <h4 id=\"SubGraphView.remap_outputs\"><code>tf.contrib.graph_editor.SubGraphView.remap_outputs(new_output_indices)</code></h4> <p>Remap the output of the subgraph.</p> <p>If the output of the original subgraph are [t0, t1, t2], remapping to [1,1,0] will create a new instance whose outputs is [t1, t1, t0].</p> <p>Note that this is only modifying the view: the underlying tf.Graph is not affected.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>new_output_indices</code>: an iterable of integers representing a mapping between the old outputs and the new ones. This mapping can be under-complete and can have repetitions.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A new modified instance of the original subgraph view with remapped outputs.</p>  <h4 id=\"SubGraphView.remap_outputs_make_unique\"><code>tf.contrib.graph_editor.SubGraphView.remap_outputs_make_unique()</code></h4> <p>Remap the outputs so that all the tensors appears only once.</p>  <h4 id=\"SubGraphView.remap_outputs_to_consumers\"><code>tf.contrib.graph_editor.SubGraphView.remap_outputs_to_consumers()</code></h4> <p>Remap the outputs to match the number of consumers.</p>  <h4 id=\"SubGraphView.remove_unused_ops\"><code>tf.contrib.graph_editor.SubGraphView.remove_unused_ops(control_inputs=True)</code></h4> <p>Remove unused ops.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>control_inputs</code>: if True, control inputs are used to detect used ops.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A new subgraph view which only contains used operations.</p>  <h3 id=\"Transformer\"><code>class tf.contrib.graph_editor.Transformer</code></h3> <p>Transform a subgraph into another one.</p> <p>By default, the constructor create a transform which copy a subgraph and replaces inputs with placeholders. This behavior can be modified by changing the handlers.</p>  <h4 id=\"Transformer.__init__\"><code>tf.contrib.graph_editor.Transformer.__init__()</code></h4> <p>Transformer constructor.</p> <p>The following members can be modified: transform_op_handler: handle the transformation of a tf.Operation. This handler defaults to a simple copy. assign_collections_handler: handle the assignment of collections. This handler defaults to assigning new collections created under the given name-scope. transform_input_handler: handle the transform of the inputs to the given subgraph. This handler defaults to creating placeholders instead of the ops just before the input tensors of the subgraph. transform_hidden_input_handler: handle the transform of the hidden inputs of the subgraph, that is, the inputs which are not listed in sgv.inputs. This handler defaults to a transform which keep the same input if the source and destination graphs are the same, otherwise use placeholders. transform_original_op_hanlder: handle the transform of original_op. This handler defaults to transforming original_op only if they are in the subgraph, otherwise they are ignored.</p>  <h4 id=\"Transformer.new_name\"><code>tf.contrib.graph_editor.Transformer.new_name(name)</code></h4> <p>Compute a destination name from a source name.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>name</code>: the name to be \"transformed\".</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>the transformed name.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the source scope is used (that is, not an empty string) and the source name does not belong to the source scope.</li> </ul>  <h3 id=\"bypass\"><code>tf.contrib.graph_editor.bypass(sgv)</code></h3> <p>Bypass the given subgraph by connecting its inputs to its outputs.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>sgv</code>: the subgraph view to be bypassed. This argument is converted to a subgraph using the same rules than the function subgraph.make_view.</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A new subgraph view of the bypassed subgraph. Note that sgv is also modified in place.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>StandardError</code>: if sgv cannot be converted to a SubGraphView using the same rules than the function subgraph.make_view.</li> </ul>  <h3 id=\"connect\"><code>tf.contrib.graph_editor.connect(sgv0, sgv1, disconnect_first=False)</code></h3> <p>Connect the outputs of sgv0 to the inputs of sgv1.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>sgv0</code>: the first subgraph to have its outputs swapped. This argument is converted to a subgraph using the same rules as the function subgraph.make_view.</li> <li>\n<code>sgv1</code>: the second subgraph to have its outputs swapped. This argument is converted to a subgraph using the same rules as the function subgraph.make_view.</li> <li>\n<code>disconnect_first</code>: if True the current outputs of sgv0 are disconnected.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>Two new subgraph views (now connected). sgv0 and svg1 are also modified in place.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>StandardError</code>: if sgv0 or sgv1 cannot be converted to a SubGraphView using the same rules than the function subgraph.make_view.</li> </ul>  <h3 id=\"detach\"><code>tf.contrib.graph_editor.detach(sgv, control_inputs=False, control_outputs=None, control_ios=None)</code></h3> <p>Detach both the inputs and the outputs of a subgraph view.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>sgv</code>: the subgraph view to be detached. This argument is converted to a subgraph using the same rules as the function subgraph.make_view.</li> <li>\n<code>control_inputs</code>: A boolean indicating whether control inputs are enabled.</li> <li>\n<code>control_outputs</code>: An instance of util.ControlOutputs or None. If not None, control outputs are enabled.</li> <li>\n<code>control_ios</code>: An instance of util.ControlOutputs or None. If not None, both control inputs and control outputs are enabled. This is equivalent to set control_inputs to True and control_outputs to the util.ControlOutputs instance.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A new subgraph view of the detached subgraph. Note that sgv is also modified in place.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>StandardError</code>: if sgv cannot be converted to a SubGraphView using the same rules than the function subgraph.make_view.</li> </ul>  <h3 id=\"detach_inputs\"><code>tf.contrib.graph_editor.detach_inputs(sgv, control_inputs=False)</code></h3> <p>Detach the inputs of a subgraph view.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>sgv</code>: the subgraph view to be detached. This argument is converted to a subgraph using the same rules as the function subgraph.make_view.</li> <li>\n<code>control_inputs</code>: if True control_inputs are also detached.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A new subgraph view of the detached subgraph. Note that sgv is also modified in place.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>StandardError</code>: if sgv cannot be converted to a SubGraphView using the same rules than the function subgraph.make_view.</li> </ul>  <h3 id=\"detach_outputs\"><code>tf.contrib.graph_editor.detach_outputs(sgv, control_outputs=None)</code></h3> <p>Detach the outputa of a subgraph view.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>sgv</code>: the subgraph view to be detached. This argument is converted to a subgraph using the same rules as the function subgraph.make_view.</li> <li>\n<code>control_outputs</code>: a util.ControlOutputs instance or None. If not None the control outputs are also detached.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>A new subgraph view of the detached subgraph. Note that sgv is also modified in place.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>StandardError</code>: if sgv cannot be converted to a SubGraphView using the same rules than the function subgraph.make_view.</li> </ul>  <h3 id=\"matcher\"><code>class tf.contrib.graph_editor.matcher</code></h3> <p>Graph match class.</p>  <h4 id=\"matcher.__init__\"><code>tf.contrib.graph_editor.matcher.__init__(positive_filter)</code></h4> <p>Graph match constructor.</p>  <h4 id=\"matcher.control_input_ops\"><code>tf.contrib.graph_editor.matcher.control_input_ops(*args)</code></h4> <p>Add input matches.</p>  <h4 id=\"matcher.input_ops\"><code>tf.contrib.graph_editor.matcher.input_ops(*args)</code></h4> <p>Add input matches.</p>  <h4 id=\"matcher.output_ops\"><code>tf.contrib.graph_editor.matcher.output_ops(*args)</code></h4> <p>Add output matches.</p>  <h3 id=\"ph\"><code>tf.contrib.graph_editor.ph(dtype, shape=None, scope=None)</code></h3> <p>Create a tf.placeholder for the Graph Editor.</p> <p>Note that the correct graph scope must be set by the calling function. The placeholder is named using the function placeholder_name (with no tensor argument).</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>dtype</code>: the tensor type.</li> <li>\n<code>shape</code>: the tensor shape (optional).</li> <li>\n<code>scope</code>: absolute scope within which to create the placeholder. None means that the scope of t is preserved. \"\" means the root scope.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A newly created tf.placeholder.</p>  <h3 id=\"reroute_a2b\"><code>tf.contrib.graph_editor.reroute_a2b(sgv0, sgv1)</code></h3> <p>Re-route the inputs and outputs of sgv0 to sgv1 (see _reroute).</p>  <h3 id=\"reroute_a2b_inputs\"><code>tf.contrib.graph_editor.reroute_a2b_inputs(sgv0, sgv1)</code></h3> <p>Re-route all the inputs of sgv0 to sgv1 (see reroute_inputs).</p>  <h3 id=\"reroute_a2b_outputs\"><code>tf.contrib.graph_editor.reroute_a2b_outputs(sgv0, sgv1)</code></h3> <p>Re-route all the outputs of sgv0 to sgv1 (see _reroute_outputs).</p>  <h3 id=\"reroute_b2a\"><code>tf.contrib.graph_editor.reroute_b2a(sgv0, sgv1)</code></h3> <p>Re-route the inputs and outputs of sgv1 to sgv0 (see _reroute).</p>  <h3 id=\"reroute_b2a_inputs\"><code>tf.contrib.graph_editor.reroute_b2a_inputs(sgv0, sgv1)</code></h3> <p>Re-route all the inputs of sgv1 to sgv0 (see reroute_inputs).</p>  <h3 id=\"reroute_b2a_outputs\"><code>tf.contrib.graph_editor.reroute_b2a_outputs(sgv0, sgv1)</code></h3> <p>Re-route all the outputs of sgv1 to sgv0 (see _reroute_outputs).</p>  <h3 id=\"select_ops\"><code>tf.contrib.graph_editor.select_ops(*args, **kwargs)</code></h3> <p>Helper to select operations.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>*args</code>: list of 1) regular expressions (compiled or not) or 2) (array of) tf.Operation. tf.Tensor instances are silently ignored.</li> <li>\n<code>**kwargs</code>: 'graph': tf.Graph in which to perform the regex query.This is required when using regex. 'positive_filter': an elem if selected only if positive_filter(elem) is True. This is optional. 'restrict_ops_regex': a regular expression is ignored if it doesn't start with the substring \"(?#ops)\".</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>list of tf.Operation</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if the optional keyword argument graph is not a tf.Graph or if an argument in args is not an (array of) tf.Operation or an (array of) tf.Tensor (silently ignored) or a string or a regular expression.</li> <li>\n<code>ValueError</code>: if one of the keyword arguments is unexpected or if a regular expression is used without passing a graph as a keyword argument.</li> </ul>  <h3 id=\"select_ts\"><code>tf.contrib.graph_editor.select_ts(*args, **kwargs)</code></h3> <p>Helper to select tensors.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>*args</code>: list of 1) regular expressions (compiled or not) or 2) (array of) tf.Tensor. tf.Operation instances are silently ignored.</li> <li>\n<code>**kwargs</code>: 'graph': tf.Graph in which to perform the regex query.This is required when using regex. 'positive_filter': an elem if selected only if positive_filter(elem) is True. This is optional. 'restrict_ts_regex': a regular expression is ignored if it doesn't start with the substring \"(?#ts)\".</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>list of tf.Tensor</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if the optional keyword argument graph is not a tf.Graph or if an argument in args is not an (array of) tf.Tensor or an (array of) tf.Operation (silently ignored) or a string or a regular expression.</li> <li>\n<code>ValueError</code>: if one of the keyword arguments is unexpected or if a regular expression is used without passing a graph as a keyword argument.</li> </ul>  <h3 id=\"sgv\"><code>tf.contrib.graph_editor.sgv(*args, **kwargs)</code></h3> <p>Create a SubGraphView from selected operations and passthrough tensors.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>*args</code>: list of 1) regular expressions (compiled or not) or 2) (array of) tf.Operation 3) (array of) tf.Tensor. Those objects will be converted into a list of operations and a list of candidate for passthrough tensors.</li> <li>\n<code>**kwargs</code>: keyword graph is used 1) to check that the ops and ts are from the correct graph 2) for regular expression query</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>A subgraph view.</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if the optional keyword argument graph is not a tf.Graph or if an argument in args is not an (array of) tf.Tensor or an (array of) tf.Operation or a string or a regular expression.</li> <li>\n<code>ValueError</code>: if one of the keyword arguments is unexpected.</li> </ul>  <h3 id=\"sgv_scope\"><code>tf.contrib.graph_editor.sgv_scope(scope, graph)</code></h3> <p>Make a subgraph from a name scope.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>scope</code>: the name of the scope.</li> <li>\n<code>graph</code>: the tf.Graph.</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A subgraph view representing the given scope.</p>  <h3 id=\"swap\"><code>tf.contrib.graph_editor.swap(sgv0, sgv1)</code></h3> <p>Swap the inputs and outputs of sgv1 to sgv0 (see _reroute).</p>  <h3 id=\"swap_inputs\"><code>tf.contrib.graph_editor.swap_inputs(sgv0, sgv1)</code></h3> <p>Swap all the inputs of sgv0 and sgv1 (see reroute_inputs).</p>  <h3 id=\"swap_outputs\"><code>tf.contrib.graph_editor.swap_outputs(sgv0, sgv1)</code></h3> <p>Swap all the outputs of sgv0 and sgv1 (see _reroute_outputs).</p>  <h3 id=\"ts\"><code>tf.contrib.graph_editor.ts(*args, **kwargs)</code></h3> <p>Helper to select tensors.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>*args</code>: list of 1) regular expressions (compiled or not) or 2) (array of) tf.Tensor. tf.Operation instances are silently ignored.</li> <li>\n<code>**kwargs</code>: 'graph': tf.Graph in which to perform the regex query.This is required when using regex. 'positive_filter': an elem if selected only if positive_filter(elem) is True. This is optional. 'restrict_ts_regex': a regular expression is ignored if it doesn't start with the substring \"(?#ts)\".</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>list of tf.Tensor</p>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if the optional keyword argument graph is not a tf.Graph or if an argument in args is not an (array of) tf.Tensor or an (array of) tf.Operation (silently ignored) or a string or a regular expression.</li> <li>\n<code>ValueError</code>: if one of the keyword arguments is unexpected or if a regular expression is used without passing a graph as a keyword argument.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.graph_editor.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.graph_editor.html</a>\n  </p>\n</div>\n","contrib.util":"<h1 id=\"utilities-contrib\">Utilities (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#utilities-contrib\">Utilities (contrib)</a></li> <ul> <li><a href=\"#miscellaneous-utility-functions\">Miscellaneous Utility Functions</a></li> <ul> <li><a href=\"#constant_value\"><code>tf.contrib.util.constant_value(tensor)</code></a></li> <li><a href=\"#make_tensor_proto\"><code>tf.contrib.util.make_tensor_proto(values, dtype=None, shape=None)</code></a></li> <li><a href=\"#make_ndarray\"><code>tf.contrib.util.make_ndarray(tensor)</code></a></li> <li><a href=\"#ops_used_by_graph_def\"><code>tf.contrib.util.ops_used_by_graph_def(graph_def)</code></a></li> <li><a href=\"#stripped_op_list_for_graph\"><code>tf.contrib.util.stripped_op_list_for_graph(graph_def)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Utilities for dealing with Tensors.</p>  <h2 id=\"miscellaneous-utility-functions\">Miscellaneous Utility Functions</h2>  <h3 id=\"constant_value\"><code>tf.contrib.util.constant_value(tensor)</code></h3> <p>Returns the constant value of the given tensor, if efficiently calculable.</p> <p>This function attempts to partially evaluate the given tensor, and returns its value as a numpy ndarray if this succeeds.</p> <p>TODO(<a target=\"_blank\" href=\"https://teams.googleplex.com/mrry\" class=\"di-hover\" data-ldap=\"mrry\">mrry</a>): Consider whether this function should use a registration mechanism like gradients and ShapeFunctions, so that it is easily extensible.</p> <p>NOTE: If <code>constant_value(tensor)</code> returns a non-<code>None</code> result, it will no longer be possible to feed a different value for <code>tensor</code>. This allows the result of this function to influence the graph that is constructed, and permits static shape optimizations.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>tensor</code>: The Tensor to be evaluated.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A numpy ndarray containing the constant value of the given <code>tensor</code>, or None if it cannot be calculated.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if tensor is not an ops.Tensor.</li> </ul>  <h3 id=\"make_tensor_proto\"><code>tf.contrib.util.make_tensor_proto(values, dtype=None, shape=None)</code></h3> <p>Create a TensorProto.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>values</code>: Values to put in the TensorProto.</li> <li>\n<code>dtype</code>: Optional tensor_pb2 DataType value.</li> <li>\n<code>shape</code>: List of integers representing the dimensions of tensor.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A TensorProto. Depending on the type, it may contain data in the \"tensor_content\" attribute, which is not directly useful to Python programs. To access the values you should convert the proto back to a numpy ndarray with tensor_util.MakeNdarray(proto).</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if unsupported types are provided.</li> <li>\n<code>ValueError</code>: if arguments have inappropriate values.</li> </ul> <p>make_tensor_proto accepts \"values\" of a python scalar, a python list, a numpy ndarray, or a numpy scalar.</p> <p>If \"values\" is a python scalar or a python list, make_tensor_proto first convert it to numpy ndarray. If dtype is None, the conversion tries its best to infer the right numpy data type. Otherwise, the resulting numpy array has a compatible data type with the given dtype.</p> <p>In either case above, the numpy ndarray (either the caller provided or the auto converted) must have the compatible type with dtype.</p> <p>make_tensor_proto then converts the numpy array to a tensor proto.</p> <p>If \"shape\" is None, the resulting tensor proto represents the numpy array precisely.</p> <p>Otherwise, \"shape\" specifies the tensor's shape and the numpy array can not have more elements than what \"shape\" specifies.</p>  <h3 id=\"make_ndarray\"><code>tf.contrib.util.make_ndarray(tensor)</code></h3> <p>Create a numpy ndarray from a tensor.</p> <p>Create a numpy ndarray with the same shape and data as the tensor.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>tensor</code>: A TensorProto.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>A numpy array with the tensor contents.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if tensor has unsupported type.</li> </ul>  <h3 id=\"ops_used_by_graph_def\"><code>tf.contrib.util.ops_used_by_graph_def(graph_def)</code></h3> <p>Collect the list of ops used by a graph.</p> <p>Does not validate that the ops are all registered.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>graph_def</code>: A <code>GraphDef</code> proto, as from <code>graph.as_graph_def()</code>.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A list of strings, each naming an op used by the graph.</p>  <h3 id=\"stripped_op_list_for_graph\"><code>tf.contrib.util.stripped_op_list_for_graph(graph_def)</code></h3> <p>Collect the stripped OpDefs for ops used by a graph.</p> <p>This function computes the <code>stripped_op_list</code> field of <code>MetaGraphDef</code> and similar protos. The result can be communicated from the producer to the consumer, which can then use the C++ function <code>RemoveNewDefaultAttrsFromGraphDef</code> to improve forwards compatibility.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>graph_def</code>: A <code>GraphDef</code> proto, as from <code>graph.as_graph_def()</code>.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>An <code>OpList</code> of ops used by the graph.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If an unregistered op is used.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.util.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.util.html</a>\n  </p>\n</div>\n","contrib.layers":"<h1 id=\"layers-contrib\">Layers (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#layers-contrib\">Layers (contrib)</a></li> <ul> <li><a href=\"#higher-level-ops-for-building-neural-network-layers\">Higher level ops for building neural network layers.</a></li> <ul> <li><a href=\"#avg_pool2d\"><code>tf.contrib.layers.avg_pool2d(*args, **kwargs)</code></a></li> <li><a href=\"#batch_norm\"><code>tf.contrib.layers.batch_norm(*args, **kwargs)</code></a></li> <li><a href=\"#convolution2d\"><code>tf.contrib.layers.convolution2d(*args, **kwargs)</code></a></li> <li><a href=\"#convolution2d_in_plane\"><code>tf.contrib.layers.convolution2d_in_plane(*args, **kwargs)</code></a></li> <li><a href=\"#convolution2d_transpose\"><code>tf.contrib.layers.convolution2d_transpose(*args, **kwargs)</code></a></li> <li><a href=\"#flatten\"><code>tf.contrib.layers.flatten(*args, **kwargs)</code></a></li> <li><a href=\"#fully_connected\"><code>tf.contrib.layers.fully_connected(*args, **kwargs)</code></a></li> <li><a href=\"#max_pool2d\"><code>tf.contrib.layers.max_pool2d(*args, **kwargs)</code></a></li> <li><a href=\"#one_hot_encoding\"><code>tf.contrib.layers.one_hot_encoding(*args, **kwargs)</code></a></li> <li><a href=\"#repeat\"><code>tf.contrib.layers.repeat(inputs, repetitions, layer, *args, **kwargs)</code></a></li> <li><a href=\"#separable_convolution2d\"><code>tf.contrib.layers.separable_convolution2d(*args, **kwargs)</code></a></li> <li><a href=\"#stack\"><code>tf.contrib.layers.stack(inputs, layer, stack_args, **kwargs)</code></a></li> <li><a href=\"#unit_norm\"><code>tf.contrib.layers.unit_norm(*args, **kwargs)</code></a></li> </ul> <li><a href=\"#regularizers\">Regularizers</a></li> <ul> <li><a href=\"#apply_regularization\"><code>tf.contrib.layers.apply_regularization(regularizer, weights_list=None)</code></a></li> <li><a href=\"#l1_regularizer\"><code>tf.contrib.layers.l1_regularizer(scale, scope=None)</code></a></li> <li><a href=\"#l2_regularizer\"><code>tf.contrib.layers.l2_regularizer(scale, scope=None)</code></a></li> <li><a href=\"#sum_regularizer\"><code>tf.contrib.layers.sum_regularizer(regularizer_list, scope=None)</code></a></li> </ul> <li><a href=\"#initializers\">Initializers</a></li> <ul> <li><a href=\"#xavier_initializer\"><code>tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32)</code></a></li> <li><a href=\"#xavier_initializer_conv2d\"><code>tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32)</code></a></li> <li><a href=\"#variance_scaling_initializer\"><code>tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode=FAN_IN, uniform=False, seed=None, dtype=tf.float32)</code></a></li> </ul> <li><a href=\"#optimization\">Optimization</a></li> <ul> <li><a href=\"#optimize_loss\"><code>tf.contrib.layers.optimize_loss(loss, global_step, learning_rate, optimizer, gradient_noise_scale=None, gradient_multipliers=None, clip_gradients=None, moving_average_decay=None, learning_rate_decay_fn=None, update_ops=None, variables=None, name=None, summaries=None)</code></a></li> </ul> <li><a href=\"#summaries\">Summaries</a></li> <ul> <li><a href=\"#summarize_activation\"><code>tf.contrib.layers.summarize_activation(op)</code></a></li> <li><a href=\"#summarize_tensor\"><code>tf.contrib.layers.summarize_tensor(tensor, tag=None)</code></a></li> <li><a href=\"#summarize_tensors\"><code>tf.contrib.layers.summarize_tensors(tensors, summarizer=summarize_tensor)</code></a></li> <li><a href=\"#summarize_collection\"><code>tf.contrib.layers.summarize_collection(collection, name_filter=None, summarizer=summarize_tensor)</code></a></li> <li><a href=\"#summarize_activations\"><code>tf.contrib.layers.summarize_activations(name_filter=None, summarizer=summarize_activation)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Ops for building neural network layers, regularizers, summaries, etc.</p>  <h2 id=\"higher-level-ops-for-building-neural-network-layers\">Higher level ops for building neural network layers.</h2> <p>This package provides several ops that take care of creating variables that are used internally in a consistent way and provide the building blocks for many common machine learning algorithms.</p>  <h3 id=\"avg_pool2d\"><code>tf.contrib.layers.avg_pool2d(*args, **kwargs)</code></h3> <p>Adds a 2D average pooling op.</p> <p>It is assumed that the pooling is done per image but not in batch or channels.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>inputs</code>: A <code>Tensor</code> of size [batch_size, height, width, channels].</li> <li>\n<code>kernel_size</code>: A list of length 2: [kernel_height, kernel_width] of the pooling kernel over which the op is computed. Can be an int if both values are the same.</li> <li>\n<code>stride</code>: A list of length 2: [stride_height, stride_width]. Can be an int if both strides are the same. Note that presently both strides must have the same value.</li> <li>\n<code>padding</code>: The padding method, either 'VALID' or 'SAME'.</li> <li>\n<code>outputs_collections</code>: The collections to which the outputs are added.</li> <li>\n<code>scope</code>: Optional scope for op_scope.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A <code>Tensor</code> representing the results of the pooling operation.</p>  <h3 id=\"batch_norm\"><code>tf.contrib.layers.batch_norm(*args, **kwargs)</code></h3> <p>Adds a Batch Normalization layer from <a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/abs/1502.03167&amp;usg=AFQjCNEfFbC155me911plnLz7i6FE9f-jg\">http://arxiv.org/abs/1502.03167</a>.</p> <p>\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"</p> <p>Sergey Ioffe, Christian Szegedy</p> <p>Can be used as a normalizer function for conv2d and fully_connected.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>inputs</code>: a tensor of size <code>[batch_size, height, width, channels]</code> or <code>[batch_size, channels]</code>.</li> <li>\n<code>decay</code>: decay for the moving average.</li> <li>\n<code>center</code>: If True, subtract <code>beta</code>. If False, <code>beta</code> is ignored.</li> <li>\n<code>scale</code>: If True, multiply by <code>gamma</code>. If False, <code>gamma</code> is not used. When the next layer is linear (also e.g. <code>nn.relu</code>), this can be disabled since the scaling can be done by the next layer.</li> <li>\n<code>epsilon</code>: small float added to variance to avoid dividing by zero.</li> <li>\n<code>activation_fn</code>: Optional activation function.</li> <li>\n<code>updates_collections</code>: collections to collect the update ops for computation. If None, a control dependency would be added to make sure the updates are computed.</li> <li>\n<code>is_training</code>: whether or not the layer is in training mode. In training mode it would accumulate the statistics of the moments into <code>moving_mean</code> and <code>moving_variance</code> using an exponential moving average with the given <code>decay</code>. When it is not in training mode then it would use the values of the <code>moving_mean</code> and the <code>moving_variance</code>.</li> <li>\n<code>reuse</code>: whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li> <li>\n<code>variables_collections</code>: optional collections for the variables.</li> <li>\n<code>outputs_collections</code>: collections to add the outputs.</li> <li>\n<code>trainable</code>: If <code>True</code> also add variables to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li> <li>\n<code>scope</code>: Optional scope for <code>variable_op_scope</code>.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A <code>Tensor</code> representing the output of the operation.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if rank or last dimension of <code>inputs</code> is undefined.</li> </ul>  <h3 id=\"convolution2d\"><code>tf.contrib.layers.convolution2d(*args, **kwargs)</code></h3> <p>Adds a 2D convolution followed by an optional batch_norm layer.</p> <p><code>convolution2d</code> creates a variable called <code>weights</code>, representing the convolutional kernel, that is convolved with the <code>inputs</code> to produce a <code>Tensor</code> of activations. If a <code>normalizer_fn</code> is provided (such as <code>batch_norm</code>), it is then applied. Otherwise, if <code>normalizer_fn</code> is None and a <code>biases_initializer</code> is provided then a <code>biases</code> variable would be created and added the activations. Finally, if <code>activation_fn</code> is not <code>None</code>, it is applied to the activations as well.</p> <p>Performs a'trous convolution with input stride equal to rate if rate is greater than one.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>inputs</code>: a 4-D tensor <code>[batch_size, height, width, channels]</code>.</li> <li>\n<code>num_outputs</code>: integer, the number of output filters.</li> <li>\n<code>kernel_size</code>: a list of length 2 <code>[kernel_height, kernel_width]</code> of of the filters. Can be an int if both values are the same.</li> <li>\n<code>stride</code>: a list of length 2 <code>[stride_height, stride_width]</code>. Can be an int if both strides are the same. Note that presently both strides must have the same value.</li> <li>\n<code>padding</code>: one of <code>VALID</code> or <code>SAME</code>.</li> <li>\n<code>rate</code>: integer. If less than or equal to 1, a standard convolution is used. If greater than 1, than the a'trous convolution is applied and <code>stride</code> must be set to 1.</li> <li>\n<code>activation_fn</code>: activation function.</li> <li>\n<code>normalizer_fn</code>: normalization function to use instead of <code>biases</code>. If <code>normalize_fn</code> is provided then <code>biases_initializer</code> and <code>biases_regularizer</code> are ignored and <code>biases</code> are not created nor added.</li> <li>\n<code>normalizer_params</code>: normalization function parameters.</li> <li>\n<code>weights_initializer</code>: An initializer for the weights.</li> <li>\n<code>weights_regularizer</code>: Optional regularizer for the weights.</li> <li>\n<code>biases_initializer</code>: An initializer for the biases. If None skip biases.</li> <li>\n<code>biases_regularizer</code>: Optional regularizer for the biases.</li> <li>\n<code>reuse</code>: whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li> <li>\n<code>variables_collections</code>: optional list of collections for all the variables or a dictionay containing a different list of collection per variable.</li> <li>\n<code>outputs_collections</code>: collection to add the outputs.</li> <li>\n<code>trainable</code>: If <code>True</code> also add variables to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li> <li>\n<code>scope</code>: Optional scope for <code>variable_op_scope</code>.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>a tensor representing the output of the operation.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if both 'rate' and <code>stride</code> are larger than one.</li> </ul>  <h3 id=\"convolution2d_in_plane\"><code>tf.contrib.layers.convolution2d_in_plane(*args, **kwargs)</code></h3> <p>Performs the same in-plane convolution to each channel independently.</p> <p>This is useful for performing various simple channel-independent convolution operations such as image gradients:</p> <p>image = tf.constant(..., shape=(16, 240, 320, 3)) vert_gradients = layers.conv2d_in_plane(image, kernel=[1, -1], kernel_size=[2, 1]) horz_gradients = layers.conv2d_in_plane(image, kernel=[1, -1], kernel_size=[1, 2])</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>inputs</code>: a 4-D tensor with dimensions [batch_size, height, width, channels].</li> <li>\n<code>kernel_size</code>: a list of length 2 holding the [kernel_height, kernel_width] of of the pooling. Can be an int if both values are the same.</li> <li>\n<code>stride</code>: a list of length 2 <code>[stride_height, stride_width]</code>. Can be an int if both strides are the same. Note that presently both strides must have the same value.</li> <li>\n<code>padding</code>: the padding type to use, either 'SAME' or 'VALID'.</li> <li>\n<code>activation_fn</code>: activation function.</li> <li>\n<code>normalizer_fn</code>: normalization function to use instead of <code>biases</code>. If <code>normalize_fn</code> is provided then <code>biases_initializer</code> and <code>biases_regularizer</code> are ignored and <code>biases</code> are not created nor added.</li> <li>\n<code>normalizer_params</code>: normalization function parameters.</li> <li>\n<code>weights_initializer</code>: An initializer for the weights.</li> <li>\n<code>weights_regularizer</code>: Optional regularizer for the weights.</li> <li>\n<code>biases_initializer</code>: An initializer for the biases. If None skip biases.</li> <li>\n<code>biases_regularizer</code>: Optional regularizer for the biases.</li> <li>\n<code>reuse</code>: whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li> <li>\n<code>variables_collections</code>: optional list of collections for all the variables or a dictionay containing a different list of collection per variable.</li> <li>\n<code>outputs_collections</code>: collection to add the outputs.</li> <li>\n<code>trainable</code>: If <code>True</code> also add variables to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li> <li>\n<code>scope</code>: Optional scope for <code>variable_op_scope</code>.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>Tensor</code> representing the output of the operation.</p>  <h3 id=\"convolution2d_transpose\"><code>tf.contrib.layers.convolution2d_transpose(*args, **kwargs)</code></h3> <p>Adds a convolution2d_transpose with an optional batch normalization layer.</p> <p>The function creates a variable called <code>weights</code>, representing the kernel, that is convolved with the input. If <code>batch_norm_params</code> is <code>None</code>, a second variable called 'biases' is added to the result of the operation.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>inputs</code>: a tensor of size [batch_size, height, width, channels].</li> <li>\n<code>num_outputs</code>: integer, the number of output filters.</li> <li>\n<code>kernel_size</code>: a list of length 2 holding the [kernel_height, kernel_width] of of the filters. Can be an int if both values are the same.</li> <li>\n<code>stride</code>: a list of length 2: [stride_height, stride_width]. Can be an int if both strides are the same. Note that presently both strides must have the same value.</li> <li>\n<code>padding</code>: one of 'VALID' or 'SAME'.</li> <li>\n<code>activation_fn</code>: activation function.</li> <li>\n<code>normalizer_fn</code>: normalization function to use instead of <code>biases</code>. If <code>normalize_fn</code> is provided then <code>biases_initializer</code> and <code>biases_regularizer</code> are ignored and <code>biases</code> are not created nor added.</li> <li>\n<code>normalizer_params</code>: normalization function parameters.</li> <li>\n<code>weights_initializer</code>: An initializer for the weights.</li> <li>\n<code>weights_regularizer</code>: Optional regularizer for the weights.</li> <li>\n<code>biases_initializer</code>: An initializer for the biases. If None skip biases.</li> <li>\n<code>biases_regularizer</code>: Optional regularizer for the biases.</li> <li>\n<code>reuse</code>: whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li> <li>\n<code>variables_collections</code>: optional list of collections for all the variables or a dictionay containing a different list of collection per variable.</li> <li>\n<code>outputs_collections</code>: collection to add the outputs.</li> <li>\n<code>trainable</code>: whether or not the variables should be trainable or not.</li> <li>\n<code>scope</code>: Optional scope for variable_op_scope.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>a tensor representing the output of the operation.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if 'kernel_size' is not a list of length 2.</li> </ul>  <h3 id=\"flatten\"><code>tf.contrib.layers.flatten(*args, **kwargs)</code></h3> <p>Flattens the input while maintaining the batch_size.</p> <p>Assumes that the first dimension represents the batch.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>inputs</code>: a tensor of size [batch_size, ...].</li> <li>\n<code>outputs_collections</code>: collection to add the outputs.</li> <li>\n<code>scope</code>: Optional scope for op_scope.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>a flattened tensor with shape [batch_size, k].</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if inputs.shape is wrong.</li> </ul>  <h3 id=\"fully_connected\"><code>tf.contrib.layers.fully_connected(*args, **kwargs)</code></h3> <p>Adds a fully connected layer.</p> <p><code>fully_connected</code> creates a variable called <code>weights</code>, representing a fully connected weight matrix, which is multiplied by the <code>inputs</code> to produce a <code>Tensor</code> of hidden units. If a <code>normalizer_fn</code> is provided (such as <code>batch_norm</code>), it is then applied. Otherwise, if <code>normalizer_fn</code> is None and a <code>biases_initializer</code> is provided then a <code>biases</code> variable would be created and added the hidden units. Finally, if <code>activation_fn</code> is not <code>None</code>, it is applied to the hidden units as well.</p> <p>Note: that if <code>inputs</code> have a rank greater than 2, then <code>inputs</code> is flattened prior to the initial matrix multiply by <code>weights</code>.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>inputs</code>: A tensor of with at least rank 2 and value for the last dimension, i.e. <code>[batch_size, depth]</code>, <code>[None, None, None, channels]</code>.</li> <li>\n<code>num_outputs</code>: Integer, the number of output units in the layer.</li> <li>\n<code>activation_fn</code>: activation function.</li> <li>\n<code>normalizer_fn</code>: normalization function to use instead of <code>biases</code>. If <code>normalize_fn</code> is provided then <code>biases_initializer</code> and <code>biases_regularizer</code> are ignored and <code>biases</code> are not created nor added.</li> <li>\n<code>normalizer_params</code>: normalization function parameters.</li> <li>\n<code>weights_initializer</code>: An initializer for the weights.</li> <li>\n<code>weights_regularizer</code>: Optional regularizer for the weights.</li> <li>\n<code>biases_initializer</code>: An initializer for the biases. If None skip biases.</li> <li>\n<code>biases_regularizer</code>: Optional regularizer for the biases.</li> <li>\n<code>reuse</code>: whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li> <li>\n<code>variables_collections</code>: Optional list of collections for all the variables or a dictionary containing a different list of collections per variable.</li> <li>\n<code>outputs_collections</code>: collection to add the outputs.</li> <li>\n<code>trainable</code>: If <code>True</code> also add variables to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li> <li>\n<code>scope</code>: Optional scope for variable_op_scope.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>the tensor variable representing the result of the series of operations.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if x has rank less than 2 or if its last dimension is not set.</li> </ul>  <h3 id=\"max_pool2d\"><code>tf.contrib.layers.max_pool2d(*args, **kwargs)</code></h3> <p>Adds a 2D Max Pooling op.</p> <p>It is assumed that the pooling is done per image but not in batch or channels.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>inputs</code>: A <code>Tensor</code> of size [batch_size, height, width, channels].</li> <li>\n<code>kernel_size</code>: A list of length 2: [kernel_height, kernel_width] of the pooling kernel over which the op is computed. Can be an int if both values are the same.</li> <li>\n<code>stride</code>: A list of length 2: [stride_height, stride_width]. Can be an int if both strides are the same. Note that presently both strides must have the same value.</li> <li>\n<code>padding</code>: The padding method, either 'VALID' or 'SAME'.</li> <li>\n<code>outputs_collections</code>: The collections to which the outputs are added.</li> <li>\n<code>scope</code>: Optional scope for op_scope.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A <code>Tensor</code> representing the results of the pooling operation.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If 'kernel_size' is not a 2-D list</li> </ul>  <h3 id=\"one_hot_encoding\"><code>tf.contrib.layers.one_hot_encoding(*args, **kwargs)</code></h3> <p>Transform numeric labels into onehot_labels using tf.one_hot.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>labels</code>: [batch_size] target labels.</li> <li>\n<code>num_classes</code>: total number of classes.</li> <li>\n<code>on_value</code>: A scalar defining the on-value.</li> <li>\n<code>off_value</code>: A scalar defining the off-value.</li> <li>\n<code>outputs_collections</code>: collection to add the outputs.</li> <li>\n<code>scope</code>: Optional scope for op_scope.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>one hot encoding of the labels.</p>  <h3 id=\"repeat\"><code>tf.contrib.layers.repeat(inputs, repetitions, layer, *args, **kwargs)</code></h3> <p>Applies the same layer with the same arguments repeatedly.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">y = repeat(x, 3, conv2d, 64, [3, 3], scope='conv1')\n# It is equivalent to:\n\nx = conv2d(x, 64, [3, 3], scope='conv1/conv1_1')\nx = conv2d(x, 64, [3, 3], scope='conv1/conv1_2')\ny = conv2d(x, 64, [3, 3], scope='conv1/conv1_3')\n</pre> <p>If the <code>scope</code> argument is not given in <code>kwargs</code>, it is set to <code>layer.__name__</code>, or <code>layer.func.__name__</code> (for <code>functools.partial</code> objects). If neither <code>__name__</code> nor <code>func.__name__</code> is available, the layers are called with <code>scope='stack'</code>.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>inputs</code>: A <code>Tensor</code> suitable for layer.</li> <li>\n<code>repetitions</code>: Int, number of repetitions.</li> <li>\n<code>layer</code>: A layer with arguments <code>(inputs, *args, **kwargs)</code>\n</li> <li>\n<code>*args</code>: Extra args for the layer.</li> <li>\n<code>**kwargs</code>: Extra kwargs for the layer.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>a tensor result of applying the layer, repetitions times.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the op is unknown or wrong.</li> </ul>  <h3 id=\"separable_convolution2d\"><code>tf.contrib.layers.separable_convolution2d(*args, **kwargs)</code></h3> <p>Adds a depth-separable 2D convolution with optional batch_norm layer.</p> <p>This op first performs a depthwise convolution that acts separately on channels, creating a variable called <code>depthwise_weights</code>. If <code>num_outputs</code> is not None, it adds a pointwise convolution that mixes channels, creating a variable called <code>pointwise_weights</code>. Then, if <code>batch_norm_params</code> is None, it adds bias to the result, creating a variable called 'biases', otherwise it adds a batch normalization layer. It finally applies an activation function to produce the end result.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>inputs</code>: a tensor of size [batch_size, height, width, channels].</li> <li>\n<code>num_outputs</code>: the number of pointwise convolution output filters. If is None, then we skip the pointwise convolution stage.</li> <li>\n<code>kernel_size</code>: a list of length 2: [kernel_height, kernel_width] of of the filters. Can be an int if both values are the same.</li> <li>\n<code>depth_multiplier</code>: the number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to <code>num_filters_in * depth_multiplier</code>.</li> <li>\n<code>stride</code>: a list of length 2: [stride_height, stride_width], specifying the depthwise convolution stride. Can be an int if both strides are the same.</li> <li>\n<code>padding</code>: one of 'VALID' or 'SAME'.</li> <li>\n<code>activation_fn</code>: activation function.</li> <li>\n<code>normalizer_fn</code>: normalization function to use instead of <code>biases</code>. If <code>normalize_fn</code> is provided then <code>biases_initializer</code> and <code>biases_regularizer</code> are ignored and <code>biases</code> are not created nor added.</li> <li>\n<code>normalizer_params</code>: normalization function parameters.</li> <li>\n<code>weights_initializer</code>: An initializer for the weights.</li> <li>\n<code>weights_regularizer</code>: Optional regularizer for the weights.</li> <li>\n<code>biases_initializer</code>: An initializer for the biases. If None skip biases.</li> <li>\n<code>biases_regularizer</code>: Optional regularizer for the biases.</li> <li>\n<code>reuse</code>: whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li> <li>\n<code>variables_collections</code>: optional list of collections for all the variables or a dictionay containing a different list of collection per variable.</li> <li>\n<code>outputs_collections</code>: collection to add the outputs.</li> <li>\n<code>trainable</code>: whether or not the variables should be trainable or not.</li> <li>\n<code>scope</code>: Optional scope for variable_op_scope.</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A <code>Tensor</code> representing the output of the operation.</p>  <h3 id=\"stack\"><code>tf.contrib.layers.stack(inputs, layer, stack_args, **kwargs)</code></h3> <p>Builds a stack of layers by applying layer repeatedly using stack_args.</p> <p><code>stack</code> allows you to repeatedly apply the same operation with different arguments <code>stack_args[i]</code>. For each application of the layer, <code>stack</code> creates a new scope appended with an increasing number. For example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">y = stack(x, fully_connected, [32, 64, 128], scope='fc')\n# It is equivalent to:\n\nx = fully_connected(x, 32, scope='fc/fc_1')\nx = fully_connected(x, 64, scope='fc/fc_2')\ny = fully_connected(x, 128, scope='fc/fc_3')\n</pre> <p>If the <code>scope</code> argument is not given in <code>kwargs</code>, it is set to <code>layer.__name__</code>, or <code>layer.func.__name__</code> (for <code>functools.partial</code> objects). If neither <code>__name__</code> nor <code>func.__name__</code> is available, the layers are called with <code>scope='stack'</code>.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>inputs</code>: A <code>Tensor</code> suitable for layer.</li> <li>\n<code>layer</code>: A layer with arguments <code>(inputs, *args, **kwargs)</code>\n</li> <li>\n<code>stack_args</code>: A list/tuple of parameters for each call of layer.</li> <li>\n<code>**kwargs</code>: Extra kwargs for the layer.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>a <code>Tensor</code> result of applying the stacked layers.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the op is unknown or wrong.</li> </ul>  <h3 id=\"unit_norm\"><code>tf.contrib.layers.unit_norm(*args, **kwargs)</code></h3> <p>Normalizes the given input across the specified dimension to unit length.</p> <p>Note that the rank of <code>input</code> must be known.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>inputs</code>: A <code>Tensor</code> of arbitrary size.</li> <li>\n<code>dim</code>: The dimension along which the input is normalized.</li> <li>\n<code>epsilon</code>: A small value to add to the inputs to avoid dividing by zero.</li> <li>\n<code>scope</code>: Optional scope for variable_op_scope.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>The normalized <code>Tensor</code>.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If dim is smaller than the number of dimensions in 'inputs'.</li> </ul> <p>Aliases for fully_connected which set a default activation function are available: <code>relu</code>, <code>relu6</code> and <code>linear</code>.</p> <h2 id=\"regularizers\">Regularizers</h2> <p>Regularization can help prevent overfitting. These have the signature <code>fn(weights)</code>. The loss is typically added to <code>tf.GraphKeys.REGULARIZATION_LOSS</code></p>  <h3 id=\"apply_regularization\"><code>tf.contrib.layers.apply_regularization(regularizer, weights_list=None)</code></h3> <p>Returns the summed penalty by applying <code>regularizer</code> to the <code>weights_list</code>.</p> <p>Adding a regularization penalty over the layer weights and embedding weights can help prevent overfitting the training data. Regularization over layer biases is less common/useful, but assuming proper data preprocessing/mean subtraction, it usually shouldn't hurt much either.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>regularizer</code>: A function that takes a single <code>Tensor</code> argument and returns a scalar <code>Tensor</code> output.</li> <li>\n<code>weights_list</code>: List of weights <code>Tensors</code> or <code>Variables</code> to apply <code>regularizer</code> over. Defaults to the <code>GraphKeys.WEIGHTS</code> collection if <code>None</code>.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p>A scalar representing the overall regularization penalty.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>regularizer</code> does not return a scalar output, or if we find no weights.</li> </ul>  <h3 id=\"l1_regularizer\"><code>tf.contrib.layers.l1_regularizer(scale, scope=None)</code></h3> <p>Returns a function that can be used to apply L1 regularization to weights.</p> <p>L1 regularization encourages sparsity.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>scale</code>: A scalar multiplier <code>Tensor</code>. 0.0 disables the regularizer.</li> <li>\n<code>scope</code>: An optional op_scope name.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>A function with signature <code>l1(weights)</code> that apply L1 regularization.</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If scale is negative or if scale is not a float.</li> </ul>  <h3 id=\"l2_regularizer\"><code>tf.contrib.layers.l2_regularizer(scale, scope=None)</code></h3> <p>Returns a function that can be used to apply L2 regularization to weights.</p> <p>Small values of L2 can help prevent overfitting the training data.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>scale</code>: A scalar multiplier <code>Tensor</code>. 0.0 disables the regularizer.</li> <li>\n<code>scope</code>: An optional op_scope name.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A function with signature <code>l2(weights)</code> that applies L2 regularization.</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If scale is negative or if scale is not a float.</li> </ul>  <h3 id=\"sum_regularizer\"><code>tf.contrib.layers.sum_regularizer(regularizer_list, scope=None)</code></h3> <p>Returns a function that applies the sum of multiple regularizers.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>regularizer_list</code>: A list of regularizers to apply.</li> <li>\n<code>scope</code>: An optional op_scope name</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>A function with signature <code>sum_reg(weights)</code> that applies the sum of all the input regularizers.</p> <h2 id=\"initializers\">Initializers</h2> <p>Initializers are used to initialize variables with sensible values given their size, data type, and purpose.</p>  <h3 id=\"xavier_initializer\"><code>tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32)</code></h3> <p>Returns an initializer performing \"Xavier\" initialization for weights.</p> <p>This function implements the weight initialization from:</p> <p>Xavier Glorot and Yoshua Bengio (2010): Understanding the difficulty of training deep feedforward neural networks. International conference on artificial intelligence and statistics.</p> <p>This initializer is designed to keep the scale of the gradients roughly the same in all layers. In uniform distribution this ends up being the range: <code>x = sqrt(6. / (in + out)); [-x, x]</code> and for normal distribution a standard deviation of <code>sqrt(3. / (in + out))</code> is used.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>uniform</code>: Whether to use uniform or normal distributed random initialization.</li> <li>\n<code>seed</code>: A Python integer. Used to create random seeds. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>dtype</code>: The data type. Only floating point types are supported.</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>An initializer for a weight matrix.</p>  <h3 id=\"xavier_initializer_conv2d\"><code>tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32)</code></h3> <p>Returns an initializer performing \"Xavier\" initialization for weights.</p> <p>This function implements the weight initialization from:</p> <p>Xavier Glorot and Yoshua Bengio (2010): Understanding the difficulty of training deep feedforward neural networks. International conference on artificial intelligence and statistics.</p> <p>This initializer is designed to keep the scale of the gradients roughly the same in all layers. In uniform distribution this ends up being the range: <code>x = sqrt(6. / (in + out)); [-x, x]</code> and for normal distribution a standard deviation of <code>sqrt(3. / (in + out))</code> is used.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>uniform</code>: Whether to use uniform or normal distributed random initialization.</li> <li>\n<code>seed</code>: A Python integer. Used to create random seeds. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>dtype</code>: The data type. Only floating point types are supported.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>An initializer for a weight matrix.</p>  <h3 id=\"variance_scaling_initializer\"><code>tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32)</code></h3> <p>Returns an initializer that generates tensors without scaling variance.</p> <p>When initializing a deep network, it is in principle advantageous to keep the scale of the input variance constant, so it does not explode or diminish by reaching the final layer. This initializer use the following formula: if mode='FAN_IN': # Count only number of input connections. n = fan_in elif mode='FAN_OUT': # Count only number of output connections. n = fan_out elif mode='FAN_AVG': # Average number of inputs and output connections. n = (fan_in + fan_out)/2.0</p> <pre class=\"\">truncated_normal(shape, 0.0, stddev=sqrt(factor / n))\n</pre> <p>To get <a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/pdf/1502.01852v1.pdf&amp;usg=AFQjCNGaWiP9SaGsRAoqEsBIZ25CGsYo8A\">http://arxiv.org/pdf/1502.01852v1.pdf</a> use (Default): - factor=2.0 mode='FAN_IN' uniform=False To get <a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/abs/1408.5093&amp;usg=AFQjCNEV0kuar-1L2atbZHlQrwitHkgycA\">http://arxiv.org/abs/1408.5093</a> use: - factor=1.0 mode='FAN_IN' uniform=True To get <a target=\"_blank\" href=\"https://www.google.com/url?q=http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf&amp;usg=AFQjCNF3TkW9lI1HEDQX3gvc1CV3wwjI3A\">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a> use: - factor=1.0 mode='FAN_AVG' uniform=True. To get xavier_initializer use either: - factor=1.0 mode='FAN_AVG' uniform=True. - factor=1.0 mode='FAN_AVG' uniform=False.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>factor</code>: Float. A multiplicative factor.</li> <li>\n<code>mode</code>: String. 'FAN_IN', 'FAN_OUT', 'FAN_AVG'.</li> <li>\n<code>uniform</code>: Whether to use uniform or normal distributed random initialization.</li> <li>\n<code>seed</code>: A Python integer. Used to create random seeds. See <a href=\"constant_op#set_random_seed\"><code>set_random_seed</code></a> for behavior.</li> <li>\n<code>dtype</code>: The data type. Only floating point types are supported.</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>An initializer that generates tensors with unit variance.</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>dtype</code> is not a floating point type.</li> <li>\n<code>TypeError</code>: if <code>mode</code> is not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG'].</li> </ul> <h2 id=\"optimization\">Optimization</h2> <p>Optimize weights given a loss.</p>  <h3 id=\"optimize_loss\"><code>tf.contrib.layers.optimize_loss(loss, global_step, learning_rate, optimizer, gradient_noise_scale=None, gradient_multipliers=None, clip_gradients=None, moving_average_decay=None, learning_rate_decay_fn=None, update_ops=None, variables=None, name=None, summaries=None)</code></h3> <p>Given loss and parameters for optimizer, returns a training op.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>loss</code>: Tensor, 0 dimensional.</li> <li>\n<code>global_step</code>: Tensor, step counter for each update.</li> <li>\n<code>learning_rate</code>: float or Tensor, magnitude of update per each training step.</li> <li>\n<code>optimizer</code>: string, class or optimizer instance, used as trainer. string should be name of optimizer, like 'SGD', 'Adam', 'Adagrad'. Full list in OPTIMIZER_CLS_NAMES constant. class should be sub-class of tf.Optimizer that implements <code>compute_gradients</code> and <code>apply_gradients</code> functions. optimizer instance should be instantion of tf.Optimizer sub-class and have <code>compute_gradients</code> and <code>apply_gradients</code> functions.</li> <li>\n<code>gradient_noise_scale</code>: float or None, adds 0-mean normal noise scaled by this value.</li> <li>\n<code>gradient_multipliers</code>: dict of variables or variable names to floats. If present, gradients for specified variables will be multiplied by given constant.</li> <li>\n<code>clip_gradients</code>: float or <code>None</code>, clips gradients by this value.</li> <li>\n<code>moving_average_decay</code>: Deprecated. float or None, takes into account previous loss to make learning smoother due to outliers.</li> <li>\n<code>learning_rate_decay_fn</code>: function, takes <code>learning_rate</code> and <code>global_step</code> <code>Tensor</code>s, returns <code>Tensor</code>. Can be used to implement any learning rate decay functions. For example: tf.train.exponential_decay.</li> <li>\n<code>update_ops</code>: list of update <code>Operation</code>s to execute at each step. If <code>None</code>, uses elements of UPDATE_OPS collection.</li> <li>\n<code>variables</code>: list of variables to optimize or <code>None</code> to use all trainable variables.</li> <li>\n<code>name</code>: The name for this operation is used to scope operations and summaries.</li> <li>\n<code>summaries</code>: List of internal quantities to visualize on tensorboard. If not set only the loss and the learning rate will be reported. The complete list is in OPTIMIZER_SUMMARIES.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>Training op.</p>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if optimizer is wrong type.</li> </ul> <h2 id=\"summaries\">Summaries</h2> <p>Helper functions to summarize specific variables or ops.</p>  <h3 id=\"summarize_activation\"><code>tf.contrib.layers.summarize_activation(op)</code></h3> <p>Summarize an activation.</p> <p>This applies the given activation and adds useful summaries specific to the activation.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>op</code>: The tensor to summarize (assumed to be a layer activation).</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>The summary op created to summarize <code>op</code>.</p>  <h3 id=\"summarize_tensor\"><code>tf.contrib.layers.summarize_tensor(tensor, tag=None)</code></h3> <p>Summarize a tensor using a suitable summary type.</p> <p>This function adds a summary op for <code>tensor</code>. The type of summary depends on the shape of <code>tensor</code>. For scalars, a <code>scalar_summary</code> is created, for all other tensors, <code>histogram_summary</code> is used.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>tensor</code>: The tensor to summarize</li> <li>\n<code>tag</code>: The tag to use, if None then use tensor's op's name.</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>The summary op created or None for string tensors.</p>  <h3 id=\"summarize_tensors\"><code>tf.contrib.layers.summarize_tensors(tensors, summarizer=summarize_tensor)</code></h3> <p>Summarize a set of tensors.</p>  <h3 id=\"summarize_collection\"><code>tf.contrib.layers.summarize_collection(collection, name_filter=None, summarizer=summarize_tensor)</code></h3> <p>Summarize a graph collection of tensors, possibly filtered by name.</p> <p>The layers module defines convenience functions <code>summarize_variables</code>, <code>summarize_weights</code> and <code>summarize_biases</code>, which set the <code>collection</code> argument of <code>summarize_collection</code> to <code>VARIABLES</code>, <code>WEIGHTS</code> and <code>BIASES</code>, respectively.</p>  <h3 id=\"summarize_activations\"><code>tf.contrib.layers.summarize_activations(name_filter=None, summarizer=summarize_activation)</code></h3> <p>Summarize activations, using <code>summarize_activation</code> to summarize.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html</a>\n  </p>\n</div>\n","contrib.losses":"<h1 id=\"losses-contrib\">Losses (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#losses-contrib\">Losses (contrib)</a></li> <ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#absolute_difference\"><code>tf.contrib.losses.absolute_difference(predictions, targets, weight=1.0, scope=None)</code></a></li> <li><a href=\"#add_loss\"><code>tf.contrib.losses.add_loss(loss)</code></a></li> <li><a href=\"#cosine_distance\"><code>tf.contrib.losses.cosine_distance(predictions, targets, dim, weight=1.0, scope=None)</code></a></li> <li><a href=\"#get_losses\"><code>tf.contrib.losses.get_losses(scope=None)</code></a></li> <li><a href=\"#get_regularization_losses\"><code>tf.contrib.losses.get_regularization_losses(scope=None)</code></a></li> <li><a href=\"#get_total_loss\"><code>tf.contrib.losses.get_total_loss(add_regularization_losses=True, name=total_loss)</code></a></li> <li><a href=\"#hinge_loss\"><code>tf.contrib.losses.hinge_loss(logits, target, scope=None)</code></a></li> <li><a href=\"#log_loss\"><code>tf.contrib.losses.log_loss(predictions, targets, weight=1.0, epsilon=1e-07, scope=None)</code></a></li> <li><a href=\"#sigmoid_cross_entropy\"><code>tf.contrib.losses.sigmoid_cross_entropy(logits, multi_class_labels, weight=1.0, label_smoothing=0, scope=None)</code></a></li> <li><a href=\"#softmax_cross_entropy\"><code>tf.contrib.losses.softmax_cross_entropy(logits, onehot_labels, weight=1.0, label_smoothing=0, scope=None)</code></a></li> <li><a href=\"#sum_of_pairwise_squares\"><code>tf.contrib.losses.sum_of_pairwise_squares(predictions, targets, weight=1.0, scope=None)</code></a></li> <li><a href=\"#sum_of_squares\"><code>tf.contrib.losses.sum_of_squares(predictions, targets, weight=1.0, scope=None)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Ops for building neural network losses.</p>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"absolute_difference\"><code>tf.contrib.losses.absolute_difference(predictions, targets, weight=1.0, scope=None)</code></h3> <p>Adds an Absolute Difference loss to the training procedure.</p> <p><code>weight</code> acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If <code>weight</code> is a tensor of size [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the <code>weight</code> vector. If the shape of <code>weight</code> matches the shape of <code>predictions</code>, then the loss of each measurable element of <code>predictions</code> is scaled by the corresponding value of <code>weight</code>.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>predictions</code>: The predicted outputs.</li> <li>\n<code>targets</code>: The ground truth output tensor, same dimensions as 'predictions'.</li> <li>\n<code>weight</code>: Coefficients for the loss a scalar, a tensor of shape [batch_size] or a tensor whose shape matches <code>predictions</code>.</li> <li>\n<code>scope</code>: The scope for the operations performed in computing the loss.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>A scalar <code>Tensor</code> representing the loss value.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>predictions</code> doesn't match that of <code>targets</code> or if the shape of <code>weight</code> is invalid.</li> </ul>  <h3 id=\"add_loss\"><code>tf.contrib.losses.add_loss(loss)</code></h3> <p>Adds a externally defined loss to collection of losses.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>loss</code>: A loss <code>Tensor</code>.</li> </ul>  <h3 id=\"cosine_distance\"><code>tf.contrib.losses.cosine_distance(predictions, targets, dim, weight=1.0, scope=None)</code></h3> <p>Adds a cosine-distance loss to the training procedure.</p> <p>Note that the function assumes that the predictions and targets are already unit-normalized.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>predictions</code>: An arbitrary matrix.</li> <li>\n<code>targets</code>: A <code>Tensor</code> whose shape matches 'predictions'</li> <li>\n<code>dim</code>: The dimension along which the cosine distance is computed.</li> <li>\n<code>weight</code>: Coefficients for the loss a scalar, a tensor of shape [batch_size] or a tensor whose shape matches <code>predictions</code>.</li> <li>\n<code>scope</code>: The scope for the operations performed in computing the loss.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>A scalar <code>Tensor</code> representing the loss value.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If predictions.shape doesn't match targets.shape, if the ignore mask is provided and its shape doesn't match targets.shape or if the ignore mask is not boolean valued.</li> </ul>  <h3 id=\"get_losses\"><code>tf.contrib.losses.get_losses(scope=None)</code></h3> <p>Gets the list of loss variables.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>scope</code>: an optional scope for filtering the losses to return.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>a list of loss variables.</p>  <h3 id=\"get_regularization_losses\"><code>tf.contrib.losses.get_regularization_losses(scope=None)</code></h3> <p>Gets the regularization losses.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>scope</code>: an optional scope for filtering the losses to return.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A list of loss variables.</p>  <h3 id=\"get_total_loss\"><code>tf.contrib.losses.get_total_loss(add_regularization_losses=True, name='total_loss')</code></h3> <p>Returns a tensor whose value represents the total loss.</p> <p>Notice that the function adds the given losses to the regularization losses.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>add_regularization_losses</code>: A boolean indicating whether or not to use the regularization losses in the sum.</li> <li>\n<code>name</code>: The name of the returned tensor.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A <code>Tensor</code> whose value represents the total loss.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>losses</code> is not iterable.</li> </ul>  <h3 id=\"hinge_loss\"><code>tf.contrib.losses.hinge_loss(logits, target, scope=None)</code></h3> <p>Method that returns the loss tensor for hinge loss.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>logits</code>: The logits, a float tensor.</li> <li>\n<code>target</code>: The ground truth output tensor. Its shape should match the shape of logits. The values of the tensor are expected to be 0.0 or 1.0.</li> <li>\n<code>scope</code>: The scope for the operations performed in computing the loss.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>A <code>Tensor</code> of same shape as logits and target representing the loss values across the batch.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shapes of <code>logits</code> and <code>target</code> don't match.</li> </ul>  <h3 id=\"log_loss\"><code>tf.contrib.losses.log_loss(predictions, targets, weight=1.0, epsilon=1e-07, scope=None)</code></h3> <p>Adds a Log Loss term to the training procedure.</p> <p><code>weight</code> acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If <code>weight</code> is a tensor of size [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the <code>weight</code> vector. If the shape of <code>weight</code> matches the shape of <code>predictions</code>, then the loss of each measurable element of <code>predictions</code> is scaled by the corresponding value of <code>weight</code>.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>predictions</code>: The predicted outputs.</li> <li>\n<code>targets</code>: The ground truth output tensor, same dimensions as 'predictions'.</li> <li>\n<code>weight</code>: Coefficients for the loss a scalar, a tensor of shape [batch_size] or a tensor whose shape matches <code>predictions</code>.</li> <li>\n<code>epsilon</code>: A small increment to add to avoid taking a log of zero.</li> <li>\n<code>scope</code>: The scope for the operations performed in computing the loss.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>A scalar <code>Tensor</code> representing the loss value.</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>predictions</code> doesn't match that of <code>targets</code> or if the shape of <code>weight</code> is invalid.</li> </ul>  <h3 id=\"sigmoid_cross_entropy\"><code>tf.contrib.losses.sigmoid_cross_entropy(logits, multi_class_labels, weight=1.0, label_smoothing=0, scope=None)</code></h3> <p>Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits.</p> <p><code>weight</code> acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If <code>weight</code> is a tensor of size [<code>batch_size</code>], then the loss weights apply to each corresponding sample.</p> <p>If <code>label_smoothing</code> is nonzero, smooth the labels towards 1/2: new_multiclass_labels = multiclass_labels * (1 - label_smoothing) + 0.5 * label_smoothing</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>logits</code>: [batch_size, num_classes] logits outputs of the network .</li> <li>\n<code>multi_class_labels</code>: [batch_size, num_classes] target labels in (0, 1).</li> <li>\n<code>weight</code>: Coefficients for the loss. The tensor must be a scalar, a tensor of shape [batch_size] or shape [batch_size, num_classes].</li> <li>\n<code>label_smoothing</code>: If greater than 0 then smooth the labels.</li> <li>\n<code>scope</code>: The scope for the operations performed in computing the loss.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A scalar <code>Tensor</code> representing the loss value.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>predictions</code> doesn't match that of <code>targets</code> or if the shape of <code>weight</code> is invalid or if <code>weight</code> is None.</li> </ul>  <h3 id=\"softmax_cross_entropy\"><code>tf.contrib.losses.softmax_cross_entropy(logits, onehot_labels, weight=1.0, label_smoothing=0, scope=None)</code></h3> <p>Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits.</p> <p><code>weight</code> acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If <code>weight</code> is a tensor of size [<code>batch_size</code>], then the loss weights apply to each corresponding sample.</p> <p>If <code>label_smoothing</code> is nonzero, smooth the labels towards 1/num_classes: new_onehot_labels = onehot_labels * (1 - label_smoothing) + label_smoothing / num_classes</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>logits</code>: [batch_size, num_classes] logits outputs of the network .</li> <li>\n<code>onehot_labels</code>: [batch_size, num_classes] target one_hot_encoded labels.</li> <li>\n<code>weight</code>: Coefficients for the loss. The tensor must be a scalar or a tensor of shape [batch_size].</li> <li>\n<code>label_smoothing</code>: If greater than 0 then smooth the labels.</li> <li>\n<code>scope</code>: the scope for the operations performed in computing the loss.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>A scalar <code>Tensor</code> representing the loss value.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>predictions</code> doesn't match that of <code>targets</code> or if the shape of <code>weight</code> is invalid or if <code>weight</code> is None.</li> </ul>  <h3 id=\"sum_of_pairwise_squares\"><code>tf.contrib.losses.sum_of_pairwise_squares(predictions, targets, weight=1.0, scope=None)</code></h3> <p>Adds a pairwise-errors-squared loss to the training procedure.</p> <p>Unlike the sum_of_squares loss, which is a measure of the differences between corresponding elements of <code>predictions</code> and <code>targets</code>, sum_of_pairwise_squares is a measure of the differences between pairs of corresponding elements of <code>predictions</code> and <code>targets</code>.</p> <p>For example, if <code>targets</code>=[a, b, c] and <code>predictions</code>=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3</p> <p>Note that since the inputs are of size [batch_size, d0, ... dN], the corresponding pairs are computed within each batch sample but not across samples within a batch. For example, if <code>predictions</code> represents a batch of 16 grayscale images of dimenion [batch_size, 100, 200], then the set of pairs is drawn from each image, but not across images.</p> <p><code>weight</code> acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If <code>weight</code> is a tensor of size [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the <code>weight</code> vector.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>predictions</code>: The predicted outputs, a tensor of size [batch_size, d0, .. dN] where N+1 is the total number of dimensions in <code>predictions</code>.</li> <li>\n<code>targets</code>: The ground truth output tensor, whose shape must match the shape of the <code>predictions</code> tensor.</li> <li>\n<code>weight</code>: Coefficients for the loss a scalar, a tensor of shape [batch_size] or a tensor whose shape matches <code>predictions</code>.</li> <li>\n<code>scope</code>: The scope for the operations performed in computing the loss.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A scalar <code>Tensor</code> representing the loss value.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>predictions</code> doesn't match that of <code>targets</code> or if the shape of <code>weight</code> is invalid.</li> </ul>  <h3 id=\"sum_of_squares\"><code>tf.contrib.losses.sum_of_squares(predictions, targets, weight=1.0, scope=None)</code></h3> <p>Adds a Sum-of-Squares loss to the training procedure.</p> <p><code>weight</code> acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If <code>weight</code> is a tensor of size [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the <code>weight</code> vector. If the shape of <code>weight</code> matches the shape of <code>predictions</code>, then the loss of each measurable element of <code>predictions</code> is scaled by the corresponding value of <code>weight</code>.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>predictions</code>: The predicted outputs.</li> <li>\n<code>targets</code>: The ground truth output tensor, same dimensions as 'predictions'.</li> <li>\n<code>weight</code>: Coefficients for the loss a scalar, a tensor of shape [batch_size] or a tensor whose shape matches <code>predictions</code>.</li> <li>\n<code>scope</code>: The scope for the operations performed in computing the loss.</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A scalar <code>Tensor</code> representing the loss value.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>predictions</code> doesn't match that of <code>targets</code> or if the shape of <code>weight</code> is invalid.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.losses.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.losses.html</a>\n  </p>\n</div>\n","contrib.rnn":"<h1 id=\"rnn-contrib\">RNN (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#rnn-contrib\">RNN (contrib)</a></li> <ul> <li><a href=\"#this-package-provides-additional-contributed-rnncells\">This package provides additional contributed RNNCells.</a></li> <ul> <li><a href=\"#fused-rnncells\">Fused RNNCells</a></li> <li><a href=\"#LSTMFusedCell\"><code>class tf.contrib.rnn.LSTMFusedCell</code></a></li> <li><a href=\"#lstm-like-cells\">LSTM-like cells</a></li> <li><a href=\"#CoupledInputForgetGateLSTMCell\"><code>class tf.contrib.rnn.CoupledInputForgetGateLSTMCell</code></a></li> <li><a href=\"#TimeFreqLSTMCell\"><code>class tf.contrib.rnn.TimeFreqLSTMCell</code></a></li> <li><a href=\"#GridLSTMCell\"><code>class tf.contrib.rnn.GridLSTMCell</code></a></li> <li><a href=\"#rnncell-wrappers\">RNNCell wrappers</a></li> <li><a href=\"#AttentionCellWrapper\"><code>class tf.contrib.rnn.AttentionCellWrapper</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Additional RNN operations and cells.</p>  <h2 id=\"this-package-provides-additional-contributed-rnncells\">This package provides additional contributed RNNCells.</h2>  <h3 id=\"fused-rnncells\">Fused RNNCells</h3>  <h3 id=\"LSTMFusedCell\"><code>class tf.contrib.rnn.LSTMFusedCell</code></h3> <p>Basic LSTM recurrent network cell.</p> <p>The implementation is based on: <a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/abs/1409.2329&amp;usg=AFQjCNFrC4k5oZBQOzm-F3FaU62wXp_VYQ\">http://arxiv.org/abs/1409.2329</a>.</p> <p>We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training.</p> <p>Unlike BasicLSTMCell, this is a monolithic op and should be much faster. The weight and bias matrixes should be compatible as long as the variabel scope matches.</p>  <h4 id=\"LSTMFusedCell.__init__\"><code>tf.contrib.rnn.LSTMFusedCell.__init__(num_units, forget_bias=1.0, use_peephole=False)</code></h4> <p>Initialize the basic LSTM cell.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>num_units</code>: int, The number of units in the LSTM cell.</li> <li>\n<code>forget_bias</code>: float, The bias added to forget gates (see above).</li> <li>\n<code>use_peephole</code>: Whether to use peephole connections or not.</li> </ul>  <h4 id=\"LSTMFusedCell.output_size\"><code>tf.contrib.rnn.LSTMFusedCell.output_size</code></h4>  <h4 id=\"LSTMFusedCell.state_size\"><code>tf.contrib.rnn.LSTMFusedCell.state_size</code></h4>  <h4 id=\"LSTMFusedCell.zero_state\"><code>tf.contrib.rnn.LSTMFusedCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"lstm-like-cells\">LSTM-like cells</h3>  <h3 id=\"CoupledInputForgetGateLSTMCell\"><code>class tf.contrib.rnn.CoupledInputForgetGateLSTMCell</code></h3> <p>Long short-term memory unit (LSTM) recurrent network cell.</p> <p>The default non-peephole implementation is based on:</p> <p><a target=\"_blank\" href=\"https://www.google.com/url?q=http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf&amp;usg=AFQjCNFzIBcb7U6rkXHAUGBqzkT4YWuNGw\">http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf</a></p> <p>S. Hochreiter and J. Schmidhuber. \"Long Short-Term Memory\". Neural Computation, 9(8):1735-1780, 1997.</p> <p>The peephole implementation is based on:</p> <p><a target=\"_blank\" href=\"https://research.google.com/pubs/archive/43905.pdf\">https://research.google.com/pubs/archive/43905.pdf</a></p> <p>Hasim Sak, Andrew Senior, and Francoise Beaufays. \"Long short-term memory recurrent neural network architectures for large scale acoustic modeling.\" INTERSPEECH, 2014.</p> <p>The coupling of input and forget gate is based on:</p> <p><a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/pdf/1503.04069.pdf&amp;usg=AFQjCNH7pLxWO4_YUgG_87jM1ECZz9Z-1Q\">http://arxiv.org/pdf/1503.04069.pdf</a></p> <p>Greff et al. \"LSTM: A Search Space Odyssey\"</p> <p>The class uses optional peep-hole connections, and an optional projection layer.</p>  <h4 id=\"CoupledInputForgetGateLSTMCell.__init__\"><code>tf.contrib.rnn.CoupledInputForgetGateLSTMCell.__init__(num_units, use_peepholes=False, initializer=None, num_proj=None, proj_clip=None, num_unit_shards=1, num_proj_shards=1, forget_bias=1.0, state_is_tuple=False, activation=tanh)</code></h4> <p>Initialize the parameters for an LSTM cell.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>num_units</code>: int, The number of units in the LSTM cell</li> <li>\n<code>use_peepholes</code>: bool, set True to enable diagonal/peephole connections.</li> <li>\n<code>initializer</code>: (optional) The initializer to use for the weight and projection matrices.</li> <li>\n<code>num_proj</code>: (optional) int, The output dimensionality for the projection matrices. If None, no projection is performed.</li> <li><p><code>proj_clip</code>: (optional) A float value. If <code>num_proj &gt; 0</code> and <code>proj_clip</code> is provided, then the projected values are clipped elementwise to within <code>[-proj_clip, proj_clip]</code>.</p></li> <li><p><code>num_unit_shards</code>: How to split the weight matrix. If &gt;1, the weight matrix is stored across num_unit_shards.</p></li> <li><p><code>num_proj_shards</code>: How to split the projection matrix. If &gt;1, the projection matrix is stored across num_proj_shards.</p></li> <li><p><code>forget_bias</code>: Biases of the forget gate are initialized by default to 1 in order to reduce the scale of forgetting at the beginning of the training.</p></li> <li><p><code>state_is_tuple</code>: If True, accepted and returned states are 2-tuples of the <code>c_state</code> and <code>m_state</code>. By default (False), they are concatenated along the column axis. This default behavior will soon be deprecated.</p></li> <li><p><code>activation</code>: Activation function of the inner states.</p></li> </ul>  <h4 id=\"CoupledInputForgetGateLSTMCell.output_size\"><code>tf.contrib.rnn.CoupledInputForgetGateLSTMCell.output_size</code></h4>  <h4 id=\"CoupledInputForgetGateLSTMCell.state_size\"><code>tf.contrib.rnn.CoupledInputForgetGateLSTMCell.state_size</code></h4>  <h4 id=\"CoupledInputForgetGateLSTMCell.zero_state\"><code>tf.contrib.rnn.CoupledInputForgetGateLSTMCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"TimeFreqLSTMCell\"><code>class tf.contrib.rnn.TimeFreqLSTMCell</code></h3> <p>Time-Frequency Long short-term memory unit (LSTM) recurrent network cell.</p> <p>This implementation is based on:</p> <p>Tara N. Sainath and Bo Li \"Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures for LVCSR Tasks.\" submitted to INTERSPEECH, 2016.</p> <p>It uses peep-hole connections and optional cell clipping.</p>  <h4 id=\"TimeFreqLSTMCell.__init__\"><code>tf.contrib.rnn.TimeFreqLSTMCell.__init__(num_units, use_peepholes=False, cell_clip=None, initializer=None, num_unit_shards=1, forget_bias=1.0, feature_size=None, frequency_skip=None)</code></h4> <p>Initialize the parameters for an LSTM cell.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>num_units</code>: int, The number of units in the LSTM cell</li> <li>\n<code>use_peepholes</code>: bool, set True to enable diagonal/peephole connections.</li> <li>\n<code>cell_clip</code>: (optional) A float value, if provided the cell state is clipped by this value prior to the cell output activation.</li> <li>\n<code>initializer</code>: (optional) The initializer to use for the weight and projection matrices.</li> <li>\n<code>num_unit_shards</code>: int, How to split the weight matrix. If &gt;1, the weight matrix is stored across num_unit_shards.</li> <li>\n<code>forget_bias</code>: float, Biases of the forget gate are initialized by default to 1 in order to reduce the scale of forgetting at the beginning of the training.</li> <li>\n<code>feature_size</code>: int, The size of the input feature the LSTM spans over.</li> <li>\n<code>frequency_skip</code>: int, The amount the LSTM filter is shifted by in frequency.</li> </ul>  <h4 id=\"TimeFreqLSTMCell.output_size\"><code>tf.contrib.rnn.TimeFreqLSTMCell.output_size</code></h4>  <h4 id=\"TimeFreqLSTMCell.state_size\"><code>tf.contrib.rnn.TimeFreqLSTMCell.state_size</code></h4>  <h4 id=\"TimeFreqLSTMCell.zero_state\"><code>tf.contrib.rnn.TimeFreqLSTMCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"GridLSTMCell\"><code>class tf.contrib.rnn.GridLSTMCell</code></h3> <p>Grid Long short-term memory unit (LSTM) recurrent network cell.</p> <p>The default is based on: Nal Kalchbrenner, Ivo Danihelka and Alex Graves \"Grid Long Short-Term Memory,\" Proc. ICLR 2016. <a target=\"_blank\" href=\"https://www.google.com/url?q=http://arxiv.org/abs/1507.01526&amp;usg=AFQjCNHYhSVHOfRcniRKx38oP4xuSGdz3A\">http://arxiv.org/abs/1507.01526</a></p> <p>When peephole connections are used, the implementation is based on: Tara N. Sainath and Bo Li \"Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures for LVCSR Tasks.\" submitted to INTERSPEECH, 2016.</p> <p>The code uses optional peephole connections, shared_weights and cell clipping.</p>  <h4 id=\"GridLSTMCell.__init__\"><code>tf.contrib.rnn.GridLSTMCell.__init__(num_units, use_peepholes=False, share_time_frequency_weights=False, cell_clip=None, initializer=None, num_unit_shards=1, forget_bias=1.0, feature_size=None, frequency_skip=None)</code></h4> <p>Initialize the parameters for an LSTM cell.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>num_units</code>: int, The number of units in the LSTM cell</li> <li>\n<code>use_peepholes</code>: bool, default False. Set True to enable diagonal/peephole connections.</li> <li>\n<code>share_time_frequency_weights</code>: bool, default False. Set True to enable shared cell weights between time and frequency LSTMs.</li> <li>\n<code>cell_clip</code>: (optional) A float value, if provided the cell state is clipped by this value prior to the cell output activation.</li> <li>\n<code>initializer</code>: (optional) The initializer to use for the weight and projection matrices.</li> <li>\n<code>num_unit_shards</code>: int, How to split the weight matrix. If &gt;1, the weight matrix is stored across num_unit_shards.</li> <li>\n<code>forget_bias</code>: float, Biases of the forget gate are initialized by default to 1 in order to reduce the scale of forgetting at the beginning of the training.</li> <li>\n<code>feature_size</code>: int, The size of the input feature the LSTM spans over.</li> <li>\n<code>frequency_skip</code>: int, The amount the LSTM filter is shifted by in frequency.</li> </ul>  <h4 id=\"GridLSTMCell.output_size\"><code>tf.contrib.rnn.GridLSTMCell.output_size</code></h4>  <h4 id=\"GridLSTMCell.state_size\"><code>tf.contrib.rnn.GridLSTMCell.state_size</code></h4>  <h4 id=\"GridLSTMCell.zero_state\"><code>tf.contrib.rnn.GridLSTMCell.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>  <h3 id=\"rnncell-wrappers\">RNNCell wrappers</h3>  <h3 id=\"AttentionCellWrapper\"><code>class tf.contrib.rnn.AttentionCellWrapper</code></h3> <p>Basic attention cell wrapper.</p> <p>Implementation based on <a target=\"_blank\" href=\"https://www.google.com/url?q=https://arxiv.org/pdf/1601.06733.pdf&amp;usg=AFQjCNFBlZuConWU6_WHgXXoxt3Co-_WXw\">https://arxiv.org/pdf/1601.06733.pdf</a>.</p>  <h4 id=\"AttentionCellWrapper.__init__\"><code>tf.contrib.rnn.AttentionCellWrapper.__init__(cell, attn_length, attn_size=None, attn_vec_size=None, input_size=None, state_is_tuple=False)</code></h4> <p>Create a cell with attention.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>cell</code>: an RNNCell, an attention is added to it.</li> <li>\n<code>attn_length</code>: integer, the size of an attention window.</li> <li>\n<code>attn_size</code>: integer, the size of an attention vector. Equal to cell.output_size by default.</li> <li>\n<code>attn_vec_size</code>: integer, the number of convolutional features calculated on attention state and a size of the hidden layer built from base cell state. Equal attn_size to by default.</li> <li>\n<code>input_size</code>: integer, the size of a hidden linear layer, built from inputs and attention. Derived from the input tensor by default.</li> <li>\n<code>state_is_tuple</code>: If True, accepted and returned states are n-tuples, where <code>n = len(cells)</code>. By default (False), the states are all concatenated along the column axis.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if cell is not an RNNCell.</li> <li>\n<code>ValueError</code>: if cell returns a state tuple but the flag <code>state_is_tuple</code> is <code>False</code> or if attn_length is zero or less.</li> </ul>  <h4 id=\"AttentionCellWrapper.output_size\"><code>tf.contrib.rnn.AttentionCellWrapper.output_size</code></h4>  <h4 id=\"AttentionCellWrapper.state_size\"><code>tf.contrib.rnn.AttentionCellWrapper.state_size</code></h4>  <h4 id=\"AttentionCellWrapper.zero_state\"><code>tf.contrib.rnn.AttentionCellWrapper.zero_state(batch_size, dtype)</code></h4> <p>Return zero-filled state tensor(s).</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li> <li>\n<code>dtype</code>: the data type to use for the state.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p> <p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.rnn.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.rnn.html</a>\n  </p>\n</div>\n","contrib.copy_graph":"<h1 id=\"copying-graph-elements-contrib\">Copying Graph Elements (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#copying-graph-elements-contrib\">Copying Graph Elements (contrib)</a></li> <ul><ul> <li><a href=\"#copy_op_to_graph\"><code>tf.contrib.copy_graph.copy_op_to_graph(org_instance, to_graph, variables, scope=)</code></a></li> <li><a href=\"#copy_variable_to_graph\"><code>tf.contrib.copy_graph.copy_variable_to_graph(org_instance, to_graph, scope=)</code></a></li> <li><a href=\"#get_copied_op\"><code>tf.contrib.copy_graph.get_copied_op(org_instance, graph, scope=)</code></a></li> </ul></ul>\n</ul> </div> <p>Functions for copying elements from one graph to another.</p>  <h3 id=\"copy_op_to_graph\"><code>tf.contrib.copy_graph.copy_op_to_graph(org_instance, to_graph, variables, scope='')</code></h3> <p>Given an <code>Operation</code> 'org_instance<code>from one</code>Graph<code>,\ninitializes and returns a copy of it from another</code>Graph<code>,\nunder the specified scope (default</code>\"\"`).</p> <p>The copying is done recursively, so any <code>Operation</code> whose output is required to evaluate the <code>org_instance</code>, is also copied (unless already done).</p> <p>Since <code>Variable</code> instances are copied separately, those required to evaluate <code>org_instance</code> must be provided as input.</p> <p>Args: org_instance: An <code>Operation</code> from some <code>Graph</code>. Could be a <code>Placeholder</code> as well. to_graph: The <code>Graph</code> to copy <code>org_instance</code> to. variables: An iterable of <code>Variable</code> instances to copy <code>org_instance</code> to. scope: A scope for the new <code>Variable</code> (default <code>\"\"</code>).</p> <h5 id=\"returns\">Returns:</h5> <pre class=\"\">The copied `Operation` from `to_graph`.\n</pre> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>org_instance</code> is not an <code>Operation</code> or <code>Tensor</code>.</li> </ul>  <h3 id=\"copy_variable_to_graph\"><code>tf.contrib.copy_graph.copy_variable_to_graph(org_instance, to_graph, scope='')</code></h3> <p>Given a <code>Variable</code> instance from one <code>Graph</code>, initializes and returns a copy of it from another <code>Graph</code>, under the specified scope (default <code>\"\"</code>).</p> <p>Args: org_instance: A <code>Variable</code> from some <code>Graph</code>. to_graph: The <code>Graph</code> to copy the <code>Variable</code> to. scope: A scope for the new <code>Variable</code> (default <code>\"\"</code>).</p>  <h5 id=\"returns-2\">Returns:</h5> <pre class=\"\">The copied `Variable` from `to_graph`.\n</pre>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>org_instance</code> is not a <code>Variable</code>.</li> </ul>  <h3 id=\"get_copied_op\"><code>tf.contrib.copy_graph.get_copied_op(org_instance, graph, scope='')</code></h3> <p>Given an <code>Operation</code> instance from some <code>Graph</code>, returns its namesake from <code>graph</code>, under the specified scope (default <code>\"\"</code>).</p> <p>If a copy of <code>org_instance</code> is present in <code>graph</code> under the given <code>scope</code>, it will be returned.</p> <p>Args: org_instance: An <code>Operation</code> from some <code>Graph</code>. graph: The <code>Graph</code> to be searched for a copr of <code>org_instance</code>. scope: The scope <code>org_instance</code> is present in.</p>  <h5 id=\"returns-3\">Returns:</h5> <pre class=\"\">The `Operation` copy from `graph`.\n</pre><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.copy_graph.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.copy_graph.html</a>\n  </p>\n</div>\n","contrib.learn.monitors":"<h1 id=\"monitors-contrib\">Monitors (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#monitors-contrib\">Monitors (contrib)</a></li> <ul>\n<ul> <li><a href=\"#get_default_monitors\"><code>tf.contrib.learn.monitors.get_default_monitors(loss_op=None, summary_op=None, save_summary_steps=100, output_dir=None, summary_writer=None)</code></a></li> <li><a href=\"#BaseMonitor\"><code>class tf.contrib.learn.monitors.BaseMonitor</code></a></li> <li><a href=\"#CaptureVariable\"><code>class tf.contrib.learn.monitors.CaptureVariable</code></a></li> <li><a href=\"#CheckpointSaver\"><code>class tf.contrib.learn.monitors.CheckpointSaver</code></a></li> <li><a href=\"#EveryN\"><code>class tf.contrib.learn.monitors.EveryN</code></a></li> <li><a href=\"#ExportMonitor\"><code>class tf.contrib.learn.monitors.ExportMonitor</code></a></li> <li><a href=\"#GraphDump\"><code>class tf.contrib.learn.monitors.GraphDump</code></a></li> <li><a href=\"#LoggingTrainable\"><code>class tf.contrib.learn.monitors.LoggingTrainable</code></a></li> <li><a href=\"#NanLoss\"><code>class tf.contrib.learn.monitors.NanLoss</code></a></li> <li><a href=\"#PrintTensor\"><code>class tf.contrib.learn.monitors.PrintTensor</code></a></li> <li><a href=\"#StepCounter\"><code>class tf.contrib.learn.monitors.StepCounter</code></a></li> <li><a href=\"#StopAtStep\"><code>class tf.contrib.learn.monitors.StopAtStep</code></a></li> <li><a href=\"#SummarySaver\"><code>class tf.contrib.learn.monitors.SummarySaver</code></a></li> <li><a href=\"#ValidationMonitor\"><code>class tf.contrib.learn.monitors.ValidationMonitor</code></a></li> </ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#SummaryWriterCache\"><code>class tf.contrib.learn.monitors.SummaryWriterCache</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Monitors allow user instrumentation of the training process.</p> <p>Monitors are useful to track training, report progress, request early stopping and more. Monitors use the observer pattern and notify at the following points: - when training begins - before a training step - after a training step - when training ends</p> <p>Monitors are not intended to be reusable.</p> <p>There are a few pre-defined monitors: - CaptureVariable: saves a variable's values - GraphDump: intended for debug only - saves all tensor values - PrintTensor: outputs one or more tensor values to log - SummarySaver: saves summaries to a summary writer - ValidationMonitor: runs model validation, by periodically calculating eval metrics on a separate data set; supports optional early stopping</p> <p>For more specific needs, you can create custom monitors by extending one of the following classes: - BaseMonitor: the base class for all monitors - EveryN: triggers a callback every N training steps</p> <p>Example:</p> <p>class ExampleMonitor(monitors.BaseMonitor): def <strong>init</strong>(self): print 'Init'</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def begin(self, max_steps):\n  print 'Starting run. Will train until step %d.' % max_steps\n\ndef end(self):\n  print 'Completed run.'\n\ndef step_begin(self, step):\n  print 'About to run step %d...' % step\n  return ['loss_1:0']\n\ndef step_end(self, step, outputs):\n  print 'Done running step %d. The value of \"loss\" tensor: %s' % (\n    step, outputs['loss_1:0'])\n</pre> <p>linear_regressor = LinearRegressor() example_monitor = ExampleMonitor() linear_regressor.fit( x, y, steps=2, batch_size=1, monitors=[example_monitor])</p>  <h3 id=\"get_default_monitors\"><code>tf.contrib.learn.monitors.get_default_monitors(loss_op=None, summary_op=None, save_summary_steps=100, output_dir=None, summary_writer=None)</code></h3> <p>Returns a default set of typically-used monitors.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>loss_op</code>: <code>Tensor</code>, the loss tensor. This will be printed using <code>PrintTensor</code> at the default interval.</li> <li>\n<code>summary_op</code>: See <code>SummarySaver</code>.</li> <li>\n<code>save_summary_steps</code>: See <code>SummarySaver</code>.</li> <li>\n<code>output_dir</code>: See <code>SummarySaver</code>.</li> <li>\n<code>summary_writer</code>: See <code>SummarySaver</code>.</li> </ul> <h5 id=\"returns\">Returns:</h5> <p><code>list</code> of monitors.</p>  <h3 id=\"BaseMonitor\"><code>class tf.contrib.learn.monitors.BaseMonitor</code></h3> <p>Base class for Monitors.</p> <p>Defines basic interfaces of Monitors. Monitors can either be run on all workers or, more commonly, restricted to run exclusively on the elected chief worker.</p>  <h4 id=\"BaseMonitor.__init__\"><code>tf.contrib.learn.monitors.BaseMonitor.__init__()</code></h4>  <h4 id=\"BaseMonitor.begin\"><code>tf.contrib.learn.monitors.BaseMonitor.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"BaseMonitor.end\"><code>tf.contrib.learn.monitors.BaseMonitor.end(session=None)</code></h4> <p>Callback at the end of training/evaluation.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>session</code>: A <code>tf.Session</code> object that can be used to run ops.</li> </ul>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun a run.</li> </ul>  <h4 id=\"BaseMonitor.epoch_begin\"><code>tf.contrib.learn.monitors.BaseMonitor.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"BaseMonitor.epoch_end\"><code>tf.contrib.learn.monitors.BaseMonitor.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"BaseMonitor.post_step\"><code>tf.contrib.learn.monitors.BaseMonitor.post_step(step, session)</code></h4> <p>Callback after the step is finished.</p> <p>Called after step_end and receives session to perform extra session.run calls. If failure occurred in the process, will be called as well.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, global step of the model.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"BaseMonitor.run_on_all_workers\"><code>tf.contrib.learn.monitors.BaseMonitor.run_on_all_workers</code></h4>  <h4 id=\"BaseMonitor.set_estimator\"><code>tf.contrib.learn.monitors.BaseMonitor.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"BaseMonitor.step_begin\"><code>tf.contrib.learn.monitors.BaseMonitor.step_begin(step)</code></h4> <p>Callback before training step begins.</p> <p>You may use this callback to request evaluation of additional tensors in the graph.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p>List of <code>Tensor</code> objects or string tensor names to be run.</p>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a step, or <code>step</code> &lt; 0, or <code>step</code> &gt; <code>max_steps</code>.</li> </ul>  <h4 id=\"BaseMonitor.step_end\"><code>tf.contrib.learn.monitors.BaseMonitor.step_end(step, output)</code></h4> <p>Callback after training step finished.</p> <p>This callback provides access to the tensors/ops evaluated at this step, including the additional tensors for which evaluation was requested in <code>step_begin</code>.</p> <p>In addition, the callback has the opportunity to stop training by returning <code>True</code>. This is useful for early stopping, for example.</p> <p>Note that this method is not called if the call to <code>Session.run()</code> that followed the last call to <code>step_begin()</code> failed.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p><code>bool</code>. True if training should stop.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun a step, or <code>step</code> number does not match.</li> </ul>  <h3 id=\"CaptureVariable\"><code>class tf.contrib.learn.monitors.CaptureVariable</code></h3> <p>Captures a variable's values into a collection.</p> <p>This monitor is useful for unit testing. You should exercise caution when using this monitor in production, since it never discards values.</p> <p>This is an <code>EveryN</code> monitor and has consistent semantic for <code>every_n</code> and <code>first_n</code>.</p>  <h4 id=\"CaptureVariable.__init__\"><code>tf.contrib.learn.monitors.CaptureVariable.__init__(var_name, every_n=100, first_n=1)</code></h4> <p>Initializes a CaptureVariable monitor.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>var_name</code>: <code>string</code>. The variable name, including suffix (typically \":0\").</li> <li>\n<code>every_n</code>: <code>int</code>, print every N steps. See <code>PrintN.</code>\n</li> <li>\n<code>first_n</code>: <code>int</code>, also print the first N steps. See <code>PrintN.</code>\n</li> </ul>  <h4 id=\"CaptureVariable.begin\"><code>tf.contrib.learn.monitors.CaptureVariable.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"CaptureVariable.end\"><code>tf.contrib.learn.monitors.CaptureVariable.end(session=None)</code></h4>  <h4 id=\"CaptureVariable.epoch_begin\"><code>tf.contrib.learn.monitors.CaptureVariable.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"CaptureVariable.epoch_end\"><code>tf.contrib.learn.monitors.CaptureVariable.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"CaptureVariable.every_n_post_step\"><code>tf.contrib.learn.monitors.CaptureVariable.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"CaptureVariable.every_n_step_begin\"><code>tf.contrib.learn.monitors.CaptureVariable.every_n_step_begin(step)</code></h4>  <h4 id=\"CaptureVariable.every_n_step_end\"><code>tf.contrib.learn.monitors.CaptureVariable.every_n_step_end(step, outputs)</code></h4>  <h4 id=\"CaptureVariable.post_step\"><code>tf.contrib.learn.monitors.CaptureVariable.post_step(step, session)</code></h4>  <h4 id=\"CaptureVariable.run_on_all_workers\"><code>tf.contrib.learn.monitors.CaptureVariable.run_on_all_workers</code></h4>  <h4 id=\"CaptureVariable.set_estimator\"><code>tf.contrib.learn.monitors.CaptureVariable.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"CaptureVariable.step_begin\"><code>tf.contrib.learn.monitors.CaptureVariable.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"CaptureVariable.step_end\"><code>tf.contrib.learn.monitors.CaptureVariable.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h4 id=\"CaptureVariable.values\"><code>tf.contrib.learn.monitors.CaptureVariable.values</code></h4> <p>Returns the values captured so far.</p>  <h5 id=\"returns-6\">Returns:</h5> <p><code>dict</code> mapping <code>int</code> step numbers to that values of the variable at the respective step.</p>  <h3 id=\"CheckpointSaver\"><code>class tf.contrib.learn.monitors.CheckpointSaver</code></h3> <p>Saves checkpoints every N steps.</p>  <h4 id=\"CheckpointSaver.__init__\"><code>tf.contrib.learn.monitors.CheckpointSaver.__init__(checkpoint_dir, save_secs=None, save_steps=None, saver=None, checkpoint_basename='model.ckpt', scaffold=None)</code></h4> <p>Initialize CheckpointSaver monitor.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>checkpoint_dir</code>: <code>str</code>, base directory for the checkpoint files.</li> <li>\n<code>save_secs</code>: <code>int</code>, save every N secs.</li> <li>\n<code>save_steps</code>: <code>int</code>, save every N steps.</li> <li>\n<code>saver</code>: <code>Saver</code> object, used for saving.</li> <li>\n<code>checkpoint_basename</code>: <code>str</code>, base name for the checkpoint files.</li> <li>\n<code>scaffold</code>: <code>Scaffold</code>, use to get saver object.</li> </ul>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If both <code>save_steps</code> and <code>save_secs</code> are not <code>None</code>.</li> <li>\n<code>ValueError</code>: If both <code>save_steps</code> and <code>save_secs</code> are <code>None</code>.</li> </ul>  <h4 id=\"CheckpointSaver.begin\"><code>tf.contrib.learn.monitors.CheckpointSaver.begin(max_steps=None)</code></h4>  <h4 id=\"CheckpointSaver.end\"><code>tf.contrib.learn.monitors.CheckpointSaver.end(session=None)</code></h4>  <h4 id=\"CheckpointSaver.epoch_begin\"><code>tf.contrib.learn.monitors.CheckpointSaver.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"CheckpointSaver.epoch_end\"><code>tf.contrib.learn.monitors.CheckpointSaver.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"CheckpointSaver.post_step\"><code>tf.contrib.learn.monitors.CheckpointSaver.post_step(step, session)</code></h4>  <h4 id=\"CheckpointSaver.run_on_all_workers\"><code>tf.contrib.learn.monitors.CheckpointSaver.run_on_all_workers</code></h4>  <h4 id=\"CheckpointSaver.set_estimator\"><code>tf.contrib.learn.monitors.CheckpointSaver.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-16\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"CheckpointSaver.step_begin\"><code>tf.contrib.learn.monitors.CheckpointSaver.step_begin(step)</code></h4>  <h4 id=\"CheckpointSaver.step_end\"><code>tf.contrib.learn.monitors.CheckpointSaver.step_end(step, output)</code></h4> <p>Callback after training step finished.</p> <p>This callback provides access to the tensors/ops evaluated at this step, including the additional tensors for which evaluation was requested in <code>step_begin</code>.</p> <p>In addition, the callback has the opportunity to stop training by returning <code>True</code>. This is useful for early stopping, for example.</p> <p>Note that this method is not called if the call to <code>Session.run()</code> that followed the last call to <code>step_begin()</code> failed.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p><code>bool</code>. True if training should stop.</p>  <h5 id=\"raises-17\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun a step, or <code>step</code> number does not match.</li> </ul>  <h3 id=\"EveryN\"><code>class tf.contrib.learn.monitors.EveryN</code></h3> <p>Base class for monitors that execute callbacks every N steps.</p> <p>This class adds three new callbacks: - every_n_step_begin - every_n_step_end - every_n_pos_step</p> <p>The callbacks are executed every n steps, or optionally every step for the first m steps, where m and n can both be user-specified.</p> <p>When extending this class, note that if you wish to use any of the <code>BaseMonitor</code> callbacks, you must call their respective super implementation:</p> <p>def step_begin(self, step): super(ExampleMonitor, self).step_begin(step) return []</p> <p>Failing to call the super implementation will cause unpredictible behavior.</p> <p>The <code>every_n_post_step()</code> callback is also called after the last step if it was not already called through the regular conditions. Note that <code>every_n_step_begin()</code> and <code>every_n_step_end()</code> do not receive that special treatment.</p>  <h4 id=\"EveryN.__init__\"><code>tf.contrib.learn.monitors.EveryN.__init__(every_n_steps=100, first_n_steps=1)</code></h4> <p>Initializes an <code>EveryN</code> monitor.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>every_n_steps</code>: <code>int</code>, the number of steps to allow between callbacks.</li> <li>\n<code>first_n_steps</code>: <code>int</code>, specifying the number of initial steps during which the callbacks will always be executed, regardless of the value of <code>every_n_steps</code>. Note that this value is relative to the global step</li> </ul>  <h4 id=\"EveryN.begin\"><code>tf.contrib.learn.monitors.EveryN.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-18\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"EveryN.end\"><code>tf.contrib.learn.monitors.EveryN.end(session=None)</code></h4>  <h4 id=\"EveryN.epoch_begin\"><code>tf.contrib.learn.monitors.EveryN.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-19\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"EveryN.epoch_end\"><code>tf.contrib.learn.monitors.EveryN.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-20\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"EveryN.every_n_post_step\"><code>tf.contrib.learn.monitors.EveryN.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"EveryN.every_n_step_begin\"><code>tf.contrib.learn.monitors.EveryN.every_n_step_begin(step)</code></h4> <p>Callback before every n'th step begins.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <p>A <code>list</code> of tensors that will be evaluated at this step.</p>  <h4 id=\"EveryN.every_n_step_end\"><code>tf.contrib.learn.monitors.EveryN.every_n_step_end(step, outputs)</code></h4> <p>Callback after every n'th step finished.</p> <p>This callback provides access to the tensors/ops evaluated at this step, including the additional tensors for which evaluation was requested in <code>step_begin</code>.</p> <p>In addition, the callback has the opportunity to stop training by returning <code>True</code>. This is useful for early stopping, for example.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>outputs</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p><code>bool</code>. True if training should stop.</p>  <h4 id=\"EveryN.post_step\"><code>tf.contrib.learn.monitors.EveryN.post_step(step, session)</code></h4>  <h4 id=\"EveryN.run_on_all_workers\"><code>tf.contrib.learn.monitors.EveryN.run_on_all_workers</code></h4>  <h4 id=\"EveryN.set_estimator\"><code>tf.contrib.learn.monitors.EveryN.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-21\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"EveryN.step_begin\"><code>tf.contrib.learn.monitors.EveryN.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-22\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"EveryN.step_end\"><code>tf.contrib.learn.monitors.EveryN.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h3 id=\"ExportMonitor\"><code>class tf.contrib.learn.monitors.ExportMonitor</code></h3> <p>Monitor that exports Estimator every N steps.</p>  <h4 id=\"ExportMonitor.__init__\"><code>tf.contrib.learn.monitors.ExportMonitor.__init__(every_n_steps, export_dir, exports_to_keep=5, signature_fn=None, default_batch_size=1)</code></h4> <p>Initializes ExportMonitor.</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>every_n_steps</code>: Run monitor every N steps.</li> <li>\n<code>export_dir</code>: str, folder to export.</li> <li>\n<code>exports_to_keep</code>: int, number of exports to keep.</li> <li>\n<code>signature_fn</code>: Function that given <code>Tensor</code> of <code>Example</code> strings, <code>dict</code> of <code>Tensor</code>s for features and <code>dict</code> of <code>Tensor</code>s for predictions and returns default and named exporting signautres.</li> <li>\n<code>default_batch_size</code>: Default batch size of the <code>Example</code> placeholder.</li> </ul>  <h4 id=\"ExportMonitor.begin\"><code>tf.contrib.learn.monitors.ExportMonitor.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-23\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"ExportMonitor.end\"><code>tf.contrib.learn.monitors.ExportMonitor.end(session=None)</code></h4>  <h4 id=\"ExportMonitor.epoch_begin\"><code>tf.contrib.learn.monitors.ExportMonitor.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-24\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"ExportMonitor.epoch_end\"><code>tf.contrib.learn.monitors.ExportMonitor.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-25\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"ExportMonitor.every_n_post_step\"><code>tf.contrib.learn.monitors.ExportMonitor.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"ExportMonitor.every_n_step_begin\"><code>tf.contrib.learn.monitors.ExportMonitor.every_n_step_begin(step)</code></h4> <p>Callback before every n'th step begins.</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>A <code>list</code> of tensors that will be evaluated at this step.</p>  <h4 id=\"ExportMonitor.every_n_step_end\"><code>tf.contrib.learn.monitors.ExportMonitor.every_n_step_end(step, outputs)</code></h4>  <h4 id=\"ExportMonitor.post_step\"><code>tf.contrib.learn.monitors.ExportMonitor.post_step(step, session)</code></h4>  <h4 id=\"ExportMonitor.run_on_all_workers\"><code>tf.contrib.learn.monitors.ExportMonitor.run_on_all_workers</code></h4>  <h4 id=\"ExportMonitor.set_estimator\"><code>tf.contrib.learn.monitors.ExportMonitor.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-26\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"ExportMonitor.step_begin\"><code>tf.contrib.learn.monitors.ExportMonitor.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-27\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"ExportMonitor.step_end\"><code>tf.contrib.learn.monitors.ExportMonitor.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h3 id=\"GraphDump\"><code>class tf.contrib.learn.monitors.GraphDump</code></h3> <p>Dumps almost all tensors in the graph at every step.</p> <p>Note, this is very expensive, prefer <code>PrintTensor</code> in production.</p>  <h4 id=\"GraphDump.__init__\"><code>tf.contrib.learn.monitors.GraphDump.__init__(ignore_ops=None)</code></h4> <p>Initializes GraphDump monitor.</p>  <h5 id=\"args-42\">Args:</h5> <ul> <li>\n<code>ignore_ops</code>: <code>list</code> of <code>string</code>. Names of ops to ignore. If None, <code>GraphDump.IGNORE_OPS</code> is used.</li> </ul>  <h4 id=\"GraphDump.begin\"><code>tf.contrib.learn.monitors.GraphDump.begin(max_steps=None)</code></h4>  <h4 id=\"GraphDump.compare\"><code>tf.contrib.learn.monitors.GraphDump.compare(other_dump, step, atol=1e-06)</code></h4> <p>Compares two <code>GraphDump</code> monitors and returns differences.</p>  <h5 id=\"args-43\">Args:</h5> <ul> <li>\n<code>other_dump</code>: Another <code>GraphDump</code> monitor.</li> <li>\n<code>step</code>: <code>int</code>, step to compare on.</li> <li>\n<code>atol</code>: <code>float</code>, absolute tolerance in comparison of floating arrays.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>Returns tuple:</p> <ul> <li>\n<code>matched</code>: <code>list</code> of keys that matched.</li> <li>\n<code>non_matched</code>: <code>dict</code> of keys to tuple of 2 mismatched values.</li> </ul>  <h5 id=\"raises-28\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if a key in <code>data</code> is missing from <code>other_dump</code> at <code>step</code>.</li> </ul>  <h4 id=\"GraphDump.data\"><code>tf.contrib.learn.monitors.GraphDump.data</code></h4>  <h4 id=\"GraphDump.end\"><code>tf.contrib.learn.monitors.GraphDump.end(session=None)</code></h4> <p>Callback at the end of training/evaluation.</p>  <h5 id=\"args-44\">Args:</h5> <ul> <li>\n<code>session</code>: A <code>tf.Session</code> object that can be used to run ops.</li> </ul>  <h5 id=\"raises-29\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun a run.</li> </ul>  <h4 id=\"GraphDump.epoch_begin\"><code>tf.contrib.learn.monitors.GraphDump.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-45\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-30\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"GraphDump.epoch_end\"><code>tf.contrib.learn.monitors.GraphDump.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-46\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-31\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"GraphDump.post_step\"><code>tf.contrib.learn.monitors.GraphDump.post_step(step, session)</code></h4> <p>Callback after the step is finished.</p> <p>Called after step_end and receives session to perform extra session.run calls. If failure occurred in the process, will be called as well.</p>  <h5 id=\"args-47\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, global step of the model.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"GraphDump.run_on_all_workers\"><code>tf.contrib.learn.monitors.GraphDump.run_on_all_workers</code></h4>  <h4 id=\"GraphDump.set_estimator\"><code>tf.contrib.learn.monitors.GraphDump.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-48\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-32\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"GraphDump.step_begin\"><code>tf.contrib.learn.monitors.GraphDump.step_begin(step)</code></h4>  <h4 id=\"GraphDump.step_end\"><code>tf.contrib.learn.monitors.GraphDump.step_end(step, output)</code></h4>  <h3 id=\"LoggingTrainable\"><code>class tf.contrib.learn.monitors.LoggingTrainable</code></h3> <p>Writes trainable variable values into log every N steps.</p> <p>Write the tensors in trainable variables <code>every_n</code> steps, starting with the <code>first_n</code>th step.</p>  <h4 id=\"LoggingTrainable.__init__\"><code>tf.contrib.learn.monitors.LoggingTrainable.__init__(scope=None, every_n=100, first_n=1)</code></h4> <p>Initializes LoggingTrainable monitor.</p>  <h5 id=\"args-49\">Args:</h5> <ul> <li>\n<code>scope</code>: An optional string to match variable names using re.match.</li> <li>\n<code>every_n</code>: Print every N steps.</li> <li>\n<code>first_n</code>: Print first N steps.</li> </ul>  <h4 id=\"LoggingTrainable.begin\"><code>tf.contrib.learn.monitors.LoggingTrainable.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-50\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-33\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"LoggingTrainable.end\"><code>tf.contrib.learn.monitors.LoggingTrainable.end(session=None)</code></h4>  <h4 id=\"LoggingTrainable.epoch_begin\"><code>tf.contrib.learn.monitors.LoggingTrainable.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-51\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-34\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"LoggingTrainable.epoch_end\"><code>tf.contrib.learn.monitors.LoggingTrainable.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-52\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-35\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"LoggingTrainable.every_n_post_step\"><code>tf.contrib.learn.monitors.LoggingTrainable.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-53\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"LoggingTrainable.every_n_step_begin\"><code>tf.contrib.learn.monitors.LoggingTrainable.every_n_step_begin(step)</code></h4>  <h4 id=\"LoggingTrainable.every_n_step_end\"><code>tf.contrib.learn.monitors.LoggingTrainable.every_n_step_end(step, outputs)</code></h4>  <h4 id=\"LoggingTrainable.post_step\"><code>tf.contrib.learn.monitors.LoggingTrainable.post_step(step, session)</code></h4>  <h4 id=\"LoggingTrainable.run_on_all_workers\"><code>tf.contrib.learn.monitors.LoggingTrainable.run_on_all_workers</code></h4>  <h4 id=\"LoggingTrainable.set_estimator\"><code>tf.contrib.learn.monitors.LoggingTrainable.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-54\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-36\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"LoggingTrainable.step_begin\"><code>tf.contrib.learn.monitors.LoggingTrainable.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-55\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-37\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"LoggingTrainable.step_end\"><code>tf.contrib.learn.monitors.LoggingTrainable.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-56\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h3 id=\"NanLoss\"><code>class tf.contrib.learn.monitors.NanLoss</code></h3> <p>NaN Loss monitor.</p> <p>Monitors loss and stops training if loss is NaN. Can either fail with exception or just stop training.</p>  <h4 id=\"NanLoss.__init__\"><code>tf.contrib.learn.monitors.NanLoss.__init__(loss_tensor, every_n_steps=100, fail_on_nan_loss=True)</code></h4> <p>Initializes NanLoss monitor.</p>  <h5 id=\"args-57\">Args:</h5> <ul> <li>\n<code>loss_tensor</code>: <code>Tensor</code>, the loss tensor.</li> <li>\n<code>every_n_steps</code>: <code>int</code>, run check every this many steps.</li> <li>\n<code>fail_on_nan_loss</code>: <code>bool</code>, whether to raise exception when loss is NaN.</li> </ul>  <h4 id=\"NanLoss.begin\"><code>tf.contrib.learn.monitors.NanLoss.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-58\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-38\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"NanLoss.end\"><code>tf.contrib.learn.monitors.NanLoss.end(session=None)</code></h4>  <h4 id=\"NanLoss.epoch_begin\"><code>tf.contrib.learn.monitors.NanLoss.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-59\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-39\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"NanLoss.epoch_end\"><code>tf.contrib.learn.monitors.NanLoss.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-60\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-40\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"NanLoss.every_n_post_step\"><code>tf.contrib.learn.monitors.NanLoss.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-61\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"NanLoss.every_n_step_begin\"><code>tf.contrib.learn.monitors.NanLoss.every_n_step_begin(step)</code></h4>  <h4 id=\"NanLoss.every_n_step_end\"><code>tf.contrib.learn.monitors.NanLoss.every_n_step_end(step, outputs)</code></h4>  <h4 id=\"NanLoss.post_step\"><code>tf.contrib.learn.monitors.NanLoss.post_step(step, session)</code></h4>  <h4 id=\"NanLoss.run_on_all_workers\"><code>tf.contrib.learn.monitors.NanLoss.run_on_all_workers</code></h4>  <h4 id=\"NanLoss.set_estimator\"><code>tf.contrib.learn.monitors.NanLoss.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-62\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-41\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"NanLoss.step_begin\"><code>tf.contrib.learn.monitors.NanLoss.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-63\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-42\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"NanLoss.step_end\"><code>tf.contrib.learn.monitors.NanLoss.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-64\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h3 id=\"PrintTensor\"><code>class tf.contrib.learn.monitors.PrintTensor</code></h3> <p>Prints given tensors every N steps.</p> <p>This is an <code>EveryN</code> monitor and has consistent semantic for <code>every_n</code> and <code>first_n</code>.</p> <p>The tensors will be printed to the log, with <code>INFO</code> severity.</p>  <h4 id=\"PrintTensor.__init__\"><code>tf.contrib.learn.monitors.PrintTensor.__init__(tensor_names, every_n=100, first_n=1)</code></h4> <p>Initializes a PrintTensor monitor.</p>  <h5 id=\"args-65\">Args:</h5> <ul> <li>\n<code>tensor_names</code>: <code>dict</code> of tag to tensor names or <code>iterable</code> of tensor names (strings).</li> <li>\n<code>every_n</code>: <code>int</code>, print every N steps. See <code>PrintN.</code>\n</li> <li>\n<code>first_n</code>: <code>int</code>, also print the first N steps. See <code>PrintN.</code>\n</li> </ul>  <h4 id=\"PrintTensor.begin\"><code>tf.contrib.learn.monitors.PrintTensor.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-66\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-43\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"PrintTensor.end\"><code>tf.contrib.learn.monitors.PrintTensor.end(session=None)</code></h4>  <h4 id=\"PrintTensor.epoch_begin\"><code>tf.contrib.learn.monitors.PrintTensor.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-67\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-44\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"PrintTensor.epoch_end\"><code>tf.contrib.learn.monitors.PrintTensor.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-68\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-45\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"PrintTensor.every_n_post_step\"><code>tf.contrib.learn.monitors.PrintTensor.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-69\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"PrintTensor.every_n_step_begin\"><code>tf.contrib.learn.monitors.PrintTensor.every_n_step_begin(step)</code></h4>  <h4 id=\"PrintTensor.every_n_step_end\"><code>tf.contrib.learn.monitors.PrintTensor.every_n_step_end(step, outputs)</code></h4>  <h4 id=\"PrintTensor.post_step\"><code>tf.contrib.learn.monitors.PrintTensor.post_step(step, session)</code></h4>  <h4 id=\"PrintTensor.run_on_all_workers\"><code>tf.contrib.learn.monitors.PrintTensor.run_on_all_workers</code></h4>  <h4 id=\"PrintTensor.set_estimator\"><code>tf.contrib.learn.monitors.PrintTensor.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-70\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-46\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"PrintTensor.step_begin\"><code>tf.contrib.learn.monitors.PrintTensor.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-71\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-47\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"PrintTensor.step_end\"><code>tf.contrib.learn.monitors.PrintTensor.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-72\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h3 id=\"StepCounter\"><code>class tf.contrib.learn.monitors.StepCounter</code></h3> <p>Steps per second monitor.</p>  <h4 id=\"StepCounter.__init__\"><code>tf.contrib.learn.monitors.StepCounter.__init__(every_n_steps=100, output_dir=None, summary_writer=None)</code></h4>  <h4 id=\"StepCounter.begin\"><code>tf.contrib.learn.monitors.StepCounter.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-73\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-48\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"StepCounter.end\"><code>tf.contrib.learn.monitors.StepCounter.end(session=None)</code></h4>  <h4 id=\"StepCounter.epoch_begin\"><code>tf.contrib.learn.monitors.StepCounter.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-74\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-49\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"StepCounter.epoch_end\"><code>tf.contrib.learn.monitors.StepCounter.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-75\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-50\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"StepCounter.every_n_post_step\"><code>tf.contrib.learn.monitors.StepCounter.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-76\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"StepCounter.every_n_step_begin\"><code>tf.contrib.learn.monitors.StepCounter.every_n_step_begin(step)</code></h4> <p>Callback before every n'th step begins.</p>  <h5 id=\"args-77\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>A <code>list</code> of tensors that will be evaluated at this step.</p>  <h4 id=\"StepCounter.every_n_step_end\"><code>tf.contrib.learn.monitors.StepCounter.every_n_step_end(current_step, outputs)</code></h4>  <h4 id=\"StepCounter.post_step\"><code>tf.contrib.learn.monitors.StepCounter.post_step(step, session)</code></h4>  <h4 id=\"StepCounter.run_on_all_workers\"><code>tf.contrib.learn.monitors.StepCounter.run_on_all_workers</code></h4>  <h4 id=\"StepCounter.set_estimator\"><code>tf.contrib.learn.monitors.StepCounter.set_estimator(estimator)</code></h4>  <h4 id=\"StepCounter.step_begin\"><code>tf.contrib.learn.monitors.StepCounter.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-78\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-51\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"StepCounter.step_end\"><code>tf.contrib.learn.monitors.StepCounter.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-79\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h3 id=\"StopAtStep\"><code>class tf.contrib.learn.monitors.StopAtStep</code></h3> <p>Monitor to request stop at a specified step.</p>  <h4 id=\"StopAtStep.__init__\"><code>tf.contrib.learn.monitors.StopAtStep.__init__(num_steps=None, last_step=None)</code></h4> <p>Create a StopAtStep monitor.</p> <p>This monitor requests stop after either a number of steps have been executed or a last step has been reached. Only of the two options can be specified.</p> <p>if <code>num_steps</code> is specified, it indicates the number of steps to execute after <code>begin()</code> is called. If instead <code>last_step</code> is specified, it indicates the last step we want to execute, as passed to the <code>step_begin()</code> call.</p>  <h5 id=\"args-80\">Args:</h5> <ul> <li>\n<code>num_steps</code>: Number of steps to execute.</li> <li>\n<code>last_step</code>: Step after which to stop.</li> </ul>  <h5 id=\"raises-52\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If one of the arguments is invalid.</li> </ul>  <h4 id=\"StopAtStep.begin\"><code>tf.contrib.learn.monitors.StopAtStep.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-81\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-53\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"StopAtStep.end\"><code>tf.contrib.learn.monitors.StopAtStep.end(session=None)</code></h4> <p>Callback at the end of training/evaluation.</p>  <h5 id=\"args-82\">Args:</h5> <ul> <li>\n<code>session</code>: A <code>tf.Session</code> object that can be used to run ops.</li> </ul>  <h5 id=\"raises-54\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun a run.</li> </ul>  <h4 id=\"StopAtStep.epoch_begin\"><code>tf.contrib.learn.monitors.StopAtStep.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-83\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-55\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"StopAtStep.epoch_end\"><code>tf.contrib.learn.monitors.StopAtStep.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-84\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-56\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"StopAtStep.post_step\"><code>tf.contrib.learn.monitors.StopAtStep.post_step(step, session)</code></h4> <p>Callback after the step is finished.</p> <p>Called after step_end and receives session to perform extra session.run calls. If failure occurred in the process, will be called as well.</p>  <h5 id=\"args-85\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, global step of the model.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"StopAtStep.run_on_all_workers\"><code>tf.contrib.learn.monitors.StopAtStep.run_on_all_workers</code></h4>  <h4 id=\"StopAtStep.set_estimator\"><code>tf.contrib.learn.monitors.StopAtStep.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-86\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-57\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"StopAtStep.step_begin\"><code>tf.contrib.learn.monitors.StopAtStep.step_begin(step)</code></h4>  <h4 id=\"StopAtStep.step_end\"><code>tf.contrib.learn.monitors.StopAtStep.step_end(step, output)</code></h4>  <h3 id=\"SummarySaver\"><code>class tf.contrib.learn.monitors.SummarySaver</code></h3> <p>Saves summaries every N steps.</p>  <h4 id=\"SummarySaver.__init__\"><code>tf.contrib.learn.monitors.SummarySaver.__init__(summary_op, save_steps=100, output_dir=None, summary_writer=None, scaffold=None)</code></h4> <p>Initializes a <code>SummarySaver</code> monitor.</p>  <h5 id=\"args-87\">Args:</h5> <ul> <li>\n<code>summary_op</code>: <code>Tensor</code> of type <code>string</code>. A serialized <code>Summary</code> protocol buffer, as output by TF summary methods like <code>scalar_summary</code> or <code>merge_all_summaries</code>.</li> <li>\n<code>save_steps</code>: <code>int</code>, save summaries every N steps. See <code>EveryN</code>.</li> <li>\n<code>output_dir</code>: <code>string</code>, the directory to save the summaries to. Only used if no <code>summary_writer</code> is supplied.</li> <li>\n<code>summary_writer</code>: <code>SummaryWriter</code>. If <code>None</code> and an <code>output_dir</code> was passed, one will be created accordingly.</li> <li>\n<code>scaffold</code>: <code>Scaffold</code> to get summary_op if it's not provided.</li> </ul>  <h4 id=\"SummarySaver.begin\"><code>tf.contrib.learn.monitors.SummarySaver.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-88\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-58\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"SummarySaver.end\"><code>tf.contrib.learn.monitors.SummarySaver.end(session=None)</code></h4>  <h4 id=\"SummarySaver.epoch_begin\"><code>tf.contrib.learn.monitors.SummarySaver.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-89\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-59\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"SummarySaver.epoch_end\"><code>tf.contrib.learn.monitors.SummarySaver.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-90\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-60\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"SummarySaver.every_n_post_step\"><code>tf.contrib.learn.monitors.SummarySaver.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-91\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"SummarySaver.every_n_step_begin\"><code>tf.contrib.learn.monitors.SummarySaver.every_n_step_begin(step)</code></h4>  <h4 id=\"SummarySaver.every_n_step_end\"><code>tf.contrib.learn.monitors.SummarySaver.every_n_step_end(step, outputs)</code></h4>  <h4 id=\"SummarySaver.post_step\"><code>tf.contrib.learn.monitors.SummarySaver.post_step(step, session)</code></h4>  <h4 id=\"SummarySaver.run_on_all_workers\"><code>tf.contrib.learn.monitors.SummarySaver.run_on_all_workers</code></h4>  <h4 id=\"SummarySaver.set_estimator\"><code>tf.contrib.learn.monitors.SummarySaver.set_estimator(estimator)</code></h4>  <h4 id=\"SummarySaver.step_begin\"><code>tf.contrib.learn.monitors.SummarySaver.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-92\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-61\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"SummarySaver.step_end\"><code>tf.contrib.learn.monitors.SummarySaver.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-93\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h3 id=\"ValidationMonitor\"><code>class tf.contrib.learn.monitors.ValidationMonitor</code></h3> <p>Runs evaluation of a given estimator, at most every N steps.</p> <p>Note that the evaluation is done based on the saved checkpoint, which will usually be older than the current step.</p> <p>Can do early stopping on validation metrics if <code>early_stopping_rounds</code> is provided.</p>  <h4 id=\"ValidationMonitor.__init__\"><code>tf.contrib.learn.monitors.ValidationMonitor.__init__(x=None, y=None, input_fn=None, batch_size=None, eval_steps=None, every_n_steps=100, metrics=None, early_stopping_rounds=None, early_stopping_metric='loss', early_stopping_metric_minimize=True, name=None)</code></h4> <p>Initializes a ValidationMonitor.</p>  <h5 id=\"args-94\">Args:</h5> <ul> <li>\n<code>x</code>: See <code>BaseEstimator.evaluate</code>.</li> <li>\n<code>y</code>: See <code>BaseEstimator.evaluate</code>.</li> <li>\n<code>input_fn</code>: See <code>BaseEstimator.evaluate</code>.</li> <li>\n<code>batch_size</code>: See <code>BaseEstimator.evaluate</code>.</li> <li>\n<code>eval_steps</code>: See <code>BaseEstimator.evaluate</code>.</li> <li>\n<code>every_n_steps</code>: Check for new checkpoints to evaluate every N steps. If a new checkpoint is found, it is evaluated. See <code>EveryN</code>.</li> <li>\n<code>metrics</code>: See <code>BaseEstimator.evaluate</code>.</li> <li>\n<code>early_stopping_rounds</code>: <code>int</code>. If the metric indicated by <code>early_stopping_metric</code> does not change according to <code>early_stopping_metric_minimize</code> for this many steps, then training will be stopped.</li> <li>\n<code>early_stopping_metric</code>: <code>string</code>, name of the metric to check for early stopping.</li> <li>\n<code>early_stopping_metric_minimize</code>: <code>bool</code>, True if <code>early_stopping_metric</code> is expected to decrease (thus early stopping occurs when this metric stops decreasing), False if <code>early_stopping_metric</code> is expected to increase. Typically, <code>early_stopping_metric_minimize</code> is True for loss metrics like mean squared error, and False for performance metrics like accuracy.</li> <li>\n<code>name</code>: See <code>BaseEstimator.evaluate</code>.</li> </ul>  <h5 id=\"raises-62\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If both x and input_fn are provided.</li> </ul>  <h4 id=\"ValidationMonitor.begin\"><code>tf.contrib.learn.monitors.ValidationMonitor.begin(max_steps=None)</code></h4> <p>Called at the beginning of training.</p> <p>When called, the default graph is the one we are executing.</p>  <h5 id=\"args-95\">Args:</h5> <ul> <li>\n<code>max_steps</code>: <code>int</code>, the maximum global step this training will run until.</li> </ul>  <h5 id=\"raises-63\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun a run.</li> </ul>  <h4 id=\"ValidationMonitor.best_step\"><code>tf.contrib.learn.monitors.ValidationMonitor.best_step</code></h4> <p>Returns the step at which the best early stopping metric was found.</p>  <h4 id=\"ValidationMonitor.best_value\"><code>tf.contrib.learn.monitors.ValidationMonitor.best_value</code></h4> <p>Returns the best early stopping metric value found so far.</p>  <h4 id=\"ValidationMonitor.early_stopped\"><code>tf.contrib.learn.monitors.ValidationMonitor.early_stopped</code></h4> <p>Returns True if this monitor caused an early stop.</p>  <h4 id=\"ValidationMonitor.end\"><code>tf.contrib.learn.monitors.ValidationMonitor.end(session=None)</code></h4>  <h4 id=\"ValidationMonitor.epoch_begin\"><code>tf.contrib.learn.monitors.ValidationMonitor.epoch_begin(epoch)</code></h4> <p>Begin epoch.</p>  <h5 id=\"args-96\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-64\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've already begun an epoch, or <code>epoch</code> &lt; 0.</li> </ul>  <h4 id=\"ValidationMonitor.epoch_end\"><code>tf.contrib.learn.monitors.ValidationMonitor.epoch_end(epoch)</code></h4> <p>End epoch.</p>  <h5 id=\"args-97\">Args:</h5> <ul> <li>\n<code>epoch</code>: <code>int</code>, the epoch number.</li> </ul>  <h5 id=\"raises-65\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if we've not begun an epoch, or <code>epoch</code> number does not match.</li> </ul>  <h4 id=\"ValidationMonitor.every_n_post_step\"><code>tf.contrib.learn.monitors.ValidationMonitor.every_n_post_step(step, session)</code></h4> <p>Callback after a step is finished or <code>end()</code> is called.</p>  <h5 id=\"args-98\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>session</code>: <code>Session</code> object.</li> </ul>  <h4 id=\"ValidationMonitor.every_n_step_begin\"><code>tf.contrib.learn.monitors.ValidationMonitor.every_n_step_begin(step)</code></h4> <p>Callback before every n'th step begins.</p>  <h5 id=\"args-99\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>A <code>list</code> of tensors that will be evaluated at this step.</p>  <h4 id=\"ValidationMonitor.every_n_step_end\"><code>tf.contrib.learn.monitors.ValidationMonitor.every_n_step_end(step, outputs)</code></h4>  <h4 id=\"ValidationMonitor.post_step\"><code>tf.contrib.learn.monitors.ValidationMonitor.post_step(step, session)</code></h4>  <h4 id=\"ValidationMonitor.run_on_all_workers\"><code>tf.contrib.learn.monitors.ValidationMonitor.run_on_all_workers</code></h4>  <h4 id=\"ValidationMonitor.set_estimator\"><code>tf.contrib.learn.monitors.ValidationMonitor.set_estimator(estimator)</code></h4> <p>A setter called automatically by the target estimator.</p>  <h5 id=\"args-100\">Args:</h5> <ul> <li>\n<code>estimator</code>: the estimator that this monitor monitors.</li> </ul>  <h5 id=\"raises-66\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if the estimator is None.</li> </ul>  <h4 id=\"ValidationMonitor.step_begin\"><code>tf.contrib.learn.monitors.ValidationMonitor.step_begin(step)</code></h4> <p>Overrides <code>BaseMonitor.step_begin</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-101\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>A <code>list</code>, the result of every_n_step_begin, if that was called this step, or an empty list otherwise.</p>  <h5 id=\"raises-67\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if called more than once during a step.</li> </ul>  <h4 id=\"ValidationMonitor.step_end\"><code>tf.contrib.learn.monitors.ValidationMonitor.step_end(step, output)</code></h4> <p>Overrides <code>BaseMonitor.step_end</code>.</p> <p>When overriding this method, you must call the super implementation.</p>  <h5 id=\"args-102\">Args:</h5> <ul> <li>\n<code>step</code>: <code>int</code>, the current value of the global step.</li> <li>\n<code>output</code>: <code>dict</code> mapping <code>string</code> values representing tensor names to the value resulted from running these tensors. Values may be either scalars, for scalar tensors, or Numpy <code>array</code>, for non-scalar tensors.</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p><code>bool</code>, the result of every_n_step_end, if that was called this step, or <code>False</code> otherwise.</p>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"SummaryWriterCache\"><code>class tf.contrib.learn.monitors.SummaryWriterCache</code></h3> <p>Cache for summary writers.</p> <p>This class caches summary writers, one per directory.</p>  <h4 id=\"SummaryWriterCache.clear\"><code>tf.contrib.learn.monitors.SummaryWriterCache.clear()</code></h4> <p>Clear cached summary writers. Currently only used for unit tests.</p>  <h4 id=\"SummaryWriterCache.get\"><code>tf.contrib.learn.monitors.SummaryWriterCache.get(logdir)</code></h4> <p>Returns the SummaryWriter for the specified directory.</p>  <h5 id=\"args-103\">Args:</h5> <ul> <li>\n<code>logdir</code>: str, name of the directory.</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p>A <code>SummaryWriter</code>.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.learn.monitors.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.learn.monitors.html</a>\n  </p>\n</div>\n","contrib.metrics":"<h1 id=\"metrics-contrib\">Metrics (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#metrics-contrib\">Metrics (contrib)</a></li> <ul> <li><a href=\"#ops-for-evaluation-metrics-and-summary-statistics\">Ops for evaluation metrics and summary statistics.</a></li> <ul> <li><a href=\"#api\">API</a></li> </ul> <li><a href=\"#metric-ops\">Metric <code>Ops</code></a></li> <ul> <li><a href=\"#streaming_accuracy\"><code>tf.contrib.metrics.streaming_accuracy(predictions, labels, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_mean\"><code>tf.contrib.metrics.streaming_mean(values, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_recall\"><code>tf.contrib.metrics.streaming_recall(predictions, labels, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_precision\"><code>tf.contrib.metrics.streaming_precision(predictions, labels, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_auc\"><code>tf.contrib.metrics.streaming_auc(predictions, labels, ignore_mask=None, num_thresholds=200, metrics_collections=None, updates_collections=None, curve=ROC, name=None)</code></a></li> <li><a href=\"#streaming_recall_at_k\"><code>tf.contrib.metrics.streaming_recall_at_k(predictions, labels, k, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_mean_absolute_error\"><code>tf.contrib.metrics.streaming_mean_absolute_error(predictions, labels, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_mean_iou\"><code>tf.contrib.metrics.streaming_mean_iou(predictions, labels, num_classes, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_mean_relative_error\"><code>tf.contrib.metrics.streaming_mean_relative_error(predictions, labels, normalizer, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_mean_squared_error\"><code>tf.contrib.metrics.streaming_mean_squared_error(predictions, labels, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_root_mean_squared_error\"><code>tf.contrib.metrics.streaming_root_mean_squared_error(predictions, labels, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_mean_cosine_distance\"><code>tf.contrib.metrics.streaming_mean_cosine_distance(predictions, labels, dim, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_percentage_less\"><code>tf.contrib.metrics.streaming_percentage_less(values, threshold, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_sparse_precision_at_k\"><code>tf.contrib.metrics.streaming_sparse_precision_at_k(predictions, labels, k, class_id=None, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#streaming_sparse_recall_at_k\"><code>tf.contrib.metrics.streaming_sparse_recall_at_k(predictions, labels, k, class_id=None, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></a></li> <li><a href=\"#auc_using_histogram\"><code>tf.contrib.metrics.auc_using_histogram(boolean_labels, scores, score_range, nbins=100, collections=None, check_shape=True, name=None)</code></a></li> <li><a href=\"#accuracy\"><code>tf.contrib.metrics.accuracy(predictions, labels, weights=None)</code></a></li> <li><a href=\"#confusion_matrix\"><code>tf.contrib.metrics.confusion_matrix(predictions, labels, num_classes=None, dtype=tf.int32, name=None)</code></a></li> <li><a href=\"#aggregate_metrics\"><code>tf.contrib.metrics.aggregate_metrics(*value_update_tuples)</code></a></li> <li><a href=\"#aggregate_metric_map\"><code>tf.contrib.metrics.aggregate_metric_map(names_to_tuples)</code></a></li> </ul> <li><a href=\"#set-ops\">Set <code>Ops</code></a></li> <ul> <li><a href=\"#set_difference\"><code>tf.contrib.metrics.set_difference(a, b, aminusb=True, validate_indices=True)</code></a></li> <li><a href=\"#set_intersection\"><code>tf.contrib.metrics.set_intersection(a, b, validate_indices=True)</code></a></li> <li><a href=\"#set_size\"><code>tf.contrib.metrics.set_size(a, validate_indices=True)</code></a></li> <li><a href=\"#set_union\"><code>tf.contrib.metrics.set_union(a, b, validate_indices=True)</code></a></li> </ul>\n</ul>\n</ul> </div>  <h2 id=\"ops-for-evaluation-metrics-and-summary-statistics\">Ops for evaluation metrics and summary statistics.</h2> <h3 id=\"api\">API</h3> <p>This module provides functions for computing streaming metrics: metrics computed on dynamically valued <code>Tensors</code>. Each metric declaration returns a \"value_tensor\", an idempotent operation that returns the current value of the metric, and an \"update_op\", an operation that accumulates the information from the current value of the <code>Tensors</code> being measured as well as returns the value of the \"value_tensor\".</p> <p>To use any of these metrics, one need only declare the metric, call <code>update_op</code> repeatedly to accumulate data over the desired number of <code>Tensor</code> values (often each one is a single batch) and finally evaluate the value_tensor. For example, to use the <code>streaming_mean</code>:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">value = ...\nmean_value, update_op = tf.contrib.metrics.streaming_mean(values)\nsess.run(tf.initialize_local_variables())\n\nfor i in range(number_of_batches):\n  print('Mean after batch %d: %f' % (i, update_op.eval())\nprint('Final Mean: %f' % mean_value.eval())\n</pre> <p>Each metric function adds nodes to the graph that hold the state necessary to compute the value of the metric as well as a set of operations that actually perform the computation. Every metric evaluation is composed of three steps</p> <ul> <li>Initialization: initializing the metric state.</li> <li>Aggregation: updating the values of the metric state.</li> <li>Finalization: computing the final metric value.</li> </ul> <p>In the above example, calling streaming_mean creates a pair of state variables that will contain (1) the running sum and (2) the count of the number of samples in the sum. Because the streaming metrics use local variables, the Initialization stage is performed by running the op returned by <code>tf.initialize_local_variables()</code>. It sets the sum and count variables to zero.</p> <p>Next, Aggregation is performed by examining the current state of <code>values</code> and incrementing the state variables appropriately. This step is executed by running the <code>update_op</code> returned by the metric.</p> <p>Finally, finalization is performed by evaluating the \"value_tensor\"</p> <p>In practice, we commonly want to evaluate across many batches and multiple metrics. To do so, we need only run the metric computation operations multiple times:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">labels = ...\npredictions = ...\naccuracy, update_op_acc = tf.contrib.metrics.streaming_accuracy(\n    labels, predictions)\nerror, update_op_error = tf.contrib.metrics.streaming_mean_absolute_error(\n    labels, predictions)\n\nsess.run(tf.initialize_local_variables())\nfor batch in range(num_batches):\n  sess.run([update_op_acc, update_op_error])\n\naccuracy, mean_absolute_error = sess.run([accuracy, mean_absolute_error])\n</pre> <p>Note that when evaluating the same metric multiple times on different inputs, one must specify the scope of each metric to avoid accumulating the results together:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">labels = ...\npredictions0 = ...\npredictions1 = ...\n\naccuracy0 = tf.contrib.metrics.accuracy(labels, predictions0, name='preds0')\naccuracy1 = tf.contrib.metrics.accuracy(labels, predictions1, name='preds1')\n</pre> <p>Certain metrics, such as streaming_mean or streaming_accuracy, can be weighted via a <code>weights</code> argument. The <code>weights</code> tensor must be the same size as the labels and predictions tensors and results in a weighted average of the metric.</p> <p>Other metrics, such as streaming_recall, streaming_precision, and streaming_auc, are not well defined with regard to weighted samples. However, a binary <code>ignore_mask</code> argument can be used to ignore certain values at graph executation time.</p>  <h2 id=\"metric-ops\">Metric <code>Ops</code>\n</h2>  <h3 id=\"streaming_accuracy\"><code>tf.contrib.metrics.streaming_accuracy(predictions, labels, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Calculates how often <code>predictions</code> matches <code>labels</code>.</p> <p>The <code>streaming_accuracy</code> function creates two local variables, <code>total</code> and <code>count</code> that are used to compute the frequency with which <code>predictions</code> matches <code>labels</code>. This frequency is ultimately returned as <code>accuracy</code>: an idempotent operation that simply divides <code>total</code> by <code>count</code>. To facilitate the estimation of the accuracy over a stream of data, the function utilizes two operations. First, an <code>is_correct</code> operation that computes a tensor whose shape matches <code>predictions</code> and whose elements are set to 1.0 when the corresponding values of <code>predictions</code> and <code>labels match\nand 0.0 otherwise. Second, an</code>update_op<code>operation whose behavior is\ndependent on the value of</code>weights<code>. If</code>weights<code>is None, then</code>update_op<code>increments</code>total<code>with the number of elements of</code>predictions<code>that match</code>labels<code>and increments</code>count<code>with the number of elements in</code>values<code>. If</code>weights<code>is not</code>None<code>, then</code>update_op<code>increments</code>total<code>with the reduced\nsum of the product of</code>weights<code>and</code>is_correct<code>and increments</code>count<code>with\nthe reduced sum of</code>weights<code>. In addition to performing the updates,</code>update_op<code>also returns the</code>accuracy` value.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>predictions</code>: The predicted values, a <code>Tensor</code> of any shape.</li> <li>\n<code>labels</code>: The ground truth values, a <code>Tensor</code> whose shape matches <code>predictions</code>.</li> <li>\n<code>weights</code>: An optional set of weights whose shape matches <code>predictions</code> which, when not <code>None</code>, produces a weighted mean accuracy.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>accuracy</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul> <h5 id=\"returns\">Returns:</h5> <ul> <li>\n<code>accuracy</code>: A tensor representing the accuracy, the value of <code>total</code> divided by <code>count</code>.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately and whose value matches <code>accuracy</code>.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the dimensions of <code>predictions</code> and <code>labels</code> don't match or if <code>weight</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_mean\"><code>tf.contrib.metrics.streaming_mean(values, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the (weighted) mean of the given values.</p> <p>The <code>streaming_mean</code> function creates two local variables, <code>total</code> and <code>count</code> that are used to compute the average of <code>values</code>. This average is ultimately returned as <code>mean</code> which is an idempotent operation that simply divides <code>total</code> by <code>count</code>. To facilitate the estimation of a mean over a stream of data, the function creates an <code>update_op</code> operation whose behavior is dependent on the value of <code>weights</code>. If <code>weights</code> is None, then <code>update_op</code> increments <code>total</code> with the reduced sum of <code>values</code> and increments <code>count</code> with the number of elements in <code>values</code>. If <code>weights</code> is not <code>None</code>, then <code>update_op</code> increments <code>total</code> with the reduced sum of the product of <code>values</code> and <code>weights</code> and increments <code>count</code> with the reduced sum of weights. In addition to performing the updates, <code>update_op</code> also returns the <code>mean</code>.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>values</code>: A <code>Tensor</code> of arbitrary dimensions.</li> <li>\n<code>weights</code>: An optional set of weights of the same shape as <code>values</code>. If <code>weights</code> is not None, the function computes a weighted mean.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>mean</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <ul> <li>\n<code>mean</code>: A tensor representing the current mean, the value of <code>total</code> divided by <code>count</code>.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately and whose value matches <code>mean_value</code>.</li> </ul>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>weights</code> is not <code>None</code> and its shape doesn't match <code>values</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_recall\"><code>tf.contrib.metrics.streaming_recall(predictions, labels, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the recall of the predictions with respect to the labels.</p> <p>The <code>streaming_recall</code> function creates two local variables, <code>true_positives</code> and <code>false_negatives</code>, that are used to compute the recall. This value is ultimately returned as <code>recall</code>, an idempotent operation that simply divides <code>true_positives</code> by the sum of <code>true_positives</code> and <code>false_negatives</code>. To facilitate the calculation of the recall over a stream of data, the function creates an <code>update_op</code> operation whose behavior is dependent on the value of <code>ignore_mask</code>. If <code>ignore_mask</code> is None, then <code>update_op</code> increments <code>true_positives</code> with the number of elements of <code>predictions</code> and <code>labels</code> that are both <code>True</code> and increments <code>false_negatives</code> with the number of elements of <code>predictions</code> that are <code>False</code> whose corresponding <code>labels</code> element is <code>False</code>. If <code>ignore_mask</code> is not <code>None</code>, then the increments for <code>true_positives</code> and <code>false_negatives</code> are only computed using elements of <code>predictions</code> and <code>labels</code> whose corresponding values in <code>ignore_mask</code> are <code>False</code>. In addition to performing the updates, <code>update_op</code> also returns the value of <code>recall</code>.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>predictions</code>: The predicted values, a binary <code>Tensor</code> of arbitrary shape.</li> <li>\n<code>labels</code>: The ground truth values, a binary <code>Tensor</code> whose dimensions must match <code>predictions</code>.</li> <li>\n<code>ignore_mask</code>: An optional, binary tensor whose size matches <code>predictions</code>.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>recall</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <ul> <li>\n<code>recall</code>: Scalar float <code>Tensor</code> with the value of <code>true_positives</code> divided by the sum of <code>true_positives</code> and <code>false_negatives</code>.</li> <li>\n<code>update_op</code>: <code>Operation</code> that increments <code>true_positives</code> and <code>false_negatives</code> variables appropriately and whose value matches <code>recall</code>.</li> </ul>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the dimensions of <code>predictions</code> and <code>labels</code> don't match or if <code>ignore_mask</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_precision\"><code>tf.contrib.metrics.streaming_precision(predictions, labels, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the precision of the predictions with respect to the labels.</p> <p>The <code>streaming_precision</code> function creates two local variables, <code>true_positives</code> and <code>false_positives</code>, that are used to compute the precision. This value is ultimately returned as <code>precision</code>, an idempotent operation that simply divides <code>true_positives</code> by the sum of <code>true_positives</code> and <code>false_positives</code>. To facilitate the calculation of the precision over a stream of data, the function creates an <code>update_op</code> operation whose behavior is dependent on the value of <code>ignore_mask</code>. If <code>ignore_mask</code> is None, then <code>update_op</code> increments <code>true_positives</code> with the number of elements of <code>predictions</code> and <code>labels</code> that are both <code>True</code> and increments <code>false_positives</code> with the number of elements of <code>predictions</code> that are <code>True</code> whose corresponding <code>labels</code> element is <code>False</code>. If <code>ignore_mask</code> is not <code>None</code>, then the increments for <code>true_positives</code> and <code>false_positives</code> are only computed using elements of <code>predictions</code> and <code>labels</code> whose corresponding values in <code>ignore_mask</code> are <code>False</code>. In addition to performing the updates, <code>update_op</code> also returns the value of <code>precision</code>.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>predictions</code>: The predicted values, a binary <code>Tensor</code> of arbitrary shape.</li> <li>\n<code>labels</code>: The ground truth values, a binary <code>Tensor</code> whose dimensions must match <code>predictions</code>.</li> <li>\n<code>ignore_mask</code>: An optional, binary tensor whose size matches <code>predictions</code>.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>precision</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <ul> <li>\n<code>precision</code>: Scalar float <code>Tensor</code> with the value of <code>true_positives</code> divided by the sum of <code>true_positives</code> and <code>false_positives</code>.</li> <li>\n<code>update_op</code>: <code>Operation</code> that increments <code>true_positives</code> and <code>false_positives</code> variables appropriately and whose value matches <code>precision</code>.</li> </ul>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the dimensions of <code>predictions</code> and <code>labels</code> don't match or if <code>ignore_mask</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_auc\"><code>tf.contrib.metrics.streaming_auc(predictions, labels, ignore_mask=None, num_thresholds=200, metrics_collections=None, updates_collections=None, curve='ROC', name=None)</code></h3> <p>Computes the approximate AUC via a Riemann sum.</p> <p>The <code>streaming_auc</code> function creates four local variables, <code>true_positives</code>, <code>true_negatives</code>, <code>false_positives</code> and <code>false_negatives</code> that are used to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall.</p> <p>This value is ultimately returned as <code>auc</code>, an idempotent operation the computes the area under a discretized curve of precision versus recall values (computed using the afformentioned variables). The <code>num_thresholds</code> variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC.</p> <p>To faciliate the estimation of the AUC over a stream of data, the function creates an <code>update_op</code> operation whose behavior is dependent on the value of <code>ignore_mask</code>. If <code>ignore_mask</code> is None, then <code>update_op</code> increments the <code>true_positives</code>, <code>true_negatives</code>, <code>false_positives</code> and <code>false_negatives</code> counts with the number of each found in the current <code>predictions</code> and <code>labels</code> <code>Tensors</code>. If <code>ignore_mask</code> is not <code>None</code>, then the increment is performed using only the elements of <code>predictions</code> and <code>labels</code> whose corresponding value in <code>ignore_mask</code> is <code>False</code>. In addition to performing the updates, <code>update_op</code> also returns the <code>auc</code>.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>predictions</code>: A floating point <code>Tensor</code> of arbitrary shape and whose values are in the range <code>[0, 1]</code>.</li> <li>\n<code>labels</code>: A binary <code>Tensor</code> whose shape matches <code>predictions</code>.</li> <li>\n<code>ignore_mask</code>: An optional, binary tensor whose size matches <code>predictions</code>.</li> <li>\n<code>num_thresholds</code>: The number of thresholds to use when discretizing the roc curve.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>auc</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li><p><code>curve</code>: Specifies the name of the curve to be computed, 'ROC' [default] or 'PR' for the Precision-Recall-curve.</p></li> <li><p><code>name</code>: An optional variable_op_scope name.</p></li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <ul> <li>\n<code>auc</code>: A scalar tensor representing the current area-under-curve.</li> <li>\n<code>update_op</code>: An operation that increments the <code>true_positives</code>, <code>true_negatives</code>, <code>false_positives</code> and <code>false_negatives</code> variables appropriately and whose value matches <code>auc</code>.</li> </ul>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the shape of <code>predictions</code> and <code>labels</code> do not match or if <code>ignore_mask</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_recall_at_k\"><code>tf.contrib.metrics.streaming_recall_at_k(predictions, labels, k, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the recall@k of the predictions with respect to dense labels.</p> <p>The <code>streaming_recall_at_k</code> function creates two local variables, <code>total</code> and <code>count</code>, that are used to compute the recall@k frequency. This frequency is ultimately returned as <code>recall_at_&lt;k&gt;</code>: an idempotent operation that simply divides <code>total</code> by <code>count</code>. To facilitate the estimation of recall@k over a stream of data, the function utilizes two operations. First, an <code>in_top_k</code> operation computes a tensor with shape [batch_size] whose elements indicate whether or not the corresponding label is in the top <code>k</code> predictions of the <code>predictions</code> <code>Tensor</code>. Second, an <code>update_op</code> operation whose behavior is dependent on the value of <code>ignore_mask</code>. If <code>ignore_mask</code> is None, then <code>update_op</code> increments <code>total</code> with the number of elements of <code>in_top_k</code> that are set to <code>True</code> and increments <code>count</code> with the batch size. If <code>ignore_mask</code> is not <code>None</code>, then <code>update_op</code> increments <code>total</code> with the number of elements in <code>in_top_k</code> that are <code>True</code> whose corresponding element in <code>ignore_mask</code> is <code>False</code>. In addition to performing the updates, <code>update_op</code> also returns the recall value.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>predictions</code>: A floating point tensor of dimension [batch_size, num_classes]</li> <li>\n<code>labels</code>: A tensor of dimension [batch_size] whose type is in <code>int32</code>, <code>int64</code>.</li> <li>\n<code>k</code>: The number of top elements to look at for computing recall.</li> <li>\n<code>ignore_mask</code>: An optional, binary tensor whose size matches <code>labels</code>. If an element of <code>ignore_mask</code> is True, the corresponding prediction and label pair is used to compute the metrics. Otherwise, the pair is ignored.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>recall_at_k</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <ul> <li>\n<code>recall_at_k</code>: A tensor representing the recall@k, the fraction of labels which fall into the top <code>k</code> predictions.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately and whose value matches <code>recall_at_k</code>.</li> </ul>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the dimensions of <code>predictions</code> and <code>labels</code> don't match or if <code>ignore_mask</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_mean_absolute_error\"><code>tf.contrib.metrics.streaming_mean_absolute_error(predictions, labels, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the mean absolute error between the labels and predictions.</p> <p>The <code>streaming_mean_absolute_error</code> function creates two local variables, <code>total</code> and <code>count</code> that are used to compute the mean absolute error. This average is ultimately returned as <code>mean_absolute_error</code>: an idempotent operation that simply divides <code>total</code> by <code>count</code>. To facilitate the estimation of the mean absolute error over a stream of data, the function utilizes two operations. First, an <code>absolute_errors</code> operation computes the absolute value of the differences between <code>predictions</code> and <code>labels</code>. Second, an <code>update_op</code> operation whose behavior is dependent on the value of <code>weights</code>. If <code>weights</code> is None, then <code>update_op</code> increments <code>total</code> with the reduced sum of <code>absolute_errors</code> and increments <code>count</code> with the number of elements in <code>absolute_errors</code>. If <code>weights</code> is not <code>None</code>, then <code>update_op</code> increments <code>total</code> with the reduced sum of the product of <code>weights</code> and <code>absolute_errors</code> and increments <code>count</code> with the reduced sum of <code>weights</code>. In addition to performing the updates, <code>update_op</code> also returns the <code>mean_absolute_error</code> value.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>predictions</code>: A <code>Tensor</code> of arbitrary shape.</li> <li>\n<code>labels</code>: A <code>Tensor</code> of the same shape as <code>predictions</code>.</li> <li>\n<code>weights</code>: An optional set of weights of the same shape as <code>predictions</code>. If <code>weights</code> is not None, the function computes a weighted mean.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>mean_absolute_error</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <ul> <li>\n<code>mean_absolute_error</code>: A tensor representing the current mean, the value of <code>total</code> divided by <code>count</code>.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately and whose value matches <code>mean_absolute_error</code>.</li> </ul>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>weights</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_mean_iou\"><code>tf.contrib.metrics.streaming_mean_iou(predictions, labels, num_classes, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Calculate per-step mean Intersection-Over-Union (mIOU).</p> <p>Mean Intersection-Over-Union is a common evaluation metric for semantic image segmentation, which first computes the IOU for each semantic class and then computes the average over classes.</p>  <h5 id=\"iou-is-defined-as-follows\">IOU is defined as follows:</h5> <p>IOU = true_positive / (true_positive + false_positive + false_negative). The predictions are accumulated in a confusion matrix, and mIOU is then calculated from it.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>predictions</code>: A tensor of prediction results for semantic labels, whose shape is [batch size] and type <code>int32</code> or <code>int64</code>. The tensor will be flattened, if its rank &gt; 1.</li> <li>\n<code>labels</code>: A tensor of ground truth labels with shape [batch size] and of type <code>int32</code> or <code>int64</code>. The tensor will be flattened, if its rank &gt; 1.</li> <li>\n<code>num_classes</code>: The possible number of labels the prediction task can have. This value must be provided, since a confusion matrix of dimension = [num_classes, num_classes] will be allocated.</li> <li>\n<code>ignore_mask</code>: An optional, boolean tensor whose size matches <code>labels</code>. If an element of <code>ignore_mask</code> is True, the corresponding prediction and label pair is NOT used to compute the metrics. Otherwise, the pair is included.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>mean_iou</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-8\">Returns:</h5> <ul> <li>\n<code>mean_iou</code>: A tensor representing the mean intersection-over-union.</li> <li>\n<code>update_op</code>: An operation that increments the confusion matrix.</li> </ul>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If the dimensions of <code>predictions</code> and <code>labels</code> don't match or if <code>ignore_mask</code> is not <code>None</code> and its shape doesn't match <code>labels</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_mean_relative_error\"><code>tf.contrib.metrics.streaming_mean_relative_error(predictions, labels, normalizer, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the mean relative error by normalizing with the given values.</p> <p>The <code>streaming_mean_relative_error</code> function creates two local variables, <code>total</code> and <code>count</code> that are used to compute the mean relative absolute error. This average is ultimately returned as <code>mean_relative_error</code>: an idempotent operation that simply divides <code>total</code> by <code>count</code>. To facilitate the estimation of the mean relative error over a stream of data, the function utilizes two operations. First, a <code>relative_errors</code> operation divides the absolute value of the differences between <code>predictions</code> and <code>labels</code> by the <code>normalizer</code>. Second, an <code>update_op</code> operation whose behavior is dependent on the value of <code>weights</code>. If <code>weights</code> is None, then <code>update_op</code> increments <code>total</code> with the reduced sum of <code>relative_errors</code> and increments <code>count</code> with the number of elements in <code>relative_errors</code>. If <code>weights</code> is not <code>None</code>, then <code>update_op</code> increments <code>total</code> with the reduced sum of the product of <code>weights</code> and <code>relative_errors</code> and increments <code>count</code> with the reduced sum of <code>weights</code>. In addition to performing the updates, <code>update_op</code> also returns the <code>mean_relative_error</code> value.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>predictions</code>: A <code>Tensor</code> of arbitrary shape.</li> <li>\n<code>labels</code>: A <code>Tensor</code> of the same shape as <code>predictions</code>.</li> <li>\n<code>normalizer</code>: A <code>Tensor</code> of the same shape as <code>predictions</code>.</li> <li>\n<code>weights</code>: An optional set of weights of the same shape as <code>predictions</code>. If <code>weights</code> is not None, the function computes a weighted mean.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>mean_relative_error</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <ul> <li>\n<code>mean_relative_error</code>: A tensor representing the current mean, the value of <code>total</code> divided by <code>count</code>.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately and whose value matches <code>mean_relative_error</code>.</li> </ul>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>weights</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_mean_squared_error\"><code>tf.contrib.metrics.streaming_mean_squared_error(predictions, labels, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the mean squared error between the labels and predictions.</p> <p>The <code>streaming_mean_squared_error</code> function creates two local variables, <code>total</code> and <code>count</code> that are used to compute the mean squared error. This average is ultimately returned as <code>mean_squared_error</code>: an idempotent operation that simply divides <code>total</code> by <code>count</code>. To facilitate the estimation of the mean squared error over a stream of data, the function utilizes two operations. First, a <code>squared_error</code> operation computes the element-wise square of the difference between <code>predictions</code> and <code>labels</code>. Second, an <code>update_op</code> operation whose behavior is dependent on the value of <code>weights</code>. If <code>weights</code> is None, then <code>update_op</code> increments <code>total</code> with the reduced sum of <code>squared_error</code> and increments <code>count</code> with the number of elements in <code>squared_error</code>. If <code>weights</code> is not <code>None</code>, then <code>update_op</code> increments <code>total</code> with the reduced sum of the product of <code>weights</code> and <code>squared_error</code> and increments <code>count</code> with the reduced sum of <code>weights</code>. In addition to performing the updates, <code>update_op</code> also returns the <code>mean_squared_error</code> value.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>predictions</code>: A <code>Tensor</code> of arbitrary shape.</li> <li>\n<code>labels</code>: A <code>Tensor</code> of the same shape as <code>predictions</code>.</li> <li>\n<code>weights</code>: An optional set of weights of the same shape as <code>predictions</code>. If <code>weights</code> is not None, the function computes a weighted mean.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>mean_squared_error</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <ul> <li>\n<code>mean_squared_error</code>: A tensor representing the current mean, the value of <code>total</code> divided by <code>count</code>.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately and whose value matches <code>mean_squared_error</code>.</li> </ul>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>weights</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_root_mean_squared_error\"><code>tf.contrib.metrics.streaming_root_mean_squared_error(predictions, labels, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the root mean squared error between the labels and predictions.</p> <p>The <code>streaming_root_mean_squared_error</code> function creates two local variables, <code>total</code> and <code>count</code> that are used to compute the root mean squared error. This average is ultimately returned as <code>root_mean_squared_error</code>: an idempotent operation that takes the square root of the division of <code>total</code> by <code>count</code>. To facilitate the estimation of the root mean squared error over a stream of data, the function utilizes two operations. First, a <code>squared_error</code> operation computes the element-wise square of the difference between <code>predictions</code> and <code>labels</code>. Second, an <code>update_op</code> operation whose behavior is dependent on the value of <code>weights</code>. If <code>weights</code> is None, then <code>update_op</code> increments <code>total</code> with the reduced sum of <code>squared_error</code> and increments <code>count</code> with the number of elements in <code>squared_error</code>. If <code>weights</code> is not <code>None</code>, then <code>update_op</code> increments <code>total</code> with the reduced sum of the product of <code>weights</code> and <code>squared_error</code> and increments <code>count</code> with the reduced sum of <code>weights</code>. In addition to performing the updates, <code>update_op</code> also returns the <code>root_mean_squared_error</code> value.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>predictions</code>: A <code>Tensor</code> of arbitrary shape.</li> <li>\n<code>labels</code>: A <code>Tensor</code> of the same shape as <code>predictions</code>.</li> <li>\n<code>weights</code>: An optional set of weights of the same shape as <code>predictions</code>. If <code>weights</code> is not None, the function computes a weighted mean.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that <code>root_mean_squared_error</code> should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that <code>update_op</code> should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <ul> <li>\n<code>root_mean_squared_error</code>: A tensor representing the current mean, the value of <code>total</code> divided by <code>count</code>.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately and whose value matches <code>root_mean_squared_error</code>.</li> </ul>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>weights</code> is not <code>None</code> and its shape doesn't match <code>predictions</code> or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_mean_cosine_distance\"><code>tf.contrib.metrics.streaming_mean_cosine_distance(predictions, labels, dim, weights=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the cosine distance between the labels and predictions.</p> <p>The <code>streaming_mean_cosine_distance</code> function creates two local variables, <code>total</code> and <code>count</code> that are used to compute the average cosine distance between <code>predictions</code> and <code>labels</code>. This average is ultimately returned as <code>mean_distance</code> which is an idempotent operation that simply divides <code>total</code> by <code>count. To facilitate the estimation of a mean over multiple batches\nof data, the function creates an</code>update_op<code>operation whose behavior is\ndependent on the value of</code>weights<code>. If</code>weights<code>is None, then</code>update_op<code>increments</code>total<code>with the reduced sum of</code>values and increments <code>count</code> with the number of elements in <code>values</code>. If <code>weights</code> is not <code>None</code>, then <code>update_op</code> increments <code>total</code> with the reduced sum of the product of <code>values</code> and <code>weights</code> and increments <code>count</code> with the reduced sum of weights.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>predictions</code>: A tensor of the same size as labels.</li> <li>\n<code>labels</code>: A tensor of arbitrary size.</li> <li>\n<code>dim</code>: The dimension along which the cosine distance is computed.</li> <li>\n<code>weights</code>: An optional set of weights which indicates which predictions to ignore during metric computation. Its size matches that of labels except for the value of 'dim' which should be 1. For example if labels has dimensions [32, 100, 200, 3], then <code>weights</code> should have dimensions [32, 100, 200, 1].</li> <li>\n<code>metrics_collections</code>: An optional list of collections that the metric value variable should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that the metric update ops should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <ul> <li>\n<code>mean_distance</code>: A tensor representing the current mean, the value of <code>total</code> divided by <code>count</code>.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately.</li> </ul>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If labels and predictions are of different sizes or if the ignore_mask is of the wrong size or if either <code>metrics_collections</code> or <code>updates_collections</code> are not a list or tuple.</li> </ul>  <h3 id=\"streaming_percentage_less\"><code>tf.contrib.metrics.streaming_percentage_less(values, threshold, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes the percentage of values less than the given threshold.</p> <p>The <code>streaming_percentage_less</code> function creates two local variables, <code>total</code> and <code>count</code> that are used to compute the percentage of <code>values</code> that fall below <code>threshold</code>. This rate is ultimately returned as <code>percentage</code> which is an idempotent operation that simply divides <code>total</code> by <code>count.\nTo facilitate the estimation of the percentage of values that fall under</code>threshold<code>over multiple batches of data, the function creates an</code>update_op<code>operation whose behavior is dependent on the value of</code>ignore_mask<code>. If</code>ignore_mask<code>is None, then</code>update_op<code>increments</code>total<code>with the number of elements of</code>values<code>that are less\nthan</code>threshold<code>and</code>count<code>with the number of elements in</code>values<code>. If</code>ignore_mask<code>is not</code>None<code>, then</code>update_op<code>increments</code>total<code>with the\nnumber of elements of</code>values<code>that are less than</code>threshold<code>and whose\ncorresponding entries in</code>ignore_mask<code>are False, and</code>count<code>is incremented\nwith the number of elements of</code>ignore_mask` that are False.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>values</code>: A numeric <code>Tensor</code> of arbitrary size.</li> <li>\n<code>threshold</code>: A scalar threshold.</li> <li>\n<code>ignore_mask</code>: An optional mask of the same shape as 'values' which indicates which elements to ignore during metric computation.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that the metric value variable should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that the metric update ops should be added to.</li> <li>\n<code>name</code>: An optional variable_op_scope name.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <ul> <li>\n<code>percentage</code>: A tensor representing the current mean, the value of <code>total</code> divided by <code>count</code>.</li> <li>\n<code>update_op</code>: An operation that increments the <code>total</code> and <code>count</code> variables appropriately.</li> </ul>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>ignore_mask</code> is not None and its shape doesn't match <code>values\nor if either</code>metrics_collections<code>or</code>updates_collections` are supplied but are not a list or tuple.</li> </ul>  <h3 id=\"streaming_sparse_precision_at_k\"><code>tf.contrib.metrics.streaming_sparse_precision_at_k(predictions, labels, k, class_id=None, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes precision@k of the predictions with respect to sparse labels.</p> <p>If <code>class_id</code> is specified, we calculate precision by considering only the entries in the batch for which <code>class_id</code> is in the top-k highest <code>predictions</code>, and computing the fraction of them for which <code>class_id</code> is indeed a correct label. If <code>class_id</code> is not specified, we'll calculate precision as how often on average a class among the top-k classes with the highest predicted values of a batch entry is correct and can be found in the label for that entry.</p> <p><code>streaming_sparse_precision_at_k</code> creates two local variables, <code>true_positive_at_&lt;k&gt;</code> and <code>false_positive_at_&lt;k&gt;</code>, that are used to compute the precision@k frequency. This frequency is ultimately returned as <code>precision_at_&lt;k&gt;</code>: an idempotent operation that simply divides <code>true_positive_at_&lt;k&gt;</code> by total (<code>true_positive_at_&lt;k&gt;</code> + <code>false_positive_at_&lt;k&gt;</code>). To facilitate the estimation of precision@k over a stream of data, the function utilizes three steps. * A <code>top_k</code> operation computes a tensor whose elements indicate the top <code>k</code> predictions of the <code>predictions</code> <code>Tensor</code>. * Set operations are applied to <code>top_k</code> and <code>labels</code> to calculate true positives and false positives. * An <code>update_op</code> operation increments <code>true_positive_at_&lt;k&gt;</code> and <code>false_positive_at_&lt;k&gt;</code>. It also returns the precision value.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>predictions</code>: Float <code>Tensor</code> with shape [D1, ... DN, num_classes] where N &gt;= 1. Commonly, N=1 and predictions has shape [batch size, num_classes]. The final dimension contains the logit values for each class. [D1, ... DN] must match <code>labels</code>.</li> <li>\n<code>labels</code>: <code>int64</code> <code>Tensor</code> or <code>SparseTensor</code> with shape [D1, ... DN, num_labels], where N &gt;= 1 and num_labels is the number of target classes for the associated prediction. Commonly, N=1 and <code>labels</code> has shape [batch_size, num_labels]. [D1, ... DN] must match <code>predictions_idx</code>. Values should be in range [0, num_classes], where num_classes is the last dimension of <code>predictions</code>.</li> <li>\n<code>k</code>: Integer, k for @k metric.</li> <li>\n<code>class_id</code>: Integer class ID for which we want binary metrics. This should be in range [0, num_classes], where num_classes is the last dimension of <code>predictions</code>.</li> <li>\n<code>ignore_mask</code>: An optional, binary tensor whose shape is broadcastable to the the first [D1, ... DN] dimensions of <code>predictions_idx</code> and <code>labels</code>.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that values should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that updates should be added to.</li> <li>\n<code>name</code>: Name of new update operation, and namespace for other dependant ops.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <ul> <li>\n<code>precision</code>: Scalar <code>float64</code> <code>Tensor</code> with the value of <code>true_positives</code> divided by the sum of <code>true_positives</code> and <code>false_positives</code>.</li> <li>\n<code>update_op</code>: <code>Operation</code> that increments <code>true_positives</code> and <code>false_positives</code> variables appropriately, and whose value matches <code>precision</code>.</li> </ul>  <h3 id=\"streaming_sparse_recall_at_k\"><code>tf.contrib.metrics.streaming_sparse_recall_at_k(predictions, labels, k, class_id=None, ignore_mask=None, metrics_collections=None, updates_collections=None, name=None)</code></h3> <p>Computes recall@k of the predictions with respect to sparse labels.</p> <p>If <code>class_id</code> is specified, we calculate recall by considering only the entries in the batch for which <code>class_id</code> is in the label, and computing the fraction of them for which <code>class_id</code> is in the top-k <code>predictions</code>. If <code>class_id</code> is not specified, we'll calculate recall as how often on average a class among the labels of a batch entry is in the top-k <code>predictions</code>.</p> <p><code>streaming_sparse_recall_at_k</code> creates two local variables, <code>true_positive_at_&lt;k&gt;</code> and <code>false_negative_at_&lt;k&gt;</code>, that are used to compute the recall_at_k frequency. This frequency is ultimately returned as <code>recall_at_&lt;k&gt;</code>: an idempotent operation that simply divides <code>true_positive_at_&lt;k&gt;</code> by total (<code>true_positive_at_&lt;k&gt;</code> + <code>recall_at_&lt;k&gt;</code>). To facilitate the estimation of recall@k over a stream of data, the function utilizes three steps. * A <code>top_k</code> operation computes a tensor whose elements indicate the top <code>k</code> predictions of the <code>predictions</code> <code>Tensor</code>. * Set operations are applied to <code>top_k</code> and <code>labels</code> to calculate true positives and false negatives. * An <code>update_op</code> operation increments <code>true_positive_at_&lt;k&gt;</code> and <code>false_negative_at_&lt;k&gt;</code>. It also returns the recall value.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>predictions</code>: Float <code>Tensor</code> with shape [D1, ... DN, num_classes] where N &gt;= 1. Commonly, N=1 and predictions has shape [batch size, num_classes]. The final dimension contains the logit values for each class. [D1, ... DN] must match <code>labels</code>.</li> <li>\n<code>labels</code>: <code>int64</code> <code>Tensor</code> or <code>SparseTensor</code> with shape [D1, ... DN, num_labels], where N &gt;= 1 and num_labels is the number of target classes for the associated prediction. Commonly, N=1 and <code>labels</code> has shape [batch_size, num_labels]. [D1, ... DN] must match <code>labels</code>. Values should be in range [0, num_classes], where num_classes is the last dimension of <code>predictions</code>.</li> <li>\n<code>k</code>: Integer, k for @k metric.</li> <li>\n<code>class_id</code>: Integer class ID for which we want binary metrics. This should be in range [0, num_classes], where num_classes is the last dimension of <code>predictions</code>.</li> <li>\n<code>ignore_mask</code>: An optional, binary tensor whose shape is broadcastable to the the first [D1, ... DN] dimensions of <code>predictions_idx</code> and <code>labels</code>.</li> <li>\n<code>metrics_collections</code>: An optional list of collections that values should be added to.</li> <li>\n<code>updates_collections</code>: An optional list of collections that updates should be added to.</li> <li>\n<code>name</code>: Name of new update operation, and namespace for other dependant ops.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <ul> <li>\n<code>recall</code>: Scalar <code>float64</code> <code>Tensor</code> with the value of <code>true_positives</code> divided by the sum of <code>true_positives</code> and <code>false_negatives</code>.</li> <li>\n<code>update_op</code>: <code>Operation</code> that increments <code>true_positives</code> and <code>false_negatives</code> variables appropriately, and whose value matches <code>recall</code>.</li> </ul>  <h3 id=\"auc_using_histogram\"><code>tf.contrib.metrics.auc_using_histogram(boolean_labels, scores, score_range, nbins=100, collections=None, check_shape=True, name=None)</code></h3> <p>AUC computed by maintaining histograms.</p> <p>Rather than computing AUC directly, this Op maintains Variables containing histograms of the scores associated with <code>True</code> and <code>False</code> labels. By comparing these the AUC is generated, with some discretization error. See: \"Efficient AUC Learning Curve Calculation\" by Bouckaert.</p> <p>This AUC Op updates in <code>O(batch_size + nbins)</code> time and works well even with large class imbalance. The accuracy is limited by discretization error due to finite number of bins. If scores are concentrated in a fewer bins, accuracy is lower. If this is a concern, we recommend trying different numbers of bins and comparing results.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>boolean_labels</code>: 1-D boolean <code>Tensor</code>. Entry is <code>True</code> if the corresponding record is in class.</li> <li>\n<code>scores</code>: 1-D numeric <code>Tensor</code>, same shape as boolean_labels.</li> <li>\n<code>score_range</code>: <code>Tensor</code> of shape <code>[2]</code>, same dtype as <code>scores</code>. The min/max values of score that we expect. Scores outside range will be clipped.</li> <li>\n<code>nbins</code>: Integer number of bins to use. Accuracy strictly increases as the number of bins increases.</li> <li>\n<code>collections</code>: List of graph collections keys. Internal histogram Variables are added to these collections. Defaults to <code>[GraphKeys.LOCAL_VARIABLES]</code>.</li> <li>\n<code>check_shape</code>: Boolean. If <code>True</code>, do a runtime shape check on the scores and labels.</li> <li>\n<code>name</code>: A name for this Op. Defaults to \"auc_using_histogram\".</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <ul> <li>\n<code>auc</code>: <code>float32</code> scalar <code>Tensor</code>. Fetching this converts internal histograms to auc value.</li> <li>\n<code>update_op</code>: <code>Op</code>, when run, updates internal histograms.</li> </ul>  <h3 id=\"accuracy\"><code>tf.contrib.metrics.accuracy(predictions, labels, weights=None)</code></h3> <p>Computes the percentage of times that predictions matches labels.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>predictions</code>: the predicted values, a <code>Tensor</code> whose dtype and shape matches 'labels'.</li> <li>\n<code>labels</code>: the ground truth values, a <code>Tensor</code> of any shape and bool, integer, or string dtype.</li> <li>\n<code>weights</code>: None or <code>Tensor</code> of float values to reweight the accuracy.</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>Accuracy <code>Tensor</code>.</p>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if dtypes don't match or if dtype is not bool, integer, or string.</li> </ul>  <h3 id=\"confusion_matrix\"><code>tf.contrib.metrics.confusion_matrix(predictions, labels, num_classes=None, dtype=tf.int32, name=None)</code></h3> <p>Computes the confusion matrix from predictions and labels.</p> <p>Calculate the Confusion Matrix for a pair of prediction and label 1-D int arrays.</p> <p>Considering a prediction array such as: <code>[1, 2, 3]</code> And a label array such as: <code>[2, 2, 3]</code></p>  <h5 id=\"the-confusion-matrix-returned-would-be-the-following-one\">The confusion matrix returned would be the following one:</h5> <pre class=\"\">[[0, 0, 0]\n [0, 1, 0]\n [0, 1, 0]\n [0, 0, 1]]\n</pre> <p>Where the matrix rows represent the prediction labels and the columns represents the real labels. The confusion matrix is always a 2-D array of shape [n, n], where n is the number of valid labels for a given classification task. Both prediction and labels must be 1-D arrays of the same shape in order for this function to work.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>predictions</code>: A 1-D array represeting the predictions for a given classification.</li> <li>\n<code>labels</code>: A 1-D represeting the real labels for the classification task.</li> <li>\n<code>num_classes</code>: The possible number of labels the classification task can have. If this value is not provided, it will be calculated using both predictions and labels array.</li> <li>\n<code>dtype</code>: Data type of the confusion matrix.</li> <li>\n<code>name</code>: Scope name.</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>A l X l matrix represeting the confusion matrix, where l in the number of possible labels in the classification task.</p>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If both predictions and labels are not 1-D vectors and do not have the same size.</li> </ul>  <h3 id=\"aggregate_metrics\"><code>tf.contrib.metrics.aggregate_metrics(*value_update_tuples)</code></h3> <p>Aggregates the metric value tensors and update ops into two lists.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>*value_update_tuples</code>: a variable number of tuples, each of which contain the pair of (value_tensor, update_op) from a streaming metric.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>a list of value tensors and a list of update ops.</p>  <h5 id=\"raises-16\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>value_update_tuples</code> is empty.</li> </ul>  <h3 id=\"aggregate_metric_map\"><code>tf.contrib.metrics.aggregate_metric_map(names_to_tuples)</code></h3> <p>Aggregates the metric names to tuple dictionary.</p> <p>This function is useful for pairing metric names with their associated value and update ops when the list of metrics is long. For example:</p> <p>metrics_to_values, metrics_to_updates = slim.metrics.aggregate_metric_map({ 'Mean Absolute Error': new_slim.metrics.streaming_mean_absolute_error( predictions, labels, weights), 'Mean Relative Error': new_slim.metrics.streaming_mean_relative_error( predictions, labels, labels, weights), 'RMSE Linear': new_slim.metrics.streaming_root_mean_squared_error( predictions, labels, weights), 'RMSE Log': new_slim.metrics.streaming_root_mean_squared_error( predictions, labels, weights), })</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>names_to_tuples</code>: a map of metric names to tuples, each of which contain the pair of (value_tensor, update_op) from a streaming metric.</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <p>A dictionary from metric names to value ops and a dictionary from metric names to update ops.</p>  <h2 id=\"set-ops\">Set <code>Ops</code>\n</h2>  <h3 id=\"set_difference\"><code>tf.contrib.metrics.set_difference(a, b, aminusb=True, validate_indices=True)</code></h3> <p>Compute set difference of elements in last dimension of <code>a</code> and <code>b</code>.</p> <p>All but the last dimension of <code>a</code> and <code>b</code> must match.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>a</code>: <code>Tensor</code> or <code>SparseTensor</code> of the same type as <code>b</code>. If sparse, indices must be sorted in row-major order.</li> <li>\n<code>b</code>: <code>Tensor</code> or <code>SparseTensor</code> of the same type as <code>a</code>. Must be <code>SparseTensor</code> if <code>a</code> is <code>SparseTensor</code>. If sparse, indices must be sorted in row-major order.</li> <li>\n<code>aminusb</code>: Whether to subtract <code>b</code> from <code>a</code>, vs vice versa.</li> <li>\n<code>validate_indices</code>: Whether to validate the order and range of sparse indices in <code>a</code> and <code>b</code>.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>A <code>SparseTensor</code> with the same rank as <code>a</code> and <code>b</code>, and all but the last dimension the same. Elements along the last dimension contain the differences.</p>  <h3 id=\"set_intersection\"><code>tf.contrib.metrics.set_intersection(a, b, validate_indices=True)</code></h3> <p>Compute set intersection of elements in last dimension of <code>a</code> and <code>b</code>.</p> <p>All but the last dimension of <code>a</code> and <code>b</code> must match.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>a</code>: <code>Tensor</code> or <code>SparseTensor</code> of the same type as <code>b</code>. If sparse, indices must be sorted in row-major order.</li> <li>\n<code>b</code>: <code>Tensor</code> or <code>SparseTensor</code> of the same type as <code>a</code>. Must be <code>SparseTensor</code> if <code>a</code> is <code>SparseTensor</code>. If sparse, indices must be sorted in row-major order.</li> <li>\n<code>validate_indices</code>: Whether to validate the order and range of sparse indices in <code>a</code> and <code>b</code>.</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p>A <code>SparseTensor</code> with the same rank as <code>a</code> and <code>b</code>, and all but the last dimension the same. Elements along the last dimension contain the intersections.</p>  <h3 id=\"set_size\"><code>tf.contrib.metrics.set_size(a, validate_indices=True)</code></h3> <p>Compute number of unique elements along last dimension of <code>a</code>.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>a</code>: <code>SparseTensor</code>, with indices sorted in row-major order.</li> <li>\n<code>validate_indices</code>: Whether to validate the order and range of sparse indices in <code>a</code>.</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>For <code>a</code> ranked <code>n</code>, this is a <code>Tensor</code> with rank <code>n-1</code>, and the same 1st <code>n-1</code> dimensions as <code>a</code>. Each value is the number of unique elements in the corresponding <code>[0...n-1]</code> dimension of <code>a</code>.</p>  <h5 id=\"raises-17\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>a</code> is an invalid types.</li> </ul>  <h3 id=\"set_union\"><code>tf.contrib.metrics.set_union(a, b, validate_indices=True)</code></h3> <p>Compute set union of elements in last dimension of <code>a</code> and <code>b</code>.</p> <p>All but the last dimension of <code>a</code> and <code>b</code> must match.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>a</code>: <code>Tensor</code> or <code>SparseTensor</code> of the same type as <code>b</code>. If sparse, indices must be sorted in row-major order.</li> <li>\n<code>b</code>: <code>Tensor</code> or <code>SparseTensor</code> of the same type as <code>a</code>. Must be <code>SparseTensor</code> if <code>a</code> is <code>SparseTensor</code>. If sparse, indices must be sorted in row-major order.</li> <li>\n<code>validate_indices</code>: Whether to validate the order and range of sparse indices in <code>a</code> and <code>b</code>.</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>A <code>SparseTensor</code> with the same rank as <code>a</code> and <code>b</code>, and all but the last dimension the same. Elements along the last dimension contain the unions.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.metrics.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.metrics.html</a>\n  </p>\n</div>\n","contrib.learn":"<h1 id=\"learn-contrib\">Learn (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#learn-contrib\">Learn (contrib)</a></li> <ul> <li><a href=\"#estimators\">Estimators</a></li> <ul> <li><a href=\"#BaseEstimator\"><code>class tf.contrib.learn.BaseEstimator</code></a></li> <li><a href=\"#Estimator\"><code>class tf.contrib.learn.Estimator</code></a></li> <li><a href=\"#ModeKeys\"><code>class tf.contrib.learn.ModeKeys</code></a></li> <li><a href=\"#TensorFlowClassifier\"><code>class tf.contrib.learn.TensorFlowClassifier</code></a></li> <li><a href=\"#DNNClassifier\"><code>class tf.contrib.learn.DNNClassifier</code></a></li> <li><a href=\"#DNNRegressor\"><code>class tf.contrib.learn.DNNRegressor</code></a></li> <li><a href=\"#TensorFlowDNNClassifier\"><code>class tf.contrib.learn.TensorFlowDNNClassifier</code></a></li> <li><a href=\"#TensorFlowDNNRegressor\"><code>class tf.contrib.learn.TensorFlowDNNRegressor</code></a></li> <li><a href=\"#TensorFlowEstimator\"><code>class tf.contrib.learn.TensorFlowEstimator</code></a></li> <li><a href=\"#LinearClassifier\"><code>class tf.contrib.learn.LinearClassifier</code></a></li> <li><a href=\"#LinearRegressor\"><code>class tf.contrib.learn.LinearRegressor</code></a></li> <li><a href=\"#TensorFlowLinearClassifier\"><code>class tf.contrib.learn.TensorFlowLinearClassifier</code></a></li> <li><a href=\"#TensorFlowLinearRegressor\"><code>class tf.contrib.learn.TensorFlowLinearRegressor</code></a></li> <li><a href=\"#TensorFlowRNNClassifier\"><code>class tf.contrib.learn.TensorFlowRNNClassifier</code></a></li> <li><a href=\"#TensorFlowRNNRegressor\"><code>class tf.contrib.learn.TensorFlowRNNRegressor</code></a></li> <li><a href=\"#TensorFlowRegressor\"><code>class tf.contrib.learn.TensorFlowRegressor</code></a></li> </ul> <li><a href=\"#graph-actions\">Graph actions</a></li> <ul> <li><a href=\"#NanLossDuringTrainingError\"><code>class tf.contrib.learn.NanLossDuringTrainingError</code></a></li> <li><a href=\"#RunConfig\"><code>class tf.contrib.learn.RunConfig</code></a></li> <li><a href=\"#evaluate\"><code>tf.contrib.learn.evaluate(graph, output_dir, checkpoint_path, eval_dict, update_op=None, global_step_tensor=None, supervisor_master=, log_every_steps=10, feed_fn=None, max_steps=None)</code></a></li> <li><a href=\"#infer\"><code>tf.contrib.learn.infer(restore_checkpoint_path, output_dict, feed_dict=None)</code></a></li> <li><a href=\"#run_feeds\"><code>tf.contrib.learn.run_feeds(*args, **kwargs)</code></a></li> <li><a href=\"#run_n\"><code>tf.contrib.learn.run_n(output_dict, feed_dict=None, restore_checkpoint_path=None, n=1)</code></a></li> <li><a href=\"#train\"><code>tf.contrib.learn.train(graph, output_dir, train_op, loss_op, global_step_tensor=None, init_op=None, init_feed_dict=None, init_fn=None, log_every_steps=10, supervisor_is_chief=True, supervisor_master=, supervisor_save_model_secs=600, keep_checkpoint_max=5, supervisor_save_summaries_steps=100, feed_fn=None, steps=None, fail_on_nan_loss=True, monitors=None, max_steps=None)</code></a></li> </ul> <li><a href=\"#input-processing\">Input processing</a></li> <ul> <li><a href=\"#extract_dask_data\"><code>tf.contrib.learn.extract_dask_data(data)</code></a></li> <li><a href=\"#extract_dask_labels\"><code>tf.contrib.learn.extract_dask_labels(labels)</code></a></li> <li><a href=\"#extract_pandas_data\"><code>tf.contrib.learn.extract_pandas_data(data)</code></a></li> <li><a href=\"#extract_pandas_labels\"><code>tf.contrib.learn.extract_pandas_labels(labels)</code></a></li> <li><a href=\"#extract_pandas_matrix\"><code>tf.contrib.learn.extract_pandas_matrix(data)</code></a></li> <li><a href=\"#read_batch_examples\"><code>tf.contrib.learn.read_batch_examples(file_pattern, batch_size, reader, randomize_input=True, num_epochs=None, queue_capacity=10000, num_threads=1, read_batch_size=1, parse_fn=None, name=None)</code></a></li> <li><a href=\"#read_batch_features\"><code>tf.contrib.learn.read_batch_features(file_pattern, batch_size, features, reader, randomize_input=True, num_epochs=None, queue_capacity=10000, reader_num_threads=1, parser_num_threads=1, name=None)</code></a></li> <li><a href=\"#read_batch_record_features\"><code>tf.contrib.learn.read_batch_record_features(file_pattern, batch_size, features, randomize_input=True, num_epochs=None, queue_capacity=10000, reader_num_threads=1, parser_num_threads=1, name=dequeue_record_examples)</code></a></li> </ul>\n</ul>\n</ul> </div> <p>High level API for learning with TensorFlow.</p> <h2 id=\"estimators\">Estimators</h2> <p>Train and evaluate TensorFlow models.</p>  <h3 id=\"BaseEstimator\"><code>class tf.contrib.learn.BaseEstimator</code></h3> <p>Abstract BaseEstimator class to train and evaluate TensorFlow models.</p> <p>Concrete implementation of this class should provide the following functions:</p> <ul> <li>_get_train_ops</li> <li>_get_eval_ops</li> <li>_get_predict_ops</li> </ul> <p><code>Estimator</code> implemented below is a good example of how to use this class.</p>  <h4 id=\"BaseEstimator.__init__\"><code>tf.contrib.learn.BaseEstimator.__init__(model_dir=None, config=None)</code></h4> <p>Initializes a BaseEstimator instance.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>model_dir</code>: Directory to save model parameters, graph and etc. This can also be used to load checkpoints from the directory into a estimator to continue training a previously saved model.</li> <li>\n<code>config</code>: A RunConfig instance.</li> </ul>  <h4 id=\"BaseEstimator.evaluate\"><code>tf.contrib.learn.BaseEstimator.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"BaseEstimator.fit\"><code>tf.contrib.learn.BaseEstimator.fit(x=None, y=None, input_fn=None, steps=None, batch_size=None, monitors=None, max_steps=None)</code></h4> <p>See <code>Trainable</code>.</p>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>x</code> or <code>y</code> are not <code>None</code> while <code>input_fn</code> is not <code>None</code>.</li> <li>\n<code>ValueError</code>: If both <code>steps</code> and <code>max_steps</code> are not <code>None</code>.</li> </ul>  <h4 id=\"BaseEstimator.get_params\"><code>tf.contrib.learn.BaseEstimator.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul> <h5 id=\"returns\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"BaseEstimator.get_variable_names\"><code>tf.contrib.learn.BaseEstimator.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-2\">Returns:</h5> <p>List of names.</p>  <h4 id=\"BaseEstimator.get_variable_value\"><code>tf.contrib.learn.BaseEstimator.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"BaseEstimator.model_dir\"><code>tf.contrib.learn.BaseEstimator.model_dir</code></h4>  <h4 id=\"BaseEstimator.partial_fit\"><code>tf.contrib.learn.BaseEstimator.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"BaseEstimator.predict\"><code>tf.contrib.learn.BaseEstimator.predict(x=None, input_fn=None, batch_size=None, outputs=None, as_iterable=False)</code></h4> <p>Returns predictions for given features.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code> and 'batch_size' must be <code>None</code>.</li> <li>\n<code>batch_size</code>: Override default batch size. If set, 'input_fn' must be 'None'.</li> <li>\n<code>outputs</code>: list of <code>str</code>, name of the output to predict. If <code>None</code>, returns all.</li> <li>\n<code>as_iterable</code>: If True, return an iterable which keeps yielding predictions for each example until inputs are exhausted. Note: The inputs must terminate if you want the iterable to terminate (e.g. be sure to pass num_epochs=1 if you are using something like read_batch_features).</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p>A numpy array of predicted classes or regression values if the constructor's <code>model_fn</code> returns a <code>Tensor</code> for <code>predictions</code> or a <code>dict</code> of numpy arrays if <code>model_fn</code> returns a <code>dict</code>. Returns an iterable of predictions if as_iterable is True.</p>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If x and input_fn are both provided or both <code>None</code>.</li> </ul>  <h4 id=\"BaseEstimator.set_params\"><code>tf.contrib.learn.BaseEstimator.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p>self</p>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h3 id=\"Estimator\"><code>class tf.contrib.learn.Estimator</code></h3> <p>Estimator class is the basic TensorFlow model trainer/evaluator.</p>  <h4 id=\"Estimator.__init__\"><code>tf.contrib.learn.Estimator.__init__(model_fn=None, model_dir=None, config=None, params=None)</code></h4> <p>Constructs an Estimator instance.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<p><code>model_fn</code>: Model function, takes features and targets tensors or dicts of tensors and returns predictions and loss tensors. Supports next three signatures for the function:</p> <ul> <li><code>(features, targets) -&gt; (predictions, loss, train_op)</code></li> <li><code>(features, targets, mode) -&gt; (predictions, loss, train_op)</code></li> <li><code>(features, targets, mode, params) -&gt; (predictions, loss, train_op)</code></li> </ul> <p>Where</p> <ul> <li>\n<code>features</code> are single <code>Tensor</code> or <code>dict</code> of <code>Tensor</code>s (depending on data passed to <code>fit</code>),</li> <li>\n<code>targets</code> are <code>Tensor</code> or <code>dict</code> of <code>Tensor</code>s (for multi-head models). If mode is <code>ModeKeys.INFER</code>, <code>targets=None</code> will be passed. If the <code>model_fn</code>'s signature does not accept <code>mode</code>, the <code>model_fn</code> must still be able to handle <code>targets=None</code>.</li> <li>\n<code>mode</code> represents if this training, evaluation or prediction. See <code>ModeKeys</code>.</li> <li>\n<code>params</code> is a <code>dict</code> of hyperparameters. Will receive what is passed to Estimator in <code>params</code> parameter. This allows to configure Estimators from hyper parameter tunning.</li> </ul>\n</li> <li><p><code>model_dir</code>: Directory to save model parameters, graph and etc. This can also be used to load checkpoints from the directory into a estimator to continue training a previously saved model.</p></li> <li><p><code>config</code>: Configuration object.</p></li> <li><p><code>params</code>: <code>dict</code> of hyper parameters that will be passed into <code>model_fn</code>. Keys are names of parameters, values are basic python types.</p></li> </ul>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: parameters of <code>model_fn</code> don't match <code>params</code>.</li> </ul>  <h4 id=\"Estimator.evaluate\"><code>tf.contrib.learn.Estimator.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"Estimator.fit\"><code>tf.contrib.learn.Estimator.fit(x=None, y=None, input_fn=None, steps=None, batch_size=None, monitors=None, max_steps=None)</code></h4> <p>See <code>Trainable</code>.</p>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>x</code> or <code>y</code> are not <code>None</code> while <code>input_fn</code> is not <code>None</code>.</li> <li>\n<code>ValueError</code>: If both <code>steps</code> and <code>max_steps</code> are not <code>None</code>.</li> </ul>  <h4 id=\"Estimator.get_params\"><code>tf.contrib.learn.Estimator.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-7\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"Estimator.get_variable_names\"><code>tf.contrib.learn.Estimator.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-8\">Returns:</h5> <p>List of names.</p>  <h4 id=\"Estimator.get_variable_value\"><code>tf.contrib.learn.Estimator.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"Estimator.model_dir\"><code>tf.contrib.learn.Estimator.model_dir</code></h4>  <h4 id=\"Estimator.partial_fit\"><code>tf.contrib.learn.Estimator.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"Estimator.predict\"><code>tf.contrib.learn.Estimator.predict(x=None, input_fn=None, batch_size=None, outputs=None, as_iterable=False)</code></h4> <p>Returns predictions for given features.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code> and 'batch_size' must be <code>None</code>.</li> <li>\n<code>batch_size</code>: Override default batch size. If set, 'input_fn' must be 'None'.</li> <li>\n<code>outputs</code>: list of <code>str</code>, name of the output to predict. If <code>None</code>, returns all.</li> <li>\n<code>as_iterable</code>: If True, return an iterable which keeps yielding predictions for each example until inputs are exhausted. Note: The inputs must terminate if you want the iterable to terminate (e.g. be sure to pass num_epochs=1 if you are using something like read_batch_features).</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>A numpy array of predicted classes or regression values if the constructor's <code>model_fn</code> returns a <code>Tensor</code> for <code>predictions</code> or a <code>dict</code> of numpy arrays if <code>model_fn</code> returns a <code>dict</code>. Returns an iterable of predictions if as_iterable is True.</p>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If x and input_fn are both provided or both <code>None</code>.</li> </ul>  <h4 id=\"Estimator.set_params\"><code>tf.contrib.learn.Estimator.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <p>self</p>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h3 id=\"ModeKeys\"><code>class tf.contrib.learn.ModeKeys</code></h3> <p>Standard names for model modes.</p> <p>The following standard keys are defined:</p> <ul> <li>\n<code>TRAIN</code>: training mode.</li> <li>\n<code>EVAL</code>: evaluation mode.</li> <li>\n<code>INFER</code>: inference mode.</li> </ul>  <h3 id=\"TensorFlowClassifier\"><code>class tf.contrib.learn.TensorFlowClassifier</code></h3>  <h4 id=\"TensorFlowClassifier.__init__\"><code>tf.contrib.learn.TensorFlowClassifier.__init__(*args, **kwargs)</code></h4>  <h4 id=\"TensorFlowClassifier.bias_\"><code>tf.contrib.learn.TensorFlowClassifier.bias_</code></h4>  <h4 id=\"TensorFlowClassifier.dnn_bias_\"><code>tf.contrib.learn.TensorFlowClassifier.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"TensorFlowClassifier.dnn_weights_\"><code>tf.contrib.learn.TensorFlowClassifier.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"TensorFlowClassifier.evaluate\"><code>tf.contrib.learn.TensorFlowClassifier.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"TensorFlowClassifier.fit\"><code>tf.contrib.learn.TensorFlowClassifier.fit(x, y, steps=None, batch_size=None, monitors=None, logdir=None)</code></h4>  <h4 id=\"TensorFlowClassifier.get_params\"><code>tf.contrib.learn.TensorFlowClassifier.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowClassifier.get_variable_names\"><code>tf.contrib.learn.TensorFlowClassifier.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-14\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowClassifier.get_variable_value\"><code>tf.contrib.learn.TensorFlowClassifier.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowClassifier.linear_bias_\"><code>tf.contrib.learn.TensorFlowClassifier.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"TensorFlowClassifier.linear_weights_\"><code>tf.contrib.learn.TensorFlowClassifier.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"TensorFlowClassifier.model_dir\"><code>tf.contrib.learn.TensorFlowClassifier.model_dir</code></h4>  <h4 id=\"TensorFlowClassifier.partial_fit\"><code>tf.contrib.learn.TensorFlowClassifier.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"TensorFlowClassifier.predict\"><code>tf.contrib.learn.TensorFlowClassifier.predict(x=None, input_fn=None, batch_size=None, outputs=None, axis=1)</code></h4> <p>Predict class or regression for <code>x</code>.</p>  <h4 id=\"TensorFlowClassifier.predict_proba\"><code>tf.contrib.learn.TensorFlowClassifier.predict_proba(x=None, input_fn=None, batch_size=None, outputs=None)</code></h4>  <h4 id=\"TensorFlowClassifier.save\"><code>tf.contrib.learn.TensorFlowClassifier.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowClassifier.set_params\"><code>tf.contrib.learn.TensorFlowClassifier.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <p>self</p>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"TensorFlowClassifier.weights_\"><code>tf.contrib.learn.TensorFlowClassifier.weights_</code></h4>  <h3 id=\"DNNClassifier\"><code>class tf.contrib.learn.DNNClassifier</code></h3> <p>A classifier for TensorFlow DNN models.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">education = sparse_column_with_hash_bucket(column_name=\"education\",\n                                           hash_bucket_size=1000)\noccupation = sparse_column_with_hash_bucket(column_name=\"occupation\",\n                                            hash_bucket_size=1000)\n\neducation_emb = embedding_column(sparse_id_column=education, dimension=16,\n                                 combiner=\"sum\")\noccupation_emb = embedding_column(sparse_id_column=occupation, dimension=16,\n                                 combiner=\"sum\")\n\nestimator = DNNClassifier(\n    feature_columns=[education_emb, occupation_emb],\n    hidden_units=[1024, 512, 256])\n\n# Or estimator using the ProximalAdagradOptimizer optimizer with\n# regularization.\nestimator = DNNClassifier(\n    feature_columns=[education_emb, occupation_emb],\n    hidden_units=[1024, 512, 256],\n    optimizer=tf.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001\n    ))\n\n# Input builders\ndef input_fn_train: # returns x, Y\n  pass\nestimator.fit(input_fn=input_fn_train)\n\ndef input_fn_eval: # returns x, Y\n  pass\nestimator.evaluate(input_fn=input_fn_eval)\nestimator.predict(x=x)\n</pre> <p>Input of <code>fit</code> and <code>evaluate</code> should have following features, otherwise there will be a <code>KeyError</code>:</p> <ul> <li>if <code>weight_column_name</code> is not <code>None</code>, a feature with <code>key=weight_column_name</code> whose value is a <code>Tensor</code>.</li> <li>for each <code>column</code> in <code>feature_columns</code>: <ul> <li>if <code>column</code> is a <code>SparseColumn</code>, a feature with <code>key=column.name</code> whose <code>value</code> is a <code>SparseTensor</code>.</li> <li>if <code>column</code> is a <code>WeightedSparseColumn</code>, two features: the first with <code>key</code> the id column name, the second with <code>key</code> the weight column name. Both features' <code>value</code> must be a <code>SparseTensor</code>.</li> <li>if <code>column</code> is a <code>RealValuedColumn</code>, a feature with <code>key=column.name</code> whose <code>value</code> is a <code>Tensor</code>.</li> <li>if <code>feature_columns</code> is <code>None</code>, then <code>input</code> must contain only real valued <code>Tensor</code>. - - -</li> </ul>\n</li> </ul> <h4 id=\"DNNClassifier.__init__\"><code>tf.contrib.learn.DNNClassifier.__init__(hidden_units, feature_columns=None, model_dir=None, n_classes=2, weight_column_name=None, optimizer=None, activation_fn=relu, dropout=None, gradient_clip_norm=None, enable_centered_bias=True, config=None)</code></h4> <p>Initializes a DNNClassifier instance.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>hidden_units</code>: List of hidden units per layer. All layers are fully connected. Ex. <code>[64, 32]</code> means first layer has 64 nodes and second one has 32.</li> <li>\n<code>feature_columns</code>: An iterable containing all the feature columns used by the model. All items in the set should be instances of classes derived from <code>FeatureColumn</code>.</li> <li>\n<code>model_dir</code>: Directory to save model parameters, graph and etc. This can also be used to load checkpoints from the directory into a estimator to continue training a previously saved model.</li> <li>\n<code>n_classes</code>: number of target classes. Default is binary classification. It must be greater than 1.</li> <li>\n<code>weight_column_name</code>: A string defining feature column name representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example.</li> <li>\n<code>optimizer</code>: An instance of <code>tf.Optimizer</code> used to train the model. If <code>None</code>, will use an Adagrad optimizer.</li> <li>\n<code>activation_fn</code>: Activation function applied to each layer. If <code>None</code>, will use <code>tf.nn.relu</code>.</li> <li>\n<code>dropout</code>: When not <code>None</code>, the probability we will drop out a given coordinate.</li> <li>\n<code>gradient_clip_norm</code>: A float &gt; 0. If provided, gradients are clipped to their global norm with this clipping ratio. See tf.clip_by_global_norm for more details.</li> <li>\n<code>enable_centered_bias</code>: A bool. If True, estimator will learn a centered bias variable for each class. Rest of the model structure learns the residual after centered bias.</li> <li>\n<code>config</code>: <code>RunConfig</code> object to configure the runtime settings.</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>A <code>DNNClassifier</code> estimator.</p>  <h4 id=\"DNNClassifier.bias_\"><code>tf.contrib.learn.DNNClassifier.bias_</code></h4>  <h4 id=\"DNNClassifier.dnn_bias_\"><code>tf.contrib.learn.DNNClassifier.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"DNNClassifier.dnn_weights_\"><code>tf.contrib.learn.DNNClassifier.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"DNNClassifier.evaluate\"><code>tf.contrib.learn.DNNClassifier.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"DNNClassifier.fit\"><code>tf.contrib.learn.DNNClassifier.fit(x=None, y=None, input_fn=None, steps=None, batch_size=None, monitors=None, max_steps=None)</code></h4> <p>See <code>Trainable</code>.</p>  <h5 id=\"raises-16\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>x</code> or <code>y</code> are not <code>None</code> while <code>input_fn</code> is not <code>None</code>.</li> <li>\n<code>ValueError</code>: If both <code>steps</code> and <code>max_steps</code> are not <code>None</code>.</li> </ul>  <h4 id=\"DNNClassifier.get_params\"><code>tf.contrib.learn.DNNClassifier.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"DNNClassifier.get_variable_names\"><code>tf.contrib.learn.DNNClassifier.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-20\">Returns:</h5> <p>List of names.</p>  <h4 id=\"DNNClassifier.get_variable_value\"><code>tf.contrib.learn.DNNClassifier.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"DNNClassifier.linear_bias_\"><code>tf.contrib.learn.DNNClassifier.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"DNNClassifier.linear_weights_\"><code>tf.contrib.learn.DNNClassifier.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"DNNClassifier.model_dir\"><code>tf.contrib.learn.DNNClassifier.model_dir</code></h4>  <h4 id=\"DNNClassifier.partial_fit\"><code>tf.contrib.learn.DNNClassifier.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-17\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"DNNClassifier.predict\"><code>tf.contrib.learn.DNNClassifier.predict(x=None, input_fn=None, batch_size=None, as_iterable=False)</code></h4> <p>Returns predicted classes for given features.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>x</code>: features.</li> <li>\n<code>input_fn</code>: Input function. If set, x must be None.</li> <li>\n<code>batch_size</code>: Override default batch size.</li> <li>\n<code>as_iterable</code>: If True, return an iterable which keeps yielding predictions for each example until inputs are exhausted. Note: The inputs must terminate if you want the iterable to terminate (e.g. be sure to pass num_epochs=1 if you are using something like read_batch_features).</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p>Numpy array of predicted classes (or an iterable of predicted classes if as_iterable is True).</p>  <h4 id=\"DNNClassifier.predict_proba\"><code>tf.contrib.learn.DNNClassifier.predict_proba(x=None, input_fn=None, batch_size=None, as_iterable=False)</code></h4> <p>Returns prediction probabilities for given features.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>x</code>: features.</li> <li>\n<code>input_fn</code>: Input function. If set, x and y must be None.</li> <li>\n<code>batch_size</code>: Override default batch size.</li> <li>\n<code>as_iterable</code>: If True, return an iterable which keeps yielding predictions for each example until inputs are exhausted. Note: The inputs must terminate if you want the iterable to terminate (e.g. be sure to pass num_epochs=1 if you are using something like read_batch_features).</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p>Numpy array of predicted probabilities (or an iterable of predicted probabilities if as_iterable is True).</p>  <h4 id=\"DNNClassifier.set_params\"><code>tf.contrib.learn.DNNClassifier.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-25\">Returns:</h5> <p>self</p>  <h5 id=\"raises-18\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"DNNClassifier.weights_\"><code>tf.contrib.learn.DNNClassifier.weights_</code></h4>  <h3 id=\"DNNRegressor\"><code>class tf.contrib.learn.DNNRegressor</code></h3> <p>A regressor for TensorFlow DNN models.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">education = sparse_column_with_hash_bucket(column_name=\"education\",\n                                           hash_bucket_size=1000)\noccupation = sparse_column_with_hash_bucket(column_name=\"occupation\",\n                                            hash_bucket_size=1000)\n\neducation_emb = embedding_column(sparse_id_column=education, dimension=16,\n                                 combiner=\"sum\")\noccupation_emb = embedding_column(sparse_id_column=occupation, dimension=16,\n                                 combiner=\"sum\")\n\nestimator = DNNRegressor(\n    feature_columns=[education_emb, occupation_emb],\n    hidden_units=[1024, 512, 256])\n\n# Or estimator using the ProximalAdagradOptimizer optimizer with\n# regularization.\nestimator = DNNRegressor(\n    feature_columns=[education_emb, occupation_emb],\n    hidden_units=[1024, 512, 256],\n    optimizer=tf.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001\n    ))\n\n# Input builders\ndef input_fn_train: # returns x, Y\n  pass\nestimator.fit(input_fn=input_fn_train)\n\ndef input_fn_eval: # returns x, Y\n  pass\nestimator.evaluate(input_fn=input_fn_eval)\nestimator.predict(x=x)\n</pre> <p>Input of <code>fit</code> and <code>evaluate</code> should have following features, otherwise there will be a <code>KeyError</code>:</p> <ul> <li>if <code>weight_column_name</code> is not <code>None</code>, a feature with <code>key=weight_column_name</code> whose value is a <code>Tensor</code>.</li> <li>for each <code>column</code> in <code>feature_columns</code>: <ul> <li>if <code>column</code> is a <code>SparseColumn</code>, a feature with <code>key=column.name</code> whose <code>value</code> is a <code>SparseTensor</code>.</li> <li>if <code>column</code> is a <code>WeightedSparseColumn</code>, two features: the first with <code>key</code> the id column name, the second with <code>key</code> the weight column name. Both features' <code>value</code> must be a <code>SparseTensor</code>.</li> <li>if <code>column</code> is a <code>RealValuedColumn</code>, a feature with <code>key=column.name</code> whose <code>value</code> is a <code>Tensor</code>.</li> <li>if <code>feature_columns</code> is <code>None</code>, then <code>input</code> must contain only real valued <code>Tensor</code>. - - -</li> </ul>\n</li> </ul> <h4 id=\"DNNRegressor.__init__\"><code>tf.contrib.learn.DNNRegressor.__init__(hidden_units, feature_columns=None, model_dir=None, weight_column_name=None, optimizer=None, activation_fn=relu, dropout=None, gradient_clip_norm=None, enable_centered_bias=True, config=None)</code></h4> <p>Initializes a <code>DNNRegressor</code> instance.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>hidden_units</code>: List of hidden units per layer. All layers are fully connected. Ex. <code>[64, 32]</code> means first layer has 64 nodes and second one has 32.</li> <li>\n<code>feature_columns</code>: An iterable containing all the feature columns used by the model. All items in the set should be instances of classes derived from <code>FeatureColumn</code>.</li> <li>\n<code>model_dir</code>: Directory to save model parameters, graph and etc. This can also be used to load checkpoints from the directory into a estimator to continue training a previously saved model.</li> <li>\n<code>weight_column_name</code>: A string defining feature column name representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example.</li> <li>\n<code>optimizer</code>: An instance of <code>tf.Optimizer</code> used to train the model. If <code>None</code>, will use an Adagrad optimizer.</li> <li>\n<code>activation_fn</code>: Activation function applied to each layer. If <code>None</code>, will use <code>tf.nn.relu</code>.</li> <li>\n<code>dropout</code>: When not <code>None</code>, the probability we will drop out a given coordinate.</li> <li>\n<code>gradient_clip_norm</code>: A <code>float</code> &gt; 0. If provided, gradients are clipped to their global norm with this clipping ratio. See <code>tf.clip_by_global_norm</code> for more details.</li> <li>\n<code>enable_centered_bias</code>: A bool. If True, estimator will learn a centered bias variable for each class. Rest of the model structure learns the residual after centered bias.</li> <li>\n<code>config</code>: <code>RunConfig</code> object to configure the runtime settings.</li> </ul>  <h5 id=\"returns-26\">Returns:</h5> <p>A <code>DNNRegressor</code> estimator.</p>  <h4 id=\"DNNRegressor.bias_\"><code>tf.contrib.learn.DNNRegressor.bias_</code></h4>  <h4 id=\"DNNRegressor.dnn_bias_\"><code>tf.contrib.learn.DNNRegressor.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"DNNRegressor.dnn_weights_\"><code>tf.contrib.learn.DNNRegressor.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"DNNRegressor.evaluate\"><code>tf.contrib.learn.DNNRegressor.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-19\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"DNNRegressor.fit\"><code>tf.contrib.learn.DNNRegressor.fit(x=None, y=None, input_fn=None, steps=None, batch_size=None, monitors=None, max_steps=None)</code></h4> <p>See <code>Trainable</code>.</p>  <h5 id=\"raises-20\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>x</code> or <code>y</code> are not <code>None</code> while <code>input_fn</code> is not <code>None</code>.</li> <li>\n<code>ValueError</code>: If both <code>steps</code> and <code>max_steps</code> are not <code>None</code>.</li> </ul>  <h4 id=\"DNNRegressor.get_params\"><code>tf.contrib.learn.DNNRegressor.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"DNNRegressor.get_variable_names\"><code>tf.contrib.learn.DNNRegressor.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-28\">Returns:</h5> <p>List of names.</p>  <h4 id=\"DNNRegressor.get_variable_value\"><code>tf.contrib.learn.DNNRegressor.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"DNNRegressor.linear_bias_\"><code>tf.contrib.learn.DNNRegressor.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"DNNRegressor.linear_weights_\"><code>tf.contrib.learn.DNNRegressor.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"DNNRegressor.model_dir\"><code>tf.contrib.learn.DNNRegressor.model_dir</code></h4>  <h4 id=\"DNNRegressor.partial_fit\"><code>tf.contrib.learn.DNNRegressor.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-21\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"DNNRegressor.predict\"><code>tf.contrib.learn.DNNRegressor.predict(x=None, input_fn=None, batch_size=None, outputs=None, as_iterable=False)</code></h4> <p>Returns predictions for given features.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code> and 'batch_size' must be <code>None</code>.</li> <li>\n<code>batch_size</code>: Override default batch size. If set, 'input_fn' must be 'None'.</li> <li>\n<code>outputs</code>: list of <code>str</code>, name of the output to predict. If <code>None</code>, returns all.</li> <li>\n<code>as_iterable</code>: If True, return an iterable which keeps yielding predictions for each example until inputs are exhausted. Note: The inputs must terminate if you want the iterable to terminate (e.g. be sure to pass num_epochs=1 if you are using something like read_batch_features).</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <p>A numpy array of predicted classes or regression values if the constructor's <code>model_fn</code> returns a <code>Tensor</code> for <code>predictions</code> or a <code>dict</code> of numpy arrays if <code>model_fn</code> returns a <code>dict</code>. Returns an iterable of predictions if as_iterable is True.</p>  <h5 id=\"raises-22\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If x and input_fn are both provided or both <code>None</code>.</li> </ul>  <h4 id=\"DNNRegressor.set_params\"><code>tf.contrib.learn.DNNRegressor.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>self</p>  <h5 id=\"raises-23\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"DNNRegressor.weights_\"><code>tf.contrib.learn.DNNRegressor.weights_</code></h4>  <h3 id=\"TensorFlowDNNClassifier\"><code>class tf.contrib.learn.TensorFlowDNNClassifier</code></h3>  <h4 id=\"TensorFlowDNNClassifier.__init__\"><code>tf.contrib.learn.TensorFlowDNNClassifier.__init__(*args, **kwargs)</code></h4>  <h4 id=\"TensorFlowDNNClassifier.bias_\"><code>tf.contrib.learn.TensorFlowDNNClassifier.bias_</code></h4>  <h4 id=\"TensorFlowDNNClassifier.dnn_bias_\"><code>tf.contrib.learn.TensorFlowDNNClassifier.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"TensorFlowDNNClassifier.dnn_weights_\"><code>tf.contrib.learn.TensorFlowDNNClassifier.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"TensorFlowDNNClassifier.evaluate\"><code>tf.contrib.learn.TensorFlowDNNClassifier.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-24\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"TensorFlowDNNClassifier.fit\"><code>tf.contrib.learn.TensorFlowDNNClassifier.fit(x, y, steps=None, batch_size=None, monitors=None, logdir=None)</code></h4>  <h4 id=\"TensorFlowDNNClassifier.get_params\"><code>tf.contrib.learn.TensorFlowDNNClassifier.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowDNNClassifier.get_variable_names\"><code>tf.contrib.learn.TensorFlowDNNClassifier.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-34\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowDNNClassifier.get_variable_value\"><code>tf.contrib.learn.TensorFlowDNNClassifier.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowDNNClassifier.linear_bias_\"><code>tf.contrib.learn.TensorFlowDNNClassifier.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"TensorFlowDNNClassifier.linear_weights_\"><code>tf.contrib.learn.TensorFlowDNNClassifier.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"TensorFlowDNNClassifier.model_dir\"><code>tf.contrib.learn.TensorFlowDNNClassifier.model_dir</code></h4>  <h4 id=\"TensorFlowDNNClassifier.partial_fit\"><code>tf.contrib.learn.TensorFlowDNNClassifier.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-25\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"TensorFlowDNNClassifier.predict\"><code>tf.contrib.learn.TensorFlowDNNClassifier.predict(x=None, input_fn=None, batch_size=None, outputs=None, axis=1)</code></h4> <p>Predict class or regression for <code>x</code>.</p>  <h4 id=\"TensorFlowDNNClassifier.predict_proba\"><code>tf.contrib.learn.TensorFlowDNNClassifier.predict_proba(x=None, input_fn=None, batch_size=None, outputs=None)</code></h4>  <h4 id=\"TensorFlowDNNClassifier.save\"><code>tf.contrib.learn.TensorFlowDNNClassifier.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowDNNClassifier.set_params\"><code>tf.contrib.learn.TensorFlowDNNClassifier.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <p>self</p>  <h5 id=\"raises-26\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"TensorFlowDNNClassifier.weights_\"><code>tf.contrib.learn.TensorFlowDNNClassifier.weights_</code></h4>  <h3 id=\"TensorFlowDNNRegressor\"><code>class tf.contrib.learn.TensorFlowDNNRegressor</code></h3>  <h4 id=\"TensorFlowDNNRegressor.__init__\"><code>tf.contrib.learn.TensorFlowDNNRegressor.__init__(*args, **kwargs)</code></h4>  <h4 id=\"TensorFlowDNNRegressor.bias_\"><code>tf.contrib.learn.TensorFlowDNNRegressor.bias_</code></h4>  <h4 id=\"TensorFlowDNNRegressor.dnn_bias_\"><code>tf.contrib.learn.TensorFlowDNNRegressor.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"TensorFlowDNNRegressor.dnn_weights_\"><code>tf.contrib.learn.TensorFlowDNNRegressor.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"TensorFlowDNNRegressor.evaluate\"><code>tf.contrib.learn.TensorFlowDNNRegressor.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-27\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"TensorFlowDNNRegressor.fit\"><code>tf.contrib.learn.TensorFlowDNNRegressor.fit(x, y, steps=None, batch_size=None, monitors=None, logdir=None)</code></h4>  <h4 id=\"TensorFlowDNNRegressor.get_params\"><code>tf.contrib.learn.TensorFlowDNNRegressor.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowDNNRegressor.get_variable_names\"><code>tf.contrib.learn.TensorFlowDNNRegressor.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-39\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowDNNRegressor.get_variable_value\"><code>tf.contrib.learn.TensorFlowDNNRegressor.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-40\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowDNNRegressor.linear_bias_\"><code>tf.contrib.learn.TensorFlowDNNRegressor.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"TensorFlowDNNRegressor.linear_weights_\"><code>tf.contrib.learn.TensorFlowDNNRegressor.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"TensorFlowDNNRegressor.model_dir\"><code>tf.contrib.learn.TensorFlowDNNRegressor.model_dir</code></h4>  <h4 id=\"TensorFlowDNNRegressor.partial_fit\"><code>tf.contrib.learn.TensorFlowDNNRegressor.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-41\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-28\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"TensorFlowDNNRegressor.predict\"><code>tf.contrib.learn.TensorFlowDNNRegressor.predict(x=None, input_fn=None, batch_size=None, outputs=None, axis=1)</code></h4> <p>Predict class or regression for <code>x</code>.</p>  <h4 id=\"TensorFlowDNNRegressor.predict_proba\"><code>tf.contrib.learn.TensorFlowDNNRegressor.predict_proba(x=None, input_fn=None, batch_size=None, outputs=None)</code></h4>  <h4 id=\"TensorFlowDNNRegressor.save\"><code>tf.contrib.learn.TensorFlowDNNRegressor.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowDNNRegressor.set_params\"><code>tf.contrib.learn.TensorFlowDNNRegressor.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-42\">Returns:</h5> <p>self</p>  <h5 id=\"raises-29\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"TensorFlowDNNRegressor.weights_\"><code>tf.contrib.learn.TensorFlowDNNRegressor.weights_</code></h4>  <h3 id=\"TensorFlowEstimator\"><code>class tf.contrib.learn.TensorFlowEstimator</code></h3> <p>Base class for all TensorFlow estimators.</p>  <h4 id=\"TensorFlowEstimator.__init__\"><code>tf.contrib.learn.TensorFlowEstimator.__init__(model_fn, n_classes, batch_size=32, steps=200, optimizer='Adagrad', learning_rate=0.1, clip_gradients=5.0, class_weight=None, continue_training=False, config=None, verbose=1)</code></h4> <p>Initializes a TensorFlowEstimator instance.</p>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>model_fn</code>: Model function, that takes input <code>x</code>, <code>y</code> tensors and outputs prediction and loss tensors.</li> <li>\n<code>n_classes</code>: Number of classes in the target.</li> <li>\n<code>batch_size</code>: Mini batch size.</li> <li>\n<code>steps</code>: Number of steps to run over data.</li> <li>\n<code>optimizer</code>: Optimizer name (or class), for example \"SGD\", \"Adam\", \"Adagrad\".</li> <li>\n<p><code>learning_rate</code>: If this is constant float value, no decay function is used. Instead, a customized decay function can be passed that accepts global_step as parameter and returns a Tensor. e.g. exponential decay function:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def exp_decay(global_step):\n    return tf.train.exponential_decay(\n        learning_rate=0.1, global_step,\n        decay_steps=2, decay_rate=0.001)\n</pre>\n</li> <li><p><code>clip_gradients</code>: Clip norm of the gradients to this value to stop gradient explosion.</p></li> <li><p><code>class_weight</code>: None or list of n_classes floats. Weight associated with classes for loss computation. If not given, all classes are supposed to have weight one.</p></li> <li><p><code>continue_training</code>: when continue_training is True, once initialized model will be continuely trained on every call of fit.</p></li> <li><p><code>config</code>: RunConfig object that controls the configurations of the session, e.g. num_cores, gpu_memory_fraction, etc.</p></li> <li>\n<p><code>verbose</code>: Controls the verbosity, possible values:</p> <ul> <li>0: the algorithm and debug information is muted.</li> <li>1: trainer prints the progress.</li> <li>2: log device placement is printed.</li> </ul>\n</li> </ul>  <h4 id=\"TensorFlowEstimator.evaluate\"><code>tf.contrib.learn.TensorFlowEstimator.evaluate(x=None, y=None, input_fn=None, steps=None)</code></h4> <p>See base class.</p>  <h4 id=\"TensorFlowEstimator.fit\"><code>tf.contrib.learn.TensorFlowEstimator.fit(x, y, steps=None, monitors=None, logdir=None)</code></h4> <p>Neural network model from provided <code>model_fn</code> and training data.</p> <p>Note: called first time constructs the graph and initializers variables. Consecutives times it will continue training the same model. This logic follows partial_fit() interface in scikit-learn. To restart learning, create new estimator.</p>  <h5 id=\"args-42\">Args:</h5> <ul> <li><p><code>x</code>: matrix or tensor of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model.</p></li> <li><p><code>y</code>: vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression).</p></li> <li><p><code>steps</code>: int, number of steps to train. If None or 0, train for <code>self.steps</code>.</p></li> <li><p><code>monitors</code>: List of <code>BaseMonitor</code> objects to print training progress and invoke early stopping.</p></li> <li><p><code>logdir</code>: the directory to save the log file that can be used for optional visualization.</p></li> </ul>  <h5 id=\"returns-43\">Returns:</h5> <p>Returns self.</p>  <h4 id=\"TensorFlowEstimator.get_params\"><code>tf.contrib.learn.TensorFlowEstimator.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-43\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-44\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowEstimator.get_tensor\"><code>tf.contrib.learn.TensorFlowEstimator.get_tensor(name)</code></h4> <p>Returns tensor by name.</p>  <h5 id=\"args-44\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-45\">Returns:</h5> <p>Tensor.</p>  <h4 id=\"TensorFlowEstimator.get_variable_names\"><code>tf.contrib.learn.TensorFlowEstimator.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-46\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowEstimator.get_variable_value\"><code>tf.contrib.learn.TensorFlowEstimator.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-45\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-47\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowEstimator.model_dir\"><code>tf.contrib.learn.TensorFlowEstimator.model_dir</code></h4>  <h4 id=\"TensorFlowEstimator.partial_fit\"><code>tf.contrib.learn.TensorFlowEstimator.partial_fit(x, y)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training. This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-46\">Args:</h5> <ul> <li><p><code>x</code>: matrix or tensor of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model.</p></li> <li><p><code>y</code>: vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class label in classification, real numbers in regression).</p></li> </ul>  <h5 id=\"returns-48\">Returns:</h5> <p>Returns self.</p>  <h4 id=\"TensorFlowEstimator.predict\"><code>tf.contrib.learn.TensorFlowEstimator.predict(x, axis=1, batch_size=None)</code></h4> <p>Predict class or regression for <code>x</code>.</p> <p>For a classification model, the predicted class for each sample in <code>x</code> is returned. For a regression model, the predicted value based on <code>x</code> is returned.</p>  <h5 id=\"args-47\">Args:</h5> <ul> <li>\n<code>x</code>: array-like matrix, [n_samples, n_features...] or iterator.</li> <li>\n<code>axis</code>: Which axis to argmax for classification. By default axis 1 (next after batch) is used. Use 2 for sequence predictions.</li> <li>\n<code>batch_size</code>: If test set is too big, use batch size to split it into mini batches. By default the batch_size member variable is used.</li> </ul>  <h5 id=\"returns-49\">Returns:</h5> <ul> <li>\n<code>y</code>: array of shape [n_samples]. The predicted classes or predicted value.</li> </ul>  <h4 id=\"TensorFlowEstimator.predict_proba\"><code>tf.contrib.learn.TensorFlowEstimator.predict_proba(x, batch_size=None)</code></h4> <p>Predict class probability of the input samples <code>x</code>.</p>  <h5 id=\"args-48\">Args:</h5> <ul> <li>\n<code>x</code>: array-like matrix, [n_samples, n_features...] or iterator.</li> <li>\n<code>batch_size</code>: If test set is too big, use batch size to split it into mini batches. By default the batch_size member variable is used.</li> </ul>  <h5 id=\"returns-50\">Returns:</h5> <ul> <li>\n<code>y</code>: array of shape [n_samples, n_classes]. The predicted probabilities for each class.</li> </ul>  <h4 id=\"TensorFlowEstimator.restore\"><code>tf.contrib.learn.TensorFlowEstimator.restore(cls, path, config=None)</code></h4> <p>Restores model from give path.</p>  <h5 id=\"args-49\">Args:</h5> <ul> <li>\n<code>path</code>: Path to the checkpoints and other model information.</li> <li>\n<code>config</code>: RunConfig object that controls the configurations of the session, e.g. num_cores, gpu_memory_fraction, etc. This is allowed to be reconfigured.</li> </ul>  <h5 id=\"returns-51\">Returns:</h5> <p>Estimator, object of the subclass of TensorFlowEstimator.</p>  <h5 id=\"raises-30\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>path</code> does not contain a model definition.</li> </ul>  <h4 id=\"TensorFlowEstimator.save\"><code>tf.contrib.learn.TensorFlowEstimator.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-50\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowEstimator.set_params\"><code>tf.contrib.learn.TensorFlowEstimator.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-51\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-52\">Returns:</h5> <p>self</p>  <h5 id=\"raises-31\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h3 id=\"LinearClassifier\"><code>class tf.contrib.learn.LinearClassifier</code></h3> <p>Linear classifier model.</p> <p>Train a linear model to classify instances into one of multiple possible classes. When number of possible classes is 2, this is binary classification.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">education = sparse_column_with_hash_bucket(column_name=\"education\",\n                                           hash_bucket_size=1000)\noccupation = sparse_column_with_hash_bucket(column_name=\"occupation\",\n                                            hash_bucket_size=1000)\n\neducation_x_occupation = crossed_column(columns=[education, occupation],\n                                        hash_bucket_size=10000)\n\n# Estimator using the default optimizer.\nestimator = LinearClassifier(\n    feature_columns=[occupation, education_x_occupation])\n\n# Or estimator using the FTRL optimizer with regularization.\nestimator = LinearClassifier(\n    feature_columns=[occupation, education_x_occupation],\n    optimizer=tf.train.FtrlOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001\n    ))\n\n# Or estimator using the SDCAOptimizer.\nestimator = LinearClassifier(\n   feature_columns=[occupation, education_x_occupation],\n   optimizer=tf.contrib.linear_optimizer.SDCAOptimizer(\n     example_id_column='example_id',\n     symmetric_l2_regularization=2.0\n   ))\n\n# Input builders\ndef input_fn_train: # returns x, y\n  ...\ndef input_fn_eval: # returns x, y\n  ...\nestimator.fit(input_fn=input_fn_train)\nestimator.evaluate(input_fn=input_fn_eval)\nestimator.predict(x=x)\n</pre> <p>Input of <code>fit</code> and <code>evaluate</code> should have following features, otherwise there will be a <code>KeyError</code>:</p> <ul> <li>if <code>weight_column_name</code> is not <code>None</code>, a feature with <code>key=weight_column_name</code> whose value is a <code>Tensor</code>.</li> <li>for each <code>column</code> in <code>feature_columns</code>: <ul> <li>if <code>column</code> is a <code>SparseColumn</code>, a feature with <code>key=column.name</code> whose <code>value</code> is a <code>SparseTensor</code>.</li> <li>if <code>column</code> is a <code>WeightedSparseColumn</code>, two features: the first with <code>key</code> the id column name, the second with <code>key</code> the weight column name. Both features' <code>value</code> must be a <code>SparseTensor</code>.</li> <li>if <code>column</code> is a <code>RealValuedColumn</code>, a feature with <code>key=column.name</code> whose <code>value</code> is a <code>Tensor</code>.</li> <li>if <code>feature_columns</code> is <code>None</code>, then <code>input</code> must contains only real valued <code>Tensor</code>. - - -</li> </ul>\n</li> </ul> <h4 id=\"LinearClassifier.__init__\"><code>tf.contrib.learn.LinearClassifier.__init__(feature_columns=None, model_dir=None, n_classes=2, weight_column_name=None, optimizer=None, gradient_clip_norm=None, enable_centered_bias=True, config=None)</code></h4> <p>Construct a <code>LinearClassifier</code> estimator object.</p>  <h5 id=\"args-52\">Args:</h5> <ul> <li>\n<code>feature_columns</code>: An iterable containing all the feature columns used by the model. All items in the set should be instances of classes derived from <code>FeatureColumn</code>.</li> <li>\n<code>model_dir</code>: Directory to save model parameters, graph and etc. This can also be used to load checkpoints from the directory into a estimator to continue training a previously saved model.</li> <li>\n<code>n_classes</code>: number of target classes. Default is binary classification.</li> <li>\n<code>weight_column_name</code>: A string defining feature column name representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example.</li> <li>\n<code>optimizer</code>: The optimizer used to train the model. If specified, it should be either an instance of <code>tf.Optimizer</code> or the SDCAOptimizer. If <code>None</code>, the Ftrl optimizer will be used.</li> <li>\n<code>gradient_clip_norm</code>: A <code>float</code> &gt; 0. If provided, gradients are clipped to their global norm with this clipping ratio. See <code>tf.clip_by_global_norm</code> for more details.</li> <li>\n<code>enable_centered_bias</code>: A bool. If True, estimator will learn a centered bias variable for each class. Rest of the model structure learns the residual after centered bias.</li> <li>\n<code>config</code>: <code>RunConfig</code> object to configure the runtime settings.</li> </ul>  <h5 id=\"returns-53\">Returns:</h5> <p>A <code>LinearClassifier</code> estimator.</p>  <h4 id=\"LinearClassifier.bias_\"><code>tf.contrib.learn.LinearClassifier.bias_</code></h4>  <h4 id=\"LinearClassifier.dnn_bias_\"><code>tf.contrib.learn.LinearClassifier.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"LinearClassifier.dnn_weights_\"><code>tf.contrib.learn.LinearClassifier.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"LinearClassifier.evaluate\"><code>tf.contrib.learn.LinearClassifier.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-32\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"LinearClassifier.fit\"><code>tf.contrib.learn.LinearClassifier.fit(x=None, y=None, input_fn=None, steps=None, batch_size=None, monitors=None, max_steps=None)</code></h4> <p>See <code>Trainable</code>.</p>  <h5 id=\"raises-33\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>x</code> or <code>y</code> are not <code>None</code> while <code>input_fn</code> is not <code>None</code>.</li> <li>\n<code>ValueError</code>: If both <code>steps</code> and <code>max_steps</code> are not <code>None</code>.</li> </ul>  <h4 id=\"LinearClassifier.get_params\"><code>tf.contrib.learn.LinearClassifier.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-53\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-54\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"LinearClassifier.get_variable_names\"><code>tf.contrib.learn.LinearClassifier.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-55\">Returns:</h5> <p>List of names.</p>  <h4 id=\"LinearClassifier.get_variable_value\"><code>tf.contrib.learn.LinearClassifier.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-54\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-56\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"LinearClassifier.linear_bias_\"><code>tf.contrib.learn.LinearClassifier.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"LinearClassifier.linear_weights_\"><code>tf.contrib.learn.LinearClassifier.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"LinearClassifier.model_dir\"><code>tf.contrib.learn.LinearClassifier.model_dir</code></h4>  <h4 id=\"LinearClassifier.partial_fit\"><code>tf.contrib.learn.LinearClassifier.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-55\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-57\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-34\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"LinearClassifier.predict\"><code>tf.contrib.learn.LinearClassifier.predict(x=None, input_fn=None, batch_size=None, as_iterable=False)</code></h4> <p>Returns predicted classes for given features.</p>  <h5 id=\"args-56\">Args:</h5> <ul> <li>\n<code>x</code>: features.</li> <li>\n<code>input_fn</code>: Input function. If set, x must be None.</li> <li>\n<code>batch_size</code>: Override default batch size.</li> <li>\n<code>as_iterable</code>: If True, return an iterable which keeps yielding predictions for each example until inputs are exhausted. Note: The inputs must terminate if you want the iterable to terminate (e.g. be sure to pass num_epochs=1 if you are using something like read_batch_features).</li> </ul>  <h5 id=\"returns-58\">Returns:</h5> <p>Numpy array of predicted classes (or an iterable of predicted classes if as_iterable is True).</p>  <h4 id=\"LinearClassifier.predict_proba\"><code>tf.contrib.learn.LinearClassifier.predict_proba(x=None, input_fn=None, batch_size=None, as_iterable=False)</code></h4> <p>Returns prediction probabilities for given features.</p>  <h5 id=\"args-57\">Args:</h5> <ul> <li>\n<code>x</code>: features.</li> <li>\n<code>input_fn</code>: Input function. If set, x and y must be None.</li> <li>\n<code>batch_size</code>: Override default batch size.</li> <li>\n<code>as_iterable</code>: If True, return an iterable which keeps yielding predictions for each example until inputs are exhausted. Note: The inputs must terminate if you want the iterable to terminate (e.g. be sure to pass num_epochs=1 if you are using something like read_batch_features).</li> </ul>  <h5 id=\"returns-59\">Returns:</h5> <p>Numpy array of predicted probabilities (or an iterable of predicted probabilities if as_iterable is True).</p>  <h4 id=\"LinearClassifier.set_params\"><code>tf.contrib.learn.LinearClassifier.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-58\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-60\">Returns:</h5> <p>self</p>  <h5 id=\"raises-35\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"LinearClassifier.weights_\"><code>tf.contrib.learn.LinearClassifier.weights_</code></h4>  <h3 id=\"LinearRegressor\"><code>class tf.contrib.learn.LinearRegressor</code></h3> <p>Linear regressor model.</p> <p>Train a linear regression model to predict target variable value given observation of feature values.</p> <p>Example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">education = sparse_column_with_hash_bucket(column_name=\"education\",\n                                           hash_bucket_size=1000)\noccupation = sparse_column_with_hash_bucket(column_name=\"occupation\",\n                                            hash_bucket_size=1000)\n\neducation_x_occupation = crossed_column(columns=[education, occupation],\n                                        hash_bucket_size=10000)\n\nestimator = LinearRegressor(\n    feature_columns=[occupation, education_x_occupation])\n\n# Input builders\ndef input_fn_train: # returns x, y\n  ...\ndef input_fn_eval: # returns x, y\n  ...\nestimator.fit(input_fn=input_fn_train)\nestimator.evaluate(input_fn=input_fn_eval)\nestimator.predict(x=x)\n</pre> <p>Input of <code>fit</code> and <code>evaluate</code> should have following features, otherwise there will be a KeyError:</p> <ul> <li>if <code>weight_column_name</code> is not <code>None</code>: key=weight_column_name, value=a <code>Tensor</code>\n</li> <li>for column in <code>feature_columns</code>: <ul> <li>if isinstance(column, <code>SparseColumn</code>): key=column.name, value=a <code>SparseTensor</code>\n</li> <li key=\"id\" column name value=\"a\">if isinstance(column, <code>WeightedSparseColumn</code>):</li> <li>if isinstance(column, <code>RealValuedColumn</code>): key=column.name, value=a <code>Tensor</code>\n</li> <li>if <code>feature_columns</code> is <code>None</code>: input must contains only real valued <code>Tensor</code>. - - -</li> </ul>\n</li> </ul> <h4 id=\"LinearRegressor.__init__\"><code>tf.contrib.learn.LinearRegressor.__init__(feature_columns=None, model_dir=None, weight_column_name=None, optimizer=None, gradient_clip_norm=None, enable_centered_bias=True, target_dimension=1, config=None)</code></h4> <p>Construct a <code>LinearRegressor</code> estimator object.</p>  <h5 id=\"args-59\">Args:</h5> <ul> <li>\n<code>feature_columns</code>: An iterable containing all the feature columns used by the model. All items in the set should be instances of classes derived from <code>FeatureColumn</code>.</li> <li>\n<code>model_dir</code>: Directory to save model parameters, graph, etc. This can also be used to load checkpoints from the directory into a estimator to continue training a previously saved model.</li> <li>\n<code>weight_column_name</code>: A string defining feature column name representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example.</li> <li>\n<code>optimizer</code>: An instance of <code>tf.Optimizer</code> used to train the model. If <code>None</code>, will use an Ftrl optimizer.</li> <li>\n<code>gradient_clip_norm</code>: A <code>float</code> &gt; 0. If provided, gradients are clipped to their global norm with this clipping ratio. See <code>tf.clip_by_global_norm</code> for more details.</li> <li>\n<code>enable_centered_bias</code>: A bool. If True, estimator will learn a centered bias variable for each class. Rest of the model structure learns the residual after centered bias.</li> <li>\n<code>target_dimension</code>: dimension of the target for multilabels.</li> <li>\n<code>config</code>: <code>RunConfig</code> object to configure the runtime settings.</li> </ul>  <h5 id=\"returns-61\">Returns:</h5> <p>A <code>LinearRegressor</code> estimator.</p>  <h4 id=\"LinearRegressor.bias_\"><code>tf.contrib.learn.LinearRegressor.bias_</code></h4>  <h4 id=\"LinearRegressor.dnn_bias_\"><code>tf.contrib.learn.LinearRegressor.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"LinearRegressor.dnn_weights_\"><code>tf.contrib.learn.LinearRegressor.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"LinearRegressor.evaluate\"><code>tf.contrib.learn.LinearRegressor.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-36\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"LinearRegressor.fit\"><code>tf.contrib.learn.LinearRegressor.fit(x=None, y=None, input_fn=None, steps=None, batch_size=None, monitors=None, max_steps=None)</code></h4> <p>See <code>Trainable</code>.</p>  <h5 id=\"raises-37\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>x</code> or <code>y</code> are not <code>None</code> while <code>input_fn</code> is not <code>None</code>.</li> <li>\n<code>ValueError</code>: If both <code>steps</code> and <code>max_steps</code> are not <code>None</code>.</li> </ul>  <h4 id=\"LinearRegressor.get_params\"><code>tf.contrib.learn.LinearRegressor.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-60\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-62\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"LinearRegressor.get_variable_names\"><code>tf.contrib.learn.LinearRegressor.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-63\">Returns:</h5> <p>List of names.</p>  <h4 id=\"LinearRegressor.get_variable_value\"><code>tf.contrib.learn.LinearRegressor.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-61\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-64\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"LinearRegressor.linear_bias_\"><code>tf.contrib.learn.LinearRegressor.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"LinearRegressor.linear_weights_\"><code>tf.contrib.learn.LinearRegressor.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"LinearRegressor.model_dir\"><code>tf.contrib.learn.LinearRegressor.model_dir</code></h4>  <h4 id=\"LinearRegressor.partial_fit\"><code>tf.contrib.learn.LinearRegressor.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-62\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-65\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-38\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"LinearRegressor.predict\"><code>tf.contrib.learn.LinearRegressor.predict(x=None, input_fn=None, batch_size=None, outputs=None, as_iterable=False)</code></h4> <p>Returns predictions for given features.</p>  <h5 id=\"args-63\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code> and 'batch_size' must be <code>None</code>.</li> <li>\n<code>batch_size</code>: Override default batch size. If set, 'input_fn' must be 'None'.</li> <li>\n<code>outputs</code>: list of <code>str</code>, name of the output to predict. If <code>None</code>, returns all.</li> <li>\n<code>as_iterable</code>: If True, return an iterable which keeps yielding predictions for each example until inputs are exhausted. Note: The inputs must terminate if you want the iterable to terminate (e.g. be sure to pass num_epochs=1 if you are using something like read_batch_features).</li> </ul>  <h5 id=\"returns-66\">Returns:</h5> <p>A numpy array of predicted classes or regression values if the constructor's <code>model_fn</code> returns a <code>Tensor</code> for <code>predictions</code> or a <code>dict</code> of numpy arrays if <code>model_fn</code> returns a <code>dict</code>. Returns an iterable of predictions if as_iterable is True.</p>  <h5 id=\"raises-39\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If x and input_fn are both provided or both <code>None</code>.</li> </ul>  <h4 id=\"LinearRegressor.set_params\"><code>tf.contrib.learn.LinearRegressor.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-64\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-67\">Returns:</h5> <p>self</p>  <h5 id=\"raises-40\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"LinearRegressor.weights_\"><code>tf.contrib.learn.LinearRegressor.weights_</code></h4>  <h3 id=\"TensorFlowLinearClassifier\"><code>class tf.contrib.learn.TensorFlowLinearClassifier</code></h3>  <h4 id=\"TensorFlowLinearClassifier.__init__\"><code>tf.contrib.learn.TensorFlowLinearClassifier.__init__(*args, **kwargs)</code></h4>  <h4 id=\"TensorFlowLinearClassifier.bias_\"><code>tf.contrib.learn.TensorFlowLinearClassifier.bias_</code></h4>  <h4 id=\"TensorFlowLinearClassifier.dnn_bias_\"><code>tf.contrib.learn.TensorFlowLinearClassifier.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"TensorFlowLinearClassifier.dnn_weights_\"><code>tf.contrib.learn.TensorFlowLinearClassifier.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"TensorFlowLinearClassifier.evaluate\"><code>tf.contrib.learn.TensorFlowLinearClassifier.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-41\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"TensorFlowLinearClassifier.fit\"><code>tf.contrib.learn.TensorFlowLinearClassifier.fit(x, y, steps=None, batch_size=None, monitors=None, logdir=None)</code></h4>  <h4 id=\"TensorFlowLinearClassifier.get_params\"><code>tf.contrib.learn.TensorFlowLinearClassifier.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-65\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-68\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowLinearClassifier.get_variable_names\"><code>tf.contrib.learn.TensorFlowLinearClassifier.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-69\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowLinearClassifier.get_variable_value\"><code>tf.contrib.learn.TensorFlowLinearClassifier.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-66\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-70\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowLinearClassifier.linear_bias_\"><code>tf.contrib.learn.TensorFlowLinearClassifier.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"TensorFlowLinearClassifier.linear_weights_\"><code>tf.contrib.learn.TensorFlowLinearClassifier.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"TensorFlowLinearClassifier.model_dir\"><code>tf.contrib.learn.TensorFlowLinearClassifier.model_dir</code></h4>  <h4 id=\"TensorFlowLinearClassifier.partial_fit\"><code>tf.contrib.learn.TensorFlowLinearClassifier.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-67\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-71\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-42\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"TensorFlowLinearClassifier.predict\"><code>tf.contrib.learn.TensorFlowLinearClassifier.predict(x=None, input_fn=None, batch_size=None, outputs=None, axis=1)</code></h4> <p>Predict class or regression for <code>x</code>.</p>  <h4 id=\"TensorFlowLinearClassifier.predict_proba\"><code>tf.contrib.learn.TensorFlowLinearClassifier.predict_proba(x=None, input_fn=None, batch_size=None, outputs=None)</code></h4>  <h4 id=\"TensorFlowLinearClassifier.save\"><code>tf.contrib.learn.TensorFlowLinearClassifier.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-68\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowLinearClassifier.set_params\"><code>tf.contrib.learn.TensorFlowLinearClassifier.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-69\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-72\">Returns:</h5> <p>self</p>  <h5 id=\"raises-43\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"TensorFlowLinearClassifier.weights_\"><code>tf.contrib.learn.TensorFlowLinearClassifier.weights_</code></h4>  <h3 id=\"TensorFlowLinearRegressor\"><code>class tf.contrib.learn.TensorFlowLinearRegressor</code></h3>  <h4 id=\"TensorFlowLinearRegressor.__init__\"><code>tf.contrib.learn.TensorFlowLinearRegressor.__init__(*args, **kwargs)</code></h4>  <h4 id=\"TensorFlowLinearRegressor.bias_\"><code>tf.contrib.learn.TensorFlowLinearRegressor.bias_</code></h4>  <h4 id=\"TensorFlowLinearRegressor.dnn_bias_\"><code>tf.contrib.learn.TensorFlowLinearRegressor.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"TensorFlowLinearRegressor.dnn_weights_\"><code>tf.contrib.learn.TensorFlowLinearRegressor.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"TensorFlowLinearRegressor.evaluate\"><code>tf.contrib.learn.TensorFlowLinearRegressor.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-44\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"TensorFlowLinearRegressor.fit\"><code>tf.contrib.learn.TensorFlowLinearRegressor.fit(x, y, steps=None, batch_size=None, monitors=None, logdir=None)</code></h4>  <h4 id=\"TensorFlowLinearRegressor.get_params\"><code>tf.contrib.learn.TensorFlowLinearRegressor.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-70\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-73\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowLinearRegressor.get_variable_names\"><code>tf.contrib.learn.TensorFlowLinearRegressor.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-74\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowLinearRegressor.get_variable_value\"><code>tf.contrib.learn.TensorFlowLinearRegressor.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-71\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-75\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowLinearRegressor.linear_bias_\"><code>tf.contrib.learn.TensorFlowLinearRegressor.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"TensorFlowLinearRegressor.linear_weights_\"><code>tf.contrib.learn.TensorFlowLinearRegressor.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"TensorFlowLinearRegressor.model_dir\"><code>tf.contrib.learn.TensorFlowLinearRegressor.model_dir</code></h4>  <h4 id=\"TensorFlowLinearRegressor.partial_fit\"><code>tf.contrib.learn.TensorFlowLinearRegressor.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-72\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-76\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-45\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"TensorFlowLinearRegressor.predict\"><code>tf.contrib.learn.TensorFlowLinearRegressor.predict(x=None, input_fn=None, batch_size=None, outputs=None, axis=1)</code></h4> <p>Predict class or regression for <code>x</code>.</p>  <h4 id=\"TensorFlowLinearRegressor.predict_proba\"><code>tf.contrib.learn.TensorFlowLinearRegressor.predict_proba(x=None, input_fn=None, batch_size=None, outputs=None)</code></h4>  <h4 id=\"TensorFlowLinearRegressor.save\"><code>tf.contrib.learn.TensorFlowLinearRegressor.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-73\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowLinearRegressor.set_params\"><code>tf.contrib.learn.TensorFlowLinearRegressor.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-74\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-77\">Returns:</h5> <p>self</p>  <h5 id=\"raises-46\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"TensorFlowLinearRegressor.weights_\"><code>tf.contrib.learn.TensorFlowLinearRegressor.weights_</code></h4>  <h3 id=\"TensorFlowRNNClassifier\"><code>class tf.contrib.learn.TensorFlowRNNClassifier</code></h3> <p>TensorFlow RNN Classifier model.</p>  <h4 id=\"TensorFlowRNNClassifier.__init__\"><code>tf.contrib.learn.TensorFlowRNNClassifier.__init__(rnn_size, n_classes, cell_type='gru', num_layers=1, input_op_fn=null_input_op_fn, initial_state=None, bidirectional=False, sequence_length=None, attn_length=None, attn_size=None, attn_vec_size=None, batch_size=32, steps=50, optimizer='Adagrad', learning_rate=0.1, class_weight=None, clip_gradients=5.0, continue_training=False, config=None, verbose=1)</code></h4> <p>Initializes a TensorFlowRNNClassifier instance.</p>  <h5 id=\"args-75\">Args:</h5> <ul> <li>\n<code>rnn_size</code>: The size for rnn cell, e.g. size of your word embeddings.</li> <li>\n<code>cell_type</code>: The type of rnn cell, including rnn, gru, and lstm.</li> <li>\n<code>num_layers</code>: The number of layers of the rnn model.</li> <li>\n<code>input_op_fn</code>: Function that will transform the input tensor, such as creating word embeddings, byte list, etc. This takes an argument x for input and returns transformed x.</li> <li>\n<code>bidirectional</code>: boolean, Whether this is a bidirectional rnn.</li> <li>\n<code>sequence_length</code>: If sequence_length is provided, dynamic calculation is performed. This saves computational time when unrolling past max sequence length.</li> <li>\n<code>initial_state</code>: An initial state for the RNN. This must be a tensor of appropriate type and shape [batch_size x cell.state_size].</li> <li>\n<code>attn_length</code>: integer, the size of attention vector attached to rnn cells.</li> <li>\n<code>attn_size</code>: integer, the size of an attention window attached to rnn cells.</li> <li>\n<code>attn_vec_size</code>: integer, the number of convolutional features calculated on attention state and the size of the hidden layer built from base cell state.</li> <li>\n<code>n_classes</code>: Number of classes in the target.</li> <li>\n<code>batch_size</code>: Mini batch size.</li> <li>\n<code>steps</code>: Number of steps to run over data.</li> <li>\n<code>optimizer</code>: Optimizer name (or class), for example \"SGD\", \"Adam\", \"Adagrad\".</li> <li>\n<p><code>learning_rate</code>: If this is constant float value, no decay function is used. Instead, a customized decay function can be passed that accepts global_step as parameter and returns a Tensor. e.g. exponential decay function:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def exp_decay(global_step):\n    return tf.train.exponential_decay(\n        learning_rate=0.1, global_step,\n        decay_steps=2, decay_rate=0.001)\n</pre>\n</li> <li><p><code>class_weight</code>: None or list of n_classes floats. Weight associated with classes for loss computation. If not given, all classes are supposed to have weight one.</p></li> <li><p><code>continue_training</code>: when continue_training is True, once initialized model will be continuely trained on every call of fit.</p></li> <li><p><code>config</code>: RunConfig object that controls the configurations of the session, e.g. num_cores, gpu_memory_fraction, etc.</p></li> </ul>  <h4 id=\"TensorFlowRNNClassifier.bias_\"><code>tf.contrib.learn.TensorFlowRNNClassifier.bias_</code></h4> <p>Returns bias of the rnn layer.</p>  <h4 id=\"TensorFlowRNNClassifier.evaluate\"><code>tf.contrib.learn.TensorFlowRNNClassifier.evaluate(x=None, y=None, input_fn=None, steps=None)</code></h4> <p>See base class.</p>  <h4 id=\"TensorFlowRNNClassifier.fit\"><code>tf.contrib.learn.TensorFlowRNNClassifier.fit(x, y, steps=None, monitors=None, logdir=None)</code></h4> <p>Neural network model from provided <code>model_fn</code> and training data.</p> <p>Note: called first time constructs the graph and initializers variables. Consecutives times it will continue training the same model. This logic follows partial_fit() interface in scikit-learn. To restart learning, create new estimator.</p>  <h5 id=\"args-76\">Args:</h5> <ul> <li><p><code>x</code>: matrix or tensor of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model.</p></li> <li><p><code>y</code>: vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression).</p></li> <li><p><code>steps</code>: int, number of steps to train. If None or 0, train for <code>self.steps</code>.</p></li> <li><p><code>monitors</code>: List of <code>BaseMonitor</code> objects to print training progress and invoke early stopping.</p></li> <li><p><code>logdir</code>: the directory to save the log file that can be used for optional visualization.</p></li> </ul>  <h5 id=\"returns-78\">Returns:</h5> <p>Returns self.</p>  <h4 id=\"TensorFlowRNNClassifier.get_params\"><code>tf.contrib.learn.TensorFlowRNNClassifier.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-77\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-79\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowRNNClassifier.get_tensor\"><code>tf.contrib.learn.TensorFlowRNNClassifier.get_tensor(name)</code></h4> <p>Returns tensor by name.</p>  <h5 id=\"args-78\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-80\">Returns:</h5> <p>Tensor.</p>  <h4 id=\"TensorFlowRNNClassifier.get_variable_names\"><code>tf.contrib.learn.TensorFlowRNNClassifier.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-81\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowRNNClassifier.get_variable_value\"><code>tf.contrib.learn.TensorFlowRNNClassifier.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-79\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-82\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowRNNClassifier.model_dir\"><code>tf.contrib.learn.TensorFlowRNNClassifier.model_dir</code></h4>  <h4 id=\"TensorFlowRNNClassifier.partial_fit\"><code>tf.contrib.learn.TensorFlowRNNClassifier.partial_fit(x, y)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training. This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-80\">Args:</h5> <ul> <li><p><code>x</code>: matrix or tensor of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model.</p></li> <li><p><code>y</code>: vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class label in classification, real numbers in regression).</p></li> </ul>  <h5 id=\"returns-83\">Returns:</h5> <p>Returns self.</p>  <h4 id=\"TensorFlowRNNClassifier.predict\"><code>tf.contrib.learn.TensorFlowRNNClassifier.predict(x, axis=1, batch_size=None)</code></h4> <p>Predict class or regression for <code>x</code>.</p> <p>For a classification model, the predicted class for each sample in <code>x</code> is returned. For a regression model, the predicted value based on <code>x</code> is returned.</p>  <h5 id=\"args-81\">Args:</h5> <ul> <li>\n<code>x</code>: array-like matrix, [n_samples, n_features...] or iterator.</li> <li>\n<code>axis</code>: Which axis to argmax for classification. By default axis 1 (next after batch) is used. Use 2 for sequence predictions.</li> <li>\n<code>batch_size</code>: If test set is too big, use batch size to split it into mini batches. By default the batch_size member variable is used.</li> </ul>  <h5 id=\"returns-84\">Returns:</h5> <ul> <li>\n<code>y</code>: array of shape [n_samples]. The predicted classes or predicted value.</li> </ul>  <h4 id=\"TensorFlowRNNClassifier.predict_proba\"><code>tf.contrib.learn.TensorFlowRNNClassifier.predict_proba(x, batch_size=None)</code></h4> <p>Predict class probability of the input samples <code>x</code>.</p>  <h5 id=\"args-82\">Args:</h5> <ul> <li>\n<code>x</code>: array-like matrix, [n_samples, n_features...] or iterator.</li> <li>\n<code>batch_size</code>: If test set is too big, use batch size to split it into mini batches. By default the batch_size member variable is used.</li> </ul>  <h5 id=\"returns-85\">Returns:</h5> <ul> <li>\n<code>y</code>: array of shape [n_samples, n_classes]. The predicted probabilities for each class.</li> </ul>  <h4 id=\"TensorFlowRNNClassifier.restore\"><code>tf.contrib.learn.TensorFlowRNNClassifier.restore(cls, path, config=None)</code></h4> <p>Restores model from give path.</p>  <h5 id=\"args-83\">Args:</h5> <ul> <li>\n<code>path</code>: Path to the checkpoints and other model information.</li> <li>\n<code>config</code>: RunConfig object that controls the configurations of the session, e.g. num_cores, gpu_memory_fraction, etc. This is allowed to be reconfigured.</li> </ul>  <h5 id=\"returns-86\">Returns:</h5> <p>Estimator, object of the subclass of TensorFlowEstimator.</p>  <h5 id=\"raises-47\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>path</code> does not contain a model definition.</li> </ul>  <h4 id=\"TensorFlowRNNClassifier.save\"><code>tf.contrib.learn.TensorFlowRNNClassifier.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-84\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowRNNClassifier.set_params\"><code>tf.contrib.learn.TensorFlowRNNClassifier.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-85\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-87\">Returns:</h5> <p>self</p>  <h5 id=\"raises-48\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"TensorFlowRNNClassifier.weights_\"><code>tf.contrib.learn.TensorFlowRNNClassifier.weights_</code></h4> <p>Returns weights of the rnn layer.</p>  <h3 id=\"TensorFlowRNNRegressor\"><code>class tf.contrib.learn.TensorFlowRNNRegressor</code></h3> <p>TensorFlow RNN Regressor model.</p>  <h4 id=\"TensorFlowRNNRegressor.__init__\"><code>tf.contrib.learn.TensorFlowRNNRegressor.__init__(rnn_size, cell_type='gru', num_layers=1, input_op_fn=null_input_op_fn, initial_state=None, bidirectional=False, sequence_length=None, attn_length=None, attn_size=None, attn_vec_size=None, n_classes=0, batch_size=32, steps=50, optimizer='Adagrad', learning_rate=0.1, clip_gradients=5.0, continue_training=False, config=None, verbose=1)</code></h4> <p>Initializes a TensorFlowRNNRegressor instance.</p>  <h5 id=\"args-86\">Args:</h5> <ul> <li>\n<code>rnn_size</code>: The size for rnn cell, e.g. size of your word embeddings.</li> <li>\n<code>cell_type</code>: The type of rnn cell, including rnn, gru, and lstm.</li> <li>\n<code>num_layers</code>: The number of layers of the rnn model.</li> <li>\n<code>input_op_fn</code>: Function that will transform the input tensor, such as creating word embeddings, byte list, etc. This takes an argument x for input and returns transformed x.</li> <li>\n<code>bidirectional</code>: boolean, Whether this is a bidirectional rnn.</li> <li>\n<code>sequence_length</code>: If sequence_length is provided, dynamic calculation is performed. This saves computational time when unrolling past max sequence length.</li> <li>\n<code>attn_length</code>: integer, the size of attention vector attached to rnn cells.</li> <li>\n<code>attn_size</code>: integer, the size of an attention window attached to rnn cells.</li> <li>\n<code>attn_vec_size</code>: integer, the number of convolutional features calculated on attention state and the size of the hidden layer built from base cell state.</li> <li>\n<code>initial_state</code>: An initial state for the RNN. This must be a tensor of appropriate type and shape [batch_size x cell.state_size].</li> <li>\n<code>batch_size</code>: Mini batch size.</li> <li>\n<code>steps</code>: Number of steps to run over data.</li> <li>\n<code>optimizer</code>: Optimizer name (or class), for example \"SGD\", \"Adam\", \"Adagrad\".</li> <li>\n<p><code>learning_rate</code>: If this is constant float value, no decay function is used. Instead, a customized decay function can be passed that accepts global_step as parameter and returns a Tensor. e.g. exponential decay function:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">def exp_decay(global_step):\n    return tf.train.exponential_decay(\n        learning_rate=0.1, global_step,\n        decay_steps=2, decay_rate=0.001)\n</pre>\n</li> <li><p><code>continue_training</code>: when continue_training is True, once initialized model will be continuely trained on every call of fit.</p></li> <li><p><code>config</code>: RunConfig object that controls the configurations of the session, e.g. num_cores, gpu_memory_fraction, etc.</p></li> <li>\n<p><code>verbose</code>: Controls the verbosity, possible values:</p> <ul> <li>0: the algorithm and debug information is muted.</li> <li>1: trainer prints the progress.</li> <li>2: log device placement is printed.</li> </ul>\n</li> </ul>  <h4 id=\"TensorFlowRNNRegressor.bias_\"><code>tf.contrib.learn.TensorFlowRNNRegressor.bias_</code></h4> <p>Returns bias of the rnn layer.</p>  <h4 id=\"TensorFlowRNNRegressor.evaluate\"><code>tf.contrib.learn.TensorFlowRNNRegressor.evaluate(x=None, y=None, input_fn=None, steps=None)</code></h4> <p>See base class.</p>  <h4 id=\"TensorFlowRNNRegressor.fit\"><code>tf.contrib.learn.TensorFlowRNNRegressor.fit(x, y, steps=None, monitors=None, logdir=None)</code></h4> <p>Neural network model from provided <code>model_fn</code> and training data.</p> <p>Note: called first time constructs the graph and initializers variables. Consecutives times it will continue training the same model. This logic follows partial_fit() interface in scikit-learn. To restart learning, create new estimator.</p>  <h5 id=\"args-87\">Args:</h5> <ul> <li><p><code>x</code>: matrix or tensor of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model.</p></li> <li><p><code>y</code>: vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression).</p></li> <li><p><code>steps</code>: int, number of steps to train. If None or 0, train for <code>self.steps</code>.</p></li> <li><p><code>monitors</code>: List of <code>BaseMonitor</code> objects to print training progress and invoke early stopping.</p></li> <li><p><code>logdir</code>: the directory to save the log file that can be used for optional visualization.</p></li> </ul>  <h5 id=\"returns-88\">Returns:</h5> <p>Returns self.</p>  <h4 id=\"TensorFlowRNNRegressor.get_params\"><code>tf.contrib.learn.TensorFlowRNNRegressor.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-88\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-89\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowRNNRegressor.get_tensor\"><code>tf.contrib.learn.TensorFlowRNNRegressor.get_tensor(name)</code></h4> <p>Returns tensor by name.</p>  <h5 id=\"args-89\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-90\">Returns:</h5> <p>Tensor.</p>  <h4 id=\"TensorFlowRNNRegressor.get_variable_names\"><code>tf.contrib.learn.TensorFlowRNNRegressor.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-91\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowRNNRegressor.get_variable_value\"><code>tf.contrib.learn.TensorFlowRNNRegressor.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-90\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-92\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowRNNRegressor.model_dir\"><code>tf.contrib.learn.TensorFlowRNNRegressor.model_dir</code></h4>  <h4 id=\"TensorFlowRNNRegressor.partial_fit\"><code>tf.contrib.learn.TensorFlowRNNRegressor.partial_fit(x, y)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training. This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-91\">Args:</h5> <ul> <li><p><code>x</code>: matrix or tensor of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model.</p></li> <li><p><code>y</code>: vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class label in classification, real numbers in regression).</p></li> </ul>  <h5 id=\"returns-93\">Returns:</h5> <p>Returns self.</p>  <h4 id=\"TensorFlowRNNRegressor.predict\"><code>tf.contrib.learn.TensorFlowRNNRegressor.predict(x, axis=1, batch_size=None)</code></h4> <p>Predict class or regression for <code>x</code>.</p> <p>For a classification model, the predicted class for each sample in <code>x</code> is returned. For a regression model, the predicted value based on <code>x</code> is returned.</p>  <h5 id=\"args-92\">Args:</h5> <ul> <li>\n<code>x</code>: array-like matrix, [n_samples, n_features...] or iterator.</li> <li>\n<code>axis</code>: Which axis to argmax for classification. By default axis 1 (next after batch) is used. Use 2 for sequence predictions.</li> <li>\n<code>batch_size</code>: If test set is too big, use batch size to split it into mini batches. By default the batch_size member variable is used.</li> </ul>  <h5 id=\"returns-94\">Returns:</h5> <ul> <li>\n<code>y</code>: array of shape [n_samples]. The predicted classes or predicted value.</li> </ul>  <h4 id=\"TensorFlowRNNRegressor.predict_proba\"><code>tf.contrib.learn.TensorFlowRNNRegressor.predict_proba(x, batch_size=None)</code></h4> <p>Predict class probability of the input samples <code>x</code>.</p>  <h5 id=\"args-93\">Args:</h5> <ul> <li>\n<code>x</code>: array-like matrix, [n_samples, n_features...] or iterator.</li> <li>\n<code>batch_size</code>: If test set is too big, use batch size to split it into mini batches. By default the batch_size member variable is used.</li> </ul>  <h5 id=\"returns-95\">Returns:</h5> <ul> <li>\n<code>y</code>: array of shape [n_samples, n_classes]. The predicted probabilities for each class.</li> </ul>  <h4 id=\"TensorFlowRNNRegressor.restore\"><code>tf.contrib.learn.TensorFlowRNNRegressor.restore(cls, path, config=None)</code></h4> <p>Restores model from give path.</p>  <h5 id=\"args-94\">Args:</h5> <ul> <li>\n<code>path</code>: Path to the checkpoints and other model information.</li> <li>\n<code>config</code>: RunConfig object that controls the configurations of the session, e.g. num_cores, gpu_memory_fraction, etc. This is allowed to be reconfigured.</li> </ul>  <h5 id=\"returns-96\">Returns:</h5> <p>Estimator, object of the subclass of TensorFlowEstimator.</p>  <h5 id=\"raises-49\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>path</code> does not contain a model definition.</li> </ul>  <h4 id=\"TensorFlowRNNRegressor.save\"><code>tf.contrib.learn.TensorFlowRNNRegressor.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-95\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowRNNRegressor.set_params\"><code>tf.contrib.learn.TensorFlowRNNRegressor.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-96\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-97\">Returns:</h5> <p>self</p>  <h5 id=\"raises-50\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"TensorFlowRNNRegressor.weights_\"><code>tf.contrib.learn.TensorFlowRNNRegressor.weights_</code></h4> <p>Returns weights of the rnn layer.</p>  <h3 id=\"TensorFlowRegressor\"><code>class tf.contrib.learn.TensorFlowRegressor</code></h3>  <h4 id=\"TensorFlowRegressor.__init__\"><code>tf.contrib.learn.TensorFlowRegressor.__init__(*args, **kwargs)</code></h4>  <h4 id=\"TensorFlowRegressor.bias_\"><code>tf.contrib.learn.TensorFlowRegressor.bias_</code></h4>  <h4 id=\"TensorFlowRegressor.dnn_bias_\"><code>tf.contrib.learn.TensorFlowRegressor.dnn_bias_</code></h4> <p>Returns bias of deep neural network part.</p>  <h4 id=\"TensorFlowRegressor.dnn_weights_\"><code>tf.contrib.learn.TensorFlowRegressor.dnn_weights_</code></h4> <p>Returns weights of deep neural network part.</p>  <h4 id=\"TensorFlowRegressor.evaluate\"><code>tf.contrib.learn.TensorFlowRegressor.evaluate(x=None, y=None, input_fn=None, feed_fn=None, batch_size=None, steps=None, metrics=None, name=None)</code></h4> <p>See <code>Evaluable</code>.</p>  <h5 id=\"raises-51\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> or <code>y</code> is provided, and at least one of <code>input_fn</code> or <code>feed_fn</code> is provided. Or if <code>metrics</code> is not <code>None</code> or <code>dict</code>.</li> </ul>  <h4 id=\"TensorFlowRegressor.fit\"><code>tf.contrib.learn.TensorFlowRegressor.fit(x, y, steps=None, batch_size=None, monitors=None, logdir=None)</code></h4>  <h4 id=\"TensorFlowRegressor.get_params\"><code>tf.contrib.learn.TensorFlowRegressor.get_params(deep=True)</code></h4> <p>Get parameters for this estimator.</p>  <h5 id=\"args-97\">Args:</h5> <ul> <li>\n<p><code>deep</code>: boolean, optional</p> <p>If <code>True</code>, will return the parameters for this estimator and contained subobjects that are estimators.</p>\n</li> </ul>  <h5 id=\"returns-98\">Returns:</h5> <p>params : mapping of string to any Parameter names mapped to their values.</p>  <h4 id=\"TensorFlowRegressor.get_variable_names\"><code>tf.contrib.learn.TensorFlowRegressor.get_variable_names()</code></h4> <p>Returns list of all variable names in this model.</p>  <h5 id=\"returns-99\">Returns:</h5> <p>List of names.</p>  <h4 id=\"TensorFlowRegressor.get_variable_value\"><code>tf.contrib.learn.TensorFlowRegressor.get_variable_value(name)</code></h4> <p>Returns value of the variable given by name.</p>  <h5 id=\"args-98\">Args:</h5> <ul> <li>\n<code>name</code>: string, name of the tensor.</li> </ul>  <h5 id=\"returns-100\">Returns:</h5> <p>Numpy array - value of the tensor.</p>  <h4 id=\"TensorFlowRegressor.linear_bias_\"><code>tf.contrib.learn.TensorFlowRegressor.linear_bias_</code></h4> <p>Returns bias of the linear part.</p>  <h4 id=\"TensorFlowRegressor.linear_weights_\"><code>tf.contrib.learn.TensorFlowRegressor.linear_weights_</code></h4> <p>Returns weights per feature of the linear part.</p>  <h4 id=\"TensorFlowRegressor.model_dir\"><code>tf.contrib.learn.TensorFlowRegressor.model_dir</code></h4>  <h4 id=\"TensorFlowRegressor.partial_fit\"><code>tf.contrib.learn.TensorFlowRegressor.partial_fit(x=None, y=None, input_fn=None, steps=1, batch_size=None, monitors=None)</code></h4> <p>Incremental fit on a batch of samples.</p> <p>This method is expected to be called several times consecutively on different or the same chunks of the dataset. This either can implement iterative training or out-of-core/online training.</p> <p>This is especially useful when the whole dataset is too big to fit in memory at the same time. Or when model is taking long time to converge, and you want to split up training into subparts.</p>  <h5 id=\"args-99\">Args:</h5> <ul> <li>\n<code>x</code>: Matrix of shape [n_samples, n_features...]. Can be iterator that returns arrays of features. The training input samples for fitting the model. If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>y</code>: Vector or matrix [n_samples] or [n_samples, n_outputs]. Can be iterator that returns array of targets. The training target values (class labels in classification, real numbers in regression). If set, <code>input_fn</code> must be <code>None</code>.</li> <li>\n<code>input_fn</code>: Input function. If set, <code>x</code>, <code>y</code>, and <code>batch_size</code> must be <code>None</code>.</li> <li>\n<code>steps</code>: Number of steps for which to train model. If <code>None</code>, train forever.</li> <li>\n<code>batch_size</code>: minibatch size to use on the input, defaults to first dimension of <code>x</code>. Must be <code>None</code> if <code>input_fn</code> is provided.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> </ul>  <h5 id=\"returns-101\">Returns:</h5> <p><code>self</code>, for chaining.</p>  <h5 id=\"raises-52\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If at least one of <code>x</code> and <code>y</code> is provided, and <code>input_fn</code> is provided.</li> </ul>  <h4 id=\"TensorFlowRegressor.predict\"><code>tf.contrib.learn.TensorFlowRegressor.predict(x=None, input_fn=None, batch_size=None, outputs=None, axis=1)</code></h4> <p>Predict class or regression for <code>x</code>.</p>  <h4 id=\"TensorFlowRegressor.predict_proba\"><code>tf.contrib.learn.TensorFlowRegressor.predict_proba(x=None, input_fn=None, batch_size=None, outputs=None)</code></h4>  <h4 id=\"TensorFlowRegressor.save\"><code>tf.contrib.learn.TensorFlowRegressor.save(path)</code></h4> <p>Saves checkpoints and graph to given path.</p>  <h5 id=\"args-100\">Args:</h5> <ul> <li>\n<code>path</code>: Folder to save model to.</li> </ul>  <h4 id=\"TensorFlowRegressor.set_params\"><code>tf.contrib.learn.TensorFlowRegressor.set_params(**params)</code></h4> <p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each component of a nested object.</p>  <h5 id=\"args-101\">Args:</h5> <ul> <li>\n<code>**params</code>: Parameters.</li> </ul>  <h5 id=\"returns-102\">Returns:</h5> <p>self</p>  <h5 id=\"raises-53\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If params contain invalid names.</li> </ul>  <h4 id=\"TensorFlowRegressor.weights_\"><code>tf.contrib.learn.TensorFlowRegressor.weights_</code></h4>  <h2 id=\"graph-actions\">Graph actions</h2> <p>Perform various training, evaluation, and inference actions on a graph.</p>  <h3 id=\"NanLossDuringTrainingError\"><code>class tf.contrib.learn.NanLossDuringTrainingError</code></h3>  <h3 id=\"RunConfig\"><code>class tf.contrib.learn.RunConfig</code></h3> <p>This class specifies the specific configurations for the run.</p>  <h4 id=\"RunConfig.__init__\"><code>tf.contrib.learn.RunConfig.__init__(master='', task=0, num_ps_replicas=0, num_cores=4, log_device_placement=False, gpu_memory_fraction=1, tf_random_seed=42, save_summary_steps=100, save_checkpoints_secs=60, keep_checkpoint_max=5, keep_checkpoint_every_n_hours=10000)</code></h4> <p>Constructor.</p>  <h5 id=\"args-102\">Args:</h5> <ul> <li>\n<code>master</code>: TensorFlow master. Empty string (the default) for local.</li> <li>\n<code>task</code>: Task id of the replica running the training (default: 0).</li> <li>\n<code>num_ps_replicas</code>: Number of parameter server tasks to use (default: 0).</li> <li>\n<code>num_cores</code>: Number of cores to be used (default: 4).</li> <li>\n<code>log_device_placement</code>: Log the op placement to devices (default: False).</li> <li>\n<code>gpu_memory_fraction</code>: Fraction of GPU memory used by the process on each GPU uniformly on the same machine.</li> <li>\n<code>tf_random_seed</code>: Random seed for TensorFlow initializers. Setting this value allows consistency between reruns.</li> <li>\n<code>save_summary_steps</code>: Save summaries every this many steps.</li> <li>\n<code>save_checkpoints_secs</code>: Save checkpoints every this many seconds.</li> <li>\n<code>keep_checkpoint_max</code>: The maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept. Defaults to 5 (that is, the 5 most recent checkpoint files are kept.)</li> <li>\n<code>keep_checkpoint_every_n_hours</code>: Number of hours between each checkpoint to be saved. The default value of 10,000 hours effectively disables the feature.</li> </ul>  <h3 id=\"evaluate\"><code>tf.contrib.learn.evaluate(graph, output_dir, checkpoint_path, eval_dict, update_op=None, global_step_tensor=None, supervisor_master='', log_every_steps=10, feed_fn=None, max_steps=None)</code></h3> <p>Evaluate a model loaded from a checkpoint.</p> <p>Given <code>graph</code>, a directory to write summaries to (<code>output_dir</code>), a checkpoint to restore variables from, and a <code>dict</code> of <code>Tensor</code>s to evaluate, run an eval loop for <code>max_steps</code> steps, or until an exception (generally, an end-of-input signal from a reader operation) is raised from running <code>eval_dict</code>.</p> <p>In each step of evaluation, all tensors in the <code>eval_dict</code> are evaluated, and every <code>log_every_steps</code> steps, they are logged. At the very end of evaluation, a summary is evaluated (finding the summary ops using <code>Supervisor</code>'s logic) and written to <code>output_dir</code>.</p>  <h5 id=\"args-103\">Args:</h5> <ul> <li>\n<code>graph</code>: A <code>Graph</code> to train. It is expected that this graph is not in use elsewhere.</li> <li>\n<code>output_dir</code>: A string containing the directory to write a summary to.</li> <li>\n<code>checkpoint_path</code>: A string containing the path to a checkpoint to restore. Can be <code>None</code> if the graph doesn't require loading any variables.</li> <li>\n<code>eval_dict</code>: A <code>dict</code> mapping string names to tensors to evaluate. It is evaluated in every logging step. The result of the final evaluation is returned. If <code>update_op</code> is None, then it's evaluated in every step. If <code>max_steps</code> is <code>None</code>, this should depend on a reader that will raise an end-of-inupt exception when the inputs are exhausted.</li> <li>\n<code>update_op</code>: A <code>Tensor</code> which is run in every step.</li> <li>\n<code>global_step_tensor</code>: A <code>Variable</code> containing the global step. If <code>None</code>, one is extracted from the graph using the same logic as in <code>Supervisor</code>. Used to place eval summaries on training curves.</li> <li>\n<code>supervisor_master</code>: The master string to use when preparing the session.</li> <li>\n<code>log_every_steps</code>: Integer. Output logs every <code>log_every_steps</code> evaluation steps. The logs contain the <code>eval_dict</code> and timing information.</li> <li>\n<code>feed_fn</code>: A function that is called every iteration to produce a <code>feed_dict</code> passed to <code>session.run</code> calls. Optional.</li> <li>\n<code>max_steps</code>: Integer. Evaluate <code>eval_dict</code> this many times.</li> </ul>  <h5 id=\"returns-103\">Returns:</h5> <p>A tuple <code>(eval_results, global_step)</code>:</p> <ul> <li>\n<code>eval_results</code>: A <code>dict</code> mapping <code>string</code> to numeric values (<code>int</code>, <code>float</code>) that are the result of running eval_dict in the last step. <code>None</code> if no eval steps were run.</li> <li>\n<code>global_step</code>: The global step this evaluation corresponds to.</li> </ul>  <h5 id=\"raises-54\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>output_dir</code> is empty.</li> </ul>  <h3 id=\"infer\"><code>tf.contrib.learn.infer(restore_checkpoint_path, output_dict, feed_dict=None)</code></h3> <p>Restore graph from <code>restore_checkpoint_path</code> and run <code>output_dict</code> tensors.</p> <p>If <code>restore_checkpoint_path</code> is supplied, restore from checkpoint. Otherwise, init all variables.</p>  <h5 id=\"args-104\">Args:</h5> <ul> <li>\n<code>restore_checkpoint_path</code>: A string containing the path to a checkpoint to restore.</li> <li>\n<code>output_dict</code>: A <code>dict</code> mapping string names to <code>Tensor</code> objects to run. Tensors must all be from the same graph.</li> <li>\n<code>feed_dict</code>: <code>dict</code> object mapping <code>Tensor</code> objects to input values to feed.</li> </ul>  <h5 id=\"returns-104\">Returns:</h5> <p>Dict of values read from <code>output_dict</code> tensors. Keys are the same as <code>output_dict</code>, values are the results read from the corresponding <code>Tensor</code> in <code>output_dict</code>.</p>  <h5 id=\"raises-55\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>output_dict</code> or <code>feed_dicts</code> is None or empty.</li> </ul>  <h3 id=\"run_feeds\"><code>tf.contrib.learn.run_feeds(*args, **kwargs)</code></h3> <p>See run_feeds_iter(). Returns a <code>list</code> instead of an iterator.</p>  <h3 id=\"run_n\"><code>tf.contrib.learn.run_n(output_dict, feed_dict=None, restore_checkpoint_path=None, n=1)</code></h3> <p>Run <code>output_dict</code> tensors <code>n</code> times, with the same <code>feed_dict</code> each run.</p>  <h5 id=\"args-105\">Args:</h5> <ul> <li>\n<code>output_dict</code>: A <code>dict</code> mapping string names to tensors to run. Must all be from the same graph.</li> <li>\n<code>feed_dict</code>: <code>dict</code> of input values to feed each run.</li> <li>\n<code>restore_checkpoint_path</code>: A string containing the path to a checkpoint to restore.</li> <li>\n<code>n</code>: Number of times to repeat.</li> </ul>  <h5 id=\"returns-105\">Returns:</h5> <p>A list of <code>n</code> <code>dict</code> objects, each containing values read from <code>output_dict</code> tensors.</p>  <h3 id=\"train\"><code>tf.contrib.learn.train(graph, output_dir, train_op, loss_op, global_step_tensor=None, init_op=None, init_feed_dict=None, init_fn=None, log_every_steps=10, supervisor_is_chief=True, supervisor_master='', supervisor_save_model_secs=600, keep_checkpoint_max=5, supervisor_save_summaries_steps=100, feed_fn=None, steps=None, fail_on_nan_loss=True, monitors=None, max_steps=None)</code></h3> <p>Train a model.</p> <p>Given <code>graph</code>, a directory to write outputs to (<code>output_dir</code>), and some ops, run a training loop. The given <code>train_op</code> performs one step of training on the model. The <code>loss_op</code> represents the objective function of the training. It is expected to increment the <code>global_step_tensor</code>, a scalar integer tensor counting training steps. This function uses <code>Supervisor</code> to initialize the graph (from a checkpoint if one is available in <code>output_dir</code>), write summaries defined in the graph, and write regular checkpoints as defined by <code>supervisor_save_model_secs</code>.</p> <p>Training continues until <code>global_step_tensor</code> evaluates to <code>max_steps</code>, or, if <code>fail_on_nan_loss</code>, until <code>loss_op</code> evaluates to <code>NaN</code>. In that case the program is terminated with exit code 1.</p>  <h5 id=\"args-106\">Args:</h5> <ul> <li>\n<code>graph</code>: A graph to train. It is expected that this graph is not in use elsewhere.</li> <li>\n<code>output_dir</code>: A directory to write outputs to.</li> <li>\n<code>train_op</code>: An op that performs one training step when run.</li> <li>\n<code>loss_op</code>: A scalar loss tensor.</li> <li>\n<code>global_step_tensor</code>: A tensor representing the global step. If none is given, one is extracted from the graph using the same logic as in <code>Supervisor</code>.</li> <li>\n<code>init_op</code>: An op that initializes the graph. If <code>None</code>, use <code>Supervisor</code>'s default.</li> <li>\n<code>init_feed_dict</code>: A dictionary that maps <code>Tensor</code> objects to feed values. This feed dictionary will be used when <code>init_op</code> is evaluated.</li> <li>\n<code>init_fn</code>: Optional callable passed to Supervisor to initialize the model.</li> <li>\n<code>log_every_steps</code>: Output logs regularly. The logs contain timing data and the current loss.</li> <li>\n<code>supervisor_is_chief</code>: Whether the current process is the chief supervisor in charge of restoring the model and running standard services.</li> <li>\n<code>supervisor_master</code>: The master string to use when preparing the session.</li> <li>\n<code>supervisor_save_model_secs</code>: Save a checkpoint every <code>supervisor_save_model_secs</code> seconds when training.</li> <li>\n<code>keep_checkpoint_max</code>: The maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept. This is simply passed as the max_to_keep arg to tf.Saver constructor.</li> <li>\n<code>supervisor_save_summaries_steps</code>: Save summaries every <code>supervisor_save_summaries_steps</code> seconds when training.</li> <li>\n<code>feed_fn</code>: A function that is called every iteration to produce a <code>feed_dict</code> passed to <code>session.run</code> calls. Optional.</li> <li>\n<code>steps</code>: Trains for this many steps (e.g. current global step + <code>steps</code>).</li> <li>\n<code>fail_on_nan_loss</code>: If true, raise <code>NanLossDuringTrainingError</code> if <code>loss_op</code> evaluates to <code>NaN</code>. If false, continue training as if nothing happened.</li> <li>\n<code>monitors</code>: List of <code>BaseMonitor</code> subclass instances. Used for callbacks inside the training loop.</li> <li>\n<code>max_steps</code>: Number of total steps for which to train model. If <code>None</code>, train forever. Two calls fit(steps=100) means 200 training iterations. On the other hand two calls of fit(max_steps=100) means, second call will not do any iteration since first call did all 100 steps.</li> </ul>  <h5 id=\"returns-106\">Returns:</h5> <p>The final loss value.</p>  <h5 id=\"raises-56\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If <code>output_dir</code>, <code>train_op</code>, <code>loss_op</code>, or <code>global_step_tensor</code> is not provided. See <code>tf.contrib.framework.get_global_step</code> for how we look up the latter if not provided explicitly.</li> <li>\n<code>NanLossDuringTrainingError</code>: If <code>fail_on_nan_loss</code> is <code>True</code>, and loss ever evaluates to <code>NaN</code>.</li> <li>\n<code>ValueError</code>: If both <code>steps</code> and <code>max_steps</code> are not <code>None</code>.</li> </ul>  <h2 id=\"input-processing\">Input processing</h2> <p>Queue and read batched input data.</p>  <h3 id=\"extract_dask_data\"><code>tf.contrib.learn.extract_dask_data(data)</code></h3> <p>Extract data from dask.Series or dask.DataFrame for predictors.</p>  <h3 id=\"extract_dask_labels\"><code>tf.contrib.learn.extract_dask_labels(labels)</code></h3> <p>Extract data from dask.Series for labels.</p>  <h3 id=\"extract_pandas_data\"><code>tf.contrib.learn.extract_pandas_data(data)</code></h3> <p>Extract data from pandas.DataFrame for predictors.</p> <p>Given a DataFrame, will extract the values and cast them to float. The DataFrame is expected to contain values of type int, float or bool.</p>  <h5 id=\"args-107\">Args:</h5> <ul> <li>\n<code>data</code>: <code>pandas.DataFrame</code> containing the data to be extracted.</li> </ul>  <h5 id=\"returns-107\">Returns:</h5> <p>A numpy <code>ndarray</code> of the DataFrame's values as floats.</p>  <h5 id=\"raises-57\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if data contains types other than int, float or bool.</li> </ul>  <h3 id=\"extract_pandas_labels\"><code>tf.contrib.learn.extract_pandas_labels(labels)</code></h3> <p>Extract data from pandas.DataFrame for labels.</p>  <h5 id=\"args-108\">Args:</h5> <ul> <li>\n<code>labels</code>: <code>pandas.DataFrame</code> or <code>pandas.Series</code> containing one column of labels to be extracted.</li> </ul>  <h5 id=\"returns-108\">Returns:</h5> <p>A numpy <code>ndarray</code> of labels from the DataFrame.</p>  <h5 id=\"raises-58\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if more than one column is found or type is not int, float or bool.</li> </ul>  <h3 id=\"extract_pandas_matrix\"><code>tf.contrib.learn.extract_pandas_matrix(data)</code></h3> <p>Extracts numpy matrix from pandas DataFrame.</p>  <h5 id=\"args-109\">Args:</h5> <ul> <li>\n<code>data</code>: <code>pandas.DataFrame</code> containing the data to be extracted.</li> </ul>  <h5 id=\"returns-109\">Returns:</h5> <p>A numpy <code>ndarray</code> of the DataFrame's values.</p>  <h3 id=\"read_batch_examples\"><code>tf.contrib.learn.read_batch_examples(file_pattern, batch_size, reader, randomize_input=True, num_epochs=None, queue_capacity=10000, num_threads=1, read_batch_size=1, parse_fn=None, name=None)</code></h3> <p>Adds operations to read, queue, batch <code>Example</code> protos.</p> <p>Given file pattern (or list of files), will setup a queue for file names, read <code>Example</code> proto using provided <code>reader</code>, use batch queue to create batches of examples of size <code>batch_size</code>.</p> <p>All queue runners are added to the queue runners collection, and may be started via <code>start_queue_runners</code>.</p> <p>All ops are added to the default graph.</p> <p>Use <code>parse_fn</code> if you need to do parsing / processing on single examples.</p>  <h5 id=\"args-110\">Args:</h5> <ul> <li>\n<code>file_pattern</code>: List of files or pattern of file paths containing <code>Example</code> records. See <code>tf.gfile.Glob</code> for pattern rules.</li> <li>\n<code>batch_size</code>: An int or scalar <code>Tensor</code> specifying the batch size to use.</li> <li>\n<code>reader</code>: A function or class that returns an object with <code>read</code> method, (filename tensor) -&gt; (example tensor).</li> <li>\n<code>randomize_input</code>: Whether the input should be randomized.</li> <li>\n<code>num_epochs</code>: Integer specifying the number of times to read through the dataset. If <code>None</code>, cycles through the dataset forever. NOTE - If specified, creates a variable that must be initialized, so call <code>tf.initialize_all_variables()</code> as shown in the tests.</li> <li>\n<code>queue_capacity</code>: Capacity for input queue.</li> <li>\n<code>num_threads</code>: The number of threads enqueuing examples.</li> <li>\n<code>read_batch_size</code>: An int or scalar <code>Tensor</code> specifying the number of records to read at once</li> <li>\n<code>parse_fn</code>: Parsing function, takes <code>Example</code> Tensor returns parsed representation. If <code>None</code>, no parsing is done.</li> <li>\n<code>name</code>: Name of resulting op.</li> </ul>  <h5 id=\"returns-110\">Returns:</h5> <p>String <code>Tensor</code> of batched <code>Example</code> proto. If <code>keep_keys</code> is True, then returns tuple of string <code>Tensor</code>s, where first value is the key.</p>  <h5 id=\"raises-59\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: for invalid inputs.</li> </ul>  <h3 id=\"read_batch_features\"><code>tf.contrib.learn.read_batch_features(file_pattern, batch_size, features, reader, randomize_input=True, num_epochs=None, queue_capacity=10000, reader_num_threads=1, parser_num_threads=1, name=None)</code></h3> <p>Adds operations to read, queue, batch and parse <code>Example</code> protos.</p> <p>Given file pattern (or list of files), will setup a queue for file names, read <code>Example</code> proto using provided <code>reader</code>, use batch queue to create batches of examples of size <code>batch_size</code> and parse example given <code>features</code> specification.</p> <p>All queue runners are added to the queue runners collection, and may be started via <code>start_queue_runners</code>.</p> <p>All ops are added to the default graph.</p>  <h5 id=\"args-111\">Args:</h5> <ul> <li>\n<code>file_pattern</code>: List of files or pattern of file paths containing <code>Example</code> records. See <code>tf.gfile.Glob</code> for pattern rules.</li> <li>\n<code>batch_size</code>: An int or scalar <code>Tensor</code> specifying the batch size to use.</li> <li>\n<code>features</code>: A <code>dict</code> mapping feature keys to <code>FixedLenFeature</code> or <code>VarLenFeature</code> values.</li> <li>\n<code>reader</code>: A function or class that returns an object with <code>read</code> method, (filename tensor) -&gt; (example tensor).</li> <li>\n<code>randomize_input</code>: Whether the input should be randomized.</li> <li>\n<code>num_epochs</code>: Integer specifying the number of times to read through the dataset. If None, cycles through the dataset forever. NOTE - If specified, creates a variable that must be initialized, so call tf.initialize_local_variables() as shown in the tests.</li> <li>\n<code>queue_capacity</code>: Capacity for input queue.</li> <li>\n<code>reader_num_threads</code>: The number of threads to read examples.</li> <li>\n<code>parser_num_threads</code>: The number of threads to parse examples. records to read at once</li> <li>\n<code>name</code>: Name of resulting op.</li> </ul>  <h5 id=\"returns-111\">Returns:</h5> <p>A dict of <code>Tensor</code> or <code>SparseTensor</code> objects for each in <code>features</code>. If <code>keep_keys</code> is <code>True</code>, returns tuple of string <code>Tensor</code> and above dict.</p>  <h5 id=\"raises-60\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: for invalid inputs.</li> </ul>  <h3 id=\"read_batch_record_features\"><code>tf.contrib.learn.read_batch_record_features(file_pattern, batch_size, features, randomize_input=True, num_epochs=None, queue_capacity=10000, reader_num_threads=1, parser_num_threads=1, name='dequeue_record_examples')</code></h3> <p>Reads TFRecord, queues, batches and parses <code>Example</code> proto.</p> <p>See more detailed description in <code>read_examples</code>.</p>  <h5 id=\"args-112\">Args:</h5> <ul> <li>\n<code>file_pattern</code>: List of files or pattern of file paths containing <code>Example</code> records. See <code>tf.gfile.Glob</code> for pattern rules.</li> <li>\n<code>batch_size</code>: An int or scalar <code>Tensor</code> specifying the batch size to use.</li> <li>\n<code>features</code>: A <code>dict</code> mapping feature keys to <code>FixedLenFeature</code> or <code>VarLenFeature</code> values.</li> <li>\n<code>randomize_input</code>: Whether the input should be randomized.</li> <li>\n<code>num_epochs</code>: Integer specifying the number of times to read through the dataset. If None, cycles through the dataset forever. NOTE - If specified, creates a variable that must be initialized, so call tf.initialize_local_variables() as shown in the tests.</li> <li>\n<code>queue_capacity</code>: Capacity for input queue.</li> <li>\n<code>reader_num_threads</code>: The number of threads to read examples.</li> <li>\n<code>parser_num_threads</code>: The number of threads to parse examples.</li> <li>\n<code>name</code>: Name of resulting op.</li> </ul>  <h5 id=\"returns-112\">Returns:</h5> <p>A dict of <code>Tensor</code> or <code>SparseTensor</code> objects for each in <code>features</code>.</p>  <h5 id=\"raises-61\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: for invalid inputs.</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.learn.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.learn.html</a>\n  </p>\n</div>\n","contrib.distributions":"<h1 id=\"statistical-distributions-contrib\">Statistical distributions (contrib)</h1> \n<div class=\"toc\">\n<p>Contents</p> <ul> <li><a href=\"#statistical-distributions-contrib\">Statistical distributions (contrib)</a></li> <ul> <li><a href=\"#classes-for-statistical-distributions\">Classes for statistical distributions.</a></li> <ul> <li><a href=\"#base-classes\">Base classes</a></li> <li><a href=\"#Distribution\"><code>class tf.contrib.distributions.Distribution</code></a></li> <li><a href=\"#api\">API</a></li> <li><a href=\"#broadcasting-batching-and-shapes\">Broadcasting, batching, and shapes</a></li> <li><a href=\"#univariate-scalar-distributions\">Univariate (scalar) distributions</a></li> <li><a href=\"#Binomial\"><code>class tf.contrib.distributions.Binomial</code></a></li> <li><a href=\"#Bernoulli\"><code>class tf.contrib.distributions.Bernoulli</code></a></li> <li><a href=\"#Beta\"><code>class tf.contrib.distributions.Beta</code></a></li> <li><a href=\"#Categorical\"><code>class tf.contrib.distributions.Categorical</code></a></li> <li><a href=\"#Chi2\"><code>class tf.contrib.distributions.Chi2</code></a></li> <li><a href=\"#Exponential\"><code>class tf.contrib.distributions.Exponential</code></a></li> <li><a href=\"#Gamma\"><code>class tf.contrib.distributions.Gamma</code></a></li> <li><a href=\"#InverseGamma\"><code>class tf.contrib.distributions.InverseGamma</code></a></li> <li><a href=\"#Laplace\"><code>class tf.contrib.distributions.Laplace</code></a></li> <li><a href=\"#Normal\"><code>class tf.contrib.distributions.Normal</code></a></li> <li><a href=\"#StudentT\"><code>class tf.contrib.distributions.StudentT</code></a></li> <li><a href=\"#Uniform\"><code>class tf.contrib.distributions.Uniform</code></a></li> <li><a href=\"#multivariate-distributions\">Multivariate distributions</a></li> <li><a href=\"#MultivariateNormalDiag\"><code>class tf.contrib.distributions.MultivariateNormalDiag</code></a></li> <li><a href=\"#MultivariateNormalFull\"><code>class tf.contrib.distributions.MultivariateNormalFull</code></a></li> <li><a href=\"#MultivariateNormalCholesky\"><code>class tf.contrib.distributions.MultivariateNormalCholesky</code></a></li> <li><a href=\"#batch_matrix_diag_transform\"><code>tf.contrib.distributions.batch_matrix_diag_transform(matrix, transform=None, name=None)</code></a></li> <li><a href=\"#Dirichlet\"><code>class tf.contrib.distributions.Dirichlet</code></a></li> <li><a href=\"#DirichletMultinomial\"><code>class tf.contrib.distributions.DirichletMultinomial</code></a></li> <li><a href=\"#Multinomial\"><code>class tf.contrib.distributions.Multinomial</code></a></li> <li><a href=\"#transformed-distributions\">Transformed distributions</a></li> <li><a href=\"#TransformedDistribution\"><code>class tf.contrib.distributions.TransformedDistribution</code></a></li> </ul> <li><a href=\"#posterior-inference-with-conjugate-priors\">Posterior inference with conjugate priors.</a></li> <ul> <li><a href=\"#normal-likelihood-with-conjugate-prior\">Normal likelihood with conjugate prior.</a></li> <li><a href=\"#normal_conjugates_known_sigma_posterior\"><code>tf.contrib.distributions.normal_conjugates_known_sigma_posterior(prior, sigma, s, n)</code></a></li> <li><a href=\"#normal_congugates_known_sigma_predictive\"><code>tf.contrib.distributions.normal_congugates_known_sigma_predictive(prior, sigma, s, n)</code></a></li> </ul> <li><a href=\"#kullback-leibler-divergence\">Kullback Leibler Divergence</a></li> <ul> <li><a href=\"#kl\"><code>tf.contrib.distributions.kl(dist_a, dist_b, allow_nan=False, name=None)</code></a></li> <li><a href=\"#RegisterKL\"><code>class tf.contrib.distributions.RegisterKL</code></a></li> </ul> <li><a href=\"#other-functions-and-classes\">Other Functions and Classes</a></li> <ul> <li><a href=\"#BaseDistribution\"><code>class tf.contrib.distributions.BaseDistribution</code></a></li> <li><a href=\"#MultivariateNormalDiagPlusVDVT\"><code>class tf.contrib.distributions.MultivariateNormalDiagPlusVDVT</code></a></li> </ul>\n</ul>\n</ul> </div> <p>Classes representing statistical distributions and ops for working with them.</p>  <h2 id=\"classes-for-statistical-distributions\">Classes for statistical distributions.</h2> <p>Classes that represent batches of statistical distributions. Each class is initialized with parameters that define the distributions.</p>  <h3 id=\"base-classes\">Base classes</h3>  <h3 id=\"Distribution\"><code>class tf.contrib.distributions.Distribution</code></h3> <p>Fully-featured abstract base class for probability distributions.</p> <p>This class defines the API for probability distributions. Users will only ever instantiate subclasses of <code>Distribution</code>.</p> <h3 id=\"api\">API</h3> <p>The key methods for probability distributions are defined here.</p> <p>To keep ops generated by the distribution tied together by name, subclasses should override <code>name</code> and use it to prepend names of ops in other methods (see <code>cdf</code> for an example).</p> <p>Subclasses that wish to support <code>cdf</code> and <code>log_cdf</code> can override <code>log_cdf</code> and use the base class's implementation for <code>cdf</code>, or vice versa. The same goes for <code>log_prob</code> and <code>prob</code>.</p>  <h3 id=\"broadcasting-batching-and-shapes\">Broadcasting, batching, and shapes</h3> <p>All distributions support batches of independent distributions of that type. The batch shape is determined by broadcasting together the parameters.</p> <p>The shape of arguments to <code>__init__</code>, <code>cdf</code>, <code>log_cdf</code>, <code>prob</code>, and <code>log_prob</code> reflect this broadcasting, as does the return value of <code>sample</code> and <code>sample_n</code>.</p> <p><code>sample_n_shape = (n,) + batch_shape + event_shape</code>, where <code>sample_n_shape</code> is the shape of the <code>Tensor</code> returned from <code>sample_n</code>, <code>n</code> is the number of samples, <code>batch_shape</code> defines how many independent distributions there are, and <code>event_shape</code> defines the shape of samples from each of those independent distributions. Samples are independent along the <code>batch_shape</code> dimensions, but not necessarily so along the <code>event_shape</code> dimensions (dependending on the particulars of the underlying distribution).</p> <p>Using the <code>Uniform</code> distribution as an example:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">minval = 3.0\nmaxval = [[4.0, 6.0],\n          [10.0, 12.0]]\n\n# Broadcasting:\n# This instance represents 4 Uniform distributions. Each has a lower bound at\n# 3.0 as the `minval` parameter was broadcasted to match `maxval`'s shape.\nu = Uniform(minval, maxval)\n\n# `event_shape` is `TensorShape([])`.\nevent_shape = u.get_event_shape()\n# `event_shape_t` is a `Tensor` which will evaluate to [].\nevent_shape_t = u.event_shape\n\n# Sampling returns a sample per distribution.  `samples` has shape\n# (5, 2, 2), which is (n,) + batch_shape + event_shape, where n=5,\n# batch_shape=(2, 2), and event_shape=().\nsamples = u.sample_n(5)\n\n# The broadcasting holds across methods. Here we use `cdf` as an example. The\n# same holds for `log_cdf` and the likelihood functions.\n\n# `cum_prob` has shape (2, 2) as the `value` argument was broadcasted to the\n# shape of the `Uniform` instance.\ncum_prob_broadcast = u.cdf(4.0)\n\n# `cum_prob`'s shape is (2, 2), one per distribution. No broadcasting\n# occurred.\ncum_prob_per_dist = u.cdf([[4.0, 5.0],\n                           [6.0, 7.0]])\n\n# INVALID as the `value` argument is not broadcastable to the distribution's\n# shape.\ncum_prob_invalid = u.cdf([4.0, 5.0, 6.0])\n\n### Parameter values leading to undefined statistics or distributions.\n\nSome distributions do not have well-defined statistics for all initialization\nparameter values.  For example, the beta distribution is parameterized by\npositive real numbers `a` and `b`, and does not have well-defined mode if\n`a &lt; 1` or `b &lt; 1`.\n\nThe user is given the option of raising an exception or returning `NaN`.\n\n```python\na = tf.exp(tf.matmul(logits, weights_a))\nb = tf.exp(tf.matmul(logits, weights_b))\n\n# Will raise exception if ANY batch member has a &lt; 1 or b &lt; 1.\ndist = distributions.beta(a, b, allow_nan_stats=False)  # default is False\nmode = dist.mode().eval()\n\n# Will return NaN for batch members with either a &lt; 1 or b &lt; 1.\ndist = distributions.beta(a, b, allow_nan_stats=True)\nmode = dist.mode().eval()\n</pre> <p>In all cases, an exception is raised if <em>invalid</em> parameters are passed, e.g.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Will raise an exception if any Op is run.\nnegative_a = -1.0 * a  # beta distribution by definition has a &gt; 0.\ndist = distributions.beta(negative_a, b, allow_nan_stats=True)\ndist.mean().eval()\n</pre>  <h4 id=\"Distribution.allow_nan_stats\"><code>tf.contrib.distributions.Distribution.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Distribution.batch_shape\"><code>tf.contrib.distributions.Distribution.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p> <h5 id=\"args\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul> <h5 id=\"returns\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Distribution.cdf\"><code>tf.contrib.distributions.Distribution.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"Distribution.dtype\"><code>tf.contrib.distributions.Distribution.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"Distribution.entropy\"><code>tf.contrib.distributions.Distribution.entropy(name='entropy')</code></h4> <p>Entropy of the distribution in nats.</p>  <h4 id=\"Distribution.event_shape\"><code>tf.contrib.distributions.Distribution.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-2\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-2\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Distribution.get_batch_shape\"><code>tf.contrib.distributions.Distribution.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h4 id=\"Distribution.get_event_shape\"><code>tf.contrib.distributions.Distribution.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h4 id=\"Distribution.is_continuous\"><code>tf.contrib.distributions.Distribution.is_continuous</code></h4>  <h4 id=\"Distribution.is_reparameterized\"><code>tf.contrib.distributions.Distribution.is_reparameterized</code></h4>  <h4 id=\"Distribution.log_cdf\"><code>tf.contrib.distributions.Distribution.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"Distribution.log_pdf\"><code>tf.contrib.distributions.Distribution.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Distribution.log_pmf\"><code>tf.contrib.distributions.Distribution.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Distribution.log_prob\"><code>tf.contrib.distributions.Distribution.log_prob(value, name='log_prob')</code></h4> <p>Log of the probability density/mass function.</p>  <h4 id=\"Distribution.mean\"><code>tf.contrib.distributions.Distribution.mean(name='mean')</code></h4> <p>Mean of the distribution.</p>  <h4 id=\"Distribution.mode\"><code>tf.contrib.distributions.Distribution.mode(name='mode')</code></h4> <p>Mode of the distribution.</p>  <h4 id=\"Distribution.name\"><code>tf.contrib.distributions.Distribution.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"Distribution.pdf\"><code>tf.contrib.distributions.Distribution.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Distribution.pmf\"><code>tf.contrib.distributions.Distribution.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Distribution.prob\"><code>tf.contrib.distributions.Distribution.prob(value, name='prob')</code></h4> <p>Probability density/mass function.</p>  <h4 id=\"Distribution.sample\"><code>tf.contrib.distributions.Distribution.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-3\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-3\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Distribution.sample_n\"><code>tf.contrib.distributions.Distribution.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Generate <code>n</code> samples.</p>  <h5 id=\"args-4\">Args:</h5> <ul> <li>\n<code>n</code>: scalar. Number of samples to draw from each distribution.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-4\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"Distribution.std\"><code>tf.contrib.distributions.Distribution.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"Distribution.validate_args\"><code>tf.contrib.distributions.Distribution.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Distribution.variance\"><code>tf.contrib.distributions.Distribution.variance(name='variance')</code></h4> <p>Variance of the distribution.</p>  <h3 id=\"univariate-scalar-distributions\">Univariate (scalar) distributions</h3>  <h3 id=\"Binomial\"><code>class tf.contrib.distributions.Binomial</code></h3> <p>Binomial distribution.</p> <p>This distribution is parameterized by a vector <code>p</code> of probabilities and <code>n</code>, the total counts.</p>  <h4 id=\"mathematical-details\">Mathematical details</h4> <p>The Binomial is a distribution over the number of successes in <code>n</code> independent trials, with each trial having the same probability of success <code>p</code>. The probability mass function (pmf):</p> <p><code>pmf(k) = n! / (k! * (n - k)!) * (p)^k * (1 - p)^(n - k)</code></p> <h4 id=\"examples\">Examples</h4> <p>Create a single distribution, corresponding to 5 coin flips.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">dist = Binomial(n=5., p=.5)\n</pre> <p>Create a single distribution (using logits), corresponding to 5 coin flips.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">dist = Binomial(n=5., logits=0.)\n</pre> <p>Creates 3 distributions with the third distribution most likely to have successes.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">p = [.2, .3, .8]\n# n will be broadcast to [4., 4., 4.], to match p.\ndist = Binomial(n=4., p=p)\n</pre> <p>The distribution functions can be evaluated on counts.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># counts same shape as p.\ncounts = [1., 2, 3]\ndist.prob(counts)  # Shape [3]\n\n# p will be broadcast to [[.2, .3, .8], [.2, .3, .8]] to match counts.\ncounts = [[1., 2, 1], [2, 2, 4]]\ndist.prob(counts)  # Shape [2, 3]\n\n# p will be broadcast to shape [5, 7, 3] to match counts.\ncounts = [[...]]  # Shape [5, 7, 3]\ndist.prob(counts)  # Shape [5, 7, 3]\n</pre>  <h4 id=\"Binomial.__init__\"><code>tf.contrib.distributions.Binomial.__init__(n, logits=None, p=None, validate_args=True, allow_nan_stats=False, name='Binomial')</code></h4> <p>Initialize a batch of Binomial distributions.</p>  <h5 id=\"args-5\">Args:</h5> <ul> <li>\n<code>n</code>: Non-negative floating point tensor with shape broadcastable to <code>[N1,..., Nm]</code> with <code>m &gt;= 0</code> and the same dtype as <code>p</code> or <code>logits</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different Binomial distributions. Its components should be equal to integer values.</li> <li>\n<code>logits</code>: Floating point tensor representing the log-odds of a positive event with shape broadcastable to <code>[N1,..., Nm]</code> <code>m &gt;= 0</code>, and the same dtype as <code>n</code>. Each entry represents logits for the probability of success for independent Binomial distributions.</li> <li>\n<code>p</code>: Positive floating point tensor with shape broadcastable to <code>[N1,..., Nm]</code> <code>m &gt;= 0</code>, <code>p in [0, 1]</code>. Each entry represents the probability of success for independent Binomial distributions.</li> <li>\n<code>validate_args</code>: Whether to assert valid values for parameters <code>n</code> and <code>p</code>, and <code>x</code> in <code>prob</code> and <code>log_prob</code>. If <code>False</code>, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li><p><code>name</code>: The name to prefix Ops created by this distribution class.</p></li> <li><p><code>Examples</code>: </p></li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define 1-batch of a binomial distribution.\ndist = Binomial(n=2., p=.9)\n\n# Define a 2-batch.\ndist = Binomial(n=[4., 5], p=[.1, .3])\n</pre>  <h4 id=\"Binomial.allow_nan_stats\"><code>tf.contrib.distributions.Binomial.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Binomial.batch_shape\"><code>tf.contrib.distributions.Binomial.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-6\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-5\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Binomial.cdf\"><code>tf.contrib.distributions.Binomial.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"Binomial.dtype\"><code>tf.contrib.distributions.Binomial.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"Binomial.entropy\"><code>tf.contrib.distributions.Binomial.entropy(name='entropy')</code></h4> <p>Entropy of the distribution in nats.</p>  <h4 id=\"Binomial.event_shape\"><code>tf.contrib.distributions.Binomial.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-7\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-6\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Binomial.get_batch_shape\"><code>tf.contrib.distributions.Binomial.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-7\">Returns:</h5> <p>batch shape</p>  <h4 id=\"Binomial.get_event_shape\"><code>tf.contrib.distributions.Binomial.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-8\">Returns:</h5> <p>event shape</p>  <h4 id=\"Binomial.is_continuous\"><code>tf.contrib.distributions.Binomial.is_continuous</code></h4>  <h4 id=\"Binomial.is_reparameterized\"><code>tf.contrib.distributions.Binomial.is_reparameterized</code></h4>  <h4 id=\"Binomial.log_cdf\"><code>tf.contrib.distributions.Binomial.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"Binomial.log_pdf\"><code>tf.contrib.distributions.Binomial.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Binomial.log_pmf\"><code>tf.contrib.distributions.Binomial.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Binomial.log_prob\"><code>tf.contrib.distributions.Binomial.log_prob(counts, name='log_prob')</code></h4> <p><code>Log(P[counts])</code>, computed for every batch member.</p> <p>For each batch member of counts <code>k</code>, <code>P[counts]</code> is the probability that after sampling <code>n</code> draws from this Binomial distribution, the number of successes is <code>k</code>. Note that different sequences of draws can result in the same counts, thus the probability includes a combinatorial coefficient.</p>  <h5 id=\"args-8\">Args:</h5> <ul> <li>\n<code>counts</code>: Non-negative tensor with dtype <code>dtype</code> and whose shape can be broadcast with <code>self.p</code> and <code>self.n</code>. <code>counts</code> is only legal if it is less than or equal to <code>n</code> and its components are equal to integer values.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"log_prob\".</li> </ul>  <h5 id=\"returns-9\">Returns:</h5> <p>Log probabilities for each record, shape <code>[N1,...,Nm]</code>.</p>  <h4 id=\"Binomial.logits\"><code>tf.contrib.distributions.Binomial.logits</code></h4> <p>Log-odds.</p>  <h4 id=\"Binomial.mean\"><code>tf.contrib.distributions.Binomial.mean(name='mean')</code></h4> <p>Mean of the distribution.</p>  <h4 id=\"Binomial.mode\"><code>tf.contrib.distributions.Binomial.mode(name='mode')</code></h4> <p>Mode of the distribution.</p> <p>Note that when <code>(n + 1) * p</code> is an integer, there are actually two modes. Namely, <code>(n + 1) * p</code> and <code>(n + 1) * p - 1</code> are both modes. Here we return only the larger of the two modes.</p>  <h5 id=\"args-9\">Args:</h5> <ul> <li>\n<code>name</code>: The name for this op.</li> </ul>  <h5 id=\"returns-10\">Returns:</h5> <p>The mode of the Binomial distribution.</p>  <h4 id=\"Binomial.n\"><code>tf.contrib.distributions.Binomial.n</code></h4> <p>Number of trials.</p>  <h4 id=\"Binomial.name\"><code>tf.contrib.distributions.Binomial.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"Binomial.p\"><code>tf.contrib.distributions.Binomial.p</code></h4> <p>Probability of success.</p>  <h4 id=\"Binomial.pdf\"><code>tf.contrib.distributions.Binomial.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Binomial.pmf\"><code>tf.contrib.distributions.Binomial.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Binomial.prob\"><code>tf.contrib.distributions.Binomial.prob(counts, name='prob')</code></h4> <p><code>P[counts]</code>, computed for every batch member.</p> <p>For each batch member of counts <code>k</code>, <code>P[counts]</code> is the probability that after sampling <code>n</code> draws from this Binomial distribution, the number of successes is <code>k</code>. Note that different sequences of draws can result in the same counts, thus the probability includes a combinatorial coefficient.</p>  <h5 id=\"args-10\">Args:</h5> <ul> <li>\n<code>counts</code>: Non-negative tensor with dtype <code>dtype</code> and whose shape can be broadcast with <code>self.p</code> and <code>self.n</code>. <code>counts</code> is only legal if it is less than or equal to <code>n</code> and its components are equal to integer values.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"prob\".</li> </ul>  <h5 id=\"returns-11\">Returns:</h5> <p>Probabilities for each record, shape <code>[N1,...,Nm]</code>.</p>  <h4 id=\"Binomial.sample\"><code>tf.contrib.distributions.Binomial.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-11\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-12\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Binomial.sample_n\"><code>tf.contrib.distributions.Binomial.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Generate <code>n</code> samples.</p>  <h5 id=\"args-12\">Args:</h5> <ul> <li>\n<code>n</code>: scalar. Number of samples to draw from each distribution.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-13\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"Binomial.std\"><code>tf.contrib.distributions.Binomial.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"Binomial.validate_args\"><code>tf.contrib.distributions.Binomial.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Binomial.variance\"><code>tf.contrib.distributions.Binomial.variance(name='variance')</code></h4> <p>Variance of the distribution.</p>  <h3 id=\"Bernoulli\"><code>class tf.contrib.distributions.Bernoulli</code></h3> <p>Bernoulli distribution.</p> <p>The Bernoulli distribution is parameterized by p, the probability of a positive event.</p>  <h4 id=\"Bernoulli.__init__\"><code>tf.contrib.distributions.Bernoulli.__init__(logits=None, p=None, dtype=tf.int32, validate_args=True, allow_nan_stats=False, name='Bernoulli')</code></h4> <p>Construct Bernoulli distributions.</p>  <h5 id=\"args-13\">Args:</h5> <ul> <li>\n<code>logits</code>: An N-D <code>Tensor</code> representing the log-odds of a positive event. Each entry in the <code>Tensor</code> parametrizes an independent Bernoulli distribution where the probability of an event is sigmoid(logits).</li> <li>\n<code>p</code>: An N-D <code>Tensor</code> representing the probability of a positive event. Each entry in the <code>Tensor</code> parameterizes an independent Bernoulli distribution.</li> <li>\n<code>dtype</code>: dtype for samples.</li> <li>\n<code>validate_args</code>: Whether to assert that <code>0 &lt;= p &lt;= 1</code>. If not validate_args, <code>log_pmf</code> may return nans.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: A name for this distribution.</li> </ul> <h5 id=\"raises\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: If p and logits are passed, or if neither are passed.</li> </ul>  <h4 id=\"Bernoulli.allow_nan_stats\"><code>tf.contrib.distributions.Bernoulli.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Bernoulli.batch_shape\"><code>tf.contrib.distributions.Bernoulli.batch_shape(name='batch_shape')</code></h4>  <h4 id=\"Bernoulli.cdf\"><code>tf.contrib.distributions.Bernoulli.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"Bernoulli.dtype\"><code>tf.contrib.distributions.Bernoulli.dtype</code></h4>  <h4 id=\"Bernoulli.entropy\"><code>tf.contrib.distributions.Bernoulli.entropy(name='entropy')</code></h4> <p>Entropy of the distribution.</p>  <h5 id=\"args-14\">Args:</h5> <ul> <li>\n<code>name</code>: Name for the op.</li> </ul>  <h5 id=\"returns-14\">Returns:</h5> <ul> <li>\n<code>entropy</code>: <code>Tensor</code> of the same type and shape as <code>p</code>.</li> </ul>  <h4 id=\"Bernoulli.event_shape\"><code>tf.contrib.distributions.Bernoulli.event_shape(name='event_shape')</code></h4>  <h4 id=\"Bernoulli.get_batch_shape\"><code>tf.contrib.distributions.Bernoulli.get_batch_shape()</code></h4>  <h4 id=\"Bernoulli.get_event_shape\"><code>tf.contrib.distributions.Bernoulli.get_event_shape()</code></h4>  <h4 id=\"Bernoulli.is_continuous\"><code>tf.contrib.distributions.Bernoulli.is_continuous</code></h4>  <h4 id=\"Bernoulli.is_reparameterized\"><code>tf.contrib.distributions.Bernoulli.is_reparameterized</code></h4>  <h4 id=\"Bernoulli.log_cdf\"><code>tf.contrib.distributions.Bernoulli.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"Bernoulli.log_pdf\"><code>tf.contrib.distributions.Bernoulli.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Bernoulli.log_pmf\"><code>tf.contrib.distributions.Bernoulli.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Bernoulli.log_prob\"><code>tf.contrib.distributions.Bernoulli.log_prob(event, name='log_prob')</code></h4> <p>Log of the probability mass function.</p>  <h5 id=\"args-15\">Args:</h5> <ul> <li>\n<code>event</code>: <code>int32</code> or <code>int64</code> binary Tensor.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-15\">Returns:</h5> <p>The log-probabilities of the events.</p>  <h4 id=\"Bernoulli.logits\"><code>tf.contrib.distributions.Bernoulli.logits</code></h4>  <h4 id=\"Bernoulli.mean\"><code>tf.contrib.distributions.Bernoulli.mean(name='mean')</code></h4> <p>Mean of the distribution.</p>  <h5 id=\"args-16\">Args:</h5> <ul> <li>\n<code>name</code>: Name for the op.</li> </ul>  <h5 id=\"returns-16\">Returns:</h5> <ul> <li>\n<code>mean</code>: <code>Tensor</code> of the same type and shape as <code>p</code>.</li> </ul>  <h4 id=\"Bernoulli.mode\"><code>tf.contrib.distributions.Bernoulli.mode(name='mode')</code></h4> <p>Mode of the distribution.</p> <p>1 if p &gt; 1-p. 0 otherwise.</p>  <h5 id=\"args-17\">Args:</h5> <ul> <li>\n<code>name</code>: Name for the op.</li> </ul>  <h5 id=\"returns-17\">Returns:</h5> <ul> <li>\n<code>mode</code>: binary <code>Tensor</code> of type self.dtype.</li> </ul>  <h4 id=\"Bernoulli.name\"><code>tf.contrib.distributions.Bernoulli.name</code></h4>  <h4 id=\"Bernoulli.p\"><code>tf.contrib.distributions.Bernoulli.p</code></h4>  <h4 id=\"Bernoulli.pdf\"><code>tf.contrib.distributions.Bernoulli.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Bernoulli.pmf\"><code>tf.contrib.distributions.Bernoulli.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Bernoulli.prob\"><code>tf.contrib.distributions.Bernoulli.prob(event, name='prob')</code></h4> <p>Probability mass function.</p>  <h5 id=\"args-18\">Args:</h5> <ul> <li>\n<code>event</code>: <code>int32</code> or <code>int64</code> binary Tensor; must be broadcastable with <code>p</code>.</li> <li>\n<code>name</code>: A name for this operation.</li> </ul>  <h5 id=\"returns-18\">Returns:</h5> <p>The probabilities of the events.</p>  <h4 id=\"Bernoulli.q\"><code>tf.contrib.distributions.Bernoulli.q</code></h4> <p>1-p.</p>  <h4 id=\"Bernoulli.sample\"><code>tf.contrib.distributions.Bernoulli.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-19\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-19\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Bernoulli.sample_n\"><code>tf.contrib.distributions.Bernoulli.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Generate <code>n</code> samples.</p>  <h5 id=\"args-20\">Args:</h5> <ul> <li>\n<code>n</code>: scalar. Number of samples to draw from each distribution.</li> <li>\n<code>seed</code>: Python integer seed for RNG.</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-20\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"Bernoulli.std\"><code>tf.contrib.distributions.Bernoulli.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h5 id=\"args-21\">Args:</h5> <ul> <li>\n<code>name</code>: Name for the op.</li> </ul>  <h5 id=\"returns-21\">Returns:</h5> <ul> <li>\n<code>std</code>: <code>Tensor</code> of the same type and shape as <code>p</code>.</li> </ul>  <h4 id=\"Bernoulli.validate_args\"><code>tf.contrib.distributions.Bernoulli.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Bernoulli.variance\"><code>tf.contrib.distributions.Bernoulli.variance(name='variance')</code></h4> <p>Variance of the distribution.</p>  <h5 id=\"args-22\">Args:</h5> <ul> <li>\n<code>name</code>: Name for the op.</li> </ul>  <h5 id=\"returns-22\">Returns:</h5> <ul> <li>\n<code>variance</code>: <code>Tensor</code> of the same type and shape as <code>p</code>.</li> </ul>  <h3 id=\"Beta\"><code>class tf.contrib.distributions.Beta</code></h3> <p>Beta distribution.</p> <p>This distribution is parameterized by <code>a</code> and <code>b</code> which are shape parameters.</p>  <h4 id=\"mathematical-details-2\">Mathematical details</h4> <p>The Beta is a distribution over the interval (0, 1). The distribution has hyperparameters <code>a</code> and <code>b</code> and probability mass function (pdf):</p> <p><code>pdf(x) = 1 / Beta(a, b) * x^(a - 1) * (1 - x)^(b - 1)</code></p> <p>where <code>Beta(a, b) = Gamma(a) * Gamma(b) / Gamma(a + b)</code> is the beta function.</p> <p>This class provides methods to create indexed batches of Beta distributions. One entry of the broacasted shape represents of <code>a</code> and <code>b</code> represents one single Beta distribution. When calling distribution functions (e.g. <code>dist.pdf(x)</code>), <code>a</code>, <code>b</code> and <code>x</code> are broadcast to the same shape (if possible). Every entry in a/b/x corresponds to a single Beta distribution.</p>  <h4 id=\"examples-2\">Examples</h4> <p>Creates 3 distributions. The distribution functions can be evaluated on x.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">a = [1, 2, 3]\nb = [1, 2, 3]\ndist = Beta(a, b)\n</pre> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># x same shape as a.\nx = [.2, .3, .7]\ndist.pdf(x)  # Shape [3]\n\n# a/b will be broadcast to [[1, 2, 3], [1, 2, 3]] to match x.\nx = [[.1, .4, .5], [.2, .3, .5]]\ndist.pdf(x)  # Shape [2, 3]\n\n# a/b will be broadcast to shape [5, 7, 3] to match x.\nx = [[...]]  # Shape [5, 7, 3]\ndist.pdf(x)  # Shape [5, 7, 3]\n</pre> <p>Creates a 2-batch of 3-class distributions.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">a = [[1, 2, 3], [4, 5, 6]]  # Shape [2, 3]\nb = 5  # Shape []\ndist = Beta(a, b)\n\n# x will be broadcast to [[.2, .3, .9], [.2, .3, .9]] to match a/b.\nx = [.2, .3, .9]\ndist.pdf(x)  # Shape [2]\n</pre>  <h4 id=\"Beta.__init__\"><code>tf.contrib.distributions.Beta.__init__(a, b, validate_args=True, allow_nan_stats=False, name='Beta')</code></h4> <p>Initialize a batch of Beta distributions.</p>  <h5 id=\"args-23\">Args:</h5> <ul> <li>\n<code>a</code>: Positive floating point tensor with shape broadcastable to <code>[N1,..., Nm]</code> <code>m &gt;= 0</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different Beta distributions. This also defines the dtype of the distribution.</li> <li>\n<code>b</code>: Positive floating point tensor with shape broadcastable to <code>[N1,..., Nm]</code> <code>m &gt;= 0</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different Beta distributions.</li> <li>\n<code>validate_args</code>: Whether to assert valid values for parameters <code>a</code> and <code>b</code>, and <code>x</code> in <code>prob</code> and <code>log_prob</code>. If <code>False</code>, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li><p><code>name</code>: The name to prefix Ops created by this distribution class.</p></li> <li><p><code>Examples</code>: </p></li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define 1-batch.\ndist = Beta(1.1, 2.0)\n\n# Define a 2-batch.\ndist = Beta([1.0, 2.0], [4.0, 5.0])\n</pre>  <h4 id=\"Beta.a\"><code>tf.contrib.distributions.Beta.a</code></h4> <p>Shape parameter.</p>  <h4 id=\"Beta.allow_nan_stats\"><code>tf.contrib.distributions.Beta.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Beta.b\"><code>tf.contrib.distributions.Beta.b</code></h4> <p>Shape parameter.</p>  <h4 id=\"Beta.batch_shape\"><code>tf.contrib.distributions.Beta.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-24\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-23\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Beta.cdf\"><code>tf.contrib.distributions.Beta.cdf(x, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"Beta.dtype\"><code>tf.contrib.distributions.Beta.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"Beta.entropy\"><code>tf.contrib.distributions.Beta.entropy(name='entropy')</code></h4> <p>Entropy of the distribution in nats.</p>  <h4 id=\"Beta.event_shape\"><code>tf.contrib.distributions.Beta.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-25\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-24\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Beta.get_batch_shape\"><code>tf.contrib.distributions.Beta.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-25\">Returns:</h5> <p>batch shape</p>  <h4 id=\"Beta.get_event_shape\"><code>tf.contrib.distributions.Beta.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-26\">Returns:</h5> <p>event shape</p>  <h4 id=\"Beta.is_continuous\"><code>tf.contrib.distributions.Beta.is_continuous</code></h4>  <h4 id=\"Beta.is_reparameterized\"><code>tf.contrib.distributions.Beta.is_reparameterized</code></h4>  <h4 id=\"Beta.log_cdf\"><code>tf.contrib.distributions.Beta.log_cdf(x, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"Beta.log_pdf\"><code>tf.contrib.distributions.Beta.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Beta.log_pmf\"><code>tf.contrib.distributions.Beta.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Beta.log_prob\"><code>tf.contrib.distributions.Beta.log_prob(x, name='log_prob')</code></h4> <p><code>Log(P[counts])</code>, computed for every batch member.</p>  <h5 id=\"args-26\">Args:</h5> <ul> <li>\n<code>x</code>: Non-negative floating point tensor whose shape can be broadcast with <code>self.a</code> and <code>self.b</code>. For fixed leading dimensions, the last dimension represents counts for the corresponding Beta distribution in <code>self.a</code> and <code>self.b</code>. <code>x</code> is only legal if 0 &lt; x &lt; 1.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"log_prob\".</li> </ul>  <h5 id=\"returns-27\">Returns:</h5> <p>Log probabilities for each record, shape <code>[N1,...,Nm]</code>.</p>  <h4 id=\"Beta.mean\"><code>tf.contrib.distributions.Beta.mean(name='mean')</code></h4> <p>Mean of the distribution.</p>  <h4 id=\"Beta.mode\"><code>tf.contrib.distributions.Beta.mode(name='mode')</code></h4> <p>Mode of the distribution.</p> <p>Note that the mode for the Beta distribution is only defined when <code>a &gt; 1</code>, <code>b &gt; 1</code>. This returns the mode when <code>a &gt; 1</code> and <code>b &gt; 1</code>, and NaN otherwise. If <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised rather than returning <code>NaN</code>.</p>  <h5 id=\"args-27\">Args:</h5> <ul> <li>\n<code>name</code>: The name for this op.</li> </ul>  <h5 id=\"returns-28\">Returns:</h5> <p>Mode of the Beta distribution.</p>  <h4 id=\"Beta.name\"><code>tf.contrib.distributions.Beta.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"Beta.pdf\"><code>tf.contrib.distributions.Beta.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Beta.pmf\"><code>tf.contrib.distributions.Beta.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Beta.prob\"><code>tf.contrib.distributions.Beta.prob(x, name='prob')</code></h4> <p><code>P[x]</code>, computed for every batch member.</p>  <h5 id=\"args-28\">Args:</h5> <ul> <li>\n<code>x</code>: Non-negative floating point tensor whose shape can be broadcast with <code>self.a</code> and <code>self.b</code>. For fixed leading dimensions, the last dimension represents x for the corresponding Beta distribution in <code>self.a</code> and <code>self.b</code>. <code>x</code> is only legal if is between 0 and 1.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"pdf\".</li> </ul>  <h5 id=\"returns-29\">Returns:</h5> <p>Probabilities for each record, shape <code>[N1,...,Nm]</code>.</p>  <h4 id=\"Beta.sample\"><code>tf.contrib.distributions.Beta.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-29\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-30\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Beta.sample_n\"><code>tf.contrib.distributions.Beta.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Beta Distributions.</p>  <h5 id=\"args-30\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-31\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by broadcasting the hyperparameters.</li> </ul>  <h4 id=\"Beta.std\"><code>tf.contrib.distributions.Beta.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"Beta.validate_args\"><code>tf.contrib.distributions.Beta.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Beta.variance\"><code>tf.contrib.distributions.Beta.variance(name='variance')</code></h4> <p>Variance of the distribution.</p>  <h3 id=\"Categorical\"><code>class tf.contrib.distributions.Categorical</code></h3> <p>Categorical distribution.</p> <p>The categorical distribution is parameterized by the log-probabilities of a set of classes.</p>  <h4 id=\"Categorical.__init__\"><code>tf.contrib.distributions.Categorical.__init__(logits, dtype=tf.int32, validate_args=True, allow_nan_stats=False, name='Categorical')</code></h4> <p>Initialize Categorical distributions using class log-probabilities.</p>  <h5 id=\"args-31\">Args:</h5> <ul> <li>\n<code>logits</code>: An N-D <code>Tensor</code>, <code>N &gt;= 1</code>, representing the log probabilities of a set of Categorical distributions. The first <code>N - 1</code> dimensions index into a batch of independent distributions and the last dimension indexes into the classes.</li> <li>\n<code>dtype</code>: The type of the event samples (default: int32).</li> <li>\n<code>validate_args</code>: Unused in this distribution.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: A name for this distribution (optional).</li> </ul>  <h4 id=\"Categorical.allow_nan_stats\"><code>tf.contrib.distributions.Categorical.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Categorical.batch_shape\"><code>tf.contrib.distributions.Categorical.batch_shape(name='batch_shape')</code></h4>  <h4 id=\"Categorical.cdf\"><code>tf.contrib.distributions.Categorical.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"Categorical.dtype\"><code>tf.contrib.distributions.Categorical.dtype</code></h4>  <h4 id=\"Categorical.entropy\"><code>tf.contrib.distributions.Categorical.entropy(name='sample')</code></h4>  <h4 id=\"Categorical.event_shape\"><code>tf.contrib.distributions.Categorical.event_shape(name='event_shape')</code></h4>  <h4 id=\"Categorical.get_batch_shape\"><code>tf.contrib.distributions.Categorical.get_batch_shape()</code></h4>  <h4 id=\"Categorical.get_event_shape\"><code>tf.contrib.distributions.Categorical.get_event_shape()</code></h4>  <h4 id=\"Categorical.is_continuous\"><code>tf.contrib.distributions.Categorical.is_continuous</code></h4>  <h4 id=\"Categorical.is_reparameterized\"><code>tf.contrib.distributions.Categorical.is_reparameterized</code></h4>  <h4 id=\"Categorical.log_cdf\"><code>tf.contrib.distributions.Categorical.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"Categorical.log_pdf\"><code>tf.contrib.distributions.Categorical.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Categorical.log_pmf\"><code>tf.contrib.distributions.Categorical.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Categorical.log_prob\"><code>tf.contrib.distributions.Categorical.log_prob(k, name='log_prob')</code></h4> <p>Log-probability of class <code>k</code>.</p>  <h5 id=\"args-32\">Args:</h5> <ul> <li>\n<code>k</code>: <code>int32</code> or <code>int64</code> Tensor. Must be broadcastable with a <code>batch_shape</code> <code>Tensor</code>.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-32\">Returns:</h5> <p>The log-probabilities of the classes indexed by <code>k</code></p>  <h4 id=\"Categorical.logits\"><code>tf.contrib.distributions.Categorical.logits</code></h4>  <h4 id=\"Categorical.mean\"><code>tf.contrib.distributions.Categorical.mean(name='mean')</code></h4> <p>Mean of the distribution.</p>  <h4 id=\"Categorical.mode\"><code>tf.contrib.distributions.Categorical.mode(name='mode')</code></h4>  <h4 id=\"Categorical.name\"><code>tf.contrib.distributions.Categorical.name</code></h4>  <h4 id=\"Categorical.num_classes\"><code>tf.contrib.distributions.Categorical.num_classes</code></h4>  <h4 id=\"Categorical.pdf\"><code>tf.contrib.distributions.Categorical.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Categorical.pmf\"><code>tf.contrib.distributions.Categorical.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Categorical.prob\"><code>tf.contrib.distributions.Categorical.prob(k, name='prob')</code></h4> <p>Probability of class <code>k</code>.</p>  <h5 id=\"args-33\">Args:</h5> <ul> <li>\n<code>k</code>: <code>int32</code> or <code>int64</code> Tensor. Must be broadcastable with logits.</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-33\">Returns:</h5> <p>The probabilities of the classes indexed by <code>k</code></p>  <h4 id=\"Categorical.sample\"><code>tf.contrib.distributions.Categorical.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-34\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-34\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Categorical.sample_n\"><code>tf.contrib.distributions.Categorical.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Categorical distribution.</p>  <h5 id=\"args-35\">Args:</h5> <ul> <li>\n<code>n</code>: 0-D. Number of independent samples to draw for each distribution.</li> <li>\n<code>seed</code>: Random seed (optional).</li> <li>\n<code>name</code>: A name for this operation (optional).</li> </ul>  <h5 id=\"returns-35\">Returns:</h5> <p>An <code>int64</code> <code>Tensor</code> with shape <code>[n, batch_shape, event_shape]</code></p>  <h4 id=\"Categorical.std\"><code>tf.contrib.distributions.Categorical.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"Categorical.validate_args\"><code>tf.contrib.distributions.Categorical.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Categorical.variance\"><code>tf.contrib.distributions.Categorical.variance(name='variance')</code></h4> <p>Variance of the distribution.</p>  <h3 id=\"Chi2\"><code>class tf.contrib.distributions.Chi2</code></h3> <p>The Chi2 distribution with degrees of freedom df.</p> <p>The PDF of this distribution is:</p> <p><code>pdf(x) = (x^(df/2 - 1)e^(-x/2))/(2^(df/2)Gamma(df/2)), x &gt; 0</code></p> <p>Note that the Chi2 distribution is a special case of the Gamma distribution, with Chi2(df) = Gamma(df/2, 1/2).</p>  <h4 id=\"Chi2.__init__\"><code>tf.contrib.distributions.Chi2.__init__(df, validate_args=True, allow_nan_stats=False, name='Chi2')</code></h4> <p>Construct Chi2 distributions with parameter <code>df</code>.</p>  <h5 id=\"args-36\">Args:</h5> <ul> <li>\n<code>df</code>: Floating point tensor, the degrees of freedom of the distribution(s). <code>df</code> must contain only positive values.</li> <li>\n<code>validate_args</code>: Whether to assert that <code>df &gt; 0</code>, and that <code>x &gt; 0</code> in the methods <code>prob(x)</code> and <code>log_prob(x)</code>. If <code>validate_args</code> is <code>False</code> and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to prepend to all ops created by this distribution.</li> </ul>  <h4 id=\"Chi2.allow_nan_stats\"><code>tf.contrib.distributions.Chi2.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Chi2.alpha\"><code>tf.contrib.distributions.Chi2.alpha</code></h4> <p>Shape parameter.</p>  <h4 id=\"Chi2.batch_shape\"><code>tf.contrib.distributions.Chi2.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-37\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-36\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Chi2.beta\"><code>tf.contrib.distributions.Chi2.beta</code></h4> <p>Inverse scale parameter.</p>  <h4 id=\"Chi2.cdf\"><code>tf.contrib.distributions.Chi2.cdf(x, name='cdf')</code></h4> <p>CDF of observations <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-38\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-37\">Returns:</h5> <ul> <li>\n<code>cdf</code>: tensor of dtype <code>dtype</code>, the CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Chi2.df\"><code>tf.contrib.distributions.Chi2.df</code></h4>  <h4 id=\"Chi2.dtype\"><code>tf.contrib.distributions.Chi2.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"Chi2.entropy\"><code>tf.contrib.distributions.Chi2.entropy(name='entropy')</code></h4> <p>The entropy of Gamma distribution(s).</p> <p>This is defined to be</p> <pre class=\"\">entropy = alpha - log(beta) + log(Gamma(alpha))\n             + (1-alpha)digamma(alpha)\n</pre> <p>where digamma(alpha) is the digamma function.</p>  <h5 id=\"args-39\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-38\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropy.</li> </ul>  <h4 id=\"Chi2.event_shape\"><code>tf.contrib.distributions.Chi2.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-40\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-39\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Chi2.get_batch_shape\"><code>tf.contrib.distributions.Chi2.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-40\">Returns:</h5> <p><code>TensorShape</code> object.</p>  <h4 id=\"Chi2.get_event_shape\"><code>tf.contrib.distributions.Chi2.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-41\">Returns:</h5> <p><code>TensorShape</code> object.</p>  <h4 id=\"Chi2.is_continuous\"><code>tf.contrib.distributions.Chi2.is_continuous</code></h4>  <h4 id=\"Chi2.is_reparameterized\"><code>tf.contrib.distributions.Chi2.is_reparameterized</code></h4>  <h4 id=\"Chi2.log_cdf\"><code>tf.contrib.distributions.Chi2.log_cdf(x, name='log_cdf')</code></h4> <p>Log CDF of observations <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-41\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-42\">Returns:</h5> <ul> <li>\n<code>log_cdf</code>: tensor of dtype <code>dtype</code>, the log-CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Chi2.log_pdf\"><code>tf.contrib.distributions.Chi2.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Chi2.log_pmf\"><code>tf.contrib.distributions.Chi2.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Chi2.log_prob\"><code>tf.contrib.distributions.Chi2.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations in <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-42\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-43\">Returns:</h5> <ul> <li>\n<code>log_prob</code>: tensor of dtype <code>dtype</code>, the log-PDFs of <code>x</code>.</li> </ul>  <h5 id=\"raises-2\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> and <code>alpha</code> are different dtypes.</li> </ul>  <h4 id=\"Chi2.mean\"><code>tf.contrib.distributions.Chi2.mean(name='mean')</code></h4> <p>Mean of each batch member.</p>  <h4 id=\"Chi2.mode\"><code>tf.contrib.distributions.Chi2.mode(name='mode')</code></h4> <p>Mode of each batch member.</p> <p>The mode of a gamma distribution is <code>(alpha - 1) / beta</code> when <code>alpha &gt; 1</code>, and <code>NaN</code> otherwise. If <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised rather than returning <code>NaN</code>.</p>  <h5 id=\"args-43\">Args:</h5> <ul> <li>\n<code>name</code>: A name to give this op.</li> </ul>  <h5 id=\"returns-44\">Returns:</h5> <p>The mode for every batch member, a <code>Tensor</code> with same <code>dtype</code> as self.</p>  <h4 id=\"Chi2.name\"><code>tf.contrib.distributions.Chi2.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"Chi2.pdf\"><code>tf.contrib.distributions.Chi2.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Chi2.pmf\"><code>tf.contrib.distributions.Chi2.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Chi2.prob\"><code>tf.contrib.distributions.Chi2.prob(x, name='prob')</code></h4> <p>Pdf of observations in <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-44\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-45\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the PDFs of <code>x</code>\n</li> </ul>  <h5 id=\"raises-3\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> and <code>alpha</code> are different dtypes.</li> </ul>  <h4 id=\"Chi2.sample\"><code>tf.contrib.distributions.Chi2.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-45\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-46\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Chi2.sample_n\"><code>tf.contrib.distributions.Chi2.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Draws <code>n</code> samples from the Gamma distribution(s).</p> <p>See the doc for tf.random_gamma for further detail.</p>  <h5 id=\"args-46\">Args:</h5> <ul> <li>\n<code>n</code>: Python integer, the number of observations to sample from each distribution.</li> <li>\n<code>seed</code>: Python integer, the random seed for this operation.</li> <li>\n<code>name</code>: Optional name for the operation.</li> </ul>  <h5 id=\"returns-47\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"Chi2.std\"><code>tf.contrib.distributions.Chi2.std(name='std')</code></h4> <p>Standard deviation of this distribution.</p>  <h4 id=\"Chi2.validate_args\"><code>tf.contrib.distributions.Chi2.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Chi2.variance\"><code>tf.contrib.distributions.Chi2.variance(name='variance')</code></h4> <p>Variance of each batch member.</p>  <h3 id=\"Exponential\"><code>class tf.contrib.distributions.Exponential</code></h3> <p>The Exponential distribution with rate parameter lam.</p> <p>The PDF of this distribution is:</p> <p><code>prob(x) = (lam * e^(-lam * x)), x &gt; 0</code></p> <p>Note that the Exponential distribution is a special case of the Gamma distribution, with Exponential(lam) = Gamma(1, lam).</p>  <h4 id=\"Exponential.__init__\"><code>tf.contrib.distributions.Exponential.__init__(lam, validate_args=True, allow_nan_stats=False, name='Exponential')</code></h4> <p>Construct Exponential distribution with parameter <code>lam</code>.</p>  <h5 id=\"args-47\">Args:</h5> <ul> <li>\n<code>lam</code>: Floating point tensor, the rate of the distribution(s). <code>lam</code> must contain only positive values.</li> <li>\n<code>validate_args</code>: Whether to assert that <code>lam &gt; 0</code>, and that <code>x &gt; 0</code> in the methods <code>prob(x)</code> and <code>log_prob(x)</code>. If <code>validate_args</code> is <code>False</code> and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to prepend to all ops created by this distribution.</li> </ul>  <h4 id=\"Exponential.allow_nan_stats\"><code>tf.contrib.distributions.Exponential.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Exponential.alpha\"><code>tf.contrib.distributions.Exponential.alpha</code></h4> <p>Shape parameter.</p>  <h4 id=\"Exponential.batch_shape\"><code>tf.contrib.distributions.Exponential.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-48\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-48\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Exponential.beta\"><code>tf.contrib.distributions.Exponential.beta</code></h4> <p>Inverse scale parameter.</p>  <h4 id=\"Exponential.cdf\"><code>tf.contrib.distributions.Exponential.cdf(x, name='cdf')</code></h4> <p>CDF of observations <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-49\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-49\">Returns:</h5> <ul> <li>\n<code>cdf</code>: tensor of dtype <code>dtype</code>, the CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Exponential.dtype\"><code>tf.contrib.distributions.Exponential.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"Exponential.entropy\"><code>tf.contrib.distributions.Exponential.entropy(name='entropy')</code></h4> <p>The entropy of Gamma distribution(s).</p> <p>This is defined to be</p> <pre class=\"\">entropy = alpha - log(beta) + log(Gamma(alpha))\n             + (1-alpha)digamma(alpha)\n</pre> <p>where digamma(alpha) is the digamma function.</p>  <h5 id=\"args-50\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-50\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropy.</li> </ul>  <h4 id=\"Exponential.event_shape\"><code>tf.contrib.distributions.Exponential.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-51\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-51\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Exponential.get_batch_shape\"><code>tf.contrib.distributions.Exponential.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-52\">Returns:</h5> <p><code>TensorShape</code> object.</p>  <h4 id=\"Exponential.get_event_shape\"><code>tf.contrib.distributions.Exponential.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-53\">Returns:</h5> <p><code>TensorShape</code> object.</p>  <h4 id=\"Exponential.is_continuous\"><code>tf.contrib.distributions.Exponential.is_continuous</code></h4>  <h4 id=\"Exponential.is_reparameterized\"><code>tf.contrib.distributions.Exponential.is_reparameterized</code></h4>  <h4 id=\"Exponential.lam\"><code>tf.contrib.distributions.Exponential.lam</code></h4>  <h4 id=\"Exponential.log_cdf\"><code>tf.contrib.distributions.Exponential.log_cdf(x, name='log_cdf')</code></h4> <p>Log CDF of observations <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-52\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-54\">Returns:</h5> <ul> <li>\n<code>log_cdf</code>: tensor of dtype <code>dtype</code>, the log-CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Exponential.log_pdf\"><code>tf.contrib.distributions.Exponential.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Exponential.log_pmf\"><code>tf.contrib.distributions.Exponential.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Exponential.log_prob\"><code>tf.contrib.distributions.Exponential.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations in <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-53\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-55\">Returns:</h5> <ul> <li>\n<code>log_prob</code>: tensor of dtype <code>dtype</code>, the log-PDFs of <code>x</code>.</li> </ul>  <h5 id=\"raises-4\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> and <code>alpha</code> are different dtypes.</li> </ul>  <h4 id=\"Exponential.mean\"><code>tf.contrib.distributions.Exponential.mean(name='mean')</code></h4> <p>Mean of each batch member.</p>  <h4 id=\"Exponential.mode\"><code>tf.contrib.distributions.Exponential.mode(name='mode')</code></h4> <p>Mode of each batch member.</p> <p>The mode of a gamma distribution is <code>(alpha - 1) / beta</code> when <code>alpha &gt; 1</code>, and <code>NaN</code> otherwise. If <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised rather than returning <code>NaN</code>.</p>  <h5 id=\"args-54\">Args:</h5> <ul> <li>\n<code>name</code>: A name to give this op.</li> </ul>  <h5 id=\"returns-56\">Returns:</h5> <p>The mode for every batch member, a <code>Tensor</code> with same <code>dtype</code> as self.</p>  <h4 id=\"Exponential.name\"><code>tf.contrib.distributions.Exponential.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"Exponential.pdf\"><code>tf.contrib.distributions.Exponential.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Exponential.pmf\"><code>tf.contrib.distributions.Exponential.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Exponential.prob\"><code>tf.contrib.distributions.Exponential.prob(x, name='prob')</code></h4> <p>Pdf of observations in <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-55\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-57\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the PDFs of <code>x</code>\n</li> </ul>  <h5 id=\"raises-5\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> and <code>alpha</code> are different dtypes.</li> </ul>  <h4 id=\"Exponential.sample\"><code>tf.contrib.distributions.Exponential.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-56\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-58\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Exponential.sample_n\"><code>tf.contrib.distributions.Exponential.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Exponential Distributions.</p>  <h5 id=\"args-57\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-59\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by the hyperparameters.</li> </ul>  <h4 id=\"Exponential.std\"><code>tf.contrib.distributions.Exponential.std(name='std')</code></h4> <p>Standard deviation of this distribution.</p>  <h4 id=\"Exponential.validate_args\"><code>tf.contrib.distributions.Exponential.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Exponential.variance\"><code>tf.contrib.distributions.Exponential.variance(name='variance')</code></h4> <p>Variance of each batch member.</p>  <h3 id=\"Gamma\"><code>class tf.contrib.distributions.Gamma</code></h3> <p>The <code>Gamma</code> distribution with parameter alpha and beta.</p> <p>The parameters are the shape and inverse scale parameters alpha, beta.</p> <p>The PDF of this distribution is:</p> <p><code>pdf(x) = (beta^alpha)(x^(alpha-1))e^(-x*beta)/Gamma(alpha), x &gt; 0</code></p> <p>and the CDF of this distribution is:</p> <p><code>cdf(x) =  GammaInc(alpha, beta * x) / Gamma(alpha), x &gt; 0</code></p> <p>where GammaInc is the incomplete lower Gamma function.</p> <p>Examples:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">dist = Gamma(alpha=3.0, beta=2.0)\ndist2 = Gamma(alpha=[3.0, 4.0], beta=[2.0, 3.0])\n</pre>  <h4 id=\"Gamma.__init__\"><code>tf.contrib.distributions.Gamma.__init__(alpha, beta, validate_args=True, allow_nan_stats=False, name='Gamma')</code></h4> <p>Construct Gamma distributions with parameters <code>alpha</code> and <code>beta</code>.</p> <p>The parameters <code>alpha</code> and <code>beta</code> must be shaped in a way that supports broadcasting (e.g. <code>alpha + beta</code> is a valid operation).</p>  <h5 id=\"args-58\">Args:</h5> <ul> <li>\n<code>alpha</code>: Floating point tensor, the shape params of the distribution(s). alpha must contain only positive values.</li> <li>\n<code>beta</code>: Floating point tensor, the inverse scale params of the distribution(s). beta must contain only positive values.</li> <li>\n<code>validate_args</code>: Whether to assert that <code>a &gt; 0, b &gt; 0</code>, and that <code>x &gt; 0</code> in the methods <code>prob(x)</code> and <code>log_prob(x)</code>. If <code>validate_args</code> is <code>False</code> and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to prepend to all ops created by this distribution.</li> </ul>  <h5 id=\"raises-6\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>alpha</code> and <code>beta</code> are different dtypes.</li> </ul>  <h4 id=\"Gamma.allow_nan_stats\"><code>tf.contrib.distributions.Gamma.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Gamma.alpha\"><code>tf.contrib.distributions.Gamma.alpha</code></h4> <p>Shape parameter.</p>  <h4 id=\"Gamma.batch_shape\"><code>tf.contrib.distributions.Gamma.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-59\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-60\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Gamma.beta\"><code>tf.contrib.distributions.Gamma.beta</code></h4> <p>Inverse scale parameter.</p>  <h4 id=\"Gamma.cdf\"><code>tf.contrib.distributions.Gamma.cdf(x, name='cdf')</code></h4> <p>CDF of observations <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-60\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-61\">Returns:</h5> <ul> <li>\n<code>cdf</code>: tensor of dtype <code>dtype</code>, the CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Gamma.dtype\"><code>tf.contrib.distributions.Gamma.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"Gamma.entropy\"><code>tf.contrib.distributions.Gamma.entropy(name='entropy')</code></h4> <p>The entropy of Gamma distribution(s).</p> <p>This is defined to be</p> <pre class=\"\">entropy = alpha - log(beta) + log(Gamma(alpha))\n             + (1-alpha)digamma(alpha)\n</pre> <p>where digamma(alpha) is the digamma function.</p>  <h5 id=\"args-61\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-62\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropy.</li> </ul>  <h4 id=\"Gamma.event_shape\"><code>tf.contrib.distributions.Gamma.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-62\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-63\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Gamma.get_batch_shape\"><code>tf.contrib.distributions.Gamma.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-64\">Returns:</h5> <p><code>TensorShape</code> object.</p>  <h4 id=\"Gamma.get_event_shape\"><code>tf.contrib.distributions.Gamma.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-65\">Returns:</h5> <p><code>TensorShape</code> object.</p>  <h4 id=\"Gamma.is_continuous\"><code>tf.contrib.distributions.Gamma.is_continuous</code></h4>  <h4 id=\"Gamma.is_reparameterized\"><code>tf.contrib.distributions.Gamma.is_reparameterized</code></h4>  <h4 id=\"Gamma.log_cdf\"><code>tf.contrib.distributions.Gamma.log_cdf(x, name='log_cdf')</code></h4> <p>Log CDF of observations <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-63\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-66\">Returns:</h5> <ul> <li>\n<code>log_cdf</code>: tensor of dtype <code>dtype</code>, the log-CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Gamma.log_pdf\"><code>tf.contrib.distributions.Gamma.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Gamma.log_pmf\"><code>tf.contrib.distributions.Gamma.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Gamma.log_prob\"><code>tf.contrib.distributions.Gamma.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations in <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-64\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-67\">Returns:</h5> <ul> <li>\n<code>log_prob</code>: tensor of dtype <code>dtype</code>, the log-PDFs of <code>x</code>.</li> </ul>  <h5 id=\"raises-7\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> and <code>alpha</code> are different dtypes.</li> </ul>  <h4 id=\"Gamma.mean\"><code>tf.contrib.distributions.Gamma.mean(name='mean')</code></h4> <p>Mean of each batch member.</p>  <h4 id=\"Gamma.mode\"><code>tf.contrib.distributions.Gamma.mode(name='mode')</code></h4> <p>Mode of each batch member.</p> <p>The mode of a gamma distribution is <code>(alpha - 1) / beta</code> when <code>alpha &gt; 1</code>, and <code>NaN</code> otherwise. If <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised rather than returning <code>NaN</code>.</p>  <h5 id=\"args-65\">Args:</h5> <ul> <li>\n<code>name</code>: A name to give this op.</li> </ul>  <h5 id=\"returns-68\">Returns:</h5> <p>The mode for every batch member, a <code>Tensor</code> with same <code>dtype</code> as self.</p>  <h4 id=\"Gamma.name\"><code>tf.contrib.distributions.Gamma.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"Gamma.pdf\"><code>tf.contrib.distributions.Gamma.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Gamma.pmf\"><code>tf.contrib.distributions.Gamma.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Gamma.prob\"><code>tf.contrib.distributions.Gamma.prob(x, name='prob')</code></h4> <p>Pdf of observations in <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-66\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-69\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the PDFs of <code>x</code>\n</li> </ul>  <h5 id=\"raises-8\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> and <code>alpha</code> are different dtypes.</li> </ul>  <h4 id=\"Gamma.sample\"><code>tf.contrib.distributions.Gamma.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-67\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-70\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Gamma.sample_n\"><code>tf.contrib.distributions.Gamma.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Draws <code>n</code> samples from the Gamma distribution(s).</p> <p>See the doc for tf.random_gamma for further detail.</p>  <h5 id=\"args-68\">Args:</h5> <ul> <li>\n<code>n</code>: Python integer, the number of observations to sample from each distribution.</li> <li>\n<code>seed</code>: Python integer, the random seed for this operation.</li> <li>\n<code>name</code>: Optional name for the operation.</li> </ul>  <h5 id=\"returns-71\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"Gamma.std\"><code>tf.contrib.distributions.Gamma.std(name='std')</code></h4> <p>Standard deviation of this distribution.</p>  <h4 id=\"Gamma.validate_args\"><code>tf.contrib.distributions.Gamma.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Gamma.variance\"><code>tf.contrib.distributions.Gamma.variance(name='variance')</code></h4> <p>Variance of each batch member.</p>  <h3 id=\"InverseGamma\"><code>class tf.contrib.distributions.InverseGamma</code></h3> <p>The <code>InverseGamma</code> distribution with parameter alpha and beta.</p> <p>The parameters are the shape and inverse scale parameters alpha, beta.</p> <p>The PDF of this distribution is:</p> <p><code>pdf(x) = (beta^alpha)/Gamma(alpha)(x^(-alpha-1))e^(-beta/x), x &gt; 0</code></p> <p>and the CDF of this distribution is:</p> <p><code>cdf(x) =  GammaInc(alpha, beta / x) / Gamma(alpha), x &gt; 0</code></p> <p>where GammaInc is the upper incomplete Gamma function.</p> <p>Examples:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">dist = InverseGamma(alpha=3.0, beta=2.0)\ndist2 = InverseGamma(alpha=[3.0, 4.0], beta=[2.0, 3.0])\n</pre>  <h4 id=\"InverseGamma.__init__\"><code>tf.contrib.distributions.InverseGamma.__init__(alpha, beta, validate_args=True, allow_nan_stats=False, name='InverseGamma')</code></h4> <p>Construct InverseGamma distributions with parameters <code>alpha</code> and <code>beta</code>.</p> <p>The parameters <code>alpha</code> and <code>beta</code> must be shaped in a way that supports broadcasting (e.g. <code>alpha + beta</code> is a valid operation).</p>  <h5 id=\"args-69\">Args:</h5> <ul> <li>\n<code>alpha</code>: Floating point tensor, the shape params of the distribution(s). alpha must contain only positive values.</li> <li>\n<code>beta</code>: Floating point tensor, the scale params of the distribution(s). beta must contain only positive values.</li> <li>\n<code>validate_args</code>: Whether to assert that <code>a &gt; 0, b &gt; 0</code>, and that <code>x &gt; 0</code> in the methods <code>prob(x)</code> and <code>log_prob(x)</code>. If <code>validate_args</code> is <code>False</code> and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to prepend to all ops created by this distribution.</li> </ul>  <h5 id=\"raises-9\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>alpha</code> and <code>beta</code> are different dtypes.</li> </ul>  <h4 id=\"InverseGamma.allow_nan_stats\"><code>tf.contrib.distributions.InverseGamma.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"InverseGamma.alpha\"><code>tf.contrib.distributions.InverseGamma.alpha</code></h4> <p>Shape parameter.</p>  <h4 id=\"InverseGamma.batch_shape\"><code>tf.contrib.distributions.InverseGamma.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-70\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-72\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"InverseGamma.beta\"><code>tf.contrib.distributions.InverseGamma.beta</code></h4> <p>Scale parameter.</p>  <h4 id=\"InverseGamma.cdf\"><code>tf.contrib.distributions.InverseGamma.cdf(x, name='cdf')</code></h4> <p>CDF of observations <code>x</code> under these InverseGamma distribution(s).</p>  <h5 id=\"args-71\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-73\">Returns:</h5> <ul> <li>\n<code>cdf</code>: tensor of dtype <code>dtype</code>, the CDFs of <code>x</code>.</li> </ul>  <h4 id=\"InverseGamma.dtype\"><code>tf.contrib.distributions.InverseGamma.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"InverseGamma.entropy\"><code>tf.contrib.distributions.InverseGamma.entropy(name='entropy')</code></h4> <p>The entropy of these InverseGamma distribution(s).</p> <p>This is defined to be</p> <pre class=\"\">entropy = alpha - log(beta) + log(Gamma(alpha))\n             + (1-alpha)digamma(alpha)\n</pre> <p>where digamma(alpha) is the digamma function.</p>  <h5 id=\"args-72\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-74\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropy.</li> </ul>  <h4 id=\"InverseGamma.event_shape\"><code>tf.contrib.distributions.InverseGamma.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-73\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-75\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"InverseGamma.get_batch_shape\"><code>tf.contrib.distributions.InverseGamma.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-76\">Returns:</h5> <p><code>TensorShape</code> object.</p>  <h4 id=\"InverseGamma.get_event_shape\"><code>tf.contrib.distributions.InverseGamma.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-77\">Returns:</h5> <p><code>TensorShape</code> object.</p>  <h4 id=\"InverseGamma.is_continuous\"><code>tf.contrib.distributions.InverseGamma.is_continuous</code></h4>  <h4 id=\"InverseGamma.is_reparameterized\"><code>tf.contrib.distributions.InverseGamma.is_reparameterized</code></h4>  <h4 id=\"InverseGamma.log_cdf\"><code>tf.contrib.distributions.InverseGamma.log_cdf(x, name='log_cdf')</code></h4> <p>Log CDF of observations <code>x</code> under these InverseGamma distribution(s).</p>  <h5 id=\"args-74\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-78\">Returns:</h5> <ul> <li>\n<code>log_cdf</code>: tensor of dtype <code>dtype</code>, the log-CDFs of <code>x</code>.</li> </ul>  <h4 id=\"InverseGamma.log_pdf\"><code>tf.contrib.distributions.InverseGamma.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"InverseGamma.log_pmf\"><code>tf.contrib.distributions.InverseGamma.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"InverseGamma.log_prob\"><code>tf.contrib.distributions.InverseGamma.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations in <code>x</code> under these InverseGamma distribution(s).</p>  <h5 id=\"args-75\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-79\">Returns:</h5> <ul> <li>\n<code>log_prob</code>: tensor of dtype <code>dtype</code>, the log-PDFs of <code>x</code>.</li> </ul>  <h5 id=\"raises-10\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> and <code>alpha</code> are different dtypes.</li> </ul>  <h4 id=\"InverseGamma.mean\"><code>tf.contrib.distributions.InverseGamma.mean(name='mean')</code></h4> <p>Mean of each batch member.</p> <p>The mean of an inverse gamma distribution is <code>beta / (alpha - 1)</code>, when <code>alpha &gt; 1</code>, and <code>NaN</code> otherwise. If <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised rather than returning <code>NaN</code></p>  <h5 id=\"args-76\">Args:</h5> <ul> <li>\n<code>name</code>: A name to give this op.</li> </ul>  <h5 id=\"returns-80\">Returns:</h5> <p>The mean for every batch member, a <code>Tensor</code> with same <code>dtype</code> as self.</p>  <h4 id=\"InverseGamma.mode\"><code>tf.contrib.distributions.InverseGamma.mode(name='mode')</code></h4> <p>Mode of each batch member.</p> <p>The mode of an inverse gamma distribution is <code>beta / (alpha + 1)</code>.</p>  <h5 id=\"args-77\">Args:</h5> <ul> <li>\n<code>name</code>: A name to give this op.</li> </ul>  <h5 id=\"returns-81\">Returns:</h5> <p>The mode for every batch member, a <code>Tensor</code> with same <code>dtype</code> as self.</p>  <h4 id=\"InverseGamma.name\"><code>tf.contrib.distributions.InverseGamma.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"InverseGamma.pdf\"><code>tf.contrib.distributions.InverseGamma.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"InverseGamma.pmf\"><code>tf.contrib.distributions.InverseGamma.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"InverseGamma.prob\"><code>tf.contrib.distributions.InverseGamma.prob(x, name='prob')</code></h4> <p>Pdf of observations in <code>x</code> under these Gamma distribution(s).</p>  <h5 id=\"args-78\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>alpha</code> and <code>beta</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-82\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the PDFs of <code>x</code>\n</li> </ul>  <h5 id=\"raises-11\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>x</code> and <code>alpha</code> are different dtypes.</li> </ul>  <h4 id=\"InverseGamma.sample\"><code>tf.contrib.distributions.InverseGamma.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-79\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-83\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"InverseGamma.sample_n\"><code>tf.contrib.distributions.InverseGamma.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Draws <code>n</code> samples from these InverseGamma distribution(s).</p> <p>See the doc for tf.random_gamma for further details on sampling strategy.</p>  <h5 id=\"args-80\">Args:</h5> <ul> <li>\n<code>n</code>: Python integer, the number of observations to sample from each distribution.</li> <li>\n<code>seed</code>: Python integer, the random seed for this operation.</li> <li>\n<code>name</code>: Optional name for the operation.</li> </ul>  <h5 id=\"returns-84\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"InverseGamma.std\"><code>tf.contrib.distributions.InverseGamma.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"InverseGamma.validate_args\"><code>tf.contrib.distributions.InverseGamma.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"InverseGamma.variance\"><code>tf.contrib.distributions.InverseGamma.variance(name='variance')</code></h4> <p>Variance of each batch member.</p> <p>Variance for inverse gamma is defined only for <code>alpha &gt; 2</code>. If <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised rather than returning <code>NaN</code>.</p>  <h5 id=\"args-81\">Args:</h5> <ul> <li>\n<code>name</code>: A name to give this op.</li> </ul>  <h5 id=\"returns-85\">Returns:</h5> <p>The variance for every batch member, a <code>Tensor</code> with same <code>dtype</code> as self.</p>  <h3 id=\"Laplace\"><code>class tf.contrib.distributions.Laplace</code></h3> <p>The Laplace distribution with location and scale &gt; 0 parameters.</p>  <h4 id=\"mathematical-details-3\">Mathematical details</h4> <p>The PDF of this distribution is:</p> <p><code>f(x | mu, b, b &gt; 0) = 0.5 / b exp(-|x - mu| / b)</code></p> <p>Note that the Laplace distribution can be thought of two exponential distributions spliced together \"back-to-back.\"</p>  <h4 id=\"Laplace.__init__\"><code>tf.contrib.distributions.Laplace.__init__(loc, scale, validate_args=True, allow_nan_stats=False, name='Laplace')</code></h4> <p>Construct Laplace distribution with parameters <code>loc</code> and <code>scale</code>.</p> <p>The parameters <code>loc</code> and <code>scale</code> must be shaped in a way that supports broadcasting (e.g., <code>loc / scale</code> is a valid operation).</p>  <h5 id=\"args-82\">Args:</h5> <ul> <li>\n<code>loc</code>: Floating point tensor which characterizes the location (center) of the distribution.</li> <li>\n<code>scale</code>: Positive floating point tensor which characterizes the spread of the distribution.</li> <li>\n<code>validate_args</code>: Whether to validate input with asserts. If <code>validate_args</code> is <code>False</code>, and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to give Ops created by the initializer.</li> </ul>  <h5 id=\"raises-12\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>loc</code> and <code>scale</code> are of different dtype.</li> </ul>  <h4 id=\"Laplace.allow_nan_stats\"><code>tf.contrib.distributions.Laplace.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Laplace.batch_shape\"><code>tf.contrib.distributions.Laplace.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-83\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-86\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Laplace.cdf\"><code>tf.contrib.distributions.Laplace.cdf(x, name='cdf')</code></h4> <p>CDF of observations in <code>x</code> under the Laplace distribution(s).</p>  <h5 id=\"args-84\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>loc</code> and <code>scale</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-87\">Returns:</h5> <ul> <li>\n<code>cdf</code>: tensor of dtype <code>dtype</code>, the CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Laplace.dtype\"><code>tf.contrib.distributions.Laplace.dtype</code></h4>  <h4 id=\"Laplace.entropy\"><code>tf.contrib.distributions.Laplace.entropy(name='entropy')</code></h4> <p>The entropy of Laplace distribution(s).</p>  <h5 id=\"args-85\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-88\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropy.</li> </ul>  <h4 id=\"Laplace.event_shape\"><code>tf.contrib.distributions.Laplace.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-86\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-89\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Laplace.get_batch_shape\"><code>tf.contrib.distributions.Laplace.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-90\">Returns:</h5> <p>batch shape</p>  <h4 id=\"Laplace.get_event_shape\"><code>tf.contrib.distributions.Laplace.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-91\">Returns:</h5> <p>event shape</p>  <h4 id=\"Laplace.is_continuous\"><code>tf.contrib.distributions.Laplace.is_continuous</code></h4>  <h4 id=\"Laplace.is_reparameterized\"><code>tf.contrib.distributions.Laplace.is_reparameterized</code></h4>  <h4 id=\"Laplace.loc\"><code>tf.contrib.distributions.Laplace.loc</code></h4> <p>Distribution parameter for the location.</p>  <h4 id=\"Laplace.log_cdf\"><code>tf.contrib.distributions.Laplace.log_cdf(x, name='log_cdf')</code></h4> <p>Log CDF of observations <code>x</code> under the Laplace distribution(s).</p>  <h5 id=\"args-87\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>loc</code> and <code>scale</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-92\">Returns:</h5> <ul> <li>\n<code>log_cdf</code>: tensor of dtype <code>dtype</code>, the log-CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Laplace.log_pdf\"><code>tf.contrib.distributions.Laplace.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Laplace.log_pmf\"><code>tf.contrib.distributions.Laplace.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Laplace.log_prob\"><code>tf.contrib.distributions.Laplace.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations in <code>x</code> under these Laplace distribution(s).</p>  <h5 id=\"args-88\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>loc</code> and <code>scale</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-93\">Returns:</h5> <ul> <li>\n<code>log_prob</code>: tensor of dtype <code>dtype</code>, the log-probability of <code>x</code>.</li> </ul>  <h4 id=\"Laplace.mean\"><code>tf.contrib.distributions.Laplace.mean(name='mean')</code></h4> <p>Mean of this distribution.</p>  <h4 id=\"Laplace.median\"><code>tf.contrib.distributions.Laplace.median(name='median')</code></h4> <p>Median of this distribution.</p>  <h4 id=\"Laplace.mode\"><code>tf.contrib.distributions.Laplace.mode(name='mode')</code></h4> <p>Mode of this distribution.</p>  <h4 id=\"Laplace.name\"><code>tf.contrib.distributions.Laplace.name</code></h4>  <h4 id=\"Laplace.pdf\"><code>tf.contrib.distributions.Laplace.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Laplace.pmf\"><code>tf.contrib.distributions.Laplace.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Laplace.prob\"><code>tf.contrib.distributions.Laplace.prob(x, name='pdf')</code></h4> <p>The prob of observations in <code>x</code> under the Laplace distribution(s).</p>  <h5 id=\"args-89\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>loc</code> and <code>scale</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-94\">Returns:</h5> <ul> <li>\n<code>pdf</code>: tensor of dtype <code>dtype</code>, the pdf values of <code>x</code>.</li> </ul>  <h4 id=\"Laplace.sample\"><code>tf.contrib.distributions.Laplace.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-90\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-95\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Laplace.sample_n\"><code>tf.contrib.distributions.Laplace.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Laplace Distributions.</p>  <h5 id=\"args-91\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-96\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by broadcasting the parameters.</li> </ul>  <h4 id=\"Laplace.scale\"><code>tf.contrib.distributions.Laplace.scale</code></h4> <p>Distribution parameter for scale.</p>  <h4 id=\"Laplace.std\"><code>tf.contrib.distributions.Laplace.std(name='std')</code></h4> <p>Standard deviation of this distribution.</p>  <h4 id=\"Laplace.validate_args\"><code>tf.contrib.distributions.Laplace.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Laplace.variance\"><code>tf.contrib.distributions.Laplace.variance(name='variance')</code></h4> <p>Variance of this distribution.</p>  <h3 id=\"Normal\"><code>class tf.contrib.distributions.Normal</code></h3> <p>The scalar Normal distribution with mean and stddev parameters mu, sigma.</p>  <h4 id=\"mathematical-details-4\">Mathematical details</h4> <p>The PDF of this distribution is:</p> <p><code>f(x) = sqrt(1/(2*pi*sigma^2)) exp(-(x-mu)^2/(2*sigma^2))</code></p>  <h4 id=\"examples-3\">Examples</h4> <p>Examples of initialization of one or a batch of distributions.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define a single scalar Normal distribution.\ndist = tf.contrib.distributions.Normal(mu=0, sigma=3)\n\n# Evaluate the cdf at 1, returning a scalar.\ndist.cdf(1)\n\n# Define a batch of two scalar valued Normals.\n# The first has mean 1 and standard deviation 11, the second 2 and 22.\ndist = tf.contrib.distributions.Normal(mu=[1, 2.], sigma=[11, 22.])\n\n# Evaluate the pdf of the first distribution on 0, and the second on 1.5,\n# returning a length two tensor.\ndist.pdf([0, 1.5])\n\n# Get 3 samples, returning a 3 x 2 tensor.\ndist.sample(3)\n</pre> <p>Arguments are broadcast when possible.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define a batch of two scalar valued Normals.\n# Both have mean 1, but different standard deviations.\ndist = tf.contrib.distributions.Normal(mu=1, sigma=[11, 22.])\n\n# Evaluate the pdf of both distributions on the same point, 3.0,\n# returning a length 2 tensor.\ndist.pdf(3.0)\n</pre>  <h4 id=\"Normal.__init__\"><code>tf.contrib.distributions.Normal.__init__(mu, sigma, validate_args=True, allow_nan_stats=False, name='Normal')</code></h4> <p>Construct Normal distributions with mean and stddev <code>mu</code> and <code>sigma</code>.</p> <p>The parameters <code>mu</code> and <code>sigma</code> must be shaped in a way that supports broadcasting (e.g. <code>mu + sigma</code> is a valid operation).</p>  <h5 id=\"args-92\">Args:</h5> <ul> <li>\n<code>mu</code>: Floating point tensor, the means of the distribution(s).</li> <li>\n<code>sigma</code>: Floating point tensor, the stddevs of the distribution(s). sigma must contain only positive values.</li> <li>\n<code>validate_args</code>: Whether to assert that <code>sigma &gt; 0</code>. If <code>validate_args</code> is <code>False</code>, correct output is not guaranteed when input is invalid.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to give Ops created by the initializer.</li> </ul>  <h5 id=\"raises-13\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if mu and sigma are different dtypes.</li> </ul>  <h4 id=\"Normal.allow_nan_stats\"><code>tf.contrib.distributions.Normal.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Normal.batch_shape\"><code>tf.contrib.distributions.Normal.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-93\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-97\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Normal.cdf\"><code>tf.contrib.distributions.Normal.cdf(x, name='cdf')</code></h4> <p>CDF of observations in <code>x</code> under these Normal distribution(s).</p>  <h5 id=\"args-94\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>mu</code> and <code>sigma</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-98\">Returns:</h5> <ul> <li>\n<code>cdf</code>: tensor of dtype <code>dtype</code>, the CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Normal.dtype\"><code>tf.contrib.distributions.Normal.dtype</code></h4>  <h4 id=\"Normal.entropy\"><code>tf.contrib.distributions.Normal.entropy(name='entropy')</code></h4> <p>The entropy of Normal distribution(s).</p>  <h5 id=\"args-95\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-99\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropy.</li> </ul>  <h4 id=\"Normal.event_shape\"><code>tf.contrib.distributions.Normal.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-96\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-100\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Normal.get_batch_shape\"><code>tf.contrib.distributions.Normal.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-101\">Returns:</h5> <p>batch shape</p>  <h4 id=\"Normal.get_event_shape\"><code>tf.contrib.distributions.Normal.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-102\">Returns:</h5> <p>event shape</p>  <h4 id=\"Normal.is_continuous\"><code>tf.contrib.distributions.Normal.is_continuous</code></h4>  <h4 id=\"Normal.is_reparameterized\"><code>tf.contrib.distributions.Normal.is_reparameterized</code></h4>  <h4 id=\"Normal.log_cdf\"><code>tf.contrib.distributions.Normal.log_cdf(x, name='log_cdf')</code></h4> <p>Log CDF of observations <code>x</code> under these Normal distribution(s).</p>  <h5 id=\"args-97\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>mu</code> and <code>sigma</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-103\">Returns:</h5> <ul> <li>\n<code>log_cdf</code>: tensor of dtype <code>dtype</code>, the log-CDFs of <code>x</code>.</li> </ul>  <h4 id=\"Normal.log_pdf\"><code>tf.contrib.distributions.Normal.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Normal.log_pmf\"><code>tf.contrib.distributions.Normal.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Normal.log_prob\"><code>tf.contrib.distributions.Normal.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations in <code>x</code> under these Normal distribution(s).</p>  <h5 id=\"args-98\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>mu</code> and <code>sigma</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-104\">Returns:</h5> <ul> <li>\n<code>log_prob</code>: tensor of dtype <code>dtype</code>, the log-PDFs of <code>x</code>.</li> </ul>  <h4 id=\"Normal.mean\"><code>tf.contrib.distributions.Normal.mean(name='mean')</code></h4> <p>Mean of this distribution.</p>  <h4 id=\"Normal.mode\"><code>tf.contrib.distributions.Normal.mode(name='mode')</code></h4> <p>Mode of this distribution.</p>  <h4 id=\"Normal.mu\"><code>tf.contrib.distributions.Normal.mu</code></h4> <p>Distribution parameter for the mean.</p>  <h4 id=\"Normal.name\"><code>tf.contrib.distributions.Normal.name</code></h4>  <h4 id=\"Normal.pdf\"><code>tf.contrib.distributions.Normal.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Normal.pmf\"><code>tf.contrib.distributions.Normal.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Normal.prob\"><code>tf.contrib.distributions.Normal.prob(x, name='prob')</code></h4> <p>The PDF of observations in <code>x</code> under these Normal distribution(s).</p>  <h5 id=\"args-99\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>mu</code> and <code>sigma</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-105\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the prob values of <code>x</code>.</li> </ul>  <h4 id=\"Normal.sample\"><code>tf.contrib.distributions.Normal.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-100\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-106\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Normal.sample_n\"><code>tf.contrib.distributions.Normal.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Normal Distributions.</p>  <h5 id=\"args-101\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-107\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by broadcasting the hyperparameters.</li> </ul>  <h4 id=\"Normal.sigma\"><code>tf.contrib.distributions.Normal.sigma</code></h4> <p>Distribution parameter for standard deviation.</p>  <h4 id=\"Normal.std\"><code>tf.contrib.distributions.Normal.std(name='std')</code></h4> <p>Standard deviation of this distribution.</p>  <h4 id=\"Normal.validate_args\"><code>tf.contrib.distributions.Normal.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Normal.variance\"><code>tf.contrib.distributions.Normal.variance(name='variance')</code></h4> <p>Variance of this distribution.</p>  <h3 id=\"StudentT\"><code>class tf.contrib.distributions.StudentT</code></h3> <p>Student's t distribution with degree-of-freedom parameter df.</p>  <h4 id=\"mathematical-details-5\">Mathematical details</h4> <p>The PDF of this distribution is:</p> <p><code>f(t) = gamma((df+1)/2)/sqrt(df*pi)/gamma(df/2)*(1+t^2/df)^(-(df+1)/2)</code></p>  <h4 id=\"examples-4\">Examples</h4> <p>Examples of initialization of one or a batch of distributions.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define a single scalar Student t distribution.\nsingle_dist = tf.contrib.distributions.StudentT(df=3)\n\n# Evaluate the pdf at 1, returning a scalar Tensor.\nsingle_dist.pdf(1.)\n\n# Define a batch of two scalar valued Student t's.\n# The first has degrees of freedom 2, mean 1, and scale 11.\n# The second 3, 2 and 22.\nmulti_dist = tf.contrib.distributions.StudentT(df=[2, 3],\n                                               mu=[1, 2.],\n                                               sigma=[11, 22.])\n\n# Evaluate the pdf of the first distribution on 0, and the second on 1.5,\n# returning a length two tensor.\nmulti_dist.pdf([0, 1.5])\n\n# Get 3 samples, returning a 3 x 2 tensor.\nmulti_dist.sample(3)\n</pre> <p>Arguments are broadcast when possible.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define a batch of two Student's t distributions.\n# Both have df 2 and mean 1, but different scales.\ndist = tf.contrib.distributions.StudentT(df=2, mu=1, sigma=[11, 22.])\n\n# Evaluate the pdf of both distributions on the same point, 3.0,\n# returning a length 2 tensor.\ndist.pdf(3.0)\n</pre>  <h4 id=\"StudentT.__init__\"><code>tf.contrib.distributions.StudentT.__init__(df, mu, sigma, validate_args=True, allow_nan_stats=False, name='StudentT')</code></h4> <p>Construct Student's t distributions.</p> <p>The distributions have degree of freedom <code>df</code>, mean <code>mu</code>, and scale <code>sigma</code>.</p> <p>The parameters <code>df</code>, <code>mu</code>, and <code>sigma</code> must be shaped in a way that supports broadcasting (e.g. <code>df + mu + sigma</code> is a valid operation).</p>  <h5 id=\"args-102\">Args:</h5> <ul> <li>\n<code>df</code>: Floating point tensor, the degrees of freedom of the distribution(s). <code>df</code> must contain only positive values.</li> <li>\n<code>mu</code>: Floating point tensor, the means of the distribution(s).</li> <li>\n<code>sigma</code>: Floating point tensor, the scaling factor for the distribution(s). <code>sigma</code> must contain only positive values. Note that <code>sigma</code> is not the standard deviation of this distribution.</li> <li>\n<code>validate_args</code>: Whether to assert that <code>df &gt; 0, sigma &gt; 0</code>. If <code>validate_args</code> is <code>False</code> and inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to give Ops created by the initializer.</li> </ul>  <h5 id=\"raises-14\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if mu and sigma are different dtypes.</li> </ul>  <h4 id=\"StudentT.allow_nan_stats\"><code>tf.contrib.distributions.StudentT.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"StudentT.batch_shape\"><code>tf.contrib.distributions.StudentT.batch_shape(name='batch_shape')</code></h4>  <h4 id=\"StudentT.cdf\"><code>tf.contrib.distributions.StudentT.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"StudentT.df\"><code>tf.contrib.distributions.StudentT.df</code></h4> <p>Degrees of freedom in these Student's t distribution(s).</p>  <h4 id=\"StudentT.dtype\"><code>tf.contrib.distributions.StudentT.dtype</code></h4>  <h4 id=\"StudentT.entropy\"><code>tf.contrib.distributions.StudentT.entropy(name='entropy')</code></h4> <p>The entropy of Student t distribution(s).</p>  <h5 id=\"args-103\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-108\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropy.</li> </ul>  <h4 id=\"StudentT.event_shape\"><code>tf.contrib.distributions.StudentT.event_shape(name='event_shape')</code></h4>  <h4 id=\"StudentT.get_batch_shape\"><code>tf.contrib.distributions.StudentT.get_batch_shape()</code></h4>  <h4 id=\"StudentT.get_event_shape\"><code>tf.contrib.distributions.StudentT.get_event_shape()</code></h4>  <h4 id=\"StudentT.is_continuous\"><code>tf.contrib.distributions.StudentT.is_continuous</code></h4>  <h4 id=\"StudentT.is_reparameterized\"><code>tf.contrib.distributions.StudentT.is_reparameterized</code></h4>  <h4 id=\"StudentT.log_cdf\"><code>tf.contrib.distributions.StudentT.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"StudentT.log_pdf\"><code>tf.contrib.distributions.StudentT.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"StudentT.log_pmf\"><code>tf.contrib.distributions.StudentT.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"StudentT.log_prob\"><code>tf.contrib.distributions.StudentT.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations in <code>x</code> under these Student's t-distribution(s).</p>  <h5 id=\"args-104\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>mu</code> and <code>df</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-109\">Returns:</h5> <ul> <li>\n<code>log_prob</code>: tensor of dtype <code>dtype</code>, the log-PDFs of <code>x</code>.</li> </ul>  <h4 id=\"StudentT.mean\"><code>tf.contrib.distributions.StudentT.mean(name='mean')</code></h4> <p>Mean of the distribution.</p> <p>The mean of Student's T equals <code>mu</code> if <code>df &gt; 1</code>, otherwise it is <code>NaN</code>. If <code>self.allow_nan_stats=False</code>, then an exception will be raised rather than returning <code>NaN</code>.</p>  <h5 id=\"args-105\">Args:</h5> <ul> <li>\n<code>name</code>: A name to give this op.</li> </ul>  <h5 id=\"returns-110\">Returns:</h5> <p>The mean for every batch member, a <code>Tensor</code> with same <code>dtype</code> as self.</p>  <h4 id=\"StudentT.mode\"><code>tf.contrib.distributions.StudentT.mode(name='mode')</code></h4>  <h4 id=\"StudentT.mu\"><code>tf.contrib.distributions.StudentT.mu</code></h4> <p>Locations of these Student's t distribution(s).</p>  <h4 id=\"StudentT.name\"><code>tf.contrib.distributions.StudentT.name</code></h4>  <h4 id=\"StudentT.pdf\"><code>tf.contrib.distributions.StudentT.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"StudentT.pmf\"><code>tf.contrib.distributions.StudentT.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"StudentT.prob\"><code>tf.contrib.distributions.StudentT.prob(x, name='prob')</code></h4> <p>The PDF of observations in <code>x</code> under these Student's t distribution(s).</p>  <h5 id=\"args-106\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>df</code>, <code>mu</code>, and <code>sigma</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-111\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the prob values of <code>x</code>.</li> </ul>  <h4 id=\"StudentT.sample\"><code>tf.contrib.distributions.StudentT.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-107\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-112\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"StudentT.sample_n\"><code>tf.contrib.distributions.StudentT.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Student t Distributions.</p>  <h5 id=\"args-108\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-113\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"StudentT.sigma\"><code>tf.contrib.distributions.StudentT.sigma</code></h4> <p>Scaling factors of these Student's t distribution(s).</p>  <h4 id=\"StudentT.std\"><code>tf.contrib.distributions.StudentT.std(name='std')</code></h4>  <h4 id=\"StudentT.validate_args\"><code>tf.contrib.distributions.StudentT.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"StudentT.variance\"><code>tf.contrib.distributions.StudentT.variance(name='variance')</code></h4> <p>Variance of the distribution.</p> <p>Variance for Student's T equals</p> <pre class=\"\">df / (df - 2), when df &gt; 2\ninfinity, when 1 &lt; df &lt;= 2\nNaN, when df &lt;= 1\n</pre> <p>The NaN state occurs because mean is undefined for <code>df &lt;= 1</code>, and if <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised if any batch members fall into this state.</p>  <h5 id=\"args-109\">Args:</h5> <ul> <li>\n<code>name</code>: A name for this op.</li> </ul>  <h5 id=\"returns-114\">Returns:</h5> <p>The variance for every batch member, a <code>Tensor</code> with same <code>dtype</code> as self.</p>  <h3 id=\"Uniform\"><code>class tf.contrib.distributions.Uniform</code></h3> <p>Uniform distribution with <code>a</code> and <code>b</code> parameters.</p> <p>The PDF of this distribution is constant between [<code>a</code>, <code>b</code>], and 0 elsewhere.</p>  <h4 id=\"Uniform.__init__\"><code>tf.contrib.distributions.Uniform.__init__(a=0.0, b=1.0, validate_args=True, allow_nan_stats=False, name='Uniform')</code></h4> <p>Construct Uniform distributions with <code>a</code> and <code>b</code>.</p> <p>The parameters <code>a</code> and <code>b</code> must be shaped in a way that supports broadcasting (e.g. <code>b - a</code> is a valid operation).</p> <p>Here are examples without broadcasting:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Without broadcasting\nu1 = Uniform(3.0, 4.0)  # a single uniform distribution [3, 4]\nu2 = Uniform([1.0, 2.0], [3.0, 4.0])  # 2 distributions [1, 3], [2, 4]\nu3 = Uniform([[1.0, 2.0],\n              [3.0, 4.0]],\n             [[1.5, 2.5],\n              [3.5, 4.5]])  # 4 distributions\n</pre> <p>And with broadcasting:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">u1 = Uniform(3.0, [5.0, 6.0, 7.0])  # 3 distributions\n</pre>  <h5 id=\"args-110\">Args:</h5> <ul> <li>\n<code>a</code>: Floating point tensor, the minimum endpoint.</li> <li>\n<code>b</code>: Floating point tensor, the maximum endpoint. Must be &gt; <code>a</code>.</li> <li>\n<code>validate_args</code>: Whether to assert that <code>a &gt; b</code>. If <code>validate_args</code> is <code>False</code> and inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to prefix Ops created by this distribution class.</li> </ul>  <h5 id=\"raises-15\">Raises:</h5> <ul> <li>\n<code>InvalidArgumentError</code>: if <code>a &gt;= b</code> and <code>validate_args=True</code>.</li> </ul>  <h4 id=\"Uniform.a\"><code>tf.contrib.distributions.Uniform.a</code></h4>  <h4 id=\"Uniform.allow_nan_stats\"><code>tf.contrib.distributions.Uniform.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Uniform.b\"><code>tf.contrib.distributions.Uniform.b</code></h4>  <h4 id=\"Uniform.batch_shape\"><code>tf.contrib.distributions.Uniform.batch_shape(name='batch_shape')</code></h4>  <h4 id=\"Uniform.cdf\"><code>tf.contrib.distributions.Uniform.cdf(x, name='cdf')</code></h4> <p>CDF of observations in <code>x</code> under these Uniform distribution(s).</p>  <h5 id=\"args-111\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>a</code> and <code>b</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-115\">Returns:</h5> <ul> <li>\n<code>cdf</code>: tensor of dtype <code>dtype</code>, the CDFs of <code>x</code>. If <code>x</code> is <code>nan</code>, will return <code>nan</code>.</li> </ul>  <h4 id=\"Uniform.dtype\"><code>tf.contrib.distributions.Uniform.dtype</code></h4>  <h4 id=\"Uniform.entropy\"><code>tf.contrib.distributions.Uniform.entropy(name='entropy')</code></h4> <p>The entropy of Uniform distribution(s).</p>  <h5 id=\"args-112\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-116\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropy.</li> </ul>  <h4 id=\"Uniform.event_shape\"><code>tf.contrib.distributions.Uniform.event_shape(name='event_shape')</code></h4>  <h4 id=\"Uniform.get_batch_shape\"><code>tf.contrib.distributions.Uniform.get_batch_shape()</code></h4>  <h4 id=\"Uniform.get_event_shape\"><code>tf.contrib.distributions.Uniform.get_event_shape()</code></h4>  <h4 id=\"Uniform.is_continuous\"><code>tf.contrib.distributions.Uniform.is_continuous</code></h4>  <h4 id=\"Uniform.is_reparameterized\"><code>tf.contrib.distributions.Uniform.is_reparameterized</code></h4>  <h4 id=\"Uniform.log_cdf\"><code>tf.contrib.distributions.Uniform.log_cdf(x, name='log_cdf')</code></h4>  <h4 id=\"Uniform.log_pdf\"><code>tf.contrib.distributions.Uniform.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Uniform.log_pmf\"><code>tf.contrib.distributions.Uniform.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Uniform.log_prob\"><code>tf.contrib.distributions.Uniform.log_prob(x, name='log_prob')</code></h4>  <h4 id=\"Uniform.mean\"><code>tf.contrib.distributions.Uniform.mean(name='mean')</code></h4>  <h4 id=\"Uniform.mode\"><code>tf.contrib.distributions.Uniform.mode(name='mode')</code></h4> <p>Mode of the distribution.</p>  <h4 id=\"Uniform.name\"><code>tf.contrib.distributions.Uniform.name</code></h4>  <h4 id=\"Uniform.pdf\"><code>tf.contrib.distributions.Uniform.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Uniform.pmf\"><code>tf.contrib.distributions.Uniform.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Uniform.prob\"><code>tf.contrib.distributions.Uniform.prob(x, name='prob')</code></h4> <p>The PDF of observations in <code>x</code> under these Uniform distribution(s).</p>  <h5 id=\"args-113\">Args:</h5> <ul> <li>\n<code>x</code>: tensor of dtype <code>dtype</code>, must be broadcastable with <code>a</code> and <code>b</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-117\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the prob values of <code>x</code>. If <code>x</code> is <code>nan</code>, will return <code>nan</code>.</li> </ul>  <h4 id=\"Uniform.range\"><code>tf.contrib.distributions.Uniform.range(name='range')</code></h4> <p><code>b - a</code>.</p>  <h4 id=\"Uniform.sample\"><code>tf.contrib.distributions.Uniform.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-114\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-118\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Uniform.sample_n\"><code>tf.contrib.distributions.Uniform.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Uniform Distributions.</p>  <h5 id=\"args-115\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-119\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"Uniform.std\"><code>tf.contrib.distributions.Uniform.std(name='std')</code></h4>  <h4 id=\"Uniform.validate_args\"><code>tf.contrib.distributions.Uniform.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Uniform.variance\"><code>tf.contrib.distributions.Uniform.variance(name='variance')</code></h4>  <h3 id=\"multivariate-distributions\">Multivariate distributions</h3>  <h4 id=\"multivariate-normal\">Multivariate normal</h4>  <h3 id=\"MultivariateNormalDiag\"><code>class tf.contrib.distributions.MultivariateNormalDiag</code></h3> <p>The multivariate normal distribution on <code>R^k</code>.</p> <p>This distribution is defined by a 1-D mean <code>mu</code> and a 1-D diagonal <code>diag_stdev</code>, representing the standard deviations. This distribution assumes the random variables, <code>(X_1,...,X_k)</code> are independent, thus no non-diagonal terms of the covariance matrix are needed.</p> <p>This allows for <code>O(k)</code> pdf evaluation, sampling, and storage.</p>  <h4 id=\"mathematical-details-6\">Mathematical details</h4> <p>The PDF of this distribution is defined in terms of the diagonal covariance determined by <code>diag_stdev</code>: <code>C_{ii} = diag_stdev[i]**2</code>.</p> <pre class=\"\">f(x) = (2 pi)^(-k/2) |det(C)|^(-1/2) exp(-1/2 (x - mu)^T C^{-1} (x - mu))\n</pre>  <h4 id=\"examples-5\">Examples</h4> <p>A single multi-variate Gaussian distribution is defined by a vector of means of length <code>k</code>, and the square roots of the (independent) random variables.</p> <p>Extra leading dimensions, if provided, allow for batches.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Initialize a single 3-variate Gaussian with diagonal standard deviation.\nmu = [1, 2, 3.]\ndiag_stdev = [4, 5, 6.]\ndist = tf.contrib.distributions.MultivariateNormalDiag(mu, diag_stdev)\n\n# Evaluate this on an observation in R^3, returning a scalar.\ndist.pdf([-1, 0, 1])\n\n# Initialize a batch of two 3-variate Gaussians.\nmu = [[1, 2, 3], [11, 22, 33]]  # shape 2 x 3\ndiag_stdev = ...  # shape 2 x 3, positive.\ndist = tf.contrib.distributions.MultivariateNormalDiag(mu, diag_stdev)\n\n# Evaluate this on a two observations, each in R^3, returning a length two\n# tensor.\nx = [[-1, 0, 1], [-11, 0, 11]]  # Shape 2 x 3.\ndist.pdf(x)\n</pre>  <h4 id=\"MultivariateNormalDiag.__init__\"><code>tf.contrib.distributions.MultivariateNormalDiag.__init__(mu, diag_stdev, validate_args=True, allow_nan_stats=False, name='MultivariateNormalDiag')</code></h4> <p>Multivariate Normal distributions on <code>R^k</code>.</p> <p>User must provide means <code>mu</code> and standard deviations <code>diag_stdev</code>. Each batch member represents a random vector <code>(X_1,...,X_k)</code> of independent random normals. The mean of <code>X_i</code> is <code>mu[i]</code>, and the standard deviation is <code>diag_stdev[i]</code>.</p>  <h5 id=\"args-116\">Args:</h5> <ul> <li>\n<code>mu</code>: Rank <code>N + 1</code> floating point tensor with shape <code>[N1,...,Nb, k]</code>, <code>b &gt;= 0</code>.</li> <li>\n<code>diag_stdev</code>: Rank <code>N + 1</code> <code>Tensor</code> with same <code>dtype</code> and shape as <code>mu</code>, representing the standard deviations. Must be positive.</li> <li>\n<code>validate_args</code>: Whether to validate input with asserts. If <code>validate_args</code> is <code>False</code>, and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: <code>Boolean</code>, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to give Ops created by the initializer.</li> </ul>  <h5 id=\"raises-16\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>mu</code> and <code>diag_stdev</code> are different dtypes.</li> </ul>  <h4 id=\"MultivariateNormalDiag.allow_nan_stats\"><code>tf.contrib.distributions.MultivariateNormalDiag.allow_nan_stats</code></h4> <p><code>Boolean</code> describing behavior when stats are undefined.</p>  <h4 id=\"MultivariateNormalDiag.batch_shape\"><code>tf.contrib.distributions.MultivariateNormalDiag.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p>  <h4 id=\"MultivariateNormalDiag.cdf\"><code>tf.contrib.distributions.MultivariateNormalDiag.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"MultivariateNormalDiag.dtype\"><code>tf.contrib.distributions.MultivariateNormalDiag.dtype</code></h4>  <h4 id=\"MultivariateNormalDiag.entropy\"><code>tf.contrib.distributions.MultivariateNormalDiag.entropy(name='entropy')</code></h4> <p>The entropies of these Multivariate Normals.</p>  <h5 id=\"args-117\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-120\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropies.</li> </ul>  <h4 id=\"MultivariateNormalDiag.event_shape\"><code>tf.contrib.distributions.MultivariateNormalDiag.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h4 id=\"MultivariateNormalDiag.get_batch_shape\"><code>tf.contrib.distributions.MultivariateNormalDiag.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p>  <h4 id=\"MultivariateNormalDiag.get_event_shape\"><code>tf.contrib.distributions.MultivariateNormalDiag.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p>  <h4 id=\"MultivariateNormalDiag.is_continuous\"><code>tf.contrib.distributions.MultivariateNormalDiag.is_continuous</code></h4>  <h4 id=\"MultivariateNormalDiag.is_reparameterized\"><code>tf.contrib.distributions.MultivariateNormalDiag.is_reparameterized</code></h4>  <h4 id=\"MultivariateNormalDiag.log_cdf\"><code>tf.contrib.distributions.MultivariateNormalDiag.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"MultivariateNormalDiag.log_pdf\"><code>tf.contrib.distributions.MultivariateNormalDiag.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"MultivariateNormalDiag.log_pmf\"><code>tf.contrib.distributions.MultivariateNormalDiag.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"MultivariateNormalDiag.log_prob\"><code>tf.contrib.distributions.MultivariateNormalDiag.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations <code>x</code> given these Multivariate Normals.</p> <p><code>x</code> is a batch vector with compatible shape if <code>x</code> is a <code>Tensor</code> whose shape can be broadcast up to either:</p> <pre class=\"\">self.batch_shape + self.event_shape\nOR\n[M1,...,Mm] + self.batch_shape + self.event_shape\n```\n\n##### Args:\n\n\n*  &lt;b&gt;`x`&lt;/b&gt;: Compatible batch vector with same `dtype` as this distribution.\n*  &lt;b&gt;`name`&lt;/b&gt;: The name to give this op.\n\n##### Returns:\n\n\n*  &lt;b&gt;`log_prob`&lt;/b&gt;: tensor of dtype `dtype`, the log-PDFs of `x`.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiag.log_sigma_det(name='log_sigma_det')` {#MultivariateNormalDiag.log_sigma_det}\n\nLog of determinant of covariance matrix.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiag.mean(name='mean')` {#MultivariateNormalDiag.mean}\n\nMean of each batch member.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiag.mode(name='mode')` {#MultivariateNormalDiag.mode}\n\nMode of each batch member.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiag.mu` {#MultivariateNormalDiag.mu}\n\n\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiag.name` {#MultivariateNormalDiag.name}\n\n\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiag.pdf(value, name='pdf')` {#MultivariateNormalDiag.pdf}\n\nThe probability density function.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiag.pmf(value, name='pmf')` {#MultivariateNormalDiag.pmf}\n\nThe probability mass function.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiag.prob(x, name='prob')` {#MultivariateNormalDiag.prob}\n\nThe PDF of observations `x` under these Multivariate Normals.\n\n`x` is a batch vector with compatible shape if `x` is a `Tensor` whose\nshape can be broadcast up to either:\n\n</pre> <p>self.batch_shape + self.event_shape OR [M1,...,Mm] + self.batch_shape + self.event_shape ```</p>  <h5 id=\"args-118\">Args:</h5> <ul> <li>\n<code>x</code>: Compatible batch vector with same <code>dtype</code> as this distribution.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-121\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the prob values of <code>x</code>.</li> </ul>  <h4 id=\"MultivariateNormalDiag.sample\"><code>tf.contrib.distributions.MultivariateNormalDiag.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-119\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-122\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"MultivariateNormalDiag.sample_n\"><code>tf.contrib.distributions.MultivariateNormalDiag.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Multivariate Normal Distributions.</p>  <h5 id=\"args-120\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-123\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by broadcasting the hyperparameters.</li> </ul>  <h4 id=\"MultivariateNormalDiag.sigma\"><code>tf.contrib.distributions.MultivariateNormalDiag.sigma</code></h4> <p>Dense (batch) covariance matrix, if available.</p>  <h4 id=\"MultivariateNormalDiag.sigma_det\"><code>tf.contrib.distributions.MultivariateNormalDiag.sigma_det(name='sigma_det')</code></h4> <p>Determinant of covariance matrix.</p>  <h4 id=\"MultivariateNormalDiag.std\"><code>tf.contrib.distributions.MultivariateNormalDiag.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"MultivariateNormalDiag.validate_args\"><code>tf.contrib.distributions.MultivariateNormalDiag.validate_args</code></h4> <p><code>Boolean</code> describing behavior on invalid input.</p>  <h4 id=\"MultivariateNormalDiag.variance\"><code>tf.contrib.distributions.MultivariateNormalDiag.variance(name='variance')</code></h4> <p>Variance of each batch member.</p>  <h3 id=\"MultivariateNormalFull\"><code>class tf.contrib.distributions.MultivariateNormalFull</code></h3> <p>The multivariate normal distribution on <code>R^k</code>.</p> <p>This distribution is defined by a 1-D mean <code>mu</code> and covariance matrix <code>sigma</code>. Evaluation of the pdf, determinant, and sampling are all <code>O(k^3)</code> operations.</p>  <h4 id=\"mathematical-details-7\">Mathematical details</h4> <p>With <code>C = sigma</code>, the PDF of this distribution is:</p> <pre class=\"\">f(x) = (2 pi)^(-k/2) |det(C)|^(-1/2) exp(-1/2 (x - mu)^T C^{-1} (x - mu))\n</pre>  <h4 id=\"examples-6\">Examples</h4> <p>A single multi-variate Gaussian distribution is defined by a vector of means of length <code>k</code>, and a covariance matrix of shape <code>k x k</code>.</p> <p>Extra leading dimensions, if provided, allow for batches.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Initialize a single 3-variate Gaussian with diagonal covariance.\nmu = [1, 2, 3.]\nsigma = [[1, 0, 0], [0, 3, 0], [0, 0, 2.]]\ndist = tf.contrib.distributions.MultivariateNormalFull(mu, chol)\n\n# Evaluate this on an observation in R^3, returning a scalar.\ndist.pdf([-1, 0, 1])\n\n# Initialize a batch of two 3-variate Gaussians.\nmu = [[1, 2, 3], [11, 22, 33.]]\nsigma = ...  # shape 2 x 3 x 3, positive definite.\ndist = tf.contrib.distributions.MultivariateNormalFull(mu, sigma)\n\n# Evaluate this on a two observations, each in R^3, returning a length two\n# tensor.\nx = [[-1, 0, 1], [-11, 0, 11.]]  # Shape 2 x 3.\ndist.pdf(x)\n</pre>  <h4 id=\"MultivariateNormalFull.__init__\"><code>tf.contrib.distributions.MultivariateNormalFull.__init__(mu, sigma, validate_args=True, allow_nan_stats=False, name='MultivariateNormalFull')</code></h4> <p>Multivariate Normal distributions on <code>R^k</code>.</p> <p>User must provide means <code>mu</code> and <code>sigma</code>, the mean and covariance.</p>  <h5 id=\"args-121\">Args:</h5> <ul> <li>\n<code>mu</code>: <code>(N+1)-D</code> floating point tensor with shape <code>[N1,...,Nb, k]</code>, <code>b &gt;= 0</code>.</li> <li>\n<code>sigma</code>: <code>(N+2)-D</code> <code>Tensor</code> with same <code>dtype</code> as <code>mu</code> and shape <code>[N1,...,Nb, k, k]</code>. Each batch member must be positive definite.</li> <li>\n<code>validate_args</code>: Whether to validate input with asserts. If <code>validate_args</code> is <code>False</code>, and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: <code>Boolean</code>, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to give Ops created by the initializer.</li> </ul>  <h5 id=\"raises-17\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>mu</code> and <code>sigma</code> are different dtypes.</li> </ul>  <h4 id=\"MultivariateNormalFull.allow_nan_stats\"><code>tf.contrib.distributions.MultivariateNormalFull.allow_nan_stats</code></h4> <p><code>Boolean</code> describing behavior when stats are undefined.</p>  <h4 id=\"MultivariateNormalFull.batch_shape\"><code>tf.contrib.distributions.MultivariateNormalFull.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p>  <h4 id=\"MultivariateNormalFull.cdf\"><code>tf.contrib.distributions.MultivariateNormalFull.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"MultivariateNormalFull.dtype\"><code>tf.contrib.distributions.MultivariateNormalFull.dtype</code></h4>  <h4 id=\"MultivariateNormalFull.entropy\"><code>tf.contrib.distributions.MultivariateNormalFull.entropy(name='entropy')</code></h4> <p>The entropies of these Multivariate Normals.</p>  <h5 id=\"args-122\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-124\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropies.</li> </ul>  <h4 id=\"MultivariateNormalFull.event_shape\"><code>tf.contrib.distributions.MultivariateNormalFull.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h4 id=\"MultivariateNormalFull.get_batch_shape\"><code>tf.contrib.distributions.MultivariateNormalFull.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p>  <h4 id=\"MultivariateNormalFull.get_event_shape\"><code>tf.contrib.distributions.MultivariateNormalFull.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p>  <h4 id=\"MultivariateNormalFull.is_continuous\"><code>tf.contrib.distributions.MultivariateNormalFull.is_continuous</code></h4>  <h4 id=\"MultivariateNormalFull.is_reparameterized\"><code>tf.contrib.distributions.MultivariateNormalFull.is_reparameterized</code></h4>  <h4 id=\"MultivariateNormalFull.log_cdf\"><code>tf.contrib.distributions.MultivariateNormalFull.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"MultivariateNormalFull.log_pdf\"><code>tf.contrib.distributions.MultivariateNormalFull.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"MultivariateNormalFull.log_pmf\"><code>tf.contrib.distributions.MultivariateNormalFull.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"MultivariateNormalFull.log_prob\"><code>tf.contrib.distributions.MultivariateNormalFull.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations <code>x</code> given these Multivariate Normals.</p> <p><code>x</code> is a batch vector with compatible shape if <code>x</code> is a <code>Tensor</code> whose shape can be broadcast up to either:</p> <pre class=\"\">self.batch_shape + self.event_shape\nOR\n[M1,...,Mm] + self.batch_shape + self.event_shape\n```\n\n##### Args:\n\n\n*  &lt;b&gt;`x`&lt;/b&gt;: Compatible batch vector with same `dtype` as this distribution.\n*  &lt;b&gt;`name`&lt;/b&gt;: The name to give this op.\n\n##### Returns:\n\n\n*  &lt;b&gt;`log_prob`&lt;/b&gt;: tensor of dtype `dtype`, the log-PDFs of `x`.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalFull.log_sigma_det(name='log_sigma_det')` {#MultivariateNormalFull.log_sigma_det}\n\nLog of determinant of covariance matrix.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalFull.mean(name='mean')` {#MultivariateNormalFull.mean}\n\nMean of each batch member.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalFull.mode(name='mode')` {#MultivariateNormalFull.mode}\n\nMode of each batch member.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalFull.mu` {#MultivariateNormalFull.mu}\n\n\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalFull.name` {#MultivariateNormalFull.name}\n\n\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalFull.pdf(value, name='pdf')` {#MultivariateNormalFull.pdf}\n\nThe probability density function.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalFull.pmf(value, name='pmf')` {#MultivariateNormalFull.pmf}\n\nThe probability mass function.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalFull.prob(x, name='prob')` {#MultivariateNormalFull.prob}\n\nThe PDF of observations `x` under these Multivariate Normals.\n\n`x` is a batch vector with compatible shape if `x` is a `Tensor` whose\nshape can be broadcast up to either:\n\n</pre> <p>self.batch_shape + self.event_shape OR [M1,...,Mm] + self.batch_shape + self.event_shape ```</p>  <h5 id=\"args-123\">Args:</h5> <ul> <li>\n<code>x</code>: Compatible batch vector with same <code>dtype</code> as this distribution.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-125\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the prob values of <code>x</code>.</li> </ul>  <h4 id=\"MultivariateNormalFull.sample\"><code>tf.contrib.distributions.MultivariateNormalFull.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-124\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-126\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"MultivariateNormalFull.sample_n\"><code>tf.contrib.distributions.MultivariateNormalFull.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Multivariate Normal Distributions.</p>  <h5 id=\"args-125\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-127\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by broadcasting the hyperparameters.</li> </ul>  <h4 id=\"MultivariateNormalFull.sigma\"><code>tf.contrib.distributions.MultivariateNormalFull.sigma</code></h4> <p>Dense (batch) covariance matrix, if available.</p>  <h4 id=\"MultivariateNormalFull.sigma_det\"><code>tf.contrib.distributions.MultivariateNormalFull.sigma_det(name='sigma_det')</code></h4> <p>Determinant of covariance matrix.</p>  <h4 id=\"MultivariateNormalFull.std\"><code>tf.contrib.distributions.MultivariateNormalFull.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"MultivariateNormalFull.validate_args\"><code>tf.contrib.distributions.MultivariateNormalFull.validate_args</code></h4> <p><code>Boolean</code> describing behavior on invalid input.</p>  <h4 id=\"MultivariateNormalFull.variance\"><code>tf.contrib.distributions.MultivariateNormalFull.variance(name='variance')</code></h4> <p>Variance of each batch member.</p>  <h3 id=\"MultivariateNormalCholesky\"><code>class tf.contrib.distributions.MultivariateNormalCholesky</code></h3> <p>The multivariate normal distribution on <code>R^k</code>.</p> <p>This distribution is defined by a 1-D mean <code>mu</code> and a Cholesky factor <code>chol</code>. Providing the Cholesky factor allows for <code>O(k^2)</code> pdf evaluation and sampling, and requires <code>O(k^2)</code> storage.</p>  <h4 id=\"mathematical-details-8\">Mathematical details</h4> <p>The Cholesky factor <code>chol</code> defines the covariance matrix: <code>C = chol chol^T</code>.</p> <p>The PDF of this distribution is then:</p> <pre class=\"\">f(x) = (2 pi)^(-k/2) |det(C)|^(-1/2) exp(-1/2 (x - mu)^T C^{-1} (x - mu))\n</pre>  <h4 id=\"examples-7\">Examples</h4> <p>A single multi-variate Gaussian distribution is defined by a vector of means of length <code>k</code>, and a covariance matrix of shape <code>k x k</code>.</p> <p>Extra leading dimensions, if provided, allow for batches.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Initialize a single 3-variate Gaussian with diagonal covariance.\n# Note, this would be more efficient with MultivariateNormalDiag.\nmu = [1, 2, 3.]\nchol = [[1, 0, 0], [0, 3, 0], [0, 0, 2]]\ndist = tf.contrib.distributions.MultivariateNormalCholesky(mu, chol)\n\n# Evaluate this on an observation in R^3, returning a scalar.\ndist.pdf([-1, 0, 1])\n\n# Initialize a batch of two 3-variate Gaussians.\nmu = [[1, 2, 3], [11, 22, 33]]\nchol = ...  # shape 2 x 3 x 3, lower triangular, positive diagonal.\ndist = tf.contrib.distributions.MultivariateNormalCholesky(mu, chol)\n\n# Evaluate this on a two observations, each in R^3, returning a length two\n# tensor.\nx = [[-1, 0, 1], [-11, 0, 11]]  # Shape 2 x 3.\ndist.pdf(x)\n</pre> <p>Trainable (batch) Choesky matrices can be created with <code>tf.contrib.distributions.batch_matrix_diag_transform()</code></p>  <h4 id=\"MultivariateNormalCholesky.__init__\"><code>tf.contrib.distributions.MultivariateNormalCholesky.__init__(mu, chol, validate_args=True, allow_nan_stats=False, name='MultivariateNormalCholesky')</code></h4> <p>Multivariate Normal distributions on <code>R^k</code>.</p> <p>User must provide means <code>mu</code> and <code>chol</code> which holds the (batch) Cholesky factors, such that the covariance of each batch member is <code>chol chol^T</code>.</p>  <h5 id=\"args-126\">Args:</h5> <ul> <li>\n<code>mu</code>: <code>(N+1)-D</code> floating point tensor with shape <code>[N1,...,Nb, k]</code>, <code>b &gt;= 0</code>.</li> <li>\n<code>chol</code>: <code>(N+2)-D</code> <code>Tensor</code> with same <code>dtype</code> as <code>mu</code> and shape <code>[N1,...,Nb, k, k]</code>. The upper triangular part is ignored (treated as though it is zero), and the diagonal must be positive.</li> <li>\n<code>validate_args</code>: Whether to validate input with asserts. If <code>validate_args</code> is <code>False</code>, and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: <code>Boolean</code>, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to give Ops created by the initializer.</li> </ul>  <h5 id=\"raises-18\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If <code>mu</code> and <code>chol</code> are different dtypes.</li> </ul>  <h4 id=\"MultivariateNormalCholesky.allow_nan_stats\"><code>tf.contrib.distributions.MultivariateNormalCholesky.allow_nan_stats</code></h4> <p><code>Boolean</code> describing behavior when stats are undefined.</p>  <h4 id=\"MultivariateNormalCholesky.batch_shape\"><code>tf.contrib.distributions.MultivariateNormalCholesky.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p>  <h4 id=\"MultivariateNormalCholesky.cdf\"><code>tf.contrib.distributions.MultivariateNormalCholesky.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"MultivariateNormalCholesky.dtype\"><code>tf.contrib.distributions.MultivariateNormalCholesky.dtype</code></h4>  <h4 id=\"MultivariateNormalCholesky.entropy\"><code>tf.contrib.distributions.MultivariateNormalCholesky.entropy(name='entropy')</code></h4> <p>The entropies of these Multivariate Normals.</p>  <h5 id=\"args-127\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-128\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropies.</li> </ul>  <h4 id=\"MultivariateNormalCholesky.event_shape\"><code>tf.contrib.distributions.MultivariateNormalCholesky.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h4 id=\"MultivariateNormalCholesky.get_batch_shape\"><code>tf.contrib.distributions.MultivariateNormalCholesky.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p>  <h4 id=\"MultivariateNormalCholesky.get_event_shape\"><code>tf.contrib.distributions.MultivariateNormalCholesky.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p>  <h4 id=\"MultivariateNormalCholesky.is_continuous\"><code>tf.contrib.distributions.MultivariateNormalCholesky.is_continuous</code></h4>  <h4 id=\"MultivariateNormalCholesky.is_reparameterized\"><code>tf.contrib.distributions.MultivariateNormalCholesky.is_reparameterized</code></h4>  <h4 id=\"MultivariateNormalCholesky.log_cdf\"><code>tf.contrib.distributions.MultivariateNormalCholesky.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"MultivariateNormalCholesky.log_pdf\"><code>tf.contrib.distributions.MultivariateNormalCholesky.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"MultivariateNormalCholesky.log_pmf\"><code>tf.contrib.distributions.MultivariateNormalCholesky.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"MultivariateNormalCholesky.log_prob\"><code>tf.contrib.distributions.MultivariateNormalCholesky.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations <code>x</code> given these Multivariate Normals.</p> <p><code>x</code> is a batch vector with compatible shape if <code>x</code> is a <code>Tensor</code> whose shape can be broadcast up to either:</p> <pre class=\"\">self.batch_shape + self.event_shape\nOR\n[M1,...,Mm] + self.batch_shape + self.event_shape\n```\n\n##### Args:\n\n\n*  &lt;b&gt;`x`&lt;/b&gt;: Compatible batch vector with same `dtype` as this distribution.\n*  &lt;b&gt;`name`&lt;/b&gt;: The name to give this op.\n\n##### Returns:\n\n\n*  &lt;b&gt;`log_prob`&lt;/b&gt;: tensor of dtype `dtype`, the log-PDFs of `x`.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalCholesky.log_sigma_det(name='log_sigma_det')` {#MultivariateNormalCholesky.log_sigma_det}\n\nLog of determinant of covariance matrix.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalCholesky.mean(name='mean')` {#MultivariateNormalCholesky.mean}\n\nMean of each batch member.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalCholesky.mode(name='mode')` {#MultivariateNormalCholesky.mode}\n\nMode of each batch member.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalCholesky.mu` {#MultivariateNormalCholesky.mu}\n\n\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalCholesky.name` {#MultivariateNormalCholesky.name}\n\n\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalCholesky.pdf(value, name='pdf')` {#MultivariateNormalCholesky.pdf}\n\nThe probability density function.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalCholesky.pmf(value, name='pmf')` {#MultivariateNormalCholesky.pmf}\n\nThe probability mass function.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalCholesky.prob(x, name='prob')` {#MultivariateNormalCholesky.prob}\n\nThe PDF of observations `x` under these Multivariate Normals.\n\n`x` is a batch vector with compatible shape if `x` is a `Tensor` whose\nshape can be broadcast up to either:\n\n</pre> <p>self.batch_shape + self.event_shape OR [M1,...,Mm] + self.batch_shape + self.event_shape ```</p>  <h5 id=\"args-128\">Args:</h5> <ul> <li>\n<code>x</code>: Compatible batch vector with same <code>dtype</code> as this distribution.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-129\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the prob values of <code>x</code>.</li> </ul>  <h4 id=\"MultivariateNormalCholesky.sample\"><code>tf.contrib.distributions.MultivariateNormalCholesky.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-129\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-130\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"MultivariateNormalCholesky.sample_n\"><code>tf.contrib.distributions.MultivariateNormalCholesky.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Multivariate Normal Distributions.</p>  <h5 id=\"args-130\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-131\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by broadcasting the hyperparameters.</li> </ul>  <h4 id=\"MultivariateNormalCholesky.sigma\"><code>tf.contrib.distributions.MultivariateNormalCholesky.sigma</code></h4> <p>Dense (batch) covariance matrix, if available.</p>  <h4 id=\"MultivariateNormalCholesky.sigma_det\"><code>tf.contrib.distributions.MultivariateNormalCholesky.sigma_det(name='sigma_det')</code></h4> <p>Determinant of covariance matrix.</p>  <h4 id=\"MultivariateNormalCholesky.std\"><code>tf.contrib.distributions.MultivariateNormalCholesky.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"MultivariateNormalCholesky.validate_args\"><code>tf.contrib.distributions.MultivariateNormalCholesky.validate_args</code></h4> <p><code>Boolean</code> describing behavior on invalid input.</p>  <h4 id=\"MultivariateNormalCholesky.variance\"><code>tf.contrib.distributions.MultivariateNormalCholesky.variance(name='variance')</code></h4> <p>Variance of each batch member.</p>  <h3 id=\"batch_matrix_diag_transform\"><code>tf.contrib.distributions.batch_matrix_diag_transform(matrix, transform=None, name=None)</code></h3> <p>Transform diagonal of [batch-]matrix, leave rest of matrix unchanged.</p> <p>Create a trainable covariance defined by a Cholesky factor:</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Transform network layer into 2 x 2 array.\nmatrix_values = tf.contrib.layers.fully_connected(activations, 4)\nmatrix = tf.reshape(matrix_values, (batch_size, 2, 2))\n\n# Make the diagonal positive.  If the upper triangle was zero, this would be a\n# valid Cholesky factor.\nchol = batch_matrix_diag_transform(matrix, transform=tf.nn.softplus)\n\n# OperatorPDCholesky ignores the upper triangle.\noperator = OperatorPDCholesky(chol)\n</pre> <p>Example of heteroskedastic 2-D linear regression.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Get a trainable Cholesky factor.\nmatrix_values = tf.contrib.layers.fully_connected(activations, 4)\nmatrix = tf.reshape(matrix_values, (batch_size, 2, 2))\nchol = batch_matrix_diag_transform(matrix, transform=tf.nn.softplus)\n\n# Get a trainable mean.\nmu = tf.contrib.layers.fully_connected(activations, 2)\n\n# This is a fully trainable multivariate normal!\ndist = tf.contrib.distributions.MVNCholesky(mu, chol)\n\n# Standard log loss.  Minimizing this will \"train\" mu and chol, and then dist\n# will be a distribution predicting labels as multivariate Gaussians.\nloss = -1 * tf.reduce_mean(dist.log_pdf(labels))\n</pre>  <h5 id=\"args-131\">Args:</h5> <ul> <li>\n<code>matrix</code>: Rank <code>R</code> <code>Tensor</code>, <code>R &gt;= 2</code>, where the last two dimensions are equal.</li> <li>\n<code>transform</code>: Element-wise function mapping <code>Tensors</code> to <code>Tensors</code>. To be applied to the diagonal of <code>matrix</code>. If <code>None</code>, <code>matrix</code> is returned unchanged. Defaults to <code>None</code>.</li> <li>\n<code>name</code>: A name to give created ops. Defaults to \"batch_matrix_diag_transform\".</li> </ul>  <h5 id=\"returns-132\">Returns:</h5> <p>A <code>Tensor</code> with same shape and <code>dtype</code> as <code>matrix</code>.</p>  <h4 id=\"other-multivariate-distributions\">Other multivariate distributions</h4>  <h3 id=\"Dirichlet\"><code>class tf.contrib.distributions.Dirichlet</code></h3> <p>Dirichlet distribution.</p> <p>This distribution is parameterized by a vector <code>alpha</code> of concentration parameters for <code>k</code> classes.</p>  <h4 id=\"mathematical-details-9\">Mathematical details</h4> <p>The Dirichlet is a distribution over the standard n-simplex, where the standard n-simplex is defined by: <code>{ (x_1, ..., x_n) in R^(n+1) | sum_j x_j = 1 and x_j &gt;= 0 for all j }</code>. The distribution has hyperparameters <code>alpha = (alpha_1,...,alpha_k)</code>, and probability mass function (prob):</p> <p><code>prob(x) = 1 / Beta(alpha) * prod_j x_j^(alpha_j - 1)</code></p> <p>where <code>Beta(x) = prod_j Gamma(x_j) / Gamma(sum_j x_j)</code> is the multivariate beta function.</p> <p>This class provides methods to create indexed batches of Dirichlet distributions. If the provided <code>alpha</code> is rank 2 or higher, for every fixed set of leading dimensions, the last dimension represents one single Dirichlet distribution. When calling distribution functions (e.g. <code>dist.prob(x)</code>), <code>alpha</code> and <code>x</code> are broadcast to the same shape (if possible). In all cases, the last dimension of alpha/x represents single Dirichlet distributions.</p>  <h4 id=\"examples-8\">Examples</h4> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">alpha = [1, 2, 3]\ndist = Dirichlet(alpha)\n</pre> <p>Creates a 3-class distribution, with the 3rd class is most likely to be drawn. The distribution functions can be evaluated on x.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># x same shape as alpha.\nx = [.2, .3, .5]\ndist.prob(x)  # Shape []\n\n# alpha will be broadcast to [[1, 2, 3], [1, 2, 3]] to match x.\nx = [[.1, .4, .5], [.2, .3, .5]]\ndist.prob(x)  # Shape [2]\n\n# alpha will be broadcast to shape [5, 7, 3] to match x.\nx = [[...]]  # Shape [5, 7, 3]\ndist.prob(x)  # Shape [5, 7]\n</pre> <p>Creates a 2-batch of 3-class distributions.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">alpha = [[1, 2, 3], [4, 5, 6]]  # Shape [2, 3]\ndist = Dirichlet(alpha)\n\n# x will be broadcast to [[2, 1, 0], [2, 1, 0]] to match alpha.\nx = [.2, .3, .5]\ndist.prob(x)  # Shape [2]\n</pre>  <h4 id=\"Dirichlet.__init__\"><code>tf.contrib.distributions.Dirichlet.__init__(alpha, validate_args=True, allow_nan_stats=False, name='Dirichlet')</code></h4> <p>Initialize a batch of Dirichlet distributions.</p>  <h5 id=\"args-132\">Args:</h5> <ul> <li>\n<code>alpha</code>: Positive floating point tensor with shape broadcastable to <code>[N1,..., Nm, k]</code> <code>m &gt;= 0</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different <code>k</code> class Dirichlet distributions.</li> <li>\n<code>validate_args</code>: Whether to assert valid values for parameters <code>alpha</code> and <code>x</code> in <code>prob</code> and <code>log_prob</code>. If <code>False</code>, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li><p><code>name</code>: The name to prefix Ops created by this distribution class.</p></li> <li><p><code>Examples</code>: </p></li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define 1-batch of 2-class Dirichlet distributions,\n# also known as a Beta distribution.\ndist = Dirichlet([1.1, 2.0])\n\n# Define a 2-batch of 3-class distributions.\ndist = Dirichlet([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n</pre>  <h4 id=\"Dirichlet.allow_nan_stats\"><code>tf.contrib.distributions.Dirichlet.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Dirichlet.alpha\"><code>tf.contrib.distributions.Dirichlet.alpha</code></h4> <p>Shape parameter.</p>  <h4 id=\"Dirichlet.batch_shape\"><code>tf.contrib.distributions.Dirichlet.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-133\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-133\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Dirichlet.cdf\"><code>tf.contrib.distributions.Dirichlet.cdf(x, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"Dirichlet.dtype\"><code>tf.contrib.distributions.Dirichlet.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"Dirichlet.entropy\"><code>tf.contrib.distributions.Dirichlet.entropy(name='entropy')</code></h4> <p>Entropy of the distribution in nats.</p>  <h4 id=\"Dirichlet.event_shape\"><code>tf.contrib.distributions.Dirichlet.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-134\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-134\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Dirichlet.get_batch_shape\"><code>tf.contrib.distributions.Dirichlet.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-135\">Returns:</h5> <p>batch shape</p>  <h4 id=\"Dirichlet.get_event_shape\"><code>tf.contrib.distributions.Dirichlet.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-136\">Returns:</h5> <p>event shape</p>  <h4 id=\"Dirichlet.is_continuous\"><code>tf.contrib.distributions.Dirichlet.is_continuous</code></h4>  <h4 id=\"Dirichlet.is_reparameterized\"><code>tf.contrib.distributions.Dirichlet.is_reparameterized</code></h4>  <h4 id=\"Dirichlet.log_cdf\"><code>tf.contrib.distributions.Dirichlet.log_cdf(x, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"Dirichlet.log_pdf\"><code>tf.contrib.distributions.Dirichlet.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Dirichlet.log_pmf\"><code>tf.contrib.distributions.Dirichlet.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Dirichlet.log_prob\"><code>tf.contrib.distributions.Dirichlet.log_prob(x, name='log_prob')</code></h4> <p><code>Log(P[counts])</code>, computed for every batch member.</p>  <h5 id=\"args-135\">Args:</h5> <ul> <li>\n<code>x</code>: Non-negative tensor with dtype <code>dtype</code> and whose shape can be broadcast with <code>self.alpha</code>. For fixed leading dimensions, the last dimension represents counts for the corresponding Dirichlet distribution in <code>self.alpha</code>. <code>x</code> is only legal if it sums up to one.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"log_prob\".</li> </ul>  <h5 id=\"returns-137\">Returns:</h5> <p>Log probabilities for each record, shape <code>[N1,...,Nm]</code>.</p>  <h4 id=\"Dirichlet.mean\"><code>tf.contrib.distributions.Dirichlet.mean(name='mean')</code></h4> <p>Mean of the distribution.</p>  <h4 id=\"Dirichlet.mode\"><code>tf.contrib.distributions.Dirichlet.mode(name='mode')</code></h4> <p>Mode of the distribution.</p> <p>Note that the mode for the Beta distribution is only defined when <code>alpha &gt; 1</code>. This returns the mode when <code>alpha &gt; 1</code>, and NaN otherwise. If <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised rather than returning <code>NaN</code>.</p>  <h5 id=\"args-136\">Args:</h5> <ul> <li>\n<code>name</code>: The name for this op.</li> </ul>  <h5 id=\"returns-138\">Returns:</h5> <p>Mode of the Dirichlet distribution.</p>  <h4 id=\"Dirichlet.name\"><code>tf.contrib.distributions.Dirichlet.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"Dirichlet.pdf\"><code>tf.contrib.distributions.Dirichlet.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Dirichlet.pmf\"><code>tf.contrib.distributions.Dirichlet.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Dirichlet.prob\"><code>tf.contrib.distributions.Dirichlet.prob(x, name='prob')</code></h4> <p><code>P[x]</code>, computed for every batch member.</p>  <h5 id=\"args-137\">Args:</h5> <ul> <li>\n<code>x</code>: Non-negative tensor with dtype <code>dtype</code> and whose shape can be broadcast with <code>self.alpha</code>. For fixed leading dimensions, the last dimension represents x for the corresponding Dirichlet distribution in <code>self.alpha</code> and <code>self.beta</code>. <code>x</code> is only legal if it sums up to one.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"prob\".</li> </ul>  <h5 id=\"returns-139\">Returns:</h5> <p>Probabilities for each record, shape <code>[N1,...,Nm]</code>.</p>  <h4 id=\"Dirichlet.sample\"><code>tf.contrib.distributions.Dirichlet.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-138\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-140\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Dirichlet.sample_n\"><code>tf.contrib.distributions.Dirichlet.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the distributions.</p>  <h5 id=\"args-139\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-141\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by broadcasting the hyperparameters.</li> </ul>  <h4 id=\"Dirichlet.std\"><code>tf.contrib.distributions.Dirichlet.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"Dirichlet.validate_args\"><code>tf.contrib.distributions.Dirichlet.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Dirichlet.variance\"><code>tf.contrib.distributions.Dirichlet.variance(name='variance')</code></h4> <p>Variance of the distribution.</p>  <h3 id=\"DirichletMultinomial\"><code>class tf.contrib.distributions.DirichletMultinomial</code></h3> <p>DirichletMultinomial mixture distribution.</p> <p>This distribution is parameterized by a vector <code>alpha</code> of concentration parameters for <code>k</code> classes and <code>n</code>, the counts per each class..</p>  <h4 id=\"mathematical-details-10\">Mathematical details</h4> <p>The Dirichlet Multinomial is a distribution over k-class count data, meaning for each k-tuple of non-negative integer <code>counts = [c_1,...,c_k]</code>, we have a probability of these draws being made from the distribution. The distribution has hyperparameters <code>alpha = (alpha_1,...,alpha_k)</code>, and probability mass function (pmf):</p> <p><code>pmf(counts) = N! / (n_1!...n_k!) * Beta(alpha + c) / Beta(alpha)</code></p> <p>where above <code>N = sum_j n_j</code>, <code>N!</code> is <code>N</code> factorial, and <code>Beta(x) = prod_j Gamma(x_j) / Gamma(sum_j x_j)</code> is the multivariate beta function.</p> <p>This is a mixture distribution in that <code>M</code> samples can be produced by: 1. Choose class probabilities <code>p = (p_1,...,p_k) ~ Dir(alpha)</code> 2. Draw integers <code>m = (n_1,...,n_k) ~ Multinomial(N, p)</code></p> <p>This class provides methods to create indexed batches of Dirichlet Multinomial distributions. If the provided <code>alpha</code> is rank 2 or higher, for every fixed set of leading dimensions, the last dimension represents one single Dirichlet Multinomial distribution. When calling distribution functions (e.g. <code>dist.pmf(counts)</code>), <code>alpha</code> and <code>counts</code> are broadcast to the same shape (if possible). In all cases, the last dimension of alpha/counts represents single Dirichlet Multinomial distributions.</p>  <h4 id=\"examples-9\">Examples</h4> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">alpha = [1, 2, 3]\nn = 2\ndist = DirichletMultinomial(n, alpha)\n</pre> <p>Creates a 3-class distribution, with the 3rd class is most likely to be drawn. The distribution functions can be evaluated on counts.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># counts same shape as alpha.\ncounts = [0, 0, 2]\ndist.pmf(counts)  # Shape []\n\n# alpha will be broadcast to [[1, 2, 3], [1, 2, 3]] to match counts.\ncounts = [[1, 1, 0], [1, 0, 1]]\ndist.pmf(counts)  # Shape [2]\n\n# alpha will be broadcast to shape [5, 7, 3] to match counts.\ncounts = [[...]]  # Shape [5, 7, 3]\ndist.pmf(counts)  # Shape [5, 7]\n</pre> <p>Creates a 2-batch of 3-class distributions.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">alpha = [[1, 2, 3], [4, 5, 6]]  # Shape [2, 3]\nn = [3, 3]\ndist = DirichletMultinomial(n, alpha)\n\n# counts will be broadcast to [[2, 1, 0], [2, 1, 0]] to match alpha.\ncounts = [2, 1, 0]\ndist.pmf(counts)  # Shape [2]\n</pre>  <h4 id=\"DirichletMultinomial.__init__\"><code>tf.contrib.distributions.DirichletMultinomial.__init__(n, alpha, validate_args=True, allow_nan_stats=False, name='DirichletMultinomial')</code></h4> <p>Initialize a batch of DirichletMultinomial distributions.</p>  <h5 id=\"args-140\">Args:</h5> <ul> <li>\n<code>n</code>: Non-negative floating point tensor, whose dtype is the same as <code>alpha</code>. The shape is broadcastable to <code>[N1,..., Nm]</code> with <code>m &gt;= 0</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different Dirichlet multinomial distributions. Its components should be equal to integer values.</li> <li>\n<code>alpha</code>: Positive floating point tensor, whose dtype is the same as <code>n</code> with shape broadcastable to <code>[N1,..., Nm, k]</code> <code>m &gt;= 0</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different <code>k</code> class Dirichlet multinomial distributions.</li> <li>\n<code>validate_args</code>: Whether to assert valid values for parameters <code>alpha</code> and <code>n</code>, and <code>x</code> in <code>prob</code> and <code>log_prob</code>. If <code>False</code>, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li><p><code>name</code>: The name to prefix Ops created by this distribution class.</p></li> <li><p><code>Examples</code>: </p></li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define 1-batch of 2-class Dirichlet multinomial distribution,\n# also known as a beta-binomial.\ndist = DirichletMultinomial(2.0, [1.1, 2.0])\n\n# Define a 2-batch of 3-class distributions.\ndist = DirichletMultinomial([3., 4], [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n</pre>  <h4 id=\"DirichletMultinomial.allow_nan_stats\"><code>tf.contrib.distributions.DirichletMultinomial.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"DirichletMultinomial.alpha\"><code>tf.contrib.distributions.DirichletMultinomial.alpha</code></h4> <p>Parameter defining this distribution.</p>  <h4 id=\"DirichletMultinomial.batch_shape\"><code>tf.contrib.distributions.DirichletMultinomial.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-141\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-142\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"DirichletMultinomial.cdf\"><code>tf.contrib.distributions.DirichletMultinomial.cdf(x, name='cdf')</code></h4>  <h4 id=\"DirichletMultinomial.dtype\"><code>tf.contrib.distributions.DirichletMultinomial.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"DirichletMultinomial.entropy\"><code>tf.contrib.distributions.DirichletMultinomial.entropy(name='entropy')</code></h4> <p>Entropy of the distribution in nats.</p>  <h4 id=\"DirichletMultinomial.event_shape\"><code>tf.contrib.distributions.DirichletMultinomial.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-142\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-143\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"DirichletMultinomial.get_batch_shape\"><code>tf.contrib.distributions.DirichletMultinomial.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-144\">Returns:</h5> <p>batch shape</p>  <h4 id=\"DirichletMultinomial.get_event_shape\"><code>tf.contrib.distributions.DirichletMultinomial.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-145\">Returns:</h5> <p>event shape</p>  <h4 id=\"DirichletMultinomial.is_continuous\"><code>tf.contrib.distributions.DirichletMultinomial.is_continuous</code></h4>  <h4 id=\"DirichletMultinomial.is_reparameterized\"><code>tf.contrib.distributions.DirichletMultinomial.is_reparameterized</code></h4>  <h4 id=\"DirichletMultinomial.log_cdf\"><code>tf.contrib.distributions.DirichletMultinomial.log_cdf(x, name='log_cdf')</code></h4>  <h4 id=\"DirichletMultinomial.log_pdf\"><code>tf.contrib.distributions.DirichletMultinomial.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"DirichletMultinomial.log_pmf\"><code>tf.contrib.distributions.DirichletMultinomial.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"DirichletMultinomial.log_prob\"><code>tf.contrib.distributions.DirichletMultinomial.log_prob(counts, name='log_prob')</code></h4> <p><code>Log(P[counts])</code>, computed for every batch member.</p> <p>For each batch of counts <code>[n_1,...,n_k]</code>, <code>P[counts]</code> is the probability that after sampling <code>n</code> draws from this Dirichlet Multinomial distribution, the number of draws falling in class <code>j</code> is <code>n_j</code>. Note that different sequences of draws can result in the same counts, thus the probability includes a combinatorial coefficient.</p>  <h5 id=\"args-143\">Args:</h5> <ul> <li>\n<code>counts</code>: Non-negative tensor with dtype <code>dtype</code> and whose shape can be broadcast with <code>self.alpha</code>. For fixed leading dimensions, the last dimension represents counts for the corresponding Dirichlet Multinomial distribution in <code>self.alpha</code>. <code>counts</code> is only legal if it sums up to <code>n</code> and its components are equal to integer values.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"log_prob\".</li> </ul>  <h5 id=\"returns-146\">Returns:</h5> <p>Log probabilities for each record, shape <code>[N1,...,Nn]</code>.</p>  <h4 id=\"DirichletMultinomial.mean\"><code>tf.contrib.distributions.DirichletMultinomial.mean(name='mean')</code></h4> <p>Class means for every batch member.</p>  <h4 id=\"DirichletMultinomial.mode\"><code>tf.contrib.distributions.DirichletMultinomial.mode(name='mode')</code></h4> <p>Mode of the distribution.</p>  <h4 id=\"DirichletMultinomial.n\"><code>tf.contrib.distributions.DirichletMultinomial.n</code></h4> <p>Parameter defining this distribution.</p>  <h4 id=\"DirichletMultinomial.name\"><code>tf.contrib.distributions.DirichletMultinomial.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"DirichletMultinomial.pdf\"><code>tf.contrib.distributions.DirichletMultinomial.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"DirichletMultinomial.pmf\"><code>tf.contrib.distributions.DirichletMultinomial.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"DirichletMultinomial.prob\"><code>tf.contrib.distributions.DirichletMultinomial.prob(counts, name='prob')</code></h4> <p><code>P[counts]</code>, computed for every batch member.</p> <p>For each batch of counts <code>[c_1,...,c_k]</code>, <code>P[counts]</code> is the probability that after sampling <code>sum_j c_j</code> draws from this Dirichlet Multinomial distribution, the number of draws falling in class <code>j</code> is <code>c_j</code>. Note that different sequences of draws can result in the same counts, thus the probability includes a combinatorial coefficient.</p>  <h5 id=\"args-144\">Args:</h5> <ul> <li>\n<code>counts</code>: Non-negative tensor with dtype <code>dtype</code> and whose shape can be broadcast with <code>self.alpha</code>. For fixed leading dimensions, the last dimension represents counts for the corresponding Dirichlet Multinomial distribution in <code>self.alpha</code>. <code>counts</code> is only legal if it sums up to <code>n</code> and its components are equal to integer values.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"prob\".</li> </ul>  <h5 id=\"returns-147\">Returns:</h5> <p>Probabilities for each record, shape <code>[N1,...,Nn]</code>.</p>  <h4 id=\"DirichletMultinomial.sample\"><code>tf.contrib.distributions.DirichletMultinomial.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-145\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-148\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"DirichletMultinomial.sample_n\"><code>tf.contrib.distributions.DirichletMultinomial.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Generate <code>n</code> samples.</p>  <h5 id=\"args-146\">Args:</h5> <ul> <li>\n<code>n</code>: scalar. Number of samples to draw from each distribution.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-149\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"DirichletMultinomial.std\"><code>tf.contrib.distributions.DirichletMultinomial.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"DirichletMultinomial.validate_args\"><code>tf.contrib.distributions.DirichletMultinomial.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"DirichletMultinomial.variance\"><code>tf.contrib.distributions.DirichletMultinomial.variance(name='mean')</code></h4> <p>Class variances for every batch member.</p> <p>The variance for each batch member is defined as the following:</p> <pre class=\"\">Var(X_j) = n * alpha_j / alpha_0 * (1 - alpha_j / alpha_0) *\n  (n + alpha_0) / (1 + alpha_0)\n</pre> <p>where <code>alpha_0 = sum_j alpha_j</code>.</p> <p>The covariance between elements in a batch is defined as:</p> <pre class=\"\">Cov(X_i, X_j) = -n * alpha_i * alpha_j / alpha_0 ** 2 *\n  (n + alpha_0) / (1 + alpha_0)\n</pre>  <h5 id=\"args-147\">Args:</h5> <ul> <li>\n<code>name</code>: The name for this op.</li> </ul>  <h5 id=\"returns-150\">Returns:</h5> <p>A <code>Tensor</code> representing the variances for each batch member.</p>  <h3 id=\"Multinomial\"><code>class tf.contrib.distributions.Multinomial</code></h3> <p>Multinomial distribution.</p> <p>This distribution is parameterized by a vector <code>p</code> of probability parameters for <code>k</code> classes and <code>n</code>, the counts per each class..</p>  <h4 id=\"mathematical-details-11\">Mathematical details</h4> <p>The Multinomial is a distribution over k-class count data, meaning for each k-tuple of non-negative integer <code>counts = [n_1,...,n_k]</code>, we have a probability of these draws being made from the distribution. The distribution has hyperparameters <code>p = (p_1,...,p_k)</code>, and probability mass function (pmf):</p> <p><code>pmf(counts) = n! / (n_1!...n_k!) * (p_1)^n_1*(p_2)^n_2*...(p_k)^n_k</code></p> <p>where above <code>n = sum_j n_j</code>, <code>n!</code> is <code>n</code> factorial.</p>  <h4 id=\"examples-10\">Examples</h4> <p>Create a 3-class distribution, with the 3rd class is most likely to be drawn, using logits..</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">logits = [-50., -43, 0]\ndist = Multinomial(n=4., logits=logits)\n</pre> <p>Create a 3-class distribution, with the 3rd class is most likely to be drawn.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">p = [.2, .3, .5]\ndist = Multinomial(n=4., p=p)\n</pre> <p>The distribution functions can be evaluated on counts.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># counts same shape as p.\ncounts = [1., 0, 3]\ndist.prob(counts)  # Shape []\n\n# p will be broadcast to [[.2, .3, .5], [.2, .3, .5]] to match counts.\ncounts = [[1., 2, 1], [2, 2, 0]]\ndist.prob(counts)  # Shape [2]\n\n# p will be broadcast to shape [5, 7, 3] to match counts.\ncounts = [[...]]  # Shape [5, 7, 3]\ndist.prob(counts)  # Shape [5, 7]\n</pre> <p>Create a 2-batch of 3-class distributions.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\">p = [[.1, .2, .7], [.3, .3, .4]]  # Shape [2, 3]\ndist = Multinomial(n=[4., 5], p=p)\n\ncounts = [[2., 1, 1], [3, 1, 1]]\ndist.prob(counts)  # Shape [2]\n</pre>  <h4 id=\"Multinomial.__init__\"><code>tf.contrib.distributions.Multinomial.__init__(n, logits=None, p=None, validate_args=True, allow_nan_stats=False, name='Multinomial')</code></h4> <p>Initialize a batch of Multinomial distributions.</p>  <h5 id=\"args-148\">Args:</h5> <ul> <li>\n<code>n</code>: Non-negative floating point tensor with shape broadcastable to <code>[N1,..., Nm]</code> with <code>m &gt;= 0</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different Multinomial distributions. Its components should be equal to integer values.</li> <li>\n<code>logits</code>: Floating point tensor representing the log-odds of a positive event with shape broadcastable to <code>[N1,..., Nm, k], m &gt;= 0</code>, and the same dtype as <code>n</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different <code>k</code> class Multinomial distributions.</li> <li>\n<code>p</code>: Positive floating point tensor with shape broadcastable to <code>[N1,..., Nm, k]</code> <code>m &gt;= 0</code> and same dtype as <code>n</code>. Defines this as a batch of <code>N1 x ... x Nm</code> different <code>k</code> class Multinomial distributions. <code>p</code>'s components in the last portion of its shape should sum up to 1.</li> <li>\n<code>validate_args</code>: Whether to assert valid values for parameters <code>n</code> and <code>p</code>, and <code>x</code> in <code>prob</code> and <code>log_prob</code>. If <code>False</code>, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: Boolean, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member. If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li><p><code>name</code>: The name to prefix Ops created by this distribution class.</p></li> <li><p><code>Examples</code>: </p></li> </ul> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Define 1-batch of 2-class multinomial distribution,\n# also known as a Binomial distribution.\ndist = Multinomial(n=2., p=[.1, .9])\n\n# Define a 2-batch of 3-class distributions.\ndist = Multinomial(n=[4., 5], p=[[.1, .3, .6], [.4, .05, .55]])\n</pre>  <h4 id=\"Multinomial.allow_nan_stats\"><code>tf.contrib.distributions.Multinomial.allow_nan_stats</code></h4> <p>Boolean describing behavior when a stat is undefined for batch member.</p>  <h4 id=\"Multinomial.batch_shape\"><code>tf.contrib.distributions.Multinomial.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-149\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-151\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"Multinomial.cdf\"><code>tf.contrib.distributions.Multinomial.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"Multinomial.dtype\"><code>tf.contrib.distributions.Multinomial.dtype</code></h4> <p>dtype of samples from this distribution.</p>  <h4 id=\"Multinomial.entropy\"><code>tf.contrib.distributions.Multinomial.entropy(name='entropy')</code></h4> <p>Entropy of the distribution in nats.</p>  <h4 id=\"Multinomial.event_shape\"><code>tf.contrib.distributions.Multinomial.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-150\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op</li> </ul>  <h5 id=\"returns-152\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"Multinomial.get_batch_shape\"><code>tf.contrib.distributions.Multinomial.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-153\">Returns:</h5> <p>batch shape</p>  <h4 id=\"Multinomial.get_event_shape\"><code>tf.contrib.distributions.Multinomial.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-154\">Returns:</h5> <p>event shape</p>  <h4 id=\"Multinomial.is_continuous\"><code>tf.contrib.distributions.Multinomial.is_continuous</code></h4>  <h4 id=\"Multinomial.is_reparameterized\"><code>tf.contrib.distributions.Multinomial.is_reparameterized</code></h4>  <h4 id=\"Multinomial.log_cdf\"><code>tf.contrib.distributions.Multinomial.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"Multinomial.log_pdf\"><code>tf.contrib.distributions.Multinomial.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"Multinomial.log_pmf\"><code>tf.contrib.distributions.Multinomial.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"Multinomial.log_prob\"><code>tf.contrib.distributions.Multinomial.log_prob(counts, name='log_prob')</code></h4> <p><code>Log(P[counts])</code>, computed for every batch member.</p> <p>For each batch of counts <code>[n_1,...,n_k]</code>, <code>P[counts]</code> is the probability that after sampling <code>n</code> draws from this Multinomial distribution, the number of draws falling in class <code>j</code> is <code>n_j</code>. Note that different sequences of draws can result in the same counts, thus the probability includes a combinatorial coefficient.</p>  <h5 id=\"args-151\">Args:</h5> <ul> <li>\n<code>counts</code>: Non-negative tensor with dtype <code>dtype</code> and whose shape can be broadcast with <code>self.p</code> and <code>self.n</code>. For fixed leading dimensions, the last dimension represents counts for the corresponding Multinomial distribution in <code>self.p</code>. <code>counts</code> is only legal if it sums up to <code>n</code> and its components are equal to integer values.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"log_prob\".</li> </ul>  <h5 id=\"returns-155\">Returns:</h5> <p>Log probabilities for each record, shape <code>[N1,...,Nm]</code>.</p>  <h4 id=\"Multinomial.logits\"><code>tf.contrib.distributions.Multinomial.logits</code></h4> <p>Log-odds.</p>  <h4 id=\"Multinomial.mean\"><code>tf.contrib.distributions.Multinomial.mean(name='mean')</code></h4> <p>Mean of the distribution.</p>  <h4 id=\"Multinomial.mode\"><code>tf.contrib.distributions.Multinomial.mode(name='mode')</code></h4> <p>Mode of the distribution.</p>  <h4 id=\"Multinomial.n\"><code>tf.contrib.distributions.Multinomial.n</code></h4> <p>Number of trials.</p>  <h4 id=\"Multinomial.name\"><code>tf.contrib.distributions.Multinomial.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"Multinomial.p\"><code>tf.contrib.distributions.Multinomial.p</code></h4> <p>Event probabilities.</p>  <h4 id=\"Multinomial.pdf\"><code>tf.contrib.distributions.Multinomial.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"Multinomial.pmf\"><code>tf.contrib.distributions.Multinomial.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"Multinomial.prob\"><code>tf.contrib.distributions.Multinomial.prob(counts, name='prob')</code></h4> <p><code>P[counts]</code>, computed for every batch member.</p> <p>For each batch of counts <code>[n_1,...,n_k]</code>, <code>P[counts]</code> is the probability that after sampling <code>n</code> draws from this Multinomial distribution, the number of draws falling in class <code>j</code> is <code>n_j</code>. Note that different sequences of draws can result in the same counts, thus the probability includes a combinatorial coefficient.</p>  <h5 id=\"args-152\">Args:</h5> <ul> <li>\n<code>counts</code>: Non-negative tensor with dtype <code>dtype</code> and whose shape can be broadcast with <code>self.p</code> and <code>self.n</code>. For fixed leading dimensions, the last dimension represents counts for the corresponding Multinomial distribution in <code>self.p</code>. <code>counts</code> is only legal if it sums up to <code>n</code> and its components are equal to integer values.</li> <li>\n<code>name</code>: Name to give this Op, defaults to \"prob\".</li> </ul>  <h5 id=\"returns-156\">Returns:</h5> <p>Probabilities for each record, shape <code>[N1,...,Nm]</code>.</p>  <h4 id=\"Multinomial.sample\"><code>tf.contrib.distributions.Multinomial.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-153\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-157\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"Multinomial.sample_n\"><code>tf.contrib.distributions.Multinomial.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Generate <code>n</code> samples.</p>  <h5 id=\"args-154\">Args:</h5> <ul> <li>\n<code>n</code>: scalar. Number of samples to draw from each distribution.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-158\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of shape <code>(n,) + self.batch_shape + self.event_shape</code> with values of type <code>self.dtype</code>.</li> </ul>  <h4 id=\"Multinomial.std\"><code>tf.contrib.distributions.Multinomial.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"Multinomial.validate_args\"><code>tf.contrib.distributions.Multinomial.validate_args</code></h4> <p>Boolean describing behavior on invalid input.</p>  <h4 id=\"Multinomial.variance\"><code>tf.contrib.distributions.Multinomial.variance(name='variance')</code></h4> <p>Variance of the distribution.</p>  <h3 id=\"transformed-distributions\">Transformed distributions</h3>  <h3 id=\"TransformedDistribution\"><code>class tf.contrib.distributions.TransformedDistribution</code></h3> <p>A Transformed Distribution.</p> <p>A Transformed Distribution models <code>p(y)</code> given a base distribution <code>p(x)</code>, an invertible transform, <code>y = f(x)</code>, and the determinant of the Jacobian of <code>f(x)</code>.</p> <p>Shapes, type, and reparameterization are taken from the base distribution.</p>  <h4 id=\"mathematical-details-12\">Mathematical details</h4> <ul> <li>\n<code>p(x)</code> - probability distribution for random variable X</li> <li>\n<code>p(y)</code> - probability distribution for random variable Y</li> <li>\n<code>f</code> - transform</li> <li>\n<code>g</code> - inverse transform, <code>g(f(x)) = x</code>\n</li> <li>\n<code>J(x)</code> - Jacobian of f(x)</li> </ul> <p>A Transformed Distribution exposes <code>sample</code> and <code>pdf</code>:</p> <ul> <li>\n<code>sample</code>: <code>y = f(x)</code>, after drawing a sample of X.</li> <li>\n<code>pdf</code>: <code>p(y) = p(x) / det|J(x)| = p(g(y)) / det|J(g(y))|</code>\n</li> </ul> <p>A simple example constructing a Log-Normal distribution from a Normal distribution:</p> <pre class=\"\">logit_normal = TransformedDistribution(\n  base_dist=Normal(mu, sigma),\n  transform=lambda x: tf.sigmoid(x),\n  inverse=lambda y: tf.log(y) - tf.log(1. - y),\n  log_det_jacobian=(lambda x:\n      tf.reduce_sum(tf.log(tf.sigmoid(x)) + tf.log(1. - tf.sigmoid(x)),\n                    reduction_indices=[-1])))\n  name=\"LogitNormalTransformedDistribution\"\n)\n</pre>  <h4 id=\"TransformedDistribution.__init__\"><code>tf.contrib.distributions.TransformedDistribution.__init__(base_dist_cls, transform, inverse, log_det_jacobian, name='TransformedDistribution', **base_dist_args)</code></h4> <p>Construct a Transformed Distribution.</p>  <h5 id=\"args-155\">Args:</h5> <ul> <li>\n<code>base_dist_cls</code>: the base distribution class to transform. Must be a subclass of <code>Distribution</code>.</li> <li>\n<code>transform</code>: a callable that takes a <code>Tensor</code> sample from <code>base_dist</code> and returns a <code>Tensor</code> of the same shape and type. <code>x =&gt; y</code>.</li> <li>\n<code>inverse</code>: a callable that computes the inverse of transform. <code>y =&gt; x</code>. If None, users can only call <code>log_pdf</code> on values returned by <code>sample</code>.</li> <li>\n<code>log_det_jacobian</code>: a callable that takes a <code>Tensor</code> sample from <code>base_dist</code> and returns the log of the determinant of the Jacobian of <code>transform</code>.</li> <li>\n<code>name</code>: The name for the distribution.</li> <li>\n<code>**base_dist_args</code>: kwargs to pass on to dist_cls on construction.</li> </ul>  <h5 id=\"raises-19\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if <code>base_dist_cls</code> is not a subclass of <code>Distribution</code>.</li> </ul>  <h4 id=\"TransformedDistribution.allow_nan_stats\"><code>tf.contrib.distributions.TransformedDistribution.allow_nan_stats</code></h4>  <h4 id=\"TransformedDistribution.base_distribution\"><code>tf.contrib.distributions.TransformedDistribution.base_distribution</code></h4> <p>Base distribution, p(x).</p>  <h4 id=\"TransformedDistribution.batch_shape\"><code>tf.contrib.distributions.TransformedDistribution.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p> <p>The product of the dimensions of the <code>batch_shape</code> is the number of independent distributions of this kind the instance represents.</p>  <h5 id=\"args-156\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-159\">Returns:</h5> <p><code>Tensor</code> <code>batch_shape</code></p>  <h4 id=\"TransformedDistribution.cdf\"><code>tf.contrib.distributions.TransformedDistribution.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"TransformedDistribution.dtype\"><code>tf.contrib.distributions.TransformedDistribution.dtype</code></h4>  <h4 id=\"TransformedDistribution.entropy\"><code>tf.contrib.distributions.TransformedDistribution.entropy(name='entropy')</code></h4> <p>Entropy of the distribution in nats.</p>  <h4 id=\"TransformedDistribution.event_shape\"><code>tf.contrib.distributions.TransformedDistribution.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h5 id=\"args-157\">Args:</h5> <ul> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-160\">Returns:</h5> <p><code>Tensor</code> <code>event_shape</code></p>  <h4 id=\"TransformedDistribution.get_batch_shape\"><code>tf.contrib.distributions.TransformedDistribution.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>batch_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-161\">Returns:</h5> <p>batch shape</p>  <h4 id=\"TransformedDistribution.get_event_shape\"><code>tf.contrib.distributions.TransformedDistribution.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p> <p>Same meaning as <code>event_shape</code>. May be only partially defined.</p>  <h5 id=\"returns-162\">Returns:</h5> <p>event shape</p>  <h4 id=\"TransformedDistribution.inverse\"><code>tf.contrib.distributions.TransformedDistribution.inverse</code></h4> <p>Inverse function of transform, y =&gt; x.</p>  <h4 id=\"TransformedDistribution.is_continuous\"><code>tf.contrib.distributions.TransformedDistribution.is_continuous</code></h4>  <h4 id=\"TransformedDistribution.is_reparameterized\"><code>tf.contrib.distributions.TransformedDistribution.is_reparameterized</code></h4>  <h4 id=\"TransformedDistribution.log_cdf\"><code>tf.contrib.distributions.TransformedDistribution.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"TransformedDistribution.log_det_jacobian\"><code>tf.contrib.distributions.TransformedDistribution.log_det_jacobian</code></h4> <p>Function computing the log determinant of the Jacobian of transform.</p>  <h4 id=\"TransformedDistribution.log_pdf\"><code>tf.contrib.distributions.TransformedDistribution.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"TransformedDistribution.log_pmf\"><code>tf.contrib.distributions.TransformedDistribution.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"TransformedDistribution.log_prob\"><code>tf.contrib.distributions.TransformedDistribution.log_prob(y, name='log_prob')</code></h4> <p>Log prob of observations in <code>y</code>.</p> <p><code>log ( p(g(y)) / det|J(g(y))| )</code>, where <code>g</code> is the inverse of <code>transform</code>.</p>  <h5 id=\"args-158\">Args:</h5> <ul> <li>\n<code>y</code>: tensor of dtype <code>dtype</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-163\">Returns:</h5> <ul> <li>\n<code>log_pdf</code>: tensor of dtype <code>dtype</code>, the log-PDFs of <code>y</code>.</li> </ul>  <h5 id=\"raises-20\">Raises:</h5> <ul> <li>\n<code>ValueError</code>: if <code>inverse</code> was not provided to the distribution and <code>y</code> was not returned from <code>sample</code>.</li> </ul>  <h4 id=\"TransformedDistribution.mean\"><code>tf.contrib.distributions.TransformedDistribution.mean(name='mean')</code></h4> <p>Mean of the distribution.</p>  <h4 id=\"TransformedDistribution.mode\"><code>tf.contrib.distributions.TransformedDistribution.mode(name='mode')</code></h4> <p>Mode of the distribution.</p>  <h4 id=\"TransformedDistribution.name\"><code>tf.contrib.distributions.TransformedDistribution.name</code></h4>  <h4 id=\"TransformedDistribution.pdf\"><code>tf.contrib.distributions.TransformedDistribution.pdf(value, name='pdf')</code></h4> <p>The probability density function.</p>  <h4 id=\"TransformedDistribution.pmf\"><code>tf.contrib.distributions.TransformedDistribution.pmf(value, name='pmf')</code></h4> <p>The probability mass function.</p>  <h4 id=\"TransformedDistribution.prob\"><code>tf.contrib.distributions.TransformedDistribution.prob(y, name='prob')</code></h4> <p>The prob of observations in <code>y</code>.</p> <p><code>p(g(y)) / det|J(g(y))|</code>, where <code>g</code> is the inverse of <code>transform</code>.</p>  <h5 id=\"args-159\">Args:</h5> <ul> <li>\n<code>y</code>: <code>Tensor</code> of dtype <code>dtype</code>.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-164\">Returns:</h5> <ul> <li>\n<code>pdf</code>: <code>Tensor</code> of dtype <code>dtype</code>, the pdf values of <code>y</code>.</li> </ul>  <h4 id=\"TransformedDistribution.sample\"><code>tf.contrib.distributions.TransformedDistribution.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-160\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-165\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"TransformedDistribution.sample_n\"><code>tf.contrib.distributions.TransformedDistribution.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations.</p> <p>Samples from the base distribution and then passes through the transform.</p>  <h5 id=\"args-161\">Args:</h5> <ul> <li>\n<code>n</code>: scalar, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-166\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples.</li> </ul>  <h4 id=\"TransformedDistribution.std\"><code>tf.contrib.distributions.TransformedDistribution.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"TransformedDistribution.transform\"><code>tf.contrib.distributions.TransformedDistribution.transform</code></h4> <p>Function transforming x =&gt; y.</p>  <h4 id=\"TransformedDistribution.validate_args\"><code>tf.contrib.distributions.TransformedDistribution.validate_args</code></h4>  <h4 id=\"TransformedDistribution.variance\"><code>tf.contrib.distributions.TransformedDistribution.variance(name='variance')</code></h4> <p>Variance of the distribution.</p>  <h2 id=\"posterior-inference-with-conjugate-priors\">Posterior inference with conjugate priors.</h2> <p>Functions that transform conjugate prior/likelihood pairs to distributions representing the posterior or posterior predictive.</p>  <h3 id=\"normal-likelihood-with-conjugate-prior\">Normal likelihood with conjugate prior.</h3>  <h3 id=\"normal_conjugates_known_sigma_posterior\"><code>tf.contrib.distributions.normal_conjugates_known_sigma_posterior(prior, sigma, s, n)</code></h3> <p>Posterior Normal distribution with conjugate prior on the mean.</p> <p>This model assumes that <code>n</code> observations (with sum <code>s</code>) come from a Normal with unknown mean <code>mu</code> (described by the Normal <code>prior</code>) and known variance <code>sigma^2</code>. The \"known sigma posterior\" is the distribution of the unknown <code>mu</code>.</p> <p>Accepts a prior Normal distribution object, having parameters <code>mu0</code> and <code>sigma0</code>, as well as known <code>sigma</code> values of the predictive distribution(s) (also assumed Normal), and statistical estimates <code>s</code> (the sum(s) of the observations) and <code>n</code> (the number(s) of observations).</p> <p>Returns a posterior (also Normal) distribution object, with parameters <code>(mu', sigma'^2)</code>, where:</p> <pre class=\"\">mu ~ N(mu', sigma'^2)\nsigma'^2 = 1/(1/sigma0^2 + n/sigma^2),\nmu' = (mu0/sigma0^2 + s/sigma^2) * sigma'^2.\n</pre> <p>Distribution parameters from <code>prior</code>, as well as <code>sigma</code>, <code>s</code>, and <code>n</code>. will broadcast in the case of multidimensional sets of parameters.</p>  <h5 id=\"args-162\">Args:</h5> <ul> <li>\n<code>prior</code>: <code>Normal</code> object of type <code>dtype</code>: the prior distribution having parameters <code>(mu0, sigma0)</code>.</li> <li>\n<code>sigma</code>: tensor of type <code>dtype</code>, taking values <code>sigma &gt; 0</code>. The known stddev parameter(s).</li> <li>\n<code>s</code>: Tensor of type <code>dtype</code>. The sum(s) of observations.</li> <li>\n<code>n</code>: Tensor of type <code>int</code>. The number(s) of observations.</li> </ul>  <h5 id=\"returns-167\">Returns:</h5> <p>A new Normal posterior distribution object for the unknown observation mean <code>mu</code>.</p>  <h5 id=\"raises-21\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if dtype of <code>s</code> does not match <code>dtype</code>, or <code>prior</code> is not a Normal object.</li> </ul>  <h3 id=\"normal_congugates_known_sigma_predictive\"><code>tf.contrib.distributions.normal_congugates_known_sigma_predictive(prior, sigma, s, n)</code></h3> <p>Posterior predictive Normal distribution w. conjugate prior on the mean.</p> <p>This model assumes that <code>n</code> observations (with sum <code>s</code>) come from a Normal with unknown mean <code>mu</code> (described by the Normal <code>prior</code>) and known variance <code>sigma^2</code>. The \"known sigma predictive\" is the distribution of new observations, conditioned on the existing observations and our prior.</p> <p>Accepts a prior Normal distribution object, having parameters <code>mu0</code> and <code>sigma0</code>, as well as known <code>sigma</code> values of the predictive distribution(s) (also assumed Normal), and statistical estimates <code>s</code> (the sum(s) of the observations) and <code>n</code> (the number(s) of observations).</p> <p>Calculates the Normal distribution(s) <code>p(x | sigma^2)</code>:</p> <pre class=\"\">p(x | sigma^2) = int N(x | mu, sigma^2) N(mu | prior.mu, prior.sigma^2) dmu\n               = N(x | prior.mu, 1/(sigma^2 + prior.sigma^2))\n</pre> <p>Returns the predictive posterior distribution object, with parameters <code>(mu', sigma'^2)</code>, where:</p> <pre class=\"\">sigma_n^2 = 1/(1/sigma0^2 + n/sigma^2),\nmu' = (mu0/sigma0^2 + s/sigma^2) * sigma_n^2.\nsigma'^2 = sigma_n^2 + sigma^2,\n</pre> <p>Distribution parameters from <code>prior</code>, as well as <code>sigma</code>, <code>s</code>, and <code>n</code>. will broadcast in the case of multidimensional sets of parameters.</p>  <h5 id=\"args-163\">Args:</h5> <ul> <li>\n<code>prior</code>: <code>Normal</code> object of type <code>dtype</code>: the prior distribution having parameters <code>(mu0, sigma0)</code>.</li> <li>\n<code>sigma</code>: tensor of type <code>dtype</code>, taking values <code>sigma &gt; 0</code>. The known stddev parameter(s).</li> <li>\n<code>s</code>: Tensor of type <code>dtype</code>. The sum(s) of observations.</li> <li>\n<code>n</code>: Tensor of type <code>int</code>. The number(s) of observations.</li> </ul>  <h5 id=\"returns-168\">Returns:</h5> <p>A new Normal predictive distribution object.</p>  <h5 id=\"raises-22\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if dtype of <code>s</code> does not match <code>dtype</code>, or <code>prior</code> is not a Normal object.</li> </ul>  <h2 id=\"kullback-leibler-divergence\">Kullback Leibler Divergence</h2>  <h3 id=\"kl\"><code>tf.contrib.distributions.kl(dist_a, dist_b, allow_nan=False, name=None)</code></h3> <p>Get the KL-divergence KL(dist_a || dist_b).</p>  <h5 id=\"args-164\">Args:</h5> <ul> <li>\n<code>dist_a</code>: instance of distributions.Distribution.</li> <li>\n<code>dist_b</code>: instance of distributions.Distribution.</li> <li>\n<code>allow_nan</code>: If <code>False</code> (default), a runtime error is raised if the KL returns NaN values for any batch entry of the given distributions. If <code>True</code>, the KL may return a NaN for the given entry.</li> <li>\n<code>name</code>: (optional) Name scope to use for created operations.</li> </ul>  <h5 id=\"returns-169\">Returns:</h5> <p>A Tensor with the batchwise KL-divergence between dist_a and dist_b.</p>  <h5 id=\"raises-23\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: If dist_a or dist_b is not an instance of Distribution.</li> <li>\n<code>NotImplementedError</code>: If no KL method is defined for distribution types of dist_a and dist_b.</li> </ul>  <h3 id=\"RegisterKL\"><code>class tf.contrib.distributions.RegisterKL</code></h3> <p>Decorator to register a KL divergence implementation function.</p> <p>Usage:</p> <p>@distributions.RegisterKL(distributions.Normal, distributions.Normal) def _kl_normal_mvn(norm_a, norm_b): # Return KL(norm_a || norm_b)</p>  <h4 id=\"RegisterKL.__init__\"><code>tf.contrib.distributions.RegisterKL.__init__(dist_cls_a, dist_cls_b)</code></h4> <p>Initialize the KL registrar.</p>  <h5 id=\"args-165\">Args:</h5> <ul> <li>\n<code>dist_cls_a</code>: the class of the first argument of the KL divergence.</li> <li>\n<code>dist_cls_b</code>: the class of the second argument of the KL divergence.</li> </ul>  <h5 id=\"raises-24\">Raises:</h5> <ul> <li>\n<code>TypeError</code>: if dist_cls_a or dist_cls_b are not subclasses of Distribution.</li> </ul>  <h2 id=\"other-functions-and-classes\">Other Functions and Classes</h2>  <h3 id=\"BaseDistribution\"><code>class tf.contrib.distributions.BaseDistribution</code></h3> <p>Simple abstract base class for probability distributions.</p> <p>Implementations of core distributions to be included in the <code>distributions</code> module should subclass <code>Distribution</code>. This base class may be useful to users that want to fulfill a simpler distribution contract.</p>  <h4 id=\"BaseDistribution.log_prob\"><code>tf.contrib.distributions.BaseDistribution.log_prob(value, name='log_prob')</code></h4> <p>Log of the probability density/mass function.</p>  <h4 id=\"BaseDistribution.name\"><code>tf.contrib.distributions.BaseDistribution.name</code></h4> <p>Name to prepend to all ops.</p>  <h4 id=\"BaseDistribution.prob\"><code>tf.contrib.distributions.BaseDistribution.prob(value, name='prob')</code></h4> <p>Probability density/mass function.</p>  <h4 id=\"BaseDistribution.sample\"><code>tf.contrib.distributions.BaseDistribution.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample.</p>  <h5 id=\"args-166\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: int32 <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-170\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> with prepended dimensions <code>sample_shape</code>.</li> </ul>  <h4 id=\"BaseDistribution.sample_n\"><code>tf.contrib.distributions.BaseDistribution.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Generate <code>n</code> samples.</p>  <h5 id=\"args-167\">Args:</h5> <ul> <li>\n<code>n</code>: scalar. Number of samples to draw.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-171\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> with a prepended dimension (n,).</li> </ul>  <h3 id=\"MultivariateNormalDiagPlusVDVT\"><code>class tf.contrib.distributions.MultivariateNormalDiagPlusVDVT</code></h3> <p>The multivariate normal distribution on <code>R^k</code>.</p> <p>Every batch member of this distribution is defined by a mean and a lightweight covariance matrix <code>C</code>.</p>  <h4 id=\"mathematical-details-13\">Mathematical details</h4> <p>The PDF of this distribution in terms of the mean <code>mu</code> and covariance <code>C</code> is:</p> <pre class=\"\">f(x) = (2 pi)^(-k/2) |det(C)|^(-1/2) exp(-1/2 (x - mu)^T C^{-1} (x - mu))\n</pre> <p>For every batch member, this distribution represents <code>k</code> random variables <code>(X_1,...,X_k)</code>, with mean <code>E[X_i] = mu[i]</code>, and covariance matrix <code>C_{ij} := E[(X_i - mu[i])(X_j - mu[j])]</code></p> <p>The user initializes this class by providing the mean <code>mu</code>, and a lightweight definition of <code>C</code>:</p> <pre class=\"lang-m no-auto-prettify\">C = SS^T = SS = (M + V D V^T) (M + V D V^T)\nM is diagonal (k x k)\nV = is shape (k x r), typically r &lt;&lt; k\nD = is diagonal (r x r), optional (defaults to identity).\n</pre> <p>This allows for <code>O(kr + r^3)</code> pdf evaluation and determinant, and <code>O(kr)</code> sampling and storage (per batch member).</p>  <h4 id=\"examples-11\">Examples</h4> <p>A single multi-variate Gaussian distribution is defined by a vector of means of length <code>k</code>, and square root of the covariance <code>S = M + V D V^T</code>. Extra leading dimensions, if provided, allow for batches.</p> <pre class=\"lang-python no-auto-prettify\" data-language=\"python\"># Initialize a single 3-variate Gaussian with covariance square root\n# S = M + V D V^T, where V D V^T is a matrix-rank 2 update.\nmu = [1, 2, 3.]\ndiag_large = [1.1, 2.2, 3.3]\nv = ... # shape 3 x 2\ndiag_small = [4., 5.]\ndist = tf.contrib.distributions.MultivariateNormalDiagPlusVDVT(\n    mu, diag_large, v, diag_small=diag_small)\n\n# Evaluate this on an observation in R^3, returning a scalar.\ndist.pdf([-1, 0, 1])\n\n# Initialize a batch of two 3-variate Gaussians.  This time, don't provide\n# diag_small.  This means S = M + V V^T.\nmu = [[1, 2, 3], [11, 22, 33]]  # shape 2 x 3\ndiag_large = ... # shape 2 x 3\nv = ... # shape 2 x 3 x 1, a matrix-rank 1 update.\ndist = tf.contrib.distributions.MultivariateNormalDiagPlusVDVT(\n    mu, diag_large, v)\n\n# Evaluate this on a two observations, each in R^3, returning a length two\n# tensor.\nx = [[-1, 0, 1], [-11, 0, 11]]  # Shape 2 x 3.\ndist.pdf(x)\n</pre>  <h4 id=\"MultivariateNormalDiagPlusVDVT.__init__\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.__init__(mu, diag_large, v, diag_small=None, validate_args=True, allow_nan_stats=False, name='MultivariateNormalDiagPlusVDVT')</code></h4> <p>Multivariate Normal distributions on <code>R^k</code>.</p> <p>For every batch member, this distribution represents <code>k</code> random variables <code>(X_1,...,X_k)</code>, with mean <code>E[X_i] = mu[i]</code>, and covariance matrix <code>C_{ij} := E[(X_i - mu[i])(X_j - mu[j])]</code></p> <p>The user initializes this class by providing the mean <code>mu</code>, and a lightweight definition of <code>C</code>:</p> <pre class=\"lang-m no-auto-prettify\">C = SS^T = SS = (M + V D V^T) (M + V D V^T)\nM is diagonal (k x k)\nV = is shape (k x r), typically r &lt;&lt; k\nD = is diagonal (r x r), optional (defaults to identity).\n</pre>  <h5 id=\"args-168\">Args:</h5> <ul> <li>\n<code>mu</code>: Rank <code>n + 1</code> floating point tensor with shape <code>[N1,...,Nn, k]</code>, <code>n &gt;= 0</code>. The means.</li> <li>\n<code>diag_large</code>: Optional rank <code>n + 1</code> floating point tensor, shape <code>[N1,...,Nn, k]</code> <code>n &gt;= 0</code>. Defines the diagonal matrix <code>M</code>.</li> <li>\n<code>v</code>: Rank <code>n + 1</code> floating point tensor, shape <code>[N1,...,Nn, k, r]</code> <code>n &gt;= 0</code>. Defines the matrix <code>V</code>.</li> <li>\n<code>diag_small</code>: Rank <code>n + 1</code> floating point tensor, shape <code>[N1,...,Nn, k]</code> <code>n &gt;= 0</code>. Defines the diagonal matrix <code>D</code>. Default is <code>None</code>, which means <code>D</code> will be the identity matrix.</li> <li>\n<code>validate_args</code>: Whether to validate input with asserts. If <code>validate_args</code> is <code>False</code>, and the inputs are invalid, correct behavior is not guaranteed.</li> <li>\n<code>allow_nan_stats</code>: <code>Boolean</code>, default <code>False</code>. If <code>False</code>, raise an exception if a statistic (e.g. mean/mode/etc...) is undefined for any batch member If <code>True</code>, batch members with valid parameters leading to undefined statistics will return NaN for this statistic.</li> <li>\n<code>name</code>: The name to give Ops created by the initializer.</li> </ul>  <h4 id=\"MultivariateNormalDiagPlusVDVT.allow_nan_stats\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.allow_nan_stats</code></h4> <p><code>Boolean</code> describing behavior when stats are undefined.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.batch_shape\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.batch_shape(name='batch_shape')</code></h4> <p>Batch dimensions of this instance as a 1-D int32 <code>Tensor</code>.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.cdf\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.cdf(value, name='cdf')</code></h4> <p>Cumulative distribution function.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.dtype\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.dtype</code></h4>  <h4 id=\"MultivariateNormalDiagPlusVDVT.entropy\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.entropy(name='entropy')</code></h4> <p>The entropies of these Multivariate Normals.</p>  <h5 id=\"args-169\">Args:</h5> <ul> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-172\">Returns:</h5> <ul> <li>\n<code>entropy</code>: tensor of dtype <code>dtype</code>, the entropies.</li> </ul>  <h4 id=\"MultivariateNormalDiagPlusVDVT.event_shape\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.event_shape(name='event_shape')</code></h4> <p>Shape of a sample from a single distribution as a 1-D int32 <code>Tensor</code>.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.get_batch_shape\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.get_batch_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.get_event_shape\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.get_event_shape()</code></h4> <p><code>TensorShape</code> available at graph construction time.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.is_continuous\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.is_continuous</code></h4>  <h4 id=\"MultivariateNormalDiagPlusVDVT.is_reparameterized\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.is_reparameterized</code></h4>  <h4 id=\"MultivariateNormalDiagPlusVDVT.log_cdf\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.log_cdf(value, name='log_cdf')</code></h4> <p>Log CDF.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.log_pdf\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.log_pdf(value, name='log_pdf')</code></h4> <p>Log of the probability density function.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.log_pmf\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.log_pmf(value, name='log_pmf')</code></h4> <p>Log of the probability mass function.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.log_prob\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.log_prob(x, name='log_prob')</code></h4> <p>Log prob of observations <code>x</code> given these Multivariate Normals.</p> <p><code>x</code> is a batch vector with compatible shape if <code>x</code> is a <code>Tensor</code> whose shape can be broadcast up to either:</p> <pre class=\"\">self.batch_shape + self.event_shape\nOR\n[M1,...,Mm] + self.batch_shape + self.event_shape\n```\n\n##### Args:\n\n\n*  &lt;b&gt;`x`&lt;/b&gt;: Compatible batch vector with same `dtype` as this distribution.\n*  &lt;b&gt;`name`&lt;/b&gt;: The name to give this op.\n\n##### Returns:\n\n\n*  &lt;b&gt;`log_prob`&lt;/b&gt;: tensor of dtype `dtype`, the log-PDFs of `x`.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.log_sigma_det(name='log_sigma_det')` {#MultivariateNormalDiagPlusVDVT.log_sigma_det}\n\nLog of determinant of covariance matrix.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.mean(name='mean')` {#MultivariateNormalDiagPlusVDVT.mean}\n\nMean of each batch member.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.mode(name='mode')` {#MultivariateNormalDiagPlusVDVT.mode}\n\nMode of each batch member.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.mu` {#MultivariateNormalDiagPlusVDVT.mu}\n\n\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.name` {#MultivariateNormalDiagPlusVDVT.name}\n\n\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.pdf(value, name='pdf')` {#MultivariateNormalDiagPlusVDVT.pdf}\n\nThe probability density function.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.pmf(value, name='pmf')` {#MultivariateNormalDiagPlusVDVT.pmf}\n\nThe probability mass function.\n\n\n- - -\n\n#### `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.prob(x, name='prob')` {#MultivariateNormalDiagPlusVDVT.prob}\n\nThe PDF of observations `x` under these Multivariate Normals.\n\n`x` is a batch vector with compatible shape if `x` is a `Tensor` whose\nshape can be broadcast up to either:\n\n</pre> <p>self.batch_shape + self.event_shape OR [M1,...,Mm] + self.batch_shape + self.event_shape ```</p>  <h5 id=\"args-170\">Args:</h5> <ul> <li>\n<code>x</code>: Compatible batch vector with same <code>dtype</code> as this distribution.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-173\">Returns:</h5> <ul> <li>\n<code>prob</code>: tensor of dtype <code>dtype</code>, the prob values of <code>x</code>.</li> </ul>  <h4 id=\"MultivariateNormalDiagPlusVDVT.sample\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.sample(sample_shape=(), seed=None, name='sample')</code></h4> <p>Generate samples of the specified shape for each batched distribution.</p> <p>Note that a call to <code>sample()</code> without arguments will generate a single sample per batched distribution.</p>  <h5 id=\"args-171\">Args:</h5> <ul> <li>\n<code>sample_shape</code>: <code>int32</code> <code>Tensor</code> or tuple or list. Shape of the generated samples.</li> <li>\n<code>seed</code>: Python integer seed for RNG</li> <li>\n<code>name</code>: name to give to the op.</li> </ul>  <h5 id=\"returns-174\">Returns:</h5> <ul> <li>\n<code>samples</code>: a <code>Tensor</code> of dtype <code>self.dtype</code> and shape <code>sample_shape + self.batch_shape + self.event_shape</code>.</li> </ul>  <h4 id=\"MultivariateNormalDiagPlusVDVT.sample_n\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.sample_n(n, seed=None, name='sample_n')</code></h4> <p>Sample <code>n</code> observations from the Multivariate Normal Distributions.</p>  <h5 id=\"args-172\">Args:</h5> <ul> <li>\n<code>n</code>: <code>Scalar</code>, type int32, the number of observations to sample.</li> <li>\n<code>seed</code>: Python integer, the random seed.</li> <li>\n<code>name</code>: The name to give this op.</li> </ul>  <h5 id=\"returns-175\">Returns:</h5> <ul> <li>\n<code>samples</code>: <code>[n, ...]</code>, a <code>Tensor</code> of <code>n</code> samples for each of the distributions determined by broadcasting the hyperparameters.</li> </ul>  <h4 id=\"MultivariateNormalDiagPlusVDVT.sigma\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.sigma</code></h4> <p>Dense (batch) covariance matrix, if available.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.sigma_det\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.sigma_det(name='sigma_det')</code></h4> <p>Determinant of covariance matrix.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.std\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.std(name='std')</code></h4> <p>Standard deviation of the distribution.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.validate_args\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.validate_args</code></h4> <p><code>Boolean</code> describing behavior on invalid input.</p>  <h4 id=\"MultivariateNormalDiagPlusVDVT.variance\"><code>tf.contrib.distributions.MultivariateNormalDiagPlusVDVT.variance(name='variance')</code></h4> <p>Variance of each batch member.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2015 The TensorFlow Authors. All rights reserved.<br>Licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.distributions.html\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.distributions.html</a>\n  </p>\n</div>\n"}